b'information need is an uncertainty that arises in an individual and which the individual is believed to be satisfied by information more footnotes date june 2015 the concept information need is seldom if ever mentioned in the general literature about needs but is a common term in the literature of information science according to hj\xc3\xb8rland 1997 it is closely related to the concept of relevance if something is relevant for a person in relation to a given task we might say that the person needs the information for that task it is often understood as an individual or group s desire to locate and obtain information to satisfy a conscious or unconscious need the information and need in information need are an inseparable interconnection needs and interests call forth information the objectives of studying information needs are the explanation of observed phenomena of information use or expressed need the prediction of instances of information uses the control and thereby improvement of the utilization of information manipulation of essentials conditions information needs are related to but distinct from information requirements an example is that a need is hunger the requirement is food background the concept of information needs was coined by an american information journalist http www libsci sc edu bob isp taylor2 htm robert s taylor in his article http doi wiley com 10 1002 asi 5090130405 the process of asking questions published in american documentation renamed journal of the american society of information science and technology in this paper taylor attempted to describe how an inquirer obtains an answer from an information system by performing the process consciously or unconsciously also he studied the reciprocal influence between the inquirer and a given system according to taylor information need has four levels the conscious and unconscious need for information not existing in the remembered experience of the investigator in terms of the query range this level might be called the ideal question the question which would bring from the ideal system exactly what the inquirer if he could state his need it is the actual but unexpressed need for information the conscious mental description of an ill defined area of in decision in this level the inquirer might talk to someone else in the field to get an answer a researcher forms a rational statement of his question this statement is a rational and unambiguous description of the inquirer s doubts the question as presented to the information system there are variables within a system that influence the question and its formation taylor divided them into five groups general aspects physical and geographical factors system input what type of material is put into the system and what is the unit item internal organization classification indexing subject heading and similar access schemes question input what part do human operators play in the total system output interim feedback herbert menzel preferred demand studies to preference studies requests for information or documents that were actually made by scientists in the course of their activities form the data for demand studies data may be in the form of records of orders placed for bibliographics calls for books from an interlibrary loan system or inquires addressed to an information center or service menzel also investigated user study and defined information seeking behaviour from three angles when approached from the point of view of the scientist or technologists these are studies of scientists communication behaviour when approached from the point of view of any communication medium they are use studies when approached from the science communication system they are studies in the flow of information among scientists and technologists william j paisley moved from information needs uses toward strong guidelines for information system he studied the theories of information processing behavior that will generate propositions concerning channel selection amount of seeking effects on productivity of information quality quantity currency and diversity the role of motivational and personality factors etc he investigated a concentric conceptual framework for user research in the framework he places the information users at the centre of ten systems which are the scientist within his culture the scientist within a political system the scientist within a membership group the scientist within a reference group the scientist within an invisible college the scientist within a formal organization the scientist within a work team the scientist within his own head the scientist within a legal economical system the scientist within a formal see also information retrieval needs references hj\xc3\xb8rland birger 1997 information seeking and subject representation an activity theoretical approach to information science westport co greenwood press menzel herbert information needs and uses in science and technology annual review of information science and technology vol 1 interscience publishers 1966 pp 41 69 paisley william j information needs and uses annual review of information science and technology vol 3 encyclop\xc3\xa6dia britannica inc chicago 1968 pp 1 30 taylor robert s the process of asking questions american documentation vol 13 no 4 october 1962 pp 391 396 doi 10 1002 asi 5090130405 wilson t d on user studies and information needs journal of documentation vol 37 no 1 1981 pp 3 15 category information retrieval'
b'information science information retrieval ir is the activity of obtaining information resources relevant to an information need from a collection of information resources searches can be based on full text search full text or other content based indexing automated information retrieval systems are used to reduce what has been called information overload many university universities and public library public libraries use ir systems to provide access to books journals and other documents web search engine s are the most visible information retrieval applications ir applications overview an information retrieval process begins when a user enters a query string query into the system queries are formal statements of information need s for example search strings in web search engines in information retrieval a query does not uniquely identify a single object in the collection instead several objects may match the query perhaps with different degrees of relevance information retrieval relevancy an object is an entity that is represented by information in a content collection or database user queries are matched against the database information however as opposed to classical sql queries of a database in information retrieval the results returned may or may not match the query so results are typically ranked this ranking of results is a key difference of information retrieval searching compared to database searching ref jansen b j and rieh s 2010 https faculty ist psu edu jjansen academic jansen theoretical constructs pdf the seventeen theoretical constructs of information searching and information retrieval journal of the american society for information sciences and technology 61 8 1517 1534 ref depending on the information retrieval applications application the data objects may be for example text documents images ref name goodron2000 cite journal first abby a last goodrum title image information retrieval an overview of current research journal informing science volume 3 number 2 year 2000 ref audio ref name foote99 cite journal first jonathan last foote title an overview of audio information retrieval journal multimedia systems year 1999 publisher springer ref mind maps ref name beel2009 cite conference first j\xc3\xb6ran last beel first2 bela last2 gipp first3 jan olaf last3 stiller title information retrieval on mind maps what could it be good for url http www sciplore org publications en php conference proceedings of the 5th international conference on collaborative computing networking applications and worksharing collaboratecom 09 year 2009 publisher ieee place washington dc ref or videos often the documents themselves are not kept or stored directly in the ir system but are instead represented in the system by document surrogates or metadata most ir systems compute a numeric score on how well each object in the database matches the query and rank the objects according to this value the top ranking objects are then shown to the user the process may then be iterated if the user wishes to refine the query ref name frakes1992 cite book last frakes first william b title information retrieval data structures algorithms publisher prentice hall inc year 1992 isbn 0 13 463837 9 url http www scribd com doc 13742235 information retrieval data structures algorithms william b frakes ref history rquote right there is a machine called the univac whereby letters and figures are coded as a pattern of magnetic spots on a long steel tape by this means the text of a document preceded by its subject code symbol ca be recorded the machine automatically selects and types out those references which have been coded in any desired way at a rate of 120 words a minute j e holmstrom 1948 the idea of using computers to search for relevant pieces of information was popularized in the article as we may think by vannevar bush in 1945 ref name singhal2001 cite journal last singhal first amit title modern information retrieval a brief overview journal bulletin of the ieee computer society technical committee on data engineering volume 24 issue 4 pages 35 43 year 2001 url http singhal info ieee2001 pdf ref it would appear that bush was inspired by patents for a statistical machine filed by emanuel goldberg in the 1920s and 30s that searched for documents stored on film ref name sanderson2012 cite journal author mark sanderson w bruce croft title the history of information retrieval research journal proceedings of the ieee volume 100 pages 1444 1451 year 2012 url http dx doi org 10 1109 jproc 2012 2189916 doi 10 1109 jproc 2012 2189916 ref the first description of a computer searching for information was described by holmstrom in 1948 ref name holmstrom1948 cite journal author je holmstrom title section iii opening plenary session journal the royal society scientific information conference 21 june 2 july 1948 report and papers submitted pages 85 year 1948 url https books google com au books ei 44vxvzrkgyqu8qx4wypoba id m34laaaamaaj dq e2 80 98section iii opening plenary session 22 the royal society scientific information conference focus searchwithinvolume q univac ref detailing an early mention of the univac computer automated information retrieval systems were introduced in the 1950s one even featured in the 1957 romantic comedy desk set in the 1960s the first large information retrieval research group was formed by gerard salton at cornell by the 1970s several different retrieval techniques had been shown to perform well on small text corpora such as the cranfield collection several thousand documents ref name singhal2001 large scale retrieval systems such as the lockheed dialog system came into use early in the 1970s in 1992 the us department of defense along with the national institute of standards and technology nist cosponsored the text retrieval conference trec as part of the tipster text program the aim of this was to look into the information retrieval community by supplying the infrastructure that was needed for evaluation of text retrieval methodologies on a very large text collection this catalyzed research on methods that scalability scale to huge corpora the introduction of web search engine s has boosted the need for very large scale retrieval systems even further model types file information retrieval models png thumb 500px categorization of ir models translated from de informationsr\xc3\xbcckgewinnung klassifikation von modellen zur repr\xc3\xa4sentation nat\xc3\xbcrlichsprachlicher dokumente german entry original source http www logos verlag de cgi bin engbuchmid isbn 0514 lng eng id dominik kuropka for effectively retrieving relevant documents by ir strategies the documents are typically transformed into a suitable representation each retrieval strategy incorporates a specific model for its document representation purposes the picture on the right illustrates the relationship of some common models in the picture the models are categorized according to two dimensions the mathematical basis and the properties of the model first dimension mathematical basis set theoretic models represent documents as sets of words or phrases similarities are usually derived from set theoretic operations on those sets common models are standard boolean model extended boolean model fuzzy retrieval algebraic models represent documents and queries usually as vectors matrices or tuples the similarity of the query vector and document vector is represented as a scalar value vector space model generalized vector space model topic based vector space model enhanced topic based vector space model extended boolean model latent semantic indexing a k a latent semantic analysis probabilistic models treat the process of document retrieval as a probabilistic inference similarities are computed as probabilities that a document is relevant for a given query probabilistic theorems like the bayes theorem are often used in these models binary independence model probabilistic relevance model on which is based the probabilistic relevance model bm25 okapi bm25 relevance function uncertain inference language model s divergence from randomness model latent dirichlet allocation feature based retrieval models view documents as vectors of values of feature functions or just features and seek the best way to combine these features into a single relevance score typically by learning to rank methods feature functions are arbitrary functions of document and query and as such can easily incorporate almost any other retrieval model as just another feature second dimension properties of the model models without term interdependencies treat different terms words as independent this fact is usually represented in vector space models by the orthogonality assumption of term vectors or in probabilistic models by an independence mathematical logic independency assumption for term variables models with immanent term interdependencies allow a representation of interdependencies between terms however the degree of the interdependency between two terms is defined by the model itself it is usually directly or indirectly derived e g by dimension reduction dimensional reduction from the co occurrence of those terms in the whole set of documents models with transcendent term interdependencies allow a representation of interdependencies between terms but they do not allege how the interdependency between two terms is defined they rely an external source for the degree of interdependency between two terms for example a human or sophisticated algorithms performance and correctness measures further evaluation measures information retrieval the evaluation of an information retrieval system is the process of assessing how well a system meets the information needs of its users traditional evaluation metrics designed for standard boolean model boolean retrieval or top k retrieval include precision and recall many more measures for evaluating the performance of information retrieval systems have also been proposed in general measurement considers a collection of documents to be searched and a search query all common measures described here assume a ground truth notion of relevancy every document is known to be either relevant or non relevant to a particular query in practice queries may be ill posed and there may be different shades of relevancy virtually all modern evaluation metrics e g information retrieval mean average precision mean average precision information retrieval discounted cumulative gain discounted cumulative gain are designed for ranked retrieval without any explicit rank cutoff taking into account the relative order of the documents retrieved by the search engines and giving more weight to documents returned at higher ranks citation needed date june 2015 the mathematical symbols used in the formulas below mean math x cap y math intersection set theory intersection in this case specifying the documents in both sets x and y math x math cardinality in this case the number of documents in set x math int math integral math sum math summation math delta math symmetric difference precision main precision and recall precision is the fraction of the documents retrieved that are relevance information retrieval relevant to the user s information need math mbox precision frac mbox relevant documents cap mbox retrieved documents mbox retrieved documents math in binary classification precision is analogous to positive predictive value precision takes all retrieved documents into account it can also be evaluated at a given cut off rank considering only the topmost results returned by the system this measure is called precision at n or p n note that the meaning and usage of precision in the field of information retrieval differs from the definition of accuracy and precision within other branches of science and statistics recall main precision and recall recall is the fraction of the documents that are relevant to the query that are successfully retrieved math mbox recall frac mbox relevant documents cap mbox retrieved documents mbox relevant documents math in binary classification recall is often called sensitivity and specificity sensitivity so it can be looked at as the probability that a relevant document is retrieved by the query it is trivial to achieve recall of 100 by returning all documents in response to any query therefore recall alone is not enough but one needs to measure the number of non relevant documents also for example by computing the precision fall out the proportion of non relevant documents that are retrieved out of all non relevant documents available math mbox fall out frac mbox non relevant documents cap mbox retrieved documents mbox non relevant documents math in binary classification fall out is closely related to sensitivity and specificity specificity and is equal to math 1 mbox specificity math it can be looked at as the probability that a non relevant document is retrieved by the query it is trivial to achieve fall out of 0 by returning zero documents in response to any query f score f measure main f score the weighted harmonic mean of precision and recall the traditional f measure or balanced f score is math f frac 2 cdot mathrm precision cdot mathrm recall mathrm precision mathrm recall math this is also known as the math f 1 math measure because recall and precision are evenly weighted the general formula for non negative real math beta math is math f beta frac 1 beta 2 cdot mathrm precision cdot mathrm recall beta 2 cdot mathrm precision mathrm recall math two other commonly used f measures are the math f 2 math measure which weights recall twice as much as precision and the math f 0 5 math measure which weights precision twice as much as recall the f measure was derived by van rijsbergen 1979 so that math f beta math measures the effectiveness of retrieval with respect to a user who attaches math beta math times as much importance to recall as precision it is based on van rijsbergen s effectiveness measure math e 1 frac 1 frac alpha p frac 1 alpha r math their relationship is math f beta 1 e math where math alpha frac 1 1 beta 2 math f measure can be a better single metric when compared to precision and recall both precision and recall give different information that can complement each other when combined if one of them excels more than the other f measure will reflect it citation needed date june 2015 average precision average precision redirects here precision and recall are single value metrics based on the whole list of documents returned by the system for systems that return a ranked sequence of documents it is desirable to also consider the order in which the returned documents are presented by computing a precision and recall at every position in the ranked sequence of documents one can plot a precision recall curve plotting precision math p r math as a function of recall math r math average precision computes the average value of math p r math over the interval from math r 0 math to math r 1 math ref name zhu2004 cite journal first mu last zhu title recall precision and average precision url http sas uwaterloo ca stats navigation techreports 04workingpapers 2004 09 pdf year 2004 ref math operatorname avep int 0 1 p r dr math that is the area under the precision recall curve this integral is in practice replaced with a finite sum over every position in the ranked sequence of documents math operatorname avep sum k 1 n p k delta r k math where math k math is the rank in the sequence of retrieved documents math n math is the number of retrieved documents math p k math is the precision at cut off math k math in the list and math delta r k math is the change in recall from items math k 1 math to math k math ref name zhu2004 this finite sum is equivalent to math operatorname avep frac sum k 1 n p k times operatorname rel k mbox number of relevant documents math where math operatorname rel k math is an indicator function equaling 1 if the item at rank math k math is a relevant document zero otherwise ref name turpin2006 cite journal last turpin first andrew last2 scholer first2 falk title user performance versus precision measures for simple search tasks journal proceedings of the 29th annual international acm sigir conference on research and development in information retrieval seattle wa august 06 11 2006 publisher acm location new york ny pages 11 18 doi 10 1145 1148170 1148176 year 2006 isbn 1 59593 369 7 ref note that the average is over all relevant documents and the relevant documents not retrieved get a precision score of zero some authors choose to interpolate the math p r math function to reduce the impact of wiggles in the curve ref name voc2010 cite journal last everingham first mark last2 van gool first2 luc last3 williams first3 christopher k i last4 winn first4 john last5 zisserman first5 andrew title the pascal visual object classes voc challenge journal international journal of computer vision volume 88 issue 2 pages 303 338 publisher springer date june 2010 url http pascallin ecs soton ac uk challenges voc pubs everingham10 pdf accessdate 2011 08 29 doi 10 1007 s11263 009 0275 4 ref ref name nlpbook cite book last manning first christopher d last2 raghavan first2 prabhakar last3 sch\xc3\xbctze first3 hinrich title introduction to information retrieval publisher cambridge university press year 2008 url http nlp stanford edu ir book html htmledition evaluation of ranked retrieval results 1 html ref for example the pascal visual object classes challenge a benchmark for computer vision object detection computes average precision by averaging the precision over a set of evenly spaced recall levels 0 0 1 0 2 1 0 ref name voc2010 ref name nlpbook math operatorname avep frac 1 11 sum r in 0 0 1 ldots 1 0 p operatorname interp r math where math p operatorname interp r math is an interpolated precision that takes the maximum precision over all recalls greater than math r math math p operatorname interp r operatorname max tilde r tilde r geq r p tilde r math an alternative is to derive an analytical math p r math function by assuming a particular parametric distribution for the underlying decision values for example a binormal precision recall curve can be obtained by assuming decision values in both classes to follow a gaussian distribution ref k h brodersen c s ong k e stephan j m buhmann 2010 http icpr2010 org pdfs icpr2010 thbct8 28 pdf the binormal assumption on precision recall curves proceedings of the 20th international conference on pattern recognition 4263 4266 ref precision at k for modern web scale information retrieval recall is no longer a meaningful metric as many queries have thousands of relevant documents and few users will be interested in reading all of them precision and recall precision precision at k documents p k is still a useful metric e g p 10 or precision at 10 corresponds to the number of relevant results on the first search results page but fails to take into account the positions of the relevant documents among the top k citation needed date june 2015 another shortcoming is that on a query with fewer relevant results than k even a perfect system will have a score less than 1 ref name stanford it is easier to score manually since only the top k results need to be examined to determine if they are relevant or not r precision r precision requires knowing all documents that are relevant to a query the number of relevant documents math r math is used as the cutoff for calculation and this varies from query to query for example if there are 15 documents relevant to red in a corpus r 15 r precision for red looks at the top 15 documents returned counts the number that are relevant math r math turns that into a relevancy fraction math r r r 15 math ref name trec15 precision is equal to recall at the r th position ref name stanford cite web url http nlp stanford edu ir book pdf 08eval pdf title chapter 8 evaluation in information retrieval accessdate 2015 06 14 date 2009 authors christopher d manning prabhakar raghavan and hinrich sch\xc3\xbctze part of introduction to information retrieval http nlp stanford edu ir book ref empirically this measure is often highly correlated to mean average precision ref name stanford mean average precision mean average precision redirects here mean average precision for a set of queries is the mean of the average precision scores for each query math operatorname map frac sum q 1 q operatorname avep q q math where q is the number of queries discounted cumulative gain main discounted cumulative gain dcg uses a graded relevance scale of documents from the result set to evaluate the usefulness or gain of a document based on its position in the result list the premise of dcg is that highly relevant documents appearing lower in a search result list should be penalized as the graded relevance value is reduced logarithmically proportional to the position of the result the dcg accumulated at a particular rank position math p math is defined as math mathrm dcg p rel 1 sum i 2 p frac rel i log 2 i math since result set may vary in size among different queries or systems to compare performances the normalised version of dcg uses an ideal dcg to this end it sorts documents of a result list by relevance producing an ideal dcg at position p math idcg p math which normalizes the score math mathrm ndcg p frac dcg p idcg p math the ndcg values for all queries can be averaged to obtain a measure of the average performance of a ranking algorithm note that in a perfect ranking algorithm the math dcg p math will be the same as the math idcg p math producing an ndcg of 1 0 all ndcg calculations are then relative values on the interval 0 0 to 1 0 and so are cross query comparable other measures confusion matrix terms mean reciprocal rank spearman s rank correlation coefficient bpref a summation based measure of how many relevant documents are ranked before irrelevant documents ref name trec15 http trec nist gov pubs trec15 appendices ce measures06 pdf ref gmap geometric mean of per topic average precision ref name trec15 measures based on marginal relevance and document diversity see section link relevance information retrieval problems and alternatives visualization visualizations of information retrieval performance include graphs which chart precision on one axis and recall on the other ref name trec15 histograms of average precision over various topics ref name trec15 receiver operating characteristic roc curve confusion matrix timeline before the 1900s 1801 joseph marie jacquard invents the jacquard loom the first machine to use punched cards to control a sequence of operations 1880s herman hollerith invents an electro mechanical data tabulator using punch cards as a machine readable medium 1890 hollerith punched cards cards keypunch es and tabulating machine tabulators used to process the 1890 us census data 1920s 1930s emanuel goldberg submits patents for his statistical machine a document search engine that used photoelectric cells and pattern recognition to search the metadata on rolls of microfilmed documents 1940s 1950s late 1940s the us military confronted problems of indexing and retrieval of wartime scientific research documents captured from germans 1945 vannevar bush s as we may think appeared in atlantic monthly 1947 hans peter luhn research engineer at ibm since 1941 began work on a mechanized punch card based system for searching chemical compounds 1950s growing concern in the us for a science gap with the ussr motivated encouraged funding and provided a backdrop for mechanized literature searching systems allen kent et al and the invention of citation indexing eugene garfield 1950 the term information retrieval was coined by calvin mooers ref mooers calvin n https babel hathitrust org cgi pt id mdp 39015034570591 view 1up seq 3 the theory of digital handling of non numerical information and its implications to machine economics zator technical bulletin no 48 cited in cite journal last1 fairthorne first1 r a title automatic retrieval of recorded information journal the computer journal date 1958 volume 1 issue 1 page 37 doi 10 1093 comjnl 1 1 36 url http comjnl oxfordjournals org content 1 1 36 short ref 1951 philip bagley conducted the earliest experiment in computerized document retrieval in a master thesis at mit ref name doyle1975 cite book last doyle first lauren last2 becker first2 joseph title information retrieval and processing publisher melville year 1975 pages 410 pp isbn 0 471 22151 1 ref 1955 allen kent joined case western reserve university and eventually became associate director of the center for documentation and communications research that same year kent and colleagues published a paper in american documentation describing the precision and recall measures as well as detailing a proposed framework for evaluating an ir system which included statistical sampling methods for determining the number of relevant documents not retrieved ref cite journal title machine literature searching x machine language factors underlying its design and development doi 10 1002 asi 5090060411 ref 1958 international conference on scientific information washington dc included consideration of ir systems as a solution to problems identified see proceedings of the international conference on scientific information 1958 national academy of sciences washington dc 1959 1959 hans peter luhn published auto encoding of documents for information retrieval 1960s early 1960s gerard salton began work on ir at harvard later moved to cornell 1960 melvin earl maron and john lary sic kuhns ref name maron2008 cite journal title an historical note on the origins of probabilistic indexing last maron first melvin e journal information processing and management volume 44 year 2008 pages 971 972 url http yunus hacettepe edu tr tonta courses spring2008 bby703 maron on probabilistic 20indexing 2008 pdf doi 10 1016 j ipm 2007 02 012 issue 2 ref published on relevance probabilistic indexing and information retrieval in the journal of the acm 7 3 216 244 july 1960 1962 cyril w cleverdon published early findings of the cranfield studies developing a model for ir system evaluation see cyril w cleverdon report on the testing and analysis of an investigation into the comparative efficiency of indexing systems cranfield collection of aeronautics cranfield england 1962 kent published information analysis and retrieval 1963 weinberg report science government and information gave a full articulation of the idea of a crisis of scientific information the report was named after dr alvin weinberg joseph becker and robert m hayes published text on information retrieval becker joseph hayes robert mayo information storage and retrieval tools elements theories new york wiley 1963 1964 karen sp\xc3\xa4rck jones finished her thesis at cambridge synonymy and semantic classification and continued work on computational linguistics as it applies to ir the national bureau of standards sponsored a symposium titled statistical association methods for mechanized documentation several highly significant papers including g salton s first published reference we believe to the smart information retrieval system smart system mid 1960s national library of medicine developed medlars medical literature analysis and retrieval system the first major machine readable database and batch retrieval system project intrex at mit 1965 j c r licklider published libraries of the future 1966 don swanson was involved in studies at university of chicago on requirements for future catalogs late 1960s f wilfrid lancaster completed evaluation studies of the medlars system and published the first edition of his text on information retrieval 1968 gerard salton published automatic information organization and retrieval john w sammon jr s radc tech report some mathematics of information storage and retrieval outlined the vector model 1969 sammon s a nonlinear mapping for data structure analysis ieee transactions on computers was the first proposal for visualization interface to an ir system 1970s early 1970s first online systems nlm s aim twx medline lockheed s dialog sdc s orbit theodor nelson promoting concept of hypertext published computer lib dream machines 1971 nicholas jardine and cornelis j van rijsbergen published the use of hierarchic clustering in information retrieval which articulated the cluster hypothesis ref cite journal author n jardine c j van rijsbergen title the use of hierarchic clustering in information retrieval journal information storage and retrieval volume 7 issue 5 pages 217 240 date december 1971 doi 10 1016 0020 0271 71 90051 9 ref 1975 three highly influential publications by salton fully articulated his vector processing framework and term discrimination model a theory of indexing society for industrial and applied mathematics a theory of term importance in automatic text analysis jasis v 26 a vector space model for automatic indexing communications of the acm cacm 18 11 1978 the first association for computing machinery acm special interest group on information retrieval sigir conference 1979 c j van rijsbergen published information retrieval butterworths heavy emphasis on probabilistic models 1979 tamas doszkocs implemented the cite natural language user interface for medline at the national library of medicine the cite system supported free form query input ranked output and relevance feedback ref doszkocs t e rapp b a 1979 searching medline in english a prototype user inter face with natural language query ranked output and relevance feedback in proceedings of the asis annual meeting 16 131 139 ref 1980s 1980 first international acm sigir conference joint with british computer society ir group in cambridge 1982 nicholas j belkin robert n oddy and helen m brooks proposed the ask anomalous state of knowledge viewpoint for information retrieval this was an important concept though their automated analysis tool proved ultimately disappointing 1983 salton and michael j mcgill published introduction to modern information retrieval mcgraw hill with heavy emphasis on vector models 1985 david blair and bill maron publish an evaluation of retrieval effectiveness for a full text document retrieval system mid 1980s efforts to develop end user versions of commercial ir systems 1985 1993 key papers on and experimental systems for visualization interfaces work by donald b crouch robert r korfhage matthew chalmers anselm spoerri and others 1989 first world wide web proposals by tim berners lee at cern 1990s 1992 first text retrieval conference trec conference 1997 publication of robert r korfhage korfhage s information storage and retrieval ref name korfhage1997 cite book last korfhage first robert r title information storage and retrieval publisher wiley year 1997 pages 368 pp isbn 978 0 471 14338 3 url http www wiley com wileycda wileytitle productcd 0471143383 desccd authorinfo html ref with emphasis on visualization and multi reference point systems late 1990s web search engine s implementation of many features formerly found only in experimental ir systems search engines become the most common and maybe best instantiation of ir models awards in the field tony kent strix award gerard salton award leading ir research groups center for intelligent information retrieval ciir at the university of massachusetts amherst ref cite web url http ciir cs umass edu title center for intelligent information retrieval umass amherst website ciir cs umass edu access date 2016 07 29 ref information retrieval group at the university of glasgow ref cite web url http www gla ac uk schools computing research researchoverview informationretrieval title university of glasgow schools school of computing science research research overview information retrieval website www gla ac uk access date 2016 07 29 ref information and language processing systems ilps at the university of amsterdam ref cite web url http ilps science uva nl title ilps information and language processing systems website ilps language en us access date 2016 07 29 ref language technologies institutes lti at the carnegie mellon university text information management and analysis group timan at the university of illinois at urbana champaign see also div col adversarial information retrieval collaborative information seeking controlled vocabulary cross language information retrieval data mining european summer school in information retrieval human computer information retrieval hcir information extraction information retrieval facility knowledge visualization multimedia information retrieval personal information management relevance information retrieval relevance feedback rocchio classification index search engine search index social information seeking special interest group on information retrieval subject indexing temporal information retrieval tf idf xml retrieval div col end references reflist further reading christopher d manning prabhakar raghavan and hinrich sch uuml tze http www csli stanford edu hinrich information retrieval book html introduction to information retrieval cambridge university press 2008 stefan b uuml ttcher charles l a clarke and gordon v cormack http www ir uwaterloo ca book information retrieval implementing and evaluating search engines mit press cambridge mass 2010 external links wikiquote http www acm org sigir acm sigir information retrieval special interest group http irsg bcs org bcs irsg british computer society information retrieval specialist group http trec nist gov text retrieval conference trec http www isical ac in fire forum for information retrieval evaluation fire http www dcs gla ac uk keith preface html information retrieval online book by c j van rijsbergen http ir dcs gla ac uk wiki information retrieval wiki http ir facility org information retrieval facility http www nonrelevant net information retrieval duth http trec nist gov pubs trec15 appendices ce measures06 pdf trec report on information retrieval evaluation techniques http www ebaytechblog com 2010 11 10 measuring search relevance how ebay measures search relevance http retrieval ceti gr information retrieval performance evaluation tool athena research centre authority control defaultsort information retrieval category articles with inconsistent citation formats category information retrieval category natural language processing'
b'the comprehensive model of information seeking or cmis is a theoretical construct designed to predict how people will seek information it was first developed by j david johnson and has been utilized by a variety of disciplines including library and information science and health communication the cmis has been empirically tested in health and organizational contexts ref johnson j d meischke h 1993 cancer related channel selection an extensionfor a sample of women who have had a mammogram women health 20 31 44 johnson j d donohue w a atkin c k johnson s h 1995 a comprehensive model of information seeking tests focusing on a technical organization science communication 16 274 303 ref the cmis has inherent strengths for studying how people react to health problems such as cancer ref name auto johnson j d andrews j e allard s 2001 a model for understanding and affecting genetics information seeking library and information science research 23 4 335 349 ref the cmis specifies antecedents that explain why people become information seekers information carrier characteristics that shape how people go about looking for information and information seeking actions that reflect the nature of the search itself design file diagram of the comprehensive model of information seeking jpg thumb right the comprehensive model of information seeking the cmis has been quantitatively tested and performs well when it comes to health information seeking behaviors hisb ref name auto there are three main schemas in the cmis these are antecedents information field and information seeking actions the antecedents are those factors that determine how an information consumer will receive the information those factors are demographics personal experience salience and beliefs these factors are fluid and can change during the health information seeking process the second schema is the information fields that consist of characteristics and utilities this schema is concerned with the channels and carriers of information a person s understanding is developed through the information field the third schema involves the transformational processes and measured by the consumer s understanding of the messages received through the information field the final schema involves information seeking actions this is what the consumer does as a result of the first two schemas through information seeking there are three major dimensions the scope depth and method of information seeking ref name auto antecedents the cmis antecedents demographics personal experience salience and beliefs are factors that determine an individual s natural predisposition to search for information from particular information carriers certain types of health information seeking can be triggered by an individual s degree of personal experience with disease ref johnson j d 1997 cancer related information seeking cresskill nj hampton press ref in the cmis framework two personal relevance factors salience and beliefs are seen as the primary determinants in translating a perceived gap into an active search for information salience refers to the personal significance of health information to the individual such as perceptions of risk to one s health which are likely to result in information seeking action however people also may be motivated to gather information to determine the implications of health events for themselves and or others related to their future activities a factor directly related to the rapidly growing field of genetics an individual s beliefs about the nature of a particular disease its impacts and level of control all directly relate to self efficacy one of our key variables and one that plays an important role in information seeking and people s more general pattern of actions related to health ref johnson j d 1997 cancer related information seeking cresskill nj hampton press ref information carrier characteristics the information carrier characteristics are drawn from a model of media exposure and appraisal mea that has been tested on a variety of information carriers including both sources and channels and in a variety of cultural settings following the mea the cmis focuses on editorial tone communication potential and utility in the cmis characteristics are composed of editorial tone which reflects an audience member s perception of credibility while communication potential relates to issues of style and comprehensiveness utility relates the characteristics of a medium directly to the needs of an individual and shares much with the uses and gratifications perspectives for example is the information contained in the medium relevant topical and important for the individual s purposes in general utility is very important for health information seeking ref name auto information seeking actions there are several types of information seeking actions that can result from the impetus provided by the factors identified by the cmis for example search behavior can be characterized by its extent or the number of activities carried out which has two components scope the number of alternatives investigated and depth the number of dimensions of an alternative investigated there is also the method of the search or channel as another major dimension of the search for instance an individual might choose the method of consulting a telephone information service decide to have a narrow scope by only asking questions about smoking cessation clinics but investigate every recommendation in detail thus increasing the depth of the search ref name auto stages in the cmis a key concept from the cmis is the notion of stages or cancer involvement according to the cmis an individual may be at one of four stages regarding a cancer threat and thereby have differing information needs and behaviors the first stage casual is characterized by a general lack of concern or interest at this stage individuals are not purposive in their search for cancer related information rather their search is accidental and aimless even apathetic the second stage is purposive placid this is characterized by the question what can i do to prevent cancer individuals here might have some passing interest in cancer or genetic information but are generally still not affected or directly concerned the third stage is purposive clustered here an individual will be in closer proximity to cancer this is the point at which a person is motivated to look for practical information that will address the specific problem for example a first degree relative of a recently diagnosed breast cancer patient may seek genetic screening or brca mutation brca 1 2 testing the person could clearly benefit from such information seeking behavior since medical authorities acknowledge that early detection of cancer leads to earlier treatments and better treatment outcomes the fourth stage directed includes individuals who have been diagnosed as having cancer such individuals need knowledge for making informed decisions about treatment and management of the disease ref name auto references research help med reflist category communication category information retrieval category health sciences'
b'one source date october 2015 in information retrieval dwell time denotes the time which a user spends viewing a document after clicking a link on a search engine results page search engine results page serp dwell time is the duration between when a user clicks on a search engine result and when the user returns from that result or the user is otherwise seen to have left the result dwell time is a relevance information retrieval relevance indicator of the search result correctly satisfying the information needs intent of the user short dwell times indicate the user s query intent was not satisfied by viewing the result long dwell times indicate the user s query intent was satisfied ref cite web url https blogs bing com webmaster 2011 08 02 how to build quality content title how to build quality content publisher bing blogs ref references references 2 https www impression co uk blog 4004 dwell time understanding dwell time and its impact on search rankings impression digital category information retrieval category information retrieval evaluation category internet search engines web software stub'
b'orphan date june 2016 the evaluation measures of an information retrieval system is the process of assessing how well the search results satisfied the user s query intent the metrics are often split in to multiple categories online metrics measure actual users interactions with the search system offline metrics measure the relevance of the search engine by having expert judges measure how likely each result or the serp page as a whole is to meet the information needs of the user the mathematical symbols used in the formulas below mean math x cap y math intersection set theory intersection in this case specifying the documents in both sets x and y math x math cardinality in this case the number of documents in set x math int math integral math sum math summation math delta math symmetric difference online metrics online metrics are generally created from data mined from search logs the metrics are often used to determine the success of an a b testing a b test session abandonment rate session abandonment rate is a ratio of search session which do not result in a click click through rate click through rate ctr is the ratio of users who click on a specific link to the number of total users who view a page email or advertisement it is commonly used to measure the success of an online advertising campaign for a particular website as well as the effectiveness of email campaigns ref name ama american marketing association dictionary http www marketingpower com layouts dictionary aspx dead link date july 2016 bot internetarchivebot fix attempted yes retrieved 2012 11 02 the marketing accountability standards board masb endorses this definition as part of its ongoing http www commonlanguage wikispaces net common language in marketing project ref session success rate session success rate measures the ratio of user sessions that lead to a success defining success is often dependent on context but for search a successful result is often measured using dwell time information retrieval dwell time as a primary factor along with secondary user interaction for instance the user copying the result url is considered a successful result as is copy pasting from the snippet zero result rate zero result rate zrr is the ratio of search engine results page serps which returned with zero results the metric either indicates a precision and recall recall issue or that the information being searched for is not in the index offline metrics offline metrics are generally created from relevance judgement sessions where the judges score the quality of the search results the judges often score each result of a query as either binary good bad or on a multi level scale of satisfying the needs of the searcher in practice queries may be ill posed and there may be different shades of relevancy for instance there is ambiguity in the query mars the judge does not know if the user is search for mars the planet mars chocolate bar mars the chocolate bar or bruno mars the singer precision main precision and recall precision is the fraction of the documents retrieved that are relevance information retrieval relevant to the user s information need math mbox precision frac mbox relevant documents cap mbox retrieved documents mbox retrieved documents math in binary classification precision is analogous to positive predictive value precision takes all retrieved documents into account it can also be evaluated at a given cut off rank considering only the topmost results returned by the system this measure is called precision at n or p n note that the meaning and usage of precision in the field of information retrieval differs from the definition of accuracy and precision within other branches of science and statistics recall main precision and recall recall is the fraction of the documents that are relevant to the query that are successfully retrieved math mbox recall frac mbox relevant documents cap mbox retrieved documents mbox relevant documents math in binary classification recall is often called sensitivity and specificity sensitivity so it can be looked at as the probability that a relevant document is retrieved by the query it is trivial to achieve recall of 100 by returning all documents in response to any query therefore recall alone is not enough but one needs to measure the number of non relevant documents also for example by computing the precision fall out the proportion of non relevant documents that are retrieved out of all non relevant documents available math mbox fall out frac mbox non relevant documents cap mbox retrieved documents mbox non relevant documents math in binary classification fall out is closely related to sensitivity and specificity specificity and is equal to math 1 mbox specificity math it can be looked at as the probability that a non relevant document is retrieved by the query it is trivial to achieve fall out of 0 by returning zero documents in response to any query f score f measure main f score the weighted harmonic mean of precision and recall the traditional f measure or balanced f score is math f frac 2 cdot mathrm precision cdot mathrm recall mathrm precision mathrm recall math this is also known as the math f 1 math measure because recall and precision are evenly weighted the general formula for non negative real math beta math is math f beta frac 1 beta 2 cdot mathrm precision cdot mathrm recall beta 2 cdot mathrm precision mathrm recall math two other commonly used f measures are the math f 2 math measure which weights recall twice as much as precision and the math f 0 5 math measure which weights precision twice as much as recall the f measure was derived by van rijsbergen 1979 so that math f beta math measures the effectiveness of retrieval with respect to a user who attaches math beta math times as much importance to recall as precision it is based on van rijsbergen s effectiveness measure math e 1 frac 1 frac alpha p frac 1 alpha r math their relationship is math f beta 1 e math where math alpha frac 1 1 beta 2 math f measure can be a better single metric when compared to precision and recall both precision and recall give different information that can complement each other when combined if one of them excels more than the other f measure will reflect it citation needed date june 2015 average precision average precision redirects here precision and recall are single value metrics based on the whole list of documents returned by the system for systems that return a ranked sequence of documents it is desirable to also consider the order in which the returned documents are presented by computing a precision and recall at every position in the ranked sequence of documents one can plot a precision recall curve plotting precision math p r math as a function of recall math r math average precision computes the average value of math p r math over the interval from math r 0 math to math r 1 math ref name zhu2004 cite journal first mu last zhu title recall precision and average precision url http sas uwaterloo ca stats navigation techreports 04workingpapers 2004 09 pdf year 2004 ref math operatorname avep int 0 1 p r dr math that is the area under the precision recall curve this integral is in practice replaced with a finite sum over every position in the ranked sequence of documents math operatorname avep sum k 1 n p k delta r k math where math k math is the rank in the sequence of retrieved documents math n math is the number of retrieved documents math p k math is the precision at cut off math k math in the list and math delta r k math is the change in recall from items math k 1 math to math k math ref name zhu2004 this finite sum is equivalent to math operatorname avep frac sum k 1 n p k times operatorname rel k mbox number of relevant documents math where math operatorname rel k math is an indicator function equaling 1 if the item at rank math k math is a relevant document zero otherwise ref name turpin2006 cite journal last turpin first andrew last2 scholer first2 falk title user performance versus precision measures for simple search tasks journal proceedings of the 29th annual international acm sigir conference on research and development in information retrieval seattle wa august 06 11 2006 publisher acm location new york ny pages 11 18 doi 10 1145 1148170 1148176 year 2006 isbn 1 59593 369 7 ref note that the average is over all relevant documents and the relevant documents not retrieved get a precision score of zero some authors choose to interpolate the math p r math function to reduce the impact of wiggles in the curve ref name voc2010 cite journal last everingham first mark last2 van gool first2 luc last3 williams first3 christopher k i last4 winn first4 john last5 zisserman first5 andrew title the pascal visual object classes voc challenge journal international journal of computer vision volume 88 issue 2 pages 303 338 publisher springer date june 2010 url http pascallin ecs soton ac uk challenges voc pubs everingham10 pdf accessdate 2011 08 29 doi 10 1007 s11263 009 0275 4 ref ref name nlpbook cite book last manning first christopher d last2 raghavan first2 prabhakar last3 sch\xc3\xbctze first3 hinrich title introduction to information retrieval publisher cambridge university press year 2008 url http nlp stanford edu ir book html htmledition evaluation of ranked retrieval results 1 html ref for example the pascal visual object classes challenge a benchmark for computer vision object detection computes average precision by averaging the precision over a set of evenly spaced recall levels 0 0 1 0 2 1 0 ref name voc2010 ref name nlpbook math operatorname avep frac 1 11 sum r in 0 0 1 ldots 1 0 p operatorname interp r math where math p operatorname interp r math is an interpolated precision that takes the maximum precision over all recalls greater than math r math math p operatorname interp r operatorname max tilde r tilde r geq r p tilde r math an alternative is to derive an analytical math p r math function by assuming a particular parametric distribution for the underlying decision values for example a binormal precision recall curve can be obtained by assuming decision values in both classes to follow a gaussian distribution ref k h brodersen c s ong k e stephan j m buhmann 2010 http icpr2010 org pdfs icpr2010 thbct8 28 pdf the binormal assumption on precision recall curves webarchive url https web archive org web 20121208201457 http icpr2010 org pdfs icpr2010 thbct8 28 pdf date december 8 2012 proceedings of the 20th international conference on pattern recognition 4263 4266 ref precision at k for modern web scale information retrieval recall is no longer a meaningful metric as many queries have thousands of relevant documents and few users will be interested in reading all of them precision and recall precision precision at k documents p k is still a useful metric e g p 10 or precision at 10 corresponds to the number of relevant results on the first search results page but fails to take into account the positions of the relevant documents among the top k citation needed date june 2015 another shortcoming is that on a query with fewer relevant results than k even a perfect system will have a score less than 1 ref name stanford it is easier to score manually since only the top k results need to be examined to determine if they are relevant or not r precision r precision requires knowing all documents that are relevant to a query the number of relevant documents math r math is used as the cutoff for calculation and this varies from query to query for example if there are 15 documents relevant to red in a corpus r 15 r precision for red looks at the top 15 documents returned counts the number that are relevant math r math turns that into a relevancy fraction math r r r 15 math ref name trec15 precision is equal to recall at the r th position ref name stanford cite web url http nlp stanford edu ir book pdf 08eval pdf title chapter 8 evaluation in information retrieval accessdate 2015 06 14 date 2009 authors christopher d manning prabhakar raghavan and hinrich sch\xc3\xbctze part of introduction to information retrieval http nlp stanford edu ir book ref empirically this measure is often highly correlated to mean average precision ref name stanford mean average precision mean average precision redirects here mean average precision for a set of queries is the mean of the average precision scores for each query math operatorname map frac sum q 1 q operatorname avep q q math where q is the number of queries discounted cumulative gain main discounted cumulative gain dcg uses a graded relevance scale of documents from the result set to evaluate the usefulness or gain of a document based on its position in the result list the premise of dcg is that highly relevant documents appearing lower in a search result list should be penalized as the graded relevance value is reduced logarithmically proportional to the position of the result the dcg accumulated at a particular rank position math p math is defined as math mathrm dcg p rel 1 sum i 2 p frac rel i log 2 i math since result set may vary in size among different queries or systems to compare performances the normalised version of dcg uses an ideal dcg to this end it sorts documents of a result list by relevance producing an ideal dcg at position p math idcg p math which normalizes the score math mathrm ndcg p frac dcg p idcg p math the ndcg values for all queries can be averaged to obtain a measure of the average performance of a ranking algorithm note that in a perfect ranking algorithm the math dcg p math will be the same as the math idcg p math producing an ndcg of 1 0 all ndcg calculations are then relative values on the interval 0 0 to 1 0 and so are cross query comparable other measures confusion matrix terms mean reciprocal rank spearman s rank correlation coefficient bpref a summation based measure of how many relevant documents are ranked before irrelevant documents ref name trec15 http trec nist gov pubs trec15 appendices ce measures06 pdf ref gmap geometric mean of per topic average precision ref name trec15 measures based on marginal relevance and document diversity see section link relevance information retrieval problems and alternatives visualization visualizations of information retrieval performance include graphs which chart precision on one axis and recall on the other ref name trec15 histograms of average precision over various topics ref name trec15 receiver operating characteristic roc curve confusion matrix non metrics top queries list top queries is noting the most common queries over a fixed amount of time the top queries list assists in knowing the style of queries entered by users non relevance metrics queries per time measuring how many queries are performed on the search system per month day hour minute sec tracks the utilization of the search system it can be used for diagnostics to indicate an unexpected spike in queries or simply as a baseline when comparing with other metrics like query latency for example a spike in query traffic may be used to explain a spike in query latency references references category information retrieval category information retrieval evaluation category internet search engines'
b'orphan date february 2016 in mathematics applied to the study of networks the wiener connector named in honor of chemist harry wiener who first introduced the wiener index is a means of maximizing efficiency in connecting specified query vertices in a network given a connected graph connected undirected graph and a set of query vertices in a graph the minimum wiener connector is an induced subgraph that connects the query vertices and minimizes the sum of shortest path distances among all pairs of vertices in the subgraph in combinatorial optimization the minimum wiener connector problem is the problem of finding the minimum wiener connector it can be thought of as a version of the classic steiner tree problem one of karp s 21 np complete problems where instead of minimizing the size of the tree the objective is to minimize the distances in the subgraph ref name steiner cite journal last1 hwang first1 frank last2 richards first2 dana last3 winter first3 dana last4 winter first4 pawel title the steiner tree problem journal annals of discrete mathematics date 1992 url http www sciencedirect com science bookseries 01675060 53 ref ref name dimacs http dimacs11 cs princeton edu dimacs steiner tree challenge ref the minimum wiener connector was first presented by ruchansky et al in 2015 ref name sigmod cite journal last2 bonchi first2 francesco last3 garcia soriano first3 david last4 gullo first4 francesco last5 kourtellis first5 nicolas date 2015 year title the minimum wiener connector url https arxiv org abs 1504 00513 journal sigmod volume pages via last1 ruchansky first1 natali ref the minimum wiener connector has applications in many domains where there is a graph structure and an interest in learning about connections between sets of individuals for example given a set of patients infected with a viral disease which other patients should be checked to find the culprit or given a set of proteins of interest which other proteins participate in pathways with them problem definition the wiener index is the sum of shortest path distances in a sub graph using math d u v math to denote the shortest path between math u math and math v math the wiener index of a sub graph math s math denoted math w s math is defined as math w s sum u v in s d u v math the minimum wiener connector problem is defined as follows given an undirected and unweighted graph with vertex set math v math and edge set math e math and a set of query vertices math q subseteq v math find a connector math h subseteq v math of minimum wiener index more formally the problem is to compute math operatorname arg min h w h cup q math that is find a connector math h math that minimizes the sum of shortest paths in math h math relationship to steiner tree file steinerexample nicer pdf thumb upright 2 0 the optimal solutions to the steiner tree problem and the minimum wiener connector can differ define the set of query vertices q by q v sub 1 sub hellip v sub 10 sub the unique optimal solution to the steiner tree problem is q itself which has wiener index 165 whereas the optimal solution for the minimum wiener connector problem is q \xe2\x88\xaa r sub 1 sub r sub 2 sub which has wiener index 142 the minimum wiener connector problem is related to the steiner tree problem in the former the objective function in the minimization is the wiener index of the connector whereas in the latter the objective function is the sum of the weights of the edges in the connector the optimum solutions to these problems may differ given the same graph and set of query vertices in fact a solution for the steiner tree problem may be arbitrarily bad for the minimum wiener connector problem the graph on the right provides an example computational complexity hardness the problem is np hard and does not admit a polynomial time approximation scheme unless p np p np ref name sigmod this can be proven using the inapproximability of vertex cover in bounded degree graphs ref name dinursafra cite journal last1 dinur first1 irit last2 safra first2 samuel title on the hardness of approximating minimum vertex cover journal annals of mathematics date 2005 ref although there is no polynomial time approximation scheme there is a polynomial time constant factor approximation an algorithm that finds a connector whose wiener index is within a constant multiplicative factor of the wiener index of the optimum connector in terms of complexity class es the minimum wiener connector problem is in apx but is not in ptas unless p np exact algorithms an exhaustive search over all possible subsets of vertices to find the one that induces the connector of minimum wiener index yields an algorithm that finds the optimum solution in math 2 o n math time that is exponential time on graphs with n vertices in the special case that there are exactly two query vertices the optimum solution is the shortest path joining the two vertices so the problem can be solved in polynomial time by computing the shortest path in fact for any fixed constant number of query vertices an optimum solution can be found in polynomial time approximation algorithms there is a constant factor approximation algorithm for the minimum wiener connector problem that runs in time math o q m log n n log 2 n math on a graph with n vertices m edges and q query vertices roughly the same time it takes to compute shortest path distances from the query vertices to every other vertex in the graph ref name sigmod the central approach of this algorithm is to reduce the problem to the vertex weighted steiner tree problem which admits a constant factor approximation in particular instances related to the minimum wiener connector problem behavior the minimum wiener connector behaves like centrality betweenness centrality betweenness centrality when the query vertices belong to the same community the non query vertices that form the minimum wiener connector tend to belong to the same community and have high centrality within the community such vertices are likely to be influential vertices playing leadership roles in the community in a social network these influential vertices might be good users for spreading information or to target in a viral marketing campaign ref name viral cite journal first1 oliver last1 hinz first2 bernd last2 skiera first3 christian last3 barrot first4 jan u last4 becker title seeding strategies for viral marketing an empirical comparison journal journal of marketing volume 75 number 6 pages 55 71 year 2011 doi 10 1509 jm 10 0088 ref when the query vertices belong to different communities the non query vertices that form the minimum wiener connector contain vertices adjacent to edges that bridge the different communities these vertices span a social network structural holes structural hole in the graph and are important ref name structhole cite conference last1 lou first1 tiancheng last2 tang first2 jie title mining structural hole spanners through information diffusion in social networks booktitle proceedings of the 22nd international conference on world wide web date 2013 isbn 9781450320351 location rio de janeiro brazil pages 825 836 url http dl acm org citation cfm id 2488388 2488461 publisher international world wide web conferences steering committee ref applications the minimum wiener connector is useful in applications in which one wishes to learn about the relationship between a set of vertices in a graph for example in biology it provides insight into how a set of proteins in a protein protein interaction network are related in social network s like twitter it demonstrates the communities to which a set of users belong and how these communities are related in computer network s it may be useful in identifying an efficient way to route a multicast message to a set of destinations references reflist category np complete problems category trees graph theory category computational problems in graph theory category geometric algorithms category geometric graphs category graph algorithms category data mining category social networks category computational biology category computer science category algorithms category information retrieval index'
b'a metadirectory system provides for the flow of data between one or more directory service s and database s in order to maintain synchronization of that data and is an important part of identity management systems the data being synchronized typically are collections of entries that contain user profiles and possibly authentication or policy information most metadirectory deployments synchronize data into at least one lightweight directory access protocol ldap based directory server to ensure that ldap based applications such as single sign on and portal servers have access to recent data even if the data is mastered in a non ldap data source metadirectory products support filtering and transformation of data in transit most identity management suites from commercial vendors include a metadirectory product or a provisioning user provisioning user provisioning product see also virtual directory identity correlation microsoft identity integration server novell identity manager critical path inc critical path metadirectory category directory services category data management'
b'in computer science particularly the field of database s the thomas write rule is a rule in timestamp based concurrency control it can be summarized as ignore outdated writes it states that if a more recent transaction has already written the value of an object then a less recent transaction does not need perform its own write since it will eventually be overwritten by the more recent one the thomas write rule is applied in situations where a predefined logical order is assigned to transactions when they start for example a transaction might be assigned a monotonically increasing timestamp when it is created the rule prevents changes in the order in which the transactions are executed from creating different outputs the outputs will always be consistent with the predefined logical order for example consider a database with 3 variables a b c and two atomic operations c a t1 and c b t2 each transaction involves a read a or b and a write c the only conflict between these transactions is the write on c the following is one possible schedule for the operations of these transactions math begin bmatrix t 1 t 2 read a read b write c write c commit commit end bmatrix longleftrightarrow begin bmatrix t 1 t 2 read a read b write c commit commit end bmatrix math if when the transactions are created t1 is assigned a timestamp that precedes t2 i e according to the logical order t1 comes first then only t2 s write should be visible if however t1 s write is executed after t2 s write then we need a way to detect this and discard the write one practical approach to this is to label each value with a write timestamp wts that indicates the timestamp of the last transaction to modify the value enforcing the thomas write rule only requires checking to see if the write timestamp of the object is greater than the time stamp of the transaction performing a write if so the write is discarded in the example above if we call ts t the timestamp of transaction t and wts o the write timestamp of object o then t2 s write sets wts c to ts t2 when t1 tries to write c it sees that ts t1 wts c and discards the write if a third transaction t3 with ts t3 ts t2 were to then write to c it would get ts t3 wts c and the write would be allowed references cite journal author robert h thomas title a majority consensus approach to concurrency control for multiple copy databases journal acm transactions on database systems year 1979 volume 4 issue 2 pages 180 209 doi 10 1145 320071 320076 category data management category transaction processing compu sci stub'
b'refimprove date july 2007 a navigational database is a type of database in which record computer science records or object computer science objects are found primarily by following references from other objects they were a common type of database in the era when data was stored on magnetic tape the navigational references told the computer where the next record on the tape was stored allowing fast forwarding and in some cases reversing through the records without having to read every record along the way to see if it matched a given criterion the introduction of low cost hard drive s that provided semi random access to data led to new models of database storage better suited to these devices among these the relational database and especially sql became the canonical solution from the 1980s through to about 2010 at that time a reappraisal of the entire database market began the various nosql concepts which has led to the navigational model being reexamined offshoots of the concept especially the graph database are finding new uses in modern transaction processing workloads description navigational interfaces are usually procedural though some modern systems like xpath can be considered to be simultaneously navigational and declarative navigational access is traditionally associated with the network model and hierarchical model of database interfaces and some have even acquired set oriented features ref cite book author b\xc5\x82a\xc5\xbcewicz jacek author2 kr\xc3\xb3likowski zbyszko author3 morzy tadeusz title handbook on data management in information systems publisher springer year 2003 location page 18 url https books google com books id avlzihkyulcc pg pa18 dq 22navigational database 22 wikipedia network model and hierarchical model ie iso 8859 1 doi isbn 3 540 43893 9 ref navigational techniques use pointers and paths to navigate among data records also known as nodes this is in contrast to the relational model implemented in relational database s which strives to use declarative or logic programming techniques that ask the system for what to fetch instead of how to navigate to it for example to give directions to a house the navigational approach would resemble something like get on highway 25 for 8 miles turn onto horse road left at the red barn then stop at the 3rd house down the road whereas the declarative approach would resemble visit the green house s within the following coordinates hierarchical models are also considered navigational because one goes up to parent down to leaves and there are paths such as the familiar file folder paths in hierarchical file systems in general navigational systems will use combinations of paths and prepositions such as next previous first last up down owner etc paths are often formed by concatenation of node computer science node names or node addresses example file 6n graf svg thumb 250px sample database nodes a labeled graph on 6 vertices and 7 edges numbers are used for illustration purposes only in practice more meaningful names are often used other potential attributes are not shown node6 node4 node5 node1 or node6 node4 node5 node1 if there is no link between given nodes then an error condition is usually triggered with a message such as invalid path the path node6 node2 node1 would be invalid in most systems because there is no direct link between node 6 and node 2 the usage of the term navigational allegedly is derived from a statement by charles bachman in which he describes the programmer as navigator while accessing his favored type of database ref cite web url http portal acm org citation cfm id 362534 coll portal dl acm title the programmer as navigator doi 10 1145 355611 362534 publisher portal acm org accessdate 2012 10 01 ref except for hierarchical file systems which some consider a form of database navigational techniques fell out of favor by the 1980s however object oriented programming and xml have kindled a renewed but controversial interest in navigational techniques critics of navigational techniques view them as unstructured spaghetti messes and liken them to the goto command goto of pre structured programming in other words they are allegedly to data organization what goto s were to behavior flow in this view relational techniques provide improved discipline and consistency to data organization and usage because of its roots in set theory and predicate calculus some also suggest that navigational database engines are easier to build and take up less memory ram than relational equivalents however the existence of relational or relational based products of the late 1980s that possessed small engines by today s standards because they didn t use sql suggest this is not necessarily the case whatever the reason navigational techniques are still the preferred way to handle smaller scale structures a current example of navigational structuring can be found in the document object model dom often used in web browsers and closely associated with javascript the dom engine is essentially a light weight navigational database the world wide web itself and wikipedia could potentially be considered forms of navigational databases though they focus on human readable text rather than data on a large scale the web is a network model and on smaller or local scales such as domain and url partitioning it uses hierarchies in contrast the linked data facet of the semantic web is specifically concerned with network scale machine readable data and follows precisely the follow your nose paradigm implied by the navigational idea a new kind of navigational databases fact date august 2015 has recently when date august 2015 emerged the graph databases this category of databases is often included as one of the four family of the nosql databases examples ibm information management system idms see also codasyl graph database network database object database relational database references reflist external links http db engines com en ranking navigational dbms db engines ranking of navigational dbms by popularity updated by month category data management category types of databases'
b'other uses rollback disambiguation selfref for the wikipedia tool see wikipedia rollback feature no footnotes date june 2009 in database technologies a rollback is an operation which returns the database to some previous state rollbacks are important for database data integrity integrity because they mean that the database can be restored to a clean copy even after erroneous operations are performed they are crucial for recovering from database server crashes by rolling back any database transaction transaction which was active at the time of the crash the database is restored to a consistent state the rollback feature is usually implemented with a database log transaction log but can also be implemented via multiversion concurrency control cascading rollback a cascading rollback occurs in database systems when a transaction t1 causes a failure and a rollback must be performed other transactions dependent on t1 s actions must also be rollbacked due to t1 s failure thus causing a cascading effect that is one transaction s failure causes many to fail practical database recovery techniques guarantee cascadeless rollback therefore a cascading rollback is not a desirable result sql in sql code rollback code is a command that causes all data changes since the last code begin work sql begin work code or code start transaction sql start transaction code to be discarded by the relational database management systems rdbms so that the state of the data is rolled back to the way it was before those changes were made a code rollback code statement will also release any existing savepoint s that may be in use in most sql dialects code rollback code s are connection specific this means that if two connections are made to the same database a code rollback code made in one connection will not affect any other connections this is vital for proper concurrent programming concurrency see also savepoint commit data management commit undo schema migration references cite book author ramez elmasri title fundamentals of database systems publisher pearson addison wesley year 2007 isbn 0 321 36957 2 http msdn2 microsoft com en us library ms181299 aspx rollback transaction microsoft sql server http www pantz org software mysql mysqlcommands html sql commands mysql databases web syndication category data management category database theory category sql category transaction processing category reversible computing compu prog stub'
b'refimprove date november 2012 in the fields of database s and transaction processing transaction management a schedule or history of a system is an abstract model to describe execution of transactions running in the system often it is a list of operations actions ordered by time performed by a set of database transaction transactions that are executed together in the system if order in time between certain operations is not determined by the system then a partial order is used examples of such operations are requesting a read operation reading writing aborting committing requesting lock locking etc not all transaction operation types should be included in a schedule and typically only selected operation types e g data access operations are included as needed to reason about and describe certain phenomena schedules and schedule properties are fundamental concepts in database concurrency control theory formal description the following is an example of a schedule math d begin bmatrix t1 t2 t3 r x w x com r y w y com r z w z com end bmatrix math in this example the horizontal axis represents the different transactions in the schedule d the vertical axis represents time order of operations schedule d consists of three transactions t1 t2 t3 the schedule describes the actions of the transactions as seen by the dbms first t1 reads and writes to object x and then commits then t2 reads and writes to object y and commits and finally t3 reads and writes to object z and commits this is an example of a serial schedule i e sequential with no overlap in time because the actions of in all three transactions are sequential and the transactions are not interleaved in time representing the schedule d above by a table rather than a list is just for the convenience of identifying each transaction s operations in a glance this notation is used throughout the article below a more common way in the technical literature for representing such schedule is by a list d r1 x w1 x com1 r2 y w2 y com2 r3 z w3 z com3 usually for the purpose of reasoning about concurrency control in databases an operation is modeled as atomic operation atomic occurring at a point in time without duration when this is not satisfactory start and end time points and possibly other point events are specified rarely real executed operations always have some duration and specified respective times of occurrence of events within them e g exact times of beginning and completion but for concurrency control reasoning usually only the precedence in time of the whole operations without looking into the quite complex details of each operation matters i e which operation is before or after another operation furthermore in many cases the before after relationships between two specific operations do not matter and should not be specified while being specified for other pairs of operations in general operations of transactions in a schedule can interleave i e transactions can be executed concurrently while time orders between operations in each transaction remain unchanged as implied by the transaction s program since not always time orders between all operations of all transactions matter and need to be specified a schedule is in general a partial order between operations rather than a total order where order for each pair is determined as in a list of operations also in the general case each transaction may consist of several processes and itself be properly represented by a partial order of operations rather than a total order thus in general a schedule is a partial order of operations containing embedding the partial orders of all its transactions time order between two operations can be represented by an ordered pair of these operations e g the existence of a pair op1 op2 means that op1 is always before op2 and a schedule in the general case is a set mathematics set of such ordered pairs such a set a schedule is a partial order which can be represented by an acyclic directed graph or directed acyclic graph dag with operations as nodes and time order as a directed edge no cycles are allowed since a cycle means that a first any operation on a cycle can be both before and after any another second operation on the cycle which contradicts our perception of time in many cases a graphical representation of such graph is used to demonstrate a schedule comment since a list of operations and the table notation used in this article always represents a total order between operations schedules that are not a total order cannot be represented by a list but always can be represented by a dag types of schedule serial the transactions are executed non interleaved see example above i e a serial schedule is one in which no transaction starts until a running transaction has ended serializable this section is linked from concurrency control a schedule that is equivalent in its outcome to a serial schedule has the serializability property in schedule e the order in which the actions of the transactions are executed is not the same as in d but in the end e gives the same result as d math e begin bmatrix t1 t2 t3 r x r y r z w x w y w z com com com end bmatrix math conflicting actions two actions are said to be in conflict conflicting pair if the actions belong to different transactions at least one of the actions is a write operation the actions access the same object read or write the following set of actions is conflicting r1 x w2 x w3 x 3 conflicting pairs while the following sets of actions are not r1 x r2 x r3 x r1 x w2 y r3 x conflict equivalence the schedules s1 and s2 are said to be conflict equivalent if the following two conditions are satisfied both schedules s1 and s2 involve the same set of transactions including ordering of actions within each transaction both schedules have same set of conflicting operations conflict serializable a schedule is said to be conflict serializable when the schedule is conflict equivalent to one or more serial schedules another definition for conflict serializability is that a schedule is conflict serializable if and only if its precedence graph serializability graph when only committed transactions are considered is acyclic if the graph is defined to include also uncommitted transactions then cycles involving uncommitted transactions may occur without conflict serializability violation math g begin bmatrix t1 t2 r a r a w b com w a com end bmatrix math which is conflict equivalent to the serial schedule t1 t2 but not t2 t1 commitment ordered pov section commitment ordering date november 2011 a schedule is said to be commitment ordered commit ordered or commitment order serializable if it obeys the commitment ordering co also commit ordering or commit order serializability schedule property this means that the order in time of transactions commitment events is compatible with the precedence partial order of the respective transactions as induced by their schedule s acyclic precedence graph serializability graph conflict graph this implies that it is also conflict serializable the co property is especially effective for achieving global serializability in distributed systems comment commitment ordering which was discovered in 1990 is obviously not mentioned in bern1987 bernstein et al 1987 its correct definition appears in weikum2001 weikum and vossen 2001 however the description there of its related techniques and theory is partial inaccurate and misleading says who date december 2011 for an extensive coverage of commitment ordering and its sources see commitment ordering and the history of commitment ordering view equivalence two schedules s1 and s2 are said to be view equivalent when the following conditions are satisfied if the transaction math t i math in s1 reads an initial value for object x so does the transaction math t i math in s2 if the transaction math t i math in s1 reads the value written by transaction math t j math in s1 for object x so does the transaction math t i math in s2 if the transaction math t i math in s1 is the final transaction to write the value for an object x so is the transaction math t i math in s2 view serializable a schedule is said to be view serializable if it is view equivalent to some serial schedule note that by definition all conflict serializable schedules are view serializable math g begin bmatrix t1 t2 r a r a w b end bmatrix math notice that the above example which is the same as the example in the discussion of conflict serializable is both view serializable and conflict serializable at the same time there are however view serializable schedules that are not conflict serializable those schedules with a transaction performing a blind write math h begin bmatrix t1 t2 t3 r a w a com w a com w a com end bmatrix math the above example is not conflict serializable but it is view serializable since it has a view equivalent serial schedule t1 nbsp t2 nbsp t3 since determining whether a schedule is view serializable is np complete view serializability has little practical interest citation needed date april 2015 recoverable this section is linked from concurrency control transactions commit only after all transactions whose changes they read commit math f begin bmatrix t1 t2 r a w a r a w a com com end bmatrix f2 begin bmatrix t1 t2 r a w a r a w a abort abort end bmatrix math these schedules are recoverable f is recoverable because t1 commits before t2 that makes the value read by t2 correct then t2 can commit itself in f2 if t1 aborted t2 has to abort because the value of a it read is incorrect in both cases the database is left in a consistent state unrecoverable if a transaction t1 aborts and a transaction t2 commits but t2 relied on t1 we have an unrecoverable schedule math g begin bmatrix t1 t2 r a w a r a w a com abort end bmatrix math in this example g is unrecoverable because t2 read the value of a written by t1 and committed t1 later aborted therefore the value read by t2 is wrong but since t2 committed this schedule is unrecoverable avoids cascading aborts rollbacks aca also named cascadeless avoids that a single transaction abort leads to a series of transaction rollbacks a strategy to prevent cascading aborts is to disallow a transaction from reading uncommitted changes from another transaction in the same schedule the following examples are the same as the ones in the discussion on recoverable math f begin bmatrix t1 t2 r a w a r a w a com com end bmatrix f2 begin bmatrix t1 t2 r a w a r a w a abort abort end bmatrix math in this example although f2 is recoverable it does not avoid cascading aborts it can be seen that if t1 aborts t2 will have to be aborted too in order to maintain the correctness of the schedule as t2 has already read the uncommitted value written by t1 the following is a recoverable schedule which avoids cascading abort note however that the update of a by t1 is always lost since t1 is aborted math f3 begin bmatrix t1 t2 r a r a w a w a abort commit end bmatrix math note that this schedule would not be serializable if t1 would be committed cascading aborts avoidance is sufficient but not necessary for a schedule to be recoverable strict a schedule is strict has the strictness property if for any two transactions t1 t2 if a write operation of t1 precedes a conflicting operation of t2 either read or write then the commit or abort event of t1 also precedes that conflicting operation of t2 any strict schedule is cascadeless but not the converse strictness allows efficient recovery of databases from failure hierarchical relationship between serializability classes the following expressions illustrate the hierarachical containment relationships between serializability and serializability correctness recoverability recoverability classes serial sub commitment ordered sub conflict serializable sub view serializable sub all schedules serial sub strict sub avoids cascading aborts sub recoverable sub all schedules the venn diagram below illustrates the above clauses graphically file schedule serializability png frame none venn diagram for serializability and recoverability classes practical implementations in practice most general purpose database systems employ conflict serializable and recoverable primarily strict schedules see also schedule project management references cite id bern1987 phil bernstein philip a bernstein vassos hadzilacos nathan goodman http research microsoft com en us people philbe ccontrol aspx concurrency control and recovery in database systems addison wesley publishing company 1987 isbn 0 201 10715 5 cite cite id weikum2001 gerhard weikum gottfried vossen http www elsevier com wps find bookdescription cws home 677937 description description transactional information systems elsevier 2001 isbn 1 55860 508 8 cite category data management category transaction processing'
b'the enterprise objects framework or more commonly simply eof was introduced by next in 1994 as a pioneering object relational mapping product for its nextstep and openstep development platforms eof abstracts the process of interacting with a relational database mapping database rows to java programming language java or objective c object computer science objects this largely relieves developers from writing low level sql code eof enjoyed some niche success in the mid 1990s among financial institutions who were attracted to the rapid application development advantages of next s object oriented platform since apple inc s merger with next in 1996 eof has evolved into a fully integrated part of webobjects an application server also originally from next history in the early 1990s next computer recognized that connecting to databases was essential to most businesses and yet also potentially complex every data source has a different data access language or application programming interface api driving up the costs to learn and use each vendor s product the next engineers wanted to apply the advantages of object oriented programming by getting objects to talk to relational databases as the two technologies are very different the solution was to create an abstraction layer insulating developers from writing the low level procedural code sql specific to each data source the first attempt came in 1992 with the release of database kit dbkit which wrapped an object oriented framework around any database unfortunately nextstep at the time was not powerful enough and dbkit had serious design flaws next s second attempt came in 1994 with the enterprise objects framework eof version 1 a rewrite programming complete rewrite that was far more modular and openstep compatible eof 1 0 was the first product released by next using the foundation kit and introduced autoreleased objects to the developer community the development team at the time was only four people jack greenfield rich williamson linus upson and dan willhite eof 2 0 released in late 1995 further refined the architecture introducing the editing context at that point the development team consisted of dan willhite craig federighi eric noyau and charly kleissner eof achieved a modest level of popularity in the financial programming community in the mid 1990s but it would come into its own with the emergence of the world wide web and the concept of web application s it was clear that eof could help companies plug their legacy databases into the web without any rewriting of that data with the addition of frameworks to do state management load balancing and dynamic html generation next was able to launch the first object oriented web application server webobjects in 1996 with eof at its core in 2000 apple inc which had merged with next officially dropped eof as a standalone product meaning that developers would be unable to use it to create desktop applications for the forthcoming macos mac os x it would however continue to be an integral part of a major new release of webobjects webobjects 5 released in 2001 was significant for the fact that its frameworks had been ported from their native objective c programming language to the java programming language java language critics of this change argue that most of the power of eof was a side effect of its objective c roots and that eof lost the beauty or simplicity it once had third party tools such as eogenerator help fill the deficiencies introduced by java mainly due to the loss of objective c categories categories the objective c code base was re introduced with some modifications to desktop application developers as core data part of apple s cocoa api cocoa api with the release of mac os x tiger in april 2005 how eof works enterprise objects provides tools and frameworks for object relational mapping the technology specializes in providing mechanisms to retrieve data from various data sources such as relational databases via jdbc and jndi directories and mechanisms to commit data back to those data sources these mechanisms are designed in a layered abstract approach that allows developers to think about data retrieval and commitment at a higher level than a specific data source or data source vendor commented out because image was deleted image eomodeler png frame right eomodeler application icon mac os x central to this mapping is a model file an eomodel that you build with a visual tool mdash either eomodeler or the eomodeler plug in to xcode the mapping works as follows database tables are mapped to classes database columns are mapped to class attributes database rows are mapped to objects or class instances you can build data models based on existing data sources or you can build data models from scratch which you then use to create data structures tables columns joins in a data source the result is that database records can be transposed into java objects the advantage of using data models is that applications are isolated from the idiosyncrasies of the data sources they access this separation of an application s business logic from database logic allows developers to change the database an application accesses without needing to change the application eof provides a level of database transparency not seen in other tools and allows the same model to be used to access different vendor databases and even allows relationships across different vendor databases without changing source code its power comes from exposing the underlying data sources as managed graphs of persistent objects in simple terms this means that it organizes the application s model layer into a set of defined in memory data objects it then tracks changes to these objects and can reverse those changes on demand such as when a user performs an undo command then when it is time to save changes to the application s data it archives the objects to the underlying data sources using inheritance in designing enterprise objects developers can leverage the object oriented feature known as inheritance computer science inheritance a customer object and an employee object for example might both inherit certain characteristics from a more generic person object such as name address and phone number while this kind of thinking is inherent in object oriented design relational databases have no explicit support for inheritance however using enterprise objects you can build data models that reflect object hierarchies that is you can design database tables to support inheritance by also designing enterprise objects that map to multiple tables or particular views of a database table what is an enterprise object eo an enterprise object is analogous to what is often known in object oriented programming as a business object computer science business object mdash a class which models a physical or conceptual object in the business domain e g a customer an order an item etc what makes an eo different from other objects is that its instance data maps to a data store typically an enterprise object contains key value pairs that represent a row in a relational database the key is basically the column name and the value is what was in that row in the database so it can be said that an eo s properties persist beyond the life of any particular running application more precisely an enterprise object is an instance of a class that implements the com webobjects eocontrol eoenterpriseobject interface an enterprise object has a corresponding model called an eomodel that defines the mapping between the class s object model and the database schema however an enterprise object doesn t explicitly know about its model this level of abstraction means that database vendors can be switched without it affecting the developer s code this gives enterprise objects a high degree of reusability eof and core data despite their common origins the two technologies diverged with each technology retaining a subset of the features of the original objective c code base while adding some new features features supported only by eof eof supports custom sql shared editing contexts nested editing contexts and pre fetching and batch faulting of relationships all features of the original objective c implementation not supported by core data core data also does not provide the equivalent of an eomodelgroup the nsmanagedobjectmodel class provides methods for merging models from existing models and for retrieving merged models from bundles features supported only by core data core data supports fetched properties multiple configurations within a managed object model local stores and store aggregation the data for a given entity may be spread across multiple stores customization and localization of property names and validation warnings and the use of predicates for property validation these features of the original objective c implementation are not supported by the java implementation external links http www linuxjournal com article php sid 7101 mode thread order 0 thold 0 article in linuxjournal about gdl2 category data management category next category apple inc software'
b'in telecommunication s a data bank is a repository of information on one or more subjects that is organized in a way that facilitates local or remote information retrieval a data bank may be either centralized or decentralized in computers the data bank is the same as in telecommunication i e it is the repository of data the data in the data bank can be things such as credit card transactions or it can be any data base of a company where large quantities of queries are being processed on daily bases data bank may also refer to an organization primarily concerned with the construction and maintenance of a database see also star wars databank protein data bank national trauma data bank memory bank international tree ring data bank hazardous substances data bank electron microscopy data bank dortmund data bank casio databank conformational dynamics data bank databank systems limited a former new zealand banking agency sources fs1037c ms188 the american heritage dictionary of the english language fourth edition houghton mifflin 2000 external links wiktionary category data management telecomm stub'
b'a nested transaction is a database transaction that is started by an instruction within the scope of an already started transaction nested transactions are implemented differently in different databases however they have in common that the changes are not made visible to any unrelated transactions until the outermost transaction has committed this means that a commit in an inner transaction does not necessarily persist updates to the database in some databases changes made by the nested transaction are not seen by the host transaction until the nested transaction is committed according to some who date november 2009 this follows from the isolation property of transactions the capability to handle nested transactions properly is a prerequisite for true component based application architectures in a component based encapsulated architecture nested transactions can occur without the programmer knowing it a component function may or may not contain a database transaction this is the encapsulated secret of the component see information hiding if a call to such a component function is made inside a begin commit bracket nested transactions occur since popular databases like mysql ref cite web url http dev mysql com doc refman 4 1 en implicit commit html title statements that cause an implicit commit author work mysql 4 1 reference manual publisher oracle accessdate 5 december 2010 ref do not allow nesting begin commit brackets a framework or a transaction monitor is needed to handle this when we speak about nested transactions it should be made clear that this feature is dbms dependent and is not available for all databases theory for nested transactions is similar to the theory for flat transactions ref cite journal last resende first r f last2 el abbadi first2 a title on the serializability theorem for nested transactions journal information processing letters volume 50 issue 4 pages 177 183 date 1994 05 25 doi 10 1016 0020 0190 94 00033 6 ref the banking industry usually processes financial transactions using open nested transactions citation needed date august 2015 which is a looser variant of the nested transaction model that provides higher performance while accepting the accompanying trade offs of inconsistency ref cite journal last weikum first gerhard author2 hans j schek title concepts and applications of multilevel transactions and open nested transactions journal database transaction models for advanced applications pages 515 553 publisher morgan kaufmann year 1992 url http citeseerx ist psu edu viewdoc summary doi 10 1 1 17 7962 isbn 1 55860 214 3 accessdate 2007 11 13 ref further reading gerhard weikum gottfried vossen transactional information systems theory algorithms and the practice of concurrency control and recovery morgan kaufmann 2002 isbn 1 55860 508 8 references reflist category data management category transaction processing database stub'
b'merge to parchive date march 2014 infobox software logo screenshot file quickpar screenshot png 250px caption quickpar 0 9 checking a series of rar file format rar files for integrity collapsible author developer peter clements released 0 1 february 5 2003 ref cite web url http www quickpar org uk releasenotes2 htm title quickpar old release notes accessdate 2010 11 19 ref latest release version 0 9 1 latest release date start date and age 2004 07 04 ref cite web url http www quickpar org uk title quickpar for windows accessdate 2009 09 27 ref latest preview version latest preview date start date and age yyyy mm dd frequently updated programming language operating system microsoft windows platform x86 size language status genre data recovery license proprietary software proprietary freeware website url www quickpar org uk quickpar is a computer program that creates parchive s used as verification and recovery information for a file or group of files and uses the recovery information if available to attempt to reconstruct the originals from the damaged files and the par volumes designed for the microsoft windows operating system it is often used to recover damaged or missing files that have been downloaded through usenet ref cite book last wang first wallace authorlink title steal this file sharing book url https books google com books id fgfms5kymmcc pg pt183 accessdate 2009 09 24 edition 1st date 2004 10 25 publisher no starch press location san francisco california isbn 1 59327 050 x pages 164 167 chapter finding movies or tv shows recovering missing rar files with par and par2 files ref quickpar may also be used under linux via wine software wine ref cite book last petersen first richard title ubuntu 9 04 desktop handbook url https books google com books id xlrpihdyoqc pg pt224 accessdate 2009 09 27 date 2009 05 01 publisher surfing turtle press location los angeles california isbn 0 9820998 4 3 page 224 chapter internet applications ref image quickpar protect screenshot png thumb right par2 file creation screen there are two main versions of parchive par files par and par2 the par2 file format lifts many of its previous restrictions ref cite web url http www quickpar org uk aboutpar2 htm title quickpar about par2 accessdate 2009 09 27 ref quickpar is freeware but not open source it uses the reed solomon error correction algorithm internally to create the error correcting information ref name newsgroups cite journal last1 fellows first1 g title newsgroups reborn the binary posting renaissance doi 10 1016 j diin 2006 04 006 journal digital investigation volume 3 issue 2 pages 73 78 year 2006 pmid pmc ref abandonware though quickpar works well it is currently considered abandonware since there have been no updates for it in age 2004 07 04 years the software parchive windows multipar is actively being developed by another author named yutaka sawada who is adding support for the new par3 file format see also nl data archiving and networked services dans has some similar software references reflist external links official website http www quickpar org uk http www quickpar org uk tutorials htm quickpar tutorial referencing usenet downloads https www livebusinesschat com smf index php board 396 0 multipar successor to quickpar supports par2 par3 and multicore cpu s category data management storage software stub'
b'unreferenced date may 2009 in metadata a match report is a report that compares two distinct data dictionary data dictionaries and creates a list of the data element s that have been identified as semantic equivalence semantically equivalent use of match reports match reports are critical for systems that wish to automatically exchange data such as intelligent software agents if one computer system is requesting a report from a remote system that uses a distinct data dictionary and all of the data elements on the report manifest are included in the match report the report request can be executed match reports are useful if data dictionaries use a metadata tagging system such as the udef see also data dictionary data warehouse metadata semantic equivalence universal data element framework category knowledge representation category data management category technical communication category metadata compu stub'
b'the universal data element framework udef provides the foundation for building an enterprise wide controlled vocabulary it is a standard way of indexing enterprise information that can produce big cost savings udef simplifies information management through consistent classification and assignment of a global standard identifier to the data names and then relating them to similar data element concepts defined by other organizations though this approach is a small part of the overall picture it is potentially a crucial enabler of semantic interoperability how udef works udef provides semantic links through assigning an intelligent derived id as an attribute of the data element essentially labeling the element as a specific data element concept when this udef id exists in both source and target formats it can then be used as an easy analysis point via a match report and then as the primary pivot point for transformations between source and target udef takes a list of high level root object classes and assigns an integer to each class plus alpha characters to each specialization modifier it then also assigns integers to property word plus integers to each specialization modifier these object class alpha integers are concatenated together with the property integers to form a dewey decimal like code for each data element concept examples for the following examples go to http www opengroup org udefinfo htm en defs htm and expand the applicable udef object and property trees assuming an application used by a hospital needs to map the data element concepts to the udef the last name and first name within udef you will find family name and given name under the udef property name of several people that are likely to appear on a medical record that could include the following example data element concepts patient person family name find the word patient under the udef object person and find the word family under the udef property name patient person given name find the word patient under the udef object person and find the word given under the udef property name doctor person family name find the word doctor under the udef object person and find the word family under the udef property name doctor person given name find the word doctor under the udef object person and find the word given under the udef property name the associated udef ids for the above are derived by walking up each tree respectively and using an underscore to separate the object from the property for the examples above the following data element concepts are available within the current udef see http www opengroup org udefinfo htm en ob5 htm and http www opengroup org udefinfo htm en pr10 htm patient person family name the udef id is au 5 11 10 patient person given name the udef id is au 5 12 10 doctor person family name the udef id is aq 5 11 10 doctor person given name the udef id is aq 5 12 10 six basic steps to map enterprise data to the udef there are six basic steps to follow when mapping data element concepts to the udef 1 identify the applicable udef property word that characterizes the dominant attribute property of the data element concept for example name identifier date etc 2 identify the dominant udef object word that the dominant property selected in step 1 is describing for example person name product identifier document date etc 3 by reviewing the udef tree for the selected property identified in step 1 identify applicable qualifiers that are necessary to describe the property word term unambiguously for example family name 4 by reviewing the udef tree for the selected object identified in step 2 identify applicable qualifiers that are necessary to describe the object word term unambiguously for example customer person 5 concatenate the object term and the property term to create a udef naming convention compliant name where it is recognized that the name may seem artificially long for example customer person family name 6 derive a structured id based on the udef taxonomy that carries the udef inherited indexing scheme for example customerpersonfamilyname udefid as 5 11 10 the open group udef project objectives the udef project aims to establish the universal data element framework udef as the universally used classification system for data element concepts it focuses on developing and maintaining the udef as an open standard advocating and promoting it putting in place a technical infrastructure to support it implementing a registry for it and setting up education programs to train information professionals in its use organizations that implement udef will likely realize the greatest benefit by defining their controlled vocabulary based on the udef to help an organization manage its udef based controlled vocabulary it should seriously consider a metadata registry that is based on iso iec 11179 5 history of udef ron schuldt sr enterprise data architect lockheed martin originated the udef concept based on iso iec 11179 metadata standards in the early 1990s currently he is a senior partner with femto data llc ownership of udef intellectual property the open group assumed from the association for enterprise information afei the right to grant public use licensing of the udef the supplier management council electronic enterprise working group of the aerospace industry association aia supports the udef as the naming convention solution to xml interoperability between standards that include all functions throughout a product s life cycle and is working through a well defined process to obtain approval of this position from aia and its member companies criticism classification in udef is sometimes hampered by ad hoc decisions that might produce problems example b be 5 is united kingdom citizen person and c be 5 is european union citizen person as the united kingdom is part of the european union the classification is not unique response the udef is flexible and is designed to match the semantics and behaviour of existing systems therefore if one system has a table for united kingdom citizens and a different system has a table for european union citizens the udef can handle both situations some of the concepts in udef are not as universal as it is claimed they show a lot of bias to anglo american tradition and way of thinking and are not easily transferable to other languages example the following part of the hierarchy shows the concept of an officer j 5 officer person a j 5 contracting officer person a a j 5 procuring contracting officer person a a a j 5 government procuring contracting officer person b a j 5 administrative contracting officer person b j 5 police officer person c j 5 military officer person in many cultures the part of the tree below a j 5 contracting officer person would not be placed under j 5 see officer disambiguation officer as b j 5 see law enforcement officer or c j 5 see officer armed forces see also data integration iso iec 11179 national information exchange model metadata naming conventions programming semantics semantic web semantic equivalency data element representation term controlled vocabulary external links http www opengroup org udefinfo udef project of the open group https www youtube com embed y6hid5qzazq youtube udef tutorial part 1 https www youtube com embed d6dh u8tqhy youtube udef tutorial part 2 http www opengroup org udefinfo faq htm udef frequently asked questions https udef it com udef tools html obtain enhanced udef gap analysis tool in english dutch or french further reading cite book title udef six steps to cost effective data integration author ronald schuldt publisher createspace date november 15 2011 isbn 978 1 4664 6762 0 url https www createspace com 3711806 cite book title udef six steps to cost effective data integration author ronald schuldt and roberta shauger publisher amazon digital services date january 16 2012 isbn 1 4664 6762 2 url http www amazon com dp b006yk6yoq cite book title udef concepts defined reference guide author roberta shauger publisher createspace date december 20 2011 isbn 978 1 4681 1483 6 url https www createspace com 3753707 cite book title udef concepts defined reference guide author roberta shauger and ronald schuldt publisher amazon digital services date january 14 2012 isbn 1 4681 1483 2 url http www amazon com dp b006xxmlqe category data management category interoperability category knowledge representation category metadata category open group standards category software that uses motif category technical communication'
b'data transformation services or dts is a set of objects and utilities to allow the automation of extract transform load extract transform and load operations to or from a database the objects are dts packages and their components and the utilities are called dts tools dts was included with earlier versions of microsoft sql server and was almost always used with sql server databases although it could be used independently with other databases dts allows data to be transformed and loaded from heterogeneous sources using ole db odbc or text only files into any supported database dts can also allow automation of data import or transformation on a scheduled basis and can perform additional functions such as file transfer protocol ftping files and executing external programs in addition dts provides an alternative method of version control and backup for packages when used in conjunction with a version control system such as microsoft visual sourcesafe image dts designer screenshot png right thumb 300px here a dts package is edited with dts designer in windows xp dts has been superseded by sql server integration services in later releases of microsoft sql server though there was some backwards compatibility and ability to run dts packages in the new ssis for a time toc clear history in sql server versions 6 5 and earlier database administrators dbas used sql server transfer manager and bulk copy program included with sql server to transfer data these tools had significant shortcomings and many quantify date may 2014 dbas used third party tools such as pervasive data integrator to transfer data more flexibly and easily with the release of sql server 7 in 1998 data transformation services was packaged with it to replace all these tools sql server 2000 expanded dts functionality in several ways it introduced new types of tasks including the ability to file transfer protocol ftp files move databases or database components and add messages into microsoft message queuing microsoft message queue dts packages can be saved as a visual basic file in sql server 2000 and this can be expanded to save into any com compliant language microsoft also integrated packages into windows 2000 security and made dts tools more user friendly tasks can accept input and output parameters dts comes with all editions of sql server 7 and 2000 but was superseded by sql server integration services in the microsoft sql server 2005 release in 2005 dts packages the dts package is the fundamental logical component of dts every dts object is a information hiding child component of the package packages are used whenever one modifies data using dts all the metadata about the data transformation is contained within the package packages can be saved directly in a sql server or can be saved in the microsoft repository or in component object model com files sql server 2000 also allows a programmer to save packages in a visual basic or other language file when stored to a vb file the package is actually scripted that is a vb script is executed to dynamically create the package objects and its component objects a package can contain any number of activex data objects connection objects but does not have to contain any these allow the package to read data from any ole db compliant data source and can be expanded to handle other sorts of data the functionality of a package is organized into tasks and steps a dts task is a discrete set of functionalities executed as a single step in a dts package each task defines a work item to be performed as part of the data movement and data transformation process or as a job to be executed data transformation services supplies a number of tasks that are part of the dts object model and that can be accessed graphically through the dts designer or accessed programmatically these tasks which can be configured individually cover a wide variety of data copying data transformation and notification situations for example the following types of tasks represent some actions that you can perform by using data transformation service dts executing a single sql statement sending an email and transferring a file with ftp a step within a dts package describes the order in which tasks are run and the precedence constraints that describe what to do in the case damage or of failure these steps can be executed sequentially or in parallel packages can also contain global variable s which can be used throughout the package sql server 2000 allows input and output parameters for tasks greatly expanding the usefulness of global variables dts packages can be edited password protected scheduled for execution and retrieved by version dts tools dts tools packaged with sql server include the dts wizards dts designer and dts programming interfaces dts wizards the dts wizard software wizards can be used to perform simple or common dts tasks these include the import export wizard and the copy of database wizard they provide the simplest method of copying data between ole db data sources there is a great deal of functionality that is not available by merely using a wizard however a package created with a wizard can be saved and later altered with one of the other dts tools a create publishing wizard is also available to schedule packages to run at certain times this only works if sql server agent is running otherwise the package will be scheduled but will not be executed dts designer the dts designer is a graphical tool used to build complex dts packages with workflows and event driven logic dts designer can also be used to edit and customize dts packages created with the dts wizard each connection and task in dts designer is shown with a specific icon computing icon these icons are joined with precedence constraints which specify the order and requirements for tasks to be run one task may run for instance only if another task succeeds or fails other tasks may run concurrently the dts designer has been criticized for having unusual quirks and limitations such as the inability to visually copy and paste multiple tasks at one time many of these shortcomings have been overcome in sql server integration services dts s successor dts query designer a graphical tool used to build information retrieval queries in dts dts run utility dts packages can be run from the command line using the dtsrun utility br the utility is invoked using the following syntax pre dtsrun s server name instance name u user name p password e n package name g package guid string v package version guid string m package password f filename r repository database name a global variable name typeid value l log file name w nt event log completion status z x d y c pre when passing in parameters which are mapped to global variables you are required to include the typeid this is rather difficult to find on the microsoft site below are the typeids used in passing in these values class wikitable sortable type typeid boolean align right 11 currency align right 6 date align right 7 decimal align right 14 hresult align right 25 int align right 22 integer 1 byte align right 16 integer 8 byte align right 20 integer small align right 2 lpwstr align right 31 pointer align right 26 real 4 byte align right 4 real 8 byte align right 5 string align right 8 unsigned int 1 byte align right 17 unsigned int 2 byte align right 18 unsigned int 4 byte align right 19 unsigned int 1 byte align right 21 unsigned int align right 23 see also olap data warehouse data mining sql server integration services meta data services references cite book author1 chaffin mark author2 knight brian author3 robinson todd title professional sql server 2000 dts publisher wrox press wiley publishing inc year 2003 isbn 0 7645 4368 7 external links http msdn2 microsoft com en us library aa933484 sql 80 aspx microsoft sql server data transformation services dts http www sqldts com sql dts unique dts information resource http support microsoft com kb 238912 understanding microsoft repository http pragmaticworks com resources webinars default aspx dts videos training http www softrus org dts dts documenter category microsoft database software category data management category extract transform load tools category microsoft server technology'
b'coi date july 2016 unreferenced date july 2016 vmds abbreviates the relational database technology called version managed data store provided by ge energy as part of its smallworld technology platform and was designed from the outset to store and analyse the highly complex spatial and topological networks typically used by enterprise utilities such as power distribution and telecommunications vmds was originally introduced in 1990 as has been improved and updated over the years its current version is 6 0 vmds has been designed as a spatial database this gives vmds a number of distinctive characteristics when compared to conventional attribute only relational databases distributed server processing vmds is composed of two parts a simple highly scalable data block server called swmfs smallworld master file server and an intelligent client application programming interface api written in c programming language c and magik programming language magik spatial and attribute data are stored in data blocks that reside in special files called data store files on the server when the client application requests data it has sufficient intelligence to work out the optimum set of data blocks that are required this request is then made to swmfs which returns the data to the client via the network for processing this approach is particularly efficient and scalable when dealing with spatial and topological data which tends to flow in larger volumes and require more processing then plain attribute data for example during a map redraw operation this approach makes vmds well suited to enterprise deployment that might involve hundreds or even thousands of concurrent clients support for long transactions relational databases support database transaction short transactions in which changes to data are relatively small and are brief in terms in duration the maximum period between the start and the end of a transaction is typically a few seconds or less vmds supports long transactions in which the volume of data involved in the transaction can be substantial and the duration of the transaction can be significant days weeks or even months these types of transaction are common in advanced network applications used by for example power distribution utilities due to the time span of a long transaction in this context the amount of change can be significant not only within the scope of the transaction but also within the context of the database as a whole accordingly it is likely that the same record might be changed more than once to cope with this scenario vmds has inbuilt support for automatically managing such conflicts and allows applications to review changes and accept only those edits that are correct spatial and topological capabilities as well as conventional relational database features such as attribute querying join fields triggers and calculated fields vmds has numerous spatial and topological capabilities this allows spatial data such as points texts polylines polygons and raster data to be stored and analysed spatial functions include find all features within a polygon calculate the voronoi polygon s of a set of sites and perform a cluster analysis on a set of points vector spatial data such as points polylines and polygons can be given topological attributes that allow complex networks to be modelled network analysis engines are provided to answer questions such as find the shortest path between two nodes or how to optimize a delivery route the travelling salesman problem a topology engine can be configured with a set of rules that define how topological entities interact with each other when new data is added or existing data edited data abstraction in vmds all data is presented to the application as objects this is different from many relational databases that present the data as rows from a table or query result using say jdbc vmds provides a data modelling tool and underlying infrastructure as part of the smallworld technology platform that allows administrators to associate a table in the database with a magik exemplar or class magik get and set methods for the magik exemplar can be automatically generated that expose a table s field or column each vmds row manifests itself to the application as an instance of a magik programming language magik object and is known as an rwo or real world object tables are known as collections in smallworld parlance all rwos hold all the rwos in the database and is heterogeneous all rwos my application rwo set valve collection holds the valve collection valves all rwos select collection valve number of valves valves size queries are built up using predicate objects find open valves open valves valves select predicate eq operating status open number of open valves open valves size for valve over open valves elements loop write valve id endloop joins are implemented as methods on the parent rwo for example a manager might have several employees who report to him get the employee collection employees my application database collection gis employees find a manager called steve and get the first matching element steve employees select predicate eq name steve and predicate eq role manager an element display the names of his direct reports name is a field or column on the employee collection or table for employee over steve direct reports elements loop write employee name endloop performing a transaction each key in the hash table corresponds to the name of the field or column in the collection or table valve data hash table new with asset id 57648576 material iron get the valve collection directly valve collection my application database collection gis valve create an insert transaction to insert a new valve record into the collection a comment can be provide that describes the transaction transaction record transaction new insert valve collection valve data inserted a new valve transaction run see also list of relational database management systems list of object relational database management systems spatial database multiversion concurrency control category data management category gis software'
b'for save points in video games saved game refimprove date september 2014 a savepoint is a way of implementing subtransactions also known as nested transaction s within a relational database management system by indicating a point within a database transaction transaction that can be rollback data management rolled back to without affecting any work done in the transaction before the savepoint was created multiple savepoints can exist within a single transaction savepoints are useful for implementing complex error recovery in database applications if an error occurs in the midst of a multiple statement transaction the application may be able to recover from the error by rolling back to a savepoint without needing to abort the entire transaction a savepoint can be declared by issuing a code savepoint name code statement all changes made after a savepoint has been declared can be undone by issuing a code rollback to savepoint name code command issuing code release savepoint name code will cause the named savepoint to be discarded but will not otherwise affect anything issuing the commands code rollback code or code commit code will also discard any savepoints created since the start of the main transaction http docs oracle com cd b19306 01 appdev 102 b14261 savepoint statement htm savepoints are supported in some form or other in database systems like postgresql oracle database oracle microsoft sql server mysql ibm db2 db2 sqlite since 3 6 8 firebird database server firebird h2 dbms h2 database engine and informix since version 11 50xc3 savepoints are also defined in the sql interoperability and standardization sql standard databases category data management category transaction processing'
b'unreferenced date august 2016 dablink not to be confused with control break displayed in ms dos when cancelling an ongoing task by pressing break key ctrl break key combination in computer programming a control break is a change in the value of one of the key field key s on which a file is sorted which requires some extra processing for example with an input file sorted by post code the number of items found in each postal district might need to be printed on a report and a heading shown for the next district quite often there is a hierarchy of nested control breaks in a program e g streets within districts within areas with the need for a grand total at the end structured programming techniques have been developed to ensure correct processing of control breaks in languages such as cobol and to ensure that conditions such as empty input files and sequence error s are handled properly with fourth generation language s such as sql the programming language should handle most of the details of control breaks automatically category conditional constructs category data management'
b'multiple issues refimprove date august 2010 cleanup date june 2009 a distributed database is a database in which computer data storage storage devices are not all attached to a common processor computing processor ref http www its bldrdoc gov fs 1037 dir 012 1750 htm ref it may be stored in multiple computers located in the same physical location or may be dispersed over a computer network network of interconnected computers unlike parallel computing parallel systems in which the processors are tightly coupled and constitute a single database system a distributed database system consists of loosely coupled sites that share no physical components system administrators can distribute collections of data e g in a database across multiple physical locations a distributed database can reside on organized network servers or blockchain database decentralized independent computers on the internet on corporate intranets or extranets or on other organization computer network networks because they store data across multiple computers distributed databases may improve performance at end user worksites by allowing transactions to be processed on many machines instead of being limited to one ref name obrien o brien j marakas g m 2008 management information systems pp 185 189 new york ny mcgraw hill irwin ref two processes ensure that the distributed databases remain up to date and current replication computing replication and data transmission duplication replication involves using specialized software that looks for changes in the distributive database once the changes have been identified the replication process makes all the databases look the same the replication process can be complex and time consuming depending on the size and number of the distributed databases this process can also require a lot of time and computer resources duplication on the other hand has less complexity it basically identifies one database as a master slave technology master and then duplicates that database the duplication process is normally done at a set time after hours this is to ensure that each distributed location has the same data in the duplication process users may change only the master database this ensures that local data will not be overwritten both replication and duplication can keep the data current in all distributive locations ref name obrien besides distributed database replication and fragmentation there are many other distributed database design technologies for example local autonomy synchronous and asynchronous distributed database technologies these technologies implementations can and do depend on the needs of the business and the sensitivity confidentiality of the data stored in the database and the price the business is willing to spend on ensuring data security data consistency consistency and data integrity integrity when discussing access to distributed databases microsoft favors the term distributed query which it defines in protocol specific manner as a ny select insert update or delete statement that references tables and rowsets from one or more external ole db data sources ref cite web url http technet microsoft com en us library cc966484 aspx title technet glossary publisher microsoft accessdate 2013 07 16 quote distributed query any select insert update or delete statement that references tables and rowsets from one or more external ole db data sources ref oracle database oracle provides a more language centric view in which distributed queries and distributed transaction s form part of distributed sql ref cite web url http docs oracle com cd e11882 01 server 112 e25789 toc htm title oracle database concepts 11g release 2 11 2 last1 ashdown first1 lance last2 kyte first2 tom date september 2011 publisher oracle corporation accessdate 2013 07 17 quote distributed sql synchronously accesses and updates data distributed among multiple databases distributed sql includes distributed queries and distributed transactions ref today the distributed dbms market is evolving dramatically with new innovative entrants and incumbents supporting the growing use of unstructured data and nosql dbms engines as well as xml database s and newsql newsql databases these databases are increasingly supporting distributed database architecture that provides high availability and fault tolerance through replication computing replication and scale out ability some examples are aerospike database aerospike ref cite web title aerospike distributed database url http www aerospike com website aerospike ref apache cassandra cassandra ref cite web url http cassandra apache org title apache cassandra database menagement system publisher apache org ref clusterpoint ref cite web url http www clusterpoint com title clusterpoint xml distributed database publisher clusterpoint ref clustrix clustrixdb ref cite web title frequently asked questions about clustrixdb clustrix documentation url http docs clustrix com display clxdoc frequently asked questions about clustrixdb frequentlyaskedquestionsaboutclustrixdb whatisclustrixdb publisher clustrix inc ref couchbase server couchbase ref cite web title couchbase distributed database url http www couchbase com website couchbase ref druid open source data store ref cite web url http druid io title druid distributed datastore database publisher the druid community ref foundationdb ref cite web url https foundationdb com title foundationdb database publisher foundationdb ref nuodb ref clark jack http www theregister co uk 2014 02 26 nuodb funding nuodb slurps european cash for database expansion the register feb 26 2014 ref riak ref cite web url http www basho com title basho riak distributed database publisher basho ref and orientdb ref cite web url http www orientdb com title orientdb database publisher orientdb ref the blockchain database blockchain technology popularised by bitcoin is an implementation of a distributed database ref cite news last1 margaret first1 alyson title how bitcoin and the blockchain are a transformative technology url http blog blockchain com 2015 06 23 how bitcoin and the block chain are a transformative technology accessdate 23 july 2015 date 23 june 2015 ref architecture a database user accesses the distributed database through local applications applications which do not require data from other sites global applications applications which do require data from other sites a homogeneous distributed database has identical software and hardware running all databases instances and may appear through a single interface as if it were a single database a heterogeneous distributed database may have different hardware operating systems database management systems and even data models for different databases homogeneous distributed databases management system in homogeneous distributed database all sites have identical software and are aware of each other and agree to cooperate in processing user requests each site surrenders part of its autonomy in terms of right to change schema or software a homogeneous dbms appears to the user as a single system the homogeneous system is much easier to design and manage the following conditions must be satisfied for homogeneous database the operating system used at each location must be same or compatible according to whom date march 2013 elucidate date march 2013 the data structures used at each location must be same or compatible the database application or dbms used at each location must be same or compatible heterogeneous ddbms see also heterogeneous database system in a heterogeneous distributed database different sites may use different schema and software difference in schema is a major problem for query processing and transaction processing sites may not be aware of each other and may provide only limited facilities for cooperation in transaction processing in heterogeneous systems different nodes may have different hardware software and data structures at various nodes or locations are also incompatible different computers and operating systems database applications or data models may be used at each of the locations for example one location may have the latest relational database management technology while another location may store data using conventional files or old version of database management system similarly one location may have the windows 10 operating system while another may have unix heterogeneous systems are usually used when individual sites use their own hardware and software on heterogeneous system translations are required to allow communication between different sites or dbms in this system the users must be able to make requests in a database language at their local sites usually the sql database language is used for this purpose if the hardware is different then the translation is straightforward in which computer codes and word length is changed the heterogeneous system is often not technically or economically feasible in this system a user at one location may be able to read but not update the data at another location important considerations care with a distributed database must be taken to ensure the following the distribution is transparent users must be able to interact with the system as if it were one logical system this applies to the system s performance and methods of access among other things database transaction transaction s are transparent each transaction must maintain database integrity across multiple databases transactions must also be divided into sub transactions each sub transaction affecting one database system there are two principal approaches to store a relation r in a distributed database system a database replication replication b fragmentation partition database partitioning a replication in replication the system maintains several identical replicas of the same relation r in different sites data is more available in this scheme parallelism is increased when read request is served increases overhead on update operations as each site containing the replica needed to be updated in order to maintain consistency multi datacenter replication provides geographical diversity like in clusterpoint ref cite web url http www clusterpoint com solutions distributed storage title clusterpoint database distributed storage multi datacenter replication publisher clusterpoint ref or riak ref cite web url http basho com tag multi datacenter replication title riak database multi datacenter replication publisher basho ref b fragmentation the relation r is fragmented into several relations r sub 1 sub r sub 2 sub r sub 3 sub r sub n sub in such a way that the actual relation could be reconstructed from the fragments and then the fragments are scattered to different locations there are basically two schemes of fragmentation horizontal fragmentation splits the relation by assigning each tuple of r to one or more fragments vertical fragmentation splits the relation by decomposing the schema r of relation r a distributed database can be run by independent or even competing parties as for example in bitcoin or hasq advantages management of distributed data with different levels of transparency like network transparency fragmentation transparency replication transparency etc increase reliability and availability easier expansion reflects organizational structure database fragments potentially stored within the departments they relate to local autonomy or site autonomy a department can control the data about them as they are the ones familiar with it protection of valuable data if there were ever a catastrophic event such as a fire all of the data would not be in one place but distributed in multiple locations improved performance data is located near the site of greatest demand and the database systems themselves are parallelized allowing load on the databases to be balanced among servers a high load on one module of the database won t affect other modules of the database in a distributed database economics it may cost less to create a network of smaller computers with the power of a single large computer modularity systems can be modified added and removed from the distributed database without affecting other modules systems reliable transactions due to replication of the database hardware operating system network fragmentation dbms replication and location independence continuous operation even if some nodes go offline depending on design distributed query processing can improve performance single site failure does not affect performance of system for those systems that support full distributed transactions operations enjoy the acid properties a atomicity the transaction takes place as a whole or not at all c consistency maps one consistent db state to another i isolation each transaction sees a consistent db d durability the results of a transaction must survive system failures the merge replication method is popularly used to consolidate the data between databases citation needed date july 2013 disadvantages complexity database administrator dbas may have to do extra work to ensure that the distributed nature of the system is transparent extra work must also be done to maintain multiple disparate system s instead of one big one extra database design work must also be done to account for the disconnected nature of the database for example joins become prohibitively expensive when performed across multiple systems economics increased complexity and a more extensive infrastructure means extra labour costs security remote database fragments must be secured and they are not centralized so the remote sites must be secured as well the infrastructure must also be secured for example by encrypting the network links between remote sites difficult to maintain integrity but in a distributed database enforcing integrity over a network may require too much of the network s resources to be feasible inexperience distributed databases are difficult to work with and in such a young field there is not much readily available experience in proper practice lack of standards there are no tools or methodologies yet to help users convert a centralized dbms into a distributed dbms citation needed date july 2013 database design more complex in addition to traditional database design challenges the design of a distributed database has to consider fragmentation of data allocation of fragments to specific sites and data replication additional software is required operating system should support distributed environment concurrency control poses a major issue it can be solved by lock database locking and timestamp ing distributed access to data analysis of distributed data see also centralized database data grid distributed data store distributed cache routing protocol distributed hash table references reflist 30em more footnotes date april 2013 m t \xc3\xb6zsu and p valduriez principles of distributed databases 3rd edition 2011 springer isbn 978 1 4419 8833 1 elmasri and navathe fundamentals of database systems 3rd edition addison wesley longman isbn 0 201 54263 3 oracle database administrator s guide 10g release 1 http docs oracle com cd b14117 01 server 101 b10739 ds concepts htm databases category data management category types of databases category distributed computing architecture category applications of distributed computing category database management systems'
b'about serializability of database transactions serialization of objects in object oriented languages serialization in concurrency control of database s ref name bernstein87 phil bernstein philip a bernstein vassos hadzilacos nathan goodman 1987 http research microsoft com en us people philbe ccontrol aspx concurrency control and recovery in database systems free pdf download addison wesley publishing company isbn 0 201 10715 5 ref ref name weikum01 gerhard weikum gottfried vossen 2001 http www elsevier com wps find bookdescription cws home 677937 description description transactional information systems elsevier isbn 1 55860 508 8 ref transaction processing transaction management and various database transaction transactional applications e g transactional memory ref name herlihy1993 maurice herlihy and j eliot b moss transactional memory architectural support for lock free data structures proceedings of the 20th annual international symposium on computer architecture isca 93 volume 21 issue 2 may 1993 ref and software transactional memory both centralized and distributed computing distributed a transaction schedule computer science schedule is serializable if its outcome e g the resulting database state is equal to the outcome of its transactions executed serially i e sequentially without overlapping in time transactions are normally executed concurrently they overlap since this is the most efficient way serializability is the major correctness criterion for concurrent transactions executions it is considered the highest level of isolation computer science isolation between database transaction transactions and plays an essential role in concurrency control as such it is supported in all general purpose database systems two phase locking strong strict two phase locking ss2pl is a popular serializability mechanism utilized in most of the database systems in various variants since their early days in the 1970s serializability theory provides the formal framework to reason about and analyze serializability and its techniques though it is mathematics mathematical in nature its fundamentals are informally without mathematics notation introduced below database transaction main database transaction a database transaction is a specific intended run with specific parameters e g with transaction identification at least of a computer program or programs that accesses a database or databases such a program is written with the assumption that it is running in isolation from other executing programs i e when running its accessed data after the access are not changed by other running programs without this assumption the transaction s results are unpredictable and can be wrong the same transaction can be executed in different situations e g in different times and locations in parallel with different programs a live transaction i e exists in a computing environment with already allocated computing resources to distinguish from a transaction request waiting to get execution resources can be in one of three states or phases running its program s is are executing ready its program s execution has ended and it is waiting to be ended completed ended or completed it is either committed or aborted rolled back depending whether the execution is considered a success or not respectively when committed all its recoverable i e with states that can be controlled for this purpose durable resources typically database data are put in their final states states after running when aborted all its recoverable resources are put back in their initial states as before running a failure in transaction s computing environment before ending typically results in its abort however a transaction may be aborted also for other reasons as well e g see below upon being ended completed transaction s allocated computing resources are released and the transaction disappears from the computing environment however the effects of a committed transaction remain in the database while the effects of an aborted rolled back transaction disappear from the database the concept of atomic transaction all or nothing semantics was designed to exactly achieve this behavior in order to control correctness in complex faulty systems correctness serializability serializability is used to keep the data in the data item in a consistent state serializability is a property of a transaction schedule computer science schedule history it relates to the isolation database systems isolation property of a database transaction serializability of a schedule means equivalence in the outcome the database state data values to a serial schedule i e sequential with no transaction overlap in time with the same transactions it is the major criterion for the correctness of concurrent transactions schedule and thus supported in all general purpose database systems the rationale behind serializability is the following if each transaction is correct by itself i e meets certain integrity conditions then a schedule that comprises any serial execution of these transactions is correct its transactions still meet their conditions serial means that transactions do not overlap in time and cannot interfere with each other i e complete isolation between each other exists any order of the transactions is legitimate if no dependencies among them exist which is assumed see comment below as a result a schedule that comprises any execution not necessarily serial that is equivalent in its outcome to any serial execution of these transactions is correct schedules that are not serializable are likely to generate erroneous outcomes well known examples are with transactions that debit and credit accounts with money if the related schedules are not serializable then the total sum of money may not be preserved money could disappear or be generated from nowhere this and violations of possibly needed other invariant computer science invariant preservations are caused by one transaction writing and stepping on and erasing what has been written by another transaction before it has become permanent in the database it does not happen if serializability is maintained if any specific order between some transactions is requested by an application then it is enforced independently of the underlying serializability mechanisms these mechanisms are typically indifferent to any specific order and generate some unpredictable partial order that is typically compatible with multiple serial orders of these transactions this partial order results from the scheduling orders of concurrent transactions data access operations which depend on many factors a major characteristic of a database transaction is atomicity database systems atomicity which means that it either commits i e all its operations results take effect in the database or aborts rolled back all its operations results do not have any effect on the database all or nothing semantics of a transaction in all real systems transactions can abort for many reasons and serializability by itself is not sufficient for correctness schedules also need to possess the schedule computer science recoverable recoverability from abort property recoverability means that committed transactions have not read data written by aborted transactions whose effects do not exist in the resulting database states while serializability is currently compromised on purpose in many applications for better performance only in cases when application s correctness is not harmed compromising recoverability would quickly violate the database s integrity as well as that of transactions results external to the database a schedule with the recoverability property a recoverable schedule recovers from aborts by itself i e aborts do not harm the integrity of its committed transactions and resulting database this is false without recoverability where the likely integrity violations resulting incorrect database data need special typically manual corrective actions in the database implementing recoverability in its general form may result in cascading aborts aborting one transaction may result in a need to abort a second transaction and then a third and so on this results in a waste of already partially executed transactions and may result also in a performance penalty schedule computer science avoids cascading aborts rollbacks avoiding cascading aborts aca or cascadelessness is a special case of recoverability that exactly prevents such phenomenon often in practice a special case of aca is utilized schedule computer science strict strictness strictness allows an efficient database recovery from failure note that the recoverability property is needed even if no database failure occurs and no database recovery from failure is needed it is rather needed to correctly automatically handle aborts which may be unrelated to database failure and recovery from failure relaxing serializability in many applications unlike with finances absolute correctness is not needed for example when retrieving a list of products according to specification in most cases it does not matter much if a product whose data was updated a short time ago does not appear in the list even if it meets the specification it will typically appear in such a list when tried again a short time later commercial databases provide concurrency control with a whole range of isolation computer science isolation levels isolation levels which are in fact controlled serializability violations in order to achieve higher performance higher performance means better transaction execution rate and shorter average transaction response time transaction duration snapshot isolation is an example of a popular widely utilized efficient relaxed serializability method with many characteristics of full serializability but still short of some and unfit in many situations another common reason nowadays for serializability distributed serializability distributed serializability relaxation see below is the requirement of availability of internet products and internet service provider services this requirement is typically answered by large scale data replication computer science replication the straightforward solution for synchronizing replicas updates of a same database object is including all these updates in a single atomic distributed transaction however with many replicas such a transaction is very large and may span several computer s and computer network networks that some of them are likely to be unavailable thus such a transaction is likely to end with abort and miss its purpose ref name gray1996 cite conference author jim gray computer scientist gray j coauthors helland p patrick o neil o neil p dennis shasha shasha d year 1996 title the dangers of replication and a solution conference proceedings of the 1996 acm sigmod international conference on management of data pages 173 182 url ftp ftp research microsoft com pub tr tr 96 17 pdf doi 10 1145 233269 233330 ref consequently optimistic replication lazy replication is often utilized e g in many products and services by google amazon com amazon yahoo and alike while serializability is relaxed and compromised for eventual consistency again in this case relaxation is done only for applications that are not expected to be harmed by this technique classes of schedules defined by relaxed serializability properties either contain the serializability class or are incomparable with it view and conflict serializability mechanisms that enforce serializability need to execute in real time computing real time or almost in real time while transactions are running at high rates in order to meet this requirement special cases of serializability sufficient conditions for serializability which can be enforced effectively are utilized two major types of serializability exist view serializability and conflict serializability view serializability matches the general definition of serializability given above conflict serializability is a broad special case i e any schedule that is conflict serializable is also view serializable but not necessarily the opposite conflict serializability is widely utilized because it is easier to determine and covers a substantial portion of the view serializable schedules determining view serializability of a schedule is an np complete problem a class of problems with only difficult to compute excessively time consuming known solutions view serializability of a schedule is defined by equivalence to a serial schedule no overlapping transactions with the same transactions such that respective transactions in the two schedules read and write the same data values view the same data values conflict serializability is defined by equivalence to a serial schedule no overlapping transactions with the same transactions such that both schedules have the same sets of respective chronologically ordered pairs of conflicting operations same precedence relations of respective conflicting operations operations upon data are read or write a write either insert or modify or delete two operations are conflicting if they are of different transactions upon the same datum data item and at least one of them is write each such pair of conflicting operations has a conflict type it is either a read write or write read or a write write conflict the transaction of the second operation in the pair is said to be in conflict with the transaction of the first operation a more general definition of conflicting operations also for complex operations which may consist each of several simple read write operations requires that they are noncommutative changing their order also changes their combined result each such operation needs to be atomic by itself by proper system support in order to be considered an operation for a commutativity check for example read read operations are commutative unlike read write and the other possibilities and thus read read is not a conflict another more complex example the operations increment and decrement of a counter are both write operations both modify the counter but do not need to be considered conflicting write write conflict type since they are commutative thus increment decrement is not a conflict e g already has been supported in the old ibm information management system ibm s ims fast path only precedence time order in pairs of conflicting non commutative operations is important when checking equivalence to a serial schedule since different schedules consisting of the same transactions can be transformed from one to another by changing orders between different transactions operations different transactions interleaving and since changing orders of commutative operations non conflicting does not change an overall operation sequence result i e a schedule outcome the outcome is preserved through order change between non conflicting operations but typically not when conflicting operations change order this means that if a schedule can be transformed to any serial schedule without changing orders of conflicting operations but changing orders of non conflicting while preserving operation order inside each transaction then the outcome of both schedules is the same and the schedule is conflict serializable by definition conflicts are the reason for blocking transactions and delays non materialized conflicts or for aborting transactions due to serializability violations prevention both possibilities reduce performance thus reducing the number of conflicts e g by commutativity when possible is a way to increase performance a transaction can issue request a conflicting operation and be in conflict with another transaction while its conflicting operation is delayed and not executed e g blocked by a lock computer science lock only executed materialized conflicting operations are relevant to conflict serializability see more below enforcing conflict serializability testing conflict serializability schedule compliance with conflict serializability can be tested with the precedence graph serializability graph serialization graph conflict graph for committed transactions of the schedule it is the directed graph representing precedence of transactions in the schedule as reflected by precedence of conflicting operations in the transactions in the precedence graph transactions are nodes and precedence relations are directed edges there exists an edge from a first transaction to a second transaction if the second transaction is in conflict with the first see conflict serializability above and the conflict is materialized i e if the requested conflicting operation is actually executed in many cases a requested issued conflicting operation by a transaction is delayed and even never executed typically by a lock computer science lock on the operation s object held by another transaction or when writing to a transaction s temporary private workspace and materializing copying to the database itself upon commit as long as a requested issued conflicting operation is not executed upon the database itself the conflict is non materialized non materialized conflicts are not represented by an edge in the precedence graph comment in many text books only committed transactions are included in the precedence graph here all transactions are included for convenience in later discussions the following observation is a key characterization of conflict serializability a schedule is conflict serializable if and only if its precedence graph of committed transactions when only committed transactions are considered is directed acyclic graph acyclic this means that a cycle consisting of committed transactions only is generated in the general precedence graph if and only if conflict serializability is violated cycles of committed transactions can be prevented by aborting an undecided neither committed nor aborted transaction on each cycle in the precedence graph of all the transactions which can otherwise turn into a cycle of committed transactions and a committed transaction cannot be aborted one transaction aborted per cycle is both required and sufficient number to break and eliminate the cycle more aborts are possible and can happen in some mechanisms but unnecessary for serializability the probability of cycle generation is typically low but nevertheless such a situation is carefully handled typically with a considerable overhead since correctness is involved transactions aborted due to serializability violation prevention are restarted and executed again immediately serializability enforcing mechanisms typically do not maintain a precedence graph as a data structure but rather prevent or break cycles implicitly e g ss2pl below common mechanism ss2pl main two phase locking strong strict two phase locking ss2pl is a common mechanism utilized in database systems since their early days in the 1970s the ss in the name ss2pl is newer though to enforce both conflict serializability and schedule computer science strict strictness a special case of recoverability which allows effective database recovery from failure of a schedule in this mechanism each datum is locked by a transaction before accessing it any read or write operation the item is marked by associated with a lock computer science lock of a certain type depending on operation and the specific implementation various models with different lock types exist in some models locks may change type during the transaction s life as a result access by another transaction may be blocked typically upon a conflict the lock delays or completely prevents the conflict from being materialized and be reflected in the precedence graph by blocking the conflicting operation depending on lock type and the other transaction s access operation type employing an ss2pl mechanism means that all locks on data on behalf of a transaction are released only after the transaction has ended either committed or aborted ss2pl is the name of the resulting schedule property as well which is also called rigorousness ss2pl is a special case proper subset of two phase locking 2pl mutual blocking between transactions results in a deadlock where execution of these transactions is stalled and no completion can be reached thus deadlocks need to be resolved to complete these transactions execution and release related computing resources a deadlock is a reflection of a potential cycle in the precedence graph that would occur without the blocking when conflicts are materialized a deadlock is resolved by aborting a transaction involved with such potential cycle and breaking the cycle it is often detected using a wait for graph a graph of conflicts blocked by locks from being materialized it can be also defined as the graph of non materialized conflicts conflicts not materialized are not reflected in the precedence graph and do not affect serializability which indicates which transaction is waiting for lock release by which transaction and a cycle means a deadlock aborting one transaction per cycle is sufficient to break the cycle transactions aborted due to deadlock resolution are restarted and executed again immediately other enforcing techniques other known mechanisms include precedence graph or serializability graph conflict graph cycle elimination two phase locking 2pl timestamp based concurrency control timestamp ordering to snapshot isolation making snapshot isolation serializable serializable snapshot isolation ref name cahill08 michael j cahill uwe r\xc3\xb6hm alan d fekete 2008 http portal acm org citation cfm id 1376690 serializable isolation for snapshot databases proceedings of the 2008 acm sigmod international conference on management of data pp 729 738 vancouver canada june 2008 isbn 978 1 60558 102 6 sigmod 2008 best paper award ref serializablesi the above conflict serializability techniques in their general form do not provide recoverability special enhancements are needed for adding recoverability optimistic versus pessimistic techniques concurrency control techniques are of three major types pessimistic in pessimistic concurrency control a transaction blocks data access operations of other transactions upon conflicts and conflicts are non materialized until blocking is removed this is done to ensure that operations that may violate serializability and in practice also recoverability do not occur optimistic in optimistic concurrency control data access operations of other transactions are not blocked upon conflicts and conflicts are immediately materialized when the transaction reaches the ready state i e its running state has been completed possible serializability and in practice also recoverability violation by the transaction s operations relatively to other running transactions is checked if violation has occurred the transaction is typically aborted sometimes aborting another transaction to handle serializability violation is preferred otherwise it is committed semi optimistic mechanisms that mix blocking in certain situations with not blocking in other situations and employ both materialized and non materialized conflicts the main differences between the technique types is the conflict types that are generated by them a pessimistic method blocks a transaction operation upon conflict and generates a non materialized conflict while an optimistic method does not block and generates a materialized conflict a semi optimistic method generates both conflict types both conflict types are generated by the chronological orders in which transaction operations are invoked independently of the type of conflict a cycle of committed transactions with materialized conflicts in the precedence graph conflict graph represents a serializability violation and should be avoided for maintaining serializability a cycle of non materialized conflicts in the wait for graph represents a deadlock situation which should be resolved by breaking the cycle both cycle types result from conflicts and should be broken at any technique type conflicts should be detected and considered with similar overhead for both materialized and non materialized conflicts typically by using mechanisms like locking while either blocking for locks or not blocking but recording conflict for materialized conflicts in a blocking method typically a context switch ing occurs upon conflict with additional incurred overhead otherwise blocked transactions related computing resources remain idle unutilized which may be a worse alternative when conflicts do not occur frequently optimistic methods typically have an advantage with different transactions loads mixes of transaction types one technique type i e either optimistic or pessimistic may provide better performance than the other unless schedule classes are inherently blocking i e they cannot be implemented without data access operations blocking e g 2pl ss2pl and sco above see chart they can be implemented also using optimistic techniques e g serializability recoverability serializable multi version concurrency control see also multiversion concurrency control partial coverage and snapshot isolation serializable snapshot isolation serializable snapshot isolation in snapshot isolation multi version concurrency control mvcc is a common way today to increase concurrency and performance by generating a new version of a database object each time the object is written and allowing transactions read operations of several last relevant versions of each object depending on scheduling method mvcc can be combined with all the serializability techniques listed above except serializablesi which is originally mvcc based it is utilized in most general purpose dbms products mvcc is especially popular nowadays through the relaxed serializability see above method snapshot isolation si which provides better performance than most known serializability mechanisms at the cost of possible serializability violation in certain cases snapshot isolation making snapshot isolation serializable serializablesi which is an efficient enhancement of si to make it serializable is intended to provide an efficient serializable solution snapshot isolation making snapshot isolation serializable serializablesi has been analyzed ref name cahill08 ref name fekete2009 alan fekete 2009 http www it usyd edu au fekete teaching serializablesi fekete pdf snapshot isolation and serializable execution presentation page 4 2009 the university of sydney australia retrieved 16 september 2009 ref via a general theory of mvcc distributed serializability overview distributed serializability is the serializability of a schedule of a transactional distributed system e g a distributed database system such system is characterized by distributed transaction s also called global transactions i e transactions that span computer processes a process abstraction in a general sense depending on computing environment e g operating system s thread computer science thread and possibly network nodes a distributed transaction comprises more than one local sub transactions that each has states as described above for a serializability database transaction database transaction a local sub transaction comprises a single process or more processes that typically fail together e g in a single processor core distributed transactions imply a need in atomic commit protocol to reach consensus among its local sub transactions on whether to commit or abort such protocols can vary from a simple one phase hand shake among processes that fail together to more sophisticated protocols like two phase commit to handle more complicated cases of failure e g process node communication etc failure distributed serializability is a major goal of distributed concurrency control for correctness with the proliferation of the internet cloud computing grid computing and small portable powerful computing devices e g smartphone s the need for effective distributed serializability techniques to ensure correctness in and among distributed applications seems to increase distributed serializability is achieved by implementing distributed versions of the known centralized techniques ref name bernstein87 ref name weikum01 typically all such distributed versions require utilizing conflict information either of materialized or non materialized conflicts or equivalently transaction precedence or blocking information conflict serializability is usually utilized that is not generated locally but rather in different processes and remote locations thus information distribution is needed e g precedence relations lock information timestamps or tickets when the distributed system is of a relatively small scale and message delays across the system are small the centralized concurrency control methods can be used unchanged while certain processes or nodes in the system manage the related algorithms however in a large scale system e g grid and cloud due to the distribution of such information substantial performance penalty is typically incurred even when distributed versions of the methods vs centralized are used primarily due to computer and communication latency engineering latency also when such information is distributed related techniques typically do not scale well a well known example with scalability problems is a distributed lock manager which distributes lock non materialized conflict information across the distributed system to implement locking techniques see also two phase locking strong strict two phase locking ss2pl or rigorousness snapshot isolation making snapshot isolation serializable making snapshot isolation serializable ref name cahill08 in snapshot isolation global serializability where the global serializability problem and its proposed solutions are described linearizability a more general concept in concurrent computing notes reflist references more footnotes date november 2011 supposed sources for most of the material about centralized vs distributed serializability phil bernstein philip a bernstein vassos hadzilacos nathan goodman 1987 http research microsoft com en us people philbe ccontrol aspx concurrency control and recovery in database systems addison wesley publishing company isbn 0 201 10715 5 gerhard weikum gottfried vossen 2001 http www elsevier com wps find bookdescription cws home 677937 description description transactional information systems elsevier isbn 1 55860 508 8 category data management category databases category concurrency control category transaction processing category distributed computing problems el \xcf\x83\xce\xb5\xce\xb9\xcf\x81\xce\xb9\xce\xbf\xcf\x80\xce\xbf\xce\xb9\xce\xb7\xcf\x83\xce\xb9\xce\xbc\xcf\x8c\xcf\x84\xce\xb7\xcf\x84\xce\xb1 \xcf\x83\xcf\x85\xce\xb3\xce\xba\xcf\x81\xce\xbf\xcf\x8d\xcf\x83\xce\xb5\xcf\x89\xce\xbd'
b'refimprove date november 2008 in information technology data architecture is composed of models policies rules or standards that govern which data is collected and how it is stored arranged integrated and put to use in data systems and in organizations ref http www businessdictionary com definition data architecture html business dictionary data architecture ref data is usually one of several architecture domain s that form the pillars of an enterprise architecture or solution architecture ref http www learn geekinterview com data warehouse data architecture what is data architecture html what is data architecture geekinterview 2008 01 28 accessed 2011 04 28 ref overview a data architecture should pov statement date march 2013 set data standards for all its data systems as a vision or a model of the eventual interactions between those data systems data integration for example should be dependent upon data architecture standards since data integration requires data interactions between two or more data systems a data architecture in part describes the data structure s used by a business and its computer applications software data architectures address data in storage and data in motion descriptions of data stores data groups and data items and data mapping mappings of those data artifacts to data qualities applications locations etc essential to realizing the target state data architecture describes how data is processed stored and utilized in an information system it provides criteria for data processing operations so as to make it possible to design data flow s and also control the flow of data in the system the data architect is typically responsible for defining the target state aligning during development and then following up to ensure enhancements are done in the spirit of the original blueprint during the definition of the target state the data architecture breaks a subject down to the atomic level and then builds it back up to the desired form the data architect breaks the subject down by going through 3 traditional architectural processes conceptual represents all business entities logical represents the logic of how entities are related physical the realization of the data mechanisms for a specific type of functionality the data column of the zachman framework for enterprise architecture ndash border 1 layer view data what stakeholder 1 scope contextual list of things and architectural standards ref http www strins com data architecture standards html data architecture standards ref important to the business planner 2 business model conceptual semantic model or entity relationship model conceptual http tdan com the enterprise data model 5205 enterprise data model owner 3 system model logical enterprise logical data model logical data model designer 4 technology model physical physical data model physical data model builder 5 detailed representations actual database s subcontractor in this second broader sense data architecture includes a complete analysis of the relationships among an organization s functions available technologies and data type s data architecture should be defined in the planning phase of the design of a new data processing and storage system the major types and sources of data necessary to support an enterprise should be identified in a manner that is complete consistent and understandable the primary requirement at this stage is to define all of the relevant data entities not to specify computer hardware items a data entity is any real or abstracted thing about which an organization or individual wishes to store data physical data architecture physical data architecture of an information system is part of a technology roadmapping technology plan as its name implies the technology plan is focused on the actual tangible element mathematics elements to be used in the implementation of the data architecture design physical data architecture encompasses database architecture database architecture is a model abstract schema of the actual database technology that will support the designed data architecture elements of data architecture certain elements must be defined during the design phase of the data architecture schema for example administrative structure that will be established in order to manage the data resources must be described also the methodologies that will be employed to store the data must be defined in addition a description of the database technology to be employed must be generated as well as a description of the processes that will manipulate the data it is also important to design interface computing interfaces to the data by other systems as well as a design for the infrastructure that will support common data operations i e emergency procedures data import s data backup s external data transfer transfers of data without the guidance of a properly implemented data architecture design common data operations might be implemented in different ways rendering it difficult to understand and control the flow of data within such systems this sort of fragmentation is highly undesirable due to the potential increased cost and the data disconnects involved these sorts of difficulties may be encountered with rapidly growing enterprises and also enterprises that service different lines of business e g insurance product business products properly executed the data architecture phase of information system planning forces an organization to precisely specify and describe both internal and external information flows these are patterns that the organization may not have previously taken the time to conceptualize it is therefore possible at this stage to identify costly information shortfalls disconnects between departments and disconnects between organizational systems that may not have been evident before the data architecture analysis ref cite book last mittal first prashant title author year 2009 publisher global india publications location pg 256 isbn 978 93 8022 820 4 pages 314 url https books google com books id bpkhydj4tm0c dq inauthor 22prashant mittal 22 source gbs navlinks s ref constraints and influences various constraints and influences will have an effect on data architecture design these include enterprise requirements technology drivers economics business policies and data processing needs enterprise requirements these will generally include such elements as economical and effective system expansion acceptable performance levels especially system access speed financial transaction transaction reliability and transparent data management in addition the data conversion conversion of raw data such as transaction record computer science records and image computer file files into more useful information forms through such features as data warehouse s is also a common organizational requirement since this enables managerial decision making and other organizational processes one of the architecture techniques is the split between managing transaction data and master reference data another one is splitting automatic identification and data capture data capture systems from data retrieval systems as done in a data warehouse technology drivers these are usually suggested by the completed data architecture and database architecture designs in addition some technology drivers will derive from existing organizational integration frameworks and standards organizational economics and existing site resources e g previously purchased software licensing economics these are also important factors that must be considered during the data architecture phase it is possible that some solutions while optimal in principle may not be potential candidates due to their cost external factors such as the business cycle interest rates market conditions and legal considerations could all have an effect on decisions relevant to data architecture business policies business policies that also drive data architecture design include internal organizational policies rules of regulatory agency regulatory bodies professional standards and applicable governmental laws that can vary by applicable government agency agency these policies and rules will help describe the manner in which enterprise wishes to process their data data processing needs these include accurate and reproducible data transaction transactions performed in high volumes data warehousing for the support of management information systems and potential data mining repetitive periodic data reporting reporting ad hoc reporting and support of various organizational initiatives as required i e annual budgets new product business product development see also enterprise information security architecture eisa positions data security in the enterprise information framework fdic enterprise architecture framework controlled vocabulary information silo disparate system data warehouse references reflist further reading bass l john b kates j 2001 achieving usability through software architecture carnegie mellon university lewis g comella dorda s place p plakosh d seacord r 2001 enterprise information system data architecture guide carnegie mellon university adleman s moss l abai m 2005 data strategy addison wesley professional external links commons category data architecture http www sei cmu edu library abstracts reports 01tr005 cfm achieving usability through software architecture sei cmu edu 2001 http sunsite uakom sk sunworldonline swol 07 1998 swol 07 itarchitect html the logical data architecture by nirmal baid data model category computer data category data management category enterprise architecture'
b'multiple issues refimprove date february 2015 pov date february 2011 enterprise information integration eii is the ability to support a unified view of data and information for an entire organization in a data virtualization application of eii a process of information integration using data abstraction to provide a unified interface known as uniform data access for viewing all the data within an organization and a single set of structures and naming conventions known as uniform information representation to represent this data the goal of eii is to get a large set of heterogeneous data sources to appear to a user or system as a single homogeneous data source overview data within an enterprise architecture enterprise can be stored in heterogeneous formats including relational database s which themselves come in a large number of varieties text files xml files spreadsheet s and a variety of proprietary data storage device storage methods each with their own index database index ing and data access methods standardized data access application programming interface api s have emerged that offer a specific set of commands to retrieve and modify data from a generic data source many applications exist that implement these apis commands across various data sources most notably relational databases such apis include odbc jdbc xqj ole db and more recently ado net there are also standard formats for representing data within a file that are very important to information integration the best known of these is xml which has emerged as a standard universal representation format there are also more specific xml grammars defined for specific types of data such as geography markup language for expressing geographical features and directory service markup language for holding directory style information in addition non xml standard formats exist such as icalendar for representing calendar information and vcard for business card information enterprise information integration eii applies data integration commercially despite the theoretical problems described above the private sector shows more concern with the problems of data integration as a viable product ref name refthree cite conference author alon y halevy title enterprise information integration successes challenges and controversies booktitle sigmod 2005 year 2005 pages 778 787 url http www cs washington edu homes alon files eiisigmod05 pdf display authors etal ref eii emphasizes neither on correctness nor tractability but speed and simplicity an eii industry has emerged but many professionals who date june 2009 believe it does not perform to its full potential practitioners cite the following major issues which eii must address for the industry to become mature citation needed date june 2009 combining disparate data sets each data source is disparate and as such is not designed to support eii therefore data virtualization as well as federated database system data federation depends upon accidental data commonality to support combining data and information from disparate data sets because of this lack of data value commonality across data sources the return set may be inaccurate incomplete and impossible to validate one solution is to recast disparate databases to integrate these databases without the need for extract transform load etl the recast databases support commonality constraints where referential integrity may be enforced between databases the recast databases provide designed data access paths with data value commonality across databases simplicity of understanding answering queries with views arouses interest from a theoretical standpoint but difficulties in understanding how to incorporate it as an enterprise solution citation needed date june 2009 some developers who date june 2009 believe it should be merged with enterprise application integration eai others who date june 2009 believe it should be incorporated with etl systems citing customers confusion over the differences between the two services citation needed date june 2009 simplicity of deployment even if recognized as a solution to a problem eii as of 2009 lc on currently takes time to apply and offers complexities in deployment people have proposed a variety of schema less solutions such as lean middleware ref name reffour cite conference author david a maluf title lean middleware booktitle sigmod 2005 year 2005 pages 788 791 url http portal acm org citation cfm id 1066157 1066247 coll portal dl acm type series idx 1066157 part proceedings wanttype proceedings title international 20conference 20on 20management 20of 20data cfid 15151515 cftoken 6184618 display authors etal ref but ease of use and speed of employment appear inversely proportional to the generality of such systems citation needed date june 2009 others who date june 2009 cite the need for standard data interfaces to speed and simplify the integration process in practice handling higher order information analysts experience difficulty even with a functioning information integration system in determining whether the sources in the database will satisfy a given application answering these kinds of questions about a set of repositories requires semantic information like metadata and or ontologies the few commercial tools which date june 2009 that leverage this information remain in their infancy applications eii products enable loose coupling between wiktionary homogeneous homogeneous data consuming client applications and services and heterogeneous data stores such client applications and services include desktop productivity tools spreadsheets word processor s presentation software etc integrated development environment development environment s and software framework framework s java ee microsoft net net mono software mono soap or representational state transfer rest ful web service s etc business intelligence bi business activity monitoring bam software enterprise resource planning erp customer relationship management crm business process management bpm and or bpel software and web content management cms example technology vendors capsenta composite software denodo metamatrix xaware data access technologies xquery and xquery api for java service data objects sdo for java c and net clients and any type of data source see also div col 3 business intelligence 2 0 bi 2 0 data warehouse disparate system enterprise integration federated database system resource description framework semantic heterogeneity semantic integration semantic web web 2 0 web services div col end references references category data management'
b'refimprove date december 2010 no footnotes date december 2010 the world wide molecular matrix wwmm is an electronic disciplinary repository repository for unpublished chemical data first proposed in 2002 by peter murray rust and his colleagues in the chemistry department at the university of cambridge in the united kingdom wwmm provides a free easily searchable database for information about thousands of complicated molecules data that would otherwise remain inaccessible to scientists murray rust a chemical cheminformatics informatics specialist has estimated that 80 of the results produced by chemists around the world is never published in scientific journals most of this data is not ground breaking yet it could conceivably be of use to scientists doing related projects if they could access it the wwmm was proposed as a solution to this problem it would house the results of experiments on over 100 000 molecules in physical chemistry organic chemistry biochemistry and medicinal chemistry in other scientific fields the need for a similar depository to house inaccessible information could be more acute in a presentation at the cern workshop on innovations in scholarly communications open archives initiative oai4 murray rust said that chemistry actually leads other fields in published data he estimated that as much as 99 of the data in some scientific fields never reaches publication citation needed date december 2010 reason this is not found in external link oai4 paine ellsworth ed although scientific in nature the wwmm is part of the broader open archives initiative open archives and open source movements pushes to make more and more information freely available to any user via the internet or www world wide web in his cern presentation murray rust stated that the wwmm was a response to the expense of scientific journals and he asked the rhetorical question can we win the war to make data open or will it be absorbed into the publishing and pseudo publishing world murray rust and his colleagues are also responsible for the development of the chemical mark up language chemical markup language cml a variant of xml intended for chemists see also open archives initiative the open archives initiative oai informatics academic field the science of informatics chemical markup language chemical mark up language cml external links http www ch cam ac uk person pm286 the home page of dr peter murray rust at the university of cambridge http www escience cam ac uk projects mi mi call html the cambridge center for molecular informatics http www nesc ac uk events ahm2003 ahmcd pdf 157 pdf an outline of the wwmm http oai4 web cern ch oai4 cern workshop on innovations in scholarly communication oai4 verify source type application to wwmm date december 2010 category data management'
b'for the computer based analysis of archaeological data computational archaeology data archaeology refers to the art and science of recovering computer data encoded and or encrypted in now obsolete computer media media or content format formats data archaeology can also refer to recovering information from damaged electronic formats after natural or man made disasters the term originally appeared in 1993 as part of the global oceanographic data archaeology and rescue project godar the original impetus for data archaeology came from the need to recover computerized records of climatic conditions stored on old computer tape which can provide valuable evidence for testing theories of climate change these approaches allowed the reconstruction of an image of the arctic that had been captured by the nimbus program nimbus 2 satellite on september 23 1966 in higher resolution than ever seen before from this type of data ref http nsidc org monthlyhighlights january2010 html techno archaeology rescues climate data from early satellites u s national snow and ice data center nsidc january 2010 http www webcitation org 5xn1snydp archived ref nasa also utilizes the services of data archaeologists to recover information stored on 1960s era vintage computer tape as exemplified by the lunar orbiter image recovery project loirp ref http www nasa gov topics moonmars features loirp loirp overview nasa website november 14 2008 http www webcitation org 5xn1djlg4 archived ref recovery it is also important to make the distinction in data archaeology between data recovery and data intelligibility you may be able to recover the data but not understand it for data archaeology to be effective the data must be intelligible ref name www ukoln ac uk http www ukoln ac uk services elib papers supporting pdf p2 pdf study on website october 23 2011 ref disaster recovery data archaeologists can also use data recovery after natural disasters such as fires floods earthquakes or even hurricanes for example in 1995 during hurricane marilyn the national media lab assisted the national archives and records administration in recovering data at risk due to damaged equipment the hardware was damaged from rain salt water and sand yet it was possible to clean some of the disks and refit them with new cases thus saving the data within ref name www ukoln ac uk recovery techniques when deciding whether or not to try and recover data the cost must be taken into account if there is enough time and money most data will be able to be recovered in the case of magnetic media which are the most common type used for data storage there are various techniques that can be used to recover the data depending on the type of damage ref name www ukoln ac uk rp 17 humidity can cause tapes to become unusable as they begin to deteriorate and become sticky in this case a heat treatment can be applied to fix this problem by causing the oils and residues to either be reabsorbed into the tape or evaporate off the surface of the tape however this should only be done in order to provide access to the data so it can be extracted and copied to a medium that is more stable ref name www ukoln ac uk rp 17 18 lubrication loss is another source of damage to tapes this is most commonly caused by heavy use but can also be a result of improper storage or natural evaporation as a result of heavy use some of the lubricant can remain on the read write heads which then collect dust and particles this can cause damage to the tape loss of lubrication can be addressed by re lubricating the tapes this should be done cautiously as excessive re lubrication can cause tape slippage which in turn can lead to media being misread and the loss of data ref name www ukoln ac uk rp 18 water exposure will damage tapes over time this often occurs in a disaster situation if the media is in salty or dirty water it should be rinsed in fresh water the process of cleaning rinsing and drying wet tapes should be done at room temperature in order to prevent heat damage older tapes should be recovered prior to newer tapes as they are more susceptible to water damage ref name www ukoln ac uk rp 18 prevention to prevent the need of data archaeology creators and holders of digital documents should take care to employ digital preservation see also data degradation bit rot digital dark age knowledge discovery references references http www worldwidewords org turnsofphrase tp dat1 htm world wide words data archaeology o donnell james joseph avatars of the word from papyrus to cyperspace harvard university press 1998 cite book last1 ross first1 seamus last2 gow first2 ann lastauthoramp yes title digital archaeology rescuing neglected and damaged data resources publisher british library and joint information systems committee place london bristol series electronic libraries programme studies year 1999 language en url http www ukoln ac uk services elib papers supporting pdf p2 pdf isbn 1 90050 851 6 category data management archaeology category digital preservation category archaeological sub disciplines'
b'multiple issues refimprove date december 2010 primary sources date december 2010 notability date december 2010 context date december 2010 ims vdex which stands for ims vocabulary definition exchange is a mark up language or grammar for controlled vocabulary controlled vocabularies developed by ims global as an open specification with the final specification being approved in february 2004 ims vdex allows the exchange and expression of simple machine readable lists of human language terms along with information that may assist a human in understanding the meaning of the various terms i e a flat list of values a hierarchical tree of values a thesaurus a taxonomy a glossary or a dictionary structural a vocabulary has an identifier title and a list of terms each term has a unique key titles and optional descriptions a term may have nested terms thus a hierarchical structure can be created it is possible to define relationships between terms and add custom metadata to terms ims vdex support multilinguality all values supposed to be read by a human i e titles can be defined in one or more languages purposes vdex was designed to supplement other ims specifications and the ieee lom standard by giving additional semantic control to tool developers ims vdex could be used for the following purposes it is used in practice for other purposes as well interfaces providing pre defined choices providing radio buttons and drop down menus for interfaces such as metadata editors or a repository browse tool based on the vocabulary allowed in the metadata profile used distributing vocabularies among many users achieved by simple xml file sharing or possibly a searchable repository open service interface definition repository or registry of vocabularies xml stylesheets used to select and generate different views selecting an overview of an entire vocabulary as an html or pdf file for example providing scope notes for catalogues or storing a glossary of terms which are called upon by hyperlinks within a document validation of metadata instances validated against an application profile by comparison of the vocabulary terms used in certain metadata elements with those of the machine readable version of the vocabularies specified by the application profile controlled terms for other ims specifications and ieee lom both may contain elements where controlled terms should be used these elements are often specified as being of a vocabulary data type and a definition of the permitted terms and their usage may be expressed using vdex technical details image vdex model png 350px right thumb simplified vdex data model the vdex information model is represented in the diagram a vdex file describing a vocabulary comprises a number of information elements most of which are relatively simple such as a string representation of the default human language or a uri identifying the value domain or vocabulary some of the elements are containers such as a term that contain additional elements elements may be required or optional and in some cases repeatable within a term for example a description and caption may be defined multiple language definitions can be used inside a description by using a langstring element where the description is paired with the language to be used additional elements within a term include media descriptors which are one or more media files to supplement a term s description and metadata which is used to describe the vocabulary further the relationship container defines a relationship between terms by identifying the two terms and the specifying type or relationship such as a term being broader or narrower than another the term used to specify the type of relationship may conform to the iso standards for thesauri vocabulary identifiers are unique persistent uris whereas term or relationship identifiers are locally unique strings vdex also allows for a default language and vocabulary name to be given and for whether the ordering of terms within the vocabulary is significant order significance to be specified a profile type is specified to describe the type of vocabulary being expressed different features of the vdex model are permitted depending on the profile type providing a common grammar for several classes of vocabulary for example it is possible in some profile types for terms to be contained within one another and be nested which is suited to the expression of hierarchical vocabularies five profile types exist lax thesaurus hierarchicaltokenterms glossaryordictionary and flattokenterms the lax profile is the least restrictive and offers the full vdex model whereas the flattokenterms profile is the most restrictive and lightweight vdex also offers some scope for complex vocabularies assuming the existence of a well defined application profile for exchange interoperability some examples are faceted schemes faceted vocabularies are possible with the definition of appropriate relationships multi lingual thesauri metadata could be used within a relationship to achieve multilingual thesauri polyhierarchical taxonomies can be expressed using the source target value pairs in the relationship identifiers in vdex data should be persistent unique resolvable transportable and uri compliant specifically vocabulary identifiers should be unique uris whereas term and relationship identifiers should be locally unique strings implementations http aloha2 netera ca aloha metadata tagging tool java based software project that can read ims vdex files http www ivimeds org news demonstrator html ivimeds 1g v1 0 from the international virtual medical school includes vdex instances in curriculum maps partners can create their own maps in vdex format and use these to help students search the repository http www elframework org projects spws view skills profiling web service project implemented and demonstrated use of a skills profiling web service using open standards in a medical context ims vdex files were used in the representation of the spws hierarchy skills framework http www scottishdoctor org scottish doctors project used vdex as a format for expressing curricular outcome systems http prs heacademy ac uk technical vdex scripts html vdex xslt scripts developed by the higher education academy centre for philosophical and religious studies to convert vdex to xhtml and postgresql http www icbl hw ac uk vdex vdex implementation project carried out by the institute for computer based learning at heriot watt university with a primary objective of creating a tool for editing vocabularies in vdex format the project which ended in january 2004 was based on the public draft not the current final specification http sourceforge net projects vdex j vdex java binding implementation neutral java interface for vdex as well as providing a default implementation of that interface and xml marshalling functionality https pypi python org pypi imsvdex imsvdex python egg api for vdex xml files it is free software written in python programming language python http plone org products atvocabularymanager atvocabularymanager addon for plone cms uses vdex as a possible format to define vocabularies https pypi python org pypi collective vdexvocabulary collective vdexvocabulary implements ims vdex as standard zope vocabulary which can also be used in plone cms written in python programming language python https pypi python org pypi vdexcsv vdexcsv offers a commandline converter from comma separated values csv to vdex it is written in python programming language python see also ims global learning object metadata references note coillie marc van coillie http www eife l org publications standards interop europasscv europasscv ims ap usingvdex using ims vdex for the eds ap eifel note sarasa antonio sarasa jose manuel canabal juan carlos sacristan raquel jimenez http online journals org i jet article view 806 using ims vdex in agrega external links http www imsglobal org vdex ims vdex official resources by ims global http wiki cetis ac uk what is ims vdex what is ims vdex jisc cetis http metadata cetis ac uk cetis metadata and digital repository special interest group sig mailing list for those in uk higher and further education interested in creating storing and serving educational metadata category data management category educational technology standards category knowledge representation category library science category metadata category standards category standards organizations category technical communication'
b'in computer networking and database s the three phase commit protocol 3pc ref name 3pc cite journal last skeen first dale title a formal model of crash recovery in a distributed system journal ieee transactions on software engineering volume 9 issue 3 date may 1983 pages 219 228 doi 10 1109 tse 1983 236608 last2 stonebraker first2 m ref is a distributed algorithm which lets all nodes in a distributed system agree to commit data management commit a database transaction transaction unlike the two phase commit protocol 2pc however 3pc is non blocking specifically 3pc places an upper bound on the amount of time required before a transaction either commits or abort computing aborts this property ensures that if a given transaction is attempting to commit via 3pc and holds some lock computer science resource locks it will release the locks after the timeout protocol description in describing the protocol we use terminology similar to that used in the two phase commit protocol thus we have a single coordinator site leading the transaction and a set of one or more cohorts being directed by the coordinator center image three phase commit diagram png center coordinator the coordinator receives a transaction request if there is a failure at this point the coordinator aborts the transaction i e upon recovery it will consider the transaction aborted otherwise the coordinator sends a cancommit message to the cohorts and moves to the waiting state if there is a failure timeout or if the coordinator receives a no message in the waiting state the coordinator aborts the transaction and sends an abort message to all cohorts otherwise the coordinator will receive yes messages from all cohorts within the time window so it sends precommit messages to all cohorts and moves to the prepared state if the coordinator succeeds in the prepared state it will move to the commit state however if the coordinator times out while waiting for an acknowledgement from a cohort it will abort the transaction in the case where an acknowledgement is received from the majority of cohorts the coordinator moves to the commit state as well cohort the cohort receives a cancommit message from the coordinator if the cohort agrees it sends a yes message to the coordinator and moves to the prepared state otherwise it sends a no message and aborts if there is a failure it moves to the abort state in the prepared state if the cohort receives an abort message from the coordinator fails or times out waiting for a commit it aborts if the cohort receives a precommit message it sends an acknowledgement data networks ack message back and awaits a final commit or abort if after a cohort member receives a precommit message the coordinator fails or times out the cohort member goes forward with the commit motivation a two phase commit protocol cannot dependably recover from a failure of both the coordinator and a cohort member during the commit phase if only the coordinator had failed and no cohort members had received a commit message it could safely be inferred that no commit had happened if however both the coordinator and a cohort member failed it is possible that the failed cohort member was the first to be notified and had actually done the commit even if a new coordinator is selected it cannot confidently proceed with the operation until it has received an agreement from all cohort members and hence must block until all cohort members respond the three phase commit protocol eliminates this problem by introducing the prepared to commit state if the coordinator fails before sending precommit messages the cohort will unanimously agree that the operation was aborted the coordinator will not send out a docommit message until all cohort members have ack ed that they are prepared to commit this eliminates the possibility that any cohort member actually completed the transaction before all cohort members were aware of the decision to do so an ambiguity that necessitated indefinite blocking in the two phase commit protocol disadvantages the main disadvantage to this algorithm is that it cannot recover in the event the network is segmented in any manner the original 3pc algorithm assumes a fail stop model where processes fail by crashing and crashes can be accurately detected and does not work with network partitions or asynchronous communication keidar and dolev s e3pc ref name e3pc cite journal last keidar first idit author2 danny dolev title increasing the resilience of distributed and replicated database systems journal journal of computer and system sciences jcss volume 57 issue 3 date december 1998 pages 309 324 url http webee technion ac il idish abstracts jcss html doi 10 1006 jcss 1998 1566 ref algorithm eliminates this disadvantage the protocol requires at least 3 round trips to complete needing a minimum of 3 round trip times rtts this is potentially a long latency to complete each transaction references reflist see also two phase commit protocol category data management category transaction processing'
b'merge data custodian date february 2016 a data steward is a person responsible for the management and fitness of data element s both the content and metadata data stewards have a specialist role that incorporates processes policies guidelines and responsibilities for administering organizations entire data in compliance with policy and or regulatory obligations a data steward may share some responsibilities with a data custodian the overall objective of a data steward is data quality in regard to the key critical data elements existing within a specific enterprise operating structure of the elements in their respective domains this includes capturing documenting meta information for their elements such as definitions related rules governance physical manifestation related data models etc with most of these properties being specific to an attribute concept relationship identifying owners custodians various responsibilities relations insight pertaining to attribute quality aiding with project requirement data facilitation and documentation of capture rules data stewards begin the stewardship stewarding process with the identification of the elements which they will steward with the ultimate result being standards control disambiguation control s and data entry clerk data entry citation needed date october 2014 the steward works closely with business glossary standards analysts for standards with data architect data modeling modeler s for standards with data quality dq analysts for controls and with computer operator operations team member s good quality data going in per business rules while entering data data stewardship roles are common when organizations attempt to exchange data precisely and consistently between computer systems and to reuse data related resources citation needed date october 2014 master data management often quantify date october 2014 makes references to the need for data stewardship for its implementation to succeed data stewardship must have precise purpose fit for purpose or fitness data steward responsibilities a data steward ensures that each assigned data element has clear and unambiguous data element definition does not conflict with other data elements in the metadata registry removes duplicates overlap etc has clear enumerated value definitions if it is of type code metadata code is still being used remove unused data elements is being used consistently in various computer systems is being used fit for purpose data fitness has adequate documentation on appropriate usage and notes documents the origin and sources of authority on each metadata element is protected against unauthorised access or change benefits of data stewardship systematic data stewardship can foster fitness through consistent use of data management resources easy mapping of data between computer systems and exchange documents lower costs associated with migration to for example service oriented architecture soa assignment of each data element to a person sometimes seems like an unimportant process but many groups which date july 2010 have found that users have greater trust and usage rates in systems where they can contact a person with questions on each data element examples expand section date july 2010 the http www epa gov edr united states environmental protection agency epa metadata registry furnishes an example of data stewardship note that each data element therein has a poc point of contact data stewardship applications a new market for data governance applications is emerging one in which both technical and business staff stewards manage policies these new applications like previous generations deliver a strong business glossary capability but they don t stop there vendors are introducing additional features addressing the roles of business in addition to technical stewards concerns ref cite web url https www forrester com report the forrester wave data governance stewardship applications q1 2016 e res117915 title the forrester wave\xe2\x84\xa2 data governance stewardship applications q1 2016 website www forrester com access date 2016 12 20 ref information stewardship applications are business solutions used by business users acting in the role of information steward interpreting and enforcing information governance policy for example these developing solutions represent for the most part an amalgam of a number of disparate previously it centric tools already on the market but are organized and presented in such a way that information stewards a business role can support the work of information policy enforcement as part of their normal business centric day to day work in a range of use cases the initial push for the formation of this new category of packaged software came from operational use cases that is use of business data in and between transactional and operational business applications this is where most of the master data management mdm efforts are undertaken in organizations however there is also now a faster growing interest in the new data lake arena for more analytical use cases ref cite web url https www gartner com document 3284717 ref typeaheadsearch qid 744b6ad6c678d064cc2d6eb831a4c959 title market guide for information stewardship applications last de simoni first guido date 15 april 2016 website www gartner com publisher gartner access date ref see also metadata metadata registry data curation data element data element definition representation term iso iec 11179 references universal meta data models by david marco and michael jennings wiley 2004 page 93 94 isbn 0 471 08177 9 metadata solution by adrinne tannenbaum addison wesley 2002 page 412 building and managing the meta data repository by david marco wiley 2000 pages 61 62 the data warehouse lifecycle toolkit by ralph kimball et el wiley 1998 also briefly mentions the role of data steward in the context of data warehouse project management on page 70 developing geospatial intelligence stewardship for multinational operations by jeff thomas us army command general staff college 2010 www dtic mil dtic tr fulltext u2 a524227 pdf notes reflist category data management category information technology governance category knowledge representation category library occupations category metadata category technical communication'
b'file pcm svg 200px thumb right graphical representations of electrical data analog audio content format red 4 bit digital pulse code modulated content format blue file mi fu on calligraphy jpg 200px thumb right chinese calligraphy written in a language content format by song dynasty a d 1051 1108 poet mi fu file 12345678901 2 23456 barcode upc a svg 200px thumb a series of numbers encoded in a universal product code digital numeric content format a content format is an encoding encoded format for converting a specific type of data to displayable information content media and publishing content formats are used in recording and telecommunication transmission to prepare data for information processing observation or interpreting interpretation ref bob boiko content management bible nov 2004 pp 79 240 830 ref ref ann rockley managing enterprise content a unified content strategy oct 2002 pp 269 320 516 ref this includes both analog signal analog and digitizing digitized content content formats may be recorded and read by either natural or manufactured tools and mechanisms in addition to converting data to information a content format may include the encryption and or scrambler scrambling of that information ref jessica keyes technology trendlines jul 1995 pp 201 ref multiple content formats may be contained within a single section of a storage medium e g multitrack recording track disk sector computer file document page paper page column typography column or transmitted via a single channel communications channel e g wire carrier wave of a transmission medium with multimedia multiple tracks containing multiple content formats are presented simultaneously content formats may either be recorded in secondary signal processing methods such as a software container format e g digital audio digital video or recorded in the primary format e g spectrogram pictogram observable data is often known as raw data or raw content ref oge marques and borko furht content based image and video retrieval april 2002 pp 15 ref a primary raw content format may be directly information processing observable e g image sound motion physics motion odor smell haptic perception sensation or physics physical data which only requires hardware to display it such as a phonograph ic gramophone needle needle and diaphragm acoustics diaphragm or a image projector projector list of light sources lamp and magnifying glass there has been a countless number of content formats throughout history the following are examples of some common content formats and content format categories covering sensory experience model and language used for encoding information width 65 valign top width 50 audio data encoding ref david austerberry the technology of video and audio streaming second edition sep 2004 pp 328 ref audio coding format analog signal analog audio frequency audio data stereophonic sound formats digital audio data synthesizer music sequencer sequences visual data encoding art techniques and materials hand rendering materials film speed formats pixel coordinates data color space data vector graphics vector graphic coordinates dimensions texture mapping formats 3d display formats holographic formats display resolution formatting motion graphics encoding video coding format frame rate data video data ref m ghanbari standard codecs image compression to advanced video coding jun 2003 pp 364 ref computer animation formats instruction encoding musical notation computer language traffic signals width 50 language natural languages formats writing system s phonetic sign language s signal electronics communication signaling formats code formats expert language formats graphic organizer statistical model table of elements standard periodic table table of elements dna sequence human anatomy biometrics biometric data chemical formula s aroma compound psychoactive drug psychoactive drug chart drug chart electromagnetic spectrum time standard numerical weather prediction capital asset pricing model measures of national income and output national income and output celestial coordinate system app 6a military mapping geographic information system interstate highway system see also communication representation arts modulation content carrier signals multiplexing content multiplexing format transmission telecommunications content transmission wireless wireless content transmission data storage device recording format encoder analog television ntsc pal and secam information mapping references reflist category communication category media technology category data management category recording category film and video technology category sound production technology library stub'
b'online complex processing olcp is a class of realtime data processing involving complex queries lengthy queries and or simultaneous reads and writes to the same records software stub sources http www pcmag com encyclopedia term 0 2542 t online complex processing i 48345 00 asp see also online transaction processing olap transaction processing category data management category databases'
b'redirect single source single source publishing also known as single sourcing publishing is a content management method which allows the same source content media content to be used across different forms of media communication media and more than one time ref kay ethier xml and framemaker pg 19 new york city new york apress 2004 isbn 9781430207191 ref ref lucas walsh the application of single source publishing to e government taken from encyclopedia of digital government pg 64 eds ari veikko anttiroiko and matti m\xc3\xa4lki\xc3\xa4 hershey pennsylvania hershey igi global 2007 isbn 9781591407904 ref ref http www stylusstudio com single source publishing html single source publishing at stylus studio copyright \xc2\xa9 2005 2013 progress software accessed june 11 2013 ref ref name petra http www writersua com articles singlesource single source publishing with flare copyright \xc2\xa9 2010 writersua published november 16 2010 accessed june 11 2013 ref the labour intensive and expensive work of technical editing technical editing editing need only be carried out once on only one document ref name cms barry schaeffer http www cmswire com cms information management single source publishing creating customized output 015069 php single source publishing creating customized output cms wire 3 april 2012 accessed 10 june 2013 ref that source document can then be stored in one place and reused ref ann rockley and charles cooper https books google com books id 82x6jgy dhmc pg pt75 dq single source publishing hl en sa x ei rrehu txfs2w0qxt6ycqaq ved 0cdoq6aewbg v onepage q single 20source 20publishing f false managing enterprise content a unified content strategy chapter 5 product content 2nd ed berkeley california berkeley new riders press 2012 isbn 9780132931649 ref this reduces the potential for error as corrections are only made one time in the source document ref janet mackenzie the editor s companion pg 92 cambridge cambridge university press 2011 isbn 9781107402188 ref the benefits of single source publishing primarily relate to the editor rather than the user computing user the user does benefit from consistent terminology and information but this consistency is also a potential weakness of single source publishing if the content manager does not have an organized conceptualization information science conceptualization ref name petra single source publishing is sometimes used synonymously with multi channel publishing though whether or not the two terms are synonymous is a matter of discussion ref name mek http www mekon com index php pages knowledge zone single sourcing multi channel publishing technology standards single source multi channel publishing \xc2\xa9 2013 mekon accessed 23 june 2013 ref definition while there is a general definition of single source publishing there is no single official delineation between single source publishing and multi channel publishing nor are there any official governing bodies to provide such a delinieation single source publishing is most often understood as the creation of one source document in microsoft word or adobe framemaker and converting that document into different file format s or human language s or both multiple times with minimal effort multi channel publishing can either be seen as synonymous with single source publishing or similar in that there is one source document but the process itself results in more than a mere reproduction of that source ref name mek history the origins of single source publishing lie indirectly with the release of windows 3 0 in 1990 ref name bob162 bob boiko https books google com books id p6nudn3zaboc pg pa162 dq single source publishing hl en sa x ei cuiqu6ledalv0qxd44cqdw ved 0ccoq6aewazgk v onepage q single 20source 20publishing f false content management bible pg 162 hoboken new jersey hoboken john wiley sons 2005 isbn 9780764583643 ref with the eclipsing of ms dos by graphical user interface s help files went from being unreadable text along the bottom of the screen to hypertext systems such as winhelp on screen help interfaces allowed software companies to cease the printing of large expensive help manuals with their products reducing costs for both producer and consumer this system raised opportunities as well and many developers fundamentally changed the way they thought about publishing writers of software documentation did not simply move from being writers of traditional bound books to writers of electronic publishing but rather they became authors of central documents which could be reused multiple times across multiple formats ref name bob162 the first single source publishing project was started in 1993 by cornelia hofmann at schneider electric in seligenstadt using software based on interleaf to automatically create paper documentation in multiple languages based on a single original source file ref https books google com books id inceft4akxcc pg pa65 dq single source publishing hl en sa x ei rrehu txfs2w0qxt6ycqaq ved 0ccoq6aewaw v onepage q single 20source 20publishing f false translating into success cutting edge strategies for going multilingual in a global age pg 227 eds robert c sprung and simone jaroniec amsterdam john benjamins publishing company 2000 isbn 9789027231871 ref xml developed during the mid to late 1990s was also significant to the development of single source publishing as a method xml a markup language allows developers to separate their documentation into two layers a shell like layer based on presentation and a core like layer based on the actual written content this method allows developers to write the content only one time while switching it in and out of multiple different formats and delivery methods ref doug wallace and anthony levinson the xml e learning revolution is your production model holding you back taken from https books google com books id 4rk7tj oo3cc pg pa65 dq single source publishing hl en sa x ei cuiqu6ledalv0qxd44cqdw ved 0cegq6aewcdgk v onepage q single 20source 20publishing f false best of the elearning guild s learning solutions articles from the emagazine s first five years pg 63 ed bill brandon hoboken john wiley sons 2008 isbn 9780470277157 ref in the mid 1990s several firms began creating and using single source content for technical documentation boeing helicopter sikorsky aviation and pratt whitney canada and user manuals ford owners manuals based on tagged sgml and xml content generated using the arbortext epic editor with add on functions developed by a contractor the concept behind this usage was that complex hierarchical content that did not lend itself to discrete componentization could be used across a variety of requirements by tagging the differences within a single document using the capabilities built into sgml and xml ford for example was able to tag its single owner s manual files so that 12 model years could be generated via a resolution script running on the single completed file pratt whitney likewise was able to tag up to 20 subsets of its jet engine manuals in single source files calling out the desired version at publication time world book encyclopedia also used the concept to tag its articles for american and british versions of english starting from the early 2000s single source publishing was used with an increasing frequency in the field of technical translation it is still regarded as the most efficient method of publishing the same material in different languages ref bert esselink localisation and translation taken from https books google com books id a4w7lwgcqyoc pg pa73 dq single source publishing hl en sa x ei cuiqu6ledalv0qxd44cqdw ved 0ccuq6aewajgk v onepage q single 20source 20publishing f false computers and translation a translator s guide pg 73 ed h l somers amsterdam john benjamins publishing company 2003 isbn 9789027216403 ref once a printed manual was translated for example the online help for the software program which the manual accompanies could be automatically generated using the method ref burt esselink a practical guide to localization pg 228 volume 4 of language international world directory amsterdam john benjamins publishing company 2000 isbn 9781588110060 ref metadata could be created for an entire manual and individual pages or files could then be translated from that metadata with only one step removing the need to recreate information or even database structures ref cornelia hofmann and thorsten mehnert multilingual information management at schneider automation taken from translating into success pg 67 ref although single source publishing is now decades old its importance has increased urgently as of the 2010s as consumption of information products rises and the number of target audiences expands so does the work of developers and content creators within the industry of software and its documentation there is a perception that the choice is to embrace single source publishing or render one s operations obsolete ref name cms criticism single source publishing has been criticized due to the quality of work being compared to as the conveyor belt assembly of content creation by its critics ref mick hiatt http mashstream com mashups the myth of single source authoring the myth of single source authoring mashstream november 18 2009 ref while heavily used in technical translation there are risks of error in regard to index publishing indexing while two words might be synonym s in english they may not be synonyms in another language in a document produced via single sourcing however the index will be translated automatically and the two words will be rendered as synonyms because they are in the source language translation source language while in the target language translation target language they are not ref nancy mulvany https books google com books id g0eqm8fbitmc pg pa312 dq single source publishing hl en sa x ei rrehu txfs2w0qxt6ycqaq ved 0cecq6aewca v onepage q single 20source 20publishing f false indexing books pg 154 2nd ed chicago university of chicago press 2009 isbn 9780226550176 ref see also content management darwin information typing architecture epub markup language list of single source publishing tools adobe framemaker ref sarah s o keefe sheila a loring terry smith and lydia k wong https books google com books id b yekgcqmn8c pg pa6 dq single source publishing hl en sa x ei rrehu txfs2w0qxt6ycqaq ved 0ce0q6aewcq v onepage q single 20source 20publishing f false publishing fundamentals unstructured framemaker 8 pg 6 scriptorium publishing 2008 isbn 9780970473349 ref apache cocoon apache forrest altova booktype docbook xsl dita open toolkit help manual madcap flare oxygen xml editor oxygen xml editor scenari sphinx documentation generator sphinx ref cite web url http pythonic pocoo org 2008 3 21 sphinx is released title sphinx is released raquo and now for something completely pythonic publisher georg brandl work and now for something completely pythonic accessdate 2011 04 03 ref xplm publisher references reflist further reading cite book last ament first kurt authorlink title single sourcing building modular documentation publisher william andrew date 2007 12 17 location pages 245 url doi id isbn 0 8155 1491 3 cite book last hackos first joann t authorlink title content management for dynamic web delivery publisher wiley date 2002 02 14 location pages 432 url doi id isbn 0 471 08586 3 cite book last glushko first robert j authorlink author2 tim mcgrath title document engineering analyzing and designing documents for business informatics and web services publisher mit press year 2005 location pages 728 url doi id isbn 0 262 57245 1 cite book last maler first eve authorlink author2 jeanne el andaloussi title developing sgml dtds from text to model to markup publisher prentice hall ptr date 1995 12 15 location pages 560 url doi id isbn 0 13 309881 8 the bible for data modeling external links http www elkera com cms articles seminars and presentations planning a single source publishing application for business documents planning a single source publishing application for business documents a paper presented by peter meyer at openpublish sydney on 29 july 2005 https www tug org tugboat tb29 1 tb91sojka pdf single source publishing in multiple formats for different output devices http www agilemodeling com essays singlesourceinformation htm single sourcing information an agile practice for effective documentation http www stcsig org ss society for technical communication single sourcing special interest group http www wisegeek com what is single source publishing htm what is single source publishing at wisegeek http www technical communication org topics information development html tekom europe articles about information development and single source publishing category technical communication category computer file systems category data management'
b'distinguish information management knowledge management refimprove date july 2007 business administration content management cm is a set of processes and technologies that supports the collection managing and publishing of information in any form or medium when stored and accessed via computers this information may be more specifically referred to as digital content or simply as content media and publishing content digital content may take the form of text such as electronic document s multimedia files such as audio or video files or any other file type that follows a content lifecycle requiring product lifecycle management management the process is complex enough to manage that several large and small commercial software vendors such as interwoven and microsoft offer content management system content management software to control and automate significant aspects of the content lifecycle the process of content management content management practices and goals vary by mission and by organizational governance structure news organizations e commerce websites and educational institutions all use content management but in different ways this leads to differences in terminology and in the names and number of steps in the process for example some digital content is created by one or more authors over time that content may be edited one or more individuals may provide some editorial oversight approving the content for publication publishing may take many forms it may be the act of pushing content out to others or simply granting digital access rights to certain content to one or more individuals later that content may be superseded by another version of the content and thus retired or removed from use as when this wiki page is modified content management is an inherently collaborative process it often consists of the following basic roles and responsibilities creator responsible for creating and editing content editor responsible for tuning the content message and the style of delivery including translation and localization publisher responsible for releasing the content for use administrator responsible for managing access permissions to folders and files usually accomplished by assigning access rights to user groups or roles admins may also assist and support users in various ways consumer viewer or guest the person who reads or otherwise takes in content after it is published or shared a critical aspect of content management is the ability to manage versions of content as it evolves see also version control authors and editors often need to restore older versions of edited products due to a process failure or an undesirable series of edits another equally important aspect of content management involves the creation maintenance and application of review standards each member of the content creation and review process has a unique role and set of responsibilities in the development or publication of the content each review team member requires clear and concise review standards these must be maintained on an ongoing basis to ensure the long term consistency and health of the knowledge base a content management system is a set of automated processes that may support the following features import and creation of documents and multimedia material identification of all key users and their roles the ability to assign roles and responsibilities to different instances of content categories or types definition of workflow tasks often coupled with messaging so that content managers are alerted to changes in content the ability to track and manage multiple versions of a single instance of content the ability to publish the content to a repository to support access the ability to personalize content based on a set of rules increasingly the repository is an inherent part of the system and incorporates enterprise search and retrieval content management systems take the following forms web content management system software for web site management often what content management implicitly means output of a newspaper editorial staff organization workflow for essay article publication document management system single source publishing single source content management system content stored in chunks within a relational database variant management system where personnel tag source content usually text and graphics to represent variants stored as single source master content modules resolved to the desired variant at publication for example automobile owners manual content for 12 model years stored as single master content files and called by model year as needed often used in concert with database chunk storage see above for large content objects governance structures content management expert marc feldman defines three primary content management governance structures localized centralized and federated each having its unique strengths and weaknesses ref http www clickz com clickz column 1715089 governance issues content management ref localized governance by putting control in the hands of those closest to the content the context experts localized governance models empower and unleash creativity these benefits come however at the cost of a partial to total loss of managerial control and oversight centralized governance when the levers of control are strongly centralized content management systems are capable of delivering an exceptionally clear and unified brand message moreover centralized content management governance structures allow for a large number of cost savings opportunities in large enterprises realized for example 1 the avoidance of duplicated efforts in creating editing formatting repurposing and archiving content 2 through process management and the streamlining of all content related labor and or 3 through an orderly deployment or updating of the content management system federated governance federated governance models potentially realize the benefits of both localized and centralized control while avoiding the weaknesses of both while content management software systems are inherently structured to enable federated governance models realizing these benefits can be difficult because it requires for example negotiating the boundaries of control with local managers and content creators in the case of larger enterprises in particular the failure to fully implement or realize a federated governance structure equates to a failure to realize the full return on investment and cost savings that content management systems enable implementation content management implementations must be able to manage content distributions and digital rights in content life cycle ref cite journal last white first blake date april 2004 title a new era for content protection potential and profit in the digital world url http ieeexplore ieee org stamp stamp jsp arnumber 7262562 journal smpte motion imaging journal publisher society of motion picture television engineers volume 113 issue 4 pages 110 120 doi access date july 1 2016 ref ref cite web url http www giantstepsmts com cm drmwhitepaper pdf title integrating content management with digital rights management publisher giantsteps accessdate 2016 07 01 ref ref cite web url http www locklizard com content distribution drm title content distribution drm managing content distribution with drm publisher locklizard com accessdate 2016 07 01 ref ref cite book title the world beyond digital rights management url https www amazon com world beyond digital rights management ebook dp b004geaezm location last umeh first jude date october 2007 publisher british computer society page 320 isbn 978 1902505879 ref content management systems are usually involved with digital rights management in order to control user access and digital rights in this step the read only structures of digital rights management systems force some limitations on content management as they do not allow authors to change protected content in their life cycle creating new content using managed protected content is also an issue that gets protected contents out of management controlling systems a few content management implementations cover all these issues ref cite journal last white first blake date april 2004 title a new era for content protection potential and profit in the digital world url http ieeexplore ieee org stamp stamp jsp arnumber 7262562 journal smpte motion imaging journal publisher society of motion picture television engineers volume 113 issue 4 pages 110 120 doi access date july 1 2016 ref ref cite web url http www giantstepsmts com cm drmwhitepaper pdf title integrating content management with digital rights management publisher giantsteps accessdate 2016 07 01 ref ref cite web url http www locklizard com content distribution drm title content distribution drm managing content distribution with drm publisher locklizard com accessdate 2016 07 01 ref ref cite book title the world beyond digital rights management url https www amazon com world beyond digital rights management ebook dp b004geaezm location last umeh first jude date october 2007 publisher british computer society page 320 isbn 978 1902505879 ref see also div col colwidth 22em content delivery content engineering content management interoperability services content management system digital asset management enterprise content management enterprise information management information architecture list of content management systems single source publishing snippet management web content lifecycle web design website architecture website governance div col end references reflist colwidth 30em external links cite book last boiko first bob authorlink title content management bible publisher wiley date 2004 11 26 location pages 1176 url doi id isbn 0 7645 7371 3 cite book last rockley first ann authorlink title managing enterprise content a unified content strategy publisher new riders press date 2002 10 27 location pages 592 url doi id isbn 0 7357 1306 5 cite book last hackos first joann t authorlink title content management for dynamic web delivery publisher wiley date 2002 02 14 location pages 432 url doi id isbn 0 471 08586 3 cite book last glushko first robert j authorlink author2 tim mcgrath title document engineering analyzing and designing documents for business informatics and web services publisher mit press year 2005 location pages 728 url doi id isbn 0 262 57245 1 cite book last ferran first n\xc3\xbaria authorlink author2 juli\xc3\xa0 minguill\xc3\xb3n title content management for e learning publisher springer date 2011 location pages 215 url http www springer com us book 9781441969583 doi id isbn 978 1 4419 6958 3 content management systems webmantools category technical communication category data management category content management systems fr syst\xc3\xa8me de gestion de contenu'
b'master data represents the business objects which are agreed on and shared across the enterprise ref http mitiq mit edu iciq documents iq 20conference 202010 papers 2b1 enterprisemasterdataarchitecture pdf enterprise master data architecture design decisions and options boris otto alexander schmidt institute of information management university of st gallen ref it can cover relatively static reference data dynamic data transactional unstructured data unstructured analytical hierarchical database model hierarchical and metadata meta data ref what is master data roger wolter and kirk haselden microsoft corporation http msdn microsoft com en us library bb190163 aspx the what why and how of master data management november 2006 ref it is the primary focus of the information technology it discipline of master data management mdm while master data is often non transactional in nature it is not limited to non transactional data and often supports transactional processes and operations for example master data may be about customers products employees materials suppliers and vendors and it may also cover sales documents and aggregated sales types of master data reference data is the set of permissible values to be used by other master or transaction data fields reference data normally changes slowly reflecting changes in the modes of operation of the business rather than changing in the normal course of business master data is a single source of common business data used across multiple systems applications and or processes enterprise master data is the single source of common business data used across all systems applications and processes for an entire enterprise all departments divisions companies and countries market master data is the single source of common business data for an entire marketplace market master data is used among enterprises within the value chain an example of market master data is the upc universal product code found on consumer products market master data is compatible with enterprise specific and domain specific systems compliant with or linked to industry standards and incorporated within market research analytics market master data also facilitates integration of multiple data sources and literally puts everyone in the market on the same page excerpted from master data management for media a call to action for business leaders in marketing advertising and the media a microsoft white paper by scott taylor and robin laylin january 2010 master data and master reference data master data is also called master reference data this is to avoid confusion with the usage of the term master data for original data like an original recording see also master tape master data is nothing but unique data i e there are no duplicate values citation needed date january 2011 material master data is a specific data set holding structured information about spare parts raw materials and products within enterprise resource planning erp software the data is held centrally and used across organisations vendor master refers to the centralised location of information pertinent to the vendor often this will include the legal entity name tax identification and contact information master data management main master data management curating and managing master data is key to ensuring master data quality analysis and reporting is greatly dependent on the quality of an organization s master data master data may either be stored in a central repository sourced from one or more systems or referenced centrally using an index however when it is used by several functional groups it may be distributed and redundantly stored in different applications across an organization and this copy data may be inconsistent and if so inaccurate ref http blogs gartner com andrew white 2014 01 14 the elephant in the room master data and application data comments the elephant in the room master data and application data andrew white gartner 14 january 2014 ref thus master data should have an agreed upon view that is shared across the organization care should be taken to properly version master data if the need arises to modify it to avoid issues with distributed copies see also master data management customer data integration product information management data governance external links reflist http www stibosystems com files billeder stibo 20systems 20images uk resource library images what is master data management en png what is master data http www semarchy com overview what is master data semarchy what is master data http www orchestranetworks com rdm managing reference data rdm https www youtube com watch v 2tzvuqwaovg aaron zornes understanding reference data category data management fr donn\xc3\xa9es de r\xc3\xa9f\xc3\xa9rence'
b'a workflow engine is a software application that manages business processes it is a key component in workflow technology and typically makes use of a database server a workflow engine manages and monitors the state of activities in a workflow such as the processing and approval of a loan application form and determines which new activity to transition to according to defined processes workflows ref http docs oracle com cd b13789 01 workflow 101 b10286 wfapi htm ref the actions may be anything from saving an application form in a document management system to sending a reminder e mail to users or escalating overdue items to management a workflow engine facilitates the flow of information tasks and events workflow engines may also be referred to as workflow orchestration engines ref http pic dhe ibm com infocenter tivihelp v48r1 index jsp topic 2fcom ibm sco doc 2 2 2fenablement 2fworkfloworchestration html ref workflow engines mainly have three functions verification of the current status check whether the command is valid in executing a task determine the authority of users check if the current user is permitted to execute the task executing condition script after passing the previous two steps the workflow engine begins to evaluate the condition script in which the two processes are carried out if the condition is true workflow engine execute the task and if the execution successfully completes it returns the success if not it reports the error to trigger and roll back the change ref the workflow engine model http msdn microsoft com en us library aa188337 28office 10 29 aspx the workflow engine model accessed 1 dec 2010 ref a workflow engine is a core technique for task allocation software such as business process management in which the workflow engine allocates tasks to different executors while communicating data among participants a workflow engine can execute any arbitrary sequence of steps for example a healthcare data analysis ref name hf2010 cite journal last1 huser first1 v last2 rasmussen first2 l v last3 oberg first3 r last4 starren first4 j b title implementation of workflow engine technology to deliver basic clinical decision support functionality doi 10 1186 1471 2288 11 43 journal bmc medical research methodology volume 11 pages 43 year 2011 pmid 21477364 pmc 3079703 ref see also business rules engine business rule management system comparison of bpel engines inference engine java rules engine api rete algorithm ripple down rules semantic reasoner bpel business process execution language production system computer science production system workflow management system references reflist category data management category servers computing category workflow technology category workflow software'
b'essay like date may 2012 a distributed data store is a computer network where information is stored on more than one node networking node often in a replication computing replicated fashion ref citation author yaniv pessach url http openlibrary org books ol25423189m distributed storage concepts algorithms and implementations title distributed storage edition distributed storage concepts algorithms and implementations ref it is usually specifically used to refer to either a distributed database where users store information on a number of nodes or a computer network in which users store information on a number of peer network nodes distributed databases distributed database s are usually non relational database s that make a quick access to data over a large number of nodes possible some distributed databases expose rich query abilities while others are limited to a key value store semantics examples of limited distributed databases are google s bigtable which is much more than a distributed file system or a peer to peer network ref cite web accessdate 2011 04 05 location http the paper trail org publisher paper trail title bigtable google s distributed data store quote although gfs provides google with reliable scalable distributed file storage it does not provide any facility for structuring the data contained in the files beyond a hierarchical directory structure and meaningful file names it s well known that more expressive solutions are required for large data sets google s terabytes upon terabytes of data that they retrieve from web crawlers amongst many other sources need organising so that client applications can quickly perform lookups and updates at a finer granularity than the file level the very first thing you need to know about bigtable is that it isn t a relational database this should come as no surprise one persistent theme through all of these large scale distributed data store papers is that rdbmss are hard to do with good performance there is no hard fixed schema in a bigtable no referential integrity between tables so no foreign keys and therefore little support for optimised joins url http the paper trail org blog p 86 ref amazon com amazon s dynamo storage system dynamo ref cite web accessdate 2011 04 05 author sarah pidcock date 2011 01 31 location http www cs uwaterloo ca page 2 22 publisher waterloo cheriton school of computer science title dynamo amazon s highly available key value store quote dynamo a highly available and scalable distributed data store url http www cs uwaterloo ca kdaudjee courses cs848 slides sarah1 pdf ref and azure services platform windows azure storage ref cite web url http www microsoft com windowsazure features storage title windows azure storage author date 2011 09 16 work publisher accessdate 6 november 2011 ref as the ability of arbitrary querying is not as important as the availability designers of distributed data stores have increased the latter at an expense of consistency but the high speed read write access results in reduced consistency as it is not possible to have both consistency database systems consistency availability and partition tolerance of the network as it has been proven by the cap theorem peer network node data stores in peer network data stores the user can usually reciprocate and allow other users to use their computer as a storage node as well information may or may not be accessible to other users depending on the design of the network most peer to peer networks do not have distributed data stores in that the user s data is only available when their node is on the network however this distinction is somewhat blurred in a system such as bittorrent protocol bittorrent where it is possible for the originating node to go offline but the content to continue to be served still this is only the case for individual files requested by the redistributors as contrasted with a network such as freenet where all computers are made available to serve all files distributed data stores typically use an error detection and correction technique some distributed data stores such as parchive over nntp use forward error correction techniques to recover the original file when parts of that file are damaged or unavailable others try again to download that file from a different mirror examples distributed non relational databases aerospike database aerospike apache cassandra former data store of facebook bigtable the data store of google crateio druid open source data store used by netflix yahoo and others dynamo storage system dynamo of amazon com amazon hazelcast hbase current data store of facebook s messaging platform couchbase data store used by linkedin paypal ebay and others mongodb riak hypertable from baidu voldemort distributed data store voldemort data store used by linkedin peer network node data stores bittorrent protocol bittorrent blockchain database chord project gnunet freenet unity of the software perfect dark p2p perfect dark mnet computer program mnet network news transfer protocol nntp the distributed data storage protocol used for usenet news storage home tahoe lafs see also portal computer science data store distributed file system keyspace distributed data store keyspace the dds schema database schema peer to peer distributed hash table distributed cache references reflist category data management category distributed data storage category distributed data stores ja \xe5\x88\x86\xe6\x95\xa3\xe3\x83\x95\xe3\x82\xa1\xe3\x82\xa4\xe3\x83\xab\xe3\x82\xb7\xe3\x82\xb9\xe3\x83\x86\xe3\x83\xa0 \xe5\x88\x86\xe6\x95\xa3\xe3\x83\x87\xe3\x83\xbc\xe3\x82\xbf\xe3\x82\xb9\xe3\x83\x88\xe3\x82\xa2'
b'about the association dama disambiguation about dama dama the http dama org data management association is a not for profit vendor independent international association of technical and business professionals dedicated to advancing the concepts and practices of information resource management irm and data resource management drm dama s primary purpose is to promote the understanding development and practice of managing information and data as a key enterprise asset the group is organized as a set of more than 40 chapters and members at large around the world with an international conference held every year chapters chapter structure and central membership dama international is organised through a chapter structure with each chapter being a separate legal entity that formally affiliates with dama international there are over 40 chapters established in over 16 countries around the world the united states is disproportionately represented in the number of chapters due to the number of city based chapters as opposed to country level in other jurisdictions in 2015 dama introduced a central membership to help support and develop member services in a more consistent manner internationally and to provide a clear rallying point for members world wide who may lack a local chapter structure a full listing of chapters can be found on the http www dama org browse chapters dama international website this list does not include chapters in formation which have yet to meet the criteria for recognition as chapters and formal affiliation with dama i the data management body of knowledge dmbok the dama guide to the data management data management body of knowledge dama dmbok guide was first published in april 5th 2009 it defines ten knowledge domains which are at the core of information and data management data governance the central knowledge domain that connects all the others data architecture management data development data operations management data security management reference and master data management data warehousing and business intelligence document and content management metadata management data quality management the dmbok is copyright dama international cdmp dama international is the owner of the certified data management professional certification this certification is based on a range of learning objectives derived from the dmbok in october 2015 dama international terminated its relationship with iccp who had provided administrative services for the delivery of the cdmp certification dama awards from its inception in 1989 through to 2002 the dama individual achievement awards have recognized a data professional who has made significant demonstrable contributions to the information resource management industry consistent with dama international s vision from 2003 to 2015 dama created additional categories to recognize more people who have made special contributions to the world of data management the awards categories are academic achievement award to a member from academia for outstanding research or theoretical contributions in the area of irm drm dama community award to a member of the dama community who has gone beyond the call of volunteer service to enhance the efforts of providing exceptional benefits to the dama membership government achievement award to a member of the leadership populace for instituting the inclusion and adherence to drm irm principles professional achievement award to a member from the industry business discipline specialist who has made significant demonstrable contributions to the irm drm lifetime achievement and contribution award this special award has been presented to john zachman in 2002 michael brackett in 2006 and catherine nolan in 2015 starting in 2016 dama international created the dama international award for data management excellence this award will be presented to organizations or individuals who have made contributions to data management principles the first awards under this new structure will be given in april 2016 list of award winners a list of award winners can be found on the https www dama org content award results dama website speakers bureau dama international provides a speakers bureau service to connect conference and event organisers with internationally regarded expert speakers a full listing of speakers can be found http www dama org speakers here references references external links http www dama org dama international prof assoc stub category data management'
b'more sources date october 2015 long running transactions also known as saga transactions are computer database transaction s that avoid lock computer science locks on non local resources use compensation to handle failures potentially aggregate smaller acid transactions also referred to as atomic transaction s and typically use a coordinator to complete or abort the transaction in contrast to rollback data management rollback in acid transactions compensation restores the original state or an equivalent and is business specific for example the compensating action for making a hotel reservation is canceling that reservation possibly with a penalty a number of protocols have been specified for long running transactions using web services within business processes oasis business transaction processing ref http www oasis open org committees tc home php wg abbrev business transaction ref and ws caf ref http www oasis open org committees tc home php wg abbrev ws caf ref are examples these protocols use a coordinator to mediate the successful completion or use of compensation in a long running transaction see also optimistic concurrency control long lived transaction references reflist category data management category transaction processing'
b'more footnotes date march 2011 database administration is the function of managing and maintaining database management system s dbms software mainstream dbms software such as oracle database oracle ibm db2 and microsoft sql server need ongoing management as such corporations that use dbms software often hire specialized it information technology information technology personnel called database administrator database administrators or dbas dba responsibilities installation configuration and upgrading of database server software and related products evaluate database features and database related products establish and maintain sound backup and recovery policies and procedures take care of the database design and implementation implement and maintain database security create and maintain users and roles assign privileges database tuning and performance monitoring application tuning and performance monitoring setup and maintain documentation and standards plan growth and changes capacity planning work as part of a team and provide 24x7 support when required do general technical troubleshooting and give cons database recovery types of database administration there are three types of dbas systems dbas also referred to as physical dbas operations dbas or production support dbas focus on the physical aspects of database administration such as dbms installation configuration patching upgrades backups restores refreshes performance optimization maintenance and disaster recovery development dbas focus on the logical and development aspects of database administration such as data model design and maintenance ddl data definition language data definition language generation sql writing and tuning coding stored procedure s collaborating with developers to help choose the most appropriate dbms feature functionality and other pre production activities application dbas usually found in organizations that have purchased third party developer 3rd party application software such as erp enterprise resource planning and crm customer relationship management systems examples of such application software includes oracle applications siebel and peoplesoft both now part of oracle corp and sap application dbas straddle the fence between the dbms and the application software and are responsible for ensuring that the application is fully optimized for the database and vice versa they usually manage all the software componentry application components that interact with the database and carry out activities such as application installation and patching application upgrades database cloning building and running data cleanup routines data load process management etc while individuals usually specialize in one type of database administration in smaller organizations it is not uncommon to find a single individual or group performing more than one type of database administration nature of database administration the degree to which the administration of a database is automated dictates the skills and personnel required to manage databases on one end of the spectrum a system with minimal automation will require significant experienced resources to manage perhaps 5 10 databases per dba alternatively an organization might choose to automate a significant amount of the work that could be done manually therefore reducing the skills required to perform tasks as automation increases the personnel needs of the organization splits into highly skilled worker s to create and manage the automation and a group of lower skilled line dbas who simply execute the automation database administration work is complex repetitive time consuming and requires significant training since databases hold valuable and mission critical data companies usually look for candidates with multiple years of experience database administration often requires dbas to put in work during off hours for example for planned after hours downtime in the event of a database related outage or if performance has been severely degraded dbas are commonly well compensated for the long hours one key skill required and often overlooked when selecting a dba is database recovery under disaster recovery it is not a case of if but a case of when a database suffers a failure ranging from a simple failure to a full catastrophic failure the failure may be data corruption media failure or user induced errors in either situation the dba must have the skills to recover the database to a given point in time to prevent a loss of data a highly skilled dba can spend a few minutes or exceedingly long hours to get the database back to the operational point database administration tools often the dbms software comes with certain tools to help dbas manage the dbms such tools are called native tools for example microsoft sql server comes with sql server management studio and oracle has tools such as sql plus and oracle enterprise manager grid control in addition 3rd parties such as bmc quest software embarcadero technologies ems database management solutions and sql maestro group offer gui tools to monitor the dbms and help dbas carry out certain functions inside the database more easily another kind of database software exists to manage the provisioning of new databases and the management of existing databases and their related resources the process of creating a new database can consist of hundreds or thousands of unique steps from satisfying prerequisites to configuring backups where each step must be successful before the next can start a human cannot be expected to complete this procedure in the same exact way time after time exactly the goal when multiple databases exist as the number of dbas grows without automation the number of unique configurations frequently grows to be costly difficult to support all of these complicated procedures can be modeled by the best dbas into database automation software and executed by the standard dbas software has been created specifically to improve the reliability and repeatability of these procedures such as stratavia s data palette and gridapp systems clarity the impact of it automation on database administration recently automation has begun to impact this area significantly newer technologies such as stratavia s data palette suite and gridapp systems clarity have begun to increase the automation of databases causing the reduction of database related tasks however at best this only reduces the amount of mundane repetitive activities and does not eliminate the need for dbas the intention of dba automation is to enable dbas to focus on more proactive activities around database architecture deployment performance and service level management every database requires a database owner account that can perform all schema management operations this account is specific to the database and cannot log in to data director you can add database owner accounts after database creation data director users must log in with their database specific credentials to view the database its entities and its data or to perform database management tasks database administrators and application developers can manage databases only if they have appropriate permissions and roles granted to them by the organization administrator the permissions and roles must be granted on the database group or on the database and they only apply within the organization in which they are granted learning database administration there are several education institutes that offer professional courses including late night programs to allow candidates to learn database administration also dbms vendors such as oracle microsoft and ibm offer certification programs to help companies to hire qualified dba practitioners college degree in computer science or related field is helpful but not necessarily a prerequisite see also column oriented dbms data warehouse directory service distributed database management system hierarchical model navigational database network model object model object database oodbms object relational database ordbms run book automation rba relational model rdbms comparison of relational database management systems comparison of database tools sql is a language for database management external links cite journal publisher acm special interest group on information retrieval work sigir forum volume 7 issue 4 date winter 1972 pages 45 55 url http portal acm org citation cfm id 1095495 1095500 title a set theoretic data structure and retrieval language cite journal publisher sigmod acm special interest group on management of data work sigmod record volume 35 issue 2 date june 2006 url http www tomandmaria com tom writing veritablebucketoffactssigmod pdf title origins of the data base management system format pdf author thomas haigh databases foldoc category database management systems category data management'
b'unreferenced date may 2009 content engineering is a term applied to an engineering speciality dealing with the issues around the use of content media and publishing content in computer facilitated environments content production content management content modelling content conversion and content use and repurposing are all areas involving this speciality it is not a speciality with wide industry recognition and is often performed on an ad hoc basis by members of software development or content production staff but is beginning to be recognized as a necessary function in any complex content centric project involving both content production as well as software system development content engineering tends to bridge the gap between groups involved in the production of content publishing and editing editorial staff marketing sales human resources hr and more technologically oriented departments such as software development or information technology it that put this content to use in web or other software based environments and requires an understanding of the issues and processes of both sides typically content engineering involves extensive use of embedded xml technologies xml being the most widespread language for representing structured content content management system content management systems are often key technology used in this practice though frequently content engineering fills the gap where no formal cms has been put into place category data management'
b'unreferenced date march 2007 a data field is a place where you can store data commonly used to refer to a column in a database or a field in a data entry clerk data entry form or web form the field may contain data to be entered as well as data to be displayed see also wiktionary data data dictionary data element data acquisition data hierarchy category data management it campo informatica'
b'versomatic installs as a file system service where it tracks file changes and preemptively archives a copy of a file before it is modified archiving copies pre emptively obviates the need to archive a reference copy of the files beforehand as would be the case if the files were archived after being edited starting from the moment versomatic is installed the last version of a file remains where the user expects it to be and prior versions if any reside in a separate archive without this capability we would have to scan your entire hard drive beforehand and make a duplicate copy of every file in order to create a baseline for subsequent revisions files can be moved renamed and copied without losing connection to their revision histories depending on user preferences files can be monitored on local removable and network drives file revision histories are stored in a central database thus for example a user may insert a usb drive into his computer and edit a file on the usb drive the edited file remains on the usb drive but a copy of the original unedited version is copied to the archive when a file is deleted the deleted file is added to the repository together with any previous versions of the deleted file this provides excellent level of protection against inadvertent file deletion access to prior versions of a file is easy a user merely selects a file and clicks the right mouse button to display a contextual pop up menu listing the x most recent previous versions of the file if any upon user selection of one of the entries the appropriate previous version is opened with read only privileges using the same application that created the file versomatic can also retrieve a copy of a previous version and move it into the same directory where the current version resides the previous version will have a date time stamp added to its file name in order to distinguish it from the current version if a previous version is exported from versomatic s archive and changes are made thereto a new revision history is created for the new file history versomatic is the first product from the long time team of joaquin de soto jorge miranda and manny menendez released under their new company acertant the team has a long list of hit products to their credit maclightning acd canvas canvas spelling coach bigthesaurus comment ultrapaint artworks acdsee denebacad etc external links http www acertant com company web page category data management'
b'multiple issues unreferenced date march 2012 notability products date march 2012 infobox software name scriptella logo file scriptella logo png 160px scriptella logo latest release version 1 1 latest release date 28 december 2012 operating system cross platform genre extract transform load etl data migration and sql license apache software license website http scriptella org http scriptella org scriptella is an open source extract transform load etl extract transform load and script execution tool written in java its primary focus is simplicity it doesn t require the user to learn another complex xml based language to use it but allows the use of sql or another scripting language suitable for the data source to perform required transformations scriptella does not offer any graphical user interface typical use database migration database creation update scripts cross database etl operations import export alternative for ant sql task automated database schema upgrade features simple xml syntax for scripts add dynamics to your existing sql scripts by creating a thin wrapper xml file source lang xml doctype etl system http scriptella javaforge com dtd etl dtd etl connection driver driver url url user user password password script include href path to your script sql and or directly insert sql statements here script etl source support for multiple datasources or multiple connections to a single database in an etl file support for many useful java database connectivity jdbc features e g parameters in sql including file blobs and jdbc escaping performance performance and low memory usage are one of the primary goals support for evaluated expressions and properties jexl syntax support for cross database etl scripts by using dialect elements transactional execution error handling via onerror elements conditional scripts queries execution similar to ant if unless attributes but more powerful easy to use as a standalone tool or ant task no deployment installation required easy to run etl files directly from java code built in adapters for popular databases for a tight integration support for any database with jdbc open database connectivity odbc compliant driver service provider interface spi for interoperability with non jdbc datasources and integration with scripting languages out of the box support for jsr 223 jsr 223 scripting for the java platform compatible languages built in comma separated values csv text xml lightweight directory access protocol ldap apache lucene lucene apache velocity velocity jexl and janino providers integration with java ee spring framework spring framework java management extensions jmx and jndi for enterprise ready scripts external links http scriptella org scriptella etl site https github com scriptella scriptella etl github page http groups google com group scriptella discussion forum http www javaforge com proj forum browseforum do forum id 3126 discussion forum deprecated http jroller com page ejboy scriptella etl author s blog ohloh project id 4526 name scriptella etl category extract transform load tools category data warehousing products category data management compu stub'
b'note the following content requires a knowledge of database technologies the following is a comparison of two different database access technologies from microsoft namely activex data objects activex data objects ado and ado net before comparing the two technologies it is essential to get an overview of microsoft data access components mdac and the net framework microsoft data access components provide a uniform and comprehensive way of developing applications for accessing almost any data store entirely from managed code managed and unmanaged unmanaged code the net framework is an virtual machine application virtual machine application virtual machine based software environment that provides security mechanisms memory management and exception handling and is designed so that developers need not consider the capabilities of the specific cpu that will execute the net application the net virtual machine application virtual machine application virtual machine turns intermediate language il into machine code high level language compilers for c sharp programming language c visual basic net vb net and c are provided to turn source code into il ado net is shipped with the microsoft net framework activex data objects ado relies on component object model com whereas ado net relies on managed providers defined by the net common language runtime clr ado net does not replace ado for the com programmer rather it provides the net programmer with access to relational data sources xml and application data class wikitable ado ado net business model connection oriented models used mostly disconnected models are used message like models disconnected access provided by record set provided by data adapter and data set xml support limited robust support connection model client application needs to be connected always to data server while working on the data unless using client side cursors or a disconnected record set client disconnected as soon as the data is processed dataset is disconnected at all times data passing ado objects communicate in binary mode ado net uses xml for passing the data control of data access behaviors includes implicit behaviors that may not always be required in an application and that may therefore limit performance provides well defined factored components with predictable behavior performance and semantics design time support derives information about data implicitly at run time based on metadata that is often expensive to obtain leverages known metadata at design time in order to provide better run time performance and more consistent run time behavior references http msdn2 microsoft com en us library ms973217 aspx ado net for the ado programmer category data management category net framework category microsoft application programming interfaces category sql data access category software comparisons ado and ado net'
b'multiple issues notability date may 2010 unreferenced date may 2010 peacock date september 2012 holos is an influential olap online analytical processing product of the 1990s developed by holistic systems in 1987 the product remained in use until around 2004 conception the holos product succeeded an older generation of mainframe products such as system w it was the first to use an industry standard sql database as opposed to a proprietary one and also the first to use the new gui pc for the user interface citation needed date september 2012 in physically separating the number crunching from the user interface the product was immediately client server although the term didn t come into use until some time later in fact the process was described as cooperative processing at the time as client server was not a current term at that time the client server model used for holos was initially for a very light client as it was not clear at that time 1986 7 that pcs were going to be so commonplace and most were still running ms dos in fact it was technically possible to run the system using dumb terminal with reduced functionality in early versions although save for in holistic s test environment this was rarely if ever done in time due to the increased popularly of pcs and their increased power and the available of a stable and more functional version of microsoft windows additional functionality was added to the client end mostly in the form of development aids in addition to data services the holos server supplied business logic and calculation services it also provided complementary services to the holos client which meant the internal processing associated with the report writer worksheet etc was distributed between the two components architecture the core of the holos server was a business intelligence bi virtual machine the holos language hl was compiled into a soft instruction code and executed in this virtual machine similar in concept to java in more modern systems the virtual machine was fully fault tolerant using structured exception handling internally and provided a debugger interface the debugger was machine level until quite late on after which it also supported source level access olap data was handled as a core data type of hl with specific syntax to accommodate multidimensional data concepts and complete programmatic freedom to explore and utilise the data this made it very different from the industry trend of query based olap and sql engines on the upside it allowed amazing flexibility in the applications to which it could be applied on the downside it mean that 3 tier configurations were never successfully implemented since the processing had to be close to the data itself this hindered large scale deployment to many clients and the use of olap data from other vendors in reality its own data access times were probably some of the fastest around at the individual cell level they had to be in order to be practical however when fetching back bulk data for non cooperating clients or data from other vendors the queries could not be optimised as a whole its own data access used a machine wide shared memory cache language the holos language was a very broad language in that it covered a wide range of statements and concepts including the reporting system business rules olap data sql data using the embedded sql syntax within the hosting hl device properties analysis forecasting and data mining it even supported elements to enable self documentation and self verification placing all these areas on a common footing and allowing them to co operate by sharing data events etc was key to the number of possibilities that resulted for instance the report writer supported input as well as output plus interactive graphics and a comprehensive event mechanism to pass back information about the viewed data to event handlers also reports and data were separate entities thus allowing the same report to be applied to different data as long as it was described by similar meta data this meant that when terms like enterprise information system eis and management information systems mis were first coined the industry norm was slideshows i e pre programmed transitions between views whereas holos provided data driven drill down i e no pre programmed views or links the transitions could be made dependent upon the data values and trends in conjunction with the available business logic olap storage holos server provided an array of different but compatible storage mechanisms for its multi cube architecture memory disk sql it was therefore the first product to provide hybrid olap holap it provided a very versatile mechanism for joining cubes irrespective of their storage technology dimensionality or meta data and this was eventually given a us patent called coa compound olap architecture us patent 6289352 us patent 6490593 one novel aspect of this was a stack feature that allowed read write cubes to be stacked over read only cubes read operations to the overall virtual cube then visited both racks top first and then the bottom whereas write operations only affected the top the resulting valve like mechanism found many applications in data sharing what if forecasting and aggregation of slow sql based data since the overhead of the joining was small it was not uncommon to have stacks 7 levels deep and joining terabytes of real olap data around about v8 5 holos server implemented a hierarchical lock manager allowing nesting of fine and coarse grain olap locks and full transaction control business rules the business logic supported full cross dimensional calculations automatic ordering of rules using static data flow analysis and the identification and solution of simultaneous equations the rules treated all dimensions in an orthogonal fashion the aggregation process did not distinguish between simple summation or average calculations and more complex non commutative calculations both could be applied to any dimension member the process allowed aggregation levels i e those calculation levels starting with base data level 0 and proceeding up to the overall grand total to be individually pre stored or left to be calculated on demand holos client the holos client was both a design and delivery vehicle and this made it quite large around about 2000 the holos language was made object oriented hl with a view to allowing the replacement of the holos client with a custom java or vb product however the company were never sold on this and so the project was abandoned one of the biggest failures was not to provide a thin client interface to the holos server and this must have contributed to the product s demise although an html toolkit was sold it was clumsy and restricted by the time a real thin client mechanism was developed it was far too late and it never got to market deployment before its demise the holos server product ran under windows nt intel and alpha vms vax and alpha plus about 10 flavours of unix and accessed over half a dozen different sql databases it was also ported to several different locales including japanese company main crystal decisions holistic systems was purchased by the hardware company seagate technology in 1996 along with other companies such as crystal services it was used to create a new subsidiary company called seagate software only holistic and crystal remained and seagate software was renamed to crystal decisions holistic and crystal had very different sales models the average sale for the holos product in the united states was in excess of 250 000 and was sold primarily to fortune 500 companies by a direct sales force the crystal sales model was based upon a shrink wrapped product crystal reports sold primarily through resellers as crystal was acquired prior to holistic the senior management in the sales and marketing arena were mostly drawn from that organisation they felt that all the product range should be sold through third parties and over a period of time dismantled the direct sales force culmination in a significant drop in sales for the holos product subsequently after some in fighting and argument over product strategy the main holos development team finally started to leave around 2000 and crystal decisions was finally taken over by business objects company business objects in 2004 following the takeover support for holos was outsourced to raspberry software which was set up by former employees of crystal decisions category data management category online analytical processing'
b'orphan date february 2009 fundamental to modern information architecture s and driven by http www webreference com internet semantic semantic web technologies content re appropriation is the act of searching filtering gathering grouping and aggregation which allows information to be related classified and identified this is achieved by applying syntactic or semantic meaning though intelligent tagging or artificial interpretation of fragmented content see resource description framework hence all information becomes valuable and interpretable domain since the domain of content applies to areas of software applications document s and computer media media these can be processed though a pipeline of generation aggregation transform many and serialization see http www w3 org tr xml pipeline xml pipeline the output of this can viewed in a medium most effect for decision making the desired outcomes of content re appropriation are seamless integrated and shared user experiences software visualization visualization detection analysis investigation personalization unique to the user inbound or outbound web syndication syndication of information publish or subscribe to information dynamically adapted output to users medium essentially to make information disparities transparent to the user computing user getting to the bottom line \xe2\x80\xa6 quickly areas of use content re appropriation is effective across the content tier that is places where content exists identity directory management e g lightweight directory access protocol ldap security assertion markup language saml jndi content management e g http jakarta apache org slide apache slide content systems e g file system s e mail network share s storage area network san database business systems e g enterprise resource planning erp customer relationship management crm data warehouse e g olap internet web services e g hypertext transfer protocol http simple object access protocol soap instant messenger presence and peer to peer see also knowledge visualization web indexing taxonomy general taxonomy category data management category technical communication'
b'in the field of data management data classification as a part of information lifecycle management ilm process can be defined as a tool for categorization of data to enable help organization to effectively answer following questions what data type s are available where are certain data located what access level s are implemented what protection level is implemented and does it adhere to compliance regulation compliance regulations when implemented it provides a bridge between it professionals and process or application owners it staff is informed about the data value and on the other hand management usually application owners understands better to what segment of data centre has to be invested to keep operations running effectively this can be of particular importance in risk management legal discovery and compliance with government regulations data classification is typically a manual process however there are many tools from different vendors that can help gather information about the data how to start process of data classification note that this classification structure is written from a data management perspective and therefore has a focus for text and text convertible binary data sources images videos and audio files are highly structured formats built for industry standard api s and do not readily fit within the classification scheme outlined below first step is to evaluate and divide the various applications and data into their respective category as follows relational or tabular data around 15 of non audio video data generally describes proprietary data which can be accessible only through application or application programming interfaces api applications that produce structured data are usually database applications this type of data usually brings complex procedures of data evaluation and migration between the storage tiers to ensure adequate quality standards the classification process has to be monitored by subject matter experts semi structured or poly structured data all other non audio video data that does not conform to a system or platform defined relational or tabular form generally describes data files that have a dynamic or non relational semantic structure e g documents xml json device or system log output sensor output relatively simple process of data classification is criteria assignment simple process of data migration between assigned segments of predefined storage tiers types of data classification note that this designation is entirely orthogonal to the application centric designation outlined above regardless of structure inherited from application data may be of the types below 1 geographical i e according to area supposing the rice production of a state or country etc 2 chronological i e according to time sale of last 3 months 3 qualitative i e according to distinct categories e g population on the basis of poor and rich 4 quantitative i e according to magnitude a discrete and b continuous basic criteria for semi structured or poly structured data classification time criteria is the simplest and most commonly used where different type of data is evaluated by time of creation time of access time of update etc metadata criteria as type name owner location and so on can be used to create more advanced classification policy content criteria which involve usage of advanced content classification algorithms are most advanced forms of unstructured data classification note that any of these criteria may also apply to tabular or relational data as basic criteria these criteria are application specific rather than inherent aspects of the form in which the data is presented basic criteria for relational or tabular data classification these criteria are usually initiated by application requirements such as disaster recovery and business continuity rules data centre resources optimization and consolidation hardware performance limitations and possible improvements by reorganization note that any of these criteria may also apply to semi poly structured data as basic criteria these criteria are application specific rather than inherent aspects of the form in which the data is presented benefits of data classification benefits of effective implementation of appropriate data classification can significantly improve ilm process and save data centre storage resources if implemented systemically it can generate improvements in data centre performance and utilization data classification can also reduce costs and administration overhead good enough data classification can produce these results data compliance and easier risk management data are located where expected on predefined storage tier and point in time simplification of data encryption because all data need not be encrypted this saves valuable processor cycles and all related consecutiveness data indexing to improve user access times data protection is redefined where rto recovery time objective is improved see also data classification business intelligence references josh judd and dan kruger 2005 principles of san design infinity publishing stephen j bigelown november 2005 searchstorage com http searchstorage techtarget com news article 0 289142 sid5 gci1139240 00 html category data management category information technology category regulations'
b'ca gen is a computer aided software engineering case application development environment marketed by ca technologies gen was previously known as ief information engineering facility composer by ief composer cool gen advantage gen and allfusion gen the toolset originally supported the information engineering methodology developed by clive finkelstein james martin author james martin and others in the early 1980s early versions supported ibm s ibm db2 db2 database ibm 3270 3270 block mode screens and generated cobol code in the intervening years the toolset has been expanded to support additional development techniques such as component based software engineering component based development creation of client server model client server and web application s and generation of c programming language c java programming language java and c sharp programming language c in addition other platforms are now supported such as many variants of ix like operating systems aix hp ux solaris linux as well as windows its range of supported database technologies have widened to include oracle database oracle microsoft sql server odbc java database connectivity jdbc as well as the original db2 the toolset is fully integrated objects identified during analysis carry forward into design without redefinition all information is stored in a repository central encyclopedia the encyclopedia allows for large team development controlling access so that multiple developers may not change the same object simultaneously ref https communities ca com web ca gen edge global user community wiki wiki edge user group ca gen wiki what is ca gen p 36 ref overview it was initially produced by texas instruments with input from james martin author james martin and his consultancy firm james martin associates and was based on the information engineering methodology iem the first version was launched in 1987 ief became popular among large government departments and public utilities it initially supported a cics cobol db2 target environment however it now supports a wider range of relational databases and operating systems ief was intended to shield the developer from the complexities of building complete multi tier cross platform applications in 1995 texas instruments decided to change their marketing focus for the product part of this change included a new name composer by 1996 ief had become a popular tool however it was criticized by some it professionals for being too restrictive as well as for having a high per workstation cost 15k usd but it is claimed that ief reduces development time and costs by removing complexity and allowing rapid development of large scale enterprise transaction processing systems in 1997 composer had another change of branding texas instruments sold the texas instruments software division including the composer rights to sterling software sterling software changed the well known name information engineering facility to cool gen cool was an acronym for common object oriented language despite the fact that there was little object oriented programming object orientation in the product in 2000 sterling software was acquired by computer associates now ca ca has rebranded the product three times to date and the product is still used widely today under ca recent releases of the tool added support for the ca datacom db datacom dbms the linux operating system c code generation and asp net web clients the current version is known as ca gen version 8 being released in may 2010 with support for customised web services and more of the toolset being based around the eclipse software eclipse framework there are a variety of add on tools available for ca gen including project phoenix from jumar a collection of software tools and services focused on the modernisation and re platforming of existing legacy ca gen applications to new environments ref http www jumar solutions com jumar ref guardien a configuration management and developer productivity suite ref http www iet co uk iet ltd ref qat wizard ref http www qat com qat wizard asp qat wizard ref an interview style wizard that takes advantage of the meta model in gen products for multi platform application reporting and xml soap enabling of gen applications ref http www canamsoftware com canam software labs ref and developer productivity tools such as access gen apmconnect qa console and upgrade console from response systems ref http www response systems com response systems ref recently ca gen has released its latest version 8 5 references reflist external links http www uk capgemini com public sector tax welfare regenerate capgemini regenerate offering support update migrate http www edgeusergroup org edge user group the user group for ca gen http www edgeusergroup org wiki ca gen wiki sponsored by the edge user group http www gentalk biz gentalk biz ca gen blog inactive http www qat com qat global ca gen services and training provider usa http www iet co uk iet ca gen product and services provider uk http www jumar solutions com jumar solutions ca gen product and services provider uk http www response systems com response systems ca gen product and services provider uk http www facet com au facet consulting ca gen services provider australia http www canamsoftware com canam software labs inc ca gen product and service provider canada category computer aided software engineering tools category data management category ca technologies edited by sambit mishra'
b'pov commitment ordering date november 2011 global concurrency control typically pertains to the concurrency control of a system comprising several components each with its own concurrency control the overall concurrency control of the whole system the global concurrency control is determined by the concurrency control of its components modular programming module s in this case also the term modular concurrency control is used in many cases a system may be distributed over a communication network in this case we deal with distributed concurrency control of the system and the two terms sometimes overlap however distributed concurrency control typically relates to a case where the distributed system s components do not have each concurrency control of its own but rather are involved with a concurrency control mechanism that spans several components in order to operate for example as typical in a distributed database in database systems and transaction processing transaction management global concurrency control relates to the concurrency control of a multidatabase system for example a federated database other examples are grid computing and cloud computing environments it deals with the properties of the global schedule computer science schedule which is the unified schedule of the multidatabase system comprising all the individual schedules of the database system s and possibly other transactional object s in the system a major goal for global concurrency control is global serializability or modular serializability the problem of achieving global serializability in a heterogeneous environment had been open problem open for many years until an effective solution based on commitment ordering co has been proposed see global serializability global concurrency control deals also with global serializability relaxing global serializability relaxed forms of global serializability which compromise global serializability and in many applications also correctness and thus are avoided there while local to a database system serializability relaxing serializability relaxed serializability methods compromise serializability for performance gain utilized when the application allows it is unclear that the various proposed relaxed global serializability methods provide any performance gain over co which guarantees global serializability see also concurrency control global serializability commitment ordering distributed concurrency control category data management category distributed computing problems category databases category concurrency control category transaction processing'
b'image drm collaboration process jpg thumb 320px the drm collaboration process the data reference model drm is one of the five reference models of the federal enterprise architecture fea overview the drm is a framework whose primary purpose is to enable information sharing and reuse across the united states federal government via the standard description and discovery of common data and the promotion of uniform data management practices the drm describes artifacts which can be generated from the data architectures of federal government agencies the drm provides a flexible and standards based approach to accomplish its purpose the scope of the drm is broad as it may be applied within a single agency within a community of interest computer security community of interest coi 1 or cross coi data reference model topics drm structure the drm provides a standard means by which data may be described categorized and shared these are reflected within each of the drm s three standardization areas data description provides a means to uniformly describe data thereby supporting its discovery and sharing data context facilitates discovery of data through an approach to the categorization of data according to taxonomies additionally enables the definition of authoritative data assets within a coi data sharing supports the access and exchange of data where access consists of ad hoc requests such as a query of a data asset and exchange consists of fixed re occurring transactions between parties enabled by capabilities provided by both the data context and data description standardization areas drm version 2 the data reference model version 2 released in november 2005 is a 114 page document with detailed architectural diagrams and an extensive glossary of terms the drm also make many references to iso standards specifically the iso iec 11179 metadata registry standard drm usage the drm is not technically a published technical interoperability standard such as web services it is an excellent starting point for data architects within federal and state agencies any federal or state agencies that are involved with exchanging information with other agencies or that are involved in data warehouse ing efforts should use this document as a guide see also enterprise architecture framework enterprise application integration enterprise service bus federal enterprise architecture iso iec 11179 metadata publishing semantic spectrum semantic web synonym ring references reflist external links https web archive org web 20070617034325 http www defenselink mil cio nii docs dod drm v04 5aug pdf us department of defense data reference model http www whitehouse gov sites default files omb assets egov docs drm 2 0 final pdf us federal enterprise architecture program data reference model version 2 0 category computer data category data management category reference models'
b'unreferenced date march 2013 ontology merging defines the act of bringing together two conceptually divergent ontology computer science ontologies or the instance data associated to two ontologies this is similar to work in database merging schema matching this merging process can be performed in a number of ways manually semi automatically or automatically manual ontology merging although ideal is extremely labour intensive and current research attempts to find semi or entirely automated techniques to merge ontologies these techniques are statistically driven often taking into account similarity of concepts and raw similarity of instances through textual string metrics and semantic knowledge these techniques are similar to those used in information integration employing string metrics from open source similarity libraries see also ontology mapping data integration category ontology information science category data management'
b'refimprove date august 2007 the dynamic knowledge repository dkr is a concept developed by douglas c engelbart as a primary strategic focus for allowing humans to address complex problems when date september 2011 doug has proposed that a dkr will enable us to develop a collective iq greater than any individual s iq references and discussion of engelbart s dkr concept are available at the doug engelbart institute ref cite web url http www dougengelbart org about dkrs html title about dynamic knowledge repositories an introduction accessdate september 15 2011 author christina engelbart ref definition a knowledge repository is a computerized system that systematically captures organizes and categorizes an organization s knowledge the repository can be searched and data can be quickly retrieved the effective knowledge repositories include factual conceptual procedural and meta cognitive techniques the key features of knowledge repositories include communication forums a knowledge repository can take many forms to contain the knowledge it holds a customer database is a knowledge repository of customer information and insights or electronic explicit knowledge a library is a knowledge repository of books physical explicit knowledge a community of experts is a knowledge repository of tacit knowledge or experience the nature of the repository only changes to contain manage the type of knowledge it holds a repository as opposed to an archive is designed to get knowledge out it should therefore have some rules of structure classification taxonomy record management etc to facilitate user engagement references reflist 2 external links http dougengelbart org doug engelbart institute category knowledge representation category data management compu storage stub'
b'pov date october 2012 infobox software name tagsistant logo file tagsistant logo png 300px developer tx0 tx0 strumentiresistenti org latest release version 0 6 frequently updated yes release version update don t edit this page just click on the version number programming language c programming language c operating system linux kernel language english genre semantic file system license gnu general public license gnu gpl website http www tagsistant net infobox filesystem name tagsistant developer tx0 full name introduction date introduction os partition id directory struct file struct bad blocks struct max file size max files no max filename size max volume size dates recorded date range date resolution forks streams attributes file system permissions compression encryption os tagsistant is a semantic file system for the linux kernel written in c programming language c and based on filesystem in userspace fuse unlike traditional file systems that use hierarchies of directories to locate objects tagsistant introduces the concept of tag metadata tags design and differences with hierarchical file systems in computing a file system is a type of data store which could be used to store retrieve and update computer file files each file can be uniquely located by its path computing path the user must know the path in advance to access a file and the path does not necessarily include any information about the content of the file tagsistant uses a complementary approach based on tag metadata tags the user can create a set of tags and apply those tags to files file directory directories and other objects device file devices named pipe pipes the user can then search all the objects that match a subset of tags called a query this kind of approach is well suited for managing user contents like pictures audio recordings movies and text documents but is incompatible with system files like libraries commands and configurations where the univocity of the path is a computer security security requirement to prevent the access to a wrong content the tags directory a tagsistant file system features four main directories archive relations stats tags tags are created as sub directories of the code tags code directory and can be used in queries complying to this syntax code tags subquery subquery subquery code ref cite web title tags and relations directories url http www tagsistant net documents about tagsistant 0 6 howto showall start 3 ref where a subquery is an arbitrarily long list of tags concatenated as directories code tag1 tag2 tag3 tagn code the portion of a path delimited by code tags code and code code is the actual query the code code operator joins the results of different sub queries in one single list the code code operator ends the query to be returned as a result of the following query code tags t1 t2 t1 t4 code an object must be tagged as both code t1 code and code t2 code or as both code t1 code and code t4 code any object tagged as code t2 code or code t4 code but not as code t1 code will not be retrieved the query syntax deliberately violates the posix file system semantics by allowing a path token to be a descendant of itself like in code tags t1 t2 t1 t4 code where code t1 code appears twice as a consequence a recursive scan of a tagsistant file system will exit with an error or endlessly loop as done by unix code find find code syntaxhighlight lang bash tagsistant mountpoint find tags tags tags document tags document tags document document tags document document tags document document document tags document document document syntaxhighlight this drawback is balanced by the possibility to list the tags inside a query in any order the query code tags t1 t2 code is completely equivalent to code tags t2 t1 code and code tags t1 t2 t3 code is equivalent to code tags t2 t3 t1 code the code code element has the precise purpose of restoring the posix semantics the path code tags t1 directory code refers to a traditional directory and a recursive scan of this path will properly perform the reasoner and the relations directory tagsistant features a simple semantic reasoner reasoner which expands the results of a query by including objects tagged with related tags a relation between two tags can be established inside the code relations code directory following a three level pattern code relations tag1 rel tag2 code the code rel code element can be includes or is equivalent to include the rock tag in the music tag the unix command code mkdir code can be used code mkdir p relations music includes rock code the reasoner can recursively resolve relations allowing the creation of complex structures code mkdir p relations music includes rock code code mkdir p relations rock includes hard rock code code mkdir p relations rock includes grunge code code mkdir p relations rock includes heavy metal code code mkdir p relations heavy metal includes speed metal code the web of relations created inside the code relations code directory constitutes a basic form of ontology information science ontology autotagging plugins tagsistant features an autotagging plugin stack which gets called when a file or a symlink is written ref cite web title how to write a plugin for tagsistant url http www tagsistant net documents about tagsistant coding and debugging 7 how to write a plugin for tagsistant ref each plugin is called if its declared mime type mime type matches the list of working plugins released with tagsistant 0 6 is limited to text html tags the file with each word in code title code and code keywords code elements and with document webpage and html too image jpeg tags the file with each exchangeable image file format exif tag the repository each tagsistant file system has a corresponding repository containing an code archive code directory where the objects are actually saved and a code tags sql code file holding tagging information as an sqlite database if the mysql database engine was specified with the code db code argument the code tags sql code file will be empty another file named code repository ini code is a glib ini store with the repository configuration ref cite web title key value file parser url https developer gnome org glib 2 32 glib key value file parser html ref tagsistant 0 6 is compatible with the mysql and sqlite dialects of sql for tag reasoning and tagging resolution while porting its logic to other sql dialects is possible differences in basic constructs especially the intersect sql keyword must be considered the archive and stats directories the code archive code directory has been introduced to provide a quick way to access objects without using tags objects are listed with their inode number prefixed ref cite web title tagsistant 0 6 howto inodes url http www tagsistant net documents about tagsistant 0 6 howto showall start 6 ref the code stats code directory features some read only files containing usage statistics a file code configuration code holds both compile time information and current repository configuration main criticisms it has been highlighted that relying on an external database to store tags and tagging information could cause the complete loss of metadata if the database gets corrupted ref cite web title extended attributes and tag file systems url http www lesbonscomptes com pages tagfs html ref it has been highlighted that using a flat namespace tends to overcrowd the code tags code directory ref cite web title the major problem with this approach is scalability publisher https news ycombinator com item id 2573318 ref this could be mitigated introducing tag metadata triple tags triple tags see also portal free software semantic file system references reflist external links official website http www tagsistant net https aur archlinux org packages php id 54644 arch linux package https news ycombinator com item id 2573318 discussion on hacker news http www lesbonscomptes com pages tagfs html extended attributes and tag file systems http lakm us logit 2010 03 tagsistant on production 2 tagsistant on production category computer file systems category data management category semantic file systems'
b'governance data governance is a control management control that ensures that the data entry by an operations team member or by automated processes meets precise standards such as a business rule a data definition and data integrity constraints in the data model the data governor uses data quality monitoring against production data to communicate errors in data back to operational team members or to the technical support team for corrective action data governance is used by organizations to exercise control over processes and methods used by their data stewards and data custodian s in order to improve data quality data governance is a set of processes that ensures that important data assets are formally managed throughout the enterprise data governance ensures that data can be trusted and that people can be made accountable for any adverse event that happens because of low data quality it is about putting people in charge of fixing and preventing issues with data so that the enterprise can become more efficient data governance also describes an evolutionary process for a company altering the company s way of thinking and setting up the processes to handle information so that it may be utilized by the entire organization it s about using technology when necessary in many forms to help aid the process when companies desire or are required to gain control of their data they empower their people set up processes and get help from technology to do it ref name sarsfield sarsfield steve 2009 the data governance imperative it governance ref according to one vendor data governance is a quality control discipline for assessing managing using improving monitoring maintaining and protecting organizational information it is a system of decision rights and accountabilities for information related processes executed according to agreed upon models which describe who can take what actions with what information and when under what circumstances using what methods ref name the dgi data governance framework cite web url http www datagovernance com wp content uploads 2014 11 dgi framework pdf title the dgi data governance framework ref overview data governance encompasses the people processes and information technology required to create a consistent and proper handling of an organization s data across the business enterprise goals may be defined at all levels of the enterprise and doing so may aid in acceptance of processes by those who will use them some goals include increasing consistency and confidence in decision making decreasing the risk of regulatory fines improving information security data security also defining and verifying the requirements for data distribution policies ref gianni d 2015 jan data policy definition and verification for system of systems governance in modeling and simulation support for system of systems engineering http onlinelibrary wiley com doi 10 1002 9781118501757 ch5 summary ref maximizing the income generation potential of data designating accountability for information quality enable better planning by supervisory staff minimizing or eliminating re work optimize staff effectiveness establish process performance baselines to enable improvement efforts acknowledge and hold all gain these goals are realized by the implementation of data governance programs or initiatives using change management techniques data governance drivers while data governance initiatives can be driven by a desire to improve data quality they are more often driven by c level leaders responding to external regulations examples of these regulations include sarbanes oxley basel i basel ii hipaa general data protection regulation gdpr and a number of data privacy regulations to achieve compliance with these regulations business processes and controls require formal management processes to govern the data subject to these regulations ref http www rimes com rimes data governance handbook rimes data governance handbook rimes ref successful programs identify drivers meaningful to both supervisory and executive leadership common themes among the external regulations center on the need to manage risk the risks can be financial misstatement inadvertent release of sensitive data or poor data quality for key decisions methods to manage these risks vary from industry to industry examples of commonly referenced best practices and guidelines include cobit iso iec 38500 and others the proliferation of regulations and standards creates challenges for data governance professionals particularly when multiple regulations overlap the data being managed organizations often launch data governance initiatives to address these challenges data governance initiatives dimensions data governance initiatives improve data quality by assigning a team responsible for data s accuracy accessibility consistency and completeness among other metrics this team usually consists of executive leadership project management line function line of business managers and data steward s the team usually employs some form of methodology for tracking and improving enterprise data such as six sigma and tools for data mapping data profiling profiling cleansing and monitoring data data governance initiatives may be aimed at achieving a number of objectives including offering better visibility to internal and external customers such as supply chain management compliance with compliance regulation regulatory law improving operations after rapid company growth or mergers and acquisitions corporate mergers or to aid the efficiency of enterprise knowledge worker s by reducing confusion and error and increasing their scope of knowledge many data governance initiatives are also inspired by past attempts to fix information quality at the departmental level leading to incongruent and redundant data quality processes most large companies have many applications and databases that can t easily share information therefore knowledge workers within large organizations often don t have access to the information they need to best do their jobs when they do have access to the data the data quality may be poor by setting up a data governance practice or corporate data corporate data authority these problems can be mitigated the structure of a data governance initiative will vary not only with the size of the organization but with the desired objectives or the focus areas ref name focus areas cite web url http datagovernance com fc focus areas for data governance html title data governance focus areas deadurl yes archiveurl https web archive org web 20081006152845 http www datagovernance com fc focus areas for data governance html archivedate 2008 10 06 df ref of the effort implementation implementation of a data governance initiative may vary in scope as well as origin sometimes an executive mandate will arise to initiate an enterprise wide effort sometimes the mandate will be to create a pilot project or projects limited in scope and objectives aimed at either resolving existing issues or demonstrating value sometimes an initiative will originate lower down in the organization s hierarchy and will be deployed in a limited scope to demonstrate value to potential sponsors higher up in the organization the initial scope of an implementation can vary greatly as well from review of a one off it system to a cross organization initiative data governance tools leaders of successful data governance programs declared in december 2006 at the data governance conference in orlando fl that data governance is between 80 and 95 percent communication ref cite web url http www dmreview com issues 2007 48 10001356 1 html title data governance one size does not fit all last hopwood first peter authorlink peter hopwood publisher dm review magazine date june 2008 accessdate 2008 10 02 archiveurl http www webcitation org 5bghaz1ga url http www dmreview com issues 2007 48 10001356 1 html archivedate 2008 10 02 quote at the inaugural data governance conference in orlando florida in december 2006 leaders of successful data governance programs declared that in their experience data governance is between 80 and 95 percent communication clearly data governance is not a typical it project deadurl yes df ref that stated it is a given that many of the objectives of a data governance program must be accomplished with appropriate tools many vendors are now positioning their products as data governance tools due to the different focus areas of various data governance initiatives any given tool may or may not be appropriate in addition many tools that are not marketed as governance tools address governance needs ref cite web url http www datagovernancesoftware com title datagovernancesoftware com publisher the data governance institute accessdate 2008 10 02 archiveurl http www webcitation org 5bgi3dfhv url http www datagovernancesoftware com archivedate 2008 10 02 quote deadurl yes df ref data governance organizations dama international ref http www dama org i4a pages index cfm pageid 1 dama international ref dama the data management association is a not for profit vendor independent international association of technical and business professionals dedicated to advancing the concepts and practices of information resource management irm and data resource management drm data governance professionals organization dgpo ref http www dgpo org data governance professionals organization bot generated title ref the data governance professionals organization dgpo is a non profit vendor neutral association of business it and data professionals dedicated to advancing the discipline of data governance the objective of the dgpo is to provide a forum that fosters discussion and networking for members and to encourage develop and advance the skills of members working in the data governance discipline the data governance society ref http www datagovernancesociety org data governance society ref the data governance society inc is dedicated to fostering a new paradigm for the effective use and protection of information in which data is governed and leveraged as a unique corporate asset the data governance council ref https www 935 ibm com services uk cio pdf leverage wp data gov council maturity model pdf data governance council ref the data governance council is an organization formed by ibm consisting of companies institutions and technology solution providers with the stated objective to build consistency and quality control in governance which will help companies better protect critical data iq international the international association for information and data quality ref http iaidq org iq international the international association for information and data quality ref iq international is a not for profit vendor neutral professional association formed in 2004 dedicated to building the information and data quality profession data governance conferences a number of major conferences relevant to data governance are held annually data governance and information quality conference ref http dgiq conference com data governance and information quality conference bot generated title ref commercial conferences held each year in the usa data governance conference europe ref http www irmuk co uk data governance conference europe bot generated title ref commercial conferences held annually in london england information and data quality conference ref http idq conference com information and data quality conference ref not for profit conference run by iq international in the usa master data management data governance conferences ref http www tcdii com events cdimdmsummitseries html mdm summit conference bot generated title ref six major conferences are run annually by the mdm institute in london san francisco sydney toronto madrid frankfurt and new york city financial information summit series of conferences ref http www financialinformationsummit com bot generated title ref hosted by inside reference data magazine in new york london hong kong toronto chicago frankfurt paris and tokyo see also information architecture information technology governance semantics of business vocabulary and business rules master data management cobit iso iec 38500 iso tc 215 operational risk management basel ii accord hipaa sarbanes oxley act information technology controls data protection directive eu universal data element framework asset description metadata schema references to cite a web resource use this template ref cite web url mandatory title mandatory last first authorlink coauthors work publisher date format language doi accessdate archiveurl should be used on pages allowing archiving use a service like webcitation org or archive org archivedate mandatory if archiveurl quote ref reflist category information technology governance category data management'
b'client side persistent data or cspd is a term used in computing for storing data required by web application web applications to complete internet tasks on the client side as needed rather than exclusively on the server computing server as a framework it is one solution to the needs of occasionally connected computing or occ a major challenge for http as a stateless server stateless protocol computing protocol has been asynchronous tasks the ajax programming ajax pattern using xmlhttprequest was first introduced by microsoft in the context of the outlook web app outlook e mail product the first cspd were the http cookie cookies introduced by the netscape netscape web browser navigator activex components which have entries in the windows registry can also be viewed as a form of client side persistence computer science persistence see also occasionally connected computing curl programming language ajax programming ajax http web storage external links http www curl com developer faq cspd cspd http safari ciscopress com 0596101996 jscript5 chp 19 sect 6 safari preview http wp netscape com newsref std cookie spec html netscape on persistent client state category clients computing category data management category web applications'
b'multiple issues advert date march 2011 update inaccurate yes date april 2010 microsoft sql server master data services is a master data management mdm product from microsoft that ships as a part of the microsoft sql server relational database management system ref https msdn microsoft com en us library ms130214 aspx ref master data services mds is the sql server solution for master data management master data management mdm enables your organization to discover and define non transactional lists of data and compile maintainable reliable master lists master data services first shipped with microsoft sql server 2008 r2 microsoft sql server 2016 includes many enhancements to master data services such as improved performance and security and the ability to clear transaction logs create custom indexes share entity data between different models and support for many to many relationships for more information see https msdn microsoft com en us library ff929136 aspx what s new in master data services mds overview in master data services the model is the highest level container in the structure of your master data you create a model to manage groups of similar data a model contains one or more entities and entities contain members that are the data records an entity is similar to a table like other mdm products master data services aims to create a centralized data source and keep it synchronized and thus reduce redundancies across the applications which process the data cn date january 2015 sharing the architectural core with stratature edm master data services uses a microsoft sql server database as the physical data store it is a part of the master data hub which uses the database to store and manage data entity data model entities cn date january 2015 it is a database with the software to validate and manage the data and keep it synchronized with the systems that use the data ref name arch cite web url http msdn2 microsoft com en us library bb410798 aspx title master data management mdm hub architecture author roger walter publisher msdn technet accessdate 2007 09 25 ref the master data hub has to extract the data from the source system validate sanitize and shape the data remove duplicates and update the hub repositories as well as synchronize the external sources ref name arch the entity schemas attributes data hierarchies validation rules and access control information are specified as metadata to the master data services runtime master data services does not impose any limitation on the data model master data services also allows custom business rules used for validating and sanitizing the data entering the data hub to be defined which is then run against the data matching the specified criteria all changes made to the data are validated against the rules and a log of the transaction is stored persistently violations are logged separately and optionally the owner is notified automatically all the data entities can be revision control system versioned cn date january 2015 master data services allows the master data to be categorized by hierarchical relationships such as employee data are a subtype of organization data hierarchies are generated by relating data attributes data can be automatically categorized using rules and the categories are introspected programmatically master data services can also expose the data as microsoft sql server view database views which can be pulled by any sql compatible client it uses a role based access control system to restrict access to the data the views are generated dynamically so they contain the latest data entities in the master hub it can also push out the data by writing to some external journals master data services also includes a web based ui for viewing and managing the data it uses ajax in the front end and asp net in the back end cn date january 2015 master data services also includes certain features not available in the stratature edm product it gains a web service interface to expose the data as well as an api which internally uses the exposed web services exposing the feature set programmatically to access and manipulate the data it also integrates with active directory for authentication purposes unlike edm master data services supports unicode characters as well as support multilingual user interfaces cn date january 2015 there has been a significant http www faceofit com why is sql server 2016 is faster than ever performance increase in master data services in sql server 2016 as well as the excel add in ref http www faceofit com why is sql server 2016 is faster than ever ref terminology model is the highest level of an mds instance it is the primary container for specific groupings of master data in many ways it is very similar to the idea of a database entities are containers created within a model entities provide a home for members and are in many ways analogous to database tables e g customer members are analogous to the records in a database table entity e g will smith members are contained within entities each member is made up of two or more attributes attributes are analogous to the columns within a database table entity e g surname attributes exist within entities and help describe members the records within the table name and code attributes are created by default for each entity and serve to describe and uniquely identify leaf members attributes can be related to other attributes from other entities which are called domain based attributes this is similar to the concept of a foreign key other attributes however will be of type free form most common or file attribute groups are explicitly defined collections of particular attributes say you have an entity customer that has 50 attributes mdash too much information for many of your users attribute groups enable the creation of custom sets of hand picked attributes that are relevant for specific audiences e g customer delivery details that would include just their name and last known delivery address this is very similar to a database view hierarchies organize members into either derived or explicit hierarchical structures derived hierarchies as the name suggests are derived by the mds engine based on the relationships that exist between attributes explicit hierarchies are created by hand using both leaf and consolidated members business rules can be created and applied against model data to ensure that custom business logic is adhered to in order to be committed into the system data must pass all business rule validations applied to them e g within the customer entity you may want to create a business rule that ensures all members of the country attribute contain either the text usa or canada the business rule once created and ran will then verify all the data is correct before it accepts it into the approved model versions provide system owners administrators with the ability to open lock or commit a particular version of a model and the data contained within it at a particular point in time as the content within a model varies grows or shrinks over time versions provide a way of managing metadata so that subscribing systems can access to the correct content references reflist external links https msdn microsoft com en us library ee633763 aspx microsoft sql server 2016 master data services category data management category microsoft software sql server master data services category 2010 software'
b'ontology based data integration involves the use of ontology computer science ontology s to effectively combine data or information from multiple heterogeneous sources ref name wache cite conference author1 h wache author2 t v\xc3\xb6gele author3 u visser author4 h stuckenschmidt author5 g schuster author6 h neumann author7 s h\xc3\xbcbner title ontology based integration of information a survey of existing approaches year 2001 citeseerx 10 1 1 142 4390 ref it is one of the multiple data integration approaches and may be classified as global as view gav ref name refone cite conference author maurizio lenzerini title data integration a theoretical perspective year 2002 pages 243 246 url http www dis uniroma1 it lenzerin homepagine talks tutorialpods02 pdf ref the effectiveness of ontology based data integration is closely tied to the consistency and expressivity of the ontology used in the integration process background data from multiple sources are characterized by multiple types of heterogeneity the following hierarchy is often used ref name sheth cite book author a p sheth title changing focus on interoperability in information systems from system syntax structure to semantics booktitle interoperating geographic information systems m f goodchild m j egenhofer r fegeas and c a kottman eds kluwer academic publishers year 1999 pages 5 30 url http lsdis cs uga edu library download s98 changing pdf ref ref http daks ucdavis edu ludaesch paper ahm02 tutorial5 html ahm02 tutorial 5 data integration and mediation contributors b ludaescher i altintas a gupta m martone r marciano x qian ref syntactic heterogeneity is a result of differences in representation format of data schematic or structural heterogeneity the native model or structure to store data differ in data sources leading to structural heterogeneity schematic heterogeneity that particularly appears in structured databases is also an aspect of structural heterogeneity ref name sheth semantic heterogeneity differences in interpretation of the meaning of data are source of semantic heterogeneity system heterogeneity use of different operating system hardware platforms lead to system heterogeneity ontology computer science ontologies as formal models of representation with explicitly defined concepts and named relationships linking them are used to address the issue of semantic heterogeneity in data sources in domains like bioinformatics and biomedicine the rapid development adoption and public availability of ontologies http www bioontology org repositories html obo has made it possible for the data integration community to leverage them for semantic integration of data and information the role of ontologies ontologies enable the unambiguous identification of entities in heterogeneous information systems and assertion of applicable named relationships that connect these entities together specifically ontologies play the following roles content explication ref name wache the ontology enables accurate interpretation of data from multiple sources through the explicit definition of terms and relationships in the ontology query model ref name wache in some systems like sims ref name arens cite conference author1 y arens author2 c hsu author3 c a knoblock title query processing in sims information mediator year 1996 url http www isi edu integration papers arens98 agents pdf ref the query is formulated using the ontology as a global query schema verification ref name wache the ontology verifies the mappings used to integrate data from multiple sources these mappings may either be user specified or generated by a system approaches using ontologies for data integration there are three main architectures that are implemented in ontology based data integration applications ref name wache namely single ontology approach a single ontology is used as a global reference model in the system this is the simplest approach as it can be simulated by other approaches ref name wache sims ref name arens a prominent example of this approach the structured knowledge source integration component of cyc research cyc is another prominent example of this approach ref http www cyc com content semantic knowledge source integration ref ref http www aaai org ojs index php aimagazine article viewarticle 2299 ref title harnessing cyc to answer clinical researchers ad hoc queries multiple ontologies multiple ontologies each modeling an individual data source are used in combination for integration though this approach is more flexible than the single ontology approach it requires creation of mappings between the multiple ontologies ontology mapping is a challenging issue and is focus of large number of research efforts in computer science http www ontologymatching org the observer system ref name mena cite conference author1 e mena author2 v kashyap author3 a sheth author4 a illarramendi title observer an approach for query processing in global information systems based on interoperation across pre existing ontologies year 1996 url http dit unitn it p2p relatedwork matching mksi96 pdf ref is an example of this approach hybrid approaches the hybrid approach involves the use of multiple ontologies that subscribe to a common top level vocabulary ref name goh cite conference author cheng hian goh title representing and reasoning about semantic conflicts in heterogeneous information systems year 1997 url http context2 mit edu coin publications goh thesis goh thesis pdf ref the top level vocabulary defines the basic terms of the domain thus the hybrid approach makes it easier to use multiple ontologies for integration in presence of the common vocabulary see also data mapping enterprise application integration enterprise information integration ontology mapping schema matching references references external links http sid cps unizar es observer observer home page http www cyc com content semantic knowledge source integration cyc semantic knowledge source integration sksi category ontology information science category data management'
b'pov commitment ordering date november 2011 distributed concurrency control is the concurrency control of a system distributed computing distributed over a computer network bern87 bernstein et al 1987 weikum01 weikum and vossen 2001 in database systems and transaction processing transaction management distributed concurrency control refers primarily to the concurrency control of a distributed database it also refers to the concurrency control in a multidatabase and other multi transactional object environment e g federated database grid computing and cloud computing environments a major goal for distributed concurrency control is distributed serializability or global serializability for multidatabase systems distributed concurrency control poses special challenges beyond centralized one primarily due to communication and computer latency engineering latency it often requires special techniques like distributed lock manager over fast computer network s with low latency like switched fabric e g infiniband commitment ordering or commit ordering is a general serializability technique that achieves distributed serializability and global serializability in particular effectively on a large scale without concurrency control information distribution e g local precedence relations locks timestamps or tickets and thus without performance penalties that are typical to other serializability techniques raz92 raz 1992 the most common distributed concurrency control technique is strong strict two phase locking two phase locking strong strict two phase locking ss2pl also named rigorousness which is also a common centralized concurrency control technique ss2pl provides both the serializability schedule computer science strict strictness and commitment ordering properties strictness a special case of recoverability is utilized for effective recovery from failure and commitment ordering allows participating in a general solution for global serializability for large scale distribution and complex transactions distributed locking s typical heavy performance penalty due to delays latency can be saved by using the atomic commitment protocol which is needed in a distributed database for distributed transactions atomicity database systems atomicity e g two phase commit protocol two phase commit or a simpler one in a reliable system together with some local commitment ordering variant e g local two phase locking strong strict two phase locking ss2pl instead of distributed locking to achieve global serializability in the entire system all the commitment ordering theoretical results are applicable whenever atomic commitment is utilized over partitioned distributed recoverable transactional data including automatic distributed deadlock resolution such technique can be utilized also for a large scale parallel database where a single large database residing on many nodes and using a distributed lock manager is replaced with a homogeneous multidatabase comprising many relatively small databases loosely defined any process that supports transactions over partitioned data and participates in atomic commitment complies fitting each into a single node and using commitment ordering e g ss2pl strict co together with some appropriate atomic commitment protocol without using a distributed lock manager see also global concurrency control references cite id bern87 phil bernstein philip a bernstein vassos hadzilacos nathan goodman 1987 http research microsoft com en us people philbe ccontrol aspx concurrency control and recovery in database systems addison wesley publishing company 1987 isbn 0 201 10715 5 cite cite id weikum01 gerhard weikum gottfried vossen 2001 http www elsevier com wps find bookdescription cws home 677937 description description transactional information systems elsevier isbn 1 55860 508 8 cite cite id raz92 yoav raz 1992 http www informatik uni trier de ley db conf vldb raz92 html the principle of commitment ordering or guaranteeing serializability in a heterogeneous environment of multiple autonomous resource managers using atomic commitment proceedings of the eighteenth international conference on very large data bases vldb pp 292 312 vancouver canada august 1992 also dec tr 841 digital equipment corporation november 1990 cite category data management category distributed computing problems category databases category concurrency control category transaction processing'
b'multiple issues more footnotes date august 2013 refimprove date august 2013 infobox software name microsoft office performancepoint server developer microsoft released start date 2007 11 latest release version 1 0 sp2 latest release date 2008 operating system microsoft windows genre enterprise performance management license proprietary software proprietary eula website https web archive org web 20071016055516 http www microsoft com business performancepoint www microsoft com business performancepoint microsoft office performancepoint server is a business intelligence computer software software product released in 2007 by microsoft although discontinued in 2009 the dashboard scorecard and analytics capabilities of performancepoint server were incorporated into sharepoint 2010 sharepoint 2010 and later versions performancepoint server also provided a planning and budgeting component directly integrated with excel history microsoft offered preview releases of performancepoint server starting in mid 2006 previews of the product were formed from business scorecard manager 2005 and the planning server component acquisitions proclarity corporation proclarity and great plains software great plains brought additional analytics and planning reporting capabilities as well as companion products proclarity 6 3 and microsoft frx frx performancepoint server was officially released in november 2007 microsoft discontinued performancepoint server as an independent product in 2009 and folded its dashboard scorecard and analytics capabilities into performancepoint services in sharepoint server 2010 ref cite web url http www informationweek com news business intelligence analytics showarticle jhtml articleid 212902915 subsection business intelligence title microsoft makes sweeping changes to bi software strategy date january 27 2009 last weier first mary hayes work informationweek ref monitoring server component business monitoring capabilities including dashboards scorecards key performance indicators navigable reports for deeper analysis strategy maps and linked filtering are provided by performancepoint s monitoring server component a dashboard designer application that is distributed from monitoring server enables business analysts or it administrators to create test data source connections create views that use those data connections assemble the views into a dashboard deploy the dashboard as a sharepoint page dashboard designer saved content and security information back to the monitoring server data source connections such as olap cubes or relational tables were also made through monitoring server after a dashboard has been published to the monitoring server database it would be deployed as a sharepoint page and shared with other users as such when the pages were opened in a web browser monitoring server updated the data in the views by connecting back to the original data sources planning server component performancepoint s planning server component supported maintenance of logical business models budget approval workflows enterprise data sources and it followed generally accepted accounting principles planning server made use of excel for input and line of business reporting as well as sql server for storing and processing business models management reporter component the management reporter component was designed to perform financial reporting and can read performancepoint planning models directly a development kit was also available to allow this component to read other models which date august 2013 references reflist external links http msdn2 microsoft com en us office bb660518 aspx performancepoint server 2007 developer portal http blogs technet com datapuzzle data puzzle http performancepointinsider com blogs default aspx performancepoint insider http alanwhitehouse wordpress com 2009 01 26 pps planning being discontinued performance point planning being discontinued microsoft office category microsoft office servers category business intelligence category data management'
b'xldb refers to extremely large database data bases the definition of extremely large refers to data sets that are too big in terms of volume too much and or velocity too fast and of variety too many places too many formats to be handled using conventional solutions history in october 2007 the xldb experts gathered at slac for the https web archive org web 20080417002612 http www conf slac stanford edu xldb07 first workshop on extremely large databases as a result the xldb research community was formed to meet rapidly growing demands in addition to the original invitational workshop an open conference tutorials and annual satellite events on different continents were added the main event held annually at stanford gathers over 300 technically savvy attendees xldb is one of the premier database events catered towards both academic and industrial communities goals the main goals of this community include ref cite web url http www conf slac stanford edu xldb09 docs xldb09 welcometalk ppt year 2009 last becla first jacek title xldb 3 welcome accessdate 2009 08 29 ref identify trends commonalities and major roadblocks related to building extremely large databases bridge the gap between users trying to build extremely large databases and database solution providers worldwide facilitate development and growth of practical technologies for extremely large data stores xldb community as of 2013 the community consisted of about a thousand members including scientists who develop use or plan to develop or use xldb for their research from laboratories commercial users of xldb providers of database products including commercial vendors and representatives from open source database communities academic database researchers xldb conferences workshops and tutorials the community meets annually at stanford where the main event is held each fall usually in september these who live too far from california to attend have the opportunity to attend satellite events organized annually around may june either in asia or in europe a detailed report is produced after each workshop class wikitable year place link report comments 2015 stanford https web archive org web 20150521105100 http www conf slac stanford edu xldb2015 8th xldb conference 2014 http www on br observat\xc3\xb3rio nacional rio de janeiro https web archive org web 20150219081443 http xldb rio2014 linea gov br satellite xldb workshop in south america 2014 stony brook university https web archive org web 20150521052839 http www3 cs stonybrook edu xldb xldb healthcare workshop 2013 stanford https conf slac stanford edu xldb 2013 7th xldb conference 2013 cern geneva switzerland http xldb europe workshop 2013 web cern ch dead link date july 2016 bot internetarchivebot fix attempted yes satellite xldb workshop in europe 2012 stanford http www conf slac stanford edu xldb2012 http www jstage jst go jp article dsj 12 0 12 12 023 pdf 6th xldb conference workshop tutorials 2012 beijing china https web archive org web 20120708164351 http idke ruc edu cn xldb www xldb asia org home html http www xldb org wp content uploads 2012 09 xldbasia2012report pdf satellite xldb conference in asia 2011 slac https web archive org web 20110426125951 http www conf slac stanford edu xldb2011 http www jstage jst go jp article dsj 11 0 37 pdf dead link date july 2016 bot internetarchivebot fix attempted yes 5th xldb conference and workshop 2011 edinburgh uk https web archive org web 20160303221547 http xldb eu xldb europe 2011 not available satellite xldb workshop in europe 2010 slac https web archive org web 20110727234052 http www conf slac stanford edu xldb2010 http www jstage jst go jp article dsj 9 0 9 mr1 article dead link date july 2016 bot internetarchivebot fix attempted yes 4th xldb conference and workshop 2009 lyon france https web archive org web 20110727234623 http www conf slac stanford edu xldb2009 http www jstage jst go jp article dsj 8 0 mr1 pdf dead link date july 2016 bot internetarchivebot fix attempted yes 3rd xldb workshop 2008 slac https web archive org web 20110727234818 http www conf slac stanford edu xldb2008 http www jstage jst go jp article dsj 7 0 196 pdf dead link date july 2016 bot internetarchivebot fix attempted yes 2nd xldb workshop 2007 slac https web archive org web 20110727235121 http www conf slac stanford edu xldb2007 http www jstage jst go jp article dsj 7 0 1 pdf dead link date july 2016 bot internetarchivebot fix attempted yes 1st xldb workshop tangible results the xldb events led to initiating the effort of building a new open source science database https web archive org web 20090220121225 http scidb org scidb ref cite web url http www jstage jst go jp article dsj 7 0 88 pdf year 2008 last becla first jacek title report from the scidb workshop accessdate 2008 09 29 dead link date july 2016 bot internetarchivebot fix attempted yes ref the xldb organizers started defining a http www xldb org science benchmark science benchmark for scientific data management systems called ss db at http xldb org 2012 xldb 2012 dead link date july 2016 bot internetarchivebot fix attempted yes the xldb organizers announced that two major databases that support arrays as first class objects monetdb sciql and scidb have formed a working group in conjunction with xldb this working group is proposing a common syntax provisionally named arrayql for manipulating arrays including array creation and query references reflist further reading pavlo a paulson e rasin a abadi d j dewitt d j madden s and stonebraker m a comparison of approaches to large scale data analysis proceedings of the 2009 acm sigmod http web archive org web 20090611174944 http database cs brown edu 80 sigmod09 benchmarks sigmod09 pdf becla j et al 2006 designing a multi petabyte database for lsst http arxiv org abs cs 0604112 becla j wang d l 2005 lessons learned from managing a petabyte downloaded from http web archive org web 20110604223735 http www slac stanford edu pubs slacpubs 10750 slac pub 10963 pdf on 2007 11 25 bell g gray j szalay a 2005 petascale computations systems balanced cyberinfrastructure in a data centric world http arxiv org abs cs 0701165 duellmann d 1999 petabyte databases acm sigmod record vol 28 p 506 http web archive org web 20071012015357 http www sigmod org sigmod record issues 9906 index html tutorialsessions hanushevsky a nowak m 1999 pursuit of a scalable high performance multi petabyte database 16th ieee symposium on mass storage systems pp 169 175 http citeseer ist psu edu 217883 html shiers j building very large distributed object databases downloaded from http web archive org web 20070915101842 http wwwasd web cern ch 80 wwwasd cernlib rd45 papers dbprog html on 2007 11 25 category types of databases category data management'
b'citations missing date december 2007 core data integration is the use of data integration technology for a significant centrally planned and managed it initiative within a company examples of core data integration initiatives could include etl extract transform load implementations eai enterprise application integration implementations soa service oriented architecture implementations esb enterprise service bus implementations core data integrations are often designed to be enterprise wide integration solutions they may be designed to provide a data abstraction layer which in turn will be used by individual core data integration implementations such as etl servers or applications integrated through eai because it is difficult to promptly roll out a centrally managed data integration solution that anticipates and meets all data integration requirements across an organization it engineers and even business users create edge data integration using technology that may be incompatible with that used at the core in contrast to a core data integration an edge data integration is not centrally planned and is generally completed with a smaller budget and a tighter deadline see also data integration edge data integration references http searchsoa techtarget com tip 0 289483 sid26 gci1171085 00 html category data management'
b'advert article date may 2013 image virtualfacility jpg thumb 250px a virtual facility snapshot created with 6sigmadc software right a virtual facility vf is a highly realistic digital representation of a data center primarily the term virtual in virtual facility refers to the use of the word as in virtual reality virtual reality rather than the abstraction of computer resources as in platform virtualization the vf mirrors the characteristics of the physical facility over time and allows modeling all relevant characteristics of a physical data center with a high degree of precision vf model includes three dimensional physical facility layout network connectivity of facility equipment full inventory of facility equipment including electronics and electrical systems such as power distribution unit power distribution units pdu s and uninterruptible power supplies uninterruptible power supplies ups s full air conditioning system acu s and controls within the room the term virtual facility was introduced by future facilities a data centre design consultancy focused on delivering design and operational solutions to address the emerging environmental problems facing the modern mission critical facility mcf the concept is in essence a convergence of the fields of virtual reality virtual reality vr computer simulation computer simulation and expert systems expert systems applied to the specific domain of facilities the vf type of computer simulation allows detailed analysis and prototyping of air flow in the data center by making use of computational fluid dynamics computational fluid dynamics cfd techniques this in turn allows the air flow and temperatures of the facility to be analyzed visually scientific visualization scientific visualisation and numerically to study and predict what will happen in the real facility the importance of scientific methods in design of mission critical facilities has become a necessity since the performance gains predicted by moore s law moore s law go hand in hand with a rise in power and heat dissipated by equipment rules of thumb have proven to be no longer adequate vf design purposes green field design asset management troubleshooting existing data centers making existing data centers more resilient making existing data centers more energy efficient cost prediction staff training capacity planning load growth management the vf is now being employed by many large organizations as a way of virtually assessing a situation before having to spend huge sums of money trying to solve a problem in the real facility it is essential to know whether adding new equipment or changing equipment will cause a logistical or thermal problem the vf allows the designer or operator to assess the best course of action and gives in depth understanding on unintuive behaviours references reflist citation last seymour first mark title virtual data centre design a blueprint for success url http www futurefacilities com newsarticles articles commerzbankarticlesummerzdtjournal pdf accessdate 2007 09 26 category data management'
b'refimprove date january 2008 an edge data integration is an implementation of data integration technology undertaken in an ad hoc or tactical fashion this is also sometimes referred to as point to point integration because it connects two types of data directly to serve a narrow purpose many edge integrations and actually the vast majority of all data integration involves hand coded scripts some may take the form of business mashups web application hybrids rich internet application s or other browser based models that take advantage of web 2 0 technologies to combine data in a web browser examples of edge data integration projects might be extracting a list of customers from a host sales force automation application and writing the results to an microsoft excel excel spreadsheet creating a script driven framework for managing rss feeds combining data from a weather web site a shipping company s web site and a company s internal logistics database to track shipments and estimated arrival times of packages it has been claimed that edge data integration do not typically require large budgets and centrally managed technologies which is in contrast to a core data integration see also core data integration business mashups rich internet application web 2 0 yahoo pipes microsoft popfly ibm mashup center category data management'
b'ad date march 2014 peacock date march 2014 the integration competency center icc sometimes referred to as an integration center of excellence coe is a shared services shared service function within an organization particularly large corporate enterprises as well as public sector institutions for performing methodical data integration system integration or enterprise application integration data integration allows companies to access their enterprise data and functions fragmented across disparate systems in order to create a combined accurate and consistent view of their core information as well as process assets and leverage them across the enterprise to drive business decisions and operations system integration is the bringing together of component subsystems into one system and ensuring that they function together effectively enterprise application integration enables efficient information exchanges and business process automation across separate computer applications in a cohesion computer science cohesive fashion overview the term may be better understood by examining each of the three words that comprise the acronym integration refers to the objective of the icc to take a holistic perspective and optimize certain qualities such as cost efficiency organizational agility and effectiveness operational risk customer internal or external experience etc across multiple functional groups competency refers to the expertise knowledge or capability that the icc offers as services center means that the service is managed or coordinated from a common central point independent from the functional areas that it supports large organizations are usually sub divided into functional areas such as marketing sales distribution finance human resources to name just a few these functional groups have separate operations and are vertically integrated and are therefore sometimes referred to as silos or stovepipes from an organizational perspective an icc is a group of people with special skills who are centrally coordinated and offer services to accomplish a mission that requires separate functional areas to work together key objectives of an icc are lead and support enterprise integration data system and process projects with the cooperation coordination of subject matter experts promote enterprise integration as a formal discipline for example data integration will include data warehousing data migration data quality management data integration for service oriented architecture deployments and data synchronization similar system integration will include common messaging services business service virtualization etc develop staff specialists in integration processes and operations and leverage their expertise company wide assess and select integration technology and tools from the marketplace manage integration pilots and projects across the organization optimize integration investments across the enterprise level leverage economies of scale for the integration tools portfolio at enterprise level iccs allow companies to optimize scarce resources by combining integration skills resources and processes into one group reduce project delivery times and development and maintenance costs through effectiveness and efficiency improve roi through creation and reuse of enterprise assets like source definitions application interfaces and codified business rules decrease duplication of integration related effort across the enterprise build on past successes instead of reinventing the wheel with each project lower total technology cost of ownership by leveraging technology investments across multiple projects an icc may be a temporary group in support of a program management program or a permanent part of the organization furthermore icc s can be established at various scales or levels within a division of a company at the enterprise level or across multiple companies in a supply chain history the term integration competency center and its acronym icc was popularized by roy schulte of gartner in a series of articles and conference presentations beginning in 2001 with the integration competency center citation missing date february 2013 ref spa 14 0456 mostly useless cannot be found anywhere he picked up the term from one of his colleagues gary long who found some of his clients using it they took the established term competency center and applied it to integration prior to that from 1997 to 2001 gartner had been referring to it as the central integration team the concept itself even before it was given a label goes back to 1996 in one of gartner s first reports on integration citation missing date february 2013 a major milestone was the publication in 2005 of the first book on the topic integration competency center an implementation methodology ref name icc john g schmidt and david lyle 2005 integration competency center an implementation methodology isbn 0 9769163 0 4 ref by john g schmidt and david lyle the book introduced five icc organizational models and explored the people process and technology dimensions of icc s several reviews of the book can be found at http blogs ittoolbox com eai business archives soa competency center 5731 it toolbox and at http www amazon com integration competency center implementation methodology dp 0976916304 amazon the concept of integration as a competency in the it domain has now survived for over 10 years and appears to be picking up momentum and broad based acceptance these days icc s are often called integration center of excellence soa center of excellence the data management center of excellence and other variants the most advanced icc s are using lean integration practices to optimize end to end processes and to drive continuous improvements universities are also beginning to include integration topics in their mba programs and computer science curricula for example the college of information sciences and technology at penn state university has established a http ist psu edu facultyresearch facilities eii enterprise informatics and integration center with the following mission the enterprise informatics and integration center ei\xc2\xb2 will actively engage industry non profit and government agency leaders to address critical issues in enterprise processes knowledge management and decision making operating models there are a number of ways an icc can be organized and a wide range of responsibilities with which it can be chartered the icc book ref name icc introduced five icc organizational models and explored the people process and technology dimensions of iccs they include best practices icc the primary function of this icc model is to document best practices it does not include a central support or development team to implement those standards across projects and probably not metadata either to implement a best practices icc companies need a flexible development environment that supports diverse teams and that enables the team to enhance and extend existing systems and processes such a team might be a subset of an existing enterprise architecture capability and generally consists of a small number of staff 1 5 standard services icc a standard services icc provides the same knowledge leverage as a best practices icc but enforces technical consistency in software development and hardware choices a standard services icc focuses on processes including standardizing and enforcing naming conventions establishing metadata standards instituting change management procedures and providing standards training this type of icc also reviews emerging technologies selects vendors and manages hardware and software systems this style of icc is often tightly linked with the enterprise architecture team and may be slightly larger than a typical best practices icc shared services icc a shared services icc provides a supported technical environment and services ranging from development support all the way through to a help desk for projects in production this type of icc is significantly more complex than a best practices or standard services model it establishes processes for knowledge management including product training standards enforcement technology benchmarking and metadata management and it facilitates impact analysis software quality and effective use of developer resources across projects the organizational structure of a shared services icc is sometimes referred to as a hybrid or federated model which often includes a small central coordinating team plus dotted line reporting relationships with multiple distributed teams central services icc a central services icc controls integration across the enterprise it carries out the same processes as the other models but in addition usually has its own budget and a charge back methodology it also offers more support for development projects providing management development resources data profiling data quality and unit testing because a central services icc is more involved in development activities than the other models it requires a production operator and a data integration developer the staff in a central services icc does not necessarily need to be a central location and may be distributed geographically the important distinction is that the staffs have a solid line reporting relationship to the icc director the size of these teams can vary and may be as large as 10 15 of the it staff in an organization self service icc the self service icc represents the highest level of maturity in an organization the icc itself may be almost invisible in that its functions are so ingrained in the day to day systems development life cycle and its operations are so tightly integrated with the infrastructure that it may require only small central team to sustain itself this icc model achieves both a highly efficient operation and provides an environment where independent development and innovation can flourish this goal is achieved by strict enforcement of a set of application integration standards through automated processes enabled by tools and systems key challenges icc as a concept is fairly simple it is embodiment of the it management best practices to deliver shared services however being an organizational concept it is far more challenging to implement in practice than the conceptual view because every organization has different dna and it takes specific personalization customization effort for icc that makes the icc initiative successful here are some of the common challenges in icc establishment journey change management in terms of technology processes organization structure ability of the organization to deal with the pace and quantum of change alignment of stakeholders and process owners for icc strategy inappropriate ownership level for icc program and lack of senior management sponsorship highly tactical focus and business program level constraints ignoring foundation elements and jumping to implementation directly inappropriate funding these issues are important to consider when embarking on the icc investment since the last leg of the implementation of icc that s what matters most intellectual definition of icc that is not implemented in the organisation has no real value for the enterprise see also lean integration references references further reading maurizio lenzerini 2002 data integration a theoretical perspective pods 2002 243 246 external links integration competency center book http www amazon com integration competency center implementation methodology dp 0976916304 informatica icc blog http blogs informatica com perspectives category data integration integration competency centers gartner paper http www ebizq net topics tech in biz features 5360 html integration consortium http www integrationconsortium org infosys icc blogs http www infosysblogs com bpm eai integration competency center icc icc handbook http www unthink fi global pdf icc handbook pdf integration warstories article about avoiding icc pitfalls http integrationwarstories com 2013 10 25 avoiding pitfalls of integration competency centers category data management category software development philosophies category information technology'
b'unreferenced date july 2010 transaction data are data describing an event the change as a result of a transaction processing transaction and is usually described with verbs transaction data always has a time dimension a numerical value and refers to one or more objects i e the reference data typical transactions are financial orders invoices payments work plans activity records logistics deliveries storage records travel records etc typical transaction processing system s systems generating transactions are sap erp sap and oracle financials records management main records management recording and retaining transactions is called records management the record of the transaction is stored in a place where the wikt retention retention can be guaranteed and where data are archived removed following a retention period the format of the transaction can be data to be stored in a database but it can also be a document data warehousing transaction data can be summarised in a data warehouse which helps accessibility and analysis of the data see also data modeling data architecture information lifecycle management reference data category data management category transaction processing compsci stub'
b'for use in finance reference data financial markets reference data are data that define the set of permissible values to be used by other data field s reference data gain in value when they are widely re used and widely referenced typically they do not change overly much in terms of definition apart from occasional revisions reference data are often defined by standards organizations such as country codes as defined in iso 3166 1 ref cite web title ibm redbooks reference data management url http www redbooks ibm com abstracts tips1016 html website www redbooks ibm com date 2013 05 16 accessdate 2015 12 09 language en ref ref cite web title reference data management and master data are they related oracle master data management url https blogs oracle com mdm entry reference data management and master website blogs oracle com accessdate 2015 12 09 ref examples of reference data include units of measurement country code s corporate codes conversion of units fixed conversion rates e g weight temperature and length calendar structure and constraints differences with master data reference data should be distinguished from master data which represent key business entities such as customers and materials in all their necessary detail e g for customers number name address and date of account creation in contrast reference data usually consist only of a list of permissible values and attached textual descriptions a further difference between reference data and master data is that a change to the reference data values may require an associated change in business process to support the change a change in master data will always be managed as part of existing business processes for example adding a new customer or sales product is part of the standard business process however adding a new product classification e g restricted sales item or a new customer type e g gold level customer will result in a modification to the business processes to manage those items references references further reading book reference title managing reference data in enterprise databases last chisholm first malcolm publisher morgan kaufmann publishers year 2001 isbn 1558606971 location pages book reference title master data management for saas applications last whei jen first chen publisher ibm redbooks year 2014 isbn 978 0738440040 location pages book reference title master data management and data governance last berson first alex publisher mcgraw hill osborne media year 2011 isbn 978 0071744584 location pages see also master data data modeling master data management enterprise bookmarking data architecture transaction data code metadata external links https msdn microsoft com en us library hh213066 aspx microsoft msdn reference data services in dqs 2012 category data management'
b'more footnotes date march 2013 operational database management systems also referred to as oltp on line transaction processing databases are used to manage dynamic data in real time these types of databases allow you to do more than simply view archived data operational databases allow you to modify that data add change or delete data doing it in real time computing real time since the early 90 s the operational database software market has been largely taken over by sql engines today the operational dbms market formerly oltp is evolving dramatically with new innovative entrants and incumbents supporting the growing use of unstructured data and nosql dbms engines as well as xml database s and newsql newsql databases operational databases are increasingly supporting distributed database architecture that provides high availability and fault tolerance through replication computing replication and scale out ability recognizing the growing role of operational databases in the it industry that is fast moving from legacy databases to real time operational databases capable to handle distributed web and mobile demand and to address big data challenges in october 2013 gartner started to publish the magic quadrant for operational database management systems ref name gartner magic quadrant for operational database management systems cite web url https www gartner com doc 2610218 magic quadrant operational database management title gartner magic quadrant for operational database management systems publisher gartner com ref list of operational databases style text align left class wikitable sortable database platform database model sql support nosql support managed objects acid transactions aerospike database aerospike key value store no yes key value pairs none altibase relational database yes no tabular data real time acid transactions apache cassandra key value store no yes key value pairs none cloudant document oriented database no yes json none clusterpoint document oriented database yes essential sql yes xml json text data distributed acid transactions clustrix relational database yes newsql no tabular data acid transactions couchbase document oriented database yes n1ql yes json none couchdb document oriented database no yes json none enterprisedb relational database yes no tabular data acid transactions foundationdb key value store yes no key value pairs acid transactions ibm db2 relational database yes no tabular data acid transactions ingres database ingres relational database yes no tabular data acid transactions marklogic document oriented database no yes xquery xml acid transactions microsoft sql server relational database yes no tabular data acid transactions mongodb document oriented database no yes bson none nuodb relational database yes newsql no tabular data acid compliant oracle database oracle relational database yes no tabular data acid transactions orientdb document oriented database yes yes key value pairs acid transactions ref http orientdb com docs last transactions html ref riak key value store no yes key value pairs none sap hana relational database yes no tabular data acid transactions voltdb relational database yes newsql no tabular data acid transactions use in business operational databases are used to store manage and track real time business information for example a company might have an operational database used to track warehouse stock quantities as customers order products from an online web store an operational database can be used to keep track of how many items have been sold and when the company will need to reorder stock an operational database stores information about the activities of an organization for example customer relationship management transactions or financial operations in a computer database operational databases allow a business to enter gather and retrieve large quantities of specific information such as company legal data financial data call data records personal employee information sales data customer data data on assets and many other information an important feature of storing information in an operational database is the ability to share information across the company and over the internet operational databases can be used to manage mission critical business data to monitor activities to audit suspicious transactions or to review the history of dealings with a particular customer they can also be part of the actual process of making and fulfilling a purchase for example in e commerce data warehouse terminology in data warehouse data warehousing the term is even more specific the operational database is the one which is accessed by an operational system for example a customer facing website or the application used by the customer service department to carry out regular operations of an organization operational databases usually use an online transaction processing database which is optimized for faster transaction processing create read update and delete operations see also document database document oriented databases newsql newsql databases nosql nosql databases xml xml databases sql sql databases distributed database s references reflist 33em o brien jason and marakas gorila 2008 management information technology systems computer software pp nbsp 185 new york new york mcgraw hill category data warehousing category data management category information technology management category business intelligence category types of databases'
b'refimprove date august 2012 paper data storage refers to the use of paper as a data storage device this includes writing illustrating and the use of data that can be interpreted by a machine or is the result of the functioning of a machine a defining feature of paper data storage is the ability of humans to produce it with only simple tools and interpret it visually though this is now mostly obsolete paper was once also an important form of computer data storage history before paper was used for storing data it had been used in several applications for storing instructions to specify a machine s operation the earliest use of paper to store instructions for a machine was the work of basile bouchon who in 1725 used punched paper rolls to control textile looms this technology was later developed into the wildly successful jacquard loom the 19th century saw several other uses of paper for controlling machines in 1846 telegrams could be prerecorded on punched tape and rapidly transmitted using alexander bain inventor alexander bain s automatic telegraph several inventors took the concept of a mechanical organ and used paper to represent the music in the late 1880s herman hollerith invented the recording of data on a medium that could then be read by a machine prior uses of machine readable media above had been for control automaton s piano roll s jacquard loom looms not data after some initial trials with paper tape he settled on punched card s ref http www columbia edu acis history hollerith html columbia university computing history herman hollerith ref hollerith s method was used in the 1890 census the census bureau is not an independent 3rd party source as required by wikipedia for census bureau performance claims following claim deleted and the completed results were finished months ahead of schedule and far under budget ref http www census gov history www technology 010873 html u s census bureau tabulation and processing ref hollerith s company eventually became the core of international business machines ibm other technologies were also developed that allowed machines to work with marks on paper instead of punched holes this technology was widely used for optical scan voting system tabulating votes and grading scantron standardized tests barcode s made it possible for any object that was to be sold or transported to have some computer readable information securely attached to it banks used magnetic ink on checks supporting micr scanning in an early electronic computing device the atanasoff berry computer electric sparks were used to singe small holes in paper cards to represent binary data the altered dielectric constant of the paper at the location of the holes could then be used to read the binary data back into the machine by means of electric sparks of lower voltage than the sparks used to create the holes this form of paper data storage was never made reliable and was not used in any subsequent machine as of 2014 universal product code barcodes first used in 1974 are ubiquitous some people recommend a width of at least 3 pixels for each minimum width gap and each minimum width bar for 1d barcodes and a width of at least 4 pixels e g a 4 nbsp \xc3\x97 nbsp 4 pixel 16 pixel module for 2d barcode s ref accusoft http www accusoft com whitepapers barcodes barcodesindocuments bestpractices pdf using barcodes in documents best practices 2007 retrieved 2014 04 25 ref for a typical black and white barcode scanned by a typical 300 dpi image scanner and assuming roughly half the space is occupied by finder patterns fiducial alignment patterns and error detection and correction codes that recommendation gives a maximum data density of roughly 50 bits per linear inch about 2 bit mm for 1d barcodes and roughly 2 800 bits per square inch about 4 4 bit mm sup 2 sup limits the limits of data storage depend on the technology to write and read such data for example an 8\xe2\x80\xb3 nbsp \xc3\x97 nbsp 10\xe2\x80\xb3 roughly a4 without margins 300dpi 8 bit greyscale image map contains 7 2 megabytes of data assuming a scanner can accurately reproduce the printed image to that resolution and color depth and a program can accurately interpret such an image a similarly sized image in 2400dpi 24 bit true color theoretically contains 1 38 gigabytes of information see also div col 3 banknote read by a vending machine book music edge notched card index card kimball tag machine readable medium magnetic ink character recognition mark sense music roll optical mark recognition paper disc perfin perforation punched tape spindle stationery stenotype ticker tape div col end references reflist external links http www microglyphs com english html dataglyphs shtml dataglyphs http ollydbg de paperbak paperback data storage paper data storage media category data management category storage media'
b'infobox software name cognos reportnet logo screenshot caption author cognos an ibm company developer released september 2003 latest release version cognos reportnet 1 3 latest release date latest preview version latest preview date operating system multiple platform multiple language multi lingual status inactive ref http www 01 ibm com software analytics cognos products reportnet ibm com cognos reportnet now part of cognos enterprise consult\xc3\xa9 le 12 f\xc3\xa9vrier 2014 ref genre business intelligence license website http www 01 ibm com software data cognos products reportnet ibm com cognos reportnet crn is a web based software product for creating and managing ad hoc and custom made reports reportnet is developed by the ottawa based company cognos formerly cognos incorporated an ibm company the web based reporting tool was launched in september 2003 since ibm s acquisition of cognos reportnet has been renamed ibm cognos reportnet like all other cognos products reportnet uses web services standards such as xml and simple object access protocol and also supports dynamic html and java programming language java ref https web archive org web 20080312025955 http www vnunet com vnunet news 2123232 bear sterns chooses cognos reportnet cognos reportnet in news ref reportnet is compatible with multiple databases including oracle database oracle sap ag sap teradata microsoft sql server ibm db2 db2 and sybase ref http www cognos com solutions data ibm advantages html data sources ref ref http support cognos com en support products crn101 software environments html crn environment details ref the product provides interface in over 10 languages ref http www cognos com products business intelligence reporting features html crn features ref has web services architecture to meet the needs of multi national diversified enterprises and helps reduce total cost of ownership multiple versions of cognos reportnet have since been released by the company cognos reportnet was awarded the software and information industry association siia 2005 codie awards for the best business intelligence or knowledge management solution category ref http www mywire com pubs prnewswire 2005 06 08 885642 extid 10051 cognos reportnet wins award ref crn s capabilities have been further used in ibm cognos 8 business intelligence ibm cognos 8 bi 2005 the latest reporting tool ref http www cognos com products cognos8businessintelligence cognos 8 bi ref crn comes with its own software development kit sdk launch early adopters of cognos reportnet for their corporate reporting needs included bear stearns bmw and alfred publishing around this same time of launch cognos competitor business objects released version 6 1 of its enterprise reporting tool cognos reportnet has been successful since its launch raising revenues in 2004 from licensing fees ref http www highbeam com doc 1g1 131525446 html cognos reportnet delivers 30 million in license revenue in one quarter ref subsequently other major corporations like mcdonald s adopted cognos reportnet ref http www ebizq net news 5538 html reportnet and fries ref controversy cognos rival business objects company business objects announced in 2005 that businessobjects xi significantly outperformed cognos reportnet in benchmark tests conducted by veritest an independent software testing firm the tests performed showed cognos reportnet performed poorly when processing styled reports complex business reports and combination of both ref http www crm2day com news crm 114773 php bo xi vs cognos reportnet ref the tests reported a massive 21 times higher report throughput for businessobjects xi than cognos reportnet at capacity loads ref http goliath ecnext com coms2 summary 0199 4404821 itm bo xi outperforms cognos reportnet ref cognos soon dismissed the claims by stating business objects dictated the environment and testing criteria and cognos did not provide the software to participate in benchmark test ref http www cognos com news releases 2005 0624 3 html cognos dismisses the test results ref cognos later performed their own test to demonstrate cognos reportnet capabilities ref http www cognos com pdfs whitepapers wp cognos reportnet scalability benchmakrs ms windows pdf cognos scalability results ref components cognos report studio a web based product for creating complex professional looking reports ref http web princeton edu sites datamall documents ug cr rptstd pdf refer definition in introduction page ref cognos query studio a web based product for creating ad hoc reports ref http web princeton edu sites datamall documents ug cr qstd pdf refer introduction page ref cognos framework manager a metadata modeling tool to create bi metadata for reporting and dashboard applications ref http www cognos com products framework services framework manager services webarchive url https web archive org web 20080417030129 http www cognos com products framework services date april 17 2008 ref cognos connection main enterprise portal portal used to access reports schedule reports and perform administrator activities ref http web princeton edu sites datamall documents ug cr qstd pdf refer page9 ref versions cognos reportnet 1 1 java ee style professional web based authoring tool base version cognos reportnet ibm special edition comes with an embedded version of ibm websphere as its application server and ibm db2 as its data store cognos linux for intel based linux platforms ref http www ebizq net news 5688 html reportnet on linux ref see also ibm cognos business intelligence references reflist 30em category business intelligence category data management category ibm software'
b'data proliferation refers to the prodigious amount of data structured data structured and unstructured that businesses and governments continue to generate at an unprecedented rate and the usability problems that result from attempting to store and manage that data while originally pertaining to problems associated with paper documentation data proliferation has become a major problem in primary and secondary data storage device data storage on computers while digital storage has become cheaper the associated costs from raw power to maintenance and from metadata to search engines have not kept up with the proliferation of data although the power required to maintain a unit of data has fallen the cost of facilities which house the digital storage has tended to rise ref cite web url http www deloitte co uk tmtpredictions technology downsizing digital attic data storage cfm title downsizing the digital attic work deloitte technology predictions archiveurl https web archive org web 20110722194032 http www deloitte co uk tmtpredictions technology downsizing digital attic data storage cfm archivedate july 22 2011 ref rquote right at the simplest level company e mail systems spawn large amounts of data business e mail some of it important to the enterprise some much less so is estimated to be growing at a rate of 25 30 annually and whether it s relevant or not the load on the system is being magnified by practices such as multiple addressing and the attaching of large text audio and even video file formats video file s ibm global technology services ref name ibm https web archive org web 20090206010415 http www 03 ibm com systems resources systems storage solutions pdf toxic tb pdf the toxic terabyte ibm global technology services july 2006 ref data proliferation has been documented as a problem for the u s military since august 1971 in particular regarding the excessive documentation submitted during the acquisition of major weapon systems ref name dodpp http stinet dtic mil oai oai verb getrecord metadataprefix html identifier ad0892652 evolution of the data proliferation problem within major air force acquisition programs ref efforts to mitigate data proliferation and the problems associated with it are ongoing ref http www thic org pdf jun02 dod rroderique 020612 pdf data proliferation stop that ref problems caused the problem of data proliferation is affecting all areas of commerce as the result of the availability of relatively inexpensive data storage devices this has made it very easy to dump data into secondary storage immediately after its window of usability has passed this masks problems that could gravely affect the profitability of businesses and the efficient functioning of health services police and security forces local and national governments and many other types of organizations ref name ibm data proliferation is problematic for several reasons difficulty when trying to find and retrieve information at xerox on average it takes employees more than one hour per week to document retrieval find hard copy documents costing 2 152 a year to manage and store them for businesses with more than 10 employees this increases to almost two hours per week at 5 760 per year ref http www itbusiness ca it client en home news asp id 40615 cid 13 dealing with data proliferation vawn himmelsbach it business ca canadian technology news september 19 2006 ref in large storage network networks of primary and secondary data storage problems finding electronic data are analogous to problems finding hard copy data data loss and legal liability when data is disorganized not properly replicated or cannot be found in a timely manner in april 2005 the td ameritrade ameritrade holding corporation told 200 000 current and past customers that a magnetic tape data storage tape containing confidential information had been lost or destroyed in transit in may of the same year time warner incorporated reported that 40 tapes containing personal data on 600 000 current and former employees had been lost en route to a storage facility in march 2005 a florida judge hearing a 2 7 billion lawsuit against morgan stanley issued an adverse inference order against the company for willful and gross abuse of its discovery obligations the judge cited morgan stanley for repeatedly finding misplaced tapes of e mail messages long after the company had claimed that it had turned over all such tapes to the court ref http www computerworld com printthis 2005 0 4814 103541 00 html data lost stolen or strayed computer world security ref increased manpower requirements to manage increasingly chaotic data storage resources slower networks and application performance due to excess traffic as users search and search again for the material they need ref name ibm high cost in terms of the energy resources required to operate storage hardware a 100 terabyte system will cost up to 35 040 a year to run not counting cooling costs ref http findarticles com p articles mi m0brz is 10 23 ai 111062988 power and storage the hidden cost of ownership computer technology review october 2003 ref proposed solutions applications that better utilize modern technology reductions in duplicate data especially as caused by data movement improvement of metadata structures improvement of file and storage transfer structures user education and discipline ref name dodpp the implementation of information lifecycle management solutions to eliminate low value information as early as possible before putting the rest into actively managed long term storage in which it can be quickly and cheaply accessed ref name ibm see also backup digital asset management disk storage document management system hierarchical storage management information lifecycle management information repository magnetic tape data storage retention period retention schedule references reflist category information technology management category content management systems category data management'
b'information science information architecture ia is the structural design of shared information environments the art and science of organizing and labelling website s intranet s online communities and software to support usability and findability and an emerging community of practice focused on bringing principles of design and architecture to the digital landscape ref name what cite journal title what is ia publisher information architecture institute url http www iainstitute org documents learn what is ia pdf format pdf postscript bot inserted parameter either remove it or change its value to for the cite to end in a as necessary inconsistent citations ref typically it involves a scientific modelling model or concept of information that is used and applied to activities which require explicit details of complex information system s these activities include library systems and database development information architecture is considered to have been founded by richard saul wurman ref name richard saul wurman cooper hewitt cite web title richard saul wurman awarded for lifetime achievement url http wurman com rsw publisher smithsonian cooper hewitt national design museum accessdate 19 april 2014 ref today there is a growing network of active ia specialists who constitute the information architecture institute ref cite journal title join the ia network publisher information architecture institute url http www iainstitute org en network postscript bot inserted parameter either remove it or change its value to for the cite to end in a as necessary inconsistent citations ref definition information architecture has somewhat different meanings in different branches of information system is or information technology it the structural design of shared information environments sfn morville rosenfeld 2007 rp 4 the art and science of organizing and labeling web sites intranets online communities and software to support findability and usability ref name what ref morville nbsp amp rosenfeld 2007 p nbsp 4 the art and science of shaping information products and experienced to support usability and findability ref an emerging community of practice focused on bringing principles of design and architecture to the digital landscape sfn morville rosenfeld 2007 rp 4 ref resmini a rosati l 2012 a brief history of information architecture journal of information architecture vol 3 no 2 available at http journalofia org volume3 issue2 03 resmini originally published in resmini a rosati l 2011 pervasive information architecture morgan kauffman edited by the authors ref the combination of organization labeling search and navigation systems within websites and intranets sfn morville rosenfeld 2007 rp 4 extracting required parameters data of engineering designs in the process of creating a knowledge base linking different systems and standards a subset of data architecture where usable data a k a information is constructed in and designed or arranged in a fashion most useful or empirically holistic to the users of this data the practice of organizing the information content functionality of a web site so that it presents the best user experience it can with information and services being easily usable and findable as applied to web design and development ref cite web url https developer mozilla org en us docs glossary information architecture title information architecture last first date website publisher mozilla developer network access date ref debate the difficulty in establishing a common definition for information architecture arises partly from the term s existence in multiple fields in the field of systems design for example information architecture is a component of enterprise architecture that deals with the information component when describing the structure of an enterprise while the definition of information architecture is relatively well established in the field of systems design it is much more debatable within the context of online information systems i e websites andrew dillon refers to the latter as the big ia little ia debate ref cite journal last dillon first a year 2002 title information architecture in jasist just where did we come from journal journal of the american society for information science and technology volume 53 pages 821 23 issue 10 doi 10 1002 asi 10090 postscript bot inserted parameter either remove it or change its value to for the cite to end in a as necessary inconsistent citations ref in the little ia view information architecture is essentially the application of information science to web design which considers for example issues of classification and information retrieval in the big ia view information architecture involves more than just the organization of a website it also factors in user experience thereby considering usability issues of information design information architect about the term information architect richard saul wurman wrote i mean architect as used in the words architect of foreign policy i mean architect as in the creating of systemic structural and orderly principles to make something work the thoughtful making of either artifact or idea or policy that informs because it is clear ref wurman introduction in information architects 1997 p 16 ref notable people in information architecture pioneers richard saul wurman peter morville louis rosenfeld first generation jorge arango jesse james garrett adam greenfield peter merholz eric reiss donna spencer christina wodtke second generation abby covert andrew hinton dan klyn andrea resmini influencers david weinberger see also div col applications architecture card sorting chief experience officer content management content strategy controlled vocabulary data management data presentation architecture digital humanities ecological interface design enterprise information security architecture faceted classification human factors and ergonomics informatics interaction design process architecture site map social information architecture tree testing user experience design section link visualization graphics knowledge visualization wayfinding web graph web literacy infrastructure div col end references reflist bibliography cite book editor last1 wurman editor first1 richard saul editor1 link richard saul wurman isbn 1 888 00138 0 url http www amazon com dp 1888001380 year 1997 title information architects edition 1st publisher graphis inc cite book last2 rosenfeld first2 louis author2 link lou rosenfeld last1 morville first1 peter author1 link peter morville isbn 0 596 52734 9 url https books google com books id 2d2ry2hzc2mc printsec frontcover dq information architecture v onepage q f false year 2007 title information architecture for the world wide web edition 3rd publisher o reilly associates place sebastopol ca ref harv cite book last1 brown first1 peter isbn 0 471 48679 5 url http www amazon com dp 0471486795 year 2003 title information architecture with xml edition 1st publisher john wiley sons ltd cite book last1 wodtke first1 christina author1 link christina wodtke isbn 0 321 60080 0 url https books google com books id tp40qfgcu2sc year 2009 title information architecture blueprints for the web edition 2nd publisher new riders cite book last1 resmini first1 andrea last2 rosati first2 luca isbn 0 123 82094 4 url https books google com books id ntwc13nsinkc year 2011 title pervasive information architecture designing cross channel user experiences edition 1st publisher morgan kauffman further reading cite book author1 wei ding author2 xia lin title information architecture the design and integration of information spaces url https books google com books id wy3rhkowwqc date 15 may 2009 publisher morgan claypool isbn 978 1 59829 959 5 cite book author1 sue batley title information architecture for information professionals url https books google com books id 6g0paqaamaaj date january 2007 publisher woodhead publishing isbn 978 1 84334 233 5 cite book author1 earl morrogh title information architecture an emerging 21st century profession url https books google com books id jzlmqgaacaaj dq year 2003 publisher prentice hall isbn 9780130967466 cite book author1 peter van dijck title information architecture for designers structuring websites for business success url https books google com books id wy2sb0r udyc dq date august 1 2003 publisher rotovision isbn 9782880467319 cite book author1 alan gilchrist author2 barry mahon title information architecture designing information environments for purpose url https books google com books id akxqaaaamaaj q year 2004 publisher facet isbn 9781856044875 semantic web category data management category enterprise architecture category information architects category information governance category information science category information technology management category information technology category records management category technical communication category information architecture'
b'holistic data management hdm framework is ahisdata indigenous standard for implementing software implementations within an organization network this framework extends the existing data management solutions such as data quality data governance data integration data processing master data management and data validation solutions the hdm framework specifies that all data objects must exist as a child data object or a parent data object only one unique parent data object must exist within a data network scope dns all child data objects must have a data mapping link defined within a data network scope a data object relationship must exist at least in one of the following four data management modules data mapping data validation data integration data processing hdm framework the following entities are specified in the hdm framework data network scope dns the data network scope dns is the logical boundary that a software application database system of record sor exists within an enterprise network there can be multiple dns within an enterprise network data network domain dnd the data network domain dnd is the logical boundary representing a collection of multiple data network scope dns there can be multiple dnd within an enterprise network system of record sor a system of record applies to the master or principal database system that a parent data objects resides on there can only be one sor within a data network scope parent data object pdo a parent data object pdo is the system of record schema object name only one unique parent data object must exist within a data network scope child data object cdo a child data object cdo is a schema object name that derives its data from one or more parent data object s data mapping link dml a data mapping link dml is the data requirement specification applied to the relationship between multiple database schema objects where one data object derives its data from one or more data objects dml is only applicable for a data mapping data management module data object relationship dor the data object relationship dor is the data requirement business rule program function that applies to one or multiple data objects dor can be applied on data mapping links for each data management modules only one dor can exist on a dml within a data management module data management modules dmm data management modules are the common user interface ui programs that defines and manage the data object relationship s within a data network scope there are four data management modules data mapping this is the base data management user interface module the data mapping module provide the functionalities for managing data mapping links and data object relationships for all database schemas within a data network scope a data network scope must have at least one data mapping design defined data validation this user interface module provides the functionalities for defining and managing validation events on data object relationships validation events include auditing reporting scheduler logger triggers and dns health check data validation events requires a data mapping design defined within a data network scope data integration this user interface module provides the functionalities for defining and managing interface configurations on data object relationships the interface configurations include scheduler transmission mode listener interface api and reporting the interface apis would allow third party systems to transfer data using the data object relationship defined within a data network scope data integration interface configuration requires a data mapping design defined within a data network scope data processing this user interface module provides the functionalities for defining and managing interface configurations and batch runtime engines on data object relationships the interface configurations include scheduler transmission mode multi batch transmission user defined dor api and reporting data processing interface configuration requires a data mapping design defined within a data network scope implementing the hdm framework the hdm framework presents a standard for software implementations within an organization the objective is to shed visibility increase efficiency and centralized management of all other software implementations within an organization the hdm framework should be implemented as a major organization project that is supervised by the project management office this would require a project charter developed and a project manager assigned for managing the implementation process there are several phases involve in implementing the hdm framework choose a data management module dmm this exercise requires the acquisition of a data management module software application to be used for implementing the rest of the hdm framework ahisdata integrity software is an integrated solution that provides dmm functionalities scrub inventory of existing applications and data sources this exercise identifies all applications within an organization and the data sources that they are connected to formation applications and data schema relation this exercise is to align all applications in relation to the data schemas within the data sources the applications are grouped in the order of the data schemas that they access first axe applications eligible for decommission this exercise is to identify all applications that are rogue obsolete and completely redundant these applications are eligible for removal second axe application eligible for consolidation this exercise is to identify all applications that have some functional similarities and some uniqueness in the data requirement these applications are eligible for consolidation the functionalities that are similar are left intact on one application and turned off or disabled on the other s define data network domain dnd this exercise is to define the data network domain for all the approved applications within the enterprise network define data network scope dns this exercise is to define the data network scope s required for each dnd define system of records sor this exercise is to define the sor for each dns define parent data objects pdo this exercise is to define all pdos in each dns define child data objects cdo this exercise is to define all cdos in each dns define data mapping links dml this exercise is to define all data mapping links and object relationship in all dns define data object relationships dor this exercise is to define the dor requirement for each data management module implemented see also reference data master data customer data integration product information management identity resolution external links http www ahisdata com ehdms ahisdata hdm whitepaper v35 pdf ahisdata hdm whitepaper http msdn2 microsoft com en us library bb190163 aspx mdm04 topic4 the what why and how of master data management data warehouse category business intelligence category data management category data warehousing products category information technology management'
b'multiple issues orphan date february 2009 no sources date october 2015 a long lived transaction is a database transaction transaction that spans multiple database transactions the transaction is considered long lived because its boundaries must by necessity of business logic extend past a single database transaction a long lived transaction can be thought of as a sequence of database transactions grouped to achieve a single atomic result a common example is a multi step sequence of requests and responses of an interaction with a user through a web client a long lived transaction creates challenges of concurrency control and scalability a chief strategy in designing long lived transactions is optimistic concurrency control with version control versioning so much research work related to these long lived transactions was carried out by several professors from the oxford university and michigan state university and the central university of hyderabad dr james from the oxford university created several hypotheses for long lived transactions dr copperfield of the michigan state university was regarded highly for his contributions in this field dr a b sagar of hyderabad central university has also done very creative work in relating long lived transactions with financial transactions in microfinance however the study is not complete and is still open to challenges and research issues see also long running transaction category data management category transaction processing software eng stub'
b'sales intelligence si refers to technologies applications and practices for the collection integration analysis and presentation of information to help salespeople keep up to date with clients prospect research prospect data and drive business in addition to providing performance metric metric s for win loss and sales confidence ref http chapmanhq com solutions strategic account management sam metrics and measurements metrics for sales intelligence ref si can present contextually relevant customer and product information the 2008 survey of 300 companies by the aberdeen group ref http www aberdeen com aberdeen library 5379 ra sales intelligence nirvana aspx sales intelligence aberdeen group study 2008 ref show that the recent economic downturn has lengthened traditional sales cycles as businesses have been forced to reduce spending sales representatives have been challenged to meet sales quota quota s top performing companies have implemented sales intelligence programs to improve the quality and quantity of sales leads si contextualizes opportunities by providing relevant industry corporate and personal information frequently si s fact based information is integrated or includes customer relationship management crm although some aspects of sales intelligence overlaps business intelligence bi si is specifically designed for the use of salespeople and sales managers unlike customer relationship management crm and traditional business intelligence bi applications si provides real time analysis of current sales data and assists with suggesting and delivering actionable relevant information sales intelligence solutions are predominantly designed for companies in the manufacturing distribution business distribution and wholesale sectors these are highly competitive markets where volumes are high profit margin margin s are low si solutions provide unique insight into customer buying pattern s by automatically analysing and evaluating these patterns sales intelligence pro actively identifies and delivers up sell cross sell and switch sell opportunities see also analytics augmented learning business intelligence tools dashboards management information systems location intelligence market intelligence marketing intelligence operational intelligence ooda loop predictive analytics business intelligence 2 0 process mining right time marketing integrated business planning references reflist external links http blog findable me post 52963306183 sales intelligence a short primer sales intelligence a short primer category business intelligence category data management da business intelligence de business intelligence es inteligencia empresarial fr informatique d\xc3\xa9cisionnelle ko \xea\xb2\xbd\xec\x98\x81 \xec\xa0\x95\xeb\xb3\xb4\xed\x95\x99 hr poslovna inteligencija id intelijen bisnis it business intelligence lt verslo analitika nl business intelligence pl business intelligence pt business intelligence ru business intelligence fi business intelligence sv business intelligence'
b'a schema crosswalk is a table that shows equivalent elements or fields in more than one database schema it maps the elements in one schema to the equivalent elements in another schema crosswalk tables are often employed within or in parallel to enterprise systems especially when multiple systems are interfaced or when the system includes legacy system data in the context of interfaces they function as a sort of internal extract transform load etl mechanism for example this is a metadata crosswalk from marc standards marc to dublin core center class wikitable marc field dublin core element 260c date of publication distribution etc \xe2\x86\x92 date created 522 geographic coverage note \xe2\x86\x92 coverage spatial 300a physical description \xe2\x86\x92 format extent center crosswalks show people where to put the data from one scheme into a different scheme they are often used by libraries archives museums and other cultural institutions to translate data to or from marc standards marc dublin core text encoding initiative tei and other metadata schemes for example say an archive has a marc record in their catalog describing a manuscript if the archive makes a digital copy of that manuscript and wants to display it on the web along with the information from the catalog it will have to translate the data from the marc catalog record into a different format such as metadata object description schema mods that is viewable in a webpage because marc has different fields than mods decisions must be made about where to put the data into mods this type of translating from one format to another is often called metadata mapping or field mapping and is related to data mapping and semantic mapper semantic mapping crosswalks also have several technical capabilities they help databases using different metadata schemes to share information they help metadata harvesters create union catalogs they enable search engines to search multiple databases simultaneously with a single query challenges for crosswalks one of the biggest challenges for crosswalks is that no two metadata schemes are 100 equivalent one scheme may have a field that doesn t exist in another scheme or it may have a field that is split into two different fields in another scheme this is why you often lose data when mapping from a complex scheme to a simpler one for example when mapping from marc to simple dublin core you lose the distinction between types of titles center class wikitable marc field dublin core element 210 abbreviated title \xe2\x86\x92 title 222 key title \xe2\x86\x92 title 240 uniform title \xe2\x86\x92 title 242 translated title \xe2\x86\x92 title 245 title statement \xe2\x86\x92 title 246 variant title \xe2\x86\x92 title center simple dublin core only has one single title element so all of the different types of marc titles get lumped together without any further distinctions this is called many to one mapping this is also why once you ve translated these titles into simple dublin core you can t translate them back into marc once they re simple dublin core you ve lost the marc information about what types of titles they are so when you map from simple dublin core back to marc all the data in the title element maps to the basic marc 245 title statement field ref http www loc gov marc dccross html dublin core to marc crosswalk network development and marc standards office library of congress ref center class wikitable dublin core element marc field title \xe2\x86\x92 245 title statement title \xe2\x86\x92 245 title statement title \xe2\x86\x92 245 title statement title \xe2\x86\x92 245 title statement title \xe2\x86\x92 245 title statement title \xe2\x86\x92 245 title statement center this is why crosswalks are said to be lateral one way mappings from one scheme to another separate crosswalks would be required to map from scheme a to scheme b and from scheme b to scheme a ref cite book title metadata fundamentals for all librarians last caplan first priscilla publisher american library association year 2003 isbn 0838908470 location chicago pages 39 quote via ref difficulties in mapping other mapping problems arise when one scheme has one element that needs to be split up with different parts of it placed in multiple other elements in the second scheme one to many mapping one scheme allows an element to be repeated more than once while another only allows that element to appear once with multiple terms in it schemes have different data formats e g john doe or doe john an element in one scheme is indexed but the equivalent element in the other scheme is not schemes may use different controlled vocabularies schemes change their standards over time some of these problems are simply not fixable as karen coyle says in crosswalking citation metadata the university of california s experience blockquote the more metadata experience we have the more it becomes clear that metadata perfection is not attainable and anyone who attempts it will be sorely disappointed when metadata is crosswalked between two or more unrelated sources there will be data elements that cannot be reconciled in an ideal manner the key to a successful metadata crosswalk is intelligent flexibility it is essential to focus on the important goals and be willing to compromise in order to reach a practical conclusion to projects ref u in u metadata in practice diane i hillmann and elaine l westbrooks eds american library association chicago 2004 p 91 ref blockquote examples marc to dublin core library of congress http loc gov marc marc2dc html dublin core to marc21 library of congress http www loc gov marc dccross html dublin core to unimarc ukoln http www ukoln ac uk metadata interoperability dc unimarc html tei to and from marc http purl oclc org net teiinlibraries fgdc to usmarc alexandria http www alexandria ucsb edu public documents metadata fgdc2marc html onix to marc21 lc http www loc gov marc onix2marc html vra to marc indiana university http php indiana edu 7efryp marcmap html metadata mappings mit library http web archive org web 20080720134522 http libraries mit edu guides subjects metadata mappings html mapping between metadata formats ukoln http www ukoln ac uk metadata interoperability international metadata standard mappings academia sinica http www sinica edu tw 7emetadata standard mapping foreign eng htm jats to marc http webservices itcs umich edu mediawiki jats index php jats to marc mapping see also meta tag metadata database references reflist external links http www oclc org research researchworks schematrans default htm metadata crosswalk depository schematrans oclc http www ukoln ac uk metadata interoperability mapping between metadata formats ukoln http www getty edu research conducting research standards intrometadata path html crosswalks the path to universal access getty http www dlib org dlib june06 chan 06chan html metadata interoperability and standardization a study of methodology part i d lib category data management category knowledge representation category library cataloging and classification category metadata category technical communication'
b'vector field consistency ref group nb sub designation coined by l veiga sub ref is a consistency model for replicated data for example objects initially described in a paper ref cite conference author1 nuno santos author2 lu\xc3\xads veiga author3 paulo ferreira year 2007 title vector field consistency for adhoc gaming booktitle acm ifip usenix middleware conference 2007 url http www gsd inesc id pt pjpf middleware07vector pdf format pdf ref which was awarded the best paper prize in the acm ifip usenix middleware conference 2007 it has since been enhanced for increased scalability and fault tolerance in a recent paper ref cite conference author1 lu\xc3\xads veiga author2 andr\xc3\xa9 negr\xc3\xa3o author3 nuno santos author4 paulo ferreira year 2010 title unifying divergence bounding and locality awareness in replicated systems with vector field consistency booktitle jisa journal of internet services and applications volume 1 number 2 95 115 springer 2010 url http www gsd inesc id pt lveiga vfc jisa 2010 pdf format pdf ref description this consistency model was initially designed for replicated data management in adhoc gaming in order to minimize bandwidth usage without sacrificing playability intuitively it captures the notion that although players require wish and take advantage of information regarding the whole of the game world as opposed to a restricted view to rooms arenas etc of limited size employed in many multiplayer game s they need to know information with greater freshness frequency and accuracy as other game entities are located closer and closer to the player s position it prescribes a multidimensional divergence bounding scheme based on a vector field that employs consistency vectors k \xce\xb8 \xcf\x83 \xce\xbd standing for maximum allowed t ime or replica staleness s equence or missing updates and v alue ref group nb sub since in the greek alphabet there was no letter for the vee sound the nu letter was preferred for its resemblance with the roman v for v alue instead of \xce\xb2 beta for the vee sound in contemporary greek speaking sub ref or user defined measured replica divergence applied to all space coordinates in game scenario or world the consistency vector fields emanate from field generators designated as pivots for example players and field strength field intensity attenuates as distance grows from these pivots in concentric or square like regions this consistency model unifies locality awareness techniques employed in message routing and consistency enforcement for multiplayer games with divergence bounding techniques traditionally employed in replicated database and web scenarios notes references group nb references references category data management'
b'the retention period of information is an aspect of records management records and information management rim and the records life cycle it identifies the duration of time for which the information should be maintained or retained irrespective of format paper electronic or other retention periods vary on different types of information based on content and a variety of other factors including internal organizational need regulatory requirements for inspection or audit legal statutes of limitation involvement in litigation taxation and financial reporting needs as well as other factors as defined by local regional state national and or international governing entities once an applicable retention period has elapsed for a given type or series of information and all holds moratoriums have been released the information is typically destroyed using an approved and effective destruction method which renders the information completely and irreversibly unusable via any means information with historical value beyond its usable value may be accessioned to the custody of an archive organization for permanent or extended long term preservation defensible retention defensible retention refers to the ability of an identified and applied retention period to effectively provide for the defense of the record and its eventual destruction or accessioning when scrutinized within a court of law or by other review it is commonly advised by records management records and information management rim professionals that any and all retention periods applied to organizational information should be reviewed and approved for use by competent legal counsel which represents the organization and is familiar with the specific legal and regulatory requirements of the organization guidance and education organizations arma international information and records management society see also retention schedule references references category legal documents category data management category public records category records management'
b'infobox organization name british oceanographic data centre image bodc logo jpg image border size 150px alt british oceanographic data centre caption map msize malt mcaption abbreviation motto formation 1969 extinction type status purpose headquarters location liverpool uk br uk postcodes l3 5da region served membership language leader title head of bodc leader name dr graham allen main organ parent organization natural environment research council nerc affiliations num staff approx 500 num volunteers budget website url http www bodc ac uk remarks the british oceanographic data centre bodc is a national facility for looking after and distributing data about the marine ocean marine environment bodc is the designated marine science data centre for the uk and part of the natural environment research council nerc the centre provides a resource for science education and industry as well as the general public bodc is hosted by the national oceanography centre noc primarily at its facility in liverpool with small number of its staff in southampton file bidston observatory jpg 250px right thumb bidston observatory home of bodc from 1975 to 2004 file joseph proudman building jpg 250px right thumb joseph proudman building liverpool history the origins of bodc go back to 1969 when nerc created the british oceanographic data service bods located at the national institute of oceanography wormley surrey wormley in surrey its purpose was to act as the uk s national oceanographic data centre participate in the international exchange of data as part of the intergovernmental oceanographic commission ioc network of national data centres in 1975 bods was transferred to bidston observatory on the wirral peninsula wirral near liverpool as part of the newly formed institute of oceanographic sciences the following year bods became the marine information and advisory service mias http www soton ac uk library about nol mias html its primary activity was to manage the data collected from weather ships oil rigs and data buoys data buoys the data banking component of mias was restructured to form bodc in april 1989 its mission was to operate as a world class data centre in support of uk marine science bodc pioneered a start to finish approach to marine data management this involved assisting in the collection of data at sea quality control of data assembling the data for use by the scientists the publication of data sets on cd rom in december 2004 bodc moved to the purpose built joseph proudman building on the campus of the university of liverpool a small number of its staff are based in the national oceanography centre noc southampton aims work alongside scientists during marine research projects provide quality control and archiving of oceanographic data maintain an online source of information and improve public access to marine data provide innovative marine data products national role file current meter inventory jpg 250px right thumb bodc current meter data holdings from around the uk bodc is one of six designated data centres that manage nerc s environmental data and has a number of national roles and responsibilities performing data management for nerc funded marine projects maintaining and developing its archive of marine data the national oceanographic database nodb managing checking and archiving data from tide gauge s around the uk coast for the uk national tide gauge network national tide gauge network which aims to obtain high quality tidal information and to provide warning of possible flooding of coastal areas around the british isles this is part of the national tidal and sea level facility national tidal sea level facility ntslf hosting the marine environmental data and information network http www oceannet org medin working in partnership with other nerc marine research centres british antarctic survey bas national oceanography centre noc liverpool formerly proudman oceanographic laboratory pol national oceanography centre noc southampton plymouth marine laboratory pml scottish association for marine science sams sea mammal research unit smru international role bodc s international roles and responsibilities include contributing to the international council for the exploration of the sea ices marine data management creating maintaining and publishing the general bathymetric chart of the oceans gebco digital atlas bodc is one of over 60 national oceanographic data centres that form part of the ioc international oceanographic data and information exchange iode projects and initiatives the following are a selection of the projects that bodc is or has been involved with image rapid mooring jpg 250px right thumb servicing of a rapid mooring atlantic meridional transect amt the amt programme http www bodc ac uk projects uk amt undertook a twice yearly transect between the uk and the falkland islands to study the factors determining the ecological and biogeochemical variability in the planktonic ecosystems autosub under ice aui the aui programme http www bodc ac uk projects uk aui investigated the role of sub ice shelf processes in the climate system the marine environment beneath floating ice shelves was explored using autosub an autonomous underwater vehicle auv marine productivity marprod marprod http www bodc ac uk projects uk marprod helped to develop coupled computer simulation models and observation systems for the pelagic ecosystem with emphasis on the physical factors affecting zooplankton dynamics rapid climate change rapid the rapid programme http www bodc ac uk projects uk rapid aimed to improve understanding of the causes of sudden changes in the earth s climate ocean margin exchange omex the omex project http www bodc ac uk projects european omex studied measured and modelled the physical chemical and biological processes and fluxes at the ocean margin the interface between the open atlantic ocean and the european continental shelf seadatanet seadatanet http www bodc ac uk projects european seadatanet aims to develop a standardised distributed system providing transparent access to marine data sets and data products from countries in and around europe system of industry metocean data for the offshore and research communities simorc simorc http www bodc ac uk projects european simorc aimed to create a central index and database of metocean data sets collected globally by the oil and gas industry vocabulary server bodc operates the nerc vocabulary server web service http www bodc ac uk products web services vocab which provides access to controlled vocabulary controlled vocabularies of relevance to the scientific community external links http www bodc ac uk bodc homepage http www bodc ac uk about news and events bodc news and events http www nerc ac uk natural environment research council nerc homepage http www nerc ac uk research sites data nerc data centres http ndg nerc ac uk discovery nerc data discovery service http www ntslf org national tidal and sea level facility ntslf coord 53 24 27 5 n 2 58 8 2 w type landmark display title category oceanographic organizations category scientific organisations based in the united kingdom category data management category oceanography category marine biology category marine geology category environmental science category environment of the united kingdom category public bodies and task forces of the united kingdom government category 1969 establishments in the united kingdom category scientific organizations established in 1969 category organisations based in liverpool'
b'the association for computing machinery acm sigmod edgar f codd innovations award is a lifetime research achievement award given by the acm special interest group on management of data at its yearly flagship conference also called sigmod according to its homepage it is given for innovative and highly significant contributions of enduring value to the development understanding or use of database systems and databases ref http www sigmod org sigmod awards ref the award has been given since 1992 recipients class wikitable year name 2016 gerhard weikum 2015 laura m haas ref http www sigmod org all news dr laura haas is the recipient of the 2015 sigmod edgar f codd innovation award dr laura haas is the recipient of the 2015 sigmod edgar f codd innovation award sigmod retrieved 2015 06 21 ref 2014 martin l kersten ref cite web url https www cwi nl news 2014 international innovation award big data research martin kersten title international innovation award to martin kersten date 26 june 2014 website cwi amsterdam accessdate 26 june 2014 ref 2013 stefano ceri 2012 bruce lindsay 2011 surajit chaudhuri 2010 http www hpl hp com people umesh dayal umeshwar dayal 2009 masaru kitsuregawa 2008 moshe y vardi 2007 jennifer widom 2006 jeffrey d ullman 2005 michael carey 2004 ronald fagin 2003 don chamberlin 2002 patricia selinger 2001 rudolf bayer 2000 rakesh agrawal computer scientist rakesh agrawal 1999 hector garcia molina 1998 serge abiteboul 1997 david maier 1996 c mohan 1995 david dewitt 1994 phil bernstein philip bernstein 1993 jim gray computer scientist jim gray 1992 michael stonebraker references references category association for computing machinery category awards established in 1992 category computer science awards category data management'
b'notability date july 2015 no footnotes date november 2009 enterprise data management edm is a concept referring to the ability of an organization to precisely define easily integrate and effectively retrieve data for both internal applications and external communication a business objective focused on the creation of accurate consistent and transparent content edm emphasizes data precision granularity and meaning and is concerned with how the content is integrated into business application s as well as how it is passed along from one business process to another edm arose to address circumstances where users within organizations independently source model manage and computer data storage store data uncoordinated approaches by various segments of the organization can result in data conflict s and quality inconsistencies lowering the trustworthiness of the data as it is used for operations and business reporting reporting the goal of edm is trust and confidence in data assets its components are strategy and governance edm requires a strategic approach to choosing the right processes technologies and resources i e data owners governance stewardship data analyst s and data architect s edm is a challenge for organizations because it requires alignment among multiple stakeholders including it operations finance strategy and end user s and relates to an area creation and use of common data that has not traditionally had a clear owner the governance challenge can be a big obstacle to the implementation of an effective edm because of the difficulties associated with providing a business case on the benefits of data management the core of the challenge is due to the fact that data quality has no intrinsic value it is an enabler of other processes and the true benefits of effective data management are systematic and intertwined with other processes this makes it hard to quantify all the downstream implications or upstream improvements the difficulties associated with quantification of edm benefits translate into challenges with the positioning of edm as an organizational priority achieving organizational alignment on the importance of data management as well as managing data as an ongoing area of focus is the domain of governance in recent years the establishment of an edm and the edm governance practice has become commonplace despite these difficulties program implementation implementation of an edm program encompasses many processes all of which need to be coordinated throughout the organization and managed while maintaining operational continuity below are some of the major components of edm implementation that should be given consideration stakeholder requirements edm requires alignment among multiple stakeholders at the right level of authority who all need to understand and support the edm objectives edm begins with a thorough understanding of the requirements of the end users and the organization as a whole managing stakeholder requirements is a critical and ongoing process based in an understanding of workflow data dependencies and the tolerance of the organization for operational disruption many organizations use formal processes such as service level agreement s to specify requirements and establish edm program objectives policies and procedures effective edm usually includes the creation documentation and enforcement of operating policies and procedures associated with change management i e data model business glossary master data shared domains data cleansing and data normalization normalization data stewardship security constraints and dependency rules in many cases these policies and procedures are documented for the first time as part of the edm initiative data definitions and tagging one of the core challenges associated with edm is the ability to compare data that is obtained from multiple internal and external sources in many circumstances these sources use inconsistent terms and definitions to describe the data content itself making it hard to compare data hard to automate business processes hard to feed complex applications and hard to exchange data this frequently results in a difficult process of data mapping and cross referencing normalization of all the terms and definitions at the data attribute level is referred to as the metadata component of edm and is an essential prerequisite for effective data management platform requirements even though edm is fundamentally a data content challenge there is a core technology dimension that must be addressed organizations need to have a functional storage platform a comprehensive data model and a robust messaging infrastructure they must be able to integrate data into applications and deal with the challenges of the existing i e legacy technology infrastructure building the platform or partnering with an established technology provider on how the data gets stored and integrated into business applications is an essential component of the edm process enterprise data management as an essential business requirement has emerged as a priority for many organizations the objective is confidence and trust in data as the glue that holds business strategy together see also master data management master data references reflist general enterprise data management council http www edmcouncil org http www thegoldensource com files edm finextra report final pdf issues in enterprise data management a survey report 12 06 category data management category product lifecycle management'
b'more footnotes date february 2014 data exchange is the process of taking data structured under a source database schema schema and transforming it into data structured under a target schema so that the target data is an accurate representation of the source data ref a doan a halevy and z ives principles of data integration morgan kaufmann 2012 pp 276 ref data exchange allows data to be cross platform shared between different computer program s it is similar to the related concept of data integration except that data is actually restructured with possible loss of content in data exchange there may be no way to transform an instance computer science instance given all of the constraints conversely there may be numerous ways to transform the instance possibly infinitely many in which case a best choice of solutions has to be identified and justified single domain data exchange often there are a few dozen different source and target schema proprietary data formats in some specific domain often people develop an exchange format or interchange format for some single domain and then write a few dozen different routines to indirectly translate each and every source schema to each and every target schema by using the interchange format as an intermediate step that requires a lot less work than writing and debugging the hundreds of different routines that would be required to directly translate each and every source schema directly to each and every target schema for example standard interchange format for geospatial data data interchange format for spreadsheet data gps exchange format or keyhole markup language for indicating gps coordinates on the globe quicken interchange format for financial data gdsii for integrated circuit layout etc citation needed date september 2016 data exchange languages redirect target if you change this fix the redirect too merge to modeling language date may 2016 a data exchange language citation needed date may 2016 is a language that is domain independent and can be used for any kind of data its semantic expression capabilities and qualities are largely determined by comparison with the capabilities of natural languages the term is also applied to any file format that can be read by more than one program including proprietary formats such as microsoft office documents however a file format is not a real language as it lacks a grammar and vocabulary practice has shown that certain types of formal language s are better suited for this task than others since their specification is driven by a formal process instead of a particular software implementation needs for example xml is a markup language that was designed to enable the creation of dialects the definition of domain specific sublanguages and a popular choice now in particular on the internet however it does not contain domain specific dictionaries or fact types beneficial to a reliable data exchange is the availability of standard dictionaries taxonomies and tools libraries such as parser s schema validator s and transformation tools citation needed date september 2016 popular languages used for data exchange the following is a partial list of popular generic languages used for data exchange in multiple domains this currently is very rough and ad hoc feel free to extend and change it please verify definitions for the column headers of the table class wikitable sortable style font size 85 text align center width auto schemas flexible semantic verification dictionary information model synonyms and homonyms dialecting web standard transformations lightweight human readable compatibility rh resource description framework rdf yes ref label feat rdf 1 yes yes yes yes yes yes yes yes yes partial subset of semantic web rh xml yes ref label feat schema 1 yes no no no no yes yes yes no yes subset of sgml html rh atom file format atom yes unk unk unk no unk yes yes yes no no xml dialect rh json no unk unk unk no unk no yes no yes yes subset of yaml rh yaml no ref label feat ext 2 unk unk unk no unk no no no ref label feat ext 2 yes yes ref label feat yaml readable 3 superset of json rh rebol yes ref label feat rebol parse 6 yes no yes no yes yes no yes ref label feat rebol parse 6 yes yes ref label feat rebol readable 4 rh gellish yes yes yes yes ref label feat gellish dict 7 no yes yes iso no yes partial ref label feat gellish readable 5 sql rdf xml owl nomenclature schemas whether the language definition is available in a computer interpretable form flexible whether the language enables extension of the semantic expression capabilities without modifying the schema semantic verification whether the language definition enables semantic verification of the correctness of expressions in the language dictionary taxonomy whether the language includes a dictionary and a taxonomy subtype supertype hierarchy of concepts with inheritance synonyms and homonyms whether the language includes and supports the use of synonyms and homonyms in the expressions dialecting whether the language definition is available in multiple natural languages or dialects web or iso standard organization that endorsed the language as a standard transformations whether the language includes a translation to other standards lightweight whether a lightweight version is available in addition to a full version human readable whether expressions in the language are human readable readable by humans without training citation needed date september 2016 compatibility which other tools are possible or required when using the language citation needed date september 2016 notes note feat rdf rdf is a schema flexible language note feat schema the schema of xml contains a very limited grammar and vocabulary note feat ext available as extension note feat yaml readable in the default format not the compact syntax note feat rebol readable the syntax is fairly simple the language was designed to be human readable the dialects may require domain knowledge note feat gellish readable the standardized fact types are denoted by standardized english phrases which interpretation and use needs some training note feat rebol parse the rebol parse parse dialect is used to specify validate and transform dialects note feat gellish dict the english version includes a gellish english dictionary taxonomy that also includes standardized fact types kinds of relations xml for data exchange the popularity of xml for data exchange on the world wide web has several reasons first of all it is closely related to the preexisting standards standard generalized markup language sgml and hypertext markup language html and as such a parser written to support these two languages can be easily extended to support xml as well for example xhtml has been defined as a format that is formal xml but understood correctly by most if not all html parsers this led to quick adoption of xml support in web browsers and the toolchains used for generating web pages citation needed date september 2016 yaml for data exchange yaml is a language that was designed to be human readable and as such to be easy to edit with any standard text editor its notion often is similar to restructuredtext or a wiki syntax who also try to be readable both by humans and computers yaml 1 2 also includes a shorthand notion that is compatible with json and as such any json document is also valid yaml this however does not hold the other way citation needed date september 2016 rebol for data exchange rebol is a language that was designed to be human readable and easy to edit using any standard text editor to achieve that it uses a simple free form syntax with minimal punctuation and a rich set of datatypes rebol datatypes like urls e mails date and time values tuples strings tags etc respect the common standards rebol is designed to not need any additional meta language being designed in a metacircular fashion the metacircularity of the language is the reason why e g the parse dialect used not exclusively for definitions and transformations of rebol dialects is also itself a dialect of rebol rebol was used as a source of inspiration by the designer of json citation needed date september 2016 gellish for data exchange gellish english is a formalized subset of natural english which includes a simple grammar and a large extensible english dictionary english dictionary taxonomy that defines the general and domain specific terminology terms for concepts whereas the concepts are arranged in a subtype supertype hierarchy a taxonomy which supports inheritance of knowledge and requirements the dictionary taxonomy also includes standardized fact types also called relation types the terms and relation types together can be used to create and interpret expressions of facts knowledge requirements and other information gellish can be used in combination with sql rdf xml web ontology language owl and various other meta languages the gellish standard is being adopted as iso 15926 11 citation needed date september 2016 see also atom file format lightweight markup language rss references reflist refbegin r fagin p kolaitis r miller and l popa data exchange semantics and query answering theoretical computer science 336 1 89 124 2005 p kolaitis schema mappings data exchange and metadata management proceedings of the twenty fourth acm sigmod sigact sigart symposium on principles of database systems pages 61 75 2005 refend data exchange category data management'
b'electronically stored information esi for the purpose of the federal rules of civil procedure frcp is information created manipulated communicated stored and best utilized in digital form requiring the use of computer hardware and software ref name nwjtip http www law northwestern edu journals njtip v4 n2 3 electronically stored information the december 2006 amendments to the federal rules of civil procedure kenneth j withers northwestern journal of technology and intellectual property vol 4 2 171 ref esi has become a legally defined phrase as the federal government of the united states u s government determined for the purposes of the frcp rules of 2006 that promulgating procedures for maintenance and discovery for electronically stored information was necessary references to electronically stored information in the federal rules of civil procedure frcp invoke an expansive approach to what may be discovered during the fact finding stage of civil litigation ref name federal rules of civil procedure cite web title federal rules of civil procedure frcp url https www law cornell edu rules frcp rule 34 website legal information institute lii publisher cornell university law school accessdate october 31 2015 ref rule 34 ref rule 34 a enables a party in a civil lawsuit to request another party to produce and permit the requesting party or its representative to inspect copy test or sample the following items in the responding party s possession custody or control blockquote any designated documents or electronically stored information including writings drawings graphs charts photographs sound recordings images and other data or data compilations stored in any medium from which information can be obtained either directly or if necessary after translation by the responding party into a reasonably usable form rule 34 a 1 is intended to be broad enough to cover all current types of computer based information and flexible enough to encompass future changes and developments blockquote types native files the term native files refers to user created documents which could be in microsoft office or apache openoffice open office document formats as well as other files stored on computer but could include video surveillance footage saved on a computer hard drive computer aided design files such as blueprint s or maps digital photography digital photographs scanned images archive file s e mail and digital audio files among other data logical data a judge ruled that random access memory ram is reasonably accessible and retainable for anticipation of litigation citation needed date november 2011 in australia ram can be used in litigation post 1996 references reflist further reading cite book chapter meet the new rules title the discovery revolution author1 george l paul author2 bruce h nearon publisher american bar association year 2006 isbn 9781590316054 cite book title discovery of electronically stored information author ronald j hedges publisher bna books year 2007 isbn 9781570186721 cite book title the sedona principles 2007 best practices recommendations amp principles for addressing electronic document production author jonathan m redgrave publisher bna books year 2007 isbn 9781570186776 cite book title cyber forensics author1 albert j marcella author2 albert j marcella jr author3 doug menendez chapter electronically stored information and cyber forensics publisher crc press year 2007 isbn 9780849383281 cite book title litigating with electronically stored information author1 marian k riedy author2 suman beros author3 kim sperduto publisher artech house telecommunications library year 2007 isbn 9781596932203 category computer data category data management category united states discovery law category records management us law stub'
b'refimprove date february 2015 ui data binding is a design pattern computer science software design pattern to simplify development of gui applications ui data binding binds ui elements to an application domain model most frameworks employ the observer pattern as the underlying binding mechanism to work efficiently ui data binding has to address data validation input validation and data type mapping a bound control is a gui widget widget whose value is tied or data binding bound to a field in a recordset e g a column database column in a row database row of a table database table changes made to data within the control are automatically saved to the database when the control s exit event trigger s data binding frameworks and tools embarcadero delphi delphi dsharp data binding dsharp 3rd party data binding tool cn date december 2016 openwire library openwire visual live binding 3rd party visual data binding tool java jface data binding javafx property ref https docs oracle com javafx 2 binding jfxpub binding htm ref net windows forms data binding overview windows presentation foundation wpf data binding overview unity 3d data binding framework available in modifications for ngui igui and ezgui libraries cn date december 2016 javascript angularjs backbone js ember js datum js ref cite web url http datumjs com title datum js accessdate 7 november 2016 ref knockout js meteor web framework meteor via its blaze live update engine ref cite web title meteor blaze url https www meteor com blaze quote meteor blaze is a powerful library for creating live updating user interfaces blaze fulfills the same purpose as angular backbone ember react polymer or knockout but is much easier to use we built it because we thought that other libraries made user interface programming unnecessarily difficult and confusing ref openui5 react javascript library react see also data binding references reflist category data management category software design patterns compu prog stub database stub'
b'multiple issues notability events date september 2011 primary sources date september 2011 infobox academic conference history 2002 discipline database abbreviation cidr publisher cidr conference country united states frequency biennial the conference on innovative data systems research cidr is a biennial computer science conference focused on research into new techniques for data management it was started in 2002 by michael stonebraker jim gray computer scientist jim gray and david dewitt and is held at the asilomar conference grounds in pacific grove california cidr focuses on presenting work that is more speculative radical or provocative than what is typically accepted by the traditional database research conferences such as the international conference on very large data bases vldb and the acm sigmod conference see also international conference on very large data bases vldb acm sigmod conference external links http www db cs wisc edu cidr cidr website category data management category computer science conferences database stub compu conference stub'
b'in databases and transaction processing the term locks with ordered sharing comprises several variants of the two phase locking 2pl concurrency control protocol generated by changing the blocking semantics of locks upon serializability view and conflict serializability conflicts one variant is identical to commitment ordering strict co sco strict commitment ordering sco references d agrawal a el abbadi a e lang http portal acm org citation cfm id 627615 the performance of protocols based on locks with ordered sharing ieee transactions on knowledge and data engineering volume 6 issue 5 october 1994 pp 805 818 issn 1041 4347 category data management category databases category concurrency control category transaction processing database stub'
b'metadata controller or mdc is a storage area network san technology for managing file locking space allocation and data access authorization this is needed when several clients are given block level access to the same disk volume computer data storage data storage sharing the abstract for the patent describing this technology can be read http www freepatentsonline com 7448077 html here category data management category telecommunications engineering category storage area networks category local area networks compu storage stub'
b'merge from quickpar date march 2014 infobox file format name parchive extension par par2 p par3 future mime owner creatorcode genre erasure code containerfor containedby extendedfrom extendedto parchive a portmanteau of parity archive and formally known as parity volume set specification ref https www livebusinesschat com smf index php topic 5736 msg38234 msg38234 re correction to parchive on wikipedia reply 3 by yutaka sawada their formal title are parity volume set specification 1 0 and parity volume set specification 2 0 ref is an erasure code system that produces par files for checksum verification of data integrity with the capability to perform data recovery operations that can repair or regenerate corrupted or missing data parchive was originally written to solve the problem of reliable file sharing on usenet ref cite web url http parchive sourceforge net desc title parchive parity archive volume set accessdate 2009 10 29 quote the original idea behind this project was to provide a tool to apply the data recovery capability concepts of raid like systems to the posting and recovery of multi part archives on usenet ref but it is now commonly used for protecting any kind of data from data corruption disc rot data degradation bit rot and accidental or malicious damage despite the name parchive uses more advanced techniques that do not utilize simplistic parity bit parity methods of error detection and correction as of 2014 par1 is obsolete par2 is mature for widespread use and par3 is an experimental version being developed by multipar author yutaka sawada ref http www livebusinesschat com smf index php topic 5098 0 possibility of new par3 file ref ref http www livebusinesschat com smf index php topic 3339 0 question about your usage of par3 ref ref http www livebusinesschat com smf index php topic 5025 msg29912 topicseen msg29912 risk of undetectable intended modification ref ref http www livebusinesschat com smf index php topic 3527 msg8850 topicseen msg8850 par3 specification proposal not finished as of april 2011 ref the original sourceforge parchive project has been inactive since november 9 2010 ref cite web url http sourceforge net projects parchive title parchive parity archive tool accessdate 2012 09 02 ref history parchive was intended to increase the reliability of transferring files via usenet newsgroup s usenet was originally designed for informal conversations and the underlying protocol nntp was not designed to transmit arbitrary binary data another limitation which was acceptable for conversations but not for files was that messages were normally fairly short in length and limited to 7 bit ascii text ref cite ietf title network news transfer protocol rfc 977 sectionname character codes section 2 2 page 5 last1 kantor first1 brian authorlink1 last2 lapsley first2 phil authorlink2 phil lapsley year 1986 month february publisher internet engineering task force ietf accessdate 2009 10 29 ref various techniques were devised to send files over usenet such as uuencode uuencoding and base64 later usenet software allowed 8 bit extended ascii which permitted new techniques like yenc large files were broken up to reduce the effect of a corrupted download but the unreliable nature of usenet remained with the introduction of parchive parity files could be created that were then uploaded along with the original data files if any of the data files were damaged or lost while being propagated between usenet servers users could download parity files and use them to reconstruct the damaged or missing files parchive included the construction of small index files par in version 1 and par2 in version 2 that do not contain any recovery data these indexes contain hash function file hash es that can be used to quickly identify the target files and verify their integrity because the index files were so small they minimized the amount of extra data that had to be downloaded from usenet to verify that the data files were all present and undamaged or to determine how many parity volumes were required to repair any damage or reconstruct any missing files they were most useful in version 1 where the parity volumes were much larger than the short index files these larger parity volumes contain the actual recovery data along with a duplicate copy of the information in the index files which allows them to be used on their own to verify the integrity of the data files if there is no small index file available in july 2001 tobias rieper and stefan wehlus proposed the parity volume set specification and with the assistance of other project members version 1 0 of the specification was published in october 2001 ref cite web url http sourceforge net docman display doc php docid 7273 group id 30568 title parchive parity volume set specification 1 0 accessdate 2009 04 07 last nahas first michael date 2001 10 14 deadurl yes archiveurl https web archive org web 20081220184024 http sourceforge net docman display doc php docid 7273 group id 30568 archivedate december 20 2008 ref par1 used reed solomon error correction to create new recovery files any of the recovery files can be used to rebuild a missing file from an incomplete download version 1 became widely used on usenet but it did suffer some limitations it was restricted to handle at most 255 files the recovery files had to be the size of the largest input file so it did not work well when the input files were of various sizes this limited its usefulness when not paired with the proprietary rar compression tool the recovery algorithm had a bug due to a flaw ref cite web url http web eecs utk edu plank plank papers cs 03 504 html title note correction to the 1997 tutorial on reed solomon coding accessdate 2009 10 29 last plank first james s author2 ding ying date april 2003 ref in the academic paper ref cite web url http web eecs utk edu plank plank papers spe 9 97 html title a tutorial on reed solomon coding for fault tolerance in raid like systems accessdate 2009 10 29 last plank first james s date september 1997 ref on which it was based it was strongly tied to usenet and it was felt that a more general tool might have a wider audience in january 2002 howard fukada proposed that a new par2 specification should be devised with the significant changes that data verification and repair should work on blocks of data rather than whole files and that the algorithm should switch to using 16 bit numbers rather than the 8 bit numbers that par 1 used michael nahas and peter clements took up these ideas in july 2002 with additional input from paul nettle and ryan gallagher who both wrote par1 clients version 2 0 of the parchive specification was published by michael nahas in september 2002 ref cite web url http parchive sourceforge net docs specifications parity volume spec article spec html title parity volume set specification 2 0 accessdate 2009 10 29 last nahas first michael author2 clements peter author3 nettle paul author4 gallagher ryan date 2003 05 11 ref peter clements then went on to write the first two par2 implementations quickpar and par2cmdline abandoned since 2004 paul houle created phpar2 to supersede par2cmdline yutaka sawada created multipar to supersede quickpar sawada maintains par2cmdline to use as multipar s par engine backend on may 10 2014 sawada reported a hash collision security problem in par2cmdline the backend for multipar ref name livebusinesschat com https www livebusinesschat com smf index php topic 5579 0 v1 2 5 3 is public ref blockquote i m not sure this problem can be used for dos attack against automated par2 usage if someone has a skill to forge crc 32 it is possible to make a set of source file and par2 file which freeze a par2 client for several hours blockquote versions versions 1 and 2 of the file format are incompatible however many clients support both parity volume set specification 1 0 for par1 the files f1 f2 fn the parchive consists of an index file f par which is crc type file with no recovery blocks and a number of parity volumes f p01 f p02 etc given all of the original files except for one for example f2 it is possible to create the missing f2 given all of the other original files and any one of the parity volumes alternatively it is possible to recreate two missing files from any two of the parity volumes and so forth ref cite book last wang first wallace authorlink title steal this file sharing book url https books google com books id fgfms5kymmcc pg pt183 accessdate 2009 09 24 edition 1st date 2004 10 25 publisher no starch press location san francisco california isbn 1 59327 050 x pages 164 167 chapter finding movies or tv shows recovering missing rar files with par and par2 files ref par1 supports up to 256 recovery files each recovery file must be the size of the largest input file parity volume set specification 2 0 par2 files generally use this naming extension system filename vol000 01 par2 filename vol001 02 par2 filename vol003 04 par2 filename vol007 06 par2 etc the 01 02 etc in the filename indicates how many blocks it contains and the vol000 vol001 vol003 etc indicates the number of the first recovery block within the par2 file if an index file of a download states that 4 blocks are missing the easiest way to repair the files would be by downloading filename vol003 04 par2 however due to the redundancy filename vol007 06 par2 is also acceptable there is also an index file filename par2 it is identical in function to the small index file used in par1 par2 supports up to 65536 2 sup 16 sup recovery blocks however par2cmdline the official par2 implementation it limited to 32767 blocks at once input files are split into multiple equal sized blocks so that recovery files do not need to be the size of the largest input file although unicode is mentioned in the par2 specification as an option most par2 implementations do not support unicode ref http www quickpar co uk forum viewtopic php id 1065 quickpar forum posting webarchive url https web archive org web 20120302104523 http www quickpar co uk forum viewtopic php id 1065 date march 2 2012 ref directory support is included in the par2 specification but most or all implementations do not support it parity volume set specification 3 0 par3 is a planned improvement over par2 ref cite web url http hp vector co jp authors va021385 title multipar announcement publisher ref ref http www quickpar org uk forum viewtopic php id 1264 quickpar forum posting nbsp status par3 webarchive url https web archive org web 20101127125317 http www quickpar org uk forum viewtopic php id 1264 date november 27 2010 ref ref http www quickpar co uk forum viewtopic php id 1047 quickpar forum posting nbsp par3 specifications webarchive url https web archive org web 20120316104813 http www quickpar co uk forum viewtopic php id 1047 date march 16 2012 ref ref http hp vector co jp authors va021385 par3 spec prop htm par3 proposal webarchive url https web archive org web 20100911002706 http hp vector co jp authors va021385 par3 spec prop htm date september 11 2010 ref the authors intend to fix problems related to creating or repairing when the block count or block size is very high par3 also adds support for including directories file folders in a parchive and unicode characters in file names in addition the authors plan to enable the par3 algorithm to identify files that have been moved or renamed ref http www livebusinesschat com smf index php topic 4751 0 par3 move rename brainstorming ref software windows multipar freeware nbsp builds upon quickpar s features and gui and yutaka sawada s fork of par2cmdline as the par2 backend ref name livebusinesschat com it has support for par3 multithreading software multithreading symmetric multiprocessor system multiple processors and the ability to recurse subfolders multipar is able to add recovery data to zip file format zip and 7 zip ref cite web url https sourceforge net p sevenzip feature requests 1006 title 7 zip publisher ref files with a few minor caveats ref http www livebusinesschat com smf index php topic 4922 0 how to add recovery record to zip or 7 zip archive ref multipar is also verified to work with wine software wine under trueos and may work with other operating systems too ref http www livebusinesschat com smf index php topic 4902 0 multipar works with pcbsd 9 0 ref although the par2 and par3 components are or will be open source the multipar gui on top of them is currently not open source ref https www livebusinesschat com smf index php topic 5402 0 contacted you asking about sourcecode ref download from https www livebusinesschat com smf index php board 396 0 multipar forum quickpar freeware nbsp unmaintained since 2004 superseded by multipar https web archive org web 20110311041855 http chuchusoft com par2 tbb par2 tbb gnu general public license gplv2 nbsp a concurrent multithreaded version of par2cmdline 0 4 using threading building blocks tbb par n rar gnu general public license gpl http paulhoule com phpar2 index php phpar2 nbsp advanced par2cmdline with multithreading and highly optimized assemblercode about 66 faster than quickpar 0 9 1 rarslave gnu general public license gplv2 smartpar freeware nbsp unmaintained since 2002 and obsolete as this application written for microsoft windows only works with the original par1 par parchive format parity files superseded by quickpar it uses reed solomon error correction to create new recovery files smartpar is able to correct errors and recover missing parts of distributed files from par files ref cite book last wang first wallace authorlink title steal this file sharing book url https books google com books id fgfms5kymmcc pg pt183 accessdate 2009 09 24 edition 1st date 2004 10 25 publisher no starch press location san francisco california isbn 1 59327 050 x pages 164 167 chapter finding movies or tv shows recovering missing rar files with par and par2 files ref last stable release 0 13d1 dated start date and age 2002 01 22 ref cite web url http parchive sourceforge net title parchive parity archive tool accessdate 2009 09 26 ref http www wehlus de mirror index html mirror nbsp first par implementation unmaintained since 2001 http parchive sourceforge net original par2cmdline nbsp obsolete https github com parchive par2cmdline par2cmdline by blackikeeagle mac os x https gp home xs4all nl site macpar deluxe html macpar deluxe 4 2 http www unrarx com unrarx https web archive org web 20110311041855 http chuchusoft com par2 tbb par2 tbb is a concurrent multithreaded version of par2cmdline 0 4 using threading building blocks tbb gnu general public license gplv2 or later linux the https github com parchive par2cmdline par2 utility which is a maintained fork of par2cmdline http pypar2 silent blade org index php n main homepage pypar2 1 4 a frontend for par2 http sourceforge net projects parchive gpar2 2 03 https web archive org web 20110311041855 http chuchusoft com par2 tbb par2 tbb is a concurrent multithreaded version of par2cmdline 0 4 using threading building blocks tbb gnu general public license gplv2 or later https github com jkansanen par2cmdline mt par2cmdline mt is another multithreaded version of par2cmdline using openmp gnu general public license gplv2 or later freebsd https web archive org web 20110311041855 http chuchusoft com par2 tbb par2 tbb is a concurrent multithreaded version of par2cmdline 0 4 using threading building blocks tbb gnu general public license gplv2 or later it is available in the freebsd ports system as https www freshports org archivers par2cmdline tbb par2cmdline tbb http parchive sourceforge net par2cmdline is available in the freebsd ports system as https www freshports org archivers par2cmdline par2cmdline posix software for posix conforming operating systems http sourceforge net projects ekpar2 par2 for kde 4 see also data degradation bit rot disc rot data corruption checksum comparison of file archivers some file archivers are capable of integrating parity data into their formats for error detection and correction raid nbsp raid levels at and above raid 5 make use of parity data to detect and repair errors references reflist 30em external links http parchive sourceforge net parchive project full specifications and math behind it http www ydecode com page articles003 htm introduction to par and par2 http www slyck com newsgroups guide par par2 files slyck s guide to the usenet newsgroups par par2 files http www warezfaq com allaboutpar htm another introduction to par and par2 and http www warezfaq com more info htm more information from the same site http www binaries4all com quickpar repair php guide to repair files using par2 https web archive org web 20100912073937 http chuchusoft com par2 tbb par2 tbb http www milow net public projects parnrar parnrar html par n rar http www irasnyder com devel rarslave rarslave category archive formats category data management category usenet'
b'data verification is a process in which different types of data are checked for accuracy and data consistency inconsistencies after data migration is done ref http www datacap com products features verify ref it helps to determine whether data was accurately translated when data is data transfer transferred from one source to another is complete and supports processes in the new system during verification there may be a need for a parallel run of both systems to identify areas of disparity and forestall erroneous data loss a type of data verification is double entry and proofreading data proofreading data involves someone checking the data entered against the original document this is also time consuming and costly references reflist 2 external links http www pcguide com care bu howverification c html pc guide article category data management category data quality'
b'in computer science in the field of database s write read conflict also known as reading uncommitted data is a computational anomaly associated with interleaved execution of transactions given a schedule s math s begin bmatrix t1 t2 r a w a r a w a r b w b com r b w b com end bmatrix math t2 could read a database object a modified by t1 which hasn t committed this is a dirty read t1 may write some value into a which makes the database inconsistent it is possible that interleaved execution can expose this inconsistency and lead to inconsistent final database state violating acid rules strict two phase locking strict 2pl overcomes this inconsistency by locking t2 out from performing a read write on a note however that strict two phase locking strict 2pl can have a number of drawbacks such as the possibility of deadlock s see also concurrency control read write conflict write write conflict references reflist unreferenced date august 2009 defaultsort write read conflict category data management category transaction processing'
b'this flow diagrams in business process modeling clarify reason from the lead i m unable to give a more informative characterization of control flow diagram date january 2014 directed graphs representing the control flow of imperative computer programs control flow graph file performance seeking control flow diagram jpg thumb 240px example of a performance seeking control flow diagram ref name go92 glenn b gilyard and john s orme 1992 http www nasa gov centers dryden pdf 88262main h 1808 pdf subsonic flight test evaluationof a performance seeking controlalgorithm on an f 15 airplane nasa technical memorandum 4400 ref a control flow diagram cfd is a diagram to describe the control flow of a business process process engineering process or review control flow diagrams were developed in the 1950s and are widely used in multiple engineering disciplines they are one of the classic business process modeling methodologies along with flow chart s data flow diagram s functional flow block diagram gantt chart s pert diagrams and idef ref name td03 thomas dufresne james martin 2003 http mason gmu edu tdufresn paper doc process modeling for e business infs 770 methods for information systems engineering knowledge management and e business spring 2003 ref overview a control flow diagram can consist of a subdivision to show sequential steps with if then else conditions repetition and or case conditions suitably annotated geometrical figures are used to represent operations data or equipment and arrows are used to indicate the sequential flow from one to another ref http www fda gov ora inspect ref igs gloss html fda glossary of terminology applicable to software development and computerized systems accessed 14 jan 2008 ref there are several types of control flow diagrams for example change control flow diagram used in project management configuration decision control flow diagram used in configuration management process control flow diagram used in process management quality control flow diagram used in quality control in software and systems development control flow diagrams can be used in control flow analysis data flow analysis algorithm analysis and simulation control and data are most applicable for real time and data driven systems these flow analyses transform logic and data requirements text into graphic flows which are easier to analyze than the text pert state transition and transaction diagrams are examples of control flow diagrams ref dolores r wallace et al 1996 http hissa nist gov hhrfdata artifacts itldoc 234 val proc html reference information for the software verification and validation process nist special publication 500 234 ref types of control flow diagrams process control flow diagram a flow diagram can be developed for the process control system for each critical activity process control is normally a closed cycle in which a sensor provides information to a process control software application through a communications system the application determines if the sensor information is within the predetermined or calculated data parameters and constraints the results of this comparison are fed to an actuator which controls the critical component this feedback may control the component electronically or may indicate the need for a manual action ref name nioj02 national institute of justice 2002 http www ncjrs gov txtfiles1 nij 195171 txt a method to assess the vulnerability of u s chemical facilities series special report ref this closed cycle process has many checks and balances to ensure that it stays safe the investigation of how the process control can be subverted is likely to be extensive because all or part of the process control may be oral instructions to an individual monitoring the process it may be fully computer controlled and automated or it may be a hybrid in which only the sensor is automated and the action requires manual intervention further some process control systems may use prior generations of hardware and software while others are state of the art ref name nioj02 performance seeking control flow diagram the figure presents an example of a performance seeking control flow diagram of the algorithm the control law consists of estimation modeling and optimization processes in the kalman filter estimator the inputs outputs and residuals were recorded at the compact propulsion system modeling stage all the estimated inlet and engine parameters were recorded ref name go92 in addition to temperatures pressures and control positions such estimated parameters as stall margins thrust and drag components were recorded in the optimization phase the operating condition constraints optimal solution and linear programming health status condition codes were recorded finally the actual commands that were sent to the engine through the deec were recorded ref name go92 see also data flow diagram control flow graph drakon flow process chart references nist pd reflist category information systems category data management category diagrams category systems analysis defaultsort control flow diagram'
b'consumer relationship systems crs are specialized customer relationship management crm software applications that are used to handle a company s dealings with its customers ref name insight44 50 http www nxtbook com nxtbooks cmp cmi 200709 index php icmi customer management insight magazine september 2007 pp 44 50 retrieved 11 january 2012 ref current consumer relationship systems integrate the software with telephone and call recording systems as well as with corporate systems for input and reporting customers can provide input from the company s website directly into the crs these systems are popular because they can deliver the voice of the consumer that contributes to product quality improvement and that ultimately increases corporate profits ref name insight44 50 consumer relationship systems that provide automated support as well as advanced systems may have artificial intelligence ai interfaces that can extract and analyse data collected or handle basic questions and complaints ref cite web last1 smith first1 s e title what is consumer relationship system date publisher wisegeek net url http www wisegeek net what is consumer relationship system htm accessdate 1 february 2013 ref history the first crs was developed in the 1980s in 1981 michael wilke and robert thornton founded wilke thornton inc in columbus ohio to develop new crs software ref http www wilke thornton com wti pages products html wilke thornton inc products retrieved 11 january 2012 ref see also ecrm business intelligence customer experience customer intelligence customer service contains iso standards customer value maximization enterprise relationship management erm sales force management system sales intelligence sales process engineering references reflist category business intelligence category data management'
b'a mobile content management system mcms is a type of content management system cms capable of storing and delivering content and services to mobile devices such as mobile phones smart phones and pdas mobile content management systems may be discrete systems or may exist as features modules or add ons of larger content management systems capable of multi channel content delivery mobile content delivery has unique specific constraints including widely variable device capacities small screen size limited wireless bandwidth small storage capacity and comparatively weak device processors ref http www insight corp com 5cexecsummaries 5ccontent08execsum pdf content management for wireless networks 2008 2013 insight research report ref demand for mobile content management increased as mobile devices became increasingly ubiquitous and sophisticated mcms technology initially focused on the business to consumer b2c mobile market place with ringtones games text messaging news and other related content since mobile content management systems have also taken root in business to business b2b and business to employee b2e situations allowing companies to provide more timely information and functionality to business partners and mobile workforces in an increasingly efficient manner a 2008 estimate put global revenue for mobile content management at us 8 billion ref http www wirelessweek com content management systems mobile embrace aspx content management systems mobile embrace by evan koblentz wirelessweek august 28 2008 ref key features multi channel content delivery multi channel content delivery capabilities allow users to manage a central content repository while simultaneously delivering that content to mobile devices such as mobile phones smartphones tablets and other mobile devices content can be stored in a raw format such as microsoft word excel powerpoint pdf text html etc to which device specific presentation styles can be applied ref http www apoorv info 2007 05 26 content management for mobile delivery content management for mobile delivery posted by apoorv pcm blog may 26 2007 ref content access control access control includes authorization authentication access approval to each content in many cases the access control also includes download control wipe out for specific user time specific access for the authentication mcm shall have basic authentication which has user id and password for higher security many mcm supports ip authentication and mobile device authentication specialized templating system while traditional web content management systems handle templates for only a handful of web browsers mobile cms templates must be adapted to the very wide range of target devices with different capacities and limitations there are two approaches to adapting templates multi client and multi site the multi client approach makes it possible to see all versions of a site at the same domain e g sitename com and templates are presented based on the device client used for viewing the multi site approach displays the mobile site on a targeted sub domain e g mobile sitename com location based content delivery location based content delivery provides targeted content such as information advertisements maps directions and news to mobile devices based on current physical location currently gps global positioning system navigation systems offer the most popular location based service s navigation systems are specialized systems but incorporating mobile phone functionality makes greater exploitation of location aware content delivery possible see also mobile web content management web content management system enterprise content management apache mobile filter references references category content management systems category mobile web category data management'
b'data aggregation is the compiling of information from databases with intent to prepare combined datasets for data processing ref cite journal author1 stanley jay author2 steinhardt barry title bigger monster weaker chains the growth of an american surveillance society publisher american civil liberties union date january 2003 ref description the source information for data aggregation may originate from public records and criminal databases the information is packaged into aggregate reports and then sold to business es as well as to local government local state government state and government agencies this information can also be useful for marketing purposes in the united states many data brokers activities fall under the fair credit reporting act fcra which regulates credit bureau consumer reporting agencies the agencies then gather and package personal information into consumer reports that are sold to creditor s employer s insurer s and other businesses various reports of information are provided by database aggregators individuals may request their own consumer reports which contain basic biographical information such as name date of birth current address and phone number employee background check reports which contain highly detailed information such as past addresses and length of residence professional licensure licenses and criminal history may be requested by eligible and qualified third parties not only can this data be used in employee background checks but it may also be used to make decisions about insurance coverage pricing and law enforcement privacy activists argue that database aggregators can provide erroneous information ref cite web url http www privacyactivism org docs dataaggregatorsstudy html title data aggregators a study of data quality and responsiveness author1 pierce deborah author2 ackerman linda publisher privacyactivism org date 2005 05 19 accessdate 2007 04 02 archiveurl https web archive org web 20070319220412 http www privacyactivism org docs dataaggregatorsstudy html archivedate 2007 03 19 deadurl yes df ref role of the internet the potential of the internet to consolidate and manipulate information has a new application in data aggregation also known as screen scraping the internet gives users the opportunity to consolidate their username s and password s or pins such consolidation enables consumers to access a wide variety of pin protected website s containing personal information by using one master pin on a single website online account providers include financial institution s stockbroker s airline and frequent flyer and other reward programs and e mail accounts data aggregators can gather account or other information from designated websites by using account holders pins and then making the users account information available to them at a single website operated by the aggregator at an account holder s request aggregation services may be offered on a standalone basis or in conjunction with other financial services such as portfolio finance portfolio tracking and bill payment bill payment provided by a specialized website or as an additional service to augment the online presence of an enterprise established beyond the virtual world many established companies with an internet presence appear to recognize the value of offering an aggregation service to enhance other web based services and attract visitors offering a data aggregation service to a website may be attractive because of the potential that it will frequently draw users of the service to the hosting website local business data aggregation when it comes to compiling location information on local businesses there are several major data aggregators that collect information such as the business name address phone number website description and hours of operation they then validate this information using various validation methods once the business information has been verified to be accurate the data aggregators make it available to publishers like google and yelp when yelp for example goes to update their yelp listings they will pull data from these local data aggregators publishers take local business data from different sources and compare it to what they currently have in their database they then update their database it with what information they deem accurate legal implications financial institutions are concerned about the possibility of legal liability liability arising from data aggregation activities potential security problems infringement on intellectual property rights and the possibility of diminishing traffic to the institution s website the aggregator and financial institution may agree on a data feed arrangement activated on the customer s request using an open financial exchange ofx standard to request and deliver information to the site selected by the customer as the place from which they will view their account data agreements provide an opportunity for institutions to negotiate to protect their customers interests and offer aggregators the opportunity to provide a robust service aggregators who agree with information providers to extract data without using an ofx standard may reach a lower level of consensual relationship therefore screen scraping may be used to obtain account data but for business or other reasons the aggregator may decide to obtain prior consent and negotiate the terms on which customer data is made available screen scraping without consent by the content provider has the advantage of allowing subscribers to view almost any and all accounts they happen to have opened anywhere on the internet through one website outlook over time the transfer of large amounts of account data from the account provider to the aggregator s server could develop into a comprehensive profile of a user detailing their banking and credit card transactions balances securities transactions and portfolios and travel history and preferences as the sensitivity to data protection considerations grows it is likely there will be a considerable focus on the extent to which data aggregators may seek to use this data either for their own purposes or to share it on some basis with the operator of a website on which the service is offered or with other third parties ref cite web url http www ffhsj com bancmail bmarts aba art htm title scrape it scrub it and show it the battle over data aggregation author1 ledig robert h author2 vartanian thomas p publisher fried frank date 2002 09 11 accessdate 2007 04 02 ref references references defaultsort data aggregator category data management category information privacy category data laws'
b'refimprove date january 2009 in database systems isolation determines how transaction integrity is visible to other users and systems for example when a user is creating a purchase order and has created the header but not the purchase order lines is the header available for other systems users carrying out concurrency computer science concurrent operations such as a report on purchase orders to see a lower isolation level increases the ability of many users to access data at the same time but increases the number of concurrency effects such as dirty reads or lost updates users might encounter conversely a higher isolation level reduces the types of concurrency effects that users may encounter but requires more system resources and increases the chances that one transaction will block another ref isolation levels in the database engine technet microsoft http technet microsoft com en us library ms189122 v sql 105 aspx ref isolation is typically defined at database level as a property that defines how when the changes made by one operation become visible to other on older systems it may be implemented systemically for example through the use of temporary tables in two tier systems a transaction processing tp manager is required to maintain isolation in n tier systems such as multiple websites attempting to book the last seat on a flight a combination of stored procedures and transaction management is required to commit the booking and send confirmation to the customer ref the architecture of transaction processing systems chapter 23 evolution of processing systems department of computer science stony brook university retrieved 20 march 2014 http www cs sunysb edu liu cse315 23 pdf ref isolation is one of the acid atomicity consistency isolation durability properties concurrency control concurrency control comprises the underlying mechanisms in a dbms which handles isolation and guarantees related correctness it is heavily utilized by the database and storage engines see above both to guarantee the correct execution of concurrent transactions and different mechanisms the correctness of other dbms processes the transaction related mechanisms typically constrain the database data access operations timing schedule computer science transaction schedules to certain orders characterized as the serializability and recoverability schedule properties constraining database access operation execution typically means reduced performance rates of execution and thus concurrency control mechanisms are typically designed to provide the best performance possible under the constraints often when possible without harming correctness the serializability property is compromised for better performance however recoverability cannot be compromised since such typically results in a quick database integrity violation two phase locking is the most common transaction concurrency control method in dbmss used to provide both serializability and recoverability for correctness in order to access a database object a transaction first needs to acquire a lock database lock for this object depending on the access operation type e g reading or writing an object and on the lock type acquiring the lock may be blocked and postponed if another transaction is holding a lock for that object isolation levels of the four acid properties in a database management system dbms database management system the isolation property is the one most often relaxed when attempting to maintain the highest level of isolation a dbms usually acquires lock database locks on data or implements multiversion concurrency control which may result in a loss of concurrency computer science concurrency this requires adding logic for the software application application to function correctly most dbmss offer a number of transaction isolation levels which control the degree of locking that occurs when selecting data for many database applications the majority of database transactions can be constructed to avoid requiring high isolation levels e g serializable level thus reducing the locking overhead for the system the programmer must carefully analyze database access code to ensure that any relaxation of isolation does not cause software bugs that are difficult to find conversely if higher isolation levels are used the possibility of deadlock is increased which also requires careful analysis and programming techniques to avoid the isolation levels defined by the american national standards institute ansi international organization for standardization iso sql standard are listed as follows serializable this is the highest isolation level with a lock based concurrency control dbms implementation serializability requires read and write locks acquired on selected data to be released at the end of the transaction also range locks must be acquired when a select sql select query uses a ranged where clause especially to avoid the phantom reads phenomenon see below when using non lock based concurrency control no locks are acquired however if the system detects a write collision among several concurrent transactions only one of them is allowed to commit see snapshot isolation for more details on this topic from second informal review draft iso iec 9075 1992 database language sql july 30 1992 the execution of concurrent sql transactions at isolation level serializable is guaranteed to be serializable a serializable execution is defined to be an execution of the operations of concurrently executing sql transactions that produces the same effect as some serial execution of those same sql transactions a serial execution is one in which each sql transaction executes to completion before the next sql transaction begins repeatable reads in this isolation level a lock based concurrency control dbms implementation keeps read and write locks acquired on selected data until the end of the transaction however range locks are not managed so isolation database systems phantom reads phantom reads can occur also write skew is possible when one transaction updates column to some color whereas competing transactions updates the same column to some other color s in serial execution of the transactions you should end up with the whole column unicolored whereas repeatable read admits a mixture of colors ref https wiki postgresql org wiki ssi simple write skew postgresql wiki ssi ref read committed in this isolation level a lock based concurrency control dbms implementation keeps write locks acquired on selected data until the end of the transaction but read locks are released as soon as the select sql select operation is performed so the non repeatable reads phenomenon can occur in this isolation level as discussed below as in the previous level range locks are not managed putting it in simpler words read committed is an isolation level that guarantees that any data read is committed at the moment it is read it simply restricts the reader from seeing any intermediate uncommitted dirty read it makes no promise whatsoever that if the transaction re issues the read it will find the same data data is free to change after it is read read uncommitted this is the lowest isolation level in this level isolation database systems dirty reads dirty reads are allowed so one transaction may see not yet committed changes made by other transactions since each isolation level is stronger than those below in that no higher isolation level allows an action forbidden by a lower one the standard permits a dbms to run a transaction at an isolation level stronger than that requested e g a read committed transaction may actually be performed at a repeatable read isolation level default isolation level the default isolation level of different database management system dbms s varies quite widely most databases that feature transactions allow the user to set any isolation level some dbms s also require additional syntax when performing a select statement to acquire locks e g select for update to acquire exclusive write locks on accessed rows however the definitions above have been criticized as being ambiguous and as not accurately reflecting the isolation provided by many databases this paper shows a number of weaknesses in the anomaly approach to defining isolation levels the three ansi phenomena are ambiguous and even in their loosest interpretations do not exclude some anomalous behavior this leads to some counter intuitive results in particular lock based isolation levels have different characteristics than their ansi equivalents this is disconcerting because commercial database systems typically use locking implementations additionally the ansi phenomena do not distinguish between a number of types of isolation level behavior that are popular in commercial systems ref name sql isolation cite web url http www cs umb edu poneil iso pdf title a critique of ansi sql isolation levels accessdate 29 july 2012 ref there are also other criticisms concerning ansi sql s isolation definition in that it encourages implementors to do bad things it relies in subtle ways on an assumption that a locking schema is used for concurrency control as opposed to an optimistic or multi version concurrency scheme this implies that the proposed semantics are ill defined ref cite web accessdate 2010 03 09 publisher datastax location www datastax com title customer testimonials simplegeo cloudstock 2010 author salesforce date 2010 12 06 url https www youtube com v 7j61ppg9j90 version 3 quote see above at about 13 30 minutes of the webcast ref read phenomena the ansi iso standard sql 92 refers to three different read phenomena when transaction 1 reads data that transaction 2 might have changed in the following examples two transactions take place in the first query 1 is performed then in the second transaction query 2 is performed and committed finally in the first transaction query 1 is performed again the queries use the following data table class wikitable users id name age 1 joe 20 2 jill 25 dirty reads a dirty read aka uncommitted dependency occurs when a transaction is allowed to read data from a row that has been modified by another running transaction and not yet committed dirty reads work similarly to isolation database systems non repeatable reads non repeatable reads however the second transaction would not need to be committed for the first query to return a different result the only thing that may be prevented in the read uncommitted isolation level is updates appearing out of order in the results that is earlier updates will always appear in a result set before later updates in our example transaction 2 changes a row but does not commit the changes transaction 1 then reads the uncommitted data now if transaction 2 rolls back its changes already read by transaction 1 or updates different changes to the database then the view of the data may be wrong in the records of transaction 1 style font size 94 transaction 1 transaction 2 source lang sql query 1 select age from users where id 1 will read 20 source source lang sql query 2 update users set age 21 where id 1 no commit here source source lang sql query 1 select age from users where id 1 will read 21 source source lang sql rollback lock based dirty read source but in this case no row exists that has an id of 1 and an age of 21 non repeatable reads a non repeatable read occurs when during the course of a transaction a row is retrieved twice and the values within the row differ between reads non repeatable reads phenomenon may occur in a lock based concurrency control method when read locks are not acquired when performing a select sql select or when the acquired locks on affected rows are released as soon as the select operation is performed under the multiversion concurrency control method non repeatable reads may occur when the requirement that a transaction affected by a commit conflict must roll back is relaxed style font size 94 transaction 1 transaction 2 source lang sql query 1 select from users where id 1 source source lang sql query 2 update users set age 21 where id 1 commit in multiversion concurrency control or lock based read committed source source lang sql query 1 select from users where id 1 commit lock based repeatable read source in this example transaction 2 commits successfully which means that its changes to the row with id 1 should become visible however transaction 1 has already seen a different value for age in that row at the serializable and repeatable read isolation levels the dbms must return the old value for the second select at read committed and read uncommitted the dbms may return the updated value this is a non repeatable read there are two basic strategies used to prevent non repeatable reads the first is to delay the execution of transaction 2 until transaction 1 has committed or rolled back this method is used when locking is used and produces the serial schedule computer science schedule t1 t2 a serial schedule exhibits repeatable reads behaviour in the other strategy as used in multiversion concurrency control transaction 2 is permitted to commit first which provides for better concurrency however transaction 1 which commenced prior to transaction 2 must continue to operate on a past version of the database nbsp a snapshot of the moment it was started when transaction 1 eventually tries to commit the dbms checks if the result of committing transaction 1 would be equivalent to the schedule t1 t2 if it is then transaction 1 can proceed if it cannot be seen to be equivalent however transaction 1 must roll back with a serialization failure using a lock based concurrency control method at the repeatable read isolation mode the row with id 1 would be locked thus blocking query 2 until the first transaction was committed or rolled back in read committed mode the second time query 1 was executed the age would have changed under multiversion concurrency control at the serializable isolation level both select queries see a snapshot of the database taken at the start of transaction 1 therefore they return the same data however if transaction 1 then attempted to update that row as well a serialization failure would occur and transaction 1 would be forced to roll back at the read committed isolation level each query sees a snapshot of the database taken at the start of each query therefore they each see different data for the updated row no serialization failure is possible in this mode because no promise of serializability is made and transaction 1 will not have to be retried phantom reads a phantom read occurs when in the course of a transaction two identical queries are executed and the collection of rows returned by the second query is different from the first this can occur when range locks are not acquired on performing a select sql select where operation the phantom reads anomaly is a special case of non repeatable reads when transaction 1 repeats a ranged select where query and between both operations transaction 2 creates i e insert new rows in the target table which fulfill that where clause style font size 95 transaction 1 transaction 2 source lang sql query 1 select from users where age between 10 and 30 source source lang sql query 2 insert into users id name age values 3 bob 27 commit source source lang sql query 1 select from users where age between 10 and 30 commit source note that transaction 1 executed the same query twice if the highest level of isolation were maintained the same set of rows should be returned both times and indeed that is what is mandated to occur in a database operating at the sql serializable isolation level however at the lesser isolation levels a different set of rows may be returned the second time in the serializable isolation mode query 1 would result in all records with age in the range 10 to 30 being locked thus query 2 would block until the first transaction was committed in repeatable read mode the range would not be locked allowing the record to be inserted and the second execution of query 1 to include the new row in its results isolation levels read phenomena and locks isolation levels vs read phenomena class wikitable isolation level dirty reads non repeatable reads phantoms read uncommitted may occur may occur may occur read committed may occur may occur repeatable read may occur serializable anomaly serializable is not the same as serializable that is it is necessary but not sufficient that a serializable schedule should be free of all three phenomena types ref name sql isolation may occur means that the isolation level suffers that phenomenon while means that it does not suffer it isolation levels vs lock duration citation needed reason this is a still a mess write operations always place locks until commit whatever isolation level is used queries selects reads never place any kind of lock under read uncommitted isolation level locks placed by read operations the only operations that are affected by the isolation level used should not be referred as read and range but as data locks and predicate locks write operations also place data and predicate locks always until commit whatever isolation level is used date january 2014 in lock based concurrency control isolation level determines the duration that locks are held br c denotes that locks are held until the transaction commits br s denotes that the locks are held only during the currently executing statement note that if locks are released after a statement the underlying data could be changed by another transaction before the current transaction commits thus creating a violation class wikitable isolation level write operation read operation range operation where read uncommitted s s s read committed c s s repeatable read c c s serializable c c c see also atomicity database systems atomicity consistency database systems consistency durability database systems durability lock database optimistic concurrency control relational database management system snapshot isolation references reflist external links http docs oracle com cd b12037 01 server 101 b10743 toc htm oracle\xc2\xae database concepts http docs oracle com cd b12037 01 server 101 b10743 consist htm sthref1919 chapter 13 data concurrency and consistency preventable phenomena and transaction isolation levels http docs oracle com cd b19306 01 server 102 b14200 toc htm oracle\xc2\xae database sql reference http docs oracle com cd b19306 01 server 102 b14200 statements 10 htm i2068385 chapter 19 sql statements savepoint to update http docs oracle com cd b19306 01 server 102 b14200 statements 10005 htm i2067247 set transaction representations in api java in java database connectivity jdbc http docs oracle com javase 7 docs api java sql connection html field summary connection constant fields http docs oracle com javase 7 docs api java sql connection html gettransactionisolation connection gettransactionisolation http docs oracle com javase 7 docs api java sql connection html settransactionisolation int connection settransactionisolation int in spring framework http static springsource org spring docs current javadoc api org springframework transaction annotation transactional html transactional http static springsource org spring docs current javadoc api org springframework transaction annotation isolation html isolation representations in api net framework http www bailis org blog when is acid acid rarely p bailis when is acid acid rarely authority control defaultsort isolation database systems category data management category transaction processing'
b'multiple issues unreferenced date november 2008 confusing date august 2009 cleanup rewrite date august 2009 in data management semantic warehousing is a methodology of digitalized text data using similar functions to data warehousing dw such as etl extract transform load ods operational data store and model value computer science key value operation is less useful for the digitalized text semantic warehousing is different from dw in that semantic information base from text semantic data semantic warehousing is different from search engine in that semantic information base from text data is stored in the database dbms though data is most important word in computing era it can not explain human knowledge well yet data numeric data is key element of computing systems for certain organization especially companies enterprises but no performance oriented organization needs something to gather and use knowledge or human feeling semantic warehousing will be equally or more important than data warehousing in the future definition semantic warehousing is a conceptual and functional term meaning to gather from a source semantically defining and providing information from digitalized text type data background data warehousing dw is popular these days gathering data from systems that generate transactions data warehouses become a base of information key of data warehouse is a model called datamart and that model is made up of dimensions key and measures value users get information from the models by doing certain operations online analytical processing olap is most the important operation for the users to get information from the dw models handling dimensions with pivoting drilling slice dice operations users get numeric values like sales amounts growth rates etc various areas of this world defined and appeared on the world wide web internet eager to present their contents in a semantic way briefly speaking semantic warehousing has datawarehousing boby and search head and ontology features data warehousing contributed to companies business values and lots of solutions and tools are commercially successful analysis of internal data delivers a certain level of business values on the contrary to this semantic warehousing environment has not yet matured capacity of social data is increasing rapidly and various efforts of finding value from that data are made widely known as big data etc semantic warehousing can be the mainstream of treat data and intelligence of social world in the future though it is defined with other keywords at the big data era semantic processing is going to become major it process semantic warehousing is digital infra of intelligence practices \xe2\x96\xa3 medical area clinical information some hospital implement semantic warehousing for clinic al information swci medical information is now knowledge network level umls define semantic knowledge network of medical language currently medical information stored in database and not fully used for clinic semantic warehousing is next stage of digitalized medical information swci is a name of conceptual system of clinical information br named by juhan kim snuh seoul national university hospital and bohyon hwang yongchan keum in 2008 defined architecture on swci br 1 semantic oriented cleansing br 2 semantic oriented meta management br 3 clinical medical knowledge basement br 4 semantic oriented user intelligence \xe2\x96\xa3 intelligence area at the point of big data usage intelligence reporting can be valuable results 1 source information br 2 manage intelligence semantic data br 3 intelligence service use http www globalintelligence kr gibigdata connected area big data br semantic web br ontology br knowledge br medical and healthcare emr electronic medical record electronic medical record ehr electronic health record electronic health record br data warehouse br ai artificial intelligence references http www snubi org bi laboratory of seoul national university hospital smith barry kumar anand and schulze kremer steffen 2004 http ontology buffalo edu medo umls sn pdf revising the umls semantic network in m fieschi et al eds medinfo 2004 amsterdam ios press 1700 foundations of data warehouse quality data quality article mentioning that semantically rich dw http www cs brown edu courses cs227 papers projects iq97 dwq pdf an integrative and uniform model for metadata management in data warehousing environment semantic metadata and technical metadata http ftp informatik rwth aachen de publications ceur ws vol 19 paper12 pdf effective query expansion using condensed umls metathesaurus for medical information retrieval http www e hir org journal view html uid 201 start sort scale key all oper key word umls year1 year2 vol num pg book mod vol sflag sub box y aut box y sos box pub box y key box abs box year a study of effective unified medical language system concept indexing in radiology reports http www e hir org journal view html uid 226 start sort scale key all oper key word umls year1 year2 vol num pg book mod vol sflag sub box y aut box y sos box pub box y key box abs box year developing a reference terminology model for health care using an object oriented approach http www e hir org journal view html uid 311 start sort scale key all oper key word umls year1 year2 vol num pg book mod vol sflag sub box y aut box y sos box pub box y key box abs box year umls unified medical language system \xec\x9d\x98 \xec\xa6\x9d\xec\x83\x81\xec\x9a\xa9\xec\x96\xb4\xec\x99\x80 \xea\xb5\xad\xeb\x82\xb4\xec\x9d\x98\xeb\xac\xb4\xea\xb8\xb0\xeb\xa1\x9d\xec\x97\x90\xec\x84\x9c \xec\x82\xac\xec\x9a\xa9\xeb\x90\x98\xeb\x8a\x94 \xec\xa6\x9d\xec\x83\x81\xec\x9a\xa9\xec\x96\xb4\xec\x99\x80\xec\x9d\x98 \xeb\xb9\x84\xea\xb5\x90\xec\x97\xb0\xea\xb5\xac http www e hir org journal view html uid 922 start sort scale key all oper key word umls year1 year2 vol num pg book mod vol sflag sub box y aut box y sos box pub box y key box abs box year category data management'
b'redirect 2pc the play in american and canadian football two point conversion the cryptographic protocol commitment scheme in transaction processing database s and computer networking the two phase commit protocol 2pc is a type of atomic commit atomic commitment protocol acp it is a distributed algorithm that coordinates all the processes that participate in a distributed transaction distributed atomic transaction on whether to commit data management commit or abort roll back the transaction it is a specialized type of consensus computer science consensus protocol the protocol achieves its goal even in many cases of temporary system failure involving either process network node communication etc failures and is thus widely used ref name bernstein1987 phil bernstein philip a bernstein vassos hadzilacos nathan goodman 1987 http research microsoft com en us people philbe ccontrol aspx concurrency control and recovery in database systems chapter 7 addison wesley publishing company isbn 0 201 10715 5 ref ref name weikum2001 gerhard weikum gottfried vossen 2001 http www elsevier com wps find bookdescription cws home 677937 description description transactional information systems chapter 19 elsevier isbn 1 55860 508 8 ref ref name bern2009 philip a bernstein eric newcomer 2009 http www elsevierdirect com product jsp isbn 9781558606234 principles of transaction processing 2nd edition chapter 8 morgan kaufmann elsevier isbn 978 1 55860 623 4 ref however it is not resilient to all possible failure configurations and in rare cases user e g a system s administrator intervention is needed to remedy an outcome to accommodate recovery from failure automatic in most cases the protocol s participants use server log logging of the protocol s states log records which are typically slow to generate but survive failures are used by the protocol s recovery procedure s many protocol variants exist that primarily differ in logging strategies and recovery mechanisms though usually intended to be used infrequently recovery procedures compose a substantial portion of the protocol due to many possible failure scenarios to be considered and supported by the protocol in a normal execution of any single distributed transaction i e when no failure occurs which is typically the most frequent situation the protocol consists of two phases the commit request phase or voting phase in which a coordinator process attempts to prepare all the transaction s participating processes named participants cohorts or workers to take the necessary steps for either committing or aborting the transaction and to vote either yes commit if the transaction participant s local portion execution has ended properly or no abort if a problem has been detected with the local portion and the commit phase in which based on voting of the cohorts the coordinator decides whether to commit only if all have voted yes or abort the transaction otherwise and notifies the result to all the cohorts the cohorts then follow with the needed actions commit or abort with their local transactional resources also called recoverable resources e g database data and their respective portions in the transaction s other output if applicable note that the two phase commit 2pc protocol should not be confused with the two phase locking 2pl protocol a concurrency control protocol assumptions the protocol works in the following manner one node is a designated coordinator which is the master site and the rest of the nodes in the network are designated the cohorts the protocol assumes that there is stable storage at each node with a write ahead logging write ahead log that no node crashes forever that the data in the write ahead log is never lost or corrupted in a crash and that any two nodes can communicate with each other the last assumption is not too restrictive as network communication can typically be rerouted the first two assumptions are much stronger if a node is totally destroyed then data can be lost the protocol is initiated by the coordinator after the last step of the transaction has been reached the cohorts then respond with an agreement message or an abort message depending on whether the transaction has been processed successfully at the cohort basic algorithm commit request phase or voting phase the coordinator sends a query to commit message to all cohorts and waits until it has received a reply from all cohorts the cohorts execute the transaction up to the point where they will be asked to commit they each write an entry to their undo log and an entry to their redo log each cohort replies with an agreement message cohort votes yes to commit if the cohort s actions succeeded or an abort message cohort votes no not to commit if the cohort experiences a failure that will make it impossible to commit commit phase or completion phase success if the coordinator received an agreement message from all cohorts during the commit request phase the coordinator sends a commit message to all the cohorts each cohort completes the operation and releases all the locks and resources held during the transaction each cohort sends an acknowledgment to the coordinator the coordinator completes the transaction when all acknowledgments have been received failure if any cohort votes no during the commit request phase or the coordinator s timeout expires the coordinator sends a rollback message to all the cohorts each cohort undoes the transaction using the undo log and releases the resources and locks held during the transaction each cohort sends an acknowledgement to the coordinator the coordinator undoes the transaction when all acknowledgements have been received message flow pre coordinator cohort query to commit vote yes no prepare abort commit abort commit rollback acknowledgment commit abort end pre an next to the record type means that the record is forced to stable storage ref name mohan1986 c mohan bruce lindsay and r obermarck 1986 http dl acm org citation cfm id 7266 transaction management in the r distributed database management system acm transactions on database systems tods volume 11 issue 4 dec 1986 pages 378 396 ref disadvantages the greatest disadvantage of the two phase commit protocol is that it is a blocking protocol if the coordinator fails permanently some cohorts will never resolve their transactions after a cohort has sent an agreement message to the coordinator it will block until a commit or rollback is received implementing the two phase commit protocol common architecture in many cases the 2pc protocol is distributed in a computer network it is easily distributed by implementing multiple dedicated 2pc components similar to each other typically named transaction manager s tms also referred to as 2pc agents or transaction processing monitors that carry out the protocol s execution for each transaction e g the open group s x open xa the databases involved with a distributed transaction the participants both the coordinator and cohorts register to close tms typically residing on respective same network nodes as the participants for terminating that transaction using 2pc each distributed transaction has an ad hoc set of tms the tms to which the transaction participants register a leader the coordinator tm exists for each transaction to coordinate 2pc for it typically the tm of the coordinator database however the coordinator role can be transferred to another tm for performance or reliability reasons rather than exchanging 2pc messages among themselves the participants exchange the messages with their respective tms the relevant tms communicate among themselves to execute the 2pc protocol schema above representing the respective participants for terminating that transaction with this architecture the protocol is fully distributed does not need any central processing component or data structure and scales up with number of network nodes network size effectively this common architecture is also effective for the distribution of other atomic commitment protocol s besides 2pc since all such protocols use the same voting mechanism and outcome propagation to protocol participants ref name bernstein1987 ref name weikum2001 protocol optimizations database research has been done on ways to get most of the benefits of the two phase commit protocol while reducing costs by protocol optimizations ref name bernstein1987 ref name weikum2001 ref name bern2009 and protocol operations saving under certain system s behavior assumptions presumed abort and presumed commit presumed abort or presumed commit are common such optimizations ref name weikum2001 ref name bern2009 ref name mohan1983 c mohan bruce lindsay 1985 http portal acm org citation cfm id 850772 efficient commit protocols for the tree of processes model of distributed transactions acm sigops operating systems review 19 2 pp 40 52 april 1985 ref an assumption about the outcome of transactions either commit or abort can save both messages and logging operations by the participants during the 2pc protocol s execution for example when presumed abort if during system recovery from failure no logged evidence for commit of some transaction is found by the recovery procedure then it assumes that the transaction has been aborted and acts accordingly this means that it does not matter if aborts are logged at all and such logging can be saved under this assumption typically a penalty of additional operations is paid during recovery from failure depending on optimization type thus the best variant of optimization if any is chosen according to failure and transaction outcome statistics tree two phase commit protocol the tree data structure tree 2pc protocol ref name weikum2001 also called nested 2pc or recursive 2pc is a common variant of 2pc in a computer network which better utilizes the underlying communication infrastructure the participants in a distributed transaction are typically invoked in an order which defines a tree structure the invocation tree where the participants are the nodes and the edges are the invocations communication links the same tree is commonly utilized to complete the transaction by a 2pc protocol but also another communication tree can be utilized for this in principle in a tree 2pc the coordinator is considered the root top of a communication tree inverted tree while the cohorts are the other nodes the coordinator can be the node that originated the transaction invoked recursively transitively the other participants but also another node in the same tree can take the coordinator role instead 2pc messages from the coordinator are propagated down the tree while messages to the coordinator are collected by a cohort from all the cohorts below it before it sends the appropriate message up the tree except an abort message which is propagated up immediately upon receiving it or if the current cohort initiates the abort the dynamic two phase commit dynamic two phase commitment d2pc protocol ref name weikum2001 ref name raz1995 yoav raz 1995 http www springerlink com content pv12p828kk616258 the dynamic two phase commitment d2pc protocol database theory icdt 95 lecture notes in computer science volume 893 1995 pp 162 176 springer isbn 978 3 540 58907 5 ref is a variant of tree 2pc with no predetermined coordinator it subsumes several optimizations that have been proposed earlier agreement messages yes votes start to propagate from all the leaves each leaf when completing its tasks on behalf of the transaction becoming ready an intermediate non leaf node sends when ready an agreement message to the last single neighboring node from which agreement message has not yet been received the coordinator is determined dynamically by racing agreement messages over the transaction tree at the place where they collide they collide either at a transaction tree node to be the coordinator or on a tree edge in the latter case one of the two edge s nodes is elected as a coordinator any node d2pc is time optimal among all the instances of a specific transaction tree and any specific tree 2pc protocol implementation all instances have the same tree each instance has a different node as coordinator by choosing an optimal coordinator d2pc commits both the coordinator and each cohort in minimum possible time allowing the earliest possible release of locked resources in each transaction participant tree node see also atomic commit commit data management three phase commit protocol x open xa xa paxos algorithm two generals problem references reflist external links http exploredatabase blogspot in 2014 07 two phase commit protocol in pictures html two phase commit protocol explained in pictures by exploredatabase defaultsort two phase commit protocol category data management category transaction processing'
b'unreferenced date december 2009 semantic translation is the process of using semantic information to aid in the translation of data in one representation or data model to another representation or data model semantic translation takes advantage of semantics that associate meaning with individual data element s in one data dictionary dictionary to create an equivalent meaning in a second system an example of semantic translation is the conversion of xml data from one data model to a second data model using formal ontologies for each system such as the web ontology language owl this is frequently required by intelligent agents that wish to perform searches on remote computer systems that use different data models to store their data elements the process of allowing a single user to search multiple systems with a single search request is also known as federated search semantic translation should be differentiated from data mapping tools that do simple one to one translation of data from one system to another without actually associating meaning with each data element semantic translation requires that data elements in the source and destination systems have semantic mappings to a central registry or registries of data elements the simplest mapping is of course where there is equivalence there are three types of semantic equivalence class computer science class equivalence anchor class equivalence indicating that class or concepts are equivalent for example person is the same as individual relation mathematics property equivalence anchor property equivalence indicating that two properties are equivalent for example persongivenname is the same as firstname instance computer science instance equivalence anchor instance equivalence indicating that two individual instances of objects are equivalent for example dan smith is the same person as daniel smith semantic translation is very difficult if the terms in a particular data model do not have direct one to one mappings to data elements in a foreign data model in that situation an alternative approach must be used to find mappings from the original data to the foreign data elements this problem can be alleviated by centralized metadata registries that use the iso 11179 standards such as the national information exchange model niem see also data mapping semantic heterogeneity semantic mapper federated search intelligent agents iso iec 11179 national information exchange model semantic web vocabulary based transformation web ontology language defaultsort semantic translation category data management category enterprise application integration'
b'unreferenced date july 2013 in metadata a vocabulary based transformation vbt is a transformation aided by the use of a semantic equivalence statements within a controlled vocabulary many organizations today require communication between two or more computers although many standards exist to exchange data between computers such as html or email there is still much structured information that needs to be exchanged between computers that is not standardized the process of mapping one source of data into another is often a slow and labor intensive process vbt is a possible way to avoid much of the time and cost of manual data mapping using traditional extract transform load technologies history the term vocabulary based transformation was first defined by roy shulte of the gartner group around may 2003 and appeared in annual hype cycle hype cycle for data integration integration application vbt allows computer systems integrators to more automatically look up the definitions of data elements in a centralized data dictionary and use that definition and the equivalent mappings to transform that data element into a foreign namespace the web ontology language owl language also support three semantic equivalence statements companies or products iona technologies http liaison com products transform contivo and delta by http liaison com liaison technologies enleague systems itemfield unicorn solutions vitria technology zonar see also metadata controlled vocabulary data dictionary semantic spectrum semantic equivalence xslt enterprise application integration external links http www gartner com 6 help glossary glossaryv jsp gartner glossary of terms gartner definition vocabulary based transformation http www sun com service openwork analyst gartner hype cycle pdf gartner hype cycle 2003 defaultsort vocabulary based transformation category data management'
b'distinguish dictionary data structure use dmy dates date july 2013 a data dictionary or metadata repository as defined in the ibm dictionary of computing is a centralized repository of information about data such as meaning relationships to other data origin usage and format ref acm http portal acm org citation cfm id 541721 ibm dictionary of computing 10th edition 1993 ref the term can have one of several closely related meanings pertaining to database s and database management system s dbms a document describing a database or collection of databases an integral software component component of a database management system dbms that is required to determine its structure a piece of middleware that extends or supplants the native data dictionary of a dbms documentation the terms data dictionary and data repository indicate a more general software utility than a catalogue a catalogue is closely coupled with the dbms software it provides the information stored in it to the user and the dba but it is mainly accessed by the various software modules of the dbms itself such as data definition language ddl and data manipulation language dml compilers the query optimiser the transaction processor report generators and the constraint enforcer on the other hand a data dictionary is a data structure that stores metadata i e structured data about information the software package for a stand alone data dictionary or data repository may interact with the software modules of the dbms but it is mainly used by the designers users and administrators of a computer system for information resource management these systems maintain information on system hardware and software configuration documentation application and users as well as other information relevant to system administration ref ramez elmasri shamkant b navathe fundamentals of database systems 3rd ed sect 17 5 p 582 ref if a data dictionary system is used only by the designers users and administrators and not by the dbms software it is called a passive data dictionary otherwise it is called an active data dictionary or data dictionary when a passive data dictionary is updated it is done so manually and independently from any changes to a dbms database structure with an active data dictionary the dictionary is updated first and changes occur in the dbms automatically as a result database user computing users and application software application developers can benefit from an authoritative data dictionary document that catalogs the organization contents and conventions of one or more databases ref techtarget searchsoa http searchsoa techtarget com sdefinition 0 sid26 gci211896 00 html what is a data dictionary ref this typically includes the names and descriptions of various table database tables records or entities and their contents column database fields plus additional details like the data type type and length of each data element another important piece of information that a data dictionary can provide is the relationship between tables this is sometimes referred to in entity relationship diagrams or if using set descriptors identifying which sets database tables participate in in an active data dictionary constraints may be placed upon the underlying data for instance a range may be imposed on the value of numeric data in a data element field or a record in a table may be forced to participate in a set relationship with another record type additionally a distributed dbms may have certain location specifics described within its active data dictionary e g where tables are physically located the data dictionary consists of record types tables created in the database by systems generated command files tailored for each supported back end dbms command files contain sql statements for create table create unique index alter table for referential integrity etc using the specific statement required by that type of database there is no universal standard as to the level of detail in such a document middleware in the construction of database applications it can be useful to introduce an additional layer of data dictionary software i e middleware which communicates with the underlying dbms data dictionary such a high level data dictionary may offer additional features and a degree of flexibility that goes beyond the limitations of the native low level data dictionary whose primary purpose is to support the basic functions of the dbms not the requirements of a typical application for example a high level data dictionary can provide alternative entity relationship model s tailored to suit different applications that share a common database ref u s patent 4774661 http www freepatentsonline com 4774661 html database management system with active data dictionary 19 november 1985 at t ref extensions to the data dictionary also can assist in query optimization against distributed database s ref u s patent 4769772 http www freepatentsonline com 4769772 html automated query optimization method using both global and parallel local optimizations for materialization access planning for distributed databases 28 february 1985 honeywell bull ref additionally dba functions are often automated using restructuring tools that are tightly coupled to an active data dictionary software framework s aimed at rapid application development sometimes include high level data dictionary facilities which can substantially reduce the amount of programming required to build menu computing menus form programming forms reports and other components of a database application including the database itself for example phplens includes a php class library to automate the creation of tables indexes and foreign key constraints portability software portably for multiple databases ref phplens http phplens com lens adodb docs datadict htm adodb data dictionary library for php ref another php based data dictionary part of the radicore toolkit automatically generates program object computer science objects scripting language scripts and sql code for menus and forms with data validation and complex join sql joins ref radicore http www radicore org viewarticle php article id 5 what is a data dictionary ref for the asp net environment base one international base one s data dictionary provides cross dbms facilities for automated database creation data validation performance enhancement cache computing caching and index utilization application security and extended data type s ref base one international corp http www boic com b1ddic htm base one data dictionary ref visual dataflex features ref visual dataflex http www visualdataflex com features asp pageid 1030 features ref provides the ability to use datadictionaries as class files to form middle layer between the user interface and the underlying database the intent is to create standardized rules to maintain data integrity and enforce business rules throughout one or more related applications platform specific examples developers use a data description specification dds to describe data attributes in file descriptions that are external to the application program that processes the data in the context of an ibm system i ref cite web url http publib boulder ibm com infocenter iseries v5r3 topic dds rbafpddsmain htm title dds documentation for ibm system i v5r3 ref see also data hierarchy data modeling database schema iso iec 11179 metadata registry semantic spectrum vocabulary onesource metadata repository references reflist 30em external links commons category data dictionary yourdon structured analysis wiki http yourdon com strucanalysis wiki index php title chapter 10 data dictionaries data warehouse authority control defaultsort data dictionary category data management category data modeling category knowledge representation category metadata'
b'distinguish flat file system refimprove date march 2015 originalresearch date march 2015 image flat file model svg thumb 280px example of a flat file model ref name usdt01 http knowledge fhwa dot gov tam aashto nsf all documents 4825476b2b5c687285256b1f00544258 file digloss pdf data integration glossary webarchive url https web archive org web 20090320001015 http knowledge fhwa dot gov tam aashto nsf all documents 4825476b2b5c687285256b1f00544258 file digloss pdf date march 20 2009 u s department of transportation august 2001 ref a flat file database is a database which is stored on its host computer system as an ordinary unstructured file called a flat file to access the structure of the data and manipulate it the file must be read in its entirety into the computer s memory upon completion of the database operations the file is again written out in its entirety to the host s file system in this stored mode the database is said to be flat meaning that it has no structure for indexing and there are usually no structural relationships between the records a flat file can be a plain text file or a binary file the term has generally implied a small simple database as computer memory has become cheaper more sophisticated databases can now be entirely held in memory for faster access these newer databases would not generally be referred to as flat file databases overview plain text files usually contain one record computer science record per line ref citation last fowler first glenn year 1994 title cql flat file database query language periodical wtec 94 proceedings of the usenix winter 1994 technical conference on usenix winter 1994 technical conference url http www research att com astopen publications cql 1994 pdf ref there are different conventions for depicting data in comma separated values and delimiter separated values files field computer science field s can be separated by delimiters such as comma separated values comma or tab separated values tab characters in other cases each field may have a fixed length short values may be padded with space character s extra formatting may be needed to avoid delimiter collision more complex solutions are markup language s and programming language s using delimiters incurs some computational overhead overhead in locating them every time they are processed unlike fixed width formatting which may have computer performance performance implications however use of character delimiters especially commas is also a crude form of data compression which may assist overall performance by reducing data volumes nbsp especially for data transmission purposes use of character delimiters which include a length component string literal declarative notation declarative notation is comparatively rare but vastly reduces the overhead associated with locating the extent of each field typical examples of flat files are code etc passwd code and code etc group code on unix like operating systems another example of a flat file is a name and address list with the fields name address and phone number a list of names addresses and phone numbers written by hand on a sheet of paper is a flat file database this can also be done with any typewriter or word processor a spreadsheet or text editor program may be used to implement a flat file database which may then be printed or used online for improved search capabilities history herman hollerith conceived the idea that data could be represented by holes punched in paper cards then tabulated by machine he implemented this concept for the united states census bureau us census bureau thus the 1890 united states census processing created the first database consisting of thousands of boxes full of punched card s hollerith s enterprise grew into the computer giant ibm which dominated the data processing market for most of the 20th century ibm s fixed length field 80 column punch cards became the ubiquitous means of inputting electronic data until the 1970s in the 1980s configurable flat file database computer application s were popular on dos and the apple macintosh macintosh these programs were designed to make it easy for individuals to design and use their own databases and were almost on par with word processors and spreadsheet s in popularity citation needed date september 2011 examples of flat file database products were early versions of filemaker and the shareware pc file some of these like dbase ii offered limited relational database relational capabilities allowing some data to be shared between files in the 2010s flat file databases were used in content management system s instead of using a database web developers were able to change the content directly in the file system or at the command line contemporary implementations faircom s c tree is an example of a modern enterprise level solution and spreadsheet software and text editor s can be used for this purpose webdna is a scripting language designed for the world wide web with a hybrid flat file in memory database system making it easy to build resilient database driven websites with the in memory concept webdna searches and database updates are almost realtime while the data is stored as text files within the website itself otherwise flat file database is implemented in microsoft works and apple works over time products like borland s paradox and microsoft s microsoft access access started offering some relational capabilities as well as built in programming languages database management systems dbms like mysql or oracle database oracle generally require programmers to build applications faceless flat file database engines are used internally by mac os x firefox and other computer software to store configuration data programs to manage collections of books or appointments and address book are essentially single purpose flat file database applications allowing users to store and retrieve information from flat files using a predefined set of fields data transfer operations flat files are used not only as data storage tools in db and content management system cms systems but also as data transfer tools to remote servers in which case they become known as information streams in recent years this latter implementation has been replaced with xml files which not only contain but also describe the data those still using flat files to transfer information are mainframes employing specific procedures which are too expensive to modify one criticism often raised against the xml format as a way to perform mass data transfer operations is that file size is significantly larger than that of flat files which is generally reduced to the bare minimum the solution to this problem consists in xml file compression a solution that applies equally well to flat files which has nowadays gained efficient xml interchange exi standards i e efficient xml interchange which is often used by mobile devices it is advisable that transfer data be performed via exi rather than flat files because defining the compression method is not required because libraries reading the file contents are readily available and because there is no need for the two communicating systems to preliminarily establish a protocol describing data properties such as position alignment type and format however in those circumstances where the sheer mass of data and or the inadequacy of legacy systems becomes a problem the only viable solution remains the use of flat files in order to successfully handle those problems connected with data communication format validation control and much else be it a flat file or an xml file data source it is advisable to adopt a data quality firewall terminology flat file database may be defined very narrowly or more broadly the narrower interpretation is correct in database database theory the broader covers the term as generally used strictly a flat file database should consist of nothing but data and if records vary in length delimiters more broadly the term refers to any database which exists in a single file in the form of rows and columns with no relationships or links between records and fields except the table structure terms used to describe different aspects of a database and its tools differ from one implementation to the next but the concepts remain the same filemaker uses the term find while mysql uses the term query but the concept is the same filemaker files in version 7 and above are equivalent to mysql databases and so forth to avoid confusing the reader one consistent set of terms is used throughout this article however the basic terms record and field are used in nearly every flat file database implementation example database the following example illustrates the basic elements of a flat file database the data arrangement consists of a series of columns and rows organized into a table information tabular format this specific example uses only one table the columns include name a person s name second column team the name of an athletic team supported by the person third column and a numeric unique id used to uniquely identify records first column here is an example textual representation of the described data id name team 1 amy blues 2 bob reds 3 chuck blues 4 richard blues 5 ethel reds 6 fred blues 7 gilly blues 8 hank reds 9 hank blues this type of data representation is quite standard for a flat file database although there are some additional considerations that are not readily apparent from the text data types each column in a database table such as the one above is ordinarily restricted to a specific data type such restrictions are usually established by convention but not formally indicated unless the data is transferred to a relational database system separated columns in the above example individual columns are separated using whitespace computer science whitespace characters this is also called indentation or fixed width data formatting another common convention is to separate columns using one or more delimiter characters more complex solutions are markup and programming languages relational algebra each row or record in the above table meets the standard definition of a tuple under relational algebra the above example depicts a series of 3 tuples additionally the first row specifies the tuple field names field names that are associated with the values of each row database management system since the formal operations possible with a text file are usually more limited than desired the text in the above example would ordinarily represent an intermediary state of the data prior to being transferred into a database management system references commons category flat file models reflist database models defaultsort flat file database category data management category computer file formats category database models it flat file'
b'an enterprise information system eis is any kind of information system which improves the functions of an enterprise business processes by integration this means typically offering high quality of service dealing with large volumes of data and capable of supporting some large and possibly complex organization or enterprise an eis must be able to be used by all parts and all levels of an enterprise ref name eidvtai the word enterprise can have various connotations frequently the term is used only to refer to very large organizations such as multi national companies or public sector organizations however the term may be used to mean virtually anything by virtue of it having become the latest corporate speak buzzword citation needed date june 2016 purpose enterprise information systems provide a technology platform that enables organizations to enterprise integration integrate and coordinate their business processes on a robust foundation an eis is currently used in conjunction with customer relationship management and supply chain management to automate business processes ref name eidvtai an enterprise information system provides a single system that is central to the organization that ensures information can be shared across all functional levels and management hierarchies an eis can be used to increase business productivity and reduce service cycles product development cycles and marketing life cycles ref name eidvtai cite book title enterprise information systems contemporary trends and issues last olson first david l author2 subodh kesharwani year 2010 publisher world scientific isbn 9814273163 pages 2 13 16 url https books google com books id awdap7fe2uc accessdate 20 august 2013 ref it may be used to amalgamate existing applications other outcomes include higher operational efficiency and cost savings ref name eidvtai financial value is not usually a direct outcome from the implementation of an enterprise information system ref name eiscmta cite book title enterprise information systems concepts methodologies tools and applications author information resources management association year 2010 publisher idea group inc isbn 1616928530 pages 38 43 url https books google com books id hpc6 sfs2scc accessdate 20 august 2013 ref design stage at the design stage the main characteristic of eis efficiency evaluation is the probability of timely delivery of various messages such as command service etc ref cite journal title \xd0\xbe\xd1\x86\xd0\xb5\xd0\xbd\xd0\xba\xd0\xb0 \xd1\x85\xd0\xb0\xd1\x80\xd0\xb0\xd0\xba\xd1\x82\xd0\xb5\xd1\x80\xd0\xb8\xd1\x81\xd1\x82\xd0\xb8\xd0\xba \xd1\x84\xd1\x83\xd0\xbd\xd0\xba\xd1\x86\xd0\xb8\xd0\xbe\xd0\xbd\xd0\xb8\xd1\x80\xd0\xbe\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x8f \xd0\xba\xd0\xbe\xd1\x80\xd0\xbf\xd0\xbe\xd1\x80\xd0\xb0\xd1\x82\xd0\xb8\xd0\xb2\xd0\xbd\xd1\x8b\xd1\x85 \xd0\xb8\xd0\xbd\xd1\x84\xd0\xbe\xd1\x80\xd0\xbc\xd0\xb0\xd1\x86\xd0\xb8\xd0\xbe\xd0\xbd\xd0\xbd\xd1\x8b\xd1\x85 \xd1\x81\xd0\xb8\xd1\x81\xd1\x82\xd0\xb5\xd0\xbc \xd1\x81 \xd0\xbd\xd0\xb5\xd0\xbe\xd0\xb4\xd0\xbd\xd0\xbe\xd1\x80\xd0\xbe\xd0\xb4\xd0\xbd\xd0\xbe\xd0\xb9 \xd0\xbd\xd0\xb0\xd0\xb3\xd1\x80\xd1\x83\xd0\xb7\xd0\xba\xd0\xbe\xd0\xb9 trans title efficiency evaluation of enterprise information systems with non uniform load language ru url http ntv ifmo ru en article 13881 ocenka harakteristik funkcionirovaniya korporativnyhinformacionnyh sistem s neodnorodnoy nagruzkoy htm author1 kalinin i v author2 maharevs e author3 muravyeva vitkovskaya l a journal scientific and technical journal of information technologies mechanics and optics volume 15 issue 5 pages 863 868 year 2015 doi 10 17586 2226 1494 2015 15 5 863 868 ref information systems main information systems enterprise systems create a standard data structure and are invaluable in eliminating the problem of information fragmentation caused by multiple information systems within an organization an eis differentiates itself from legacy system s in that it self transactional self helping and adaptable to general and specialist conditions ref name eidvtai unlike an enterprise information system legacy systems are limited to department wide communications ref name eiscmta a typical enterprise information system would be housed in one or more data center s would run enterprise software and could include applications that typically cross organizational borders such as content management systems see also portal computing business executive information system management information system enterprise planning systems enterprise software references reflist category data management category enterprise architecture category enterprise modelling category website management'
b'a hybrid array is a form of hierarchical storage management that combines hard disk drive s hdds with solid state drive s ssds for i o speed improvements hybrid storage arrays aim to mitigate the ever increasing price performance gap between hdds and dram by adding a non volatile flash level to the memory hierarchy ref name michelonimarelli2012 cite book author1 rino micheloni author2 alessia marelli author3 kam eshghi title inside solid state drives ssds url https books google com books id s8xrtkf7hukc pg pa62 year 2012 publisher springer isbn 978 94 007 5145 3 page 62 ref hybrid arrays thus aim to lower the cost per i o compared to using only ssds for storage hybrid architectures can be as simple as involving a single ssd cache computing cache for desktop or laptop computers or can be more complex as configurations for data center s and cloud computing implementations please only add products covered in wp secondary sources at adequate depth some commercial products for building hybrid arrays include adaptec demonstrated the maxiq series in 2009 ref cite web author charlie demerjian url http semiaccurate com 2009 09 09 adaptecs maxiq caches raids ssds title adaptec s maxiq caches raids with ssds publisher semiaccurate date september 9 2009 accessdate october 10 2016 ref apple s fusion drive linux software includes bcache dm cache and flashcache and its fork enhanceio condusive s expresscache is marketed for laptops emc corporation vfcache was announced in 2012 ref cite web last larry dignan url http www zdnet com blog btl emc unveils vfcache targets fusion io 68657 title emc unveils vfcache targets fusion io publisher zdnet work between the lines date february 5 2012 accessdate october 10 2016 ref ref cite news title one day later emc declares war on all flash array server flash card rivals rolls out xtremio array renamed vfcache date march 5 2013 work the register author chris mellor url http www theregister co uk 2013 03 05 emc xtremsf accessdate october 10 2016 ref fusion io acquired ioturbine in 2011 ref name io cite web url http www theregister co uk 2013 06 25 fusionio spins ioturbine faster title fusion io spins up ioturbine enhances server flash caching work the register accessdate october 10 2016 ref and the product line it acquired by buying nexgen in 2013 ref cite web url http www theregister co uk 2013 04 24 fusion io nexgen title fusion io buys nexgen work theregister co uk accessdate 2015 03 26 ref hitachi accelerated flash storage hafs used together with the hitachi dynamic tiering software ref citation url http www computerweekly com feature big storage turns the tide in the hybrid flash array market title big storage turns the tide in the hybrid flash array market work computer weekly date september 2013 accessdate 2015 03 26 ref ibm flash cache storage accelerator fcsa server software ref cite web author the ssd guy url http thessdguy com ibm adds server side caching title ibm adds server side caching publisher the ssd guy date 2013 08 20 accessdate 2013 12 23 ref intel s smart response technology for desktop intel s cache acceleration software for servers and workstations lsi corporation lsi cachecade software for their controllers ref cite web url http www storagereview com lsi megaraid cachecade pro 20 review title lsi megaraid cachecade pro 2 0 review accessdate 2015 03 26 work storagereview com ref marvell technology group marvell s hyperduo controllers ref cite web url http www cnet com 8301 32254 1 20027657 283 html title hands on with the marvell hyperduo hybrid storage controller accessdate 2015 03 26 publisher cbs interactive work cnet ref microsoft s automated tiering since windows 2012 r2 netapp s flash cache flash pool flash accel ref cite web url http www theregister co uk 2012 08 21 netapp server flash title netapp flash as a storage tier you must be joking accessdate 2015 03 26 work theregister co uk ref oracle corporation markets products such as exadata smart cache flash and the fs1 flash storage system ref cite news title oracle crashes all flash bash behold our hybrid fs1 arrays mutant flash disk box a pillar of storage it s axiomatic date september 30 2014 work the register author chris mellor url http www theregister co uk 2014 09 30 the fs1 pillar of oracle arrays storage accessdate october 10 2016 ref microsoft readyboost allows personal computers to usb flash drive s as cache nvelo dataplex ssd caching software was announced in 2011 ref cite web url http www thessdreview com our reviews nvelo dataplex ssd caching software review seven msata ssds prove an amazing concept title nvelo dataplex ssd caching software review seven msata ssds prove an amazing concept work the ssd review date december 4 2011 author les tokar accessdate october 10 2016 ref and was acquired by samsung in 2012 ref cite web url http www anandtech com show 6518 samsung acquires ssd caching company nvelo title samsung acquires ssd caching company nvelo publisher anandtech author kristian v\xc3\xa4tt\xc3\xb6 date december 16 2012 accessdate october 10 2016 ref sandisk flashsoft for windows linux and vsphere ref name io products are offered by vendors like ami stortrends ref cite web author ian barker url http betanews com 2014 01 27 ami stortrends 3500i offers high performance storage for smaller enterprises title ami stortrends 3500i offers high performance storage for smaller enterprises publisher betanews date 2014 01 27 accessdate 2014 10 17 ref tegile systems reduxio and tintri ref cite web url http www theregister co uk 2013 04 09 blind spot accessdate 2015 03 26 title mutant array upstarts feast on emc netapp s leavings work the register ref zfs using hybrid storage pools are used for example in some oracle corporation products ref cite web url http www enterprisestorageforum com san nas storage oracles flash friendly sun zfs storage is ready for new sparcs html title oracle s flash friendly sun zfs storage is ready for new sparcs date 3 april 2013 accessdate 2015 03 26 work enterprisestorageforum com ref see also hybrid drive spaced ndash built in flash cache handled by firmware automated tiered storage spaced ndash another name for hierarchical storage management the five minute rule for caching references reflist 30em category data management category solid state caching category memory management software'
b'redirect global serializability category data management category databases category transaction processing category concurrency control'
b'redirect global concurrency control modular concurrency control category data management category databases category transaction processing category concurrency control'
b'data extraction is the act or process of retrieving data out of usually unstructured data unstructured or poorly structured data sources for further data processing or data storage device data storage data migration the data import import into the intermediate extracting system is thus usually followed by data transformation and possibly the addition of metadata prior to data export export to another stage in the data workflow ref http www extractingdata com definition of data extraction ref usually the term data extraction is applied when experiment al data is first imported into a computer from primary sources like measuring device measuring or recording device s today s electronic device s will usually present an electrical connector e g usb through which raw data can be data stream streamed into a personal computer typical unstructured data sources include web pages emails documents pdfs scanned text mainframe reports spool files classifieds etc which is further used for sales marketing leads ref http www suntecdata com data extraction services html data extraction services retrieved april 4 2016 ref extracting data from these unstructured sources has grown into a considerable technical challenge where as historically data extraction has had to deal with changes in physical hardware formats the majority of current data extraction deals with extracting data from these unstructured data sources and from different software formats this growing process of data extraction ref http www loginworks com blogs web scraping blogs 209 web data extraction data extraction ref from the web is referred to as web scraping the act of adding structure to unstructured data takes a number of forms using text pattern matching such as regular expression s to identify small or large scale structure e g records in a report and their associated data from headers and footers using a table based approach to identify common sections within a limited domain e g in emailed resumes identifying skills previous work experience qualifications etc using a standard set of commonly used headings these would differ from language to language e g education might be found under education qualification courses using text analytics to attempt to understand the text and link it to other information references reflist external links http www etltools org extraction html data extraction as a part of the etl process in a data warehousing environment data warehouse defaultsort data extraction category data management category data warehousing'
b'data integration involves combining data residing in different sources and providing users with a unified view of these data ref name refone cite conference author maurizio lenzerini title data integration a theoretical perspective booktitle pods 2002 year 2002 pages 233 246 url http www dis uniroma1 it lenzerin homepagine talks tutorialpods02 pdf ref this process becomes significant in a variety of situations which include both commercial when two similar companies need to merge their database s and scientific combining research results from different bioinformatics repositories for example domains data integration appears with increasing frequency as the volume and the need to share existing data information explosion explodes ref name dataexplode cite news author frederick lane title idc world created 161 billion gigs of data in 2006 year 2006 url http www toptechnews com story xhtml story id 01300000e3d0 full skip 1 ref it has become the focus of extensive theoretical work and numerous open problems remain unsolved history file datawarehouse png thumb right figure 1 simple schematic for a data warehouse the extract transform load etl process extracts information from the source databases transforms it and then loads it into the data warehouse file dataintegration png thumb right figure 2 simple schematic for a data integration solution a system designer constructs a mediated schema against which users can run queries the virtual database interfaces with the source databases via wrapper pattern wrapper code if required issues with combining heterogeneous data sources often referred to as information silo s under a single query interface have existed for some time in the early 1980s computer scientists began designing systems for interoperability of heterogeneous databases ref cite news author john miles smith title multibase integrating heterogeneous distributed database systems year 1982 journal afips 81 proceedings of the may 4 7 1981 national computer conference pages 487 499 url http dl acm org citation cfm id 1500483 display authors etal ref the first data integration system driven by structured metadata was designed at the university of minnesota in 1991 for the ipums integrated public use microdata series ipums ipums used a data warehousing approach which extract transform load extracts transforms and loads data from heterogeneous sources into a single view logical schema schema so data from different sources become compatible ref cite news author steven ruggles j david hacker and matthew sobek title order out of chaos the integrated public use microdata series year 1995 journal historical methods volume 28 pages 33 39 ref by making thousands of population databases interoperable ipums demonstrated the feasibility of large scale data integration the data warehouse approach offers a coupling computer science tightly coupled architecture because the data are already physically reconciled in a single queryable repository so it usually takes little time to resolve queries ref cite news author jennifer widom title research problems in data warehousing year 1995 journal cikm 95 proceedings of the fourth international conference on information and knowledge management pages 25 30 url http dl acm org citation cfm id 221319 ref the data warehouse approach is less feasible for datasets that are frequently updated requiring the extract transform load etl process to be continuously re executed for synchronization difficulties also arise in constructing data warehouses when one has only a query interface to summary data sources and no access to the full data this problem frequently emerges when integrating several commercial query services like travel or classified advertisement web applications as of 2009 the trend in data integration favored loosening the coupling between data citation needed date june 2009 and providing a unified query interface to access real time data over a data mediation mediated schema see figure 2 which allows information to be retrieved directly from original databases this is consistent with the service oriented architecture soa approach popular in that era this approach relies on mappings between the mediated schema and the schema of original sources and transform a query into specialized queries to match the schema of the original databases such mappings can be specified in 2 ways as a mapping from entities in the mediated schema to entities in the original sources the global as view gav approach or as a mapping from entities in the original sources to the mediated schema the local as view lav approach the latter approach requires more sophisticated inferences to resolve a query on the mediated schema but makes it easier to add new data sources to a stable mediated schema as of 2010 some of the work in data integration research concerns the semantic integration problem this problem addresses not the structuring of the architecture of the integration but how to resolve semantic conflicts between heterogeneous data sources for example if two companies merge their databases certain concepts and definitions in their respective schemas like earnings inevitably have different meanings in one database it may mean profits in dollars a floating point number while in the other it might represent the number of sales an integer a common strategy for the resolution of such problems involves the use of ontology computer science ontologies which explicitly define schema terms and thus help to resolve semantic conflicts this approach represents ontology based data integration on the other hand the problem of combining research results from different bioinformatics repositories requires bench marking of the similarities computed from different data sources on a single criterion such as positive predictive value this enables the data sources to be directly comparable and can be integrated even when the natures of experiments are distinct ref cite journal url http shubhrasankar tripod com cgi bin combiningmultisourceieee pdf journal ieee transactions on biomedical engineering title combining multi source information through functional annotation based weighting gene function prediction in yeast author shubhra s ray volume 56 pages 229 236 pmid 19272921 year 2009 issue 2 doi 10 1109 tbme 2008 2005955 display authors etal ref as of 2011 it was determined that current data modeling methods were imparting data isolation into every data architecture in the form of islands of disparate data and information silos this data isolation is an unintended artifact of the data modeling methodology that results in the development of disparate data models disparate data models when instantiated as databases form disparate databases enhanced data model methodologies have been developed to eliminate the data isolation artifact and to promote the development of integrated data models ref cite news author michael mireku kwakye title a practical approach to merging multidimensional data models year 2011 url http hdl handle net 10393 20457 ref ref cite web url http www iri com pdf rapidace brochure pdf title rapid architectural consolidation engine nbsp the enterprise solution for disparate data models year 2011 ref one enhanced data modeling method recasts data models by augmenting them with structural metadata in the form of standardized data entities as a result of recasting multiple data models the set of recast data models will now share one or more commonality relationships that relate the structural metadata now common to these data models commonality relationships are a peer to peer type of entity relationships that relate the standardized data entities of multiple data models multiple data models that contain the same standard data entity may participate in the same commonality relationship when integrated data models are instantiated as databases and are properly populated from a common set of master data then these databases are integrated since 2011 data hub approaches have been of greater interest than fully structured typically relational enterprise data warehouses since 2013 data lake approaches have risen to the level of data hubs see all three search terms popularity on google trends ref cite web title hub lake and warehouse search trends url https www google com trends explore q enterprise 20data 20warehouse 2c 20 22data 20hub 22 2c 20 22data 20lake 22 cmpt q tz etc 2fgmt 2b5 ref these approaches combine unstructured or varied data into one location but do not necessarily require an often complex master relational schema to structure and define all data in the hub example consider a web application where a user can query a variety of information about cities such as crime statistics weather hotels demographics etc traditionally the information must be stored in a single database with a single schema but any single enterprise would find information of this breadth somewhat difficult and expensive to collect even if the resources exist to gather the data it would likely duplicate data in existing crime databases weather websites and census data a data integration solution may address this problem by considering these external resources as materialized view s over a virtual database virtual mediated schema resulting in virtual data integration this means application developers construct a virtual schema the mediated schema to best model the kinds of answers their users want next they design wrappers or adapters for each data source such as the crime database and weather website these adapters simply transform the local query results those returned by the respective websites or databases into an easily processed form for the data integration solution see figure 2 when an application user queries the mediated schema the data integration solution transforms this query into appropriate queries over the respective data sources finally the virtual database combines the results of these queries into the answer to the user s query this solution offers the convenience of adding new sources by simply constructing an adapter or an application software blade for them it contrasts with extract transform load etl systems or with a single database solution which require manual integration of entire new dataset into the system the virtual etl solutions leverage virtual database virtual mediated schema to implement data harmonization whereby the data are copied from the designated master source to the defined targets field by field advanced data virtualization is also built on the concept of object oriented modeling in order to construct virtual mediated schema or virtual metadata repository using hub and spoke architecture each data source is disparate and as such is not designed to support reliable joins between data sources therefore data virtualization as well as data federation depends upon accidental data commonality to support combining data and information from disparate data sets because of this lack of data value commonality across data sources the return set may be inaccurate incomplete and impossible to validate one solution is to recast disparate databases to integrate these databases without the need for extract transform load etl the recast databases support commonality constraints where referential integrity may be enforced between databases the recast databases provide designed data access paths with data value commonality across databases theory the theory of data integration ref name refone forms a subset of database theory and formalizes the underlying concepts of the problem in first order logic applying the theories gives indications as to the feasibility and difficulty of data integration while its definitions may appear abstract they have sufficient generality to accommodate all manner of integration systems ref cite web url http link springer com chapter 10 1007 3 540 46093 4 14 title a model theory for generic schema management ref including those that include nested relational xml databases ref cite web url http www vldb org conf 2006 p67 fuxman pdf title nested mappings schema mapping reloaded ref and those that treat databases as programs ref cite web url http homepages inf ed ac uk dts pub psi pdf title the common framework initiative for algebraic specification and development of software ref connections to particular databases systems such as oracle or db2 are provided by implementation level technologies such as jdbc and are not studied at the theoretical level definitions data integration systems are formally defined as a triple mathematics triple math left langle g s m right rangle math where math g math is the global or mediated schema math s math is the heterogeneous set of source schemas and math m math is the mapping that maps queries between the source and the global schemas both math g math and math s math are expressed in formal language languages over alphabet computer science alphabets composed of symbols for each of their respective relational database relations the functional predicate mapping math m math consists of assertions between queries over math g math and queries over math s math when users pose queries over the data integration system they pose queries over math g math and the mapping then asserts connections between the elements in the global schema and the source schemas a database over a schema is defined as a set of sets one for each relation in a relational database the database corresponding to the source schema math s math would comprise the set of sets of tuples for each of the heterogeneous data sources and is called the source database note that this single source database may actually represent a collection of disconnected databases the database corresponding to the virtual mediated schema math g math is called the global database the global database must satisfy the mapping math m math with respect to the source database the legality of this mapping depends on the nature of the correspondence between math g math and math s math two popular ways to model this correspondence exist global as view or gav and local as view or lav file gavlav png thumb right figure 3 illustration of tuple space of the gav and lav mappings ref name refseven cite journal author christoph koch title data integration against multiple evolving autonomous schemata year 2001 url http www csd uoc gr hy562 papers thesis final pdf deadurl yes archiveurl https web archive org web 20070926211342 http www csd uoc gr hy562 papers thesis final pdf archivedate 2007 09 26 df ref in gav the system is constrained to the set of tuples mapped by the mediators while the set of tuples expressible over the sources may be much larger and richer in lav the system is constrained to the set of tuples in the sources while the set of tuples expressible over the global schema can be much larger therefore lav systems must often deal with incomplete answers gav systems model the global database as a set of view database views over math s math in this case math m math associates to each element of math g math a query over math s math query optimizer query processing becomes a straightforward operation due to the well defined associations between math g math and math s math the burden of complexity falls on implementing mediator code instructing the data integration system exactly how to retrieve elements from the source databases if any new sources join the system considerable effort may be necessary to update the mediator thus the gav approach appears preferable when the sources seem unlikely to change in a gav approach to the example data integration system above the system designer would first develop mediators for each of the city information sources and then design the global schema around these mediators for example consider if one of the sources served a weather website the designer would likely then add a corresponding element for weather to the global schema then the bulk of effort concentrates on writing the proper mediator code that will transform predicates on weather into a query over the weather website this effort can become complex if some other source also relates to weather because the designer may need to write code to properly combine the results from the two sources on the other hand in lav the source database is modeled as a set of view database views over math g math in this case math m math associates to each element of math s math a query over math g math here the exact associations between math g math and math s math are no longer well defined as is illustrated in the next section the burden of determining how to retrieve elements from the sources is placed on the query processor the benefit of an lav modeling is that new sources can be added with far less work than in a gav system thus the lav approach should be favored in cases where the mediated schema is less stable or likely to change ref name refone in an lav approach to the example data integration system above the system designer designs the global schema first and then simply inputs the schemas of the respective city information sources consider again if one of the sources serves a weather website the designer would add corresponding elements for weather to the global schema only if none existed already then programmers write an adapter or wrapper for the website and add a schema description of the website s results to the source schemas the complexity of adding the new source moves from the designer to the query processor query processing the theory of query processing in data integration systems is commonly expressed using conjunctive database query language queries and datalog a purely declarative logic programming language ref name reffive cite conference author jeffrey d ullman title information integration using logical views booktitle icdt 1997 year 1997 pages 19 40 url http www db stanford edu pub papers integration using views ps ref one can loosely think of a conjunctive query as a logical function applied to the relations of a database such as math f a b math where math a b math if a tuple or set of tuples is substituted into the rule and satisfies it makes it true then we consider that tuple as part of the set of answers in the query while formal languages like datalog express these queries concisely and without ambiguity common sql queries count as conjunctive queries as well in terms of data integration query containment represents an important property of conjunctive queries a query math a math contains another query math b math denoted math a supset b math if the results of applying math b math are a subset of the results of applying math a math for any database the two queries are said to be equivalent if the resulting sets are equal for any database this is important because in both gav and lav systems a user poses conjunctive queries over a virtual schema represented by a set of view database views or materialized conjunctive queries integration seeks to rewrite the queries represented by the views to make their results equivalent or maximally contained by our user s query this corresponds to the problem of answering queries using views aquv ref name refsix cite conference author alon y halevy title answering queries using views a survey booktitle the vldb journal year 2001 pages 270 294 url http www cs uwaterloo ca david cs740 answering queries using views pdf ref in gav systems a system designer writes mediator code to define the query rewriting each element in the user s query corresponds to a substitution rule just as each element in the global schema corresponds to a query over the source query processing simply expands the subgoals of the user s query according to the rule specified in the mediator and thus the resulting query is likely to be equivalent while the designer does the majority of the work beforehand some gav systems such as http www db stanford edu tsimmis tsimmis involve simplifying the mediator description process in lav systems queries undergo a more radical process of rewriting because no mediator exists to align the user s query with a simple expansion strategy the integration system must execute a search over the space of possible queries in order to find the best rewrite the resulting rewrite may not be an equivalent query but maximally contained and the resulting tuples may be incomplete as of 2009 the minicon algorithm ref name refsix is the leading query rewriting algorithm for lav data integration systems in general the complexity of query rewriting is np complete ref name refsix if the space of rewrites is relatively small this does not pose a problem even for integration systems with hundreds of sources tools alteryx analytics canvas capsenta s ultrawrap platform cloud elements api integration datawatch denodo denodo platform https github com mhaghighat dcafuse discriminant correlation analysis dca ref name dca m haghighat m abdel mottaleb w alhalabi 2016 http ieeexplore ieee org document 7470527 discriminant correlation analysis real time feature level fusion for multimodal biometric recognition ieee transactions on information forensics and security 11 9 1984 1996 ref elastic io integration platform http www hiperfabric com hiperfabric lavastorm analytics lavastorm informatica platform oracle data integration services parsekit enigma io paxata rapidminer studio red hat jboss data virtualization community project teiid microsoft azure azure data factory adf sql server integration services sql server integration services ssis http www tmmdata com tmmdata http www dataladder com data ladder in the life sciences large scale questions in science such as global warming invasive species spread and resource depletion are increasingly requiring the collection of disparate data sets for meta analysis this type of data integration is especially challenging for ecological and environmental data because metadata standards are not agreed upon and there are many different data types produced in these fields national science foundation initiatives such as datanet are intended to make data integration easier for scientists by providing cyberinfrastructure and setting standards the five funded datanet initiatives are dataone ref cite web author william michener url https www dataone org title dataone observation network for earth publisher www dataone org accessdate 2013 01 19 display authors etal ref led by william michener at the university of new mexico the data conservancy ref cite web author sayeed choudhury url https dataconservancy org title data conservancy publisher dataconservancy org accessdate 2013 01 19 display authors etal ref led by sayeed choudhury of johns hopkins university sead sustainable environment through actionable data ref cite web author margaret hedstrom url http sead data net title sead sustainable environment actionable data publisher sead data net accessdate 2013 01 19 display authors etal ref led by margaret hedstrom of the university of michigan the datanet federation consortium ref cite web author reagan moore url http datafed org title datanet federation consortium publisher datafed org accessdate 2013 01 19 display authors etal ref led by reagan moore of the university of north carolina and terra populus ref cite web author steven ruggles url http www terrapop org title terra populus integrated data on population and the environment publisher terrapop org accessdate 2013 01 19 display authors etal ref led by steven ruggles of the university of minnesota the research data alliance ref cite web author bill nichols url http rd alliance org title research data alliance publisher rd alliance org accessdate 2014 10 01 ref has more recently explored creating global data integration frameworks the openphacts project funded through the european union innovative medicines initiative built a drug discovery platform by linking datasets from providers such as european bioinformatics institute royal society of chemistry uniprot wikipathways and drugbank see also div col 20em business semantics management core data integration customer data integration data curation data fusion data mapping data wrangling database model dataspaces edge data integration enterprise application integration enterprise architecture framework enterprise information integration eii enterprise integration geodi geoscientific data integration information integration information server information silo integration competency center integration consortium jxta master data management object relational mapping open text schema matching three schema approach udef web service div col end external links https github com mhaghighat dcafuse discriminant correlation analysis dca ref name dca ref references reflist 30em data defaultsort data integration category data management'
b'an operational data store or ods is a database designed to data integration integrate data from multiple sources for additional operations on the data unlike a master data store the data is not passed back to operational system s it may be passed for further operations and to the data warehouse for reporting because the data originate from multiple sources the integration often involves data cleaning cleaning resolving redundancy and checking against business rule s for data integrity integrity an ods is usually designed to contain low level or atomic indivisible data such as transactions and prices with limited history that is captured real time or near real time as opposed to the much greater volumes of data stored in the data warehouse generally on a less frequent basis general use the general purpose of an ods is to integrate data from disparate source systems in a single structure using data integration technologies like data virtualization data virtualization federated database system data federation or extract transform load extract transform and load this will allow operational access to the data for operational reporting master data master data or reference data management an ods is not a replacement or substitute for a data warehouse but in turn could become a source see also some examples of ods architecture patterns can be found in the article architectural pattern computer science examples architecture patterns publications cite book last1 inmon first1 william author1 link bill inmon title building the operational data store edition 2nd location new york publisher john wiley sons year 1999 isbn 0 471 32888 x external links architectural pattern computer science examples ods architecture patterns ea reference architecture http www dmreview com issues 19980701 469 1 html bill inmon information management article on ods http www information management com issues 20000101 1749 1 html bill inmon information management article on the five classes of ods http www intelsols com documents imhoff 10 02 pdf claudia imhoff information management article on ods pdf data warehouse see also wikipedia books enterprise architecture enterprise architecture defaultsort operational data store category data management category data warehousing database stub'
b'distinguish operating system an operational system is a term used in data warehousing to refer to a system that is used to process the day to day transactions of an organization these systems are designed in a manner that processing of day to day transactions is performed efficiently and the integrity of the transactional data is preserved synonyms sometimes operational systems are referred to as operational database s transaction processing system s or online transaction processing systems oltp however the use of the last two terms as synonyms may be confusing because operational systems can be batch processing systems as well any enterprise must necessarily maintain a lot of data about its operation this is its operational data class wikitable border 1 organization probably manufacturing company product data bank account data hospital patient data university student data government department planning data see also operating system os data warehouse data warehouses versus operational systems data warehouses versus operational systems database stub defaultsort operational system category data warehousing category data management category information technology management category business intelligence'
b'refimprove date july 2015 file networkoperations jpg thumb right an operation engineer overseeing a network operations control room of a data center a data center is a facility used to house computer systems and associated components such as telecommunication s and computer data storage storage systems it generally includes redundant or backup power supply power supplies redundant data communications connections environmental controls e g air conditioning fire suppression and various security devices large data centers are industrial scale operations using as much electricity as a small town ref name nyt92212 cite news title power pollution and the internet url http www nytimes com 2012 09 23 technology data centers waste vast amounts of energy belying industry image html accessdate 2012 09 25 newspaper the new york times date september 22 2012 author james glanz ref ref name referencedc2 http www academia edu 6982393 power management techniques for data centers a survey power management techniques for data centers a survey 2014 ref history file indiana university data center p1100134 jpg thumb indiana university data center bloomington indiana copypaste section url http www rackspace com blog datacenter evolution 1960 to 2000 date august 2014 unreferenced section date august 2014 data centers have their roots in the huge computer rooms of the early ages when date september 2015 of the computing industry early computer systems complex to operate and maintain required a special environment in which to operate many cables were necessary to connect all the components and methods to accommodate and organize these were devised such as standard 19 inch rack racks to mount equipment raised floor s and cable tray s installed overhead or under the elevated floor a single mainframe required a great deal of power and had to be cooled to avoid overheating security became important nbsp computers were expensive and were often used for military purposes basic design guidelines for controlling access to the computer room were therefore devised during the boom of the microcomputer industry and especially during the 1980s users started to deploy computers everywhere in many cases with little or no care about operating requirements however as information technology it operations started to grow in complexity organizations grew aware of the need to control it resources the advent of unix from the early 1970s led to the subsequent proliferation of freely available linux compatible personal computer pc operating systems during the 1990s these were called server computing servers as timesharing operating systems like unix rely heavily on the client server model to facilitate sharing unique resources between multiple users the availability of inexpensive networking hardware networking equipment coupled with new standards for network structured cabling made it possible to use a hierarchical design that put the servers in a specific room inside the company the use of the term data center as applied to specially designed computer rooms started to gain popular recognition about this time citation needed date september 2015 the boom of data centers came during the dot com bubble of 1997 2000 company companies needed fast internet connectivity and non stop operation to deploy systems and to establish a presence on the internet installing such equipment was not viable for many smaller companies many companies started building very large facilities called internet data centers idcs which provide customer commercial client s with a range of solutions for systems deployment and operation new technologies and practices were designed to handle the scale and the operational requirements of such large scale operations these practices eventually migrated toward the private data centers and were adopted largely because of their practical results data centers for cloud computing are called cloud data centers cdcs but nowadays the division of these terms has almost disappeared and they are being integrated into a term data center with an increase in the uptake of cloud computing business and government organizations scrutinize data centers to a higher degree in areas such as security availability environmental impact and adherence to standards standards documents from accredited professional groups such as the telecommunications industry association specify the requirements for data center design well known operational metrics for data availability data center availability can serve to evaluate the business impact analysis commercial impact of a disruption development continues in operational practice and also in environmentally friendly data center design data centers typically cost a lot to build and to maintain citation needed date september 2015 requirements for modern data centers file datacenter telecom jpg thumb left racks of telecommunications equipment in part of a data center copypaste section url https global ihs com doc detail cfm rid tia input doc number tia 942 item s key 00414811 item key date 860905 input doc number tia 942 input doc title abstract date august 2014 it operations are a crucial aspect of most organizational operations around the world one of the main concerns is business continuity companies rely on their information systems to run their operations if a system becomes unavailable company operations may be impaired or stopped completely it is necessary to provide a reliable infrastructure for it operations in order to minimize any chance of disruption information security is also a concern and for this reason a data center has to offer a secure environment which minimizes the chances of a security breach a data center must therefore keep high standards for assuring the integrity and functionality of its hosted computer environment this is accomplished through redundancy of mechanical cooling and power systems including emergency backup power generators serving the data center along with fiber optic cables the telecommunications industry association s telecommunications infrastructure standard for data centers ref http www tia 942 org tia 942 telecommunications infrastructure standard for data centers ref specifies the minimum requirements for telecommunications infrastructure of data centers and computer rooms including single tenant enterprise data centers and multi tenant internet hosting data centers the topology proposed in this document is intended to be applicable to any size data center ref cite web url http www tiaonline org standards title archived copy accessdate 2011 11 07 deadurl yes archiveurl https web archive org web 20111106042758 http www tiaonline org standards archivedate 2011 11 06 df ref telcordia gr 3160 nebs requirements for telecommunications data center equipment and spaces ref http telecom info telcordia com site cgi ido docs cgi id search document gr 3160 gr 3160 nebs requirements for telecommunications data center equipment and spaces ref provides guidelines for data center spaces within telecommunications networks and environmental requirements for the equipment intended for installation in those spaces these criteria were developed jointly by telcordia and industry representatives they may be applied to data center spaces housing data processing or information technology it equipment the equipment may be used to operate and manage a carrier s telecommunication network provide data center based applications directly to the carrier s customers provide hosted applications for a third party to provide services to their customers provide a combination of these and similar data center applications effective data center operation requires a balanced investment in both the facility and the housed equipment the first step is to establish a baseline facility environment suitable for equipment installation standardization and modularity can yield savings and efficiencies in the design and construction of telecommunications data centers standardization means integrated building and equipment engineering modularity has the benefits of scalability and easier growth even when planning forecasts are less than optimal for these reasons telecommunications data centers should be planned in repetitive building blocks of equipment and associated power and support conditioning equipment when practical the use of dedicated centralized systems requires more accurate forecasts of future needs to prevent expensive over construction or perhaps worse nbsp under construction that fails to meet future needs the lights out data center also known as a darkened or a dark data center is a data center that ideally has all but eliminated the need for direct access by personnel except under extraordinary circumstances because of the lack of need for staff to enter the data center it can be operated without lighting all of the devices are accessed and managed by remote systems with automation programs used to perform unattended operations in addition to the energy savings reduction in staffing costs and the ability to locate the site further from population centers implementing a lights out data center reduces the threat of malicious attacks upon the infrastructure ref cite book first victor last kasacavage year 2002 page 227 title complete book of remote access connectivity and security series the auerbach best practices series publisher crc press isbn 0 8493 1253 1 ref ref cite book author1 burkey roxanne e author2 breakfield charles v year 2000 title designing a total data solution technology implementation and deployment page 24 series auerbach best practices publisher crc press isbn 0 8493 0893 3 ref there is a trend to modernize data centers in order to take advantage of the performance and energy efficiency increases of newer it equipment and capabilities such as cloud computing this process is also known as data center transformation ref name mspmentor net mukhar nicholas hp updates data center transformation solutions august 17 2011 http www mspmentor net 2011 08 17 hp updates data transformation solutions ref organizations are experiencing rapid it growth but their data centers are aging industry research company international data corporation idc puts the average age of a data center at nine years old ref name mspmentor net gartner another research company says data centers older than seven years are obsolete ref cite web url http www forbes com 2010 03 12 cloud computing ibm technology cio network data centers html title sperling ed next generation data centers forbes march 15 2010 publisher forbes com date accessdate 2013 08 30 ref in may 2011 data center research organization uptime institute reported that 36 percent of the large companies it surveyed expect to exhaust it capacity within the next 18 months ref niccolai james data centers turn to outsourcing to meet capacity needs cio com may 10 2011 http www cio com article 681897 data centers turn to outsourcing to meet capacity needs ref data center transformation takes a step by step approach through integrated projects carried out over time this differs from a traditional method of data center upgrades that takes a serial and siloed approach ref tang helen three signs it s time to transform your data center august 3 2010 data center knowledge http www datacenterknowledge com archives 2010 08 03 three signs it e2 80 99s time to transform your data center ref the typical projects within a data center transformation initiative include standardization consolidation virtualization automation and security standardization consolidation the purpose of this project is to reduce the number of data centers a large organization may have this project also helps to reduce the number of hardware software platforms tools and processes within a data center organizations replace aging data center equipment with newer ones that provide increased capacity and performance computing networking and management platforms are standardized so they are easier to manage ref name datacenterknowledge com miller rich complexity growing data center challenge data center knowledge may 16 2007 http www datacenterknowledge com archives 2007 05 16 complexity growing data center challenge ref virtualize there is a trend to use it virtualization technologies to replace or consolidate multiple data center equipment such as servers virtualization helps to lower capital and operational expenses ref sims david carousel s expert walks through major benefits of virtualization tmc net july 6 2010 http virtualization tmcnet com topics virtualization articles 193652 carousels expert walks through major benefits virtualization htm ref and reduce energy consumption ref delahunty stephen the new urgency for server virtualization informationweek august 15 2011 http www informationweek com news government enterprise architecture 231300585 ref virtualization technologies are also used to create virtual desktops which can then be hosted in data centers and rented out on a subscription basis ref cite web title hvd the cloud s silver lining url http www intrinsictechnology co uk fileuploads hvd whitepaper pdf publisher intrinsic technology accessdate 2012 08 30 ref data released by investment bank lazard capital markets reports that 48 percent of enterprise operations will be virtualized by 2012 gartner views virtualization as a catalyst for modernization ref miller rich gartner virtualization disrupts server vendors data center knowledge december 2 2008 http www datacenterknowledge com archives 2008 12 02 gartner virtualization disrupts server vendors ref automating data center automation involves automating tasks such as provisioning configuration patch computing patching release management and compliance as enterprises suffer from few skilled it workers ref name datacenterknowledge com automating tasks make data centers run more efficiently securing in modern data centers the security of data on virtual systems is integrated with existing security of physical infrastructures ref ritter ted nemertes research securing the data center transformation aligning security and data center dynamics http lippisreport com 2011 05 securing the data center transformation aligning security and data center dynamics ref the security of a modern data center must take into account physical security network security and data and user security carrier neutrality today many data centers are run by internet service provider s solely for the purpose of hosting their own and third party server computing servers however traditionally data centers were either built for the sole use of one large company or as carrier hotel s or network neutral data center s these facilities enable interconnection of carriers and act as regional fiber hubs serving local business in addition to hosting content server computing servers data center tiers linked from data availability the telecommunications industry association is a trade association accredited by ansi american national standards institute in 2005 it published http global ihs com doc detail cfm currency code usd customer id 2125452b2c0a oshid 2125452b2c0a shopping cart id 292558332c4a2020495a4d3b200a country code us lang code engl item s key 00414811 item key date 940819 input doc number tia 942 input doc title ansi tia 942 telecommunications infrastructure standard for data centers which defined four levels called tiers of data centers in a thorough quantifiable manner tia 942 was amended in 2008 and again in 2010 tia 942 data center standards overview describes the requirements for the data center infrastructure the simplest is a tier 1 data center which is basically a server room following basic guidelines for the installation of computer systems the most stringent level is a tier 4 data center which is designed to host mission critical computer systems with fully redundant subsystems and compartmentalized security zones controlled by biometric access controls methods another consideration is the placement of the data center in a subterranean context for data security as well as environmental considerations such as cooling requirements ref a connectkentucky article mentioning stone mountain data center complex cite web title global data corp to use old mine for ultra secure data storage facility url http connectkentucky org documents connected fall final pdf format pdf publisher connectkentucky accessdate 2007 11 01 date 2007 11 01 ref the german datacenter star audit program uses an auditing process to certify 5 levels of gratification that affect data center criticality independent from the ansi tia 942 standard the uptime institute a think tank and professional services organization based in santa fe new mexico santa fe new mexico has defined its own four levels the levels describe the availability of data from the hardware at a location the higher the tier the greater the availability the levels are ref a document from the uptime institute describing the different tiers click through the download page cite web title data center site infrastructure tier standard topology url http uptimeinstitute org index php option com docman task doc download gid 82 format pdf publisher uptime institute accessdate 2010 02 13 date 2010 02 13 deadurl yes archiveurl https web archive org web 20100613072610 http uptimeinstitute org index php option com docman task doc download gid 82 archivedate 2010 06 13 df ref ref the rating guidelines from the uptime institute cite web title data center site infrastructure tier standard topology url http professionalservices uptimeinstitute com uips pdf tierstandard pdf format pdf publisher uptime institute accessdate 2010 02 13 date 2010 02 13 deadurl yes archiveurl https web archive org web 20091007121511 http professionalservices uptimeinstitute com 80 uips pdf tierstandard pdf archivedate 2009 10 07 df ref class wikitable tier level requirements 1 single non redundant distribution path serving the it equipment non redundant capacity components basic site infrastructure with expected availability of 99 671 2 meets or exceeds all tier 1 requirements redundant site infrastructure capacity components with expected availability of 99 741 3 meets or exceeds all tier 2 requirements multiple independent distribution paths serving the it equipment all it equipment must be dual powered and fully compatible with the topology of a site s architecture concurrently maintainable site infrastructure with expected availability of 99 982 4 meets or exceeds all tier 3 requirements all cooling equipment is independently dual powered including chillers and heating ventilating and air conditioning hvac systems fault tolerant site infrastructure with electrical power storage and distribution facilities with expected availability of 99 995 the difference between 99 671 99 741 99 982 and 99 995 while seemingly nominal could be significant depending on the application whilst no down time is ideal the tier system allows for unavailability of services as listed below over a period of one year 525 600 minutes tier 1 99 671 status would allow 1729 224 minutes or 28 817 hours tier 2 99 741 status would allow 1361 304 minutes or 22 688 hours tier 3 99 982 status would allow 94 608 minutes or 1 5768 hours tier 4 99 995 status would allow 26 28 minutes or 0 438 hours the uptime institute also classifies the tiers in different categories design documents constructed facility operational sustainability ref name uptimeinstitute cite web url http uptimeinstitute com tiercertification title uptime institute tier certification publisher uptimeinstitute com accessdate 2014 08 27 ref design considerations file rack001 jpg thumb right a typical server rack commonly seen in colocation center colocation a data center can occupy one room of a building one or more floors or an entire building most of the equipment is often in the form of servers mounted in 19 inch rack cabinets which are usually placed in single rows forming corridors so called aisles between them this allows people access to the front and rear of each cabinet servers differ greatly in size from rack unit 1u servers to large freestanding storage silos which occupy many square feet of floor space some equipment such as mainframe computer s and computer storage storage devices are often as big as the racks themselves and are placed alongside them very large data centers may use intermodal container shipping containers packed with 1 000 or more servers each ref cite web url https www youtube com watch v zrwpsfplx8i title google container datacenter tour video ref when repairs or upgrades are needed whole containers are replaced rather than repairing individual servers ref cite web title walking the talk microsoft builds first major container based data center url http www computerworld com action article do command viewarticlebasic articleid 9075519 archiveurl https web archive org web 20080612193106 http www computerworld com action article do command viewarticlebasic articleid 9075519 archivedate 2008 06 12 accessdate 2008 09 22 ref local building codes may govern the minimum ceiling heights design programming design programming also known as architectural programming is the process of researching and making decisions to identify the scope of a design project ref cherry edith architectural programming introduction whole building design guide sept 2 2009 ref other than the architecture of the building itself there are three elements to design programming for data centers facility topology design space planning engineering infrastructure design mechanical systems such as cooling and electrical systems including power and technology infrastructure design cable plant each will be influenced by performance assessments and modelling to identify gaps pertaining to the owner s performance wishes of the facility over time various vendors who provide data center design services define the steps of data center design slightly differently but all address the same basic aspects as given below modeling criteria modeling criteria are used to develop future state scenarios for space power cooling and costs in the data center ref mullins robert romonet offers predictive modelling tool for data center planning network computing june 29 2011 http www networkcomputing com data center 231000669 ref the aim is to create a master plan with parameters such as number size location topology it floor system layouts and power and cooling technology and configurations the purpose of this is to allow for efficient use of the existing mechanical and electrical systems and also growth in the existing data center without the need for developing new buildings and further upgrading of incoming power supply design recommendations design recommendations plans generally follow the modelling criteria phase the optimal technology infrastructure is identified and planning criteria are developed such as critical power capacities overall data center power requirements using an agreed upon pue power utilization efficiency mechanical cooling capacities kilowatts per cabinet raised floor space and the resiliency level for the facility conceptual design conceptual designs embody the design recommendations or plans and should take into account what if scenarios to ensure all operational outcomes are met in order to future proof the facility conceptual floor layouts should be driven by it performance requirements as well as lifecycle costs associated with it demand energy efficiency cost efficiency and availability future proofing will also include expansion capabilities often provided in modern data centers through modular designs these allow for more raised floor space to be fitted out in the data center whilst utilising the existing major electrical plant of the facility detailed design detailed design is undertaken once the appropriate conceptual design is determined typically including a proof of concept the detailed design phase should include the detailed architectural structural mechanical and electrical information and specification of the facility at this stage development of facility schematics and construction documents as well as schematics and performance specification and specific detailing of all technology infrastructure detailed it infrastructure design and it infrastructure documentation are produced mechanical engineering infrastructure designs file crac cabinets 2 jpg thumb crac air handler mechanical engineering infrastructure design addresses mechanical systems involved in maintaining the interior environment of a data center such as heating ventilation and air conditioning hvac humidification and dehumidification equipment pressurization and so on ref name nxtbook com jew jonathan bicsi data center standard a resource for today s data center operators and designers bicsi news magazine may june 2010 page 28 http www nxtbook com nxtbooks bicsi news 20100506 26 ref this stage of the design process should be aimed at saving space and costs while ensuring business and reliability objectives are met as well as achieving pue and green requirements ref data center energy management best practices checklist mechanical lawrence berkeley national laboratory http hightech lbl gov dctraining strategies mam html ref modern designs include modularizing and scaling it loads and making sure capital spending on the building construction is optimized electrical engineering infrastructure design electrical engineering infrastructure design is focused on designing electrical configurations that accommodate various reliability requirements and data center sizes aspects may include utility service planning distribution switching and bypass from power sources uninterruptable power source ups systems and more ref name nxtbook com these designs should dovetail to energy standards and best practices while also meeting business objectives electrical configurations should be optimized and operationally compatible with the data center user s capabilities modern electrical design is modular and scalable ref clark jeff hedging your data center power the data center journal oct 5 2011 http www datacenterjournal com design hedging your data center power ref and is available for low and medium voltage requirements as well as dc direct current technology infrastructure design file under floor cable runs tee jpg thumb under floor cable runs technology infrastructure design addresses the telecommunications cabling systems that run throughout data centers there are cabling systems for all data center environments including horizontal cabling voice modem and facsimile telecommunications services premises switching equipment computer and telecommunications management connections keyboard video mouse connections and data communications ref jew jonathan bicsi data center standard a resource for today s data center operators and designers bicsi news magazine may june 2010 page 30 http www nxtbook com nxtbooks bicsi news 20100506 26 ref wide area local area and storage area networks should link with other building signaling systems e g fire security power hvac ems availability expectations the higher the availability needs of a data center the higher the capital and operational costs of building and managing it business needs should dictate the level of availability required and should be evaluated based on characterization of the criticality of it systems estimated cost analyses from modeled scenarios in other words how can an appropriate level of availability best be met by design criteria to avoid financial and operational risks as a result of downtime if the estimated cost of downtime within a specified time unit exceeds the amortized capital costs and operational expenses a higher level of availability should be factored into the data center design if the cost of avoiding downtime greatly exceeds the cost of downtime itself a lower level of availability should be factored into the design ref clark jeffrey the price of data center availability how much availability do you need oct 12 2011 the data center journal http www datacenterjournal com home news languages item 2792 the price of data center availability ref site selection aspects such as proximity to available power grids telecommunications infrastructure networking services transportation lines and emergency services can affect costs risk security and other factors to be taken into consideration for data center design whilst a wide array of location factors are taken into account e g flight paths neighbouring uses geological risks access to suitable available power is often the longest lead time item location affects data center design also because the climatic conditions dictate what cooling technologies should be deployed in turn this impacts uptime and the costs associated with cooling ref tucci linda five tips on selecting a data center location may 7 2008 searchcio com http searchcio techtarget com news 1312614 five tips on selecting a data center location ref for example the topology and the cost of managing a data center in a warm humid climate will vary greatly from managing one in a cool dry climate modularity and flexibility file cabinet asile jpg thumb cabinet aisle in a data center main article modular data center modularity and flexibility are key elements in allowing for a data center to grow and change over time data center modules are pre engineered standardized building blocks that can be easily configured and moved as needed ref niles susan standardization and modularity in data center physical infrastructure 2011 schneider electric page 4 http www apcmedia com salestools vavr 626vpd r1 en pdf ref a modular data center may consist of data center equipment contained within shipping containers or similar portable containers ref pitchaikani bala strategies for the containerized data center datacenterknowledge com sept 8 2011 http www datacenterknowledge com archives 2011 09 08 strategies for the containerized data center ref but it can also be described as a design style in which components of the data center are prefabricated and standardized so that they can be constructed moved or added to quickly as needs change ref niccolai james hp says prefab data center cuts costs in half infoworld july 27 2010 http www infoworld com d green it hp says prefab data center cuts costs in half 837 page 0 0 ref environmental control main article data center environmental control the physical environment of a data center is rigorously controlled air conditioning is used to control the temperature and humidity in the data center ashrae s thermal guidelines for data processing environments ref cite book title thermal guidelines for data processing environments year 2012 publisher american society of heating refrigerating and air conditioning engineers isbn 978 1936504 33 6 author ashrae technical committee 9 9 mission critical facilities technology spaces and electronic equipment edition 3 ref recommends a temperature range of convert 18 27 c f a dew point range of convert 5 15 c f and a relative humidity between 40 to 60 for data center environments ref name serverscheck cite web title best practices for data center monitoring and server room monitoring url https serverscheck com sensors temperature best practices asp author serverscheck accessdate 2016 10 07 ref the temperature in a data center will naturally rise because the electrical power used heats the air unless the heat is removed the ambient temperature will rise resulting in electronic equipment malfunction by controlling the air temperature the server components at the board level are kept within the manufacturer s specified temperature humidity range air conditioning systems help control humidity by cooling the return space air below the dew point too much humidity and water may begin to condensation condense on internal components in case of a dry atmosphere ancillary humidification systems may add water vapor if the humidity is too low which can result in electrostatics static electricity discharge problems which may damage components subterranean data centers may keep computer equipment cool while expending less energy than conventional designs modern data centers try to use economizer cooling where they use outside air to keep the data center cool at least one data center located in upstate new york will cool servers using outside air during the winter they do not use chillers air conditioners which creates potential energy savings in the millions ref cite news url http www reuters com article pressrelease idus141369 14 sep 2009 prn20090914 work reuters title tw telecom and nyserda announce co location expansion date 2009 09 14 ref increasingly http www datacenterdynamics com focus archive 2013 09 air air combat indirect air cooling wars 0 indirect air cooling is being deployed in data centers globally which has the advantage of more efficient cooling which lowers power consumption costs in the data center telcordia http telecom info telcordia com site cgi ido docs cgi id search document gr 2930 gr 2930 nebs raised floor generic requirements for network and data centers presents generic engineering requirements for raised floors that fall within the strict nebs guidelines there are many types of commercially available floors that offer a wide range of structural strength and loading capabilities depending on component construction and the materials used the general types of raised floor s include stringer stringerless and structural platforms all of which are discussed in detail in gr 2930 and summarized below stringered raised floors this type of raised floor generally consists of a vertical array of steel pedestal assemblies each assembly is made up of a steel base plate tubular upright and a head uniformly spaced on two foot centers and mechanically fastened to the concrete floor the steel pedestal head has a stud that is inserted into the pedestal upright and the overall height is adjustable with a leveling nut on the welded stud of the pedestal head stringerless raised floors one non earthquake type of raised floor generally consists of an array of pedestals that provide the necessary height for routing cables and also serve to support each corner of the floor panels with this type of floor there may or may not be provisioning to mechanically fasten the floor panels to the pedestals this stringerless type of system having no mechanical attachments between the pedestal heads provides maximum accessibility to the space under the floor however stringerless floors are significantly weaker than stringered raised floors in supporting lateral loads and are not recommended structural platforms one type of structural platform consists of members constructed of steel angles or channels that are welded or bolted together to form an integrated platform for supporting equipment this design permits equipment to be fastened directly to the platform without the need for toggle bars or supplemental bracing structural platforms may or may not contain panels or stringers data centers typically have raised floor ing made up of convert 60 cm ft abbr on 0 removable square tiles the trend is towards convert 80 100 cm in abbr on void to cater for better and uniform air distribution these provide a plenum space plenum for air to circulate below the floor as part of the air conditioning system as well as providing space for power cabling metal whiskers raised floors and other metal structures such as cable trays and ventilation ducts have caused many problems with zinc whiskers in the past and likely are still present in many data centers this happens when microscopic metallic filaments form on metals such as zinc or tin that protect many metal structures and electronic components from corrosion maintenance on a raised floor or installing of cable etc can dislodge the whiskers which enter the airflow and may short circuit server components or power supplies sometimes through a high current metal vapor plasma arc this phenomenon is not unique to data centers and has also caused catastrophic failures of satellites and military hardware ref cite web title nasa metal whiskers research url http nepp nasa gov whisker other whisker index htm publisher nasa accessdate 2011 08 01 ref electrical power file datacenter backup batteries jpg thumb right a bank of batteries in a large data center used to provide power until diesel generators can start backup power consists of one or more uninterruptible power supply uninterruptible power supplies battery banks and or diesel generator diesel gas turbine generators ref detailed explanation of ups topologies cite web url http www emersonnetworkpower com en us brands liebert documents white 20papers evaluating 20the 20economic 20impact 20of 20ups 20technology pdf format pdf title evaluating the economic impact of ups technology deadurl yes archiveurl https web archive org web 20101122074817 http emersonnetworkpower com en us brands liebert documents white 20papers evaluating 20the 20economic 20impact 20of 20ups 20technology pdf archivedate 2010 11 22 df ref to prevent single point of failure single points of failure all elements of the electrical systems including backup systems are typically fully duplicated and critical servers are connected to both the a side and b side power feeds this arrangement is often made to achieve n 1 redundancy in the systems transfer switch static transfer switch static transfer switches are sometimes used to ensure instantaneous switchover from one supply to the other in the event of a power failure low voltage cable routing data cabling is typically routed through overhead cable tray s in modern data centers but some who date may 2012 are still recommending under raised floor cabling for security reasons and to consider the addition of cooling systems above the racks in case this enhancement is necessary smaller less expensive data centers without raised flooring may use anti static tiles for a flooring surface computer cabinets are often organized into a data center environmental control aisle containment hot aisle arrangement to maximize airflow efficiency fire protection file fm200 three jpg thumb fm200 fire suppression tanks data centers feature fire protection systems including passive fire protection passive and active design elements as well as implementation of fire prevention programs in operations smoke detectors are usually installed to provide early warning of a fire at its incipient stage this allows investigation interruption of power and manual fire suppression using hand held fire extinguishers before the fire grows to a large size an active fire protection system such as a fire sprinkler system or a clean agent fire suppression gaseous system is often provided to control a full scale fire if it develops high sensitivity smoke detectors such as aspirating smoke detector s activating clean agent fire suppression gaseous systems activate earlier than fire sprinklers sprinklers structure protection and building life safety clean agents business continuity and asset protection no water no collateral damage or clean up passive fire protection elements include the installation of firewall construction fire walls around the data center so a fire can be restricted to a portion of the facility for a limited time in the event of the failure of the active fire protection systems fire wall penetrations into the server room such as cable penetrations coolant line penetrations and air ducts must be provided with fire rated penetration assemblies such as fire stop ping security physical security also plays a large role with data centers physical access to the site is usually restricted to selected personnel with controls including a layered security system often starting with fencing bollard s and mantrap access control mantraps ref cite web author sarah d scalet url http www csoonline com article 220665 title 19 ways to build physical security into a data center publisher csoonline com date 2005 11 01 accessdate 2013 08 30 ref video camera surveillance and permanent security guard s are almost always present if the data center is large or contains sensitive information on any of the systems within the use of finger print recognition mantrap snare mantrap s is starting to be commonplace energy use file google data center the dalles jpg thumb google data centers google data center the dalles oregon main article it energy management energy use is a central issue for data centers power draw for data centers ranges from a few kw for a rack of servers in a closet to several tens of mw for large facilities some facilities have power densities more than 100 times that of a typical office building ref cite web url http www1 eere energy gov femp program dc energy consumption html title data center energy consumption trends publisher u s department of energy accessdate 2010 06 10 ref for higher power density facilities electricity costs are a dominant operating expense and account for over 10 of the total cost of ownership tco of a data center ref j koomey c belady m patterson a santos k d lange http www intel com assets pdf general servertrendsreleasecomplete v25 pdf assessing trends over time in performance costs and energy use for servers released on the web august 17th 2009 ref by 2012 the cost of power for the data center is expected to exceed the cost of the original capital investment ref cite web url http www1 eere energy gov femp pdfs data center qsguide pdf title quick start guide to increase data center energy efficiency publisher u s department of energy accessdate 2010 06 10 deadurl yes archiveurl https web archive org web 20101122035456 http www1 eere energy gov 80 femp pdfs data center qsguide pdf archivedate 2010 11 22 df ref greenhouse gas emissions in 2007 the entire information and communication technologies or ict sector was estimated to be responsible for roughly 2 of global greenhouse gas carbon emissions with data centers accounting for 14 of the ict footprint ref name smart1 cite web url http www smart2020 org assets files 03 smart2020report lo res pdf title smart 2020 enabling the low carbon economy in the information age publisher the climate group for the global e sustainability initiative accessdate 2008 05 11 deadurl yes archiveurl https web archive org web 20110728032834 http www smart2020 org assets files 03 smart2020report lo res pdf archivedate 2011 07 28 df ref the us epa estimates that servers and data centers are responsible for up to 1 5 of the total us electricity consumption ref name energystar1 cite web url http www energystar gov ia partners prod development downloads epa datacenter report congress final1 pdf title report to congress on server and data center energy efficiency publisher u s environmental protection agency energy star program ref or roughly 5 of us ghg emissions ref a calculation of data center electricity burden cited in the http www energystar gov ia partners prod development downloads epa datacenter report congress final1 pdf report to congress on server and data center energy efficiency and electricity generation contributions to green house gas emissions published by the epa in the http epa gov climatechange emissions downloads10 us ghg inventory 2010 executivesummary pdf greenhouse gas emissions inventory report retrieved 2010 06 08 ref for 2007 given a business as usual scenario greenhouse gas emissions from data centers is projected to more than double from 2007 levels by 2020 ref name smart1 siting is one of the factors that affect the energy consumption and environmental effects of a datacenter in areas where climate favors cooling and lots of renewable electricity is available the environmental effects will be more moderate thus countries with favorable conditions such as canada ref http www theglobeandmail com report on business canada called prime real estate for massive data computers article2071677 canada called prime real estate for massive data computers globe mail retrieved june 29 2011 ref finland ref http datacenter siting weebly com finland first choice for siting your cloud computing data center retrieved 4 august 2010 ref sweden ref cite web url http www stockholmbusinessregion se templates page 41724 aspx epslanguage en title stockholm sets sights on data center customers accessdate 4 august 2010 archiveurl https web archive org web 20100819190918 http www stockholmbusinessregion se templates page 41724 aspx epslanguage en archivedate 19 august 2010 ref norway ref http www innovasjonnorge no en start page invest in norway industries datacenters in a world of rapidly increasing carbon emissions from the ict industry norway offers a sustainable solution retrieved 1 march 2016 ref and switzerland ref http www greenbiz com news 2010 06 30 swiss carbon neutral servers hit cloud swiss carbon neutral servers hit the cloud retrieved 4 august 2010 ref are trying to attract cloud computing data centers in an 18 month investigation by scholars at rice university s baker institute for public policy in houston and the institute for sustainable and applied infodynamics in singapore data center related emissions will more than triple by 2020 ref cite news author katrice r jalbuena title green business news quote publisher ecoseed date october 15 2010 pages url http ecoseed org en business article list article 1 business 8219 i t industry risks output cut in low carbon economy accessdate 2010 11 11 deadurl yes archiveurl https web archive org web 20160618081417 http ecoseed org en business article list article 1 business 8219 i t industry risks output cut in low carbon economy archivedate 2016 06 18 df ref energy efficiency the most commonly used metric to determine the energy efficiency of a data center is power usage effectiveness or pue this simple ratio is the total power entering the data center divided by the power used by the it equipment math mathrm pue mbox total facility power over mbox it equipment power math total facility power consists of power used by it equipment plus any overhead power consumed by anything that is not considered a computing or data communication device i e cooling lighting etc an ideal pue is 1 0 for the hypothetical situation of zero overhead power the average data center in the us has a pue of 2 0 ref name energystar1 meaning that the facility uses two watts of total power overhead it equipment for every watt delivered to it equipment state of the art data center energy efficiency is estimated to be roughly 1 2 ref cite web url https microsite accenture com svlgreport documents pdf svlg report pdf title data center energy forecast publisher silicon valley leadership group ref some large data center operators like microsoft and yahoo have published projections of pue for facilities in development google publishes quarterly actual efficiency performance from data centers in operation ref cite web url https www google com about datacenters efficiency internal title efficiency how we do it data centers publisher google accessdate 2015 01 19 ref the u s environmental protection agency has an energy star rating for standalone or large data centers to qualify for the ecolabel a data center must be within the top quartile of energy efficiency of all reported facilities ref commentary on introduction of energy star for data centers cite web title introducing epa energy star for data centers url http www emerson com edc post 2010 06 15 introducing epa energy starc2ae for data centers aspx format web site publisher jack pouchet accessdate 2010 09 27 date 2010 09 27 deadurl yes archiveurl https web archive org web 20100925210539 http emerson com edc post 2010 06 15 introducing epa energy starc2ae for data centers aspx archivedate 2010 09 25 df ref european union also has a similar initiative eu code of conduct for data centres ref cite web url http iet jrc ec europa eu energyefficiency ict codes conduct data centres energy efficiency title eu code of conduct for data centres publisher iet jrc ec europa eu date accessdate 2013 08 30 ref energy use analysis often the first step toward curbing energy use in a data center is to understand how energy is being used in the data center multiple types of analysis exist to measure data center energy use aspects measured include not just energy used by it equipment itself but also by the data center facility equipment such as chillers and fans ref sweeney jim reducing data center power and energy consumption saving money and going green gtsi solutions pages 2 3 http www gtsi com cms documents white papers green it pdf ref power and cooling analysis power is the largest recurring cost to the user of a data center ref name drj choosing citation title choosing a data center url http www atlantic net images pdf choosing a data center pdf publisher disaster recovery journal year 2009 author cosmano joe accessdate 2012 07 21 ref a power and cooling analysis also referred to as a thermal assessment measures the relative temperatures in specific areas as well as the capacity of the cooling systems to handle specific ambient temperatures ref needle david hp s green data center portfolio keeps growing internetnews july 25 2007 http www internetnews com xsp article php 3690651 hps green data center portfolio keeps growing htm ref a power and cooling analysis can help to identify hot spots over cooled areas that can handle greater power use density the breakpoint of equipment loading the effectiveness of a raised floor strategy and optimal equipment positioning such as ac units to balance temperatures across the data center power cooling density is a measure of how much square footage the center can cool at maximum capacity ref name inc howtochoose citation title how to choose a data center url http www inc com guides 2010 11 how to choose a data center pagen 2 html year 2010 author inc staff accessdate 2012 07 21 ref energy efficiency analysis an energy efficiency analysis measures the energy use of data center it and facilities equipment a typical energy efficiency analysis measures factors such as a data center s power use effectiveness pue against industry standards identifies mechanical and electrical sources of inefficiency and identifies air management metrics ref siranosian kathryn hp shows companies how to integrate energy management and carbon reduction triplepundit april 5 2011 http www triplepundit com 2011 04 hp launches program companies integrate manage energy carbon reduction strategies ref computational fluid dynamics cfd analysis main article computational fluid dynamics this type of analysis uses sophisticated tools and techniques to understand the unique thermal conditions present in each data center predicting the temperature airflow and pressure behavior of a data center to assess performance and energy consumption using numerical modeling ref bullock michael computation fluid dynamics hot topic at data center world transitional data services march 18 2010 http blog transitionaldata com aggregate bid 37840 seeing the invisible data center with cfd modeling software webarchive url https web archive org web 20120103183406 http blog transitionaldata com aggregate bid 37840 seeing the invisible data center with cfd modeling software date january 3 2012 ref by predicting the effects of these environmental conditions cfd analysis in the data center can be used to predict the impact of high density racks mixed with low density racks ref bouley dennis editor impact of virtualization on data center physical infrastructure the green grid 2010 http www thegreengrid org media whitepapers white paper 27 impact of virtualization data on center physical infrastructure 020210 pdf lang en ref and the onward impact on cooling resources poor infrastructure management practices and ac failure of ac shutdown for scheduled maintenance thermal zone mapping thermal zone mapping uses sensors and computer modeling to create a three dimensional image of the hot and cool zones in a data center ref fontecchio mark hp thermal zone mapping plots data center hot spots searchdatacenter july 25 2007 http searchdatacenter techtarget com news 1265634 hp thermal zone mapping plots data center hot spots ref this information can help to identify optimal positioning of data center equipment for example critical servers might be placed in a cool zone that is serviced by redundant ac units green data centers file magazin vauban e jpg thumb this water cooled data center in the independent port of strasbourg port of strasbourg france claims the attribute green data centers use a lot of power consumed by two main usages the power required to run the actual equipment and then the power required to cool the equipment the first category is addressed by designing computers and storage systems that are increasingly power efficient ref name referencedc2 to bring down cooling costs data center designers try to use natural ways to cool the equipment many data centers are located near good fiber connectivity power grid connections and also people concentrations to manage the equipment but there are also circumstances where the data center can be miles away from the users and don t need a lot of local management examples of this are the mass data centers like google or facebook these dc s are built around many standardized servers and storage arrays and the actual users of the systems are located all around the world after the initial build of a data center staff numbers required to keep it running are often relatively low especially data centers that provide mass storage or computing power which don t need to be near population centers data centers in arctic locations where outside air provides all cooling are getting more popular as cooling and electricity are the two main variable cost components ref cite web url http www gizmag com fjord cooled data center 20938 title fjord cooled dc in norway claims to be greenest access date 23 december 2011 ref network infrastructure file paris servers dsc00190 jpg thumb left an example of rack mounted servers communications in data centers today are most often based on computer network networks running the internet protocol ip protocol computing protocol suite data centers contain a set of router computing routers and network switch switches that transport traffic between the servers and to the outside world redundancy engineering redundancy of the internet connection is often provided by using two or more upstream service providers see multihoming some of the servers at the data center are used for running the basic internet and intranet services needed by internal users in the organization e g e mail servers proxy server s and domain name system dns servers network security elements are also usually deployed firewall networking firewalls vpn gateway computer networking gateways intrusion detection system s etc also common are monitoring systems for the network and some of the applications additional off site monitoring systems are also typical in case of a failure of communications inside the data center data center infrastructure management data center infrastructure management dcim is the integration of information technology it and facility management disciplines to centralize monitoring management and intelligent capacity planning of a data center s critical systems achieved through the implementation of specialized software hardware and sensors dcim enables common real time monitoring and management platform for all interdependent systems across it and facility infrastructures depending on the type of implementation dcim products can help data center managers identify and eliminate sources of risk to increase availability of critical it systems dcim products also can be used to identify interdependencies between facility and it infrastructures to alert the facility manager to gaps in system redundancy and provide dynamic holistic benchmarks on power consumption and efficiency to measure the effectiveness of green it initiatives it s important to measure and understand data center efficiency metrics a lot of the discussion in this area has focused on energy issues but other metrics beyond the pue can give a more detailed picture of the data center operations server storage and staff utilization metrics can contribute to a more complete view of an enterprise data center in many cases disc capacity goes unused and in many instances the organizations run their servers at 20 utilization or less ref cite web url http content dell com us en enterprise d large business measure data center efficiency aspx title measuring data center efficiency easier said than done publisher dell com accessdate 2012 06 25 deadurl yes archiveurl https web archive org web 20101027083349 http content dell com 80 us en enterprise d large business measure data center efficiency aspx archivedate 2010 10 27 df ref more effective automation tools can also improve the number of servers or virtual machines that a single admin can handle dcim providers are increasingly linking with computational fluid dynamics providers to predict complex airflow patterns in the data center the cfd component is necessary to quantify the impact of planned future changes on cooling resilience capacity and efficiency ref name gartner cite web url http www gartner com it glossary computational fluid dynamic cfd analysis title computational fluid dynamic cfd analysis 124 gartner it glossary publisher gartner com accessdate 2014 08 27 ref managing the capacity of a data center unreferenced section date august 2016 file capacity of a datacenter life cycle jpg thumbnail left capacity of a datacenter life cycle several parameters may limit the capacity of a data center for long term usage the main limitations will be available area then available power in the first stage of its life cycle a data center will see its occupied space growing more rapidly than consumed energy with constant densification of new it technologies the need in energy is going to become dominant equaling then overcoming the need in area second then third phase of cycle the development and multiplication of connected objects the needs in storage and data treatment lead to the necessity of data centers to grow more and more rapidly it is therefore important to define a data center strategy before being cornered the decision conception and building cycle lasts several years therefore it is imperative to initiate this strategic consideration when the data center reaches about 50 of its power capacity maximum occupation of a data center needs to be stabilized around 85 be it in power or occupied area resources thus managed will allow a rotation zone for managing hardware replacement and will allow temporary cohabitation of old and new generations in the case where this limit would be overcrossed durably it would not be possible to proceed to material replacements which would invariably lead to smothering the information system the data center is a resource in its own right of the information system with its own constraints of time and management life span of 25 years it therefore needs to be taken into consideration in the framework of the si midterm planning between 3 and 5 years applications file ibmportablemodulardatacenter jpg thumb right a 40 foot portable modular data center the main purpose of a data center is running the it systems applications that handle the core business and operational data of the organization such systems may be proprietary and developed internally by the organization or bought from enterprise software vendors such common applications are enterprise resource planning erp and customer relationship management crm systems a data center may be concerned with just operations architecture or it may provide other services as well often these applications will be composed of multiple hosts each running a single component common components of such applications are database s file server s application server s middleware and various others data centers are also used for off site backups companies may subscribe to backup services provided by a data center this is often used in conjunction with tape drive backup tapes backups can be taken off servers locally on to tapes however tapes stored on site pose a security threat and are also susceptible to fire and flooding larger companies may also send their backups off site for added security this can be done by backing up to a data center encrypted backups can be sent over the internet to another data center where they can be stored securely for quick deployment or disaster recovery several large hardware vendors have developed mobile modular solutions that can be installed and made operational in very short time companies such as file edge night 02 jpg thumb a modular data center connected to the power grid at a utility substation cisco systems ref cite web title info and video about cisco s solution url http www datacenterknowledge com archives 2008 may 15 ciscos mobile emergency data center html publisher datacentreknowledge accessdate 2008 05 11 date may 15 2007 deadurl yes archiveurl https web archive org web 20080519213241 http www datacenterknowledge com 80 archives 2008 may 15 ciscos mobile emergency data center html archivedate 2008 05 19 df ref sun microsystems sun modular datacenter ref cite web url http www sun com products sunmd s20 specifications jsp archiveurl https web archive org web 20080513090300 http www sun com products sunmd s20 specifications jsp archivedate 2008 05 13 title technical specs of sun s blackbox accessdate 2008 05 11 ref ref and english wiki article on sun modular datacenter sun s modular datacentre ref groupe bull bull mobull ref cite web title mobull plug and boot datacenter url http www bull com extreme computing mobull html publisher bull first daniel last kidger accessdate 2011 05 24 ref ibm portable modular data center schneider electric portable modular data center hewlett packard hp hp performance optimized datacenter performance optimized datacenter ref cite web url http h18004 www1 hp com products servers solutions datacentersolutions pod index html title hp performance optimized datacenter pod 20c and 40c product overview publisher h18004 www1 hp com date accessdate 2013 08 30 ref huawei container data center solution ref cite web title huawei s container data center solution url http www huawei com ilink enenterprise download hw 143893 publisher huawei accessdate 2014 05 17 ref and google google modular data center have developed systems that could be used for this purpose ref cite web url http www crn com hardware 208403225 publisher channelweb accessdate 2008 05 11 title ibm s project big green takes second step first brian last kraemer date june 11 2008 deadurl yes archiveurl https web archive org web 20080611114732 http www crn com 80 hardware 208403225 archivedate 2008 06 11 df ref ref cite web url http hightech lbl gov documents data centers modular dc procurement guide pdf title modular container data centers procurement guide optimizing for energy efficiency and quick deployment format pdf date accessdate 2013 08 30 deadurl yes archiveurl https web archive org web 20130531191212 http hightech lbl gov documents data centers modular dc procurement guide pdf archivedate 2013 05 31 df ref baselayer has a patent on the software defined modular data center ref citation title system and method of providing computer resources url http www google com patents us8434804 date may 7 2013 accessdate 2016 02 24 first george last slessman ref ref cite web title modular data center firm io to split into two companies url http www datacenterknowledge com archives 2014 12 02 modular data center firm io to split into two companies website data center knowledge access date 2016 02 24 language en us ref us wholesale and retail colocation providers according to synergy research group the scale of the wholesale colocation market in the united states is very significant relative to the retail market with q3 wholesale revenues reaching almost 700 million digital realty trust is the wholesale market leader followed at a distance by dupont fabros synergy research also describes the us colocation market as the most mature and well developed in the world based on revenue and the continued adoption of cloud infrastructure services contains estimates from synergy research group ref name srgresearch cite web url https www srgresearch com articles mature us colocation market led equinix and centurylink savvis title mature us colocation market led by equinix and centurylink savvis 124 synergy research group author synergy research group reno nv publisher srgresearch com accessdate 2014 08 27 ref class wikitable sortable rank company name us market share 1 various providers 34 2 equinix 18 3 centurylink savvis 8 4 sungard 5 5 at t 5 6 verizon 5 7 telx 4 8 cyrusone 4 9 level 3 communications 3 10 internap 2 see also columns list colwidth 20em central apparatus room colocation center data center infrastructure management disaster recovery dynamic infrastructure electrical network hvac internet exchange point internet hosting service modular data center neher mcgrath network operations center open compute project by facebook peering server farm server room server room environment monitoring system server sprawl sun modular datacenter telecommunications network utah data center web hosting service anderson powerpole connector references reflist colwidth 30em external links commons category data centers wikibooks the design and organization of data centers wiktionary http hightech lbl gov datacenters html lawrence berkeley lab research development demonstration and deployment of energy efficient technologies and practices for data centers http hightech lbl gov dc powering faq html dc power for data centers of the future faq 380vdc testing and demonstration at a sun data center http www dccompendium com dc compendium repository and compendium of data centers globally http media wix com ugd fb8983 e929404b24874e4fa7a8279f1cda58f8 pdf white paper property taxes the new challenge for data centers authority control cloud computing defaultsort data center category computer networking category applications of distributed computing category cloud storage category data management category distributed data storage category distributed data storage systems category servers computing category data centers'
b'multiple issues pov date august 2015 one source date august 2015 notability date august 2015 data conditioning is the use of data management and optimization techniques which result in the intelligent routing optimization and protection of data for storage or data movement in a computer system data conditioning features enable enterprise and cloud data center s to dramatically improve system utilization and increase application performance lowering both capital expenditures and operating cost s data conditioning technologies delivered through a data conditioning platform optimize data as it moves through a computer s i o input output path or i o bus the data path between the main processor complex and storage subsystems the functions of a data conditioning platform typically reside on a storage controller add in card inserted into the pci e slots of a server this enables easy integration of new features in a server or a whole data center data conditioning features delivered via a data conditioning platform are designed to simplify system integration and minimize implementation risks associated with deploying new technologies by ensuring seamless compatibility with all leading server and storage hardware operating systems and applications and meeting all current commercial off the shelf cots standards by delivering optimization features via a data conditioning platform data center managers can improve system efficiency and reduce cost with minimal disruption and avoid the need to modify existing applications or operating systems and leverage existing hardware systems summary data conditioning builds on existing data storage functionality delivered in the i o path including raid redundant arrays of inexpensive disks intelligent i o based http www adaptec com en us common greenpower refurl greenpower power management and solid state drive ssd solid state drive performance caching techniques data conditioning is enabled both by advanced asic controller technology and intelligent software new data conditioning capabilities can be designed into and delivered via storage controllers in the i o path or to achieve the data center s technical and business goals data conditioning strategies can also be applied to improving server and storage utilization and for better managing a wide range of hardware and system level capabilities background and purpose data conditioning principles can be applied to any demanding computing environment to create significant cost performance and system utilization efficiencies and are typically deployed by data center managers system integrators and storage and server oems seeking to optimize hardware and software utilization simplified non intrusive technology integration and minimal risks and performance hits traditionally associated with incorporating new data center technologies references http www adaptec maxiq adaptec maxiq category data management'
b'about 1 business continuity planning 2 societal disaster recovery 3 emergency management other uses dr disambiguation merge to business continuity date june 2015 disaster recovery dr involves a set of policies and procedures to enable the recovery or continuation of vital technology infrastructure and systems following a natural disaster natural or man made hazards human induced disaster disaster recovery focuses on the it or technology systems supporting critical business functions ref http continuity georgetown edu dr systems and operations continuity disaster recovery georgetown university university information services retrieved 3 august 2012 ref as opposed to business continuity which involves keeping all essential aspects of a business functioning despite significant disruptive events disaster recovery is therefore a subset of business continuity ref http www 304 ibm com partnerworld gsd solutiondetails do solution 44832 expand true lc en disaster recovery and business continuity version 2011 webarchive url https web archive org web 20130111203921 http www 304 ibm com partnerworld gsd solutiondetails do solution 44832 expand true lc en date january 11 2013 ibm retrieved 3 august 2012 ref history disaster recovery developed in the mid to late 1970s as computer center managers began to recognize the dependence of their organizations on their computer systems at that time most systems were batch processing batch oriented mainframe computer mainframe s which in many cases could be downtime down for a number of days before significant damage would be done to the organization as awareness of the potential business disruption that would follow an it related disaster the disaster recovery industry developed to provide backup computer centers with sun information systems which later became sungard availability services becoming the first major us commercial hot site vendor established in 1978 in philadelphia during the 1980s and 90s customer awareness and industry both grew rapidly driven by the advent of open systems and real time computing real time processing which increased the dependence of organizations on their it systems regulations mandating business continuity and disaster recovery plans for organizations in various sectors of the economy imposed by the authorities and by business partners increased the demand and led to the availability of commercial disaster recovery services including mobile data centers delivered to a suitable recovery location by truck with the rapid growth of the internet through the late 1990s and into the 2000s organizations of all sizes became further dependent on the continuous availability of their it systems with some organizations setting objectives of 2 3 4 or 5 nines 99 999 availability of critical systems citation needed date march 2016 this increasing dependence on it systems as well as increased awareness from large scale disasters such as tsunami earthquake flood and volcanic eruption spawned disaster recovery related products and services ranging from high availability solutions to hot site facilities improved networking meant critical it services could be served remotely hence on site recovery became less important the rise of cloud computing since 2010 continues that trend nowadays it matters even less where computing services are physically served just so long as the network itself is sufficiently reliable a separate issue and less of a concern since modern networks are highly resilient by design recovery as a service raas is one of the security features or benefits of cloud computing being promoted by the cloud security alliance ref https cloudsecurityalliance org download secaas category 9 bcdr implementation guidance secaas category 9 bcdr implementation guidance csa retrieved 14 july 2014 ref classification of disasters disasters can be classified into two broad categories the first is natural disasters such as floods hurricanes tornadoes or earthquakes while preventing a natural disaster is impossible risk management measures such as avoiding disaster prone situations and good planning can help the second category is man made disasters such as hazardous material spills infrastructure failure bio terrorism and disastrous it bugs or failed change implementations in these instances surveillance testing and mitigation planning are invaluable importance of disaster recovery planning recent research supports the idea that implementing a more holistic pre disaster planning approach is more cost effective in the long run every 1 spent on hazard mitigation such as a disaster recovery plan saves society 4 in response and recovery costs ref cite web first partnership for disaster resilience title post disaster recovery planning forum how to guide url http nthmp tsunami gov minutes oct nov07 post disaster recovery planning forum uo csc 2 pdf publisher university of oregon s community service center accessdate 2013 05 23 ref as information technology it systems have become increasingly critical to the smooth operation of a company and arguably the economy as a whole the importance of ensuring the continued operation of those systems and their rapid recovery has increased for example of companies that had a major loss of business data 43 never reopen and 29 close within two years as a result preparation for continuation or recovery of systems needs to be taken very seriously this involves a significant investment of time and money with the aim of ensuring minimal losses in the event of a disruptive event ref cite web url http www ready gov business implementation it title it disaster recovery plan date 25 october 2012 publisher fema accessdate 11 may 2013 ref control measures control measures are steps or mechanisms that can reduce or eliminate various threats for organizations different types of measures can be included in disaster recovery plan drp disaster recovery planning is a subset of a larger process known as business continuity planning and includes planning for resumption of applications data hardware electronic communications such as networking and other it infrastructure a business continuity plan bcp includes planning for non it related aspects such as key personnel facilities crisis communication and reputation protection and should refer to the disaster recovery plan drp for it related infrastructure recovery continuity it disaster recovery control measures can be classified into the following three types preventive measures controls aimed at preventing an event from occurring detective measures controls aimed at detecting or discovering unwanted events corrective measures controls aimed at correcting or restoring the system after a disaster or an event good disaster recovery plan measures dictate that these three types of controls be documented and exercised regularly using so called dr tests strategies prior to selecting a disaster recovery strategy a disaster recovery planner first refers to their organization s business continuity plan which should indicate the key metrics of recovery point objective rpo and recovery time objective rto for various business processes such as the process to run payroll generate an order etc the metrics specified for the business processes are then mapped to the underlying it systems and infrastructure that support those processes ref gregory peter cisa certified information systems auditor all in one exam guide 2009 isbn 978 0 07 148755 9 page 480 ref incomplete rtos and rpos can quickly derail a disaster recovery plan every item in the dr plan requires a defined recovery point and time objective as failure to create them may lead to significant problems that can extend the disaster s impact ref cite web url http content dell com us en enterprise d large business mistakes that kill disaster aspx title five mistakes that can kill a disaster recovery plan publisher dell com accessdate 2012 06 22 deadurl yes archiveurl https web archive org web 20130116112225 http content dell com us en enterprise d large business mistakes that kill disaster aspx archivedate 2013 01 16 df ref once the rto and rpo metrics have been mapped to it infrastructure the dr planner can determine the most suitable recovery strategy for each system the organization ultimately sets the it budget and therefore the rto and rpo metrics need to fit with the available budget while most business unit heads would like zero data loss and zero time loss the cost associated with that level of protection may make the desired high availability solutions impractical a cost benefit analysis often dictates which disaster recovery measures are implemented traditionally a disaster recovery system involved cutover or switch over recovery systems citation needed date april 2016 such measures would allow an organization to preserve its technology and information by having a remote disaster recovery location that produced backups on a regular basis however this strategy proved to be expensive and time consuming therefore more affordable and effective cloud based systems were introduced some of the most common strategies for data recovery data protection include backups made to tape and sent off site at regular intervals backups made to disk on site and automatically copied to off site disk or made directly to off site disk replication of data to an off site location which overcomes the need to restore the data only the systems then need to be restored or synchronized often making use of storage area network san technology private cloud solutions which replicate the management data vms templates and disks into the storage domains which are part of the private cloud setup these management data are configured as an xml representation called ovf open virtualization format and can be restored once a disaster occurs hybrid cloud solutions that replicate both on site and to off site data centers these solutions provide the ability to instantly fail over to local on site hardware but in the event of a physical disaster servers can be brought up in the cloud data centers as well the use of high availability systems which keep both the data and system replicated off site enabling continuous access to systems and data even after a disaster often associated with cloud storage ref cite web url http www inc com guides 201106 how to use the cloud as a disaster recovery strategy html title how to use the cloud as a disaster recovery strategy last brandon first john date 23 june 2011 publisher inc accessdate 11 may 2013 ref in many cases an organization may elect to use an outsourced disaster recovery provider to provide a stand by site and systems rather than using their own remote facilities increasingly via cloud computing in addition to preparing for the need to recover systems organizations also implement precautionary measures with the objective of preventing a disaster in the first place these may include local mirrors of systems and or data and use of disk protection technology such as raid surge protectors to minimize the effect of power surges on delicate electronic equipment use of an uninterruptible power supply ups and or backup generator to keep systems going in the event of a power failure fire prevention mitigation systems such as alarms and fire extinguishers anti virus software and other security measures see also backup site high availability continuous data protection data recovery emergency management it service continuity remote backup service seven tiers of disaster recovery virtual tape library references reflist further reading iso iec 22301 2012 replacement of bs 25999 2007 societal security business continuity management systems requirements iso iec 27001 2013 replacement of iso iec 27001 2005 formerly bs 7799 2 2002 information security management system iso iec 27002 2013 replacement of iso iec 27002 2005 renumbered iso17799 2005 information security management code of practice iso iec 22399 2007 guideline for incident preparedness and operational continuity management iso iec 24762 2008 guidelines for information and communications technology disaster recovery services iwa 5 2006 emergency preparedness british standards institution bs 25999 1 2006 business continuity management part 1 code of practice bs 25999 2 2007 business continuity management part 2 specification bs 25777 2008 information and communications technology continuity management code of practice others a guide to business continuity planning by james c barnes business continuity planning a step by step guide with planning forms on cdrom by kenneth l fulmer disaster survival planning a practical guide for businesses by judy bell ice data management in case of emergency made simple by myriadoptima com harney j 2004 business continuity and disaster recovery back up or shut down aiim e doc magazine 18 4 42 48 dimattia s november 15 2001 planning for continuity library journal 32 34 external links https www ready gov business implementation it it disaster recovery plan from ready gov no more links please be cautious in adding more links to this article wikipedia is not a collection of links nor should it be used for advertising excessive or inappropriate links will be deleted see wikipedia external links wikipedia spam for details if there are already plentiful links please propose additions or replacements on this article s discussion page or submit your link to the relevant category at the open directory project dmoz org and link back to that category using the dmoz template no more links disasters defaultsort disaster recovery category disaster recovery category data management category backup category it risk management'
b'for the revision control concept commit revision control unreferenced date may 2014 in computer science and data management a commit is the making of a set of tentative changes permanent a popular usage is at the end of a database transaction transaction a commit is an act of committing data management a code commit sql commit code statement in sql ends a database transaction transaction within a relational database management system rdbms and makes all changes visible to other users the general format is to issue a code begin work sql begin work code statement one or more sql statements and then the code commit code statement alternatively a code rollback data management rollback code statement can be issued which undoes all the work performed since code begin work code was issued a code commit code statement will also release any existing savepoint s that may be in use in terms of transactions the opposite of commit is to discard the tentative changes of a transaction a rollback data management rollback see also commit version control atomic commit two phase commit protocol three phase commit protocol databases defaultsort commit data management category data management category sql category transaction processing comp sci stub'
b'pp pc1 in information technology and computer science especially in the fields of computer programming operating systems multiprocessor s and database s concurrency control ensures that correct results for concurrent computing concurrent operations are generated while getting those results as quickly as possible computer systems both software and computer hardware hardware consist of modules or components each component is designed to operate correctly i e to obey or to meet certain consistency rules when components that operate concurrently interact by messaging or by sharing accessed data in computer memory memory or computer data storage storage a certain component s consistency may be violated by another component the general area of concurrency control provides rules methods design methodologies and scientific theory theories to maintain the consistency of components operating concurrently while interacting and thus the consistency and correctness of the whole system introducing concurrency control into a system means applying operation constraints which typically result in some performance reduction operation consistency and correctness should be achieved with as good as possible efficiency without reducing performance below reasonable levels concurrency control can require significant additional complexity and overhead in a concurrent algorithm compared to the simpler sequential algorithm for example a failure in concurrency control can result in data corruption from torn data access operation torn read or write operations concurrency control in databases comments this section is applicable to all transactional systems i e to all systems that use database transaction s atomic transactions e g transactional objects in systems management and in networks of smartphone s which typically implement private dedicated database systems not only general purpose database management system s dbmss dbmss need to deal also with concurrency control issues not typical just to database transactions but rather to operating systems in general these issues e g see concurrency control concurrency control in operating systems concurrency control in operating systems below are out of the scope of this section concurrency control in database management system s dbms e g bern87 bernstein et al 1987 weikum01 weikum and vossen 2001 other database transaction transactional objects and related distributed applications e g grid computing and cloud computing ensures that database transaction s are performed concurrency computer science concurrently without violating the data integrity of the respective database s thus concurrency control is an essential element for correctness in any system where two database transactions or more executed with time overlap can access the same data e g virtually in any general purpose database system consequently a vast body of related research has been accumulated since database systems emerged in the early 1970s a well established concurrency control scientific theory theory for database systems is outlined in the references mentioned above serializability serializability theory which allows to effectively design and analyze concurrency control methods and mechanisms an alternative theory for concurrency control of atomic transactions over abstract data type s is presented in lynch1993 lynch et al 1993 and not utilized below this theory is more refined complex with a wider scope and has been less utilized in the database literature than the classical theory above each theory has its pros and cons emphasis and insight to some extent they are complementary and their merging may be useful to ensure correctness a dbms usually guarantees that only serializability serializable transaction schedule computer science schedule s are generated unless serializability is serializability relaxing serializability intentionally relaxed to increase performance but only in cases where application correctness is not harmed for maintaining correctness in cases of failed aborted transactions which can always happen for many reasons schedules also need to have the serializability correctness recoverability recoverability from abort property a dbms also guarantees that no effect of committed transactions is lost and no effect of aborted rollback data management rolled back transactions remains in the related database overall transaction characterization is usually summarized by the acid rules below as databases have become distributed database distributed or needed to cooperate in distributed environments e g federated database s in the early 1990 and cloud computing currently the effective distribution of concurrency control mechanisms has received special attention database transaction and the acid rules main database transaction acid the concept of a database transaction or atomic transaction has evolved in order to enable both a well understood database system behavior in a faulty environment where crashes can happen any time and recovery from a crash to a well understood database state a database transaction is a unit of work typically encapsulating a number of operations over a database e g reading a database object writing acquiring lock etc an abstraction supported in database and also other systems each transaction has well defined boundaries in terms of which program code executions are included in that transaction determined by the transaction s programmer via special transaction commands every database transaction obeys the following rules by support in the database system i e a database system is designed to guarantee them for the transactions it runs atomicity database systems atomicity either the effects of all or none of its operations remain all or nothing semantics when a database transaction transaction is completed committed or aborted respectively in other words to the outside world a committed transaction appears by its effects on the database to be indivisible atomic and an aborted transaction does not affect the database at all consistency database systems consistency every transaction must leave the database in a consistent correct state i e maintain the predetermined integrity rules of the database constraints upon and among the database s objects a transaction must transform a database from one consistent state to another consistent state however it is the responsibility of the transaction s programmer to make sure that the transaction itself is correct i e performs correctly what it intends to perform from the application s point of view while the predefined integrity rules are enforced by the dbms thus since a database can be normally changed only by transactions all the database s states are consistent isolation database systems isolation transactions cannot interfere with each other as an end result of their executions moreover usually depending on concurrency control method the effects of an incomplete transaction are not even visible to another transaction providing isolation is the main goal of concurrency control durability database systems durability effects of successful committed transactions must persist through crash computing crash es typically by recording the transaction s effects and its commit event in a non volatile memory the concept of atomic transaction has been extended during the years to what has become business transaction management business transactions which actually implement types of workflow and are not atomic however also such enhanced transactions typically utilize atomic transactions as components why is concurrency control needed if transactions are executed serially i e sequentially with no overlap in time no transaction concurrency exists however if concurrent transactions with interleaving operations are allowed in an uncontrolled manner some unexpected undesirable result may occur such as the lost update problem a second transaction writes a second value of a data item datum on top of a first value written by a first concurrent transaction and the first value is lost to other transactions running concurrently which need by their precedence to read the first value the transactions that have read the wrong value end with incorrect results the dirty read problem transactions read a value written by a transaction that has been later aborted this value disappears from the database upon abort and should not have been read by any transaction dirty read the reading transactions end with incorrect results the incorrect summary problem while one transaction takes a summary over the values of all the instances of a repeated data item a second transaction updates some instances of that data item the resulting summary does not reflect a correct result for any usually needed for correctness precedence order between the two transactions if one is executed before the other but rather some random result depending on the timing of the updates and whether certain update results have been included in the summary or not most high performance transactional systems need to run transactions concurrently to meet their performance requirements thus without concurrency control such systems can neither provide correct results nor maintain their databases consistent concurrency control mechanisms categories the main categories of concurrency control mechanisms are optimistic concurrency control optimistic delay the checking of whether a transaction meets the isolation and other integrity rules e g serializability and serializability correctness recoverability recoverability until its end without blocking any of its read write operations and be optimistic about the rules being met and then abort a transaction to prevent the violation if the desired rules are to be violated upon its commit an aborted transaction is immediately restarted and re executed which incurs an obvious overhead versus executing it to the end only once if not too many transactions are aborted then being optimistic is usually a good strategy pessimistic block an operation of a transaction if it may cause violation of the rules until the possibility of violation disappears blocking operations is typically involved with performance reduction semi optimistic block operations in some situations if they may cause violation of some rules and do not block in other situations while delaying rules checking if needed to transaction s end as done with optimistic different categories provide different performance i e different average transaction completion rates throughput depending on transaction types mix computing level of parallelism and other factors if selection and knowledge about trade offs are available then category and method should be chosen to provide the highest performance the mutual blocking between two transactions where each one blocks the other or more results in a deadlock where the transactions involved are stalled and cannot reach completion most non optimistic mechanisms with blocking are prone to deadlocks which are resolved by an intentional abort of a stalled transaction which releases the other transactions in that deadlock and its immediate restart and re execution the likelihood of a deadlock is typically low blocking deadlocks and aborts all result in performance reduction and hence the trade offs between the categories methods many methods for concurrency control exist most of them can be implemented within either main category above the major methods ref name bern2009 phil bernstein philip a bernstein eric newcomer 2009 http www elsevierdirect com product jsp isbn 9781558606234 principles of transaction processing 2nd edition morgan kaufmann elsevier june 2009 isbn 978 1 55860 623 4 page 145 ref which have each many variants and in some cases may overlap or be combined are locking e g two phase locking 2pl controlling access to data by lock computer science locks assigned to the data access of a transaction to a data item database object locked by another transaction may be blocked depending on lock type and access operation type until lock release serialization serializability testing conflict serializability graph checking also called serializability or conflict or precedence graph checking checking for cycle graph theory cycles in the schedule s directed graph graph and breaking them by aborts timestamp based concurrency control timestamp ordering to assigning timestamps to transactions and controlling or checking access to data by timestamp order commitment ordering or commit ordering co controlling or checking transactions chronological order of commit events to be compatible with their respective serializability testing conflict serializability precedence order other major concurrency control types that are utilized in conjunction with the methods above include multiversion concurrency control mvcc increasing concurrency and performance by generating a new version of a database object each time the object is written and allowing transactions read operations of several last relevant versions of each object depending on scheduling method index locking index concurrency control synchronizing access operations to index database index es rather than to user data specialized methods provide substantial performance gains private workspace model deferred update each transaction maintains a private workspace for its accessed data and its changed data become visible outside the transaction only upon its commit e g weikum01 weikum and vossen 2001 this model provides a different concurrency control behavior with benefits in many cases the most common mechanism type in database systems since their early days in the 1970s has been two phase locking strong strict two phase locking ss2pl also called rigorous scheduling or rigorous 2pl which is a special case variant of both two phase locking 2pl and commitment ordering co it is pessimistic in spite of its long name for historical reasons the idea of the ss2pl mechanism is simple release all locks applied by a transaction only after the transaction has ended ss2pl or rigorousness is also the name of the set of all schedules that can be generated by this mechanism i e these are ss2pl or rigorous schedules have the ss2pl or rigorousness property major goals of concurrency control mechanisms concurrency control mechanisms firstly need to operate correctly i e to maintain each transaction s integrity rules as related to concurrency application specific integrity rule are out of the scope here while transactions are running concurrently and thus the integrity of the entire transactional system correctness needs to be achieved with as good performance as possible in addition increasingly a need exists to operate effectively while transactions are distributed transaction distributed over process computing processes computer s and computer network s other subjects that may affect concurrency control are data recovery recovery and replication computer science replication correctness serializability main serializability for correctness a common major goal of most concurrency control mechanisms is generating schedule computer science schedule s with the serializability property without serializability undesirable phenomena may occur e g money may disappear from accounts or be generated from nowhere serializability of a schedule means equivalence in the resulting database values to some serial schedule with the same transactions i e in which transactions are sequential with no overlap in time and thus completely isolated from each other no concurrent access by any two transactions to the same data is possible serializability is considered the highest level of isolation database systems isolation among database transaction s and the major correctness criterion for concurrent transactions in some cases compromised serializability relaxing serializability relaxed forms of serializability are allowed for better performance e g the popular snapshot isolation mechanism or to meet availability requirements in highly distributed systems see eventual consistency but only if application s correctness is not violated by the relaxation e g no relaxation is allowed for money transactions since by relaxation money can disappear or appear from nowhere almost all implemented concurrency control mechanisms achieve serializability by providing serializability view and conflict serializability conflict serializablity a broad special case of serializability i e it covers enables most serializable schedules and does not impose significant additional delay causing constraints which can be implemented efficiently recoverability see serializability correctness recoverability recoverability in serializability comment while in the general area of systems the term recoverability may refer to the ability of a system to recover from failure or from an incorrect forbidden state within concurrency control of database systems this term has received a specific meaning concurrency control typically also ensures the serializability correctness recoverability recoverability property of schedules for maintaining correctness in cases of aborted transactions which can always happen for many reasons recoverability from abort means that no committed transaction in a schedule has read data written by an aborted transaction such data disappear from the database upon the abort and are parts of an incorrect database state reading such data violates the consistency rule of acid unlike serializability recoverability cannot be compromised relaxed at any case since any relaxation results in quick database integrity violation upon aborts the major methods listed above provide serializability mechanisms none of them in its general form automatically provides recoverability and special considerations and mechanism enhancements are needed to support recoverability a commonly utilized special case of recoverability is schedule computer science strict strictness which allows efficient database recovery from failure but excludes optimistic implementations e g commitment ordering strict co sco strict co sco cannot have an optimistic implementation but the history of commitment ordering semi optimistic database scheduler has semi optimistic ones comment note that the recoverability property is needed even if no database failure occurs and no database recovery from failure is needed it is rather needed to correctly automatically handle transaction aborts which may be unrelated to database failure and recovery from it distribution with the fast technological development of computing the difference between local and distributed computing over low latency computer network networks or bus computing buses is blurring thus the quite effective utilization of local techniques in such distributed environments is common e g in computer cluster s and multi core processor s however the local techniques have their limitations and use multi processes or threads supported by multi processors or multi cores to scale this often turns transactions into distributed ones if they themselves need to span multi processes in these cases most local concurrency control techniques do not scale well distributed serializability and commitment ordering pov section commitment ordering date november 2011 see serializability distributed serializability distributed serializability in serializability main global serializability main commitment ordering as database systems have become distributed database distributed or started to cooperate in distributed environments e g federated database s in the early 1990s and nowadays grid computing cloud computing and networks with smartphone s some transactions have become distributed a distributed transaction means that the transaction spans process computing processes and may span computer s and geographical sites this generates a need in effective distributed concurrency control mechanisms achieving the serializability property of a distributed system s schedule see serializability distributed serializability distributed serializability and global serializability modular serializability effectively poses special challenges typically not met by most of the regular serializability mechanisms originally designed to operate locally this is especially due to a need in costly distribution of concurrency control information amid communication and computer latency engineering latency the only known general effective technique for distribution is commitment ordering which was disclosed publicly in 1991 after being patent ed commitment ordering commit ordering co raz92 raz 1992 means that transactions chronological order of commit events is kept compatible with their respective serializability testing conflict serializability precedence order co does not require the distribution of concurrency control information and provides a general effective solution reliability engineering reliable high performance and scalability scalable for both distributed and global serializability also in a heterogeneous environment with database systems or other transactional objects with different any concurrency control mechanisms ref name bern2009 co is indifferent to which mechanism is utilized since it does not interfere with any transaction operation scheduling which most mechanisms control and only determines the order of commit events thus co enables the efficient distribution of all other mechanisms and also the distribution of a mix of different any local mechanisms for achieving distributed and global serializability the existence of such a solution has been considered unlikely until 1991 and by many experts also later due to misunderstanding of the commitment ordering summary co solution see global serializability quotations quotations in global serializability an important side benefit of co is commitment ordering exact characterization of voting deadlocks by global cycles automatic distributed deadlock resolution contrary to co virtually all other techniques when not combined with co are prone to deadlock distributed deadlock distributed deadlocks also called global deadlocks which need special handling co is also the name of the resulting schedule property a schedule has the co property if the chronological order of its transactions commit events is compatible with the respective transactions serializability testing conflict serializability precedence partial order two phase locking ss2pl mentioned above is a variant special case of co and thus also effective to achieve distributed and global serializability it also provides automatic distributed deadlock resolution a fact overlooked in the research literature even after co s publication as well as strictness and thus recoverability possessing these desired properties together with known efficient locking based implementations explains ss2pl s popularity ss2pl has been utilized to efficiently achieve distributed and global serializability since the 1980 and has become the de facto standard for it however ss2pl is blocking and constraining pessimistic and with the proliferation of distribution and utilization of systems different from traditional database systems e g as in cloud computing less constraining types of co e g commitment ordering distributed optimistic co doco optimistic co may be needed for better performance comments the distributed conflict serializability property in its general form is difficult to achieve efficiently but it is achieved efficiently via its special case distributed co each local component e g a local dbms needs both to provide some form of co and enforce a special vote ordering strategy for the two phase commit protocol 2pc utilized to commit distributed transaction s differently from the general distributed co distributed ss2pl commitment ordering strong strict two phase locking ss2pl exists automatically when all local components are ss2pl based in each component co exists implied and the vote ordering strategy is now met automatically this fact has been known and utilized since the 1980s i e that ss2pl exists globally without knowing about co for efficient distributed ss2pl which implies distributed serializability and strictness e g see raz92 raz 1992 page 293 it is also implied in bern87 bernstein et al 1987 page 78 less constrained distributed serializability and strictness can be efficiently achieved by distributed commitment ordering strict co sco strict co sco or by a mix of ss2pl based and sco based local components about the references and commitment ordering bern87 bernstein et al 1987 was published before the discovery of co in 1990 the co schedule property is called the history of commitment ordering dynamic atomicity dynamic atomicity in lynch1993 lynch et al 1993 page 201 co is described in weikum2001 weikum and vossen 2001 pages 102 700 but the description is partial and misses commitment ordering summary co s essence raz92 raz 1992 was the first refereed and accepted for publication article about co algorithms however publications about an equivalent dynamic atomicity property can be traced to 1988 other commitment ordering references co articles followed bernstein and newcomer 2009 ref name bern2009 note co as one of the four major concurrency control methods and co s ability to provide interoperability among other methods distributed recoverability unlike serializability distributed recoverability and distributed strictness can be achieved efficiently in a straightforward way similarly to the way distributed co is achieved in each database system they have to be applied locally and employ a vote ordering strategy for the two phase commit protocol 2pc raz92 raz 1992 page 307 as has been mentioned above distributed two phase locking ss2pl including distributed strictness recoverability and distributed commitment ordering serializability automatically employs the needed vote ordering strategy and is achieved globally when employed locally in each local database system as has been known and utilized for many years as a matter of fact locality is defined by the boundary of a 2pc participant raz92 raz 1992 other major subjects of attention the design of concurrency control mechanisms is often influenced by the following subjects recovery main data recovery all systems are prone to failures and handling data recovery recovery from failure is a must the properties of the generated schedules which are dictated by the concurrency control mechanism may affect the effectiveness and efficiency of recovery for example the strictness property mentioned in the section concurrency control recoverability recoverability above is often desirable for an efficient recovery replication main replication computer science for high availability database objects are often replication computer science replicated updates of replicas of a same database object need to be kept synchronized this may affect the way concurrency control is done e g gray et al 1996 ref name gray1996 cite conference author jim gray computer scientist gray j author2 helland p author3 patrick o neil o neil p author4 dennis shasha shasha d year 1996 conference the dangers of replication and a solution title proceedings of the 1996 acm sigmod international conference on management of data pages 173 182 conference url ftp ftp research microsoft com pub tr tr 96 17 pdf doi 10 1145 233269 233330 ref see also schedule computer science schedule isolation computer science distributed concurrency control global concurrency control references cite id bern87 phil bernstein philip a bernstein vassos hadzilacos nathan goodman 1987 http research microsoft com en us people philbe ccontrol aspx concurrency control and recovery in database systems free pdf download addison wesley publishing company 1987 isbn 0 201 10715 5 cite cite id weikum01 gerhard weikum gottfried vossen 2001 http www elsevier com wps find bookdescription cws home 677937 description description transactional information systems elsevier isbn 1 55860 508 8 cite cite id lynch1993 nancy lynch michael merritt william weihl alan fekete 1993 http www elsevier com wps find bookdescription cws home 680521 description description atomic transactions in concurrent and distributed systems morgan kauffman elsevier august 1993 isbn 978 1 55860 104 8 isbn 1 55860 104 x cite cite id raz92 yoav raz 1992 http www informatik uni trier de ley db conf vldb raz92 html the principle of commitment ordering or guaranteeing serializability in a heterogeneous environment of multiple autonomous resource managers using atomic commitment http www vldb org conf 1992 p292 pdf pdf proceedings of the eighteenth international conference on very large data bases vldb pp 292 312 vancouver canada august 1992 also dec tr 841 digital equipment corporation november 1990 cite footnotes reflist concurrency control in operating systems expand section date december 2010 computer multitasking multitasking operating systems especially real time operating system s need to maintain the illusion that all tasks running on top of them are all running at the same time even though only one or a few tasks really are running at any given moment due to the limitations of the hardware the operating system is running on such multitasking is fairly simple when all tasks are independent from each other however when several tasks try to use the same resource or when tasks try to share information it can lead to confusion and inconsistency the task of concurrent computing is to solve that problem some solutions involve locks similar to the locks used in databases but they risk causing problems of their own such as deadlock other solutions are non blocking algorithm s and read copy update see also linearizability mutual exclusion semaphore programming lock computer science software transactional memory transactional synchronization extensions references andrew s tanenbaum albert s woodhull 2006 operating systems design and implementation 3rd edition prentice hall isbn 0 13 142938 8 cite book last silberschatz first avi author2 galvin peter author3 gagne greg title operating systems concepts 8th edition publisher john wiley sons year 2008 isbn 0 470 12872 0 databases defaultsort concurrency control category concurrency control category data management category databases category transaction processing'
b'database centric architecture or data centric architecture has several distinct meanings generally relating to software architecture s in which database s play a crucial role often this description is meant to contrast the design to an alternative approach for example the characterization of an architecture as database centric may mean any combination of the following using a standard general purpose relational database management system as opposed to customized in memory computers memory or computer file file based data structures and access method s with the evolution of sophisticated database management system dbms software much of which is either free or included with the operating system application developers have become increasingly reliant on standard database tools especially for the sake of rapid application development using dynamic table database table driven logic as opposed to logic embodied in previously compiled computer program program s the use of table driven logic i e behavior that is heavily dictated by the contents of a database allows programs to be simpler and more flexible this capability is a central feature of dynamic programming language s see also control table s for tables that are normally coded and embedded within programs as data structures i e not compiled statements but could equally be read in from a flat file database or even retrieved from a spreadsheet using stored procedure s that run on database server s as opposed to greater reliance on logic running in middle tier application server s in a multi tier architecture the extent to which business logic should be placed at the back end versus another tier is a subject of ongoing debate for example toon koppelaars presents a detailed analysis of alternative oracle database oracle based architectures that vary in the placement of business logic concluding that a database centric approach has practical advantages from the standpoint of ease of development and maintainability ref https web archive org web 20060525094651 http www oracle com technology pub articles odtug award pdf a database centric approach to j2ee application development ref using a shared database as the basis for communicating between parallel computing parallel processes in distributed computing applications as opposed to direct inter process communication via message passing functions and message oriented middleware a potential benefit of database centric architecture in distributed application s is that it simplifies the design by utilizing dbms provided transaction processing and index database indexing to achieve a high degree of reliability performance and capacity ref citation author lind p alm m title a database centric virtual chemistry system journal j chem inf model volume 46 issue 3 pages 1034 9 year 2006 pmid 16711722 doi 10 1021 ci050360b postscript ref for example base one describes a database centric distributed computing architecture for grid computing grid and computer cluster cluster computing and explains how this design provides enhanced security fault tolerance and scalability ref http www boic com dbgrid htm database centric grid and cluster computing ref an overall enterprise architecture that favors shared data models ref cite news url http tdan com the data centric revolution 18780 title the data centric revolution newspaper tdan com access date 2017 01 09 ref over allowing each application to have its own idiosyncratic data model see also control table s category data centric programming languages data centric programming languages the data driven programming paradigm which makes the information used in a system the primary design driver see the http datacentricmanifesto org datacentricmanifesto org references reflist database defaultsort database centric architecture category software architecture category data management category distributed computing architecture'
b'a database engine or storage engine is the underlying software component that a database management system dbms uses to create read update and delete crud data from a database most database management systems include their own application programming interface api that allows the user to interact with their underlying engine without going through the user interface of the dbms the term database engine is frequently used interchangeably with database server or database management system a database instance refers to the processes and memory structures of the running database engine storage engines many of the modern dbms support multiple storage engines within the same database for example mysql supports innodb as well as myisam some storage engines are database transaction transactional class wikitable name license transactional aria storage engine aria gpl no blitzdb gpl no falcon storage engine falcon gpl yes innodb gpl yes myisam gpl no infinidb cpl no tokudb gpl yes wiredtiger gpl yes xtradb gpl yes additional engine types include embedded database engines in memory database engines design considerations database bits are laid out in storage in data structures and groupings that can take advantage of both known effective algorithms to retrieve and manipulate them and the storage own properties typically the storage itself is designed to meet requirements of various areas that extensively utilize storage including databases a dbms in operation always simultaneously utilizes several storage types e g memory and external storage with respective layout methods in principle the database storage can be viewed as a linear address space where every bit of data has its unique address in this address space in practice only a very small percentage of addresses are kept as initial reference points which also requires storage most data is accessed by indirection using displacement calculations distance in bits from the reference points and data structures which define access paths using pointers to all needed data in an effective manner optimized for the needed data access operations database storage hierarchy a database while in operation resides simultaneously in several types of storage forming a storage hierarchy by the nature of contemporary computers most of the database part inside a computer that hosts the dbms resides partially replicated in volatile storage data pieces of the database that are being processed manipulated reside inside a processor possibly in cpu cache processor s caches these data are being read from written to memory typically through a computer bus computing bus so far typically volatile storage components computer memory is communicating data transferred to from external storage typically through standard storage interfaces or networks e g fibre channel iscsi a disk array storage array a common external storage unit typically has storage hierarchy of its own from a fast cache typically consisting of volatile and fast dram which is connected again via standard interfaces to drives possibly with different speeds like usb flash drive flash drives and magnetic disk drive s non volatile the drives may be connected to magnetic tape s on which typically the least active parts of a large database may reside or database backup generations typically a correlation exists currently between storage speed and price while the faster storage is typically volatile data structures main database storage structures a data structure is an abstract construct that embeds data in a well defined manner an efficient data structure allows to manipulate the data in efficient ways the data manipulation may include data insertion deletion updating and retrieval in various modes a certain data structure type may be very effective in certain operations and very ineffective in others a data structure type is selected upon dbms development to best meet the operations needed for the types of data it contains type of data structure selected for a certain task typically also takes into consideration the type of storage it resides in e g speed of access minimal size of storage chunk accessed etc in some dbmss database administrators have the flexibility to select among options of data structures to contain user data for performance reasons sometimes the data structures have selectable parameters to tune the database performance databases may store data in many data structure types ref name physical database design harvnb lightstone teorey nadeau 2007 ref common examples are the following ordered unordered flat file database flat files hash table s b tree s isam heap data structure heaps data orientation and clustering in contrast to conventional row orientation relational databases can also be column oriented dbms column oriented or correlational database correlational in the way they store data in any particular structure in general substantial performance improvement is gained if different types of database objects that are usually utilized together are laid in storage in proximity being clustered this usually allows to retrieve needed related objects from storage in minimum number of input operations each sometimes substantially time consuming even for in memory databases clustering provides performance advantage due to common utilization of large caches for input output operations in memory with similar resulting behavior for example it may be beneficial to cluster a record of an item in stock with all its respective order records the decision of whether to cluster certain objects or not depends on the objects utilization statistics object sizes caches sizes storage types etc database indexing main database index indexing is a technique some storage engines use for improving database performance the many types of indexes share the common property that they reduce the need to examine every entry when running a query in large databases this can reduce query time cost by orders of magnitude the simplest form of index is a sorted list of values that can be searched using a binary search with an adjacent reference to the location of the entry analogous to the index in the back of a book the same data can have multiple indexes an employee database could be indexed by last name and hire date indexes affect performance but not results database designers can add or remove indexes without changing application logic reducing maintenance costs as the database grows and database usage evolves indexes can speed up data access but they consume space in the database and must be updated each time the data is altered indexes therefore can speed data access but slow data maintenance these two properties determine whether a given index is worth the cost see also cleanup merge into chart architecture of btrieve micro kernel database engine btrieve s micro kernel database engine berkeley db c treeace c treeace database engine flaim database engine microsoft jet database engine mysql cluster on the ndb storage engine of mysql nuodb references reflist external links http dev mysql com tech resources articles storage engine part 3 html https books google com books id pqz6qytcemcc pg pt287 dq storage engines mysql administrator s bible chapter 11 storage engines defaultsort database engine category data management category database engines category database management systems'
b'refimprove date august 2010 a transaction symbolizes a unit of work performed within a database management system or similar system against a database and treated in a coherent and reliable way independent of other transactions a transaction generally represents any change in database transactions in a database environment have two main purposes to provide reliable units of work that allow correct recovery from failures and keep a database consistent even in cases of system failure when execution stops completely or partially and many operations upon a database remain uncompleted with unclear status to provide isolation between programs accessing a database concurrently if this isolation is not provided the programs outcomes are possibly erroneous a database transaction by definition must be atomicity database systems atomic consistency database systems consistent isolation database systems isolated and durability database systems durable ref http msdn microsoft com en us library aa366402 vs 85 aspx a transaction is a group of operations that are atomic consistent isolated and durable acid ref database practitioners often refer to these properties of database transactions using the acronym acid transactions provide an all or nothing proposition stating that each work unit performed in a database must either complete in its entirety or have no effect whatsoever further the system must isolate each transaction from other transactions results must conform to existing constraints in the database and transactions that complete successfully must get written to durable storage purpose database s and other data stores which treat the data integrity integrity of data as paramount often include the ability to handle transactions to maintain the integrity of data a single transaction consists of one or more independent units of work each reading and or writing information to a database or other data store when this happens it is often important to ensure that all such processing leaves the database or data store in a consistent state examples from double entry bookkeeping system double entry accounting systems often illustrate the concept of transactions in double entry accounting every debit requires the recording of an associated credit if one writes a check for 100 to buy groceries a transactional double entry accounting system must record the following two entries to cover the single transaction debit 100 to groceries expense account credit 100 to checking account a transactional system would make both entries pass or both entries would fail by treating the recording of multiple entries as an atomic transactional unit of work the system maintains the integrity of the data recorded in other words nobody ends up with a situation in which a debit is recorded but no associated credit is recorded or vice versa transactional databases a transactional database is a dbms where write transactions on the database are able to be rolled back if they are not completed properly e g due to power or connectivity loss most as of 2008 alt modern relational database management system s fall into the category of databases that support transactions in a database system a transaction might consist of one or more data manipulation statements and queries each reading and or writing information in the database users of database system s consider data consistency consistency and data integrity integrity of data as highly important a simple transaction is usually issued to the database system in a language like structured query language sql wrapped in a transaction using a pattern similar to the following begin the transaction execute a set of data manipulations and or queries if no errors occur then commit the transaction and end it if errors occur then rollback the transaction and end it if no errors occurred during the execution of the transaction then the system commits the transaction a transaction commit operation applies all data manipulations within the scope of the transaction and persists the results to the database if an error occurs during the transaction or if the user specifies a rollback data management rollback operation the data manipulations within the transaction are not persisted to the database in no case can a partial transaction be committed to the database since that would leave the database in an inconsistent state internally multi user databases store and process transactions often by using a transaction identifier id or xid there are multiple varying ways for transactions to be implemented other than the simple way documented above nested transaction s for example are transactions which contain statements within them that start new transactions i e sub transactions multi level transactions are a variant of nested transactions where the sub transactions take place at different levels of a layered system architecture e g with one operation at the database engine level one operation at the operating system level ref beeri c bernstein p a and goodman n a model for concurrency in nested transactions systems journal of the acm 36 1 230 269 1989 ref another type of transaction is the compensating transaction in sql transactions are available in most sql database implementations though with varying levels of robustness mysql for example does not support transactions in the myisam storage engine which was its default storage engine before version 5 5 a transaction is typically started using the command code begin code although the sql standard specifies code start transaction code when the system processes a code commit sql commit code statement the transaction ends with successful completion a code rollback data management rollback code statement can also end the transaction undoing any work performed since code begin transaction code if autocommit was disabled using code start transaction code autocommit will also be re enabled at the transaction s end one can set the isolation database systems isolation level for individual transactional operations as well as globally at the read committed level the result of any work done after a transaction has commenced but before it has ended will remain invisible to other database users until it has ended at the lowest level read uncommitted which may occasionally be used to ensure high concurrency such changes will be visible object databases relational databases traditionally comprise tables with fixed size fields and thus records object databases comprise variable sized binary large object blobs possibly incorporating a mime type or serializable databases serialized the fundamental similarity though is the start and the commit data management commit or rollback data management rollback after starting a transaction database records or objects are locked either read only or read write actual reads and writes can then occur once the user and application is happy any changes are committed or rolled back atomicity database systems atomically such that at the end of the transaction there is no consistency database systems inconsistency distributed transactions database systems implement distributed transaction s as transactions against multiple applications or hosts a distributed transaction enforces the acid properties over multiple systems or data stores and might include systems such as databases file systems messaging systems and other applications in a distributed transaction a coordinating service ensures that all parts of the transaction are applied to all relevant systems as with database and other transactions if any part of the transaction fails the entire transaction is rolled back across all affected systems transactional filesystems the namesys reiser4 filesystem for linux ref http namesys com v4 v4 html committing namesys com bot generated title ref supports transactions and as of microsoft windows vista the microsoft ntfs filesystem ref cite web url http msdn microsoft com library default asp url library en us fileio fs portal asp title msdn library publisher accessdate 16 october 2014 dead link date may 2014 ref supports distributed transaction s across networks see also concurrency control references reflist further reading cite id bern2009 philip a bernstein eric newcomer 2009 http www elsevierdirect com product jsp isbn 9781558606234 principles of transaction processing 2nd edition morgan kaufmann elsevier isbn 978 1 55860 623 4 cite gerhard weikum gottfried vossen 2001 transactional information systems theory algorithms and the practice of concurrency control and recovery morgan kaufmann isbn 1 55860 508 8 external links c2 transactionprocessing https docs oracle com database 121 cncpt transact htm cncpt016 https docs oracle com cd b28359 01 server 111 b28318 transact htm databases defaultsort database transaction category data management category transaction processing'
b'unreferenced date december 2009 in database system s durability is the acid property which guarantees that database transaction transaction s that have committed will survive permanently for example if a flight booking reports that a seat has successfully been booked then the seat will remain booked even if the system crashes durability can be achieved by flushing the transaction s log records to non volatile storage before acknowledging commitment in distributed transaction s all participating servers must coordinate before commit can be acknowledged this is usually done by a two phase commit protocol many dbmss implement durability by writing transactions into a transaction log that can be reprocessed to recreate the system state right before any later failure a transaction is deemed committed only after it is entered in the log see also atomicity database systems atomicity consistency database systems consistency isolation database systems isolation relational database management system defaultsort durability database systems category data management category transaction processing compu sci stub database stub'
b'redirect database language other types of database languages database languages multiple issues prose date october 2010 refimprove date october 2010 query languages are computer language s used to make queries in database s and information system s types broadly query languages can be classified according to whether they are database query languages or information retrieval query language s the difference is that a database query language attempts to give factual answers to factual questions while an information retrieval query language attempts to find documents containing information that is relevant to an area of inquiry examples examples include ql is a proprietary object oriented query language for querying relational database s successor of datalog contextual query language cql a formal language for representing queries to information retrieval systems such as web indexes or bibliographic catalogues cqlf codyasyl query language flat is a query language for codasyl type databases concept oriented query language coql is used in the concept oriented model com it is based on a novel data modeling construct concept and uses such operations as projection and de projection for multi dimensional analysis analytical operations and inference cypher query language cypher is a query language for the neo4j graph database data mining extensions dmx is a query language for data mining models datalog is a query language for deductive database s f logic is a declarative object oriented language for deductive database s and knowledge representation facebook query language fql enables you to use a sql style interface to query the data exposed by the graph api it provides advanced features not available in the graph api ref cite web url https developers facebook com docs technical guides fql title fql overview work facebook developers ref gellish english is a language that can be used for queries in gellish english databases for dialogues requests and responses as well as for information modeling and knowledge modeling ref http gellish wiki sourceforge net querying a gellish english database dead link date july 2016 bot internetarchivebot fix attempted yes ref gremlin programming language gremlin is an apache software foundation graph traversal language for oltp and olap graph systems htsql is a query language that translates http queries to sql isbl is a query language for prtv one of the earliest relational database management systems linq query expressions is a way to query various data sources from net framework net languages ldap is an application protocol for querying and modifying directory services running over tcp ip logiql is a variant of datalog and is the query language for the logicblox system molecular query language mql is a cheminformatics query language for a substructure search allowing beside nominal properties also numerical properties multidimensional expressions mdx is a query language for olap databases n1ql is a couchbase inc couchbase s query language finding data in couchbase server s object query language oql is object query language object constraint language ocl object constraint language despite its name ocl is also an object query language and an object management group omg standard opath intended for use in querying winfs stores ottoql intended for querying tables xml and databases poliqarp query language is a special query language designed to analyze annotated text used in the poliqarp search engine pql is a special purpose programming language for managing process model s based on information about wiktionary scenario scenarios that these models describe quel query languages quel is a relational database access language similar in most ways to sql rdql is a resource description framework rdf query language reql is a query language used in http rethinkdb com docs introduction to reql rethinkdb smiles arbitrary target specification smarts is the cheminformatics standard for a substructure search sparql is a query language for resource description framework rdf graph discrete mathematics graphs spl search processing language spl is a search language for machine generated big data based upon unix piping and sql scl is the software control language to query and manipulate endevor objects sql is a well known query language and data manipulation language for relational database s suprtool is a proprietary query language for suprtool a database access program used for accessing data in image sql formerly turboimage and oracle databases tmql topic map query language is a query language for topic maps tql is a language used to http cmshelpcenter saas hp com cms 10 21 ucmdb docs docs eng doc lib content modeling tql c overview htm query topology for hp products d data language specification tutorial d is a query language for relational database management system truly relational database management systems trdbms xquery is a query language for xml database xml data sources xpath is a declarative language for navigating xml documents xsparql is an integrated query language combining xquery with sparql to query both xml and rdf data sources at once yahoo query language yql is an sql like query language created by yahoo search engine query languages e g as used by google search google ref cite web title search operators url https support google com websearch answer 2466433 hl en accessdate august 22 2015 publisher google ref or bing search engine bing ref cite web title bing query language url https msdn microsoft com en us library ff795667 aspx accessdate august 22 2015 publisher microsoft ref see also data manipulation language references reflist database databases computer language query languages authority control defaultsort query language category computer languages category data management category query languages no database sp\xc3\xb8rrespr\xc3\xa5k'
b'orphan date october 2010 style width 80 margin 0 0 0 10 border collapse collapse background fbfbfb border 1px solid aaa border left 10px solid f28500 style width 52px padding 2px 0px 2px 0 5em text align center image newspaper nicu buculei 01 svg 50px style padding 0 25em 0 5em this article or section reads like an wikipedia what wikipedia is not wikipedia is not a soapbox advertisement for edmworks br to meet wikipedia s category wikipedia style guidelines quality standards and comply with wikipedia s wikipedia neutral point of view neutral point of view policy it may require wikipedia cleanup cleanup enterprise data planning is the starting point for enterprise wide change it states the destination and describes how you will get there it defines benefits costs and potential risks it provides measures to be used along the way to judge progress and adjust the journey according to changing circumstances data is fundamental to investment enterprises effective economic management of data underpins operations and enables transformations needed to satisfy customer demands competition and regulation data warehouse s and other aspects of the overall data architecture are critical to the enterprise edmworks has created a strategic data planning approach for the investment sector it consists of a planning process planning intranets templates and training materials edmworks planning process is based on the belief that extensive domain knowledge significantly shortens planning iterations and enables progressively higher quality plans to be produced and implemented ref name hull introduction to futures and options markets john hull 1995 ref ref name taylor mastering derivatives markets francesca taylor 2007 ref this approach drives the development of an effective and economic enterprise data architecture enterprise data planning is based on proven business disciplines ref name stutely the definitive business plan richard stutely 2002 ref key architectural layers for data and applications are then added in order to provide an enterprise wide understanding of the uses and interdependencies of data ref name tozer planning for effective business information systems edwin e tozer 1998 ref this enables the definition of the core components of the enterprise data management edm plan industry structure and business objectives assessment of systems and services target architecture for applications data and infrastructure target organization structures systems database infrastructure and organizational plans business case costs benefits results and risks edmworks uses several components from the open systems group togaf enterprise systems planning process togaf acts as an extension to good business planning methods to provide a framework for the development of the systems and data architectural components history james martin author james martin was one of the pathfinders in data planning methodologies he was one of the first to identify data as being an enterprise wide asset that required management he developed a series of tools and methods to support that process ref name martin martin 1982 ref most of the large consulting firms developed their own methods to address the same basic issue frequently their approaches were incorporated into their own branded system development methodologies that encompassed the complete systems development life cycle others such as edwin e tozer ed tozer developed more focused offerings that dealt with the complexities of extracting key business needs from senior management and then defining relevant architectural visions for the specific enterprise ref name tozer from these various sources the concepts of business data applications and technology architectures emerged the open group architectural framework togaf has taken this work forward and has established a sound method in togaf version 9 edmworks approach is to adopt these planning and architectural practices as a basis and then add two additional dimensions to the planning and implementation focus domain knowledge of the investments sector investments is a complex global industry with a common set of characteristics about clients information vendors competition and regulation domain knowledge significantly improves the quality of the planning and implementation processes development of people and teams change is a major feature of in any enterprise data management program and people and teams both need development in order to make edm effective throughout an organization references reflist external links http www edmworks com enterprise data management works category data management'
b'refimprove date january 2016 in web archiving an archive site is a website that stores information on webpages from the past for anyone to view common techniques two common techniques for archiving web sites are using a web crawler or soliciting user submissions using a web crawler by using a web crawler e g the internet archive the service will not depend on an active community for its content and thereby can build a larger database faster however web crawlers are only able to index and archive information the public has chosen to post to the internet or that is available to be crawled as web site developers and system administrators have the ability to block web crawlers from accessing certain web pages using a robots exclusion standard robots txt user submissions while it can be difficult to start user submissions services due to potentially low rates of user submission this system can yield some of the best results by crawling web pages one is only able to obtain the information the public has chosen to post online however potential content providers may not bother to post certain information assuming no one would be interested in it because they lack a proper venue in which to post it or because of copyright concerns ref cite news url http dlib org dlib march12 niu 03niu1 html journal d lib magazine date march april 2012 volume 18 number 3 4 title an overview of web archiving author jinfang niu university of south florida doi 10 1045 march2012 niu1 ref however users who see someone wants their information may be more apt to submit it examples google groups on february 12 2001 google acquired the usenet discussion group archives from deja com and turned it into their google groups service ref cite web title google acquires usenet discussion service and significant assets from deja com work date february 12 2001 url https googlepress blogspot com 2001 02 google acquires usenet discussion html ref they allow users to search old discussions with google s search technology while still allowing users to post to the mailing list s internet archive the internet archive is building a compendium of websites and digital media starting in 1996 the archive has been employing a web crawler to build up their database it is one of the best known archive sites nbcuniversal archives nbcuniversal archives offer access to exclusive content from nbcuniversal and its subsidiaries their nbcuniversal archives website provides easy viewing of past and recent news clips and it is a prime example of a news archive ref http www nbcuniversalarchives com nbcuni home do nbcuniversal archives ref nextpoint nextpoint offers an automated cloud computing cloud based software as a service saas for marketing compliance and litigation related needs including electronic discovery pandora archive pandora pandora archive founded in 1996 by the national library of australia stands for preserving and accessing networked documentary resources of australia which encapsulates their mission they provide a long term catalog of select online publications and web sites authored by australians or that are of an australian topic they employ their pandas pandora digital archiving system when building their catalog textfiles com textfiles com is a large library of old text files maintained by jason scott sadofsky its mission is to archive the old documents that had floated around the bulletin board systems bbs of his youth and to document other people s experiences on the bulletin board systems see also internet archive pandora archive webcite web archiving references reflist defaultsort archive site category data management category online archives category web archiving initiatives'
b'in computing the term virtual directory has a couple of meanings it may simply designate for example in internet information services iis a folder computing folder which appears in a path computing path but which is not actually a subfolder of the preceding folder in the path however this article will discuss the term in the context of directory service s and identity management a virtual directory or virtual directory server in this context is a software layer that delivers a single access point for identity management applications and service platforms a virtual directory operates as a high performance lightweight abstraction layer that resides between client applications and disparate types of identity data repositories such as proprietary and standard directories databases web services and applications a virtual directory receives queries and directs them to the appropriate data sources by abstracting and virtualizing data the virtual directory integrates identity data from multiple heterogeneous data stores and presents it as though it were coming from one source this ability to reach into disparate repositories makes virtual directory technology ideal for consolidating data stored in a distributed environment as of 2011 virtual directory servers most commonly use the lightweight directory access protocol ldap protocol but more sophisticated virtual directories can also support sql as well as directory services markup language dsml and service provisioning markup language spml industry experts have heralded the importance of the virtual directory in modernizing the identity infrastructure according to dave kearns of network world virtualization is hot and a virtual directory is the building block or foundation you should be looking at for your next identity management project ref cite web url http www networkworld com article 2305608 access control virtual directory finally gains recognition html title virtual directory finally gains recognition publisher networkworld date 7 august 2006 accessdate 14 july 2014 author kearns dave ref in addition gartner analyst bob blakley ref the emerging architecture of identity management bob blakley april 16 2010 ref said that virtual directories are playing an increasingly vital role in his report the emerging architecture of identity management blakley wrote in the first phase production of identities will be separated from consumption of identities through the introduction of a virtual directory interface capabilities virtual directories can have some or all of the following capabilities ref cite web url http optimalidm com resources blog virtual directory server 2 title an introduction to virtual directories publisher optimal idm accessdate 15 july 2014 ref aggregate identity data across sources to create a single point of access create high availability for authoritative data stores act as identity firewall by preventing denial of service attack s on the primary data stores through an additional virtual layer support a common searchable namespace for centralized authentication present a unified virtual view of user information stored across multiple systems delegate authentication to backend sources through source specific security means virtualize data sources to support migration from legacy data stores without modifying the applications that rely on them enrich identities with attributes pulled from multiple data stores based on a link between user entries some advanced identity virtualization platforms can also enable application specific customized views of identity data without violating internal or external regulations governing identity data reveal contextual relationships between objects through hierarchical directory structures develop advanced correlation across diverse sources using correlation rules build a global user identity by correlating unique user accounts across various data stores and enrich identities with attributes pulled from multiple data stores based on a link between user entries enable constant data refresh for real time updates through a persistent cache advantages virtual directories enable faster deployment because users do not need to add and sync additional application specific data sources leverage existing identity infrastructure and security investments to deploy new services deliver high availability of data sources provide application specific views of identity data which can help avoid the need to develop a master enterprise schema allow a single view of identity data without violating internal or external regulations governing identity data act as identity firewalls by preventing denial of service attacks on the primary data stores and providing further security on access to sensitive data can reflect changes made to authoritative sources in real time present a unified virtual view of user information from multiple systems so that it appears to reside in a single system can secure all backend storage locations with a single security policy disadvantages an original disadvantage is public perception of push pull technologies which is the general classification of virtual directories depending on the nature of their deployment virtual directories were initially designed and later deployed with push technologies in mind which also contravened with privacy laws of the united states this is no longer the case there are however other disadvantages in the current technologies the classical virtual directory based on proxy cannot modify underlying data structures or create new views based on the relationships of data from across multiple systems so if an application requires a different structure such as a flattened list of identities or a deeper hierarchy for delegated administration a virtual directory is limited many virtual directories cannot correlate same users across multiple diverse sources in the case of duplicate users virtual directories without advanced caching technologies cannot scale to heterogeneous high volume environments sample terminology overly detailed section yes date july 2014 unify metadata extract schemas from the local data source map them to a common format and link the same identities from different data silos based on a unique identifier namespace joining create a single large directory by bringing multiple directories together at the namespace level for instance if one directory has the namespace ou internal dc domain dc com and a second directory has the namespace ou external dc domain dc com then creating a virtual directory with both namespaces is an example of namespace joining identity joining enrich identities with attributes pulled from multiple data stores based on a link between user entries for instance if the user joeuser exists in a directory as cn joeuser ou users and in a database with a username of joeuser then the joeuser identity can be constructed from both the directory and the database data remapping the translation of data inside of the virtual directory for instance mapping uid to samaccountname so a client application that only supports a standard ldap compliant data source is able to search an active directory namespace as well query routing route requests based on certain criteria such as write operations going to a master while read operations are forwarded to replicas identity routing virtual directories may support the routing of requests based on certain criteria such as write operations going to a master while read operations being forwarded to replicas authoritative source a virtualized data repository such as a directory or database that the virtual directory can trust for user data server groups group one or more servers containing the same data and functionality a typical implementation is the multi master multi replica environment in which replicas process read requests and are in one server group while masters process write requests and are in another so that servers are grouped by their response to external stimuli even though all share the same data use cases the following are sample use cases of virtual directories integrating multiple directory namespaces to create a central enterprise directory supporting infrastructure integrations after mergers and acquisitions centralizing identity storage across the infrastructure making identity information available to applications through various protocols including ldap jdbc and web services creating a single access point for web access management web access management wam tools enabling web single sign on sso across varied sources or domains supporting role based fine grained authorization policies enabling authentication across different security domains using each domain s specific credential checking method improving secure access to information both inside and outside of the firewall references references defaultsort virtual directory category data management'
b'citation style date may 2014 synthetic data are any production data applicable to a given situation that are not obtained by direct measurement according to the mcgraw hill dictionary of scientific and technical terms ref name mcgraw synthetic data n d mcgraw hill dictionary of scientific and technical terms retrieved november 29 2009 from answers com web site http www answers com topic synthetic data ref where craig s mullins an expert in data management defines production data as information that is persistently stored and used by professionals to conduct business processes ref name mullins mullins craig s 2009 february 5 what is production data message posted to http www neon com blog blogs cmullins archive 2009 02 05 what is production data 3f00 aspx ref the creation of synthetic data is an involved process of data anonymity anonymization that is to say that synthetic data is a subset of anonymized data ref name machanavajjhalaetal cite journal title privacy theory meets practice on the map journal 2008 ieee 24th international conference on data engineering doi 10 1109 icde 2008 4497436 pages 277 286 year 2008 last1 machanavajjhala first1 ashwin last2 kifer first2 daniel last3 abowd first3 john last4 gehrke first4 johannes last5 vilhuber first5 lars ref synthetic data is used in a variety of fields as a filter for information that would otherwise compromise the confidentiality of particular aspects of the data many times the particular aspects come about in the form of human information i e name home address ip address telephone number social security number credit card number etc usefulness synthetic data are generated to meet specific needs or certain conditions that may not be found in the original real data this can be useful when designing any type of system because the synthetic data are used as a simulation or as a theoretical value situation etc this allows us to take into account unexpected results and have a basic solution or remedy if the results prove to be unsatisfactory synthetic data are often generated to represent the authentic data and allows a baseline to be set ref name barse barse e l kvarnstr\xc3\xb6m h jonsson e 2003 synthesizing test data for fraud detection systems manuscript submitted for publication department of computer engineering chalmbers university of technology g\xc3\xb6teborg sweden retrieved from http ieeexplore ieee org stamp stamp jsp tp arnumber 1254343 isnumber 28060 ref another use of synthetic data is to protect privacy and confidentiality of authentic data as stated previously synthetic data is used in testing and creating many different types of systems below is a quote from the abstract of an article that describes a software that generates synthetic data for testing fraud detection systems that further explains its use and importance this enables us to create realistic behavior profiles for users and attackers the data is used to train the fraud detection system itself thus creating the necessary adaptation of the system to a specific environment ref name barse history the history of the generation of synthetic data dates back to 1993 in 1993 the idea of original fully synthetic data was created by donald rubin rubin ref name rubin1993 cite journal authorlink rubin donald b title discussion statistical disclosure limitation journal journal of official statistics volume 9 pages 461 468 year 1993 ref rubin originally designed this to synthesize the decennial census long form responses for the short form households he then released samples that did not include any actual long form records in this he preserved anonymity of the household ref name abowd cite web last abowd first john m title confidentiality protection of social science micro data synthetic data and related methods powerpoint slides url http www idre ucla edu events ppt 2006 01 30 abowd ucla synthetic data presentation ppt accessdate 17 february 2011 ref later that year the idea of original partially synthetic data was created by little little used this idea to synthesize the sensitive values on the public use file ref name little cite journal authorlink little rod title statistical analysis of masked data journal journal of official statistics volume 9 pages 407 426 year 1993 ref in 1994 stephen fienberg fienberg came up with the idea of critical refinement in which he used a parametric posterior predictive distribution instead of a bayes bootstrap to do the sampling ref name abowd later other important contributors to the development of synthetic data generation are trivellore raghunathan raghunathan jerry reiter reiter donald rubin rubin john m abowd abowd jim woodcock woodcock collectively they came up with a solution for how to treat partially synthetic data with missing data similarly they came up with the technique of sequential regression multivariate imputation statistics imputation ref name abowd applications synthetic data are used in the process of data mining testing and training fraud detection systems confidentiality systems and any type of system is devised using synthetic data as described previously synthetic data may seem as just a compilation of made up data but there are specific algorithms and generators that are designed to create realistic data ref name deng deng r 2002 information and communications security proceedings of the 4th international conference icics 2002 singapore december 2002 retrieved from https books google com books id 6mod7enqa8cc pg pa265 dq 22synthetic data 22 v onepage q 22synthetic 20data 22 f false ref this synthetic data assists in teaching a system how to react to certain situations or criteria researcher doing clinical trials or any other research may generate synthetic data to aid in creating a baseline for future studies and testing for example intrusion detection software is tested using synthetic data this data is a representation of the authentic data and may include intrusion instances that are not found in the authentic data the synthetic data allows the software to recognize these situations and react accordingly if synthetic data was not used the software would only be trained to react to the situations provided by the authentic data and it may not recognize another type of intrusion ref name barse synthetic data is also used to protect the privacy and confidentiality of a set of data real data contains personal private confidential information that a programmer software creator or research project may not want to be disclosed ref name abowd2 abowd j m lane j 2004 new approaches to confidentiality protection synthetic data remote access and research data centers manuscript submitted for publication cornell institute for social and economic research ciser cornell university ithica new york retrieved from http www springerlink com content 27nud7qx09qurg3p fulltext pdf ref synthetic data holds no personal information and cannot be traced back to any individual therefore the use of synthetic data reduces confidentiality and privacy issues calculations researchers test the framework on synthetic data which is the only source of ground truth on which they can objectively assess the performance of their algorithm s sup 10 sup synthetic data can be generated with random orientations and positions sup 8 sup datasets can be get fairly complicated a more complicated dataset can be generated by using a synthesizer build to create a synthesizer build first use the original data to create a model or equation that fits the data the best this model or equation will be called a synthesizer build this build can be used to generate more data sup 9 sup constructing a synthesizer build involves constructing a statistical model in a linear regression line example the original data can be plotted and a best fit linear regression linear line can be created from the data this linear regression line is a synthesizer created from the original data the next step will be generating more synthetic data from the synthesizer build or from this linear line equation in this way the new data can be used for studies and research and it protects the confidentiality of the original data sup 9 sup david jensen from the knowledge discovery laboratory mentioned how to generate synthetic data in his proximity 4 3 tutorial chapter 6 researchers frequently need to explore the effects of certain data characteristics on their data model to help construct data set datasets exhibiting specific properties such as autocorrelation auto correlation or degree disparity proximity can generate synthetic data having one of several types of graph structure sup 10 sup random graph s that is generated by some random process lattice graph s having a ring structure lattice graph s having a grid structure etc in all cases the data generation process follows the same process 1 generate the empty graph data structure graph structure 2 generate attribute value system attribute values based on user supplied prior probabilities since the attribute value system attribute values of one object may depend on the attribute value system attribute values of related objects the attribute generation process assigns values collectively sup 10 sup references reflist wang a qiu t shao l 2009 a simple method of radial distortion correction with centre of distortion estimation 35 retrieved from http www springerlink com content 8180144q56t30314 fulltext pdf duncan g 2006 statistical confidentiality is synthetic data the answer retrieved from http www idre ucla edu events ppt 2006 02 13 duncan synthetic data ppt jensen d 2004 proximity 4 3 tutorial chapter 6 retrieved from http kdl cs umass edu proximity documentation tutorial ch06s09 html jackson c murphy r kova\xcb\x87cevic\xc2\xb4 j 2009 intelligent acquisition and learning of fluorescence microscope data models 18 9 retrieved from http www andrew cmu edu user jelenak repository 08 jacksonmk pdf cite book author adam coates and blake carpenter and carl case and sanjeev satheesh and bipin suresh and tao wang and david j wu and andrew y ng chapter text detection and character recognition in scene images with unsupervised feature learning title icdar year 2011 pages 440 445 accessdate 13 may 2014 external links the datagenerator a model based synthetic data generator http finraos github io datagenerator the datgen synthetic data generator http www datasetgenerator com fienberg s e 1994 conflicts between the needs for access to statistical information and demands for confidentiality journal of official statistics 10 115 132 little r 1993 statistical analysis of masked data journal of official statistics 9 407 426 raghunathan t e reiter j p and rubin d b 2003 multiple imputation for statistical disclosure limitation journal of official statistics 19 1 16 reiter j p 2004 simultaneous use of multiple imputation for missing data and disclosure limitation survey methodology 30 235 242 foldoc statistics category data category computer data category data management'
b'unreferenced date december 2009 a linear medium is any medium which is intended to be written to or accessed in a linear fashion literally meaning in a line this means that the information is written to or read from the medium in a given order so for example a book containing a novel is intended to be read from front to back beginning to end and is therefore a linear medium it may be written in the same way but would not necessarily need to be to be considered a linear medium a book containing an encyclopedia however is a non linear medium as it is not necessary for the articles to be accessed or written in any particular order even though both non linear and linear mediums have perimeters to which they are restricted linear mediums have a set path of how to get from point a to point b whereas non linear mediums do not examples in technology are a pre recorded videocassette which is usually accessed one item after another compared with a pre recorded dvd which can be accessed in any order types of linear medium scroll magnetic tape data storage paper tape photographic film novel story book s compact cassette s see also sequential access random access defaultsort linear medium category data management'
b'multiple issues unreferenced date december 2009 lead rewrite date july 2015 network transparency in its most general sense refers to the ability of a protocol to transmit data over the computer network network in a manner which is transparency human computer interaction transparent invisible to those using the applications that are using the protocol x window the term is often partially correctly applied in the context of the x window system which is able to transmit graphical data over the network and integrate it seamlessly with applications running and displaying locally however certain extensions of the x window system are not capable of working over the network ref cite web url https lwn net articles 553415 title the wayland situation facts about x vs wayland phoronix publisher lwn net date 23 june 2013 ref databases in a centralized database system the only available resource that needs to be shielded from the user is the data that is the storage system in a distributed database management system distributed dbms a second resource needs to be managed in much the same manner the computer network network preferably the user should be protected from the network operational details then there would be no difference between database applications that would run on the centralized database and those that would run on a distributed one this kind of transparency is referred to as network transparency or distribution transparency from a database management system dbms perspective distribution transparency requires that users do not have to specify where data is located some have separated distribution transparency into location transparency and naming transparency location transparency in commands used to perform a task is independent both of the locations of the data and of the system on which an operation is carried out naming transparency means that a unique name is provided for each object in the database firewalls see also proxy server transparent proxy transparency in firewall technology can be defined at the networking ip or internet layer internet layer or at the internet layer application layer transparency at the ip layer means the client targets the real ip address of the server if a connection is non transparent then the client targets an intermediate host address which could be a proxy or a caching server ip layer transparency could be also defined from the point of server s view if the connection is transparent the server sees the real client ip if it is non transparent the server sees the ip of the intermediate host transparency at the application layer means the client application uses the protocol in a different way an example of a transparent http request for a server syntaxhighlight lang text get http 1 1 host example org connection keep alive syntaxhighlight an example non transparent http request for a proxy cache syntaxhighlight lang text get http foo bar http 1 1 proxy connection keep alive syntaxhighlight application layer transparency is symmetric when the same working mode is used on both the sides the transparency is asymmetric when the firewall usually a proxy converts server type requests to proxy type or vice versa transparency at the ip layer does not mean automatically application layer transparency see also portal computer networking data independence replication transparency references reflist defaultsort network transparency category telecommunications category data management'
b'unreferenced date december 2009 a white pages schema is a data model specifically a logical schema for organizing the data contained in entries in a directory service database or application such as an address book in a white pages directory each entry typically represents an individual user computing person that makes use of network resources such as by receiving email or having an account to log in to a system in some environments the schema may also include the representation of organizational divisions roles groups and devices the term is derived from the white pages the listing of individuals in a telephone directory typically sorted by the individual s home location e g city and then by their name while many postal telephone and telegraph telephone service providers have for decades published a list of their subscriber s in a telephone directory and similarly corporations published a list of their employees in an internal directory it was not until the rise of electronic mail systems that a requirement for standards for the electronic exchange of subscriber information between different systems appeared a white pages schema typically defines for each real world object being represented what attributes of that object are to be represented in the entry for that object what relationships of that object to other objects are to be represented how is the entry to be named in a directory information tree dit how an entry is to be located by a client searching for it how similar entries are to be distinguished how are entries to be ordered when displayed in a list one of the earliest attempts to standardize a white pages schema for electronic mail use was in x 520 and x 521 part of the x 500 specifications that was derived from the addressing requirements of x 400 and defined a directory information tree that mirrored the international telephone system with entries representing residential and organizational subscribers this evolved into the lightweight directory access protocol standard schema in rfc 2256 one of the most widely deployed white pages schemas used in ldap for representing individuals in an organizational context is inetorgperson defined in rfc 2798 although versions of active directory require a different object class user many large organizations have also defined their own white pages schemas for their employees or customers as part of their identity management architecture converting between data bases and directories using different schemas is often the function of a metadirectory and data interchange standards such as common indexing protocol some early directory deployments suffered due to poor design choices in their white pages schema such as attributes used for naming purposes were non unique in large environments such as a person s common name attributes used for naming purposes were likely to change such as surnames attributes were included which could lead to identity theft such as a social security number users were required during provisioning to choose attributes which are unique but still memorable to them numerous other proposed schemas exist both as standalone definitions suitable for use with general purpose directories or as embedded into network protocols examples of other generic white pages schemas include vcard defined in rfc 2426 and foaf software foaf see also recognition of human individuals defaultsort white pages schema category data modeling category data management category identity management'
b'unreferenced auto yes date december 2009 a reference table or table of reference may mean a set of references that an author may have cited or gained inspiration from whilst writing an article similar to a bibliography it can also mean an table information information table that is used as a quick and easy reference for things that are difficult to remember such as comparing imperial unit imperial with si metric measurements this kind of data is known as reference data in the context of database design a reference table is a table into which an enumeration enumerated set of possible values of a certain field data type is divested for example in a relational model relational database model of a warehouse the entity item may have a field called status with a predefined set of values such as sold reserved out of stock in a purely designed database these values would be divested into an extra entity or reference table called status in order to achieve database normalisation the entity status in this case has no true representative in the real world but rather would an exceptional case where the attribute of a certain database entity is divested into its own table the advantage of doing this is that internal functionality and optional conditions within the database and the software which utilizes it are easier to modify and extend on that particular aspect establishing an enterprise wide view of reference tables is called master data management defaultsort reference table category data management publish stub'
b'unreferenced date december 2009 uniform information representation allows information from several realms or disciplines to be displayed and worked with as if it came from the same realm or discipline it takes information from a number of sources which may have used different methodologies and metrics in their data collection and builds a single large collection of information where some records may be more complete than others across all fields of data uniform information representation is particularly important in the fields of enterprise information integration eii and electronic data interchange edi where different departments of a large organization may have collected information for different purposes with different labels and units until one department realized that data already collected by those other departments could be re purposed for their own needs saving the enterprise the effort and cost of re collecting the same information defaultsort uniform information representation category data management comp sci stub'
b'unreferenced stub auto yes date december 2009 uniform data access is a computational concept describing an even ness of connectivity and controllability across numerous target data sources necessary to fields such as enterprise information integration eii and electronic data interchange edi it is most often used regarding analysis of disparate data types and data sources which must be rendered into a uniform information representation and generally must appear wiktionary homogenous homogenous to the analysis tools when the data being analyzed is typically heterogeneous and widely varying in size type and original representation defaultsort uniform data access category data management comp sci stub'
b'distinguish non linear media unreferenced date december 2009 a nonlinear medium is one which is intended to be accessed in a nonlinear fashion it is the opposite of a linear medium linear medium examples include a hard drive a newspaper a phone book a dictionary see also nonlinear linear medium random access defaultsort nonlinear medium category data management tech stub'
b'unreferenced date december 2009 file cylinder head sector svg thumb 300px right a cylinder head and sector of a hard drive the sectors are a recording container format the digital data on the disks may be both secondary container format digital container file formats and raw digital data content formats such as digital audio or ascii encoded text file worldmaplonglat eq circles tropics non png thumb 440px a map of earth showing lines of latitude horizontally and longitude vertically the lines are a grid a method for dividing and containing recorded cartographical data the land masses and oceans are cartographical data in a raw content pictorial graphical format the text is in an alphanumeric al symbolic raw content format a recording format is a content format format for encoder encoding data for storage on a storage medium the format can be container information such as cylinder head sector sectors on a disk or user audience information content media and publishing content such as analog signal analog stereo sound recording and reproduction audio multiple levels of encoding may be achieved in one format for example a text encoded page may contain html and xml encoding combined in a plain text file format using either ebcdic or ascii character encoding on a universal disk format udf digital data digital ly formatted disk in electronic media the primary format is the encoding that requires hardware to interpret decode data while secondary encoding is interpreted by secondary signal processing methods usually computer software recording container formats a container format is a system for dividing physical storage space or virtual space for data data space can be divided evenly by a systems of measurement system of measurement or divided unevenly with meta data a grid may divide physical or virtual space with physical or virtual dividers borders evenly or unevenly just as a physical container such as a file cabinet is divided by physical borders such as drawer furniture drawer s and file folder s data space is divided by virtual borders meta data such as a unit of measurement address geography address or meta tags act as virtual borders in a container format a template may be considered an abstract format for containing a solution as well as the content itself systems of measurement metric system geographic coordinate system grid page layout page grid film formats audio format audio data format video tape video tape format disk format file format meta data formatted text text formatting template file format template data structure raw content formats main content format a raw content format is a system of converting data to displayable information raw content formats may either be recorded in secondary signal processing methods such as a software container format e g digital audio digital video or recorded in the primary format a primary raw content format may be directly information processing observable e g image sound motion physics motion odor smell haptic perception sensation or physics physical data which only requires hardware to display it such as a phonograph ic gramophone needle needle and diaphragm acoustics diaphragm or a image projector projector list of light sources lamp and magnifying glass audio format homevid defaultsort recording format category communication category information science category data management category film and video technology category computer storage media category recording'
b'unreferenced date december 2009 data auditing is the process of conducting a data audit to assess how company s data is fit for given purpose this involves data profiling profiling the data and assessing the impact of data quality poor quality data on the organization s performance and profits defaultsort data auditing tech stub category data management category data quality'
b'redirect pitr other uses pitr disambiguation pitr unreferenced stub auto yes date december 2009 point in time recovery pitr in the context of computer s involves systems whereby an administrator can restore or recover a set of data or a particular setting from a time in the past note for example windows xp s capability to restore operating system settings from a past date before data corruption occurred for example time machine os x time machine for mac os x provides another example of point in time recovery once pitr logging starts for a pitr capable database a database administrator can restore that database from backup s to the state that it had at any time since external links http blog ganneff de blog 2008 02 15 postgresql continuous archivin html postgresql continuous archiving and point in time recovery pitr blog article http dev mysql com doc refman 5 5 en point in time recovery html mysql 5 5 point in time recovery defaultsort point in time recovery category data management compu stub'
b'the following tables compare general and technical information for a number of online analytical processing olap servers supporting mdx language please see the individual products articles for further information general information class wikitable sortable style font size 100 text align center width auto olap server company website latest stable version software license license pricing tm1 ibm cognos tm1 ibm ref cite web url http www 01 ibm com software data cognos index html title cognos business intelligence and financial performance management ref 10 2 2 fp4 proprietary software proprietary essbase oracle corporation oracle ref cite web url http www oracle com us solutions ent performance bi business intelligence essbase index html title oracle essbase ref 11 1 2 4 proprietary software proprietary http www oracle com us corporate pricing index htm iccube iccube ref cite web url http www iccube com title iccube olap server ref 6 0 proprietary software proprietary community http www iccube com prices jedox jedox olap server jedox ref cite web url http www jedox com en home overview html title jedox ag business intelligence deadurl yes archiveurl https web archive org web 20100514124342 http www jedox com 80 en home overview html archivedate 2010 05 14 df ref 7 0 gnu general public license gpl v2 or eula proprietary software proprietary infor bi olap server infor ref cite web url http www infor com title infor ref 10 6 0 proprietary software proprietary microsoft analysis services microsoft ref cite web url http www microsoft com sqlserver 2008 en us analysis services aspx title microsoft sql server 2008 analysis services ref 2016 proprietary software proprietary http www microsoft com sqlserver 2008 en us pricing aspx microstrategy intelligence server microstrategy ref cite web url http www microstrategy com software products intelligence server title microstrategy intelligence server ref 9 proprietary software proprietary mondrian olap server pentaho ref cite web url http mondrian pentaho org title pentaho analysis services mondrian project ref 3 7 eclipse public license epl free oracle olap oracle database olap option oracle corporation oracle ref cite web url http www oracle com technology documentation olap html title oracle olap documentation ref 11g r2 proprietary software proprietary http www oracle com us corporate pricing index htm sas system sas olap server sas institute ref cite web url http www sas com technologies dw storage mddb index html title sas olap server ref 9 4 proprietary software proprietary sap netweaver business intelligence sap netweaver bw sap ag sap ref cite web url http www sap com usa platform netweaver components businesswarehouse index epx title components tools ref 7 30 proprietary software proprietary cubes olap server cubes open source open source community ref cite web url http cubes databrewery org title cubes lightweight olap python toolkit ref 1 0 1 mit license mit data storage modes class wikitable sortable style font size 100 text align center width auto olap server molap rolap holap in memory offline tm1 ibm cognos tm1 yes no no no yes http www 01 ibm com support knowledgecenter ssvj22 10 2 2 com ibm swg ba cognos dsk ug 10 2 2 doc t dsk maintain offline html 23t dsk maintain offline cognos insight distributed mode essbase yes no no no iccube yes no no yes http www iccube com support documentation user guide using offline cubes html offline cubes infor bi olap server yes no no yes local cubes jedox jedox olap server yes yes yes no no microsoft analysis services yes yes yes yes yes local cubes br powerpivot powerpivot for excel br power bi power bi desktop microstrategy microstrategy intelligence server yes yes yes no yes http www microstrategy com software products user interfaces office microstrategy office br http www microstrategy com software products service modules report services dynamic dashboards mondrian olap server no yes no no oracle olap oracle database olap option no yes no no sas system sas olap server yes yes yes no ibm cognos bi yes yes yes no sap netweaver business intelligence sap netweaver bw yes yes no no cubes olap server no yes no apis and query languages apis and query languages olap servers support class wikitable sortable style font size 100 text align center width auto olap server xml for analysis ole db for olap multidimensional expressions mdx stored procedures custom functions sql linq ref name linq cite web url http agiledesignllc com products title ssas entity framework provider ref visualization json rest api essbase yes yes yes yes yes no yes smartview excel addin webanalysis financial reports dunno dunno iccube yes yes yes yes java programming language java ref cite web url http www iccube com support documentation mdx integration java integration html title iccube java integration documentation publisher iccube ref r programming language r ref cite web url http www iccube com support documentation mdx integration r integration html title iccube r language integration documentation publisher iccube ref yes no yes yes java programming language java javascript dunno dunno infor bi olap server yes yes yes yes olap rules push rules application engine yes yes no yes application studio dunno dunno jedox jedox olap server yes yes yes yes cube rules svs triggers yes no yes dunno dunno dunno microsoft analysis services yes yes yes yes net framework net ref cite web url http msdn microsoft com en us library ms176113 aspx title sql server 2008 books online october 2009 defining stored procedures publisher msdn ref yes ref cite web url http msdn microsoft com en us library ms145486 aspx title sql server 2008 books online october 2009 using stored procedures publisher msdn ref yes ref cite web url http support microsoft com kb 218592 en gb title how to perform a sql server distributed query with olap server publisher msdn ref yes yes microsoft excel sharepoint microsoft power bi and 70 other visualization tools ref cite web url http www ssas info com analysis services client tools frontend title a collection of ssas frontend tools publisher ssas info com ref dunno dunno microstrategy microstrategy intelligence server yes no yes yes yes yes yes dunno dunno dunno mondrian olap server yes yes ref cite web url http www simba com news pentaho simba partner for excel connectivity htm title pentaho and simba technologies partner to bring world s most popular open source olap project to microsoft excel users ref yes yes yes ref cite web url http mondrian pentaho org documentation schema php user defined function title how to define a mondrian schema publisher pentaho ref yes yes no dunno dunno oracle olap oracle database olap option no yes ref name oraclemdx cite web url http www oracle com us corporate press 036550 title oracle and simba technologies introduce mdx provider for oracle olap ref yes ref name oraclemdx yes java programming language java pl sql olap dml yes yes ref cite web url http www oracle com technology products bi olap 11g demos olap sql demo html title querying oracle olap cubes fast answers to tough questions using simple sql ref no dunno dunno dunno sas system sas olap server yes yes yes no no no yes yes web report studio dunno dunno sap netweaver business intelligence sap netweaver bw yes yes yes no yes no yes dunno dunno dunno tm1 cognos tm1 yes yes yes yes yes no yes tm1 web tm1 contributor ibm cognos insight ibm performance modeler ibm cognos cafe for excel cognos bi tm1 perspectives for excel dunno yes cubes olap server cubes no no no no yes no no cubes viewer ref cite web url https github com jjmontesl cubesviewer title cubes viewer publisher jjmontes ref yes dunno olap distinctive features a list of olap features that are not supported by all vendors all vendors support features such as parent child multilevel hierarchy drilldown data processing management and performance related features class wikitable sortable style font size 100 text align center width auto olap server real time write back partitioning usage based optimizations load balancing and clustering essbase no yes yes yes yes iccube yes ref cite web url http www iccube com support documentation user guide walkthrough walkthrough rt html title iccube real time walkthrough ref yes ref cite web url http www iccube com support documentation mdx update 20cube html title iccube writeback update cube ref yes ref cite web url http www iccube com support documentation user guide reference partitioning edition html title iccube partitioning ref dunno dunno infor bi olap server yes yes yes dunno dunno jedox jedox olap server yes yes yes dunno dunno microsoft analysis services yes yes yes yes yes microstrategy microstrategy intelligence server dunno yes ref cite web url http www microstrategy com software products dev tools sdk extensions asp title common extensions of the microstrategy platform ref yes dunno dunno mondrian olap server yes yes2 planned yes dunno dunno oracle olap oracle database olap option dunno yes yes no dunno ibm cognos tm1 yes yes yes dunno dunno ibm cognos bi yes no yes yes dunno sas olap server yes yes dunno dunno sap netweaver business intelligence sap netweaver bw yes yes dunno dunno cubes olap server cubes dunno dunno dunno dunno dunno data modeling features class wikitable sortable style font size 100 text align center width auto olap server semi additive measures many to many multi cube model perspectives kpi translations named sets multi attribute hierarchies actions essbase yes dunno dunno dunno yes yes yes yes dunno iccube yes ref cite web url http www iccube com support documentation user guide schemas cubes facts aggregation html title iccube aggregatin types ref yes ref cite web url http www iccube com support documentation user guide schemas cubes facts many2many html title iccube many to many ref yes dunno dunno dunno dunno dunno dunno infor bi olap server yes dunno yes dunno yes dunno dunno dunno dunno jedox jedox olap server yes yes dunno yes dunno dunno dunno dunno dunno microsoft analysis services yes yes yes yes yes yes yes yes yes microstrategy microstrategy intelligence server yes dunno dunno dunno dunno dunno dunno dunno dunno mondrian olap server yes ref cite web url http jira pentaho com browse mondrian 962 title support for non additive and semi additive measures ref dunno dunno dunno dunno dunno dunno dunno dunno oracle olap oracle database olap option yes dunno dunno dunno dunno dunno dunno dunno dunno ibm cognos tm1 yes yes yes dunno dunno dunno dunno dunno dunno ibm cognos bi yes yes dunno dunno dunno dunno yes yes dunno sas olap server yes dunno dunno dunno dunno dunno dunno dunno dunno sap netweaver business intelligence sap netweaver bw yes dunno dunno dunno dunno dunno dunno dunno dunno cubes olap server cubes dunno yes dunno dunno dunno dunno dunno dunno dunno system limits class wikitable sortable style font size 100 text align center width auto olap server cubes measures dimensions dimensions in cube hierarchies in dimension levels in hierarchy dimension members essbase ref cite web url http docs oracle com cd e57185 01 epm 1112 essbase db frameset htm limits html title essbase server limits publisher oracle ref 255 255 20 000 000 aso 1 000 000 bso iccube java integer 32 bits 2 147 483 647 2 147 483 647 2 147 483 647 2 147 483 647 2 147 483 647 2 147 483 647 infor bi olap server 10 000 000 30 10 000 000 jedox jedox olap server 2 32 32 bits 2 64 64 bits 2 32 2 32 32 bits 2 64 64 bits 250 2 32 2 32 2 32 microsoft analysis services ref cite web url http technet microsoft com en us library ms365363 aspx title sql server 2008 books online october 2009 maximum capacity specifications analysis services multidimensional data publisher microsoft ref 2 147 483 647 2 147 483 647 2 147 483 647 2 147 483 647 max number of dimensions in a database 2 147 483 647 2 147 483 647 2 147 483 647 xolap unrestricted in memory microstrategy microstrategy intelligence server unrestricted by server based on hardware limits infinite it s not possible unrestricted efn name fn0 unrestricted efn name fn0 unrestricted efn name fn0 unrestricted efn name fn0 unrestricted efn name fn0 unrestricted efn name fn0 sas system sas olap server ref cite web url http support sas com documentation cdl en olapug 63148 html default viewer htm p0m66bhcbgqwjen1jyfhf6woysu3 htm title sas olap cube size specifications ref unrestricted efn name fn0 1024 128 128 19 4 294 967 296 ibm cognos tm1 unrestricted efn name fn0 unrestricted unrestricted efn name fn0 256 unrestricted efn name fn0 unrestricted unrestricted notelist notes efn name fn0 please update as unrestricted is just not possible security class wikitable sortable style font size 100 text align center width auto rowspan 2 olap server rowspan 2 authentication rowspan 2 network encryption rowspan 2 on the fly efn name fn1 colspan 3 data access cell security dimension security visual totals essbase yes essbase authentication ldap authentication microsoft active directory yes transport layer security ssl yes yes yes no iccube yes http basic form authentication windows sso ntlm kerberos yes transport layer security ssl yes yes yes yes infor bi olap server yes olap authentication infor federation services ldap microsoft active directory yes yes yes yes dunno jedox jedox olap server yes jedox authentication ldap microsoft active directory yes transport layer security ssl yes yes yes dunno microsoft analysis services yes ntlm kerberos protocol kerberos yes transport layer security ssl and sspi yes yes yes yes microstrategy microstrategy intelligence server yes host authentication database authentication ldap br microsoft active directory ntlm siteminder tivoli sap kerberos protocol kerberos yes transport layer security ssl aes ref http latam microstrategy com software products intelligence server features asp microstrategy intelligence server features ref yes yes yes oracle olap oracle database olap option yes oracle database authentication yes transport layer security ssl yes yes dunno sas system sas olap server ref cite web url http support sas com documentation cdl en mdxag 59575 html default a003230130 htm title sas olap security totals and permission conditions ref yes host authentication sas token authentication ldap microsoft active directory yes ref cite web url http support sas com documentation cdl en bisecag 61133 html default a003275910 htm title how to change over the wire encryption settings for sas servers ref yes yes yes ibm cognos tm1 yes builtin ldap microsoft active directory ntlm ibm cognos bi authentication yes transport layer security ssl yes yes yes yes notelist notes efn name fn1 on the fly the ability to define authentication dynamically via programmatic interfaces new users do not require restarting the server or redefining the security operating systems the olap servers can run on the following operating system s class wikitable sortable style font size 100 text align center width auto olap server windows linux unix z os aix essbase yes yes yes no iccube yes yes yes yes yes infor bi olap server yes no no no jedox jedox olap server yes yes yes no microsoft analysis services yes no no no microstrategy microstrategy intelligence server yes yes yes no mondrian olap server yes yes yes yes oracle olap oracle database olap option yes yes yes yes sas system sas olap server yes yes yes yes sap netweaver business intelligence sap netweaver bw yes yes yes yes ibm cognos tm1 yes yes yes no yes cubes olap server cubes yes yes yes no cite id os java note 1 cite the server availability depends on jvm java virtual machine not on the operating system cite support information class wikitable sortable style font size 100 text align center width auto olap server issue tracking system forum blog roadmap source code essbase yes http support oracle com myoracle support http forums oracle com forums main jspa categoryid 84 http communities ioug org portals 2 oracle essbase roadmap sep 09 pdf closed iccube yes http issues iccube com youtrack http www iccube com forum open infor bi olap server yes infor xtreme available upon request closed jedox jedox olap server yes http bugs palo net mantis main page php mantis http www jedox com community palo forum board php boardid 9 open microsoft analysis services yes https connect microsoft com sqlserver connect http social msdn microsoft com forums en us sqlanalysisservices threads closed microstrategy microstrategy intelligence server yes https resource microstrategy com support mainsearch aspx microstrategy resource center https resource microstrategy com forum closed mondrian olap server yes http jira pentaho com browse mondrian jira http forums pentaho org forumdisplay php f 79 http mondrian pentaho org documentation roadmap php open oracle olap oracle database olap option yes http support oracle com myoracle support http forums oracle com forums main jspa categoryid 84 closed sas system sas olap server yes http support sas com forums index jspa support http blogs sas com closed sap netweaver business intelligence sap netweaver bw yes http service sap com oss http forums sdn sap com index jspa http esworkplace sap com socoview bd1lbizjptawmszkpw1pbg render asp id 2270ead629814d05a7ececececc8d002 fragid packageid dee98d07df9fa9f1b3c7001a64d3f462 closed ibm cognos tm1 yes http ibm com support servicerequest ibm service request http www tm1forum com viewforum php f 3 closed cubes olap server cubes yes https github com databrewery cubes issues cubes github issues https groups google com forum forum cubes discuss https github com databrewery cubes wiki roadmap https github com databrewery cubes open see also cubes olap server cubes light weight open source olap server iccube palo olap database references reflist data warehouse defaultsort comparison of olap servers category online analytical processing category software comparisons olap servers category data management category data warehousing products'
b'data virtualization is any approach to data management that allows an application to retrieve and manipulate data without requiring technical details about the data such as how it is formatted at source or where it is physically located ref http searchdatamanagement techtarget com definition data virtualization what is data virtualization margaret rouse techtarget com retrieved 19 august 2013 ref unlike the traditional extract transform load etl process the data remains in place and real time access is given to the source system for the data thus reducing the risk of data errors and reducing the workload of moving data around that may never be used unlike a federated database system it does not attempt to impose a single data model on the data heterogeneous data the technology also supports the writing of transaction data updates back to the source systems ref name morgan http www computerweekly com feature data virtualisation on rise as etl alternative for data integration data virtualisation on rise as etl alternative for data integration gareth morgan computer weekly retrieved 19 august 2013 ref to resolve differences in source and consumer formats and semantics various abstraction and transformation techniques are used this concept and software is a subset of data integration and is commonly used within business intelligence service oriented architecture data services cloud computing enterprise search and master data management examples the phone house the trading name for the european operations of uk based mobile phone retail chain carphone warehouse implemented denodo s data virtualization technology between its spanish subsidiary s transactional systems and the web based systems of mobile operators ref name morgan novartis which implemented a data virtualization tool from composite software to enable its researchers to quickly combine data from both internal and external sources into a searchable virtual data store ref name morgan the storage agnostic http primarydata com primary data data virtualization platform enables applications servers and clients to transparently access data while it is intelligently migrated between direct attached network attached private and public cloud storage server flash memory pioneer fusion io co founder david flynn now primary data cto saw the need to move data across storage types to maximize efficiency with data virtualization linked data can use a single hyperlink based data source name data source name dsn to provide a connection to a virtual database layer that is internally connected to a variety of back end data sources using odbc jdbc ole db ado net service oriented architecture soa style services and or rest patterns database virtualization may use a single odbc based dsn to provide a connection to a similar virtual database layer functionality data virtualization software provides some or all of the following capabilities abstraction abstract the technical aspects of stored data such as location storage structure api access language and storage technology virtualized data access connect to different data sources and make them accessible from a common logical data access point transformation transform improve quality reformat etc source data for consumer use data federation combine result sets from across multiple source systems data delivery publish result sets as views and or data services executed by client application or users when requested data virtualization software may include functions for development operation and or management benefits include reduce risk of data errors reduce systems workload through not moving data around increase speed of access to data on a real time basis significantly reduce development and support time increase governance and reduce risk through the use of policies ref http www informatica com us products data virtualization data services rapid access to disparate data across projects without rework informatica retrieved 19 august 2013 ref reduce data storage required ref http www zdnet com blog service oriented data virtualization 6 best practices to help the business get it 7897 data virtualization 6 best practices to help the business get it joe mckendrick zdnet 27 october 2011 ref drawbacks include may impact operational systems response time particularly if under scaled to cope with unanticipated user queries or not tuned early on ref http searchdatamanagement techtarget com news 2240165242 it pros reveal the benefits drawbacks of data virtualization software it pros reveal benefits drawbacks of data virtualization software mark brunelli searchdatamanagement 11 october 2012 ref does not impose a heterogeneous data model meaning the user has to interpret the data unless combined with federated database system data federation and business understanding of the data ref name lawson http www itbusinessedge com cm blogs lawson the pros and cons of data virtualization cs 48794 the pros and cons of data virtualization loraine lawson businessedge 7 october 2011 ref requires a defined governance approach to avoid budgeting issues with the shared services not suitable for recording the historic snapshots of data data warehouse is better for this ref name lawson change management is a huge overhead as any changes need to be accepted by all applications and users sharing the same virtualization kit ref name lawson technology some data virtualization technologies include actifio copy data virtualization capsenta s ultrawrap platform ref https capsenta com ref cisco data virtualization formerly composite software denodo denodo platform datavirtuality data virtualization platform hiperfabric data virtualization and integration stonebond enterprise enabler data virtualization platform red hat jboss enterprise application platform data virtualization xaware data services history enterprise information integration eii first coined by metamatrix now known as red hat jboss data virtualization and federated database system s are terms used by some vendors to describe a core element of data virtualization the capability to create relational joins in a federated view see also data integration enterprise information integration eii master data management database virtualization federated database system data federation disparate system references reflist further reading data virtualization going beyond traditional data integration to achieve business agility judith r davis and robert eve data virtualization for business intelligence systems revolutionizing data integration for data warehouses rick van der lans data integration blueprint and modeling techniques for a scalable and sustainable architecture anthony giordano category data management'
b'automated tiered storage also automated storage tiering is the automated progression or demotion of data across different tiers types of storage devices and media the movement of data takes place in an automated way with the help of a software or embedded firmware and is assigned to the related media according to performance and capacity requirements more advanced implementations include the ability to define rules and policies that dictate if and when data can be moved between the tiers and in many cases provides the ability to pin data to tiers permanently or for specific periods of time implementations vary but are classed into two broad categories pure software based implementations that run on general purpose processors supporting most forms of general purpose storage media and embedded automated tiered storage controlled by firmware as part of a closed embedded storage system such as a san disk array software defined storage architectures commonly include a component of tiered storage as part of their primary functions in the most general definition automated tiered storage is a form of hierarchical storage management however the term automated tiered storage has emerged to accommodate newer forms of real time performance optimized data migration driven by the proliferation of solid state disks and storage class memory furthermore where traditional hsm systems act on files and move data between storage tiers in a batch scheduled like fashion automated storage tiered systems are capable of operating at sub file level both in batch and real time modes in the case of the latter data is moved almost as soon as it enters the storage system or relocated based on its activity levels within seconds of data being accessed whereas more traditional tiering tends to operate on an hourly daily or even weekly schedule some more background on the relative differences between hsm ilm and automated tiered storage is available at snia web site ref http www snia org sites default education tutorials 2012 spring storman larryfreeman what old is new again pdf ref a general comparison of different approaches can also be found in this comparison article on auto tiered storage http searchstorage techtarget co uk feature automated storage tiering product comparison os and software based automated tiered storage most server oriented software automated tiered storage vendors offer tiering as a component of a general storage virtualization stack offering an example being microsoft with their tiered storage spaces ref https redmondmag com articles 2013 08 30 windows storage tiering aspx m 1 ref however automated tiering is now becoming a common part of industry standard operating systems such as linux and microsoft windows and in the case of consumer pcs apple osx with its fusion drive ref http www apple com imac performance apple imac performance website october 24 2012 ref this solution allowed a single ssd and hard disk drive to be combined into a single automated tiered storage drive that ensured that the most frequently accessed data was stored on the ssd portion of the virtual disk a more os agnostic version was introduced by enmotus which supports real time tiering with its fuzedrive product for linux and windows operating systems extending support to storage class memory offerings such as nvdimm and nvram devices ref http cdn2 hubspot net hub 486631 file 2586107985 pdf pdfs 20111129 s2 102 mills pdf t 1447892865729 ref san based tiered storage an example of automated tiered storage in a hardware storage array is a feature called data progression from compellent technologies data progression has the capability to transparently move blocks of data between different drive types and raid groups such as raid 10 and raid 5 the blocks are part of the same virtual volume even as they span different raid groups and drive types compellent can do this because they keep metadata about every block which allows them to keep track of each block and its associations ref http blogs computerworld com compellent ilm tony asaro computerworld compellent intelligent tiered storage january 19 2009 ref another strong example of san based tiering is dothill s autonomous tiered storage which moves data between tiers of storage within the san disk array with decisions made every few seconds ref https www dothill com solutions tiered data storage hybrid data storage solution with ssd and hdd tiers ref automated tiered storage vs ssd caching while tiering solutions and caching may look the same on the surface the fundamental differences lie in the way the faster storage is utilized and the algorithms used to detect and accelerate frequently accessed data ssd caching operates much like sram dram caches do i e they make a copy of frequently accessed blocks of data for example in 4k cache page sizes and store the copy in the ssd and use this copy instead of the original data source on the slower backend storage every time a storage io occurs the caching software look to see if a copy of this data already exists using a variety of algorithms and service the host request from the ssd if it is found the ssd is used in this case as a lookaside device as it is not part of the primary storage while some good caching algorithms can demonstrate native ssd performance on reads and short bursts of writes caching typically operates well below the maximum sustainable rate of the underlying ssd devices as overhead cpu cycles are introduced during the host io commands that increasingly impact performance as the amount of data cached grows tiering on the other hand operates very differently using the specific case of ssds once data is identified as frequently used the identified blocks of data are moved in the background to the ssd and not copied as the ssd is being utilized as a primary storage tier not a look aside copy area when the data is subsequently accessed the ios occur at or near the native performance of the ssds as there area are few if any cpu cycles needed to do the simpler virtual to physical addressing translations ref http searchsolidstatestorage techtarget com tip tiering vs caching in flash based storage systems tiering vs caching in flash based storage systems ref see also hierarchical storage management tiered storage references russ taddiken senior storage architect 2006 automating data movement between storage tiers retrieved from the uw records management web site http www compellent com references external links http www snia org sites default education tutorials 2012 spring storman larryfreeman what old is new again pdf category data management'
b'advert date august 2010 photo recovery is the process of salvaging digital photographs from damaged failed corrupted or inaccessible computer data storage secondary storage secondary storage media when it cannot be accessed normally photo recovery can be considered a subset of the overall data recovery field photo loss or deletion failures may be due to both hardware or software failures recovering data after hardware failure an excellent explanation of hardware failures is provided in the section for data recovery data recovery typically if your drive or card is so badly damaged that your computer can not recognize that a drive card has been connected you will need to consult a data recovery service provider recovering data after logical failure logical damage or the inability to view photos can occur due to many reasons the most common reasons are deletion of photos corruption of boot sector of media corruption of file system disk formatting move or copy errors photo recovery using file carving the majority of photo recovery programs work by using a technique called file carving file carving data carving there are many different file carving techniques that are used to recover photos most of these techniques fail in the presence of file system fragmentation simson garfinkel showed that on average 16 of jpeg s are fragmented ref name garfinkel dfrws2007 simson garfinkel carving contiguous and fragmented files with fast object validation in proceedings of the 2007 digital forensics research workshop dfrws pittsburgh pa august 2007 ref which means on average 16 of jpegs are recovered partially or appear corrupt when recovered using techniques that can t handle fragmented photos header footer carving in header footer carving a recovery program attempts to recover photos based on the standard starting and ending byte signature of the photo format to take an example all jpeg s always begin with the hex sequence ffd8 and they must end with the hex sequence ffd9 header footer carving cannot be used to recover fragmented photos and fragmented photos will appear to be partially recovered or corrupt if incorrect data is added header footer carving along with header size carving are by far the most common techniques for photo recovery one of the first non gui console based programs to use this technique is photorec use of footers can often truncate a photo as many jpegs contain thumbnails as an embedded object if a file is terminated with a ffd9 it will be corrupted unless nested ffd8 ffd9s are counted header size carving in header size carving a recovery program attempts to recover photos based on the standard starting byte signature of the photo format along with the size of the photo that is either derived or explicitly stated in the photo format to take an example all 24 bit windows bitmaps bmp begin with the letters bm and store the size of the file in the header header size carving cannot be used to recover fragmented photos and fragmented photos will appear to be partially recovered or corrupt if incorrect data is added file structure based carving a more advanced form of carving a recovery program attempts to recover photos based on detailed knowledge of the structure rules of the photo format this will enable a recovery program to identify when a photo is not complete or fragmented but more needs to be done to see if a fragmented photo can be recovered this technique is rarely used by most photo recovery programs validated carving in validated carving a decoder is used to detect any errors in recovery of a photo more advanced forms of validated carving occur when each part of the recovered photo is compared against the rest of the photo to see if it fits visually validated carving is superb at detecting photos that are either fragmented or have parts over written or missing validated carving alone cannot be used to recover fragmented photos ref name pal ieee ip a pal and n memon http digital assembly com technology research pubs ieee trans 2006 pdf automated reassembly of file fragmented images using greedy algorithms in ieee transactions on image processing february 2006 pp 385393 ref log carving log carving occurs when a recovery program uses information left over in either file system structures or the log to recover a deleted photo for example occasionally ntfs will store in the logs the exact location of where the file was located prior to its deletion a program using log carving will be able to then recover the photo to be sure about the quality of recovery validated carving or file structure based carving should also be used to validate the recovered photo bi fragment gap carving a fragmented photo recovery technique where a header and footer are identified and then all combinations of blocks between the header and footer are validated to determine which combination results in the correct recovery of the photo ref name garfinkel dfrws2007 this technique will only work if the file is fragmented into two parts smartcarving a process by which fragmented photos are recovered by looking at blocks on the disk and determining which block is the best visual match for the photo being recovered this is done in parallel for all blocks that are not part of a recovered file ref name pal dfrws2008 a pal t sencar n memon http digital assembly com technology research pubs dfrws2008 pdf detecting file fragmentation point using sequential hypothesis testing digital forensic research workshop august 2008 ref references references further reading tanenbaum a woodhull a s 1997 operating systems design and implementation 2nd ed new york prentice hall http www informationweek com news windows showarticle jhtml articleid 200000329 what to do when windows vista crashes little known recovery strategies from information week category data recovery photo category computer data category data management category hard disk software category photography'
b'multiple issues primary sources date march 2011 cleanup date march 2011 content migration is the process of moving information stored on a web content management system cms digital asset management dam document management system dms or flat html based system to a new system flat html content can entail html files active server pages asp javaserver pages jsp php or content stored in some type of html javascript based system and can be either static or dynamic content content migrations can solve a number of issues ranging from consolidation from one or more cms systems into one system to allow for more centralized control governance of content and better knowledge management and sharing reorganizing content due to mergers and acquisitions to assimilate as much content from the source systems for a unified look and feel converting content that has grown organically either in a cms or flat html and standardizing the formatting so standards can be applied for a unified branding of the content there are many ways to access the content stored in a cms depending on the cms vendor they offer either an application programming interface api web services rebuilding a record by writing sql queries xml exports or through the web interface the api ref name refname1 requires a developer to read and understand how to interact with the source cms s api layer then develop an application that extracts the content and stores it in a database xml file or excel once the content is extracted the developer must read and understand the target cms api and develop code to push the content into the new system the same can be said for web services most cmss use a database to store and associate content so if no api exists the sql programmer must reverse engineer the table structure once the structure is reverse engineered very complex sql queries are written to pull all the content from multiple tables into an intermediate table or into some type of comma separated values csv or xml file once the developer has the files or database the developer must read and understand the target cms api and develop code to push the content into the new system the same can be said for web services xml export creates xml files of the content stored in a cms but after the files are exported they need to be altered to fit the new scheme of the target cms system this is typically done by a developer by writing some code to do the transformation html files jsp asp php or other application server file formats are the most difficult the structure for flat html files are based on a culmination of folder structure html file structure and image locations in the early days of content migration the developer had to use programming languages to parse the html files and save it as structured database xml or csv typically perl java c or c were used because of the regular expression handling capability jsp asp php coldfusion and other application server technologies usually rely on server side includes to help simplify development but makes it very difficult to migrate content because the content is not assembled until the user looks at it in their web browser this makes is very difficult to look at the files and extract the content from the file structure web scraping allows users to access most of the content directly from the web user interface since a web interface is visual this is the point of a cms some web scrapers leverage the ui to extract content and place it into a structure like a database xml or csv formats all cmss dams and dmss use web interfaces so extracting the content for one or many source sites is basically the same process in some cases it is possible to push the content into the new cms using the web interface but some cmss use java applets or active x control which are not supported by most web scrapers in that case the developer must read and understand the target cms api and develop code to push the content into the new system the same can be said for web services the basic content migration flow 1 obtain an inventory of the content br 2 obtain an inventory of binary content like images pdfs css files office docs flash and any binary objects br 3 find any broken links in the content or content resources br 4 determine the menu structure of the content br 5 find the parent sibling connection to the content so the links to other content and resources are not broken when moving them br 6 extract the resources from the pages and store them into a database or file structure store the reference in a database or a file br 7 extract the html content from the site and store locally br 8 upload the resources to the new cms either by using the api or the web interface and store the new location in a database or xml br 9 transform the html to meet the new cmss standards and reconnect any resources br 10 upload the transformed content into the new system references references ref name refname1 http msdn microsoft com en us library ms453426 aspx what the content migration apis are not ref references external links http www cmswire com cms web publishing no small task migrating content to a new cms 002437 php no small task migrating content to a new cms category data management'
b'the jenks optimization method also called the jenks natural breaks classification method is a data clustering method designed to determine the best arrangement of values into different classes this is done by seeking to minimize each class s average deviation from the class mean while maximizing each class s deviation from the means of the other groups in other words the method seeks to reduce the variance within classes and maximize the variance between classes ref name jenks jenks george f 1967 the data model concept in statistical mapping international yearbook of cartography 7 186 190 ref ref name mcmaster mcmaster robert in memoriam george f jenks 1916 1996 cartography and geographic information science 24 1 p 56 59 ref history george jenks george frederick jenks was a 20th century american cartography cartographer graduating with his ph d in agricultural geography from syracuse university in 1947 jenks began his career under the tutelage of richard edes harrison richard harrison cartographer for time magazine time and fortune magazine ref name mcmaster2 mcmaster robert and mcmaster susanna 2002 a history of twentieth century american academic cartography cartography and geographic information science 29 3 p 312 315 ref he joined the faculty of the university of kansas in 1949 and began to build the cartography program during his 37 year tenure at ku jenks developed the cartography program into one of three programs renowned for their graduate education in the field the others being the university of wisconsin and the university of washington much of his time was spent developing and promoting improved cartographic training techniques and programs he also spent significant time investigating three dimensional maps eye movement research thematic map communication and geostatistics ref name mcmaster ref name mcmaster2 ref name csun csun cartography specialty group http www csun edu hfgeg003 csg winter97 html winter 1997 newsletter ref development jenks was a cartographer by profession his work with statistics grew out of a desire to make choropleth map s more visually accurate for the viewer in his paper the data model concept in statistical mapping he claims that by visualizing data in a three dimensional model cartographers could devise a systematic and rational method for preparing choroplethic maps ref name jenks jenks used the analogy of a blanket of error to describe the need to use elements other than the mean to generalize data the three dimensional models were created to help jenks visualize the difference between data classes his aim was to generalize the data using as few planes as possible and maintain a constant blanket of error method the method requires an iterative process that is calculations must be repeated using different breaks in the dataset to determine which set of breaks has the smallest in class variance the process is started by dividing the ordered data into groups initial group divisions can be arbitrary there are four steps that must be repeated calculate the sum of squared deviations between classes sdbc calculate the sum of squared deviations from the array mean sdam subtract the sdbc from the sdam sdam sdbc this equals the sum of the squared deviations from the class means sdcm after inspecting each of the sdbc a decision is made to move one unit from the class with the largest sdbc toward the class with the lowest sdbc new class deviations are then calculated and the process is repeated until the sum of the within class deviations reaches a minimal value ref name jenks ref name esri esri faq http support esri com index cfm fa knowledgebase techarticles articleshow d 26442 what is the jenks optimization method ref alternatively all break combinations may be examined sdcm calculated for each combination and the combination with the lowest sdcm selected since all break combinations are examined this guarantees that the one with the lowest sdcm is found finally the gvf statistic goodness of variance fit is calculated gvf is defined as sdam sdcm sdam gvf ranges from 0 worst fit to 1 perfect fit uses main article choropleth map jenks goal in developing this method was to create a map that was absolutely accurate in terms of the representation of data s spatial attributes by following this process jenks claims the blanket of error can be uniformly distributed across the mapped surface he developed this with the intention of using relatively few data classes less than seven because that was the limit when using monochromatic shading on a choroplethic map ref name jenks alternative methods main article cluster analysis other methods of data classification include head tail breaks natural breaks without jenks optimization equal interval quantile and standard deviation see also k means clustering a generalization for multivariate data jenks natural breaks optimization seems to be one dimensional k means ref http www quantdec com sysen597 gtkav section1 chapter 9 htm ref references reflist external links esri faq http support esri com index cfm fa knowledgebase techarticles articleshow d 26442 what is the jenks optimization method volunteered geographic information daniel lewis http danieljlewis org 2010 06 07 jenks natural breaks algorithm in python jenks natural breaks algorithm with an implementation in python object vision wiki http wiki objectvision nl index php fisher 27s natural breaks classification fisher s natural breaks classification a o k n log n algorithm http www ehdp com vitalnet breaks 1 htm what is jenks natural breaks category data management category cartography'
b'for the iron maiden album bbc archives album distinguish bbc motion gallery engvarb date september 2013 use dmy dates date september 2013 file bbc information and archives logo svg thumb 300px bbc information and archives logo bbc information and archives sometimes known just as bbc archives are collections documenting the bbc s broadcasting history including copies of bbc television television and bbc radio radio broadcasts internal documents photographs bbc online online content sheet music commercially available music press cuttings and historic equipment ref name bbcarchive tv 1 the original copies of these collections are permanently retained but are now in the process of being digitised estimated to take until approximately 2015 some collections are now being uploaded onto the bbc archives website on bbc online for viewers to see the archive is one of the largest broadcast archives in the world with over 12 million items ref name perivale1 cite web last hayes first sarah title the new bbc archive centre in perivale url http www bbc co uk blogs aboutthebbc 2011 10 the new bbc archive centre at shtml work about the bbc blog publisher bbc online accessdate 17 january 2012 ref overview the bbc archives encompass numerous different archives containing different materials produced or acquired by the bbc the earliest material dates back to 1890 and now consists of 1 million hours of playable material in addition to documents photographs and equipment ref name gutechweekly cite news last kiss first jemima title in the bbc archive url https www theguardian com technology blog audio 2010 aug 18 bbc archive roly keating windmill road work tech weekly publisher guardian news media ltd accessdate 21 august 2010 location london date 18 august 2010 archiveurl https web archive org web 20100821165828 http www guardian co uk technology blog audio 2010 aug 18 bbc archive roly keating windmill road archivedate 21 august 2010 dashbot deadurl no ref the archives contain 12 million items on 66 miles of shelving spread over several sites ref name gutechweekly the stock is managed using a bar code system which help to locate material on the shelves and also track material that has been lent out ref name gutechweekly the bbc says that the budget for managing protecting and digitising the archive accounts for only a small part of the bbc s overall spend ref name gutechweekly the bbc is engaging in an ongoing project to digital reformatting digitise archived programme material converting recordings made on older analog recording analogue formats such as audio tape videotape and film to electronic formats which are compatible with modern computer systems much of the audio visual material was originally recorded on formats which are now obsolete and incompatible with modern broadcast equipment due to the fact that the machines used to reproduce many formats are no longer being manufactured additionally some film and audio formats are slowly disintegrating and digitisation also serves as a digital preservation programme as of summer 2010 bbc archive staff have spent approximately ten years digitising half of the media content ref name gutechweekly ref name bbc archive bbcinternetblog and due to improving work practices expect to complete the other half in five years current estimates suggest the digitised archive would comprise approximately 52 petabyte s of information ref name gutechweekly with one programme minute of video requiring 1 4 gigabyte s of storage ref name gutechweekly the bbc uses the material exchange format mxf ref name gutechweekly which is an uncompressed non proprietary format which the bbc has been publicising to mitigate the threat of the format becoming obsolete as digital formats can and do ref name gutechweekly the archive digitisation a key part of the bbc s programme to engineer a fully digital and tapeless tapeless production workflow across the entire corporation it was closely tied in with the ill fated digital media initiative dmi a scheme which ran from 2008 to 2013 and attempted to create a unified online archive search and programme production system ref name bbc dmi cite web title digital media initiative url http www bbc co uk careers divisions digital media initiative publisher bbc accessdate 15 february 2012 archiveurl https web archive org web 20120310041000 http www bbc co uk careers divisions digital media initiative archivedate 10 march 2012 deadurl yes ref after spiralling development costs and project delays the problems with dmi came to public attention during coverage of the death and funeral of margaret thatcher in april 2013 when it was reported that the lack of digital ingest facilities provided for bbc news staff meant that tapes had to be sent by taxi from the perivale centre to be digitised by independent companies in central london ref cite news title bbc s thatcher coverage highlights problems with non digital archives url https www theguardian com media 2013 apr 11 bbc thatcher coverage accessdate 3 may 2013 newspaper the guardian date 11 february 2012 location london first tara last conlan ref dmi was cancelled in 2013 ref name bbc abandons cite news title bbc abandons \xc2\xa3100m digital project url http www bbc co uk news entertainment arts 22651126 accessdate 25 may 2013 newspaper bbc news date 24 may 2013 ref the bbc archive website was relaunched online in 2008 and has provided newly released historical material regularly since then ref cite web last sangster first jim title a new homepage for bbc archive url http www bbc co uk blogs bbcinternet 2010 05 a new homepage for bbc archive html work bbc internet blog publisher bbc accessdate 19 january 2012 ref the bbc works in partnership with the british film institute bfi the national archives and other partners in working with and using the materials ref name gutechweekly a related project called genome is expected to complete in 2011 and will make programme listings dating back to 1923 sourced from the radio times available to search online ref name gutechweekly in july 2008 roly keating was appointed director of archive content ref cite web url http www bbc co uk pressoffice pressreleases stories 2008 07 july 22 archive shtml title roly keating appointed as director of archive content publisher bbc press office date 22 july 2008 accessdate 1 july 2011 ref with responsibility for increasing public access to the bbc s archives in october 2008 keating appointed tony ageh controller of archive development with specific responsibility for developing ways of making the archive easily understandable and accessible to users ref cite web url http www bbc co uk pressoffice pressreleases stories 2008 10 october 10 ageh shtml publisher bbc press office date 10 october 2008 accessdate 1 july 2011 title tony ageh appointed controller of archive development ref in 2012 bbc archive development produced a book primarily aimed as bbc staff titled bbc archive collections what s in the archive and how to use them ref bbc archive collections what s in the archives and how to use them edited by jake berger https www dropbox com s rz1o57nzlsf1v04 bbc 20archive 20collections 20guide 202012 pdf dl 0 ref this book describes the bbc s archive collections and offers guidance around on how items from the collections can be reused online the book s references to fabric a system due to be delivered by the digital media initiative are no longer accurate as the project was cancelled buildings from 1968 to 2010 the bbc archive was housed at the archive centre in windmill road brentford in w postcode area west london ref name perivale1 ref name bbc archive bbcinternetblog ref name perivale 2 atbbcblog the condition of the building deteriorated over the years and suffered occasional flooding incidents and eventually the archive was relocated to a new centre at perivale park perivale three miles north of the old site ref name perivale 2 atbbcblog ref name perivale centre r4 blog the new bbc archive centre was opened in summer 2010 and all material was successfully moved by march 2011 ref name perivale 2 atbbcblog ref name perivale 3 s pblog the cost of the refurbishment and of the move was approximately \xc2\xa316 6 million ref name perivale1 ref name perivale 2 atbbcblog cite news last skinner first peter title a new home for the bbc archive url http www bbc co uk blogs aboutthebbc 2010 08 a warm balmy afternoon in shtml accessdate 19 january 2012 newspaper bbc about the bbc blog date 20 august 2010 ref ref name perivale centre r4 blog cite news last bolton first roger title tears in perivale feedback in the archives url http www bbc co uk blogs radio4 2011 09 tears in perivale feedback in the archives html accessdate 19 january 2012 newspaper bbc radio 4 and 4 extra blog date 23 september 2011 ref ref name perivale 3 s pblog cite news last kane first chris title preserving the past at perivale url http www bbc co uk blogs spacesandplaces 2011 03 preserving the past at perival shtml accessdate 19 january 2012 newspaper bbc spaces places blog date 9 march 2011 ref ref cite web url http downloads bbc co uk foi classes disclosure logs rfi20111170 cost of new archive centre pdf title freedom of information act 2000 rfi20111170 last jupe first steve date 20 october 2011 work freedom of information request publisher bbc accessdate 7 february 2012 ref ref cite web title the bbc archive centre has moved url http www bbc co uk commissioning news the bbc archive centre has moved shtml work bbc commissioning publisher bbc accessdate 19 january 2012 ref material is stored in thirteen vaults ref name perivale 2 atbbcblog controlled to match the best climate for the material inside them ref name perivale1 ref name gutechweekly ref name bbc archive bbcinternetblog ref name perivale 2 atbbcblog and named after a different bbc personality depending on the content contained in them ref name perivale 2 atbbcblog in addition to the vaults new editing and workrooms have been added so that the material can easily be transferred between formats as well as viewed and restored ref name perivale 2 atbbcblog the building has also been fitted with fire suppression systems to protect the archive in the event of an incident at the centre so the total loss of the archive is avoided ref name perivale centre r4 blog television archive the bbc television archive contains over 600 000 hours of television broadcast material ref name bbcarchive tv 1 cite web last lee first adam title bbc television archive what s in the bbc archive url http www bbc co uk archive tv archive shtml work bbc archive meet the experts publisher bbc accessdate 19 january 2012 ref located on 600 000 650 000 film reels ref name windmill road cite web title a tour of the bbc archive at windmill road url https www youtube com watch v s3z2djraw2m publisher bbc accessdate 23 july 2015 date 13 aug 2010 ref ref name bbc archive bbcinternetblog cite news last williams first adrian title safeguarding the bbc s archive url http www bbc co uk blogs bbcinternet 2010 08 safeguarding the bbcs archive html accessdate 19 january 2012 newspaper bbc internet blog date 18 august 2010 ref ref name bbcarchive ptv 4 cite web last williams first adrian title preserving the television archive film url http www bbc co uk archive preserving shtml chapter 4 work bbc archives meet the experts publisher bbc accessdate 19 january 2012 ref and 2 4 2 7 million videotapes ref name bbc archive bbcinternetblog ref name windmill road the archive itself holds extensive material from approximately the mid 1970s onwards when important recordings at the broadcaster were retained for the future ref name bbcarchives tv 6 cite web last lee first adam title bbc television archive when did the bbc start to ensure that important broadcasts were not destroyed url http www bbc co uk archive tv archive shtml chapter 6 work bbc archives meet the experts publisher bbc accessdate 19 january 2012 ref recordings from before this date are less comprehensively preserved the process of kinescope telerecording was originally invented in 1947 ref name bbcarchive tv 2 cite web last lee first adam title bbc television archive why aren t there many recordings from the early days of television url http www bbc co uk archive tv archive shtml chapter 2 work bbc archive meet the experts publisher bbc accessdate 19 january 2012 ref while videotape recording was gradually introduced from the late 1950s onwards ref name bbc archive tv 4 cite web last lee first adam title bbc television archive when did the bbc start recording programmes regularly url http www bbc co uk archive tv archive shtml chapter 4 work bbc archive meet the experts publisher bbc accessdate 19 january 2012 ref but due to the expense of the tapes ref name bbcarchives ptv 8 cite web last williams first adrian title preserving the television archive why was videotape invented url http www bbc co uk archive preserving shtml chapter 8 work bbc archives meet the experts publisher bbc accessdate 19 january 2012 ref recording was seen for production use only with recordings subsequently being wiping wiped ref name bbc archive tv 4 or telerecordings being junked the exceptions in the early years were usually occasions of great importance such as the coronation of queen elizabeth ii ref name bbcarchive tv 2 in addition numerous programmes at the time were broadcast live and so utilised no recording procedure in the production process ref name bbcarchive tv 2 the earliest item in the collection is from 1936 ref name bbc archive bbcinternetblog ref name perivale 3 s pblog ref name bbcarchives ptv 6 cite web last williams first adrian title preserving the television archive the oldest bbc television film clip url http www bbc co uk archive preserving shtml chapter 6 work bbc archives meet the experts publisher bbc accessdate 19 january 2012 ref in 2013 there were 340 000 d3 tapes but the hardware they have could only transfer up to 130 000 d3 tapes ref name digitising cite web title digitising the bbc archive url http www bbc co uk academy technology article art20130704121742520 publisher bbc accessdate 23 july 2015 date 2013 ref the bbc has had to be very selective of what they are transferring ref name digitising before anything is put into the archive a team of digitisation operators watch and listen to programs looking for problems with the tapes or transfers ref name sarah bello cite web title sarah bello bbc archive url https www youtube com watch v myg1lsdskqs publisher bbc accessdate 23 july 2015 date 11 march 2013 ref today the majority of programmes are kept including news entertainment drama and a selection of other long running programmes such as quiz shows ref name bbcarchive tv 7 cite web last lee first adam title bbc television archive how does the bbc decide what to keep in its archive today url http www bbc co uk archive tv archive shtml chapter 7 work bbc archive meet the experts publisher bbc accessdate 19 january 2012 ref the remaining material from the television archive is offered to the british film institute prior to being disposed of ref name bbcarchive tv 8 cite web last lee first adam title bbc television archive does the bbc offer recordings it s not keeping for the archive to anyone else url http www bbc co uk archive tv archive shtml chapter 8 work bbc archives meet the experts publisher bbc accessdate 19 january 2012 ref sound archive main bbc sound archive the bbc sound archive contains the archived output from the bbc s radio output widespread recordings exist in the archive from the mid 1930s when recording of programmes and speeches were kept for rebroadcast the catalyst for this was the launch of the bbc empire service in 1932 and the subsequent rebroadcast of speeches from political leaders at a time convenient in the different time zones ref name bbcarchive radio 3 cite web last rooks first simon title bbc sound archive why did the bbc start making recordings url http www bbc co uk archive sound archive shtml chapter 3 work bbc archives meet the experts publisher bbc accessdate 19 january 2012 ref prior to this the broadcast of recordings was seen as being false to the listener and was avoided ref name bbcarchives radio 2 cite web last rooks first simon title bbc sound archive why aren t there many recordings from the early days of radio url http www bbc co uk archive sound archive shtml chapter 2 work bbc archives meet the experts publisher bbc accessdate 19 january 2012 ref any recordings made were frequently disposed of and it was the efforts of marie slocombe who founded the sound archive in 1937 when she retained recordings of prominent figures in the country that the archive became into being officially when she was appointed the sounds recording librarian in 1941 ref name bbcarchives pradio 6 cite web last weaver first julia title preserving the sound archive how did the sound archive begin url http www bbc co uk archive preserving sound shtml chapter 6 work bbc archives meet the experts publisher bbc accessdate 19 january 2012 ref today all of the bbc s radio output is recorded for re use ref name bbcarchive radio 8 cite web last rooks first simon title bbc sound archive does the bbc keep copies of all programmes today url http www bbc co uk archive sound archive shtml chapter 8 work bbc archives meet the experts publisher bbc accessdate 19 january 2012 ref with approximately 66 of output being preserved in the archives ref name bbcarchive radio 8 programmes involving guests or live performances from artists are kept ref name bbcarchive radio 8 whereas programmes in which the dj plays commercially available music are only sampled and not kept entirely ref name bbcarchive radio 8 prior to any material being disposed of the material is offered to the british library sound archive ref name bbcarchive tv 8 the archive consists of a number of different formats including 200 phonograph cylinder wax cylinders ref name bbcarchive pradio 3 cite web last weaver first julia title preserving the sound archive what are the earliest sound recordings url http www bbc co uk archive preserving sound shtml chapter 3 work bbc archives meet the experts publisher bbc accessdate 19 january 2012 ref numerous gramophone record s made from both shellac and vinyl ref name bbcarchives pradio 4 cite web last weaver first julia title preserving the sound archive discs url http www bbc co uk archive preserving sound shtml chapter 4 work bbc archives meet the experts publisher bbc accessdate 19 january 2012 ref as well as numerous more recordings on reel to reel audio tape recording tape cd and on digital audio tape dat ref name bbcarchive pradio 5 cite web last weaver first julia title preserving the sound archive tape url http www bbc co uk archive preserving sound shtml chapter 5 work bbc archives meet the experts publisher bbc accessdate 19 january 2012 ref the difficulty of these different formats is the availability of the machines required to play them some of the vinyl records in the archive are 16 inches in size and require large phonograph units to play ref name bbcarchives pradio 4 while the players for the wax cylinders and dats are no longer in production ref name bbcarchive pradio 5 there are 700 00 vinyl records 180 000 78 s records 400 000 lp record and 350 000 compact disc cd s in the archive ref name windmill road the oldest item is a wax cylinder containing a recording made by florence nightingale recorded on 30 july 1890 ref name bbcarchive pradio 3 another unique item is the gramophone record from mary of teck queen mary s doll house which is approximately an inch in size and had the god save the queen national anthem on it ref name bbcarchives pradio 4 the sound archive is based at the new bbc archive centre in perivale along with the television archive ref name perivale1 ref name perivale 3 s pblog and was previously based at windmill road brentford written archives the bbc written archive contains all the internal written documents and communications from the corporation from the launch in 1922 to the present day ref name written archives bbc story cite web title the written archives url http www bbc co uk historyofthebbc contacts wac shtml work the bbc story publisher bbc accessdate 19 january 2012 ref ref name bbcarchive written 1 cite web last kavanagh first jacquie title bbc written archives what are the bbc written archives url http www bbc co uk archive written shtml work bbc archives meet the experts publisher bbc accessdate 19 january 2012 ref its collections shed light into the behind the scenes workings of the corporation and also elaborate on the difficulties of getting a television or radio programme to or off the air as the case may be ref name bbc archive written 3 cite web last kavanagh first jacquie title bbc written archives what do the documents reveal url http www bbc co uk archive written shtml chapter 3 work bbc archives meet the experts publisher bbc accessdate 19 january 2012 ref the archive guidelines state that access to files post 1980 is restricted due to the current nature of the files the general exception to this rule are documents such as scripts and programme as broadcast records the written archives are located at the bbc written archive centre in caversham berkshire near reading berkshire reading ref name written archives bbc story the centre houses the archive on four and a half miles of shelving along with reading rooms the centre is different from the other bbc archives in that the centre opens for writers and academic researchers in higher education ref name written archives bbc story photographic library the bbc photographic library is responsible for approximately 10 million images ref name bbcarchive photo 1 cite web last dewar first natalie title photographic library what s in the bbc photo library url http www bbc co uk archive photo library shtml work bbc archives meet the experts publisher bbc accessdate 19 january 2012 ref dating back to 1922 ref cite web title bbc pictures url http www bbc co uk mediacentre pictures index html work bbc media centre publisher bbc accessdate 19 january 2012 ref created for publicity purposes and subsequently kept for future use ref name bbcarchive photo 2 cite web last dewar first natalie title photographic library why does the bbc have photographs url http www bbc co uk archive photo library shtml chapter 2 work bbc archives meet the experts publisher bbc accessdate 19 january 2012 ref in addition to programme promotion a large number of images are of historic events which are often incorporate into the daily news bulletins as a result half the photographic library team work specifically with these images ref name bbcarchive photo 4 cite web last dewar first natalie title photographic library the team url http www bbc co uk archive photo library shtml chapter 4 work bbc archives meet the experts publisher bbc accessdate 19 january 2012 ref the images themselves are kept as originals in the archive with digitisation only utilised when a specific image is required for use when the image is sent in a digital format ref name bbcarchive photo 5 cite web last dewar first natalie title photographic library what format are the images stored on url http www bbc co uk archive photo library shtml chapter 5 work bbc archives meet the experts publisher bbc accessdate 19 january 2012 ref copies of images are also used in case any images are damaged notable due to vinegar syndrome ref name bbcarchive photo 6 cite web last dewar first natalie title photographic library preservation url http www bbc co uk archive photo library shtml chapter 6 work bbc archives meet the experts publisher bbc accessdate 19 january 2012 ref the bbc photographic library itself is based within bbc television centre london the most popular images from the archive include colin firth in pride and prejudice 1995 tv series pride and prejudice michael parkinson interviewing muhammad ali jimmy savile presenting the first top of the pops martin bashir interviewing diana princess of wales and a picture of delia derbyshire at work in the radiophonic workshop at the bbc ref name bbcarchive photo 8 cite web last dewar first natalie title photographic library our top 10 url http www bbc co uk archive photo library shtml chapter 8 work bbc archive meet the experts publisher bbc accessdate 19 january 2012 ref heritage collection the bbc heritage collection is the newest of the bbc archives and holds a variety of historic broadcast technology art props and merchandise ref name bbcarchives heritage 2 cite web last o connell first rory title bbc heritage collection where do the items come from url http www bbc co uk archive heritage shtml chapter 2 work bbc archives meet the experts publisher bbc accessdate 19 january 2012 ref the collection was created out of personal collections and bequeaths by former staff members as the bbc had no formal policy on the heritage collection until c 2003 ref name bbcarchives heritage 2 the collection includes amongst other items the bbc one noddy globe and clock ref name bbcarchive heritage 3 cite web last o connell first rory title bbc heritage collection broadcast technology url http www bbc co uk archive heritage shtml chapter 3 work bbc archives meet the experts publisher bbc accessdate 19 january 2012 ref a bbc marconi type a microphone ref name bbcarchive heritage 3 an early crystal radio made by the british broadcasting company ref name bbcarchive heritage 3 a 405 line television system marconi emi camera used in the early bbc television experiments ref name bbcarchive heritage 3 a bbc micro computer ref name bbcarchive heritage 3 and a selection of items used to create foley filmmaking foley ref name bbcarchive heritage 3 in addition to all the broadcast technology art is also kept namely the portraits of all the bbc director general of the bbc director general s ref name bbcarchive heritage 4 cite web last o connell first rory title bbc heritage collection art url http www bbc co uk archive heritage shtml chapter 4 work bbc archives meet the experts publisher bbc accessdate 19 january 2012 ref as well as props including an original tardis from doctor who ref name bbcarchive heritage 5 cite web last o connell first rory title bbc heritage collection costumes and props url http www bbc co uk archive heritage shtml chapter 5 work bbc archives meet the experts publisher bbc accessdate 19 january 2012 ref and the children s television puppet gordon the gopher ref name bbcarchive heritage 5 the heritage collection itself has no one permanent home as the majority of objects are on display either around bbc properties or on loan to museums or other collections the most notable museum housing the collection is the national media museum in bradford ref name bbcarchive heritage 8 cite web last o connell first rory title bbc heritage collection where can i see items from the collection url http www bbc co uk archive heritage shtml chapter 8 work bbc archives meet the experts publisher bbc accessdate 19 january 2012 ref archive treasure hunt at the turn of the millennium the bbc launched the bbc archive treasure hunt a public appeal to recover pre 1980s lost bbc radio and television productions ref cite web url http www bbc co uk cult treasurehunt about about shtml title bbc online cult treasure hunt about the campaign publisher bbc co uk date accessdate 30 july 2010 archiveurl https web archive org web 20100721235531 http www bbc co uk cult treasurehunt about about shtml archivedate 21 july 2010 dashbot deadurl no ref original material featuring several popular programmes were lost due to the practice of wiping because of copyright issues and for technological reasons ref cite web url http www bbc co uk cult treasurehunt about lost shtml title bbc online cult treasure hunt about the campaign publisher bbc co uk date accessdate 30 july 2010 ref ref cite web author stuart douglas www thiswaydown org url http www btinternet com m brown1 bbchunt htm title missing episodes articles publisher btinternet com date 7 july 1965 accessdate 30 july 2010 archiveurl https web archive org web 20100814103420 http www btinternet com m brown1 bbchunt htm archivedate 14 august 2010 dashbot deadurl no ref the resolution of this appeal was that over one hundred productions were recovered ref cite web url http fiatifta org aboutfiat news old 2001 2001 04 03 light html title no 4 2001 missing believed wiped publisher fiat ifta date accessdate 30 july 2010 archiveurl https web archive org web 20100716032923 http fiatifta org aboutfiat news old 2001 2001 04 03 light html archivedate 16 july 2010 dashbot deadurl no ref including the men from the ministry something to shout about film something to shout about man and superman the doctor s dilemma play the doctor s dilemma i m sorry i ll read that again hancock s half hour i m sorry i haven t a clue and the ronnie corbett thing in addition to recording sessions with elton john ringo starr and paul simon ref name bbcth cite web url http www bbc co uk cult treasurehunt about listoffinds shtml title bbc online cult treasure hunt list of finds publisher bbc co uk date accessdate 30 july 2010 ref ref cite web url http www allbusiness com services motion pictures 4848337 1 html title hunt unearths bbc treasures from radio tv 124 business solutions from publisher allbusiness com date 9 november 2001 accessdate 30 july 2010 ref also the peter sellers estate collection donated numerous recordings featuring peter sellers ref name bbcth creative archive licence the bbc together with the british film institute the open university channel 4 and teachers tv formed a collaboration named the creative archive licence group to create a copyright licence for the re release of archived material ref name cal cite web title creative archive pilot url http www bbc co uk creativearchive publisher bbc accessdate 31 march 2016 ref the licence was a trial launched in 2005 and notable for the re release of part of the bbc news archive and programmes made by the bbc natural history unit for creative use by the public while artists and teachers were encouraged to use the content to create works of their own the terms of the licence were restrictive compared to copyleft licences use of creative archive content for commercial endorsement campaigning defamatory or derogatory purposes was forbidden any derivative works were to be released under the same licence and content was only to be used within the uk ref name cal ref cite web title creative archive license url http news bbc co uk 1 hi help 4527506 stm publisher bbc accessdate 17 january 2012 ref the trial ended in 2006 following a review by the bbc trust and works released under the licence were withdrawn ref name cal voices from the archives voices from the archives is a former bbc project launched in partnership with bbc four that provided free access to audio interviews with various notable people and professions from a variety of political religious and social backgrounds the website ceased to be updated in june 2005 and the concept was instead adopted by bbc radio 4 as a collection of film interviews from various programmes programme catalog main bbc programme catalogue over the years there the bbc has used various programme catalog databases to keep a record of the programmes in the archives internal databases include infax and bbc fabric fabric and publicly accessible databases include bbc genome and http www bbc co uk programmes bbc programmes see also portal bbc bbc genome project lost film film preservation missing believed wiped telerecording doctor who missing episodes doctor who missing episodes timeline of the bbc references reflist 2 external links bbc archives bbc co uk id archive title bbc archives bbc co uk id bbcfour collections title bbc four collections bbc co uk id archive archive pioneers title bbc archive collection archive pioneers saviours of sound at the bbc bbc co uk id programmes title bbc programmes bbc co uk id informationandarchives title bbc information and archives https www theguardian com technology blog audio 2010 aug 18 bbc archive roly keating windmill road tech weekly podcast in the bbc archives from the guardian website https www dropbox com s rz1o57nzlsf1v04 bbc 20archive 20collections 20guide 202012 pdf dl 0 bbc archive collections what s in the archives and how to use them wiped material http www missing episodes com british tv missing episodes index http www wipednews com wiped news com a news and features website devoted to missing tv film radio bbc category bbc category bbc new media archives category data management category broadcasting websites category british websites category bbc offices studios and buildings archives category organisations based in reading berkshire category history of television in the united kingdom category history of radio category bbc history category year of establishment missing category archives in berkshire category television archives'
b'industrial process data validation and reconciliation or more briefly data validation and reconciliation dvr is a technology that uses process information and mathematical methods in order to automatically correct measurements in industrial processes the use of dvr allows for extracting accurate and reliable information about the state of industry processes from raw measurement data and produces a single consistent set of data representing the most likely process operation models data and measurement errors industrial processes for example chemical or thermodynamic processes in chemical plants refineries oil or gas production sites or power plants are often represented by two fundamental means models that express the general structure of the processes data that reflects the state of the processes at a given point in time models can have different levels of detail for example one can incorporate simple mass or compound conservation balances or more advanced thermodynamic models including energy conservation laws mathematically the model can be expressed by a nonlinear system nonlinear system of equations math f y 0 math in the variables math y y 1 ldots y n math which incorporates all the above mentioned system constraints for example the mass or heat balances around a unit a variable could be the temperature or the pressure at a certain place in the plant error types gallery caption random and systematic errors widths 300 perrow 2 align right file normal no bias jpg normally distributed measurements without bias file normal with bias jpg normally distributed measurements with bias gallery data originates typically from measurements taken at different places throughout the industrial site for example temperature pressure volumetric flow rate measurements etc to understand the basic principles of dvr it is important to first recognize that plant measurements are never 100 correct i e raw measurement math y math is not a solution of the nonlinear system math f y 0 math when using measurements without correction to generate plant balances it is common to have incoherencies observational error measurement errors can be categorized into two basic types random error s due to intrinsic sensor accuracy and systematic errors or gross errors due to sensor calibration or faulty data transmission random error s means that the measurement math y math is a random variable with mean math y math where math y math is the true value that is typically not known a systematic error on the other hand is characterized by a measurement math y math which is a random variable with mean math bar y math which is not equal to the true value math y math for ease in deriving and implementing an optimal estimation solution and based on arguments that errors are the sum of many factors so that the central limit theorem has some effect data reconciliation assumes these errors are normal distribution normally distributed other sources of errors when calculating plant balances include process faults such as leaks unmodeled heat losses incorrect physical properties or other physical parameters used in equations and incorrect structure such as unmodeled bypass lines other errors include unmodeled plant dynamics such as holdup changes and other instabilities in plant operations that violate steady state algebraic models additional dynamic errors arise when measurements and samples are not taken at the same time especially lab analyses the normal practice of using time averages for the data input partly reduces the dynamic problems however that does not completely resolve timing inconsistencies for infrequently sampled data like lab analyses this use of average values like a moving average acts as a low pass filter so high frequency noise is mostly eliminated the result is that in practice data reconciliation is mainly making adjustments to correct systematic errors like biases necessity of removing measurement errors isa 95 is the international standard for the integration of enterprise and control systems ref http www isa 95 com isa 95 the international standard for the integration of enterprise and control systems isa 95 com ref it asserts that blockquote data reconciliation is a serious issue for enterprise control integration the data have to be valid to be useful for the enterprise system the data must often be determined from physical measurements that have associated error factors this must usually be converted into exact values for the enterprise system this conversion may require manual or intelligent reconciliation of the converted values systems must be set up to ensure that accurate data are sent to production and from production inadvertent operator or clerical errors may result in too much production too little production the wrong production incorrect inventory or missing inventory blockquote history dvr has become more and more important due to industrial processes that are becoming more and more complex dvr started in the early 1960s with applications aiming at closing mass balance material balances in production processes where raw measurements were available for all variable mathematics variables ref d r kuehn h davidson computer control ii mathematics of control chem eng process 57 44 47 1961 ref at the same time the problem of systematic error gross error identification and elimination has been presented ref v vaclavek studies on system engineering i on the application of the calculus of the observations of calculations of chemical engineering balances coll czech chem commun 34 3653 1968 ref in the late 1960s and 1970s unmeasured variables were taken into account in the data reconciliation process ref v vaclavek m loucka selection of measurements necessary to achieve multicomponent mass balances in chemical plant chem eng sci 31 1199 1205 1976 ref ref name mah stanley downing 1976 http gregstanleyandassociates com reconciliationrectificationprocessdata 1976 pdf r s h mah g m stanley d w downing reconciliation and rectification of process flow and inventory data ind eng chem proc des dev 15 175 183 1976 ref dvr also became more mature by considering general nonlinear equation systems coming from thermodynamic models ref j c knepper j w gorman statistical analysis of constrained data sets aiche journal 26 260 164 1961 ref ref name stanley mah 1977 http gregstanleyandassociates com aichej 1977 estimationinprocessnetworks pdf g m stanley and r s h mah estimation of flows and temperatures in process networks aiche journal 23 642 650 1977 ref ref p joris b kalitventzeff process measurements analysis and validation proc cef 87 use comput chem eng italy 41 46 1987 ref quasi steady state dynamics for filtering and simultaneous parameter estimation over time were introduced in 1977 by stanley and mah ref name stanley mah 1977 dynamic dvr was formulated as a nonlinear optimization problem by liebman et al in 1992 ref m j liebman t f edgar l s lasdon efficient data reconciliation and estimation for dynamic processes using nonlinear programming techniques computers chem eng 16 963 986 1992 ref data reconciliation data reconciliation is a technique that targets at correcting measurement errors that are due to measurement noise i e random error s from a statistical point of view the main assumption is that no systematic errors exist in the set of measurements since they may bias the reconciliation results and reduce the robustness of the reconciliation given math n math measurements math y i math data reconciliation can mathematically be expressed as an optimization problem of the following form math begin align min x y sum i 1 n left frac y i y i sigma i right 2 text subject to f x y 0 y min le y le y max x min le x le x max end align math where math y i math is the reconciled value of the math i math th measurement math i 1 ldots n math math y i math is the measured value of the math i math th measurement math i 1 ldots n math math x j math is the math j math th unmeasured variable math j 1 ldots m math and math sigma i math is the standard deviation of the math i math th measurement math i 1 ldots n math math f x y 0 math are the math p math process equality constraints and math x min x max y min y max math are the bounds on the measured and unmeasured variables the term math left frac y i y i sigma i right 2 math is called the penalty of measurement i the objective function is the sum of the penalties which will be denoted in the following by math f y sum i 1 n left frac y i y i sigma i right 2 math in other words one wants to minimize the overall correction measured in the least squares term that is needed in order to satisfy the constraint mathematics system constraints additionally each least squares term is weighted by the standard deviation of the corresponding measurement redundancy gallery caption sensor and topological redundancy heights 150px widths 225px perrow 2 align right file sensor red jpg sensor redundancy arising from multiple sensors of the same quantity at the same time at the same place file topological red jpg topological redundancy arising from model information using the mass conservation constraint math a b c math for example one can calculate math c math when math a math and math b math are known gallery data reconciliation relies strongly on the concept of redundancy to correct the measurements as little as possible in order to satisfy the process constraints here redundancy is defined differently from redundancy information theory redundancy in information theory instead redundancy arises from combining sensor data with the model algebraic constraints sometimes more specifically called spatial redundancy ref name stanley mah 1977 analytical redundancy or topological redundancy redundancy can be due to redundancy engineering sensor redundancy where sensors are duplicated in order to have more than one measurement of the same quantity redundancy also arises when a single variable can be estimated in several independent ways from separate sets of measurements at a given time or time averaging period using the algebraic constraints redundancy is linked to the concept of observability a variable or system is observable if the models and sensor measurements can be used to uniquely determine its value system state a sensor is redundant if its removal causes no loss of observability rigorous definitions of observability calculability and redundancy along with criteria for determining it were established by stanley and mah ref name stanley mah 1981a http gregstanleyandassociates com whitepapers datarec ces 1981a observabilityredundancy pdf stanley g m and mah r s h observability and redundancy in process data estimation chem engng sci 36 259 1981 ref for these cases with set constraints such as algebraic equations and inequalities next we illustrate some special cases topological redundancy is intimately linked with the degrees of freedom physics and chemistry degrees of freedom math dof math of a mathematical system ref name vdi vdi gesellschaft energie und umwelt guidelines vdi 2048 blatt 1 uncertainties of measurements at acceptance tests for energy conversion and power plants fundamentals http www vdi de 401 0 html association of german engineers 2000 ref i e the minimum number of pieces of information i e measurements that are required in order to calculate all of the system variables for instance in the example above the flow conservation requires that math a b c math one needs to know the value of two of the 3 variables in order to calculate the third one the degrees of freedom for the model in that case is equal to 2 at least 2 measurements are needed to estimate all the variables and 3 would be needed for redundancy when speaking about topological redundancy we have to distinguish between measured and unmeasured variables in the following let us denote by math x math the unmeasured variables and math y math the measured variables then the system of the process constraints becomes math f x y 0 math which is a nonlinear system in math y math and math x math if the system math f x y 0 math is calculable with the math n math measurements given then the level of topological redundancy is defined as math red n dof math i e the number of additional measurements that are at hand on top of those measurements which are required in order to just calculate the system another way of viewing the level of redundancy is to use the definition of math dof math which is the difference between the number of variables measured and unmeasured and the number of equations then one gets math begin align red n dof n n m p p m end align math i e the redundancy is the difference between the number of equations math p math and the number of unmeasured variables math m math the level of total redundancy is the sum of sensor redundancy and topological redundancy we speak of positive redundancy if the system is calculable and the total redundancy is positive one can see that the level of topological redundancy merely depends on the number of equations the more equations the higher the redundancy and the number of unmeasured variables the more unmeasured variables the lower the redundancy and not on the number of measured variables simple counts of variables equations and measurements are inadequate for many systems breaking down for several reasons a portions of a system might have redundancy while others do not and some portions might not even be possible to calculate and b nonlinearities can lead to different conclusions at different operating points as an example consider the following system with 4 streams and 2 units example of calculable and non calculable systems gallery caption calculable and non calculable systems heights 150px widths 225px perrow 2 align right file calculable system jpg calculable system from math d math one can compute math c math and knowing math a math yields math b math file uncalculable system jpg non calculable system knowing math c math does not give information about math a math and math b math gallery we incorporate only flow conservation constraints and obtain math a b c math and math c d math it is possible that the system math f x y 0 math is not calculable even though math p m ge 0 math if we have measurements for math c math and math d math but not for math a math and math b math then the system cannot be calculated knowing math c math does not give information about math a math and math b math on the other hand if math a math and math c math are known but not math b math and math d math then the system can be calculated in 1981 observability and redundancy criteria were proven for these sorts of flow networks involving only mass and energy balance constraints ref name stanley mah 1981b http gregstanleyandassociates com whitepapers datarec ces 1981b observabilityredundancyprocessnetworks pdf stanley g m and mah r s h observability and redundancy classification in process networks chem engng sci 36 1941 1981 ref after combining all the plant inputs and outputs into an environment node loss of observability corresponds to cycles of unmeasured streams that is seen in the second case above where streams a and b are in a cycle of unmeasured streams redundancy classification follows by testing for a path of unmeasured streams since that would lead to an unmeasured cycle if the measurement was removed measurements c and d are redundant in the second case above even though part of the system is unobservable benefits redundancy can be used as a source of information to cross check and correct the measurements math y math and increase their accuracy and precision on the one hand they reconciled further the data reconciliation problem presented above also includes unmeasured variables math x math based on information redundancy estimates for these unmeasured variables can be calculated along with their accuracies in industrial processes these unmeasured variables that data reconciliation provides are referred to as soft sensor s or virtual sensors where hardware sensors are not installed data validation data validation denotes all validation and verification actions before and after the reconciliation step data filtering data filtering denotes the process of treating measured data such that the values become meaningful and lie within the range of expected values data filtering is necessary before the reconciliation process in order to increase robustness of the reconciliation step there are several ways of data filtering for example taking the average of several measured values over a well defined time period result validation result validation is the set of validation or verification actions taken after the reconciliation process and it takes into account measured and unmeasured variables as well as reconciled values result validation covers but is not limited to penalty analysis for determining the reliability of the reconciliation or bound checks to ensure that the reconciled values lie in a certain range e g the temperature has to be within some reasonable bounds gross error detection result validation may include statistical tests to validate the reliability of the reconciled values by checking whether systematic error gross errors exist in the set of measured values these tests can be for example the chi square test global test the individual test if no gross errors exist in the set of measured values then each penalty term in the objective function is a normal distribution random variable that is normally distributed with mean equal to 0 and variance equal to 1 by consequence the objective function is a random variable which follows a chi square distribution since it is the sum of the square of normally distributed random variables comparing the value of the objective function math f y math with a given percentile math p alpha math of the probability density function of a chi square distribution e g the 95th percentile for a 95 confidence gives an indication of whether a gross error exists if math f y le p 95 math then no gross errors exist with 95 probability the chi square test gives only a rough indication about the existence of gross errors and it is easy to conduct one only has to compare the value of the objective function with the critical value of the chi square distribution the individual test compares each penalty term in the objective function with the critical values of the normal distribution if the math i math th penalty term is outside the 95 confidence interval of the normal distribution then there is reason to believe that this measurement has a gross error advanced data validation and reconciliation advanced data validation and reconciliation dvr is an integrated approach of combining data reconciliation and data validation techniques which is characterized by complex models incorporating besides mass balances also thermodynamics momentum balances equilibria constraints hydrodynamics etc gross error remediation techniques to ensure meaningfulness of the reconciled values robust algorithms for solving the reconciliation problem thermodynamic models simple models include mass balances only when adding thermodynamic constraints such as first law of thermodynamics energy balances to the model its scope and the level of data redundancy redundancy increases indeed as we have seen above the level of redundancy is defined as math p m math where math p math is the number of equations including energy balances means adding equations to the system which results in a higher level of redundancy provided that enough measurements are available or equivalently not too many variables are unmeasured gross error remediation image scheme reconciliation jpg thumb 350px the workflow of an advanced data validation and reconciliation process gross errors are measurement systematic errors that may bias the reconciliation results therefore it is important to identify and eliminate these gross errors from the reconciliation process after the reconciliation statistical tests can be applied that indicate whether or not a gross error does exist somewhere in the set of measurements these techniques of gross error remediation are based on two concepts gross error elimination gross error relaxation gross error elimination determines one measurement that is biased by a systematic error and discards this measurement from the data set the determination of the measurement to be discarded is based on different kinds of penalty terms that express how much the measured values deviate from the reconciled values once the gross errors are detected they are discarded from the measurements and the reconciliation can be done without these faulty measurements that spoil the reconciliation process if needed the elimination is repeated until no gross error exists in the set of measurements gross error relaxation targets at relaxing the estimate for the uncertainty of suspicious measurements so that the reconciled value is in the 95 confidence interval relaxation typically finds application when it is not possible to determine which measurement around one unit is responsible for the gross error equivalence of gross errors then measurement uncertainties of the measurements involved are increased it is important to note that the remediation of gross errors reduces the quality of the reconciliation either the redundancy decreases elimination or the uncertainty of the measured data increases relaxation therefore it can only be applied when the initial level of redundancy is high enough to ensure that the data reconciliation can still be done see section 2 ref name vdi workflow advanced dvr solutions offer an integration of the techniques mentioned above data acquisition from data historian data base or manual inputs data validation and filtering of raw measurements data reconciliation of filtered measurements result verification range check gross error remediation and go back to step 3 result storage raw measurements together with reconciled values the result of an advanced dvr procedure is a coherent set of validated and reconciled process data applications dvr finds application mainly in industry sectors where either measurements are not accurate or even non existing like for example in the upstream fossil fuel industry upstream sector where flow measurement flow meters are difficult or expensive to position see ref p delava e mar\xc3\xa9chal b vrielynck b kalitventzeff 1999 modelling of a crude oil distillation unit in term of data reconciliation with astm or tbp curves as direct input application crude oil preheating train proceedings of escape 9 conference budapest may 31 june 2 1999 supplementary volume p 17 20 ref or where accurate data is of high importance for example for security reasons in nuclear power plants see ref m langenstein j jansky b laipple 2004 finding megawatts in nuclear power plants with process data validation proceedings of icone12 arlington usa april 25 29 2004 ref another field of application is performance test assessment performance and process monitoring see ref th amand g heyen b kalitventzeff plant monitoring and fault detection synergy between data reconciliation and principal component analysis comp and chem eng 25 p 501 507 2001 ref in oil refining or in the chemical industry as dvr enables to calculate estimates even for unmeasured variables in a reliable way the german engineering society vdi gesellschaft energie und umwelt has accepted the technology of dvr as a means to replace expensive sensors in the nuclear power industry see vdi norm 2048 ref name vdi see also process simulation pinch analysis industrial processes chemical engineering references see http en wikipedia org wiki wikipedia footnotes reflist alexander dave tannar dave wasik larry mill information system uses dynamic data reconciliation for accurate energy accounting tappi fall conference 2007 http www tappi org downloads conference papers 2007 07epe 07epe87 aspx rankin j wasik l dynamic data reconciliation of batch pulping processes for on line prediction paptac spring conference 2009 s narasimhan c jordache data reconciliation and gross error detection an intelligent use of process data golf publishing company houston 2000 v veverka f madron material and energy balancing in the process industries elsevier science bv amsterdam 1997 j romagnoli m c sanchez data processing and reconciliation for chemical process operations academic press 2000 defaultsort data validation and reconciliation category data management'
b'multiple issues cleanup date january 2008 unreferenced date december 2009 data independence is the type of data transparency that matters for a centralised database management system dbms it refers to the immunity of user application software applications to changes made in the definition and organization of data physical data independence deals with hiding the details of the storage structure from user applications the application should not be involved with these issues since there is no difference in the operation carried out against the data the data independence and operation independence together gives the feature of data abstraction there are two levels of data independence first level of data independence the logical structure of the data is known as the schema definition in general if a user application operates on a subset of the attribute computing attributes of a relation database relation it should not be affected later when new attributes are added to the same relation logical data independence indicates that the conceptual schema can be changed without affecting the existing schemas second level of data independence the physical structure of the data is referred to as physical data description physical data independence deals with hiding the details of the storage structure from user applications the application should not be involved with these issues since conceptually there is no difference in the operations carried out against the data there are three types of data independence logical data independence the ability to change the logical conceptual schema without changing the external schema user view is called logical data independence for example the addition or removal of new entities attributes or relationships to the conceptual schema should be possible without having to change existing external schemas or having to rewrite existing application programs physical data independence the ability to change the physical schema without changing the logical schema is called physical data independence for example a change to the internal schema such as using different file organization or storage structures storage devices or indexing strategy should be possible without having to change the conceptual or external schemas view level data independence always independent no effect because there doesn t exist any other level above view level data independence data independence can be explained as follows each higher level of the data architecture is immune to changes of the next lower level of the architecture the logical scheme stays unchanged even though the storage space or type of some data is changed for reasons of optimization or reorganization in this external schema does not change in this internal schema changes may be required due to some physical schema were reorganized here physical data independence is present in most databases and file environment in which hardware storage of encoding exact location of data on disk merging of records so on this are hidden from user one of the biggest advantage of databases is data independence it means we can change the conceptual schema at one level without affecting the data at another level it also means we can change the structure of a database without affecting the data required by users and programs this feature was not available in the file oriented approach data independence types the ability to modify schema definition in one level without affecting schema definition in the next higher level is called data independence there are two levels of data independence they are physical data independence and logical data independence physical data independence is the ability to modify the physical schema without causing application programs to be rewritten modifications at the physical level are occasionally necessary to improve performance it means we change the physical storage level without affecting the conceptual or external view of the data the new changes are absorbed by mapping techniques logical data independence is the ability to modify the logical schema without causing application program to be rewritten modifications at the logical level are necessary whenever the logical structure of the database is altered for example when money market accounts are added to banking system logical data independence means if we add some new columns or remove some columns from table then the user view and programs should not change for example consider two users a b both are selecting the fields employeenumber and employeename if user b adds a new column e g salary to his table it will not effect the external view for user a though the internal schema of the database has been changed for both users a b logical data independence is more difficult to achieve than physical data independence since application programs are heavily dependent on the logical structure of the data that they access physical data independence means we change the physical storage level without affecting the conceptual or external view of the data mapping techniques absorbs the new changes see also network transparency replication transparency codd s 12 rules ansi sparc architecture defaultsort data independence category data management'
b'more footnotes date february 2013 data migration is the process of data transfer transferring data between computer data storage computer storage types or file format s it is a key consideration for any system implementation upgrade or consolidation data migration is usually performed programmatically to achieve an automated migration freeing up human resources from tedious tasks data migration occurs for a variety of reasons including server or storage equipment replacements maintenance or upgrades software modernization application migration website consolidation and data center relocation ref janssen c data migration http www techopedia com definition 1180 data migration retrieved 12 august 2013 ref to achieve an effective data migration procedure data on the old system is data mapping mapped to the new system utilising a design for data extraction and data loading the design relates old data format s to the new system s formats and requirements programmatic data migration may involve many phases but it minimally includes data extraction where data is read from the old system and data loading where data is written to the new system after loading into the new system results are subjected to data verification to determine whether data was accurately translated is complete and supports processes in the new system during verification there may be a need for a parallel run of both systems to identify areas of disparity and forestall erroneous data loss automated and manual data cleaning is commonly performed in migration to improve data quality eliminate data duplication redundant or obsolete information and match the requirements of the new system data migration phases design extract transform load extraction data cleansing cleansing load verification for applications of moderate to high complexity are commonly repeated several times before the new system is deployed categories data is stored on various media in computer file files or databases and is generated and consumed by software applications which in turn support business processes the need to transfer and convert data can be driven by multiple business requirements and the approach taken to the migration depends on those requirements four major migration categories are proposed on this basis storage migration a business may choose to rationalize the physical media to take advantage of more efficient storage technologies this will result in having to move physical blocks of data from one tape or disk to another often using storage virtualization virtualization techniques the data format and content itself will not usually be changed in the process and can normally be achieved with minimal or no impact to the layers above database migration main article schema migration similarly it may be necessary to move from one database vendor to another or to upgrade the version of database software being used the latter case is less likely to require a physical data migration but this can happen with major upgrades in these cases a physical transformation process may be required since the underlying data format can change significantly this may or may not affect behavior in the applications layer depending largely on whether the data manipulation language or protocol has changed but modern applications are written to be agnostic to the database technology so that a change from sybase mysql ibm db2 db2 or microsoft sql server sql server to oracle database oracle should only require a testing cycle to be confident that both functional and non functional performance has not been adversely affected application migration changing application vendor for instance a new customer relationship management crm or enterprise resource planning erp platform will inevitably involve substantial transformation as almost every application or suite operates on its own specific data model and also interacts with other applications and systems within the enterprise application integration environment furthermore to allow the application to be sold to the widest possible market commercial off the shelf packages are generally configured for each customer using metadata application programming interfaces apis may be supplied by vendors to protect the data integrity integrity of the data they have to handle business process migration business processes operate through a combination of human and application systems actions often orchestrated by business process management tools when these change they can require the movement of data from one store database or application to another to reflect the changes to the organization and information about customers products and operations examples of such migration drivers are mergers and acquisitions business optimization and reorganization to attack new markets or respond to competitive threat the first two categories of migration are usually routine operational activities that the it department takes care of without the involvement of the rest of the business the last two categories directly affect the operational users of processes and applications are necessarily complex and delivering them without significant business downtime can be challenging a highly adaptive approach concurrent synchronization a business oriented audit capability and clear visibility of the migration for stakeholders are likely to be key requirements in such migrations project versus process there is a difference between data migration and data integration activities data migration is a project by means of which data will be moved or copied from one environment to another and removed or decommissioned in the source during the migration which can take place over months or even years data can flow in multiple directions and there may be multiple migrations taking place simultaneously the extract transform load actions will be necessary although the means of achieving these may not be those traditionally associated with the extract transform load etl acronym data integration by contrast is a permanent part of the it architecture and is responsible for the way data flows between the various applications and data stores and is a process rather than a project activity standard etl technologies designed to supply data from operational systems to data warehouses would fit within the latter category migration as a form of digital preservation migration which focuses on the digital object itself is the act of transferring or rewriting data from an out of date medium to a current medium and has for many years been considered the only viable approach to long term preservation of digital objects ref cite journal author1 van der hoeven jeffrey author2 bram lohman author3 remco verdegem title emulation for digital preservation in practice the results journal the international journal of digital curation volume 2 issue 2 year 2007 pages 123 132 url http www ijdc net index php ijdc article view 50 doi 10 2218 ijdc v2i2 35 ref reproducing brittle newspapers onto microform microfilm is an example of such migration disadvantages migration addresses the possible obsolescence of the data carrier but does not address the fact that certain technologies which run the data may be abandoned altogether leaving migration useless time consuming migration is a continual process which must be repeated every time a medium reaches obsolescence for all data objects stored on a certain media costly an institution must purchase additional data storage media at each migration ref cite journal author muira gregory title pushing the boundaries of traditional heritage policy maintaining long term access to multimedia content journal ifla journal volume 33 year 2007 pages 323 326 url http www ifla org files assets hq publications ifla journal ifla journal 4 2007 pdf ref as a result of the disadvantages listed above technology professionals have begun to develop alternatives to migration such as emulator emulation see also data conversion data transformation extract transform load system migration references reflist external links dmoz computers software databases data warehousing extraction and transformation data migration authority control category data management'
b'pov commitment ordering date november 2011 a distributed transaction is a database transaction in which two or more network hosts are involved usually hosts provide transactional resources while the transaction manager is responsible for creating and managing a global transaction that encompasses all operations against such resources distributed transactions as any other database transaction transactions must have all four acid acid atomicity consistency isolation durability properties where atomicity guarantees all or nothing outcomes for the unit of work operations bundle open group a vendor consortium proposed the x open xa x open distributed transaction processing dtp model x open xa which became a de facto standard for behavior of transaction model components database are common transactional resources and often transactions span a couple of such databases in this case a distributed transaction can be seen as a database transaction that must be synchronization synchronized or provide acid properties among multiple participating database s which are distributed computing distributed among different physical locations the isolation computer science isolation property the i of acid poses a special challenge for multi database transactions since the global serializability property could be violated even if each database provides it see also global serializability in practice most commercial database systems use two phase locking strong strict two phase locking ss2pl for concurrency control which ensures global serializability if all the participating databases employ it see also commitment ordering for multidatabases a common algorithm for ensuring correctness computer science correct completion of a distributed transaction is the two phase commit 2pc this algorithm is usually applied for updates able to commit data management commit in a short period of time ranging from couple of milliseconds to couple of minutes there are also long lived distributed transactions for example a transaction to book a trip which consists of booking a flight a rental car and a hotel since booking the flight might take up to a day to get a confirmation two phase commit is not applicable here it will lock the resources for this long in this case more sophisticated techniques that involve multiple undo levels are used the way you can undo the hotel booking by calling a desk and cancelling the reservation a system can be designed to undo certain operations unless they are irreversibly finished in practice long lived distributed transactions are implemented in systems based on web services usually these transactions utilize principles of compensating transaction s optimism and isolation without locking x open standard does not cover long lived dtp several modern technologies including enterprise java beans ejbs and microsoft transaction server mts fully support distributed transaction standards see also java transaction api java transaction api jta enduro x enduro x open source x open xa and xatmi implementation references cite web title web services transactions work web services transactions url http xml sys con com read 43755 htm accessdate may 2 2005 cite web title nuts and bolts of transaction processing work article about transaction management url http www subbu org articles transactions nutsandboltsoftp html accessdate may 3 2005 cite web title a detailed comparison of enterprise javabeans ejb the microsoft transaction server mts models url http gsraj tripod com misc ejbmtscomp html further reading gerhard weikum gottfried vossen transactional information systems theory algorithms and the practice of concurrency control and recovery morgan kaufmann 2002 isbn 1 55860 508 8 defaultsort distributed transaction category data management category transaction processing'
b'a content inventory is the process and the result of cataloging the entire contents of a website an allied practice a content audit is the process of evaluating that content ref name halverson cite web url http www peachpit com articles article aspx p 1388961 title content strategy for the web why you must do a content audit first kristina last halvorson date august 2009 accessdate 6 may 2010 ref ref name baldwin cite web url http nform ca blog 2010 01 doing a content audit or inven title doing a content audit or inventory first scott last baldwin date january 2010 accessdate 29 april 2010 ref ref name marsh cite web url http www hilarymarsh com 2012 03 12 how to do a content audit title how to do a content audit first hilary last marsh date march 2012 accessdate 2 may 2013 ref a content inventory and a content audit are closely related concepts and they are often conducted in tandem description a content inventory typically includes all information assets on a website such as web page s html meta element s e g keywords description page title images audio and video files and document files e g pdf doc ppt ref name spencer2006 cite web url http maadmob net donna blog 2006 taking a content inventory title taking a content inventory first donna last spencer date january 2006 accessdate 27 april 2010 ref ref name doss2007 cite web url http www fatpurple com 2010 02 26 content inventory title content inventory sometimes referred to as web content inventory or web audit first glen last doss date january 2007 accessdate 27 april 2010 ref ref name jones2009 cite web url http www uxmatters com mt archives 2009 08 content analysis a practical approach php title content analysis a practical approach first colleen last jones date august 2009 accessdate 27 april 2010 ref ref name leise2007 cite web url http boxesandarrows com view content analysis title content analysis heuristics first fred last leise date march 2007 accessdate 27 april 2010 ref ref name baldwin2010 cite web url http nform ca blog 2010 01 doing a content audit or inven title doing a content audit or inventory first scott last baldwin date january 2010 accessdate 27 april 2010 ref ref name krozser cite web url http www alttags org content management the content inventory roadmap to a succesful cms implementation title the content inventory roadmap to a successful cms implementation first kassia last krozser date april 2005 accessdate 27 april 2010 ref a content inventory is a quantitative research quantitative analysis of a website it simply logs what is on a website the content inventory will answer the question what is there and can be the start of a website review ref name goss interactive cite web url http www gossinteractive com community whitepapers conducting a website review and implementing results for increased customer engagement and conversions title conducting a website review and implementing results for increased customer engagement and conversions first goss interactive date october 2011 accessdate 8 october 2011 ref a related and sometimes confused term is a content audit a qualitative research qualitative analysis of information assets on a website it is the assessment of that content and its place in relationship to surrounding web pages and information assets the content audit will answer the question is it any good ref name baldwin ref name marsh over the years techniques for creating and managing a content inventory have been developed and refined in the field of website content management ref name halverson ref name veen2002 cite web url http www adaptivepath com ideas essays archives 000040 php title doing a content inventory or a mind numbingly detailed odyssey through your web site first jeffrey last veen date june 2002 accessdate 27 april 2010 ref ref name bruns cite web url http donbruns net index php how to automatically index a content inventory title automatically index a content inventory with getuxindex first don last bruns date march 2010 accessdate 6 may 2010 ref a spreadsheet application e g microsoft excel or libreoffice calc is the preferred tool for keeping a content inventory the data can be easily configured and manipulated typical categories in a content inventory include the following link the url for the page format for example html pdf microsoft word doc microsoft powerpoint ppt meta page title page title as it appears in the meta title tag meta keywords keywords as they appear in the meta tag the keywords attribute meta name keywords tag element meta description text as it appears in the meta tag the description attribute meta name description tag element content owner person responsible for maintaining page content date page last updated date of last page update audit comments or notes audit findings and notes there are other descriptors that may need to be captured on the inventory sheet content management experts advise capturing information that might be useful for both short and long term purposes other information could include the overall topic or area to which the page belongs a short description of the information on the page when the page was created date of last revision and when the next page review is due pages this page links to pages that link to this page page status keep delete revise in revision process planned being written being edited in review ready for posting or posted rank of page on the website is it a top 50 page a bottom 50 page initial efforts might be more focused on those pages that visitors use the most and least other tabs in the inventory workbook can be created to track related information such as meta keywords new web pages to develop website tools and resources or content inventories for sub areas of the main website creating a single shared location for information related to a website can be helpful for all website content managers writers editors and publishers populating the spreadsheet is a painstaking task but some up front work can be automated with software and other tools and resources can assist the audit work value a content inventory and a content audit are performed to understand what is on a website and why it is there the inventory sheet once completed and revised as the site is updated with new content and information assets can also become a resource for help in maintaining website governance for an existing website the information cataloged in a content inventory and content audit will be a resource to help manage all of the information assets on the website ref name usability cite web url http www usability gov methods design site inventory html title content inventory date 26 may 2009 publisher u s department of health human services accessdate 4 may 2010 ref the information gathered in the inventory can also be used to plan a website re design or site migration to a web content management system ref name krozser when planning a new website a content inventory can be a useful project management tool as a guide to map information architecture and to track new pages page revision dates content owners and so on see also content audit web content management system design methods information architecture web design website governance references reflist further reading in his article http www boxesandarrows com view a map based approach a map based approach to a content inventory patrick walsh describes how to use microsoft access and microsoft excel to link a data attribute with a structural attribute to create a tool that can be used throughout the lifetime of a website in the article http www louisrosenfeld com home bloug archive 000448 html the rolling content inventory author louis rosenfeld argues that ongoing partial content inventories are more cost effective and realistic to implement colleen jones writes from a ux design perspective in http www uxmatters com mt archives 2009 08 content analysis a practical approach php content analysis a practical approach http xmlpress net content strategy audits and inventories content audits and inventories a handbook is a practical guide to conducting content inventories and audits external links http home snafu de tilman xenulink html xenu s link sleuth http siteorbiter com siteorbiter http www webconfs com similar page checker php similar page checker http www cryer co uk resources link checkers htm link checker tools http www kevinpnichols com downloads kpn content audit xls kevin p nichols content inventory and audit template defaultsort content inventory category data management category website management category content management systems'
b'unreferenced date april 2010 in the field of software sql programming tools provide platforms for database administrator s dbas and application software application developers to perform daily tasks efficiently and accurately database administrators and application developers often face constantly changing environments which they rarely completely control many changes result from new development projects or from modifications to existing code which when deployed to production do not always produce the expected result for organizations to better manage development projects and the team s that develop code suppliers of sql programming tools normally provide more than facility to the database administrator or application developer to aid in database management and in quality software deployment code deployment practices features sql programming tools may include the following features sql editing sql editors allow users to edit and execute sql statements they may support the following features cut copy paste undo redo find and replace bookmarks block indent print save file uppercase lowercase keyword highlighting auto completion access to frequently used files output of query result editing query results committing and rolling back transactions inside cut paper object browsing tools may display information about database object s relevant to developers or to database administrators users may view object descriptions view object data definition language definition s ddl create database objects enable and disable database trigger trigger s and database constraints constraint s recompile valid or invalid objects query or edit table database table s and view database view s some tools also provide features to display dependencies among objects and allow users to expand these dependent objects recursively for example packages may reference views views generally reference tables super subtypes and so on session browsing database administrators and application developers can use session browsing tools to view the current activities of each user in the database they can check the resource usage of individual users statistics information locked objects and the current running sql of each individual session user security management dbas can create edit delete disable or enable user accounts in the database using security management tools dbas can also assign database role role s system privilege computing privilege s object privileges and database storage storage quotas to users debugging some tools offer features for the debugging of stored procedure s program animation step in step over step out run until exception breakpoint s view set variables view call stack and so on users can debug any program unit without making any modification to it including triggers and object type s performance monitoring monitoring tools may show the database resources usage summary service time summary recent activities top sessions session history or top sql in easy to read graphs database administrators can easily monitor the health of various components in the monitoring instance application developers may also make use of such tools to diagnose and correct application performance problems as well as improve sql server performance test data test data generation tools can populate the database by realistic test data for server or client side testing purposes also this kind of software can upload sample blob files to database see also comparison of database tools defaultsort sql programming tool category data management category relational database management systems'
b'redirect oner filmmaking technique long take machine learning bar association rule learning is a rule based machine learning method for discovering interesting relations between variables in large databases it is intended to identify strong rules discovered in databases using some measures of interestingness ref name piatetsky piatetsky shapiro gregory 1991 discovery analysis and presentation of strong rules in piatetsky shapiro gregory and frawley william j eds knowledge discovery in databases aaai mit press cambridge ma ref based on the concept of strong rules rakesh agrawal computer scientist rakesh agrawal tomasz imieli\xc5\x84ski and arun swami ref name mining cite book last1 agrawal first1 r last2 imieli\xc5\x84ski first2 t last3 swami first3 a doi 10 1145 170035 170072 chapter mining association rules between sets of items in large databases title proceedings of the 1993 acm sigmod international conference on management of data sigmod 93 pages 207 year 1993 isbn 0897915925 pmid pmc ref introduced association rules for discovering regularities between products in large scale transaction data recorded by point of sale pos systems in supermarkets for example the rule math mathrm onions potatoes rightarrow mathrm burger math found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together they are likely to also buy hamburger meat such information can be used as the basis for decisions about marketing activities such as e g promotional pricing or product placement s in addition to the above example from market basket analysis association rules are employed today in many application areas including web usage mining intrusion detection continuous production and bioinformatics in contrast with sequence mining association rule learning typically does not consider the order of items either within a transaction or across transactions definition class wikitable style float right margin left 1em example database with 5 transactions and 5 items transaction id milk bread butter beer diapers 1 1 1 0 0 0 2 0 0 1 0 0 3 0 0 0 1 1 4 1 1 1 0 0 5 0 1 0 0 0 following the original definition by agrawal et al ref name mining the problem of association rule mining is defined as let math i i 1 i 2 ldots i n math be a set of math n math binary attributes called items let math d t 1 t 2 ldots t m math be a set of transactions called the database each transaction in math d math has a unique transaction id and contains a subset of the items in math i math a rule is defined as an implication of the form math x rightarrow y math where math x y subseteq i math in agrawal et al ref name mining a rule is defined only between a set and a single item math x rightarrow i j math for math i j in i math every rule is composed by two different sets of items also known as itemsets math x math and math y math where math x math is called antecedent or left hand side lhs and math y math consequent or right hand side rhs to illustrate the concepts we use a small example from the supermarket domain the set of items is math i mathrm milk bread butter beer diapers math and in the table is shown a small database containing the items where in each entry the value 1 means the presence of the item in the corresponding transaction and the value 0 represents the absence of an item in that transaction an example rule for the supermarket could be math mathrm butter bread rightarrow mathrm milk math meaning that if butter and bread are bought customers also buy milk note this example is extremely small in practical applications a rule needs a support of several hundred transactions before it can be considered statistically significant citation needed date february 2015 and datasets often contain thousands or millions of transactions useful concepts in order to select interesting rules from the set of all possible rules constraints on various measures of significance and interest are used the best known constraints are minimum thresholds on support and confidence let math x math be an itemset math x rightarrow y math an association rule and math t math a set of transactions of a given database support support is an indication of how frequently the itemset appears in the database the support of math x math with respect to math t math is defined as the proportion of transactions math t math in the database which contains itemset math x math math mathrm supp x frac t in t x subseteq t t math in the example database the itemset math x mathrm beer diapers math has a support of math 1 5 0 2 math since it occurs in 20 of all transactions 1 out of 5 transactions the argument of math mathrm supp math is a set of preconditions and thus becomes more restrictive as it grows instead of more inclusive ref name 0 cite journal last hahsler first michael date 2005 title introduction to arules a computational environment for mining association rules and frequent item sets url https mran revolutionanalytics com web packages arules vignettes arules pdf journal journal of statistical software doi pmid access date ref confidence confidence is an indication of how often the rule has been found to be true the confidence value of a rule math x rightarrow y math with respect to a set of transactions math t math is the proportion of the transactions that contains math x math which also contains math y math confidence is defined as math mathrm conf x rightarrow y mathrm supp x cup y mathrm supp x math for example the rule math mathrm butter bread rightarrow mathrm milk math has a confidence of math 0 2 0 2 1 0 math in the database which means that for 100 of the transactions containing butter and bread the rule is correct 100 of the times a customer buys butter and bread milk is bought as well note that math mathrm supp x cup y math means the support of the union of the items in x and y this is somewhat confusing since we normally think in terms of probabilities of event probability theory events and not sets of items we can rewrite math mathrm supp x cup y math as the joint probability math p e x cup e y math where math e x math and math e y math are the events that a transaction contains itemset math x math or math y math respectively ref name michael hahsler net michael hahsler 2015 a probabilistic comparison of commonly used interest measures for association rules http michael hahsler net research association rules measures html ref thus confidence can be interpreted as an estimate of the conditional probability math p e y e x math the probability of finding the rhs of the rule in transactions under the condition that these transactions also contain the lhs ref name 0 ref name hipp cite journal last1 hipp first1 j last2 g\xc3\xbcntzer first2 u last3 nakhaeizadeh first3 g title algorithms for association rule mining a general survey and comparison doi 10 1145 360402 360421 journal acm sigkdd explorations newsletter volume 2 pages 58 year 2000 pmid pmc ref lift the lift data mining lift of a rule is defined as math mathrm lift x rightarrow y frac mathrm supp x cup y mathrm supp x times mathrm supp y math or the ratio of the observed support to that expected if x and y were independence probability theory independent citation needed reason i couldn t find this in witten data mining practical machine learning tools and techniques date may 2016 for example the rule math mathrm milk bread rightarrow mathrm butter math has a lift of math frac 0 2 0 4 times 0 4 1 25 math if the rule had a lift of 1 it would imply that the probability of occurrence of the antecedent and that of the consequent are independent of each other when two events are independent of each other no rule can be drawn involving those two events if the lift is 1 that lets us know the degree to which those two occurrences are dependent on one another and makes those rules potentially useful for predicting the consequent in future data sets the value of lift is that it considers both the confidence of the rule and the overall data set ref name 0 conviction the conviction of a rule is defined as math mathrm conv x rightarrow y frac 1 mathrm supp y 1 mathrm conf x rightarrow y math for example the rule math mathrm milk bread rightarrow mathrm butter math has a conviction of math frac 1 0 4 1 0 5 1 2 math and can be interpreted as the ratio of the expected frequency that x occurs without y that is to say the frequency that the rule makes an incorrect prediction if x and y were independent divided by the observed frequency of incorrect predictions in this example the conviction value of 1 2 shows that the rule math mathrm milk bread rightarrow mathrm butter math would be incorrect 20 more often 1 2 times as often if the association between x and y was purely random chance process file frequentitems png thumb frequent itemset lattice where the color of the box indicates how many transactions contain the combination of items note that lower levels of the lattice can contain at most the minimum number of their parents items e g ac can have only at most math min a c math items this is called the downward closure property ref name mining association rules are usually required to satisfy a user specified minimum support and a user specified minimum confidence at the same time association rule generation is usually split up into two separate steps a minimum support threshold is applied to find all frequent itemsets in a database a minimum confidence constraint is applied to these frequent itemsets in order to form rules while the second step is straightforward the first step needs more attention finding all frequent itemsets in a database is difficult since it involves searching all possible itemsets item combinations the set of possible itemsets is the power set over math i math and has size math 2 n 1 math excluding the empty set which is not a valid itemset although the size of the power set grows exponentially in the number of items math n math in math i math efficient search is possible using the downward closure property of support ref name mining ref cite book last1 tan first1 pang ning last2 michael first2 steinbach last3 kumar first3 vipin title introduction to data mining publisher addison wesley year 2005 isbn 0 321 32136 7 chapter chapter 6 association analysis basic concepts and algorithms chapterurl http www users cs umn edu kumar dmbook ch6 pdf ref also called anti monotonicity ref name pei pei jian han jiawei and lakshmanan laks v s mining frequent itemsets with convertible constraints in proceedings of the 17th international conference on data engineering april 2 6 2001 heidelberg germany 2001 pages 433 442 ref which guarantees that for a frequent itemset all its subsets are also frequent and thus no infrequent itemset can be a subset of a frequent itemset exploiting this property efficient algorithms e g apriori ref name apriori agrawal rakesh and srikant ramakrishnan http rakesh agrawal family com papers vldb94apriori pdf fast algorithms for mining association rules in large databases in bocca jorge b jarke matthias and zaniolo carlo editors proceedings of the 20th international conference on very large data bases vldb santiago chile september 1994 pages 487 499 ref and eclat ref name eclat cite journal last1 zaki first1 m j title scalable algorithms for association mining doi 10 1109 69 846291 journal ieee transactions on knowledge and data engineering volume 12 issue 3 pages 372 390 year 2000 pmid pmc ref can find all frequent itemsets history the concept of association rules was popularised particularly due to the 1993 article of agrawal et al ref name mining which has acquired more than 18 000 citations according to google scholar as of august 2015 and is thus one of the most cited papers in the data mining field however it is possible that what is now called association rules is similar to what appears in the 1966 paper ref name guha oldest h\xc3\xa1jek petr havel ivan chytil metod\xc4\x9bj the guha method of automatic hypotheses determination computing 1 1966 293 308 ref on guha a general data mining method developed by petr h\xc3\xa1jek et al ref name pospaper h\xc3\xa1jek petr feglar tomas rauch jan and coufal david the guha method data preprocessing and mining database support for data mining applications springer 2004 isbn 978 3 540 22479 2 ref an early circa 1989 use of minimum support and confidence to find all association rules is the feature based modeling framework which found all rules with math mathrm supp x math and math mathrm conf x rightarrow y math greater than user defined constraints ref cite journal last1 webb first1 geoffrey title a machine learning approach to student modelling journal proceedings of the third australian joint conference on artificial intelligence ai 89 date 1989 pages 195 205 ref alternative measures of interestingness would be nice to explain each measure in addition to confidence other measures of interestingness for rules have been proposed some popular measures are all confidence ref name allconfidence omiecinski edward r alternative interest measures for mining associations in databases ieee transactions on knowledge and data engineering 15 1 57 69 jan feb 2003 ref collective strength ref name collectivestrength aggarwal charu c and yu philip s a new framework for itemset generation in pods 98 symposium on principles of database systems seattle wa usa 1998 pages 18 24 ref conviction ref name brin dynamic itemset1 brin sergey motwani rajeev ullman jeffrey d and tsur shalom dynamic itemset counting and implication rules for market basket data in sigmod 1997 proceedings of the acm sigmod international conference on management of data sigmod 1997 tucson arizona usa may 1997 pp 255 264 ref leverage ref name leverage piatetsky shapiro gregory discovery analysis and presentation of strong rules knowledge discovery in databases 1991 pp 229 248 ref lift originally called interest ref name brin dynamic itemset2 brin sergey motwani rajeev ullman jeffrey d and tsur shalom dynamic itemset counting and implication rules for market basket data in sigmod 1997 proceedings of the acm sigmod international conference on management of data sigmod 1997 tucson arizona usa may 1997 pp 265 276 ref several more measures are presented and compared by tan et al ref name measurescomp tan pang ning kumar vipin and srivastava jaideep selecting the right objective measure for association analysis information systems 29 4 293 313 2004 ref and by hahsler ref name michael hahsler net looking for techniques that can model what the user has known and using these models as interestingness measures is currently an active research trend under the name of subjective interestingness statistically sound associations one limitation of the standard approach to discovering associations is that by searching massive numbers of possible associations to look for collections of items that appear to be associated there is a large risk of finding many spurious associations these are collections of items that co occur with unexpected frequency in the data but only do so by chance for example suppose we are considering a collection of 10 000 items and looking for rules containing two items in the left hand side and 1 item in the right hand side there are approximately 1 000 000 000 000 such rules if we apply a statistical test for independence with a significance level of 0 05 it means there is only a 5 chance of accepting a rule if there is no association if we assume there are no associations we should nonetheless expect to find 50 000 000 000 rules statistically sound association discovery ref webb geoffrey i 2007 discovering significant patterns machine learning 68 1 netherlands springer pp 1 33 http link springer com article 10 1007 2fs10994 007 5006 x online access ref ref gionis aristides heikki mannila mannila heikki mielik\xc3\xa4inen taneli and tsaparas panayiotis assessing data mining results via swap randomization acm transactions on knowledge discovery from data tkdd volume 1 issue 3 december 2007 article no 14 ref controls this risk in most cases reducing the risk of finding any spurious associations to a user specified significance levels algorithms many algorithms for generating association rules were presented over time some well known algorithms are apriori algorithm apriori eclat and fp growth but they only do half the job since they are algorithms for mining frequent itemsets another step needs to be done after to generate rules from frequent itemsets found in a database apriori algorithm main article apriori algorithm apriori ref name apriori uses a breadth first search strategy to count the support of itemsets and uses a candidate generation function which exploits the downward closure property of support eclat algorithm eclat ref name eclat alt eclat stands for equivalence class transformation is a depth first search algorithm using set intersection it is a naturally elegant algorithm suitable for both sequential as well as parallel execution with locality enhancing properties it was first introduced by zaki parthasarathy li and ogihara in a series of papers written in 1997 mohammed javeed zaki srinivasan parthasarathy m ogihara wei li new algorithms for fast discovery of association rules kdd 1997 mohammed javeed zaki srinivasan parthasarathy mitsunori ogihara wei li parallel algorithms for discovery of association rules data min knowl discov 1 4 343 373 1997 fp growth algorithm fp stands for frequent pattern ref cite journal last1 han title mining frequent patterns without candidate generation journal proceedings of the 2000 acm sigmod international conference on management of data date 2000 volume sigmod 00 pages 1 12 doi 10 1145 342009 335372 ref in the first pass the algorithm counts occurrence of items attribute value pairs in the dataset and stores them to header table in the second pass it builds the fp tree structure by inserting instances items in each instance have to be sorted by descending order of their frequency in the dataset so that the tree can be processed quickly items in each instance that do not meet minimum coverage threshold are discarded if many instances share most frequent items fp tree provides high compression close to tree root recursive processing of this compressed version of main dataset grows large item sets directly instead of generating candidate items and testing them against the entire database growth starts from the bottom of the header table having longest branches by finding all instances matching given condition new tree is created with counts projected from the original tree corresponding to the set of instances that are conditional on the attribute with each node getting sum of its children counts recursive growth ends when no individual items conditional on the attribute meet minimum support threshold and processing continues on the remaining header items of the original fp tree once the recursive process has completed all large item sets with minimum coverage have been found and association rule creation begins ref witten frank hall data mining practical machine learning tools and techniques 3rd edition ref others aprioridp aprioridp ref name dharmesh2013 utilizes dynamic programming in frequent itemset mining the working principle is to eliminate the candidate generation like fp tree but it stores support count in specialized data structure instead of tree context based association rule mining algorithm main article context based association rules cbpnarm is an algorithm developed in 2013 to mine association rules on the basis of context it uses context variable on the basis of which the support of an itemset is changed on the basis of which the rules are finally populated to the rule set node set based algorithms fin ref name deng2014 prepost ref name deng2012 and ppv ref name deng2010 are three algorithms based on node sets they use nodes in a coding fp tree to represent itemsets and employ a depth first search strategy to discovery frequent itemsets using intersection of node sets guha procedure assoc guha is a general method for exploratory data analysis that has theoretical foundations in observational calculi ref name observationalcalculi rauch jan logical calculi for knowledge discovery in databases in proceedings of the first european symposium on principles of data mining and knowledge discovery springer 1997 pp 47 57 ref the assoc procedure ref cite book last h\xc3\xa1jek first petr author2 havr\xc3\xa1nek tom\xc3\xa1\xc5\xa1 title mechanizing hypothesis formation mathematical foundations for a general theory publisher springer verlag year 1978 isbn 3 540 08738 9 url http www cs cas cz hajek guhabook ref is a guha method which mines for generalized association rules using fast bitstring s operations the association rules mined by this method are more general than those output by apriori for example items can be connected both with conjunction and disjunctions and the relation between antecedent and consequent of the rule is not restricted to setting minimum support and confidence as in apriori an arbitrary combination of supported interest measures can be used opus search opus is an efficient algorithm for rule discovery that in contrast to most alternatives does not require either monotone or anti monotone constraints such as minimum support ref name opus webb geoffrey i 1995 opus an efficient admissible algorithm for unordered search journal of artificial intelligence research 3 menlo park ca aaai press pp 431 465 http www cs washington edu research jair abstracts webb95a html online access dead link date october 2016 bot internetarchivebot fix attempted yes ref initially used to find rules for a fixed consequent ref name opus ref name bayardo cite journal doi 10 1023 a 1009895914772 last1 bayardo first1 roberto j jr last2 agrawal first2 rakesh last3 gunopulos first3 dimitrios year 2000 title constraint based rule mining in large dense databases journal data mining and knowledge discovery volume 4 issue 2 pages 217 240 ref it has subsequently been extended to find rules with any item as a consequent ref name webb webb geoffrey i 2000 efficient search for association rules in ramakrishnan raghu and stolfo sal eds proceedings of the sixth acm sigkdd international conference on knowledge discovery and data mining kdd 2000 boston ma new york ny the association for computing machinery pp 99 107 http www csse monash edu webb files webb00b pdf online access ref opus search is the core technology in the popular magnum opus association discovery system lore a famous story about association rule mining is the beer and diaper story a purported survey of behavior of supermarket shoppers discovered that customers presumably young men who buy diapers tend also to buy beer this anecdote became popular as an example of how unexpected association rules might be found from everyday data there are varying opinions as to how much of the story is true ref name dss http www dssresources com newsletters 66 php ref daniel powers says ref name dss blockquote in 1992 thomas blischok manager of a retail consulting group at teradata and his staff prepared an analysis of 1 2 million market baskets from about 25 osco drug stores database queries were developed to identify affinities the analysis did discover that between 5 00 and 7 00 p m that consumers bought beer and diapers osco managers did not exploit the beer and diapers relationship by moving the products closer together on the shelves blockquote other types of association mining multi relation association rules multi relation association rules mrar is a new class of association rules which in contrast to primitive simple and even multi relational association rules that are usually extracted from multi relational databases each rule item consists of one entity but several relations these relations indicate indirect relationship between the entities consider the following mrar where the first item consists of three relations live in nearby and humid those who live in a place which is near by a city with humid climate type and also are younger than 20 their health condition is good such association rules are extractable from rdbms data or semantic web data ref name mrar mining multi relation association rules ramezani reza mohamad saraee and mohammad ali nematbakhsh mrar mining multi relation association rules journal of computing and security 1 no 2 2014 ref context based association rules is a form of association rule context based association rules claims more accuracy in association rule mining by considering a hidden variable named context variable which changes the final set of association rules depending upon the value of context variables for example the baskets orientation in market basket analysis reflects an odd pattern in the early days of month this might be because of abnormal context i e salary is drawn at the start of the month ref name context based positive and negative spatio temporal association rule mining shaheen m shahbaz m and guergachi a context based positive and negative spatio temporal association rule mining elsevier knowledge based systems jan 2013 pp 261 273 ref contrast set learning is a form of associative learning contrast set learners use rules that differ meaningfully in their distribution across subsets ref name webb03 cite conference author gi webb and s butler and d newlands year 2003 title on detecting differences between groups conference kdd 03 proceedings of the ninth acm sigkdd international conference on knowledge discovery and data mining url http portal acm org citation cfm id 956781 ref ref name busy menzies tim and hu ying data mining for very busy people ieee computer october 2003 pp 18 25 ref weighted class learning is another form of associative learning in which weight may be assigned to classes to give focus to a particular issue of concern for the consumer of the data mining results high order pattern discovery facilitate the capture of high order polythetic patterns or event associations that are intrinsic to complex real world data ref name discovere cite journal last wong first andrew k c author2 wang yang title high order pattern discovery from discrete valued data journal ieee transactions on knowledge and data engineering tkde year 1997 pages 877 893 ref k optimal pattern discovery provides an alternative to the standard approach to association rule learning that requires that each pattern appear frequently in the data approximate frequent itemset mining is a relaxed version of frequent itemset mining that allows some of the items in some of the rows to be 0 ref jinze liu susan paulsen xing sun wei wang andrew nobel j p 2006 mining approximate frequent itemsets in the presence of noise algorithm and analysis retrieved from http citeseerx ist psu edu viewdoc summary doi 10 1 1 62 3805 dead link date october 2016 bot internetarchivebot fix attempted yes ref generalized association rules hierarchical taxonomy concept hierarchy quantitative association rules categorical and quantitative data ref name quantminer cite journal last salleb aouissi first ansaf author2 vrain christel author3 nortet cyril title quantminer a genetic algorithm for mining quantitative association rules journal international joint conference on artificial intelligence ijcai year 2007 pages 1035 1040 ref interval data association rules e g partition the age into 5 year increment ranged maximal association rules sequential pattern mining discovers subsequences that are common to more than minsup sequences in a sequence database where minsup is set by the user a sequence is an ordered list of transactions ref name sequence zaki mohammed j 2001 spade an efficient algorithm for mining frequent sequences machine learning journal 42 pp 31 60 ref sequential rules discovering relationships between items while considering the time ordering it is generally applied on a sequence database for example a sequential rule found in database of sequences of customer transactions can be that customers who bought a computer and cd roms later bought a webcam with a given confidence and support subspace clustering a specific type of clustering high dimensional data is in many variants also based on the downward closure property for specific clustering models ref name zimekassent2014 cite journal last1 zimek first1 arthur last2 assent first2 ira last3 vreeken first3 jilles title frequent pattern mining algorithms for data clustering year 2014 pages 403 423 doi 10 1007 978 3 319 07821 2 16 ref warmr is shipped as part of the ace data mining suite it allows association rule learning for first order relational rules ref cite journal pmid 11272703 volume 15 issue 2 title warmr a data mining tool for chemical data date feb 2001 journal j comput aided mol des pages 173 81 ref see also sequence mining production system computer science learning classifier system rule based machine learning references reflist 3 refs ref name deng2014 z h deng and s l lv fast mining frequent itemsets using nodesets http www sciencedirect com science article pii s0957417414000463 expert systems with applications 41 10 4505 4512 2014 ref ref name deng2012 z h deng z wang\xef\xbc\x8cand j jiang a new algorithm for fast mining frequent itemsets using n lists http info scichina com 8084 scife en abstract abstract508369 shtml science china information sciences 55 9 2008 2030 2012 ref ref name deng2010 z h deng and z wang a new fast vertical method for mining frequent patterns http www tandfonline com doi abs 10 1080 18756891 2010 9727736 international journal of computational intelligence systems 3 6 733 744 2010 ref ref name dharmesh2013 d bhalodiya k m patel and c patel an efficient way to find frequent pattern with dynamic programming approach http ieeexplore ieee org xpl login jsp tp arnumber 6780102 url http 3a 2f 2fieeexplore ieee org 2fxpls 2fabs all jsp 3farnumber 3d6780102 nirma university international conference on engineering nuicone 2013 28 30 november 2013 ref external links bibliographies http www uco es grupos kdis armbibliography extensive bibliography on association rules by j m luna http michael hahsler net research bib association rules annotated bibliography on association rules by m hahsler http www statsoft com textbook association rules statsoft electronic statistics textbook association rules by dell software prone to spam date february 2016 z148 no more links please be cautious adding more external links wikipedia is not a collection of links and should not be used for advertising excessive or inappropriate links will be removed see wikipedia external links and wikipedia spam for details if there are already suitable links propose additions or replacements on the article s talk page or submit your link to the relevant category at dmoz dmoz org and link there using dmoz defaultsort association rule learning category data management category data mining'
b'use dmy dates date july 2013 a data stream management system dsms is a computer program to manage continuous data streams it is similar to a database management system dbms which is however designed for static data in conventional databases a dsms also offers a flexible query processing so that the information need can be expressed using queries however in contrast to a dbms a dsms executes a continuous query that is not only performed once but is permanently installed therefore the query is continuously executed until it is explicitly uninstalled since most dsms are data driven a continuous query produces new results as long as new data arrive at the system this basic concept is similar to complex event processing so that both technologies are partially coalescing functional principle one of the most important features of a dsms is the possibility to handle potentially infinite and rapidly changing data streams by offering flexible processing at the same time although there are only limited resources such as main memory the following table provides various principles of dsms and compares them to traditional dbms border 1 database management system dbms data stream management system dsms persistent data relations volatile data streams random access sequential access one time queries continuous queries theoretically unlimited secondary storage limited main memory only the current state is relevant consideration of the order of the input relatively low update rate potentially extremely high update rate little or no time requirements real time requirements assumes exact data assumes outdated inaccurate data plannable query processing variable data arrival and data characteristics processing and streaming models one of the biggest challenges for a dsms is to handle potentially infinite data streams using a fixed amount of memory and no random access to the data there are different approaches to limit the amount of data in one pass which can be divided into two classes for the one hand there are compression techniques that try to summarize the data and for the other hand there are window techniques that try to portion the data into finite parts synopses the idea behind compression techniques is to maintain only a synopsis of the data but not all raw data points of the data stream the algorithms range from selecting random data points called sampling to summarization using histograms wavelets or sketching one simple example of a compression is the continuous calculation of an average instead of memorizing each data point the synopsis only holds the sum and the number of items the average can be calculated by dividing the sum by the number however it should be mentioned that synopses cannot reflect the data accurately thus a processing that is based on synopses may produce inaccurate results windows instead of using synopses to compress the characteristics of the whole data streams window techniques only look on a portion of the data this approach is motivated by the idea that only the most recent data are relevant therefore a window continuously cuts out a part of the data stream e g the last ten data stream elements and only considers these elements during the processing there are different kinds of such windows like sliding windows that are similar to fifo computing and electronics fifo lists or tumbling windows that cut out disjoint parts furthermore the windows can also be differentiated into element based windows e g to consider the last ten elements or time based windows e g to consider the last ten seconds of data there are also different approaches to implementing windows there are for example approaches that use timestamps or time intervals for system wide windows or buffer based windows for each single processing step sliding window query processing is also suitable to being implemented in parallel processors by exploiting parallelism between different windows and or within each window extent ref cite journal last1 de matteis first1 tiziano last2 mencagli first2 gabriele title parallel patterns for window based stateful operators on data streams an algorithmic skeleton approach journal international journal of parallel programming date 25 march 2016 doi 10 1007 s10766 016 0413 x ref query processing since there are a lot of prototypes there is no standardized architecture however most dsms are based on the information retrieval query processing in dbms by using declarative languages to express queries which are translated into a plan of operators these plans can be optimized and executed a query processing often consists of the following steps formulation of continuous queries the formulation of queries is mostly done using declarative languages like sql in dbms since there are no standardized query languages to express continuous queries there are a lot of languages and variations however most of them are based on sql such as the continuous query language cql streamsql and event stream processing epl there are also graphical approaches where each processing step is a box and the processing flow is expressed by arrows between the boxes the language strongly depends on the processing model for example if windows are used for the processing the definition of a window has to be expressed in streamsql a query with a sliding window for the last 10 elements looks like follows source lang sql select avg price from examplestream size 10 advance 1 tuples where value 100 0 source this stream continuously calculates the average value of price of the last 10 tuples but only considers those tuples whose prices are greater than 100 0 in the next step the declarative query is translated into a logical query plan a query plan is a directed graph where the nodes are operators and the edges describe the processing flow each operator in the query plan encapsulates the semantic of a specific operation such as filtering or aggregation in dsmss that process relational data streams the operators are equal or similar to the operators of the relational algebra so that there are operators for selection projection join and set operations this operator concept allows the very flexible and versatile processing of a dsms optimization of queries the logical query plan can be optimized which strongly depends on the streaming model the basic concepts for optimizing continuous queries are equal to those from query optimizer database systems if there are relational data streams and the logical query plan is based on relational operators from the relational algebra a query optimizer can use the algebraic equivalences to optimize the plan these may be for example to push selection operators down to the sources because they are not so computationally intensive like join operators furthermore there are also cost based optimization techniques like in dbms where a query plan with the lowest costs is chosen from different equivalent query plans one example is to choose the order of two successive join operators in dbms this decision is mostly done by certain statistics of the involved databases but since the data of a data streams is unknown in advance there are no such statistics in a dsms however it is possible to observe a data stream for a certain time to obtain some statistics using these statistics the query can also be optimized later so in contrast to a dbms some dsms allows to optimize the query even during runtime therefore a dsms needs some plan migration strategies to replace a running query plan with a new one transformation of queries since a logical operator is only responsible for the semantics of an operation but does not consist of any algorithms the logical query plan must be transformed into an executable counterpart this is called a physical query plan the distinction between a logical and a physical operator plan allows more than one implementation for the same logical operator the join for example is logically the same although it can be implemented by different algorithms like a nested loop join or a sort merge join notice these algorithms also strongly depend on the used stream and processing model finally the query is available as a physical query plan execution of queries since the physical query plan consists of executable algorithms it can be directly executed for this the physical query plan is installed into the system the bottom of the graph of the query plan is connected to the incoming sources which can be everything like connectors to sensors the top of the graph is connected to the outgoing sinks which may be for example a visualization since most dsmss are data driven a query is executed by pushing the incoming data elements from the source through the query plan to the sink each time when a data element passes an operator the operator performs its specific operation on the data element and forwards the result to all successive operators data stream management systems http www sqlstream com stream processing sqlstream http www db stanford edu stream stream ref name standfordstream http ilpubs stanford edu 8090 641 arasu a et al stream the stanford data stream management system technical report 2004 stanford infolab ref http www cs brown edu research aurora aurora ref name aurora cite conference author abadi title aurora a data stream management system conference sigmod 2003 citeseerx 10 1 1 67 8671 display authors etal ref http www streambase com streambase systems inc http telegraph cs berkeley edu telegraphcq telegraphcq ref name telegraphcq http www cs berkeley edu franklin papers tcqcidr03 pdf chandrasekaran s et al telegraphcq continuous dataflow processing for an uncertain world cidr 2003 ref http research cs wisc edu niagara niagaracq ref name niagaracq http www cs wisc edu niagara papers niagaracq pdf chen j et al niagaracq a scalable continuous query system for internet databases sigmod 2000 ref http wwwdb inf tu dresden de research projects closed projects qstream qstream http dbs mathematik uni marburg de home research projects pipes pipes http www softwareag com de products wm events overview default asp webmethods business events http www db in tum de research projects streamglobe index shtml streamglobe http odysseus informatik uni oldenburg de odysseus http www microsoft com sqlserver en us solutions technologies business intelligence streaming data aspx streaminsight http www 01 ibm com software data infosphere streams infosphere streams http www sas com en us software data management event stream processing html sas event stream processing engine http go sap com uk product data mgmt complex event processing html sap event stream processor https www pipelinedb com pipeline db see also complex event processing event stream processing relational data stream management system references reflist cite book last aggarwal first charu c authorlink year 2007 title data streams models and algorithms publisher springer location new york id isbn 978 0 387 47534 9 cite book first1 lukasz last1 golab first2 m tamer last2 \xc3\xb6zsu authorlink year 2010 title data stream management publisher morgan and claypool location waterloo usa id isbn 978 1 608 45272 9 external links http www pam2004 org papers 113 pdf using data stream management systems for traffic analysis a case study last visited 2013 01 10 http infolab stanford edu stream stream stanford stream data manager last visited 2013 01 10 http datalab cs pdx edu niagara niagarast a research data stream management system at portland state university last visited 2013 01 10 http odysseus informatik uni oldenburg de odysseus an open source java based framework for data stream management systems last visited 2013 01 10 http home dei polimi it margara papers survey pdf processing flows of information from data stream to complex event processing survey article on data stream and complex event processing systems last visited 2013 01 10 http www streambase com developers docs latest streamsql index html streamsql reference last visited 2013 01 10 https web archive org web 20140706215458 http www sqlstream com stream processing with sql stream processing with sql introduction to streaming data management with sql category data management'
b'lean integration is a management system that emphasizes creating value for customers continuous improvement and eliminating waste as a sustainable data integration and system integration practice lean integration has parallels with other lean disciplines such as lean manufacturing lean it and lean software development it is a specialized collection of tools and techniques that address the unique challenges associated with seamlessly combining information and processes from systems that were independently developed are based on incompatible data models and remain independently managed to achieve a cohesive holistic operation history lean integration was first introduced by john schmidt in a series of blog articles starting in january 2009 entitled 10 weeks to lean integration ref http blogs informatica com perspectives index php 2009 01 14 10 weeks to lean integration original lean integration blog series ref this was followed by a white paper ref http www cloudyintegration com uploads lean integration afe john schmidt pdf lean integration white paper ref on the topic in april 2009 and the book lean integration an integration factory approach to business agility ref name schmidt john g schmidt david lyle 2010 lean integration an integration factory approach to business agility addison wesley pearson education isbn 0 321 71231 5 ref in may 2010 overview lean integration builds on the same set of principles that were developed for lean manufacturing and lean software development which is based on the toyota production system integration solutions can be broadly categorized as either process integration or data integration the book ref name schmidt is based on the premise that integration is an ongoing activity and not a one time activity therefore integration should be viewed as a long term strategy for an organization john schmidt and david lyle initially articulated in their book the reasons for maintaining an efficient and sustainable integration team lean integration as an integration approach must be sustainable and holistic unlike other integration approaches that either tackle only a part of the problem or tackle the problem for a short period of time lean integration drives elimination of waste by adopting reusable elements high automation and quality improvements lean is a data driven fact based methodology that relies on metrics to ensure that the quality and performance are maintained at a high level an organizational focus is required for the implementation of lean integration principles the predominant organizational model is the integration competency center which may be structured as a central group or a more loosely coupled federated team lean integration principles the principles of lean integration may at first glance appear similar to that of six sigma but there are some very clear differences between them six sigma is an analytical technique that focuses on quality and reduction of defects while lean is a management system that focuses on delivering value to the end customer by continuously improving value delivery processes lean provides a robust framework that facilitates improving efficiency and effectiveness by focusing on critical customer requirements as mentioned in lean integration there are seven core lean integration principles vital for deriving significant and sustainable business benefits they are as below focus on the customer and eliminate waste waste elimination should be viewed from the customer perspective and all activities that do not add value to the customer needs to be looked at closely and eliminated or reduced in an integration context the customer is often an internal sponsor or group within an organization that uses benefits from or pays for the integrated capabilities continuously improve a data driven cycle of hypothesis validation implementation should be used to drive innovation and continuously improve the end to end process adopting and institutionalizing lessons learned and sustaining integration knowledge are related concepts that assist in the establishment of this principle empower the team creating cross functional teams and sharing commitments across individuals empower the teams and individuals who have a clear understanding of their roles and the needs of their customers the team is also provided the support by senior management to innovate and try new ideas without fear of failure optimize the whole adopt a big picture perspective of the end to end process and optimize the whole to maximize the customer value this may at times require performing individual steps and activities that appear to be sub optimal when viewed in isolation but aid in streamlining the end to end process plan for change application of mass customization techniques like leveraging automated tools structured processes and reusable and parameterized integration elements leads to reduction in cost and time in both the build and run stages of the integration life cycle another key technique is a middleware services layer that presents applications with enduring abstractions of data through standardized interfaces allowing the underlying data structures to change without necessarily impacting the dependent applications automate processes automation of tasks increases the ability to respond to large integration projects as effectively as small changes in its ultimate form automation eliminates integration dependencies from the critical implementation path of projects build quality in process excellence is emphasized and quality is built in rather than inspected in a key metric for this principle is first time through ftt percentage which is a measure of the number of times an end to end process is executed without having to do any rework or repeat any of the steps benefits of lean integration the lean integration practices transforms integration from an art into a science a repeatable and teachable methodology that shifts the focus from integration as a point in time activity to integration as a sustainable activity that enables organizational agility once an organization adopts the integration as a science it enhances the organization s ability to change rapidly without comprising on the it risk or quality thereby transforming the organization into an agile data driven enterprise the following are the advantages derived by adopting the lean integration practices efficiency typical improvements are in the scale of 50 labor productivity improvements and 90 lead time reduction through continuous efforts to eliminate waste agility reusable components highly automated processes and self service delivery models improve the agility of the organization data quality quality and reliability of data is enhanced and data becomes a real asset governance metrics are established that drive continuous improvement innovation innovation is facilitated by using fact based approach staff morale it staff is kept engaged with high morale driving bottom up improvements see also integration competency center lean software development lean it data integration toyota production system references references external links http www integrationfactory com lean integration book microsite http blogs informatica com perspectives index php 2010 04 06 health care is ready for lean integration application of lean integration to health care http www informatica com news events press releases pages 02082010 lean aspx press release about lean integration book http www linkedin com in johnschmidt john schmidt profile http www linkedin com in davelyle david lyle profile http my safaribooksonline com 9780321712363 lean integration book publisher website http www baselinemag com c a it management how it runs lean 419352 slide show overview of lean integration http www linkedin com groups gid 2302506 linkedin group for lean integration community http www itbusinessedge com cm blogs vizard making the case for lean integration cs 42547 book review by mike vizard of itbusinessedge http www bcs org server php show conblogpost 1685 book review by john morris http www itbusinessedge com cm blogs lawson lean principles can make it better at integration cs 42041 utm source itbe utm medium email utm campaign eeb nr eeb john schmidt and david lyle interview by loraine lawson http www insurancenetworking com blogs insurance technology lean it manufacturing 25138 1 html book review by joe mckendrick http www information management com dmradio 10017194 1 html david lyle interview on dm radio category data management category software development philosophies category agile software development category information technology category quality'
b'distinguish network attached storage use dmy dates date february 2013 area networks a storage area network san ref cite web url http cctvinstitute co uk storage area network title storage area network by noor ul mushtaq ref is a network which provides access to consolidated block device block level data storage sans are primarily used to enhance storage devices such as disk array s tape library tape libraries and optical jukebox es accessible to server computing server s so that the devices appear to the operating system as direct attached storage locally attached devices a san typically has its own network of storage devices that are generally not accessible through the local area network lan by other devices the cost and complexity of sans dropped in the early 2000s to levels allowing wider adoption across both enterprise and small to medium sized business environments a san does not provide file abstraction only block level operations however file systems built on top of sans do provide file level access and are known as shared disk file system s storage refimprove section date february 2014 historically data centers first created islands of scsi disk array s as direct attached storage das each dedicated to an application often visible as a number of virtual hard drives addressed as logical unit number s luns ref cite web url http www novell com documentation oes stor ovw page documentation oes stor ovw data ami6rr0 html title novel doc oes 1 direct attached storage solutions ref essentially a san consolidates such storage islands together using a high speed network operating systems maintain their own file system s on their own dedicated non shared luns as though they were local to themselves if multiple systems were simply to attempt to share a lun these would interfere with each other and quickly corrupt the data any planned sharing of data on different computers within a lun requires software such as san file system s or clustered computing despite such issues sans help to increase storage capacity utilization since multiple servers consolidate their private storage space onto the disk arrays common uses of a san include provision of transactionally accessed data that require high speed block device block level access to the hard drives such as email servers databases and high usage file servers san compared to nas network attached storage nas was designed independently of san systems in both a nas and san the various computers in a network such as individual users desktop computers and dedicated servers running applications application server s can share a more centralized collection of storage devices via a network connection such as a local area network lan concentrating the storage on one or more nas servers or in a san instead of placing storage devices on each application server allows application server configurations to be optimized for running their applications instead of also storing all the related data and moves the storage management task to the nas or san system both nas and san have the potential to reduce the amount of excess storage that must be purchased and provisioned as spare space in a das only architecture each computer must be provisioned with enough excess storage to ensure that the computer does not run out of space at an untimely moment in a das architecture the spare storage on one computer cannot be utilized by another with a nas or san architecture where storage is shared across the needs of multiple computers one normally provisions a pool of shared spare storage that will serve the peak needs of the connected computers which typically is less than the total amount of spare storage that would be needed if individual storage devices were dedicated to each computer in a nas the storage devices are directly connected to a file server that makes the storage available at a file level to the other computers in a san the storage is made available at a lower block level leaving file system concerns to the client side san protocols include fibre channel iscsi ata over ethernet aoe and hyperscsi one way to loosely conceptualize the difference between a nas and a san is that nas appears to the client os operating system as a file server the client can map network drives to shares on that server whereas a disk available through a san still appears to the client os as a disk visible in disk and volume management utilities along with client s local disks and available to be formatted with a file system and mounted one drawback to both the nas and san architecture is that the connection between the various cpus and the storage units are no longer dedicated high speed busses tailored to the needs of storage access instead the cpus use the lan to communicate potentially creating bandwidth as well as performance bottlenecks additional data security considerations are also required for nas and san setups as information is being transmitted via a network that potentially includes design flaws security exploits and other vulnerabilities that may not exist in a das setup while it is possible to use the nas or san approach to eliminate all storage at user or application computers typically those computers still have some local direct attached storage for the operating system various program files and related temporary files used for a variety of purposes including cache computing caching content locally to understand their differences a comparison of san das and nas architectures may be helpful ref name eval cite web title storage architectures das san nas iscsi san work marketing web site url http www evaluatorgroup com document storage architectures publisher evaluator group archivedate september 17 2016 archiveurl https web archive org web 20160917144751 http www evaluatorgroup com document storage architectures accessdate november 10 2016 ref san nas hybrid image compingles3 png right thumb 260px hybrid using san direct attached storage das and nas technologies despite their differences san and nas are not mutually exclusive and may be combined as a san nas hybrid offering both file level protocols nas and block level protocols san from the same system an example of this is openfiler a free software product running on linux based systems a shared disk file system can also be run on top of a san to provide filesystem services benefits sharing storage usually simplifies storage administration and adds flexibility since cables and storage devices do not have to be physically moved to shift storage from one server to another other benefits include the ability to allow servers to boot from the san itself this allows for a quick and easy replacement of faulty servers since the san can be reconfigured so that a replacement server can use the logical unit number lun of the faulty server while this area of technology is still new many view it as being the future of the enterprise datacenter ref cite web title san vs das a cost analysis of storage in the enterprise url http capitalhead com articles san vs das a cost analysis of storage in the enterprise aspx work san vs das a cost analysis of storage in the enterprise date 31 october 2008 accessdate 2010 01 28 ref sans also tend to enable more effective disaster recovery processes a san could span a distant location containing a secondary storage array this enables storage replication either implemented by disk array controller s by server software or by specialized san devices since ip wide area network wan s are often the least costly method of long distance transport the fibre channel over ip fcip and iscsi protocols have been developed to allow san extension over ip networks the traditional physical scsi layer could support only a few meters of distance not nearly enough to ensure business continuance in a disaster the economic consolidation of disk arrays has accelerated the advancement of several features including i o caching snapshot computer storage snapshotting and volume cloning business continuance volumes or bcvs network types most storage networks use the scsi protocol for communication between servers and disk drive devices a mapping layer to other protocols is used to form a network ata over ethernet ata over ethernet aoe mapping of at attachment ata over ethernet fibre channel protocol fcp the most prominent one is a mapping of scsi over fibre channel fibre channel over ethernet fcoe escon over fibre channel ficon used by mainframe computer s hyperscsi mapping of scsi over ethernet ifcp ref cite web url http www techweb com encyclopedia defineterm jhtml term ipstorage title techencyclopedia ip storage accessdate 2007 12 09 ref or sanoip ref cite web url http www techweb com encyclopedia defineterm jhtml term sanoip title techencyclopedia sanoip accessdate 2007 12 09 ref mapping of fcp over ip iscsi mapping of scsi over tcp ip iscsi extensions for rdma iser mapping of iscsi over infiniband storage networks may also be built using serial attached scsi sas and serial ata sata technologies sas evolved from scsi direct attached storage sata evolved from parallel ata ide direct attached storage sas and sata devices can be networked using serial attached scsi sas expanders sas expanders examples of stacked protocols using scsi class wikitable style text align center colspan 5 applications colspan 5 scsi layer rowspan 4 fibre channel protocol fcp rowspan 3 fibre channel protocol fcp fibre channel protocol fcp fibre channel protocol fcp rowspan 2 iscsi fibre channel over ip fcip internet fibre channel protocol ifcp colspan 3 internet protocol tcp fibre channel over ethernet fcoe colspan 3 internet protocol ip fibre channel fc colspan 4 ethernet san infrastructure image ml qlogicnfcconn jpg thumb qlogic san fibre channel switch switch with optical fibre channel electrical connector connectors installed sans often use a fibre channel fabric topology an infrastructure specially designed to handle storage communications it provides faster and more reliable access than higher level protocols used in network attached storage nas a fabric is similar in concept to a network segment in a local area network a typical fibre channel san fabric is made up of a number of fibre channel switch es many san equipment vendors also offer some form of fibre channel routing and these can allow data to cross between different fabrics without merging them these offerings use proprietary protocol elements and the top level architectures being promoted are radically different for example they might map fibre channel traffic over ip or over synchronous optical networking sonet sdh compatibility one of the early problems with fibre channel sans was that the switches and other hardware from different manufacturers were not compatible although the basic storage protocol fcp was standard some of the higher level functions did not interoperate well similarly many host operating systems would react badly to other operating systems sharing the same fabric citation needed date october 2010 in media and entertainment video editing systems require very high data transfer rates and very low latency sans in media and entertainment are often referred to as serverless due to the nature of the configuration which places the video workflow ingest editing playout desktop clients directly on the san rather than attaching to servers control of data flow is managed by a distributed file system such as stornext by quantum ref cite web url http www quantum com products software stornext index aspx title stornext storage manager high speed file sharing data management and digital archiving software publisher quantum com date accessdate 2013 07 08 ref per node bandwidth usage control sometimes referred to as quality of service qos is especially important in video editing as it ensures fair and prioritized bandwidth usage across the network storage virtualization main storage virtualization storage virtualization is the process of abstracting logical storage from physical storage the physical storage resources are aggregated into storage pools from which the logical storage is created it presents to the user a logical space for data storage and transparently handles the process of mapping it to the physical location a concept called location transparency this is implemented in modern disk arrays often using vendor proprietary technology however the goal of storage virtualization is to group multiple disk arrays from different vendors scattered over a network into a single storage device the single storage device can then be managed uniformly citation needed date september 2011 quality of service san storage qos enables the desired storage performance to be calculated and maintained for network customers accessing the device some factors that affect san qos are bandwidth computing bandwidth the rate of data throughput available on the system latency engineering latency the time delay for a read write operation to execute queue depth the number of outstanding operations waiting to execute to the underlying disks traditional or solid date drive s qos can be impacted in a san storage system by unexpected increase in data traffic usage spike from one network user that can cause performance to decrease for other users on the same network this can be known as the noisy neighbor effect when qos services are enabled in a san storage system the noisy neighbor effect can be prevented and network storage performance can be accurately predicted using san storage qos is in contrast to using disk over provisioning in a san environment over provisioning can be used to provide additional capacity to compensate for peak network traffic loads however where network loads are not predictable over provisioning can eventually cause all bandwidth to be fully consumed and latency to increase significantly resulting in san performance degradation see also ata over ethernet aoe direct attached storage das disk array fibre channel fibre channel over ethernet file area network host bus adapter hba iscsi iscsi extensions for rdma list of networked storage hardware platforms list of storage area network management systems massive array of idle disks maid network attached storage nas redundant array of independent disks raid scsi rdma protocol srp storage management initiative specification smi s storage hypervisor storage resource management srm storage virtualization system area network references more footnotes date june 2008 references external links attention please do not add links without discussion and consensus on the talk page undiscussed links will be removed https www redbooks ibm com redbooks pdfs sg245470 pdf introduction to storage area networks exhaustive introduction into san ibm redbooks ibm redbook http capitalhead com articles san vs das a cost analysis of storage in the enterprise aspx san vs das a cost analysis of storage in the enterprise http searchstorage techtarget co uk generic 0 295582 sid181 gci1516893 00 html sas and sata solid state storage lower data center power consumption https www youtube com playlist list plivyd7w2z2hmggriwrorcqll4hmpr1die san nas videos http www storageareanetworkinfo blogspot com ar storage area network info interwikies authority control defaultsort storage area network category data management category telecommunications engineering category storage area networks'
b'infobox software name novell storage manager logo screenshot caption collapsible author developer novell released 2004 start date yyyy mm dd discontinued latest release version 4 1 latest release date start date 2014 10 07 latest preview version latest preview date start date and age yyyy mm dd frequently updated programming language operating system platform size language status genre system software license website http www novell com products storage manager novell storage manager novell storage manager is a system software package released by novell in 2004 ref citation last greyzdorf first noemi title novell delivers a new way of intelligently managing organizations file based information journal idc 216013 volume 1 issue storage software technology assessment year 2009 pages 1 3 url http www novell com docrep 2009 01 novell 20delivers 20a 20new 20way 20of 20intelligently 20managing 20organizations 20file based 20information en pdf ref that uses identity policy and novell edirectory directory events to automate full lifecycle management of file storage for individual users and organizational groups by tying storage management to an organization s existing identity infrastructure it has been pointed out ref citation last greyzdorf first noemi title efficiently delivering enterprise class file based storage journal idc spotlight year 2010 pages 1 5 ref novell storage manager enables the administration of users across all file servers as a single pool rather than in separate independently managed domains novell storage manager is a component of the novell file management suite how it works novell storage manager dynamically manages and provisions storage based on user and group events that occur in the directory including user creations group assignments moves renames and deletions when a change happens in the directory that affects a user s file storage needs or user storage policy storage manager applies the appropriate policy and makes the necessary changes at the file system level to address those storage needs ref citation title novell storage manager for novell edirectory year 2009 page 4 url http www novell com docrep 2009 04 novell storage manager for novell edirectory white paper en pdf ref the following key components comprise novell storage manager s identity and policy driven state machine architecture directory services storage policies novell storage manager event monitors novell storage manager policy engine novell storage manager agents and action objects this state machine architecture enables the engine to properly deal with transient waits with directory synchronization issues it also allows recovery from failures involving network communications a target server or a server running a component of storage manager including the policy engine itself if a failure or interruption occurs at any point during operation storage manager will be able to successfully continue the operation from where it was when the interruption occurred reviews jon toigo called novell storage manager a robust and smart approach to corralling user files into an organized and efficient management scheme ref citation last toigo first jon william title novell storage manager strikes data management gold url http esj com articles 2009 04 28 novell storage mgr aspx accessdate 26 july 2010 ref he also said it was best in class of the products he d reviewed ref citation last toigo first jon william title everything we need to know about how to screw up it\xe2\x80\xa6 url http www drunkendata com p 2916 accessdate 30 july 2010 ref references reflist external links http www novell com products storage manager novell storage manager product homepage overview features and technical information http www storagemanagersupport com nsm novell storage manager support novell category novell category novell software category storage software category data management category identity management'
b'infobox software name novell file reporter logo screenshot caption collapsible author developer novell released start date 2010 01 discontinued latest release version 2 6 1 latest release date start date 2015 10 02 latest preview version latest preview date start date yyyy mm dd frequently updated programming language operating system platform size language status genre system software license website http www novell com products file reporter novell file reporter novell file reporter a k a nfr is software that allows network administrators to identify files stored on the network and generates reports regarding the size of individual files file format file type when files were last accessed and where duplicates exist additionally the file reporter tracks storage volume capacity and usage it is a component of the novell file management suite how it works novell file reporter examines and reports on terabytes of data via a central reporting engine nfr engine and distributed agents nfr agents ref citation title novell file reporter reports on terabytes of data url http www novell com products file reporter terabytes data html accessdate 31 july 2010 ref the nfr engine schedules the scans of file instances conducted by nfr agents processes and compiles the scans for reporting purposes and provides report information to the user interface in addition to the standard reports ref citation title novell file report standard reports url http www novell com products file reporter standard reports html accessdate 31 july 2010 ref it can generate the nfr engine can also produce trigger reports in response to specific events a server volume crossing a capacity threshold for example accordingly the nfr engine monitors the data gathered by the nfr agents in order to identify these triggers the nfr engine when working in either novell edirectory edirectory or active directory connects to the directory via a directory services interface dsi and thus can monitor and check file permissions ref citation last huber first matthias journal linux magazine title novell file management suite optimizes storage date 25 january 2010 url http www linux magazine com online news novell file management suite optimizes storage accessdate 31 july 2010 ref references reflist external links http www novell com products file reporter technicalinfo novell file reporter product page overview features and technical information http www novell com documentation filereporter2 novell file reporter documentation http www filereportersupport com nfr novell file reporter support novell category novell category novell software category storage software category data management'
b'unreferenced date october 2013 semantic integration is the process of interrelating information from diverse sources for example calendars and to do lists email archives presence information physical psychological and social documents of all sorts contacts including social graph s search results and advertising and marketing relevance derived from them in this regard semantics focuses on the organization of and action upon information by acting as an intermediary between heterogeneous data sources which may conflict not only by structure but also context or value applications and methods in enterprise application integration eai semantic integration can facilitate or even automate the communication between computer systems using metadata publishing metadata publishing potentially offers the ability to automatically link ontology computer science ontologies one approach to semi automated ontology mapping requires the definition of a semantic distance or its inverse semantic similarity and appropriate rules other approaches include so called lexical methods as well as methodologies that rely on exploiting the structures of the ontologies for explicitly stating similarity equality there exist special properties or relationships in most ontology languages web ontology language owl for example has owl equivalentclass owl equivalentproperty and owl sameas eventually system designs may see the advent of composable architectures where published semantic based interfaces are joined together to enable new and meaningful capabilities citation needed date february 2014 these could predominately be described by means of design time declarative specifications that could ultimately be rendered and executed at run time citation needed date february 2014 semantic integration can also be used to facilitate design time activities of interface design and mapping in this model semantics are only explicitly applied to design and the run time systems work at the syntax level citation needed date february 2014 this early semantic binding approach can improve overall system performance while retaining the benefits of semantic driven design citation needed date february 2014 examples the pacific symposium on biocomputing has been a venue for the popularization of the ontology mapping task in the biomedical domain and a number of papers on the subject can be found in its proceedings see also data integration dataspaces enterprise integration ontology based data integration ontology matching semantic heterogeneity semantic translation semantic unification references reflist 2 external links https web archive org web 20070811204850 http zapthink com report html id zapflash 08082003 semantic integration loosely coupling the meaning of data http drops dagstuhl de opus volltexte 2005 40 ontology mapping the state of the art 2005 paper http arxiv org ftp arxiv papers 0901 0901 4934 pdf 2010 paper by carl hewitt http wwwhome portavita nl yeb ooi pdf opencyc to oracle interface semantic web defaultsort semantic integration category ontology information science category data management category semantics category bioinformatics'
b'a virtual data room sometimes called a vdr is an online repository of information that is used for the storing and distribution of documents in many cases a virtual data room is used to facilitate the due diligence process during an m a transaction loan syndication or private equity and venture capital transactions this due diligence process has traditionally used a physical data room to accomplish the disclosure of documents for reasons of cost efficiency and security virtual data rooms have widely replaced the more traditional physical data room ref http www inc com best industries 2013 jeremy quittner virtual data rooms html ref ref http www forbes com pictures fghj45fjl 2 virtual data rooms ref a virtual data room is an extranet to which the bidders and their advisers are given access via the internet an extranet is essentially a website with limited controlled access using a secure log on supplied by the vendor which can be disabled at any time by the vendor if a bidder withdraws much of the information released is confidential and restrictions are applied to the viewer s ability to release this to third parties by means of forwarding copying or printing this can be effectively applied to protect the data using digital rights management ref http www divestopedia com definition 836 virtual data room vdr ref in the process of mergers and acquisitions the data room is set up as part of the central repository of data relating to companies or divisions being acquired or sold the data room enables the interested parties to view information relating to the business in a controlled environment where confidentiality can be preserved conventionally this was achieved by establishing a supervised physical data room in secure premises with controlled access in most cases with a physical data room only one bidder team can access the room at a time a virtual data room is designed to have the same advantages as a conventional data room controlling access viewing copying and printing etc with fewer disadvantages due to their increased efficiency many businesses and industries have moved to using virtual data rooms instead of physical data rooms in 2006 a spokesperson for a company which sets up virtual deal rooms was reported claiming that the process reduced the bidding process by about thirty days compared to physical data rooms ref cite news last1 buckler first1 grant title a virtual smoke filled room url http www theglobeandmail com technology a virtual smoke filled room article1110056 accessdate 4 july 2016 work the globe and mail date 21 november 2006 ref references reflist defaultsort virtual data room category data management category disclosure category mergers and acquisitions'
b'international organization for standardization iso 8000 the global standard for data quality and enterprise master data describes the features and defines the requirements for the data quality and portability of enterprise master data master data is typically internal business information about clients products and operations the standard is currently under development but is quickly being adopted by many fortune 500 corporations and certain public agencies involved in the regulation and supervision of financial markets around the world iso 8000 is one of the emerging technology standards that large and complex organizations are turning to in order to improve business processes and control operational costs the standard will be published as a number of separate documents which international organization for standardization iso calls parts iso 8000 is being developed by iso tc 184 sc 4 iso technical committee tc 184 automation systems and integration sub committee sc 4 industrial data like other international organization for standardization iso and international electrotechnical commission iec standards iso 8000 is copyrighted and is not freely available ref http www iso org iso copyright htm iso copyright policy ref published parts the following part has already been published iso ts 8000 1 2011 data quality mdash part 1 overview ref http www iso org iso iso catalogue catalogue tc catalogue detail htm csnumber 50798 iso catalogue page for iso ts 8000 1 2011 ref iso 8000 2 2012 data quality mdash part 2 vocabulary ref http www iso org iso iso catalogue catalogue tc catalogue detail htm csnumber 57436 iso catalogue page for iso 8000 2 2012 ref iso 8000 61 2016 data quality mdash part 61 data quality management process reference model ref http www iso org iso home store catalogue tc catalogue detail htm csnumber 63086 iso catalogue page for iso 61 2016 ref iso ts 8000 100 2009 data quality mdash part 100 master data exchange of characteristic data overview ref http www iso org iso iso catalogue catalogue tc catalogue detail htm csnumber 52129 iso catalogue page for iso ts 8000 100 2009 ref iso 8000 102 2009 data quality mdash part 102 master data exchange of characteristic data vocabulary ref http www iso org iso iso catalogue catalogue tc catalogue detail htm csnumber 50799 iso catalogue page for iso 8000 102 2009 ref iso 8000 110 2009 data quality part 110 master data exchange of characteristic data syntax semantic encoding and conformance to data specification ref http www iso org iso iso catalogue catalogue tc catalogue detail htm csnumber 51653 iso catalogue page for iso 8000 110 2009 ref iso ts 8000 120 2009 data quality mdash part 120 master data exchange of characteristic data provenance ref http www iso org iso iso catalogue catalogue tc catalogue detail htm csnumber 50801 iso catalogue page for iso ts 8000 120 2009 ref iso ts 8000 130 2009 data quality mdash part 130 master data exchange of characteristic data accuracy ref http www iso org iso iso catalogue catalogue tc catalogue detail htm csnumber 50802 iso catalogue page for iso ts 8000 130 2009 ref iso ts 8000 140 2009 data quality mdash part 140 master data exchange of characteristic data completeness ref http www iso org iso iso catalogue catalogue tc catalogue detail htm csnumber 53589 iso catalogue page for iso ts 8000 140 2009 ref iso ts 8000 150 2011 data quality mdash part 150 master data quality management framework ref http www iso org iso iso catalogue catalogue tc catalogue detail htm csnumber 53589 iso catalogue page for iso ts 8000 150 2011 ref further reading citation last benson first peter journal real world decision support rwds journal year 2009 volume 3 issue 4 title iso 8000 data quality the fundamentals part 1 url http www ewsolutions com resource center rwds folder rwds archives issue 2009 10 12 0790666855 document 2009 10 12 3367922336 view searchterm iso 208000 citation last benson first peter title nato codification system as the foundation for iso 8000 the international standard for data quality oil it journal year 2008 url http www oilit com papers benson pdf citation last benson first peter title iso 8000 mdash a new international standard for data quality year 2009 url http www dataqualitypro com data quality home iso 8000 a new international standard for data quality html citation last benson first peter title peter benson discusses the certification options of iso 8000 live recording year 2010 url http www dataqualitypro com data quality home peter benson discusses the certification options of iso 8000 html citation last grantner first emily title iso 8000 mdash a standard for data quality year 2007 issue oct dec journal logistics spectrum url http www highbeam com doc 1p3 1518467381 html citation last west first matthew title iso 8000 mdash the emerging standard for data quality year 2009 journal iaidq s information and data quality newsletter volume 5 issue 3 url http iaidq org publications west 2009 07 shtml full article requires no cost registration to access references references iso standards defaultsort iso 8000 category iso standards 08000 category data management'
