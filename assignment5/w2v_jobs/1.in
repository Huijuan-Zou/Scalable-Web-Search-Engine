b'refimprove date december 2009 wikibooks structured query language wikibooks sql dialects reference microsoft query is a visual method of creating database query database queries using examples based on a text string the name of a document or a list of documents the qbe system converts the user input into a formal database query using sql structured query language sql on the backend allowing the user to perform powerful searches without having to explicitly compose them in sql and without even needing to know sql it is derived from mosh\xc3\xa9 m zloof s original query by example qbe implemented in the mid 1970s at ibm s research centre in yorktown new york ref zloof m m http dx doi org 10 1147 sj 164 0324 query by example a data base language ref in the context of microsoft access qbe is used for introducing students to database querying and as a user friendly database management system for small businesses microsoft excel allows results of qbe queries to be embedded in spreadsheets ref https support office com en us article use microsoft query to retrieve external data 42a2ea18 44d9 40b3 9c38 4c62f252da2e use microsoft query to retrieve external data ref see also query by example microsoft access microsoft sql server references reflist defaultsort microsoft query by example category data management category microsoft database software microsoft software stub database software stub'
b'primary sources date august 2009 document capture software refers to applications that provide the ability and feature set to automate the process of image scanner scanning paper documents most scanning personal computer hardware hardware both scanners and copier s provides the basic ability to scan to any number of image file formats including pdf tiff jpg bmp file format bmp etc this basic functionality is augmented by document capture software which can add efficiency and standardization to the process typical features typical features of document capture software include barcode recognition patch code recognition separation optical character recognition optical character recognition ocr optical mark recognition optical mark recognition omr quality assurance indexing migration goal for implementation of a document capture solution the goal for implementing a document capture solution is to reduce the amount of time spent in the scanning and capture process and produce metadata along with an image file and or ocr text this information is then migrated to a document management system document management or enterprise content management enterprise content management system these systems often provide a search function allowing search of the assets based on the produced metadata and then viewed using document imaging software document capture system solutions general integration with document management system main enterprise content management ecm enterprise content management and their dms component document management system are being adopted by many organizations as a corporate document management system for all types of electronic files e g ms word pdf however much of the information held by organisations is on paper and this needs to be integrated within the same document repository by converting paper documents into digital format through scanning companies can convert paper into image formats such as tif and jpg and also extract valuable index information or business data from the document using ocr technology digital documents and associated metadata can easily be stored in the ecm in a variety of formats the most popular of these formats is pdf which not only provides an accurate representation of the document but also allows all the ocr text in the document to be stored behind the pdf image this format is known as pdf with hidden text or text searchable pdf this allows users to search for documents by using keywords in the metadata fields or by searching the content of pdf files across the repository advantages of scanning documents into a ecm dms information held on paper is usually just as valuable to organisations as the electronic documents that are generated internally often this information represents a large proportion of the day to day correspondence with suppliers and customers having the ability to manage and share this information internally through a document management system such as sharepoint can improve collaboration between departments or employees and also eliminate the risk of losing this information through disasters such as floods or fire organisations adopting an ecm dms often implement electronic workflow which allows the information held on paper to be included as part of an electronic business process and incorporated into a customer record file along with other associated office documents and emails for business critical documents such as purchase orders and supplier invoices digitising documents can help speed up business transactions as well as reduce manual effort involved in keying data into business systems such as crm erp and accounting scanned invoices can also be routed to managers for payment approval via email or an electronic workflow distributed capture solutions distributed document capture is a technology which allows the scanning of documents into a central server through the use of individual capture stations a variation of distributed capture is thin client document capture in which documents are scanned into a central server through the use of web browser one of these web based products was reviewed by aiim they said this product is a thin client distributed capture system that streamlines the process of acquiring and creating documents ref association for information and image management http www aiim org community product guide capture prevalent software quillix prevalent software quillix accessed august 29 2011 ref the streamlining is a result of several factors including the lack of software which needs to be installed at every scanning station and the variety of input sources from which documents can be captured this includes things like email fax or a watched folder jeff shuey director of business development at kodak makes a distinction between distributed capture and what he calls remote capture in an article publishing in aiim he said that the key difference between the two is whether or not the information that is captured from scanning needs to be sent to the centralized server if as he points out in his article the document just needs to be scanned and committed to a sharepoint system and doesn t need to be sent to some other centralized server this is just a remote capture situation ref association for information and image management http www aiim org community blogs expert remote or distributed scanning are they different remote or distributed scanning are they different accessed august 29 2011 ref there are document capture software comparisons available featuring some of the most relevant products emc captiva ibm datacap artsyl technologies or ephesoft and extracting performance facts and their most relevant features references references defaultsort document capture software category artificial intelligence applications category optical character recognition category data management category sharepoint'
b'refimprove date march 2008 a signed overpunch is a code used to store the sign mathematics sign of a number by changing the last digit it is used in cobol especially when using ebcdic its purpose is to save a character that would otherwise be used by the sign digit ref name encycark cite web url http www 3480 3590 data conversion com article signed fields html title tech talk cobol tutorials ebcdic to ascii conversion of signed fields accessdate 2008 03 15 ref the code is derived from the punched card ibm 80 column punched card formats and character codes hollerith punched card code where both a digit and a sign can be entered in the same card column the codes class wikitable style text align center code digit sign 0 minus j 1 minus k 2 minus l 3 minus m 4 minus n 5 minus o 6 minus p 7 minus q 8 minus r 9 minus 0 a 1 b 2 c 3 d 4 e 5 f 6 g 7 h 8 i 9 examples 10 is 100 br 45a is 451 decimal points are usually implied and not explicitly stated in the text using numbers with two decimal digits 1000 is 100 00 references reflist defaultsort signed overpunch category computer programming category punched card category data management category history of software'
b'data management api dmapi is the interface defined in the x open document systems management data storage management xdsm api dated february 1997 xfs ibm jfs file system jfs vxfs advfs stornext and gpfs file systems support dmapi for hierarchical storage management hierarchical storage management hsm external links http pubs opengroup org onlinepubs 9657099 systems management data storage management xdsm api http publib boulder ibm com infocenter clresctr vxrx topic com ibm cluster gpfs34 dmapi doc bl1dmp bookmap xtoc html gpfs v3 4 data management api guide http oss sgi com projects xfs open source xfs source code with dmapi implementation and test suite category data management compu storage stub'
b'data binding is a general technique that binds data sources from the provider and consumer together and data synchronization synchronizes them this is usually done with two data information sources with different languages as in xml data binding in ui data binding data and information objects of the same language but different logic function are bound together e g java programming language java user interface ui elements to java objects ref cite web url https www techopedia com definition 15652 data binding title what is data binding work techopedia com accessdate 30 december 2015 ref in a data binding process each data change is reflected automatically by the elements that are bound to the data the term data binding is also used in cases where an outer representation of data in an element changes and the underlying data is automatically updated to reflect this change as an example a change in a text box code textbox code element could modify the underlying data value ref cite web url https msdn microsoft com en us library ms752347 v vs 110 aspx title data binding overview work microsoft developer network publisher microsoft access date 29 december 2016 ref data binding frameworks and tools embarcadero delphi delphi dsharp 3rd party data binding tool openwire library openwire visual live binding 3rd party visual data binding tool livebindings c sharp programming language c windows presentation foundation javascript angularjs backbone js bindingjs datum js ref cite web url http datumjs com title datum js accessdate 7 november 2016 ref emberjs generic data binder knockoutjs react javascript library sap open ui5 vue js java programming language java google web toolkit objective c akabeacon ios data binding framework scala programming language scala binding scala ref cite web url https github com thoughtworksinc binding scala title binding scala accessdate 30 december 2016 ref reactive data binding for scala see also windows presentation foundation xml data binding ui data binding bound property references references further reading cite book last noyes first brian title data binding with windows forms 2 0 programming smart client data applications with net url https books google com books id rxpthgj5w2cc date 12 january 2006 publisher pearson education isbn 978 0 321 63010 0 defaultsort data binding categories category data management'
b'unreferenced date february 2013 enterprise manufacturing intelligence emi or simply manufacturing intelligence mi is a term which applies to software used to bring a corporation s manufacturing related data together from many sources for the purposes of reporting analysis visual summaries and passing data between enterprise level and plant floor systems as data is combined from multiple sources it can be given a new structure or context that will help users find what they need regardless of where it came from the primary goal is to turn large amounts of manufacturing data into real knowledge and drive business results based on that knowledge reflist core functions amr research has identified five core functions every enterprise manufacturing intelligence application should possess aggregation making available data from many sources most often databases contextualization providing a structure or model for the data that will help users find what they need usually a folder tree utilizing a hierarchy such as the isa 95 standard analysis enabling users to analyze data across sources and especially across production sites this often includes the ability for true ad hoc reporting visualization providing tools to create visual summaries of the data to alert decision makers and call attention to the most important information of the moment the most common visualization tool is the dashboard business dashboard propagation automating the transfer of data from the plant floor up to enterprise level systems or vice versa defaultsort enterprise manufacturing intelligence category data management'
b'image lom base schema svg 340px right thumb a schematic representation of the hierarchy of elements in the lom data model learning object metadata is a data model usually encoded in xml used to describe a learning object and similar digital resources used to support learning the purpose of learning object metadata is to support the reusability of learning objects to aid discoverability and to facilitate their interoperability usually in the context of online learning management systems lms the ieee 1484 12 1 2002 standard for learning object metadata is an internationally recognised open standard published by the institute of electrical and electronics engineers standards association new york for the description of learning object s relevant attributes of learning objects to be described include type of object author owner terms of distribution format and pedagogy pedagogical attributes such as teaching or interaction style ieee 1484 12 1 2002 standard for learning object metadata in brief the ieee working group that developed the standard defined learning objects for the purposes of the standard as being any entity digital or non digital that may be used for learning education or training this definition has struck many commentators as being rather broad in its scope but the definition was intended to provide a broad class of objects to which lom metadata might usefully be associated rather than to give an instructional or pedagogic definition of a learning object ieee 1484 12 1 is the first part of a multipart standard and describes the lom data model the lom data model specifies which aspects of a learning object should be described and what vocabularies may be used for these descriptions it also defines how this data model can be amended by additions or constraints other parts of the standard are being drafted to define bindings of the lom data model i e define how lom records should be represented in xml and resource description framework rdf ieee 1484 12 3 and ieee 1484 12 4 respectively this article focuses on the lom data model rather than issues relating to xml or other bindings ims global learning consortium is an international consortium that contributed to the drafting of the ieee learning object metadata together with the ariadne foundation and endorsed early drafts of the data model as part of the ims learning resource meta data specification ims lrm versions 1 0 1 2 2 feedback and suggestions from the implementers of ims lrm fed into the further development of the lom resulting in some drift between version 1 2 of the ims lrm specification and what was finally published at the lom standard version 1 3 of the ims lrm specification realigns the ims lrm data model with the ieee lom data model and specifies that the ieee xml binding should be used thus we can now use the term lom in referring to both the ieee standard and version 1 3 of the ims specification the ims lrm specification also provides an extensive best practice and implementation guide and an xsl transform that can be used to migrate metadata instances from the older versions of the ims lrm xml binding to the ieee lom xml binding technical details how the data model works the lom comprises a hierarchy of elements as shown in the diagram top right at the first level there are nine categories each of which contains sub elements these sub elements may be simple elements that hold data or may themselves be aggregate elements which contain further sub elements the semantics of an element are determined by its context they are affected by the parent or container element in the hierarchy and by other elements in the same container for example the various description elements 1 4 5 10 6 3 7 2 2 8 3 and 9 3 each derive their context from their parent element in addition description element 9 3 also takes its context from the value of element 9 1 purpose in the same instance of classification the data model specifies that some elements may be repeated either individually or as a group for example although the elements 9 2 description and 9 1 purpose can only occur once within each instance of the classification container element the classification element may be repeated thus allowing many descriptions for different purposes the data model also specifies the value space and datatype for each of the simple data elements the value space defines the restrictions if any on the data that can be entered for that element for many elements the value space allows any string of unicode character to be entered whereas other elements entries must be drawn from a declared list i e a controlled vocabulary or must be in a specified format e g date and language codes some element datatypes simply allow a string of characters to be entered and others comprise two parts as described below langstring items contain language and string parts allowing the same information to be recorded in multiple languages vocabulary items are constrained in such a way that their entries have to be chosen from a controlled list of terms composed of source value pairs with the source containing the name of the list of terms being used and the value containing the chosen term datetime and duration items contain one part that allows the date or duration to be given in a machine readable format and a second that allows a description of the date or duration for example mid summer 1968 when implementing the lom as a data or service provider it is not necessary to support all the elements in the data model nor need the lom data model limit the information which may be provided the creation of an application profile allows a community of users to specify which elements and vocabularies they will use elements from the lom may be dropped and elements from other metadata schemas may be brought in likewise the vocabularies in the lom may be supplemented with values appropriate to that community requirements the key requirements for exploiting the lom as a data or service provider are to understand user community needs and to express these as an application profile have a strategy for creating high quality metadata store this metadata in a form which can be exported as lom records agree a binding for lom instances when they are exchanged be able to exchange records with other systems either as single instances or en masse related specifications there are many metadata specifications of particular interest is the dublin core metadata element set commonly known as simple dublin core standardised as ansi niso z39 85 2001 simple dublin core dc provides a non complex loosely defined set of elements which is useful for sharing metadata across a wide range of disparate services since the lom standard used dublin core as a starting point refining the simple dc schema with qualifiers relevant to learning objects there is some overlap between the lom and dc standards ref cite book last1 miller first1 steven j title metadata for digital collections a how to do it manual date 2011 publisher ala neal schuman location chicago isbn 978 1 55570 746 0 pages 56 ref the dublin core metadata initiative is also working on a set of terms which allow the dublin core element set to be used with greater semantic precision qualified dublin core the dublin education working group aims to provide refinements of dublin core for the specific needs of the education community many other education related specifications allow for lo metadata to be embedded within xml instances such as describing the resources in an ims content package or resource list describing the vocabularies and terms in an ims vdex vocabulary definition and exchange file and describing the question items in an ims qti question and test interoperability file the ims vdex ims vocabulary definition and exchange vdex specification has a double relation with the lom since not only can the lom provide metadata on the vocabularies in a vdex instance but vdex can be used to describe the controlled vocabularies which are the value space for many lom elements lom records can be transported between systems using a variety of protocols perhaps the most widely used being oai pmh application profiles uk lom core for uk further and higher education the most relevant family of application profiles are those based around the uk lom core ref http zope cetis ac uk profiles uklomcore ref the uk lom core is currently a draft schema researched by a community of practitioners to identify common uk practice in learning object content by comparing 12 metadata schemas uk lom is currently legacy work it is not in active development cancore cancore provides detailed guidance for the interpretation and implementation of each data element in the lom standard ref name cancore cite web url http cancore tru ca en guidelines html title cancore guidelines introduction author norm friesen publisher athabasca university date 2003 01 20 accessdate 2009 02 23 display authors etal ref these guidelines 2004 constitute a 250 page document and have been developed over three years under the leadership of norm friesen and through consultation with experts across canada and throughout the world these guidelines are also available at no charge from the cancore website anz lom anz lom is a metadata profile developed for the education sector in australia and new zealand the profile sets obligations for elements and illustrates how to apply controlled vocabularies including example regional vocabularies used in the classification element the anz lom profile was first published by the le rning federation tlf in january 2008 vetadata the australian vocational training and education vet sector uses an application profile of the ieee lom called vetadata the profile contains five mandatory elements and makes use of a number of vocabularies specific to the australian vet sector this application profile was first published in 2005 the vetadata and anz lom profiles are closely aligned norlom norlom is the norwegian lom profile the profile is managed by nssl the norwegian secretariat for standardization of learning technologies isracore isracore is the israeli lom profile the israel internet association isoc il and inter university computational center iucc have teamed up to manage and establish an e learning objects database swe lom swe lom is the swedish lom profile that is managed by iml at ume\xc3\xa5 university as a part of the work with the national standardization group tk450 at swedish standards institute twlom twlom is the taiwanese lom profile that is managed by industrial development and promotion of archives and e learning project lom fr lom fr is a metadata profile developed for the education sector in france this application profile was first published in 2006 nl lom nl lom is the dutch metadata profile for educational resources in the netherlands this application profile was the result of merging the dutch higher education lom profile with the one used in primary and secondary dutch education the final version was released in 2011 lom ch lom ch is a metadata profile developed for the education sector in switzerland it is currently available in french and german this application profile was published in july 2014 lom es lom es is a metadata profile developed for the education sector in spain it is available in spanish lom gr lom gr also known as lom gr photodentro is the greek lom application profile for educational resources currently being used for resources related to school education it was published in 2012 and is currently available in greek and english ref https git dschool edu gr photodentro lom gr ref it is maintained by cti diophantus as part of the photodentro federated architecture for educational content for schools that includes a number of educational content repositories for learning objects educational video and user generated content and the greek national aggregator of educational content accumulating metadata from collections stored in repositories of other organizations ref name photodentro lor cite journal last1 megalou first1 elina last2 kaklamanis first2 christos title photodentro lor the greek national learning object repository journal inted2014 proceedings date 10 12 march 2014 pages 309 319 url https library iated org view megalou2014pho accessdate 7 april 2016 series 8th international technology education and development conference publisher iated location valencia spain issn 2340 1079 ref lom gr is a working specification of the tc48 wg3 working group of the hellenic organization for standardization others other application profiles are those developed by the celebrate project ref european schoolnet http web archive org web 20071225053548 http www eun org ww en pub celebrate help application profile htm celebrate application profile 2003 ref and the metadata profile that is part of the scorm reference model ref adl http www adlnet gov capabilities scorm tab learn scorm ref see also application profile content package dublin core ims global learning object http dublincore org dcx lrmi terms 1 1 lrmi learning resource metadata initiative metadata metadata standards metadata standards oai pmh scorm xml m learning object metadata references reflist external links wikiversity introduction to learning objects http cancore athabascau ca en cancore athabascau ca is a thorough element by element guide to implementing the ieee lom http www imsglobal org metadata www imsglobal org ims global learning consortium learning resource meta data specification http ltsc ieee org wg12 files ieee 1484 12 03 d8 submitted pdf ltsc ieee org xml binding specification http www intrallect com support metadata ims2lom metadata mapping htm www intrallect com a mapping between the ieee lom and ims learning resource metadata http www ontopia net topicmaps materials tm vs thesauri html www ontopia net metadata thesauri taxonomies topic maps making sense of it all 2004 prone to spam date october 2014 z148 no more links please be cautious adding more external links wikipedia is not a collection of links and should not be used for advertising excessive or inappropriate links will be removed see wikipedia external links and wikipedia spam for details if there are already suitable links propose additions or replacements on the article s talk page or submit your link to the relevant category at dmoz dmoz org and link there using dmoz use dmy dates date october 2010 defaultsort learning object metadata category data management category educational technology category knowledge representation category library science category metadata category standards category technical communication'
b'a learning object is a collection of content items practice items and assessment items that are combined based on a single learning objective ref citation last cisco systems title reusable information object strategy url http www cisco com warp public 779 ibs solutions learning whitepapers el cisco rio pdf ref the term is credited to wayne hodgins and dates from a working group in 1994 bearing the name ref citation last gerard first r w title shaping the mind computers in education in n a sciences applied science and technological progress url https books google com books id btcraaaayaaj year 1967 ref the concept encompassed by learning objects is known by numerous other terms including content objects chunks educational objects information objects intelligent objects knowledge bits knowledge objects learning components media objects reusable curriculum components nuggets reusable information objects reusable learning objects testable reusable units of cognition training components and units of learning the core idea of the use of learning objects is characterized by the following discoverability reusability and interoperability to support discoverability learning objects are described by learning object metadata formalized as ieee 1484 12 learning object metadata ref http 129 115 100 158 txlor docs ieee lom 1484 12 1 v1 final draft pdf ieee 1484 12 learning object metadata ref to support reusability the ims consortium proposed a series of specifications such as the ims content package and to support interoperability the u s military s advanced distributed learning organization created the sharable content object reference model ref http legacy adlnet gov technologies scorm scormsdocuments 2004 204th 20edition overview aspx scorm 2004 4th edition version 1 1 overview ref learning objects were designed in order to reduce the cost of learning standardize learning content and to enable the use and reuse of learning content by learning management systems ref http www irrodl org index php irrodl article view 32 378 stephen downes learning objects resources for distance education worldwide the international review of research in open and distributed learning volume 2 number 1 2001 athabasca university press ref definitions the institute of electrical and electronics engineers ieee defines a learning object as any entity digital or non digital that may be used for learning education or training ref harvnb learning technology standards committee 2002 p 45 ref chiappe defined learning objects as a digital self contained and reusable entity with a clear educational purpose with at least three internal and editable components content learning activities and elements of context the learning objects must have an external structure of information to facilitate their identification storage and retrieval the metadata ref harvnb chiappe segovia rincon 2007 p 8 ref the following definitions focus on the relation between learning object and digital media rlo cetl a british inter university learning objects center defines reusable learning objects as web based interactive chunks of e learning designed to explain a stand alone learning objective ref citation chapter learning objects url http www rlo cetl ac uk joomla index php option com content task view id 235 itemid 28 title rlo cetl reusable learning objects accessdate 2008 04 29 ref daniel rehak and robin mason define it as a digitized entity which can be used reused or referenced during technology supported learning ref http 129 115 100 158 txlor docs ieee lom 1484 12 1 v1 final draft pdf ref ref harvnb rehak mason 2003 p ref adapting a definition from the wisconsin online resource center robert j beck suggests that learning objects have the following key characteristics learning objects are a new way of thinking about learning content traditionally content comes in a several hour chunk learning objects are much smaller units of learning typically ranging from 2 minutes to 15 minutes are self contained each learning object can be taken independently are reusable a single learning object may be used in multiple contexts for multiple purposes can be aggregated learning objects can be grouped into larger collections of content including traditional course structures are tagged with metadata every learning object has descriptive information allowing it to be easily found by a search ref name beck citation last beck first robert j chapter what are learning objects url http www4 uwm edu cie learning objects cfm gid 56 title learning objects publisher center for international education university of wisconsin milwaukee accessdate 2008 04 29 ref components the following is a list of some of the types of information that may be included in a learning object and its metadata general course descriptive data including course identifiers language of content english spanish etc subject area maths reading etc descriptive text descriptive keywords life cycle including version status instructional content including text web pages images sound video glossary of terms including terms definition acronyms quizzes and assessments including questions answers rights including cost copyrights restrictions on use relationships to other courses including prerequisite courses educational level including grade level age range typical learning time and difficulty ieee 1484 12 1 2002 typology as defined by churchill 2007 presentation practice simulation conceptual models information and contextual representation ref name churchill churchill d 2007 towards a useful classification of learning objects educational technology research development 55 5 479 497 ref metadata one of the key issues in using learning objects is their identification by search engines or content management systems citation needed date april 2008 this is usually facilitated by assigning descriptive learning object metadata just as a book in a library has a record in the card catalog learning objects must also be tagged with metadata the most important pieces of metadata typically associated with a learning object include objective the educational objective the learning object is instructing prerequisites the list of skills typically represented as objectives which the learner must know before viewing the learning object topic typically represented in a taxonomy the topic the learning object is instructing interactivity the interaction model of the learning object technology requirements the required system requirements to view the learning object mutability a mutated learning object is according to michael shaw a learning object that has been re purposed and or re engineered changed or simply re used in some way different from its original intended design shaw also introduces the term contextual learning object to describe a learning object that has been designed to have specific meaning and purpose to an intended learner ref harvnb shaw 2003 ref portability before any institution invests a great deal of time and energy into building high quality e learning content which can cost over 10 000 per classroom hour ref rumble greville 2001 the cost and costing of networked learning journal of asynchronous learning networks volume 5 issue 2 ref it needs to consider how this content can be easily loaded into a learning management system it is possible for example to package learning objects with scorm specification and load it in moodle learning management system or desire2learn learning environment if all of the properties of a course can be precisely defined in a common format the content can be serialized into a standard format such as xml and loaded into other systems when it is considered that some e learning courses need to include video mathematical equations using mathml chemistry equations using chemical markup language cml and other complex structures the issues become very complex especially if the systems needs to understand and validate each structure and then place it correctly in a database citation needed date april 2008 criticism in 2001 david wiley criticized learning object theory in his paper https web archive org web 20041019162710 http rclt usu edu whitepapers paradox html the reusability paradox which is http www darcynorman net 2003 08 21 addressing the reusability paradox summarized by d arcy norman as if a learning object is useful in a particular context by definition it is not reusable in a different context if a learning object is reusable in many contexts it isn t particularly useful in any in http www learningspaces org papers objections html three objections to learning objects and e learning standards norm friesen canada research chair in e learning practices at thompson rivers university points out that the word neutrality in itself implies a state or position that is antithetical to pedagogy and teaching see also intelligent tutoring system north carolina learning object repository nclor serious games references reflist 30em further reading citation last beck first robert j title what are learning objects learning objects center for international education university of wisconsin milwaukee url http www4 uwm edu cie learning objects cfm gid 56 year 2009 accessdate 2009 10 23 citation last learning technology standards committee title draft standard for learning object metadata ieee standard 1484 12 1 place new york publisher institute of electrical and electronics engineers year 2002 url http ltsc ieee org wg12 files lom 1484 12 1 v1 final draft pdf format pdf accessdate 2008 04 29 citation last1 rehak first1 daniel r first2 robin last2 mason chapter engaging with the learning object economy editor first allison editor last littlejohn title reusing online resources a sustainable approach to e learning place london publisher kogan page year 2003 pages 22 30 isbn 978 0 7494 3949 1 citation last shaw first michael chapter contextual and mutated learning objects in the context of design learning and re use url http www shawmultimedia com edtech oct 03 html title teaching and learning with technology date october 2003 accessdate 2008 04 29 citation first1 andr\xc3\xa9s chiappe last1 laverde first2 yasbley segovia last2 cifuentes first3 helda yadira rinc\xc3\xb3n last3 rodr\xc3\xadguez chapter toward an instructional design model based on learning objects editor first springer editor last boston title educational technology research and development place boston year 2007 pages 671 81 publisher springer us doi 10 1007 s11423 007 9059 0 issue 6 volume 55 issn 1042 1629 id print issn 1556 6501 online url http www springerlink com content u84w63873vq77h2h p 41be7fbeef9648ee9b554f1835112005 pi 6 accessdate 2008 08 21 spanish draf http andreschiappe blogspot com 2007 09 que es un objeto de aprendizaje what is html blog de andr\xc3\xa9s chiappe objetos de aprendizaje citation last northrup first pamela title learning objects for instruction design and evaluation place usa publisher information science publishing year 2007 format book citation last1 hunt first1 john p last2 bernard first2 robert title an xml based information architecture for learning content ibm developerworks url http www ibm com developerworks xml library x dita9a year 2005 accessdate 2005 08 05 churchill d 2007 towards a useful classification of learning objects educational technology research development 55 5 479 497 innayah creating an audio script with learning object unpublished 2013 external links the http www4 uwm edu cie learning objects cfm gid 55 learning objects at milwaukee s center for international education defaultsort learning object category data management category educational materials category educational technology'
b'a database schema ipac en \xcb\x88 s k i m \xc9\x99 respell skee m\xc9\x99 of a database system is its structure described in a formal language supported by the database management system dbms the term schema refers to the organization of data as a blueprint of how the database is constructed divided into database tables in the case of relational databases the formal definition of a database schema is a set of formulas sentences called integrity constraints imposed on a database citation needed date january 2016 these integrity constraints ensure compatibility between parts of the schema all constraints are expressible in the same language a database can be considered a structure in realization of the database language ref name source1 the states of a created conceptual schema are transformed into an explicit and implicit methods explicit mapping the database schema this describes how real world entities are modeled in the database a database schema specifies based on the database administrator s knowledge of possible applications the facts that can enter the database or those of interest to the possible end user s ref name source3 the notion of a database schema plays the same role as the notion of theory in predicate calculus a model of this theory closely corresponds to a database which can be seen at any instant of time as a mathematical object thus a schema can contain formulas representing data integrity types of integrity constraints integrity constraints specifically for an application and the constraints specifically for a type of database all expressed in the same database language ref name source1 in a relational database the schema defines the table database tables field computer science fields relational model relationship s view database view s index database index es software package installation package s stored procedure procedure s subroutine function s queue data structure queue s database trigger trigger s data type type s sequence s materialized view s synonym database synonym s database link s directory file systems directories xml schema s and other elements a database generally stores its schema in a data dictionary although a schema is defined in text database language the term is often used to refer to a graphical depiction of the database structure in other words schema is the structure of the database that defines the objects in the database in an oracle database system the term schema has a slightly different connotation ideal requirements for schema integration see also database normalization the requirements listed below influence the detailed structure of schemas that are produced certain applications will not require that all of these conditions are met but these four requirements are the most ideal overlap preservation each of the overlapping elements specified in the input mapping is also in a database schema relation ref name source2 extended overlap preservation source specific elements that are associated with a source s overlapping elements are passed through to the database schema ref name source2 normalization independent entities and relationships in the source data should not be grouped together in the same relation in the database schema in particular source specific schema elements should not be grouped with overlapping schema elements if the grouping co locates independent entities or relationships ref name source2 minimality if any elements of the database schema are dropped then the database schema is not ideal ref name source2 example of two schema integrations suppose we want a mediated database schema to integrate two travel databases go travel and ok travel code go travel code has two relations syntaxhighlight lang text go flight f num time meal yes no go price f num date price syntaxhighlight code f num code being the flight number code ok travel code has just one relation syntaxhighlight lang text ok flight f num date time price nonstop yes no syntaxhighlight the overlapping information in ok travel s and go travel s schemas could be represented in a mediated schema ref name source2 syntaxhighlight lang text flight f num date time price syntaxhighlight oracle database specificity in the context of oracle database s a schema object is a logical database storage structures data storage structure ref cite book first1 lance last1 ashdown first2 tom last2 kyte others et al title oracle database concepts 11g release 2 11 2 url http download oracle com docs cd e11882 01 server 112 e10713 tablecls htm cncpt111 accessdate 2010 04 14 date february 2010 publisher oracle corporation quote a database schema is a logical container for data structures called schema objects examples of schema objects are tables and indexes ref an oracle database associates a separate schema with each database user ref cite book title oracle database concepts 10g release 2 10 2 part number b14220 02 url http docs oracle com cd b19306 01 server 102 b14220 schema htm accessdate 2012 11 26 quote a schema is a collection of logical structures of data or schema objects a schema is owned by a database user and has the same name as that user each user owns a single schema schema objects can be created and manipulated with sql ref a schema comprises a collection of schema objects examples of schema objects include table database tables view database views sequence s synonym database synonyms index database indexes clusters database links snapshot computer storage snapshot s stored procedure procedure s functions packages on the other hand non schema objects may include ref cite book first1 lance last1 ashdown author1 link first2 tom last2 kyte author2 link others et al title oracle database concepts 11g release 2 11 2 url http download oracle com docs cd e11882 01 server 112 e10713 tablecls htm cncpt111 accessdate 2010 04 14 date february 2010 publisher oracle corporation location isbn quote other types of objects are also stored in the database and can be created and manipulated with sql statements but are not contained in a schema these objects include database users roles contexts and directory objects ref users roles contexts directory objects schema objects do not have a one to one correspondence to physical files on disk that store their information however oracle database s store schema objects logically within a tablespace of the database the data of each object is physically contained in one or more of the tablespace s datafile s for some objects such as tables indexes and clusters a database administrator can specify how much disk space the oracle rdbms allocates for the object within the tablespace s datafiles there is no necessary relationship between schemas and tablespaces a tablespace can contain objects from different schemas and the objects for a single schema can reside in different tablespaces see also too many see alsos date july 2013 core architecture data model cadm data definition language ddl database design data dictionary data element data modeling data mapping database integrity entity relationship model knowledge representation and reasoning object role modeling relational algebra schema matching three schema approach references reflist refs ref name source1 cite journal last rybinski first h year 1987 title on first order logic databases journal acm transactions on database systems volume 12 issue 3 pages 325 349 doi 10 1145 27629 27630 ref ref name source2 cite journal last pottinger first p last2 berstein first2 p year 2008 title schema merging and mapping creation for relational sources journal proceedings of the 11th international conference on extending database technology advances in database technology edbt 08 publisher acm location new york ny pages 73 84 doi 10 1145 1353343 1353357 ref ref name source3 cite journal last imielinski first t last2 lipski first2 w year 1982 title a systematic approach to relational database theory journal proceedings of the 1982 acm sigmod international conference on management of data sigmod 82 publisher acm location new york ny pages 8 14 doi 10 1145 582353 582356 ref external links https weblogs asp net scottgu tip 2f00 trick 3a00 online database schema samples library tip trick online database schema samples library http web archive org web 20081217074637 http msdn microsoft com en us library bb187299 28sql 80 29 aspx database schema samples http web archive org web 20080828210315 http ciobriefings com publications whitepapers designingthestarschemadatabase tabid 101 default aspx designing the star schema database defaultsort database schema category data management category data modeling'
b'file 3 dashboards jpg thumb 200px business dashboards dashboards often provide at a glance views of kpis key performance indicators relevant to a particular objective or business process e g sales marketing human resources or production economics production ref michael alexander and john walkenbach excel dashboards and reports wiley 2010 ref in real world terms dashboard is another name for progress report or report often the dashboard is displayed on a web page that is linked to a database which allows the report to be constantly updated for example a manufacturing dashboard may show numbers related to productivity such as number of parts manufactured or number of failed quality inspections per hour similarly a human resources dashboard may show numbers related to staff recruitment retention and composition for example number of open positions or average days or cost per recruitment ref name briggs cite web url http www targetdashboard com site dashboard best practice management report and dashboard best practice index aspx title management reports dashboard best practice last briggs first jonathan publisher target dashboard accessdate 18 february 2013 ref the term dashboard originates from the automobile dashboard where drivers monitor the major functions at a glance via the instrument cluster benefits digital dashboards allow managers to monitor the contribution of the various departments in their organization to gauge exactly how well an organization is performing overall digital dashboards allow you to capture and report specific data points from each department within the organization thus providing a snapshot of performance benefits of using digital dashboards include ref name briggs visual presentation of performance measures ability to identify and correct negative trends measure efficiencies inefficiencies ability to generate detailed reports showing new trends ability to make more informed decisions based on collected business intelligence align strategies and organizational goals saves time compared to running multiple reports gain total visibility of all systems instantly quick identification of data outliers and correlations classification dashboards can be broken down according to role and are either strategic analytical operational or informational ref steven few information dashboard design the effective visual communication of data o reilly 2006 ref strategic dashboards support managers at any level in an organization and provide the quick overview that decision makers need to monitor the health and opportunities of the business dashboards of this type focus on high level measures of performance and forecasts strategic dashboards benefit from static snapshots of data daily weekly monthly and quarterly that are not constantly changing from one moment to the next dashboards for analytical purposes often include more context comparisons and history along with subtler performance evaluators analytical dashboards typically support interactions with the data such as drilling down into the underlying details dashboards for monitoring operations are often designed differently from those that support strategic decision making or data analysis and often require monitoring of activities and events that are constantly changing and might require attention and response at a moment s notice types of dashboards digital dashboards may be laid out to track the flows inherent in the business processes that they monitor graphically users may see the high level processes and then data drilling drill down into low level data this level of detail is often buried deep within the corporate enterprise and otherwise unavailable to the senior executives three main types of digital dashboard dominate the market today stand alone software applications web browser based applications and desktop applications also known as desktop widgets the last are driven by a software widget widget engine specialized dashboards may track all corporate functions examples include human resources recruitment recruiting sales business operations operations security information technology project management customer relationship management and many more departmental dashboards for a smaller organization like a startup a compact startup scorecard dashboard tracks important activities across lot of domains ranging from social media to sales cn date september 2016 digital dashboard projects involve business units as the driver and the information technology department as the enabler the success of digital dashboard projects often depends on the measurement metrics that were chosen for monitoring key performance indicator s balanced scorecard s and sales performance figures are some of the content appropriate on business dashboards dashboards and scoreboards balanced scoreboards and dashboards have been linked together as if they were interchangeable however although both visually display critical information the difference is in the format scoreboards can open the quality of an operation while dashboards provide calculated direction a balanced scoreboard has what they called a prescriptive format it should always contain these components active strategy perspectives groupings of high level strategic areas objectives verb noun phrases pulled from a strategy plan measures also called metric or key performance indicators kpis spotlight indicators red yellow or green symbols that provide an at a glance view of a measure s performance each of these sections ensures that a balanced scorecard is essentially connected to the businesses critical strategic needs the design of a dashboard is more loosely defined dashboards are usually a series of graphics charts gauges and other visual indicators that can be monitored and interpreted even when there is a strategic link on a dashboard it may not be noticed as such since objectives are not normally present on dashboards however dashboards can be customized to link their graphs and charts to strategic objectives ref zsl inc dashboards vs scorecards an insight zsl inc 2006 ref design digital dashboard technology is available out of the box from many software providers some companies however continue to do in house development and maintenance of dashboard applications for example ge aviation has developed a proprietary software portal called digital cockpit to monitor the trends in aircraft spare parts business a good information design will clearly communicate key information to users and makes supporting information easily accessible ref stacey barr 7 small business dashboard design dos and don ts barr 2010 ref assessing the quality of dashboards there are four key elements to a good dashboard ref victoria hetherington dashboard demystified what is a dashboard hetherington 2009 ref simple communicates easily minimum distractions it could cause confusion supports organized business with meaning and useful data applies human visual perception to visual presentation of information history the idea of digital dashboards followed the study of decision support system s in the 1970s early predecessors of the modern business dashboard were first developed in the 1980s in the form of executive information systems eiss due to problems primarily with data refreshing and handling it was soon realized that the approach wasn t practical as information was often incomplete unreliable and spread across too many disparate sources ref steven few information dashboard design the effective visual communication of data o reilly 2006 ref thus eiss hibernated until the 1990s when the information age quickened pace and data warehousing and online analytical processing olap allowed dashboards to function adequately fact date august 2014 despite the availability of enabling technologies the dashboard use didn t become popular until later in that decade with the rise of key performance indicators kpis and the introduction of robert s kaplan and david p norton s balanced scorecard ref wayne w eckerson performance dashboards measuring monitoring and managing your business wiley 2010 ref in the late 1990s microsoft promoted a concept known as the digital nervous system and digital dashboards were described as being one leg of that concept ref cite web url http www kmworld com articles news breaking news microsoft refines digital dashboard concept 12189 aspx title microsoft refines digital dashboard concept accessdate 2009 06 09 ref today the use of dashboards forms an important part of business performance management bpm see also business activity monitoring complex event processing corporate performance management data presentation architecture enterprise manufacturing intelligence event stream processing infographic information graphics information design scientific visualization references reflist further reading cite book title information dashboard design last few first stephen publisher o reilly isbn 978 0 596 10016 2 date 2006 cite book title performance dashboards measuring monitoring and managing your business last eckerson first wayne w author link publisher john wiley sons isbn 978 0 471 77863 9 date 2006 data warehouse defaultsort dashboard business use dmy dates date april 2011 category business terms category computing terminology category data warehousing category data management category business software category information systems'
b'primary sources article date march 2009 grid oriented storage gos was a term used for data storage by a university project during the era when the term grid computing was popular description gos was a successor of the term network attached storage nas gos systems contained hard disks often raid s redundant arrays of independent disks like traditional file servers image gosongrid jpg thumb upright 1 4 gos was designed to deal with long distance cross domain and single image file operations which is typical in grid environments gos behaves like a file server via the file based gos fs protocol to any entity on the grid similar to advanced resource connector gridftp gos fs integrates a parallel stream engine and grid security infrastructure gsi conforming to the universal vfs virtual filesystem switch gos fs can be pervasively used as an underlying platform to best utilize the increased transfer bandwidth and accelerate the network file system protocol nfs cifs based applications gos can also run over scsi fibre channel or iscsi which does not affect the acceleration performance offering both file level protocols and block level protocols for storage area network san from the same system in a grid infrastructure resources may be geographically distant from each other produced by differing manufacturers and have differing access control policies this makes access to grid resources dynamic and conditional upon local constraints centralized management techniques for these resources are limited in their scalability both in terms of execution efficiency and fault tolerance provision of services across such platforms requires a distributed resource management mechanism and the peer to peer clustered gos appliances allow a single storage image to continue to expand even if a single gos appliance reaches its capacity limitations the cluster shares a common aggregate presentation of the data stored on all participating gos appliances each gos appliance manages its own internal storage space the major benefit of this aggregation is that clustered gos storage can be accessed by users as a single mount point gos products fit the thin server categorization compared with traditional fat server based storage architectures thin server gos appliances deliver numerous advantages such as the alleviation of potential network grid bottle necks cpu and os optimized for i o only ease of installation remote management and minimal maintenance low cost and plug and play etc examples of similar innovations include nas printers fax machines routers and switches an apache server has been installed in the gos operating system ensuring an https based communication between the gos server and an administrator via a web browser remote management and monitoring makes it easy to set up manage and monitor gos systems history frank zhigang wang and na helian proposed a funding proposal to the uk government titled grid oriented storage gos next generation data storage system architecture for the grid computing era in 2003 the proposal was approved and granted one million pounds citation needed date march 2009 in 2004 the first prototype was constructed in 2005 at centre for grid computing cambridge cranfield high performance computing facility the first conference presentation was at ieee symposium on cluster computing and grid ccgrid 9 12 may 2005 cardiff uk as one of the five best work in progress it was included in the ieee distributed systems online in 2006 the gos architecture and its implementations was published in ieee transactions on computers titled grid oriented storage a single image cross domain high bandwidth architecture starting in january 2007 demonstrations were presented at princeton university cambridge university computer lab and others by 2013 the cranfield centre still used future tense for the project ref name cranfield cgc cite web url http www cranfield ac uk soe departments appliedmaths gridcomputing index html title centre for grid computing accessdate june 14 2013 publisher cranfield university ref peer to peer file sharing s use similar techniques notes reflist further reading frank wang na helian sining wu yuhui deng yike guo steve thompson ian johnson dave milward robert maddock grid oriented storage ieee distributed systems online volume 6 issue 9 sept 2005 frank wang sining wu na helian andy parker yike guo yuhui deng vineet khare grid oriented storage a single image cross domain high bandwidth architecture ieee transaction on computers vol 56 no 4 pp nbsp 474 487 2007 frank zhigang wang sining wu na helian an underlying data transporting protocol for accelerating web communications international journal of computer networks elsevier 2007 frank zhigang wang sining wu na helian yuhui deng vineet khare chris thompson and michael parker grid based data access to nucleotide sequence database with 6x improvement in response times new generation computing no 2 vol 25 2007 frank wang yuhui deng na helian evolutionary storage speeding up a magnetic disk by clustering frequent data ieee transactions on magnetics issue 6 vol 43 2007 frank zhigang wang na helian sining wu yuhui deng vineet khare chris thompson and michael parker grid based storage architecture for accelerating bioinformatics computing journal of vlsi signal processing systems no 1 vol 48 2007 yuhui deng and frank wang a heterogeneous storage grid enabled by grid service acm operating system review no 1 vol 41 2007 yuhui deng frank wang optimal clustering size of small file access in network attached storage device parallel processing letters no 1 vol 17 2007 defaultsort grid oriented storage category data management'
b'no footnotes date march 2016 in database s change data capture cdc is a set of software design pattern computer science design patterns used to determine and track the data that has changed so that action can be taken using the changed data also change data capture cdc is an approach to data integration that is based on the identification capture and delivery of the changes made to enterprise data sources cdc solutions occur most often in data warehouse data warehouse environments since capturing and preserving the state of data across time is one of the core functions of a data warehouse but cdc can be utilized in any database or data repository system methodology system developers can set up cdc mechanisms in a number of ways and in any one or a combination of system layers from application logic down to physical storage in a simplified cdc context one computer system has data believed to have changed from a previous point in time and a second computer system needs to take action based on that changed data the former is the source the latter is the target it is possible that the source and target are the same system physically but that would not change the design pattern logically not uncommonly multiple cdc solutions can exist in a single system timestamps on rows tables whose changes must be captured may have a column that represents the time of last change names such as last update etc are common any row in any table that has a timestamp in that column that is more recent than the last time data was captured is considered to have changed version numbers on rows database designers give tables whose changes must be captured a column that contains a version number names such as version number etc are common when data in a row changes its version number is updated to the current version a supporting construct such as a reference table with the current version in it is needed when a change capture occurs all data with the latest version number is considered to have changed when the change capture is complete the reference table is updated with a new version number three or four major techniques exist for doing cdc with version numbers the above paragraph is just one use in optimistic locking version numbers can be useful with optimistic locking in acid transactional or relational database management system relational database management systems rdmbs for an example in read then update scenarios for create read update and delete crud applications in relational database management system s a row is first read along with the state of its version number in a separate transaction a update sql sql update statement is executed along with an additional where sql where clause that includes the version number found from the initial read if no record was updated it usually means that the version numbers didn t match because some other action transaction had already updated the row and consequently its version number several object relational mapper object relational mapping tools use this method to detect for optimistic locking scenarios including hibernate java hibernate status indicators on rows this technique can either supplement or complement timestamps and versioning it can configure an alternative if for example a status column is set up on a table row indicating that the row has changed e g a boolean column that when set to true indicates that the row has changed otherwise it can act as a complement to the previous methods indicating that a row despite having a new version number or an earlier date still shouldn t be updated on the target for example the data may require human validation time version status on rows this approach combines the three previously discussed methods as noted it is not uncommon to see multiple cdc solutions at work in a single system however the combination of time version and status provides a particularly powerful mechanism and programmers should utilize them as a trio where possible the three elements are not redundant or superfluous using them together allows for such logic as capture all data for version 2 1 that changed between 6 1 2005 12 00 a m and 7 1 2005 12 00 a m where the status code indicates it is ready for production triggers on tables may include a observer pattern publish subscribe pattern to communicate the changed data to multiple targets in this approach database trigger triggers log events that happen to the transactional table into another queue table that can later be played back for example imagine an accounts table when transactions are taken against this table triggers would fire that would then store a history of the event or even the deltas into a separate queue table the queue table might have schema with the following fields id tablename rowid timestamp operation the data inserted for our account sample might be 1 accounts 76 11 02 2008 12 15am update more complicated designs might log the actual data that changed this queue table could then be played back to replicate the data from the source system to a target more discussion needed an example of this technique is the pattern known as the log trigger event programming coding a change into an application at appropriate points is another method that can give intelligent discernment that data changed although this method involves programming vs more easily implemented dumb triggers it may provide more accurate and desirable cdc such as only after a commit or only after certain columns changed to certain values just what the target system is looking for log scanners on databases most database management systems manage a transaction log that records changes made to the database contents and to metadata by scanning and interpreting the contents of the database transaction log one can capture the changes made to the database in a non intrusive manner using transaction logs for change data capture offers a challenge in that the structure contents and use of a transaction log is specific to a database management system unlike data access no standard exists for transaction logs most database management systems do not document the internal format of their transaction logs although some provide programmatic interfaces to their transaction logs for example oracle db2 sql mp sql mx and sql server 2008 other challenges in using transaction logs for change data capture include coordinating the reading of the transaction logs and the archiving of log files database management software typically archives log files off line on a regular basis translation between physical storage formats that are recorded in the transaction logs and the logical formats typically expected by database users e g some transaction logs save only minimal buffer differences that are not directly useful for change consumers dealing with changes to the format of the transaction logs between versions of the database management system eliminating uncommitted changes that the database wrote to the transaction log and later rollback data management rolled back dealing with changes to the metadata of tables in the database cdc solutions based on transaction log files have distinct advantages that include minimal impact on the database even more so if one uses log shipping to process the logs on a dedicated host no need for programmatic changes to the applications that use the database low latency engineering latency in acquiring changes data integrity transactional integrity log scanning can produce a change stream that replays the original transactions in the order they were committed such a change stream include changes made to all tables participating in the captured transaction no need to change the database schema confounding factors as often occurs in complex domains the final solution to a cdc problem may have to balance many competing concerns unsuitable source systems change data capture both increases in complexity and reduces in value if the source system saves metadata changes when the data itself is not modified for example some data model s track the user who last looked at but did not change the data in the same structure as the data this results in noise signal processing noise in the change data capture tracking the capture actually tracking the changes depends on the data source if the data is being persisted in a modern database then change data capture is a simple matter of permissions two techniques are in common use tracking changes using database trigger s reading the transaction log as or shortly after it is written if the data is not in a modern database change data capture becomes a programming challenge push versus pull push the source process creates a snapshot of changes within its own process and delivers rows downstream the downstream process uses the snapshot creates its own subset and delivers them to the next process pull the target that is immediately downstream from the source prepares a request for data from the source the downstream target delivers the snapshot to the next target as in the push model alternatives sometimes the slowly changing dimension is used as a method this is an example file scd model png frame center scd model see also slowly changing dimension referential integrity references reflist external links https github com linkedin databus wiki linkedin databus http www informaticacloud com images whitepapers data 20replication 20best 20practices pdf data replication as a service best practices https web archive org web 20110902084451 http www attunity com 80 attunity stream attunity change data capture cdc http www 01 ibm com software data infosphere change data capture ibm infosphere cdc https web archive org web 20060523023144 http www oracle com 80 technology oramag oracle 03 nov o63tech bi html tutorial on setting up cdc in oracle 9i http social technet microsoft com wiki contents articles how to enable sql azure change data capture aspx tutorial on setting up sql azure change data capture http msdn2 microsoft com en us library bb522489 sql 100 aspx details of the cdc facility included in microsoft sql server 2008 feb 08 ctp http www jumpmind com products symmetricds features symmetricds heterogeneous cross platform cdc http www gamma soft com wp index php page id 30 gamma soft cdc technology http www talend com download talend open studio qt product tos download 3 qt product tos download talend defaultsort change data capture category computer data category data management'
b'more footnotes date august 2010 merge data steward date february 2016 in data governance data governance groups responsibilities for data management are increasingly divided between the business process owners and information technology it departments two functional titles commonly used for these roles are data steward data steward and data custodian data stewards are commonly responsible for data content context and associated business rules data custodians are responsible for the safe custody transport storage of the data and implementation of business rules ref carnegie mellon information security roles and responsibilities http www cmu edu iso governance roles data custodian html ref ref policies regulations and rules data management procedures reg 08 00 3 information technology nc state university http www ncsu edu policies informationtechnology reg08 00 3 php ref simply put data stewards are responsible for what is stored in a data field while data custodians are responsible for the technical environment and database structure common job titles for data custodians are database administrator dba data modeler and etl developer data custodian responsibilities a data custodian ensures access to the data is authorized and controlled data stewards are identified for each data set technical processes sustain data integrity processes exist for data quality issue resolution in partnership with data stewards technical controls safeguard data data added to data sets are consistent with the common data model versions of master data are maintained along with the history of changes change management practices are applied in maintenance of the database data content and changes can be audited see also data governance data steward references references references related links establishing data stewards by jonathan g geiger teradata magazine online september 2008 http www teradata com tdmo v08n03 features establishingdatastewards aspx a rose by any other name titles in data governance by anne marie smith ph d eiminstitute org archives volume 1 issue 13 march 2008 http www eiminstitute org library eimi archives volume 1 issue 13 march 2008 edition a rose by any other name 2013 titles in data governance defaultsort data custodian category information technology governance category data management category knowledge representation category library occupations category metadata category technical communication ar \xd9\x85\xd9\x8a\xd8\xaa\xd8\xa7\xd8\xaf\xd8\xa7\xd8\xaa\xd8\xa7 cs metadata da metadata de data steward et metaandmed es metadato eo meta dateno fr m\xc3\xa9tadonn\xc3\xa9e it metadata lv metadati hu metaadat nl metadata ja \xe3\x83\xa1\xe3\x82\xbf\xe3\x83\x87\xe3\x83\xbc\xe3\x82\xbf no metadata pl metadane pt metadados ru \xd0\xbc\xd0\xb5\xd1\x82\xd0\xb0\xd0\xb4\xd0\xb0\xd0\xbd\xd0\xbd\xd1\x8b\xd0\xb5 fi metatieto sv metadata th \xe0\xb9\x80\xe0\xb8\xa1\xe0\xb8\x97\xe0\xb8\xb2\xe0\xb8\x94\xe0\xb8\xb2\xe0\xb8\x95\xe0\xb8\xb2 vi metadata'
b'multiple issues expert subject computer science date october 2012 reason it is impossible to copy edit the article in its current state notability date december 2011 more footnotes date november 2011 technical date november 2011 essay like date november 2011 commitment ordering co is a class of interoperable serializability techniques in concurrency control of database s transaction processing and related applications it allows serializability optimistic versus pessimistic techniques optimistic non blocking implementations with the proliferation of multi core processor s co has been also increasingly utilized in concurrent computing concurrent programming transactional memory and especially in software transactional memory stm for achieving serializability optimistic concurrency control optimistically co is also the name of the resulting transaction schedule computer science schedule history property which was originally defined in 1988 with the name dynamic atomicity ref name fekete1988 alan fekete nancy lynch michael merritt william weihl 1988 http www dtic mil cgi bin gettrdoc ad ada200980 location u2 doc gettrdoc pdf commutativity based locking for nested transactions pdf mit lcs lab technical report mit lcs tm 370 august 1988 ref in a co compliant schedule the chronological order of commitment events of transactions is compatible with the serializability testing conflict serializability precedence order of the respective transactions co is a broad special case of serializability view and conflict serializability conflict serializability and effective means reliability engineering reliable high performance distributed and scalability scalable to achieve global serializability modular serializability across any collection of database systems that possibly use different concurrency control mechanisms co also makes each system serializability compliant if not already each not co compliant database system is augmented with a co component the commitment order coordinator coco which orders the commitment events for co compliance with neither data access nor any other transaction operation interference as such co provides a low overhead general solution for global serializability and distributed serializability instrumental for global concurrency control and distributed concurrency control of multi database systems and other transactional object s possibly highly distributed e g within cloud computing grid computing and networks of smartphone s an atomic commitment protocol acp of any type is a fundamental part of the solution utilized to break global cycles in the conflict precedence serializability graph co is the most general property a necessary condition that guarantees global serializability if the database systems involved do not share concurrency control information beyond atomic commitment protocol unmodified messages and have no knowledge whether transactions are global or local the database systems are autonomous thus co with its variants is the only general technique that does not require the typically costly distribution of local concurrency control information e g local precedence relations locks timestamps or tickets it generalizes the popular two phase locking strong strict two phase locking ss2pl property which in conjunction with the two phase commit protocol 2pc is the de facto standard to achieve global serializability across ss2pl based database systems as a result co compliant database systems with any different concurrency control types can transparently join such ss2pl based solutions for global serializability in addition locking based global deadlocks are resolved automatically in a co based multi database environment an important side benefit including the special case of a completely ss2pl based environment a previously unnoticed fact for ss2pl furthermore strict commitment ordering sco raz1991c raz 1991c the intersection of schedule computer science strict strictness and co provides better performance shorter average transaction completion time and resulting better transaction throughput than ss2pl whenever read write conflicts are present identical blocking behavior for write read and write write conflicts comparable locking overhead the advantage of sco is especially significant during lock contention strictness allows both ss2pl and sco to use the same effective database recovery mechanisms two major generalizing variants of co exist extended co eco raz1993a raz 1993a and multi version co mvco raz1993b raz 1993b they as well provide global serializability without local concurrency control information distribution can be combined with any relevant concurrency control and allow optimistic non blocking implementations both use additional information for relaxing co constraints and achieving better concurrency and performance vote ordering vo or generalized co gco raz2009 raz 2009 is a container schedule set property and technique for co and all its variants local vo is a necessary condition for guaranteeing global serializability if the atomic commitment protocol acp participants do not share concurrency control information have the generalized autonomy property co and its variants inter operate transparently guaranteeing global serializability and automatic global deadlock resolution also together in a mixed heterogeneous environment with different variants overview the commitment ordering co raz1990 raz 1990 raz1992 1992 raz1994 1994 raz2009 2009 schedule property has been referred to also as dynamic atomicity since 1988 ref name fekete1988 commit ordering commit order serializability and strong recoverability since 1991 the latter is a misleading name since co is incomparable with serializability correctness recoverability recoverability and the term strong implies a special case this means that a schedule with a strong recoverability property does not necessarily have the co property and vice versa in 2009 co has been characterized as a major concurrency control method together with the previously known since the 1980s three major methods locking time stamp ordering and serialization graph testing and as an enabler for the interoperability of systems using different concurrency control mechanisms ref name bern2009 phil bernstein philip a bernstein eric newcomer 2009 http www elsevierdirect com product jsp isbn 9781558606234 principles of transaction processing 2nd edition morgan kaufmann elsevier june 2009 isbn 978 1 55860 623 4 pages 145 360 ref in a federated database system or any other more loosely defined multidatabase system which are typically distributed in a communication network transactions span multiple and possibly distributed database s enforcing global serializability in such system is problematic even if every local schedule of a single database is serializable still the global schedule of a whole system is not necessarily serializable the massive communication exchanges of conflict information needed between databases to reach conflict serializability would lead to unacceptable performance primarily due to computer and communication latency engineering latency the problem of achieving global serializability effectively had been characterized as open problem open until the public disclosure of co in 1991 by its invention inventor yoav raz raz1991a raz 1991a see also global serializability enforcing co is an effective way to enforce conflict serializability globally in a distributed system since enforcing co locally in each database or other transactional object also enforces it globally each database may use any possibly different type of concurrency control mechanism with a local mechanism that already provides conflict serializability enforcing co locally does not cause any additional aborts since enforcing co locally does not affect the data access scheduling strategy of the mechanism this scheduling determines the serializability related aborts such a mechanism typically does not consider the commitment events or their order the co solution requires no communication overhead since it uses unmodified atomic commitment protocol messages only already needed by each distributed transaction to reach atomicity an atomic commitment protocol plays a central role in the distributed co algorithm which enforces co globally by breaking global cycles cycles that span two or more databases in the global conflict graph co its special cases and its generalizations are interoperable and achieve global serializability while transparently being utilized together in a single heterogeneous distributed environment comprising objects with possibly different concurrency control mechanisms as such commitment ordering including its special cases and together with its generalizations see co variants below provides a general high performance fully distributed solution no central processing component or central data structure are needed for guaranteeing global serializability in heterogeneous environments of multidatabase systems and other multiple transactional objects objects with states accessed and modified only by transactions e g in the framework of transactional processes and within cloud computing and grid computing the co solution scales up with network size and the number of databases without any negative impact on performance assuming the statistics of a single distributed transaction e g the average number of databases involved with a single transaction are unchanged with the proliferation of multi core processor s optimistic co oco has been also increasingly utilized to achieve serializability in software transactional memory and numerous stm articles and patents utilizing commit order have already been published e g zhang et al 2006 ref name zhang2006 the commitment ordering solution for global serializability general characterization of co commitment ordering co is a special case of conflict serializability co can be enforced with non blocking mechanisms each transaction can complete its task without having its data access blocked which allows optimistic concurrency control however commitment could be blocked in a co schedule the commitment events partial order partial precedence order of the transactions corresponds to the precedence partial order of the respective transactions in the directed graph directed conflict graph precedence graph serializability graph as induced by their conflicting access operations usually read and write insert modify delete operations co also applies to higher level operations where they are conflicting if noncommutative as well as to conflicts between operations upon multi version data definition colon commitment ordering let math t 1 t 2 math be two committed transactions in a schedule such that math t 2 math is in a conflict with math t 1 math math t 1 math precedes math t 2 math the schedule has the commitment ordering co property if for every two such transactions math t 1 math commits before math t 2 math commits the commitment decision events are generated by either a local commitment mechanism or an atomic commitment protocol if different processes need to reach consensus on whether to commit or abort the protocol may be distributed or centralized transactions may be committed concurrently if the commit partial order allows if they do not have conflicting operations if different conflicting operations induce different partial orders of same transactions then the conflict graph has cycle graph theory cycles and the schedule will violate serializability when all the transactions on a cycle are committed in this case no partial order for commitment events can be found thus cycles in the conflict graph need to be broken by aborting transactions however any conflict serializable schedule can be made co without aborting any transaction by properly delaying commit events to comply with the transactions precedence partial order co enforcement by itself is not sufficient as a concurrency control mechanism since co lacks the recoverability property which should be supported as well the distributed co algorithm a fully distributed global commitment ordering enforcement algorithm exists that uses local co of each participating database and needs only unmodified atomic commitment protocol messages with no further communication the distributed algorithm is the combination of local to each database co algorithm processes and an atomic commitment protocol which can be fully distributed atomic commitment protocol is essential to enforce atomicity of each distributed transaction to decide whether to commit or abort it this procedure is always carried out for distributed transactions independently of concurrency control and co a common example of an atomic commitment protocol is the two phase commit protocol which is resilient to many types of system failure in a reliable environment or when processes usually fail together e g in the same integrated circuit a simpler protocol for atomic commitment may be used e g a simple handshake of distributed transaction s participating processes with some arbitrary but known special participant the transaction s coordinator i e a type of one phase commit protocol an atomic commitment protocol reaches consensus among participants on whether to commit or abort a distributed global transaction that spans these participants an essential stage in each such protocol is the yes vote either explicit or implicit by each participant which means an obligation of the voting participant to obey the decision of the protocol either commit or abort otherwise a participant can unilaterally abort the transaction by an explicit no vote the protocol commits the transaction only if yes votes have been received from all participants and thus typically a missing yes vote of a participant is considered a no vote by this participant otherwise the protocol aborts the transaction the various atomic commit protocols only differ in their abilities to handle different computing environment failure situations and the amounts of work and other computing resources needed in different situations the entire co solution for global serializability is based on the fact that in case of a missing vote for a distributed transaction the atomic commitment protocol eventually aborts this transaction enforcing global co in each database system a local co algorithm determines the needed commitment order for that database by the characterization of co above this order depends on the local precedence order of transactions which results from the local data access scheduling mechanisms accordingly yes votes in the atomic commitment protocol are scheduled for each unaborted distributed transaction in what follows a vote means a yes vote if a precedence relation conflict exists between two transactions then the second will not be voted on before the first is completed either committed or aborted to prevent possible commit order violation by the atomic commitment protocol such can happen since the commit order by the protocol is not necessarily the same as the voting order if no precedence relation exists both can be voted on concurrently this vote ordering strategy ensures that also the atomic commitment protocol maintains commitment order and it is a necessary condition for guaranteeing global co and the local co of a database without it both global co and local co a property meaning that each database is co compliant may be violated however since database systems schedule their transactions independently it is possible that the transactions precedence orders in two databases or more are not compatible no global partial order exists that can embedding embed the respective local partial orders together with co precedence orders are also the commitment orders when participating databases in a same distributed transaction do not have compatible local precedence orders for that transaction without knowing it typically no coordination between database systems exists on conflicts since the needed communication is massive and unacceptably degrades performance it means that the transaction resides on a global cycle involving two or more databases in the global conflict graph in this case the atomic commitment protocol will fail to collect all the votes needed to commit that transaction by the vote ordering strategy above at least one database will delay its vote for that transaction indefinitely to comply with its own commitment precedence order since it will be waiting to the completion of another preceding transaction on that global cycle delayed indefinitely by another database with a different order this means a voting deadlock situation involving the databases on that cycle as a result the protocol will eventually abort some deadlocked transaction on this global cycle since each such transaction is missing at least one participant s vote selection of the specific transaction on the cycle to be aborted depends on the atomic commitment protocol s abort policies a timeout telecommunication timeout mechanism is common but it may result in more than one needed abort per cycle both preventing unnecessary aborts and abort time shortening can be achieved by a dedicated abort mechanism for co such abort will break the global cycle involving that distributed transaction both deadlocked transactions and possibly other in conflict with the deadlocked and thus blocked will be free to be voted on it is worthwhile noting that each database involved with the voting deadlock continues to vote regularly on transactions that are not in conflict with its deadlocked transaction typically almost all the outstanding transactions thus in case of incompatible local partial commitment orders no action is needed since the atomic commitment protocol resolves it automatically by aborting a transaction that is a cause of incompatibility this means that the above vote ordering strategy is also a sufficient condition for guaranteeing global co the following is concluded the vote ordering strategy for global co enforcing theorem let math t 1 t 2 math be undecided neither committed nor aborted transactions in a database system that enforces co for local transactions such that math t 2 math is global and in conflict with math t 1 math math t 1 math precedes math t 2 math then having math t 1 math ended either committed or aborted before math t 2 math is voted on to be committed the vote ordering strategy in each such database system in a multidatabase environment is a necessary and sufficient condition for guaranteeing global co the condition guarantees global co which may be violated without it comments the vote ordering strategy that enforces global co is referred to as math cd 3c math in raz1992 raz 1992 the local co property of a global schedule means that each database is co compliant from the necessity discussion part above it directly follows that the theorem is true also when replacing global co with local co when global transactions are present together it means that global co is guaranteed if and only if local co is guaranteed which is untrue for global conflict serializability and local conflict serializability global implies local but not the opposite global co implies global serializability the global co algorithm comprises enforcing local co in each participating database system by ordering commits of local transactions see commitment ordering enforcing co locally enforcing co locally below and enforcing the vote ordering strategy in the theorem above for global transactions exact characterization of voting deadlocks by global cycles the above global cycle elimination process by a voting deadlock can be explained in detail by the following observation first it is assumed for simplicity that every transaction reaches the ready to commit state and is voted on by at least one database this implies that no blocking by locks occurs define a wait for vote to commit graph as a directed graph with transactions as nodes and a directed edge from any first transaction to a second transaction if the first transaction blocks the vote to commit of the second transaction opposite to conventional edge direction in a wait for graph such blocking happens only if the second transaction is in a conflict with the first transaction see above thus this wait for vote to commit graph is identical to the global conflict graph a cycle in the wait for vote to commit graph means a deadlock in voting hence there is a deadlock in voting if and only if there is a cycle in the conflict graph local cycles confined to a single database are eliminated by the local serializability mechanisms consequently only global cycles are left which are then eliminated by the atomic commitment protocol when it aborts deadlocked transactions with missing blocked respective votes secondly also local commits are dealt with note that when enforcing co also waiting for a regular local commit of a local transaction can block local commits and votes of other transactions upon conflicts and the situation for global transactions does not change also without the simplifying assumption above the final result is the same also with local commitment for local transactions without voting in atomic commitment for them finally blocking by a lock which has been excluded so far needs to be considered a lock blocks a conflicting operation and prevents a conflict from being materialized if the lock is released only after transaction end it may block indirectly either a vote or a local commit of another transaction which now cannot get to ready state with the same effect as of a direct blocking of a vote or a local commit in this case a cycle is generated in the conflict graph only if such a blocking by a lock is also represented by an edge with such added edges representing events of blocking by a lock the conflict graph is becoming an augmented conflict graph definition augmented conflict graph an augmented conflict graph is a serializability testing conflict serializability conflict graph with added edges in addition to the original edges a directed edge exists from transaction math t 1 math to transaction math t 2 math if two conditions are met math t 2 math is blocked by a data access lock applied by math t 1 math the blocking prevents the conflict of math t 2 math with math t 1 math from being materialized and have an edge in the regular conflict graph and this blocking will not stop before math t 1 math ends commits or aborts true for any locking based co the graph can also be defined as the union set theory union of the regular conflict graph with the reversed edge regular wait for graph comments here unlike the regular conflict graph which has edges only for materialized conflicts all conflicts both materialized and non materialized are represented by edges note that all the new edges are all the reversed to the conventional edges of the wait for graph the wait for graph can be defined also as the graph of non materialized conflicts by the common conventions edge direction in a conflict graph defines time order between conflicting operations which is opposite to the time order defined by an edge in a wait for graph note that such global graph contains has embedded all the reversed edge regular local wait for graphs and also may include locking based global cycles which cannot exist in the local graphs for example if all the databases on a global cycle are ss2pl based then all the related vote blocking situations are caused by locks this is the classical and probably the only global deadlock situation dealt with in the database research literature this is a global deadlock case where each related database creates a portion of the cycle but the complete cycle does not reside in any local wait for graph in the presence of co the augmented conflict graph is in fact a reversed edge local commit and voting wait for graph an edge exists from a first transaction either local or global to a second if the second is waiting for the first to end in order to be either voted on if global or locally committed if local all global cycles across two or more databases in this graph generate voting deadlocks the graph s global cycles provide complete characterization for voting deadlocks and may include any combination of materialized and non materialized conflicts only cycles of only materialized conflicts are also cycles of the regular conflict graph and affect serializability one or more lock related non materialized conflicts on a cycle prevent it from being a cycle in the regular conflict graph and make it a locking related deadlock all the global cycles voting deadlocks need to be broken resolved to both maintain global serializability and resolve global deadlocks involving data access locking and indeed they are all broken by the atomic commitment protocol due to missing votes upon a voting deadlock comment this observation also explains the correctness of commitment ordering extended co eco extended co eco below global transactions voting order must follow the conflict graph order with vote blocking when order relation graph path exists between two global transactions local transactions are not voted on and their local commits are not blocked upon conflicts this results in same voting deadlock situations and resulting global cycle elimination process for eco the voting deadlock situation can be summarized as follows the co voting deadlock theorem let a multidatabase environment comprise co compliant which eliminates local cycles database systems that enforce each global co using the condition in the theorem above then a voting deadlock occurs if and only if a global cycle spans two or more databases exists in the global augmented conflict graph also blocking by a data access lock is represented by an edge if the cycle does not break by any abort then all the global transactions on it are involved with the respective voting deadlock and eventually each has its vote blocked either directly or indirectly by a data access lock if a local transaction resides on the cycle eventually it has its local commit blocked comment a rare situation of a voting deadlock by missing blocked votes can happen with no voting for any transaction on the related cycle by any of the database systems involved with these transactions this can occur when local sub transactions are thread computer science multi threaded the highest probability instance of such rare event involves two transactions on two simultaneous opposite cycles such global cycles deadlocks overlap with local cycles which are resolved locally and thus typically resolved by local mechanisms without involving atomic commitment formally it is also a global cycle but practically it is local portions of local cycles generate a global one to see this split each global transaction node to local sub transactions its portions confined each to a single database a directed edge exists between transactions if an edge exists between any respective local sub transactions a cycle is local if all its edges originate from a cycle among sub transactions of the same database and global if not global and local can overlap a same cycle among transactions can result from several different cycles among sub transactions and be both local and global also the following locking based special case is concluded the co locking based global deadlock theorem in a co compliant multidatabase system a locking based global deadlock involving at least one data access lock non materialized conflict and two or more database systems is a reflection of a global cycle in the global augmented conflict graph which results in a voting deadlock such cycle is not a cycle in the regular global conflict graph which reflects only materialized conflicts and thus such cycle does not affect serializability comments any blocking edge in the cycle that is not by a data access lock is a direct blocking of either voting or local commit all voting deadlocks are resolved almost all by atomic commitment see comment above including this locking based type locking based global deadlocks can be generated also in a completely ss2pl based distributed environment special case of co based where all the vote blocking and voting deadlocks are caused by data access locks many research articles have dealt for years with resolving such global deadlocks but none except the co articles is known as of 2009 to notice that atomic commitment automatically resolves them such automatic resolutions are regularly occurring unnoticed in all existing ss2pl based multidatabase systems often bypassing dedicated resolution mechanisms voting deadlocks are the key for the operation of distributed co global cycle elimination here voting deadlock resolution by atomic commitment and resulting aborted transactions re executions are time consuming regardless of concurrency control used if databases schedule transactions independently global cycles are unavoidable in a complete analogy to cycles deadlocks generated in local ss2pl with distribution any transaction or operation scheduling coordination results in autonomy violation and typically also in substantial performance penalty however in many cases their likelihood can be made very low by implementing database and transaction design guidelines that reduce the number of conflicts involving a global transaction this primarily by properly handling hot spots database objects with frequent access and avoiding conflicts by using commutativity when possible e g when extensively using counters as in finances and especially multi transaction accumulation counters which are typically hot spots atomic commitment protocols are intended and designed to achieve atomicity without considering database concurrency control they abort upon detecting or heuristic algorithm heuristically finding e g by timeout sometimes mistakenly unnecessarily missing votes and typically unaware of global cycles these protocols can be specially enhanced for co including co s variants below both to prevent unnecessary aborts and to accelerate aborts used for breaking global cycles in the global augmented conflict graph for better performance by earlier release upon transaction end of computing resources and typically locked data for example existing locking based global deadlock detection methods other than timeout can be generalized to consider also local commit and vote direct blocking besides data access blocking a possible compromise in such mechanisms is effectively detecting and breaking the most frequent and relatively simple to handle length 2 global cycles and using timeout for undetected much less frequent longer cycles enforcing co locally commitment ordering can be enforced locally in a single database by a dedicated co algorithm or by any algorithm protocol that provides any special case of co an important such protocol being utilized extensively in database systems which generates a co schedule is the strong strict two phase locking protocol ss2pl release transaction s locks only after the transaction has been either committed or aborted see below ss2pl is a proper subset of the intersection of two phase locking 2pl and strictness a generic local co algorithm a generic local co algorithm raz1992 raz 1992 algorithm 4 1 is an algorithm independent of implementation details that enforces exactly the co property it does not block data access nonblocking and consists of aborting a certain set of transactions only if needed upon committing a transaction it aborts a uniquely determined at any given time minimal set of other undecided neither committed nor aborted transactions that run locally and can cause serializability violation in the future can later generate cycles of committed transactions in the conflict graph this is the abort set of a committed transaction t after committing t no transaction in abort at commit time can be committed and all of them are doomed to be aborted this set consists of all undecided transactions with directed edges in the conflict graph to the committed transaction the size of this set cannot increase when that transaction is waiting to be committed in ready state processing has ended and typically decreases in time as its transactions are being decided thus unless real time computing real time constraints exist to complete that transaction it is preferred to wait with committing that transaction and let this set decrease in size if another serializability mechanism exists locally which eliminates cycles in the local conflict graph or if no cycle involving that transaction exists the set will be empty eventually and no abort of set member is needed otherwise the set will stabilize with transactions on local cycles and aborting set members will have to occur to break the cycles since in the case of co conflicts generate blocking on commit local cycles in the augments conflict graph see above indicate local commit deadlocks and deadlock resolution techniques as in serializability common mechanism ss2pl ss2pl can be used e g like timeout and wait for graph a local cycle in the augmented conflict graph with at least one non materialized conflict reflects a locking based deadlock the local algorithm above applied to the local augmented conflict graph rather than the regular local conflict graph comprises the generic enhanced local co algorithm a single local cycle elimination mechanism for both guaranteeing local serializability and handling locking based local deadlocks practically an additional concurrency control mechanism is always utilized even solely to enforce recoverability the generic co algorithm does not affect local data access scheduling strategy when it runs alongside of any other local concurrency control mechanism it affects only the commit order and for this reason it does not need to abort more transactions than those needed to be aborted for serializability violation prevention by any combined local concurrency control mechanism the net effect of co may be at most a delay of commit events or voting in a distributed environment to comply with the needed commit order but not more delay than its special cases for example ss2pl and on the average significantly less the following theorem is concluded the generic local co algorithm theorem when running alone or alongside any concurrency control mechanism in a database system then the generic local co algorithm guarantees local co a co compliant schedule the generic enhanced local co algorithm guarantees both local co and local locking based deadlock resolution and when not using timeout and no real time transaction completion constraints are applied neither algorithm aborts more transactions than the minimum needed which is determined by the transactions operations scheduling out of the scope of the algorithms example concurrent programming and transactional memory see also the history of commitment ordering concurrent programming and transactional memory concurrent programming and transactional memory with the proliferation of multi core processors variants of the generic local co algorithm have been also increasingly utilized in concurrent programming transactional memory and especially in software transactional memory for achieving serializability optimistically by commit order e g ramadan et al 2009 ref name ramadan2009 hany e ramadan indrajit roy maurice herlihy emmett witchel 2009 http portal acm org citation cfm id 1504201 committing conflicting transactions in an stm http www cs utexas edu indrajit pubs ppopp121 ramadan pdf pdf proceedings of the 14th acm sigplan symposium on principles and practice of parallel programming ppopp 09 isbn 978 1 60558 397 6 ref zhang et al 2006 ref name zhang2006 lingli zhang vinod k grover michael m magruder david detlefs john joseph duffy goetz graefe 2006 http www freepatentsonline com 7711678 html software transaction commit order and conflict management united states patent 7711678 granted 05 04 2010 ref von parun et al 2007 ref name vonparun2007 christoph von praun luis ceze calin cascaval 2007 http portal acm org citation cfm id 1229443 implicit parallelism with ordered transactions http www cs washington edu homes luisceze publications ipot ppopp07 pdf pdf proceedings of the 12th acm sigplan symposium on principles and practice of parallel programming ppopp 07 acm new york \xc2\xa92007 isbn 978 1 59593 602 8 doi 10 1145 1229428 1229443 ref numerous related articles and patents utilizing co have already been published implementation considerations the commitment order coordinator coco a database system in a multidatabase environment is assumed from a software architecture point of view a co component that implements the generic co algorithm locally the commitment order coordinator coco can be designed in a straightforward way as a mediator pattern mediator between a single database system and an atomic commitment protocol component raz1991b raz 1991b however the coco is typically an integral part of the database system the coco s functions are to vote to commit on ready global transactions processing has ended according to the local commitment order to vote to abort on transactions for which the database system has initiated an abort the database system can initiate abort for any transaction for many reasons and to pass the atomic commitment decision to the database system for local transactions when can be identified no voting is needed for determining the commitment order the coco maintains an updated representation of the local conflict graph or local augmented conflict graph for capturing also locking deadlocks of the undecided neither committed nor aborted transactions as a data structure e g utilizing mechanisms similar to lock computer science locking for capturing conflicts but with no data access blocking the coco component has an interface computer science interface with its database system to receive conflict ready processing has ended readiness to vote on a global transaction or commit a local one and abort notifications from the database system it also interfaces with the atomic commitment protocol to vote and to receive the atomic commitment protocol s decision on each global transaction the decisions are delivered from the coco to the database system through their interface as well as local transactions commit notifications at a proper commit order the coco including its interfaces can be enhanced if it implements another variant of co see below or plays a role in the database s concurrency control mechanism beyond voting in atomic commitment the coco also guarantees co locally in a single isolated database system with no interface with an atomic commitment protocol co is a necessary condition for global serializability across autonomous database systems if the databases that participate in distributed transactions i e transactions that span more than a single database do not use any shared concurrency control information and use unmodified atomic commitment protocol messages for reaching atomicity then maintaining local commitment ordering or one of its generalizing variants see below is a necessary condition for guaranteeing global serializability a proof technique can be found in raz1992 raz 1992 and a different proof method for this in raz1993a raz 1993a it is also a sufficient condition this is a mathematical fact derived from the definitions of serializability and a database transaction transaction it means that if not complying with co then global serializability cannot be guaranteed under this condition the condition of no local concurrency control information sharing between databases beyond atomic commit protocol messages atomic commitment is a minimal requirement for a distributed transaction since it is always needed which is implied by the definition of transaction raz1992 raz 1992 defines database autonomy and independence as complying with this requirement without using any additional local knowledge definition concurrency control based autonomous database system a database system is autonomous if it does not share with any other entity any concurrency control information beyond unmodified atomic commitment protocol messages in addition it does not use for concurrency control any additional local information beyond conflicts the last sentence does not appear explicitly but rather implied by further discussion in raz1992 raz 1992 using this definition the following is concluded the co and global serializability theorem co compliance of every autonomous database system or transactional object in a multidatabase environment is a necessary condition for guaranteeing global serializability without co global serializability may be violated co compliance of every database system is a sufficient condition for guaranteeing global serializability however the definition of autonomy above implies for example that transactions are scheduled in a way that local transactions confined to a single database cannot be identified as such by an autonomous database system this is realistic for some transactional objects but too restrictive and less realistic for general purpose database systems if autonomy is augmented with the ability to identify local transactions then compliance with a more general property extended commitment ordering eco see below makes eco the necessary condition only in raz2009 raz 2009 the notion of generalized autonomy captures the intended notion of autonomy definition generalized autonomy a database system has the generalized autonomy property if it does not share with any other database system any local concurrency information beyond unmodified atomic commit protocol messages however any local information can be utilized this definition is probably the broadest such definition possible in the context of database concurrency control and it makes co together with any of its useful no concurrency control information distribution generalizing variants vote ordering vo see co variants below the necessary condition for global serializability i e the union of co and its generalizing variants is the necessary set vo which may include also new unknown useful generalizing variants summary the commitment ordering co solution technique for global serializability can be summarized as follows if each database or any other transactional object in a multidatabase environment complies with co i e arranges its local transactions commitments and its votes on global distributed transactions to the atomic commitment protocol according to the local to the database partial order induced by the local conflict graph serializability graph for the respective transactions then global co and global serializability are guaranteed a database s co compliance can be achieved effectively with any local serializability view serializability and conflict serializability conflict serializability based concurrency control mechanism with neither affecting any transaction s execution process or scheduling nor aborting it also the database s autonomy is not violated the only low overhead incurred is detecting conflicts e g as with locking but with no data access blocking if not already detected for other purposes and ordering votes and local transactions commits according to the conflicts image co scheduleclasses jpg thumb 350px schedule classes containment an arrow from class a to class b indicates that class a strictly contains b a lack of a directed path between classes means that the classes are incomparable a property is inherently blocking if it can be enforced only by blocking transaction s data access operations until certain events occur in other transactions raz1992 raz 1992 in case of incompatible partial orders of two or more databases no global partial order can embedding embed the respective local partial orders together a global cycle spans two databases or more in the global conflict graph is generated this together with co results in a cycle of blocked votes and a voting deadlock occurs for the databases on that cycle however allowed concurrent voting in each database typically for almost all the outstanding votes continue to execute in this case the atomic commitment protocol fails to collect all the votes needed for the blocked transactions on that global cycle and consequently the protocol aborts some transaction with a missing vote this breaks the global cycle the voting deadlock is resolved and the related blocked votes are free to be executed breaking the global cycle in the global conflict graph ensures that both global co and global serializability are maintained thus in case of incompatible local partial commitment orders no action is needed since the atomic commitment protocol resolves it automatically by aborting a transaction that is a cause for the incompatibility furthermore also global deadlocks due to locking global cycles in the augmented conflict graph with at least one data access blocking result in voting deadlocks and are resolved automatically by the same mechanism local co is a necessary condition for guaranteeing global serializability if the databases involved do not share any concurrency control information beyond unmodified atomic commitment protocol messages i e if the databases are autonomous in the context of concurrency control this means that every global serializability solution for autonomous databases must comply with co otherwise global serializability may be violated and thus is likely to be violated very quickly in a high performance environment the co solution scalability scales up with network size and the number of databases without performance penalty when it utilizes two phase commit protocol common architecture common distributed atomic commitment architecture distributed serializability and co distributed co a distinguishing characteristic of the co solution to distributed serializability from other techniques is the fact that it requires no conflict information distributed e g local precedence relations locks timestamp based concurrency control timestamps tickets which makes it uniquely effective it utilizes unmodified atomic commitment protocol messages which are already used instead a common way to achieve distributed serializability in a distributed system distributed system is by a distributed lock manager dlm dlms which communicate lock non materialized conflict information in a distributed environment typically suffer from computer and communication latency engineering latency which reduces the performance of the system co allows to achieve distributed serializability under very general conditions without a distributed lock manager exhibiting the benefits already explored above for multidatabase environments in particular reliability high performance scalability possibility of using optimistic concurrency control when desired no conflict information related communications over the network which have incurred overhead and delays and automatic distributed deadlock resolution all distributed transactional systems rely on some atomic commitment protocol to coordinate atomicity whether to commit or abort among processes in a distributed transaction also typically recoverable data i e data under transactions control e g database data not to be confused with the recoverability property of a schedule are directly accessed by a single transactional data manager component also referred to as a resource manager that handles local sub transactions the distributed transaction s portion in a single location e g network node even if these data are accessed indirectly by other entities in the distributed system during a transaction i e indirect access requires a direct access through a local sub transaction thus recoverable data in a distributed transactional system are typically partitioned among transactional data managers in such system these transactional data managers typically comprise the participants in the system s atomic commitment protocol if each participant complies with co e g by using ss2pl or cocos or a combination see above then the entire distributed system provides co by the theorems above each participant can be considered a separate transactional object and thus distributed serializability furthermore when co is utilized together with an atomic commitment protocol also distributed deadlocks i e deadlocks that span two or more data managers caused by data access locking are resolved automatically thus the following corollary is concluded the co based distributed serializability theorem let a distributed transactional system e g a distributed database system comprise transactional data managers also called resource managers that manage all the system s recoverable data the data managers meet three conditions data partition recoverable data are partitioned among the data managers i e each recoverable datum data item is controlled by a single data manager e g as common in a shared nothing architecture even copies of a same datum under different data managers are physically distinct replicated participants in atomic commitment protocol these data managers are the participants in the system s atomic commitment protocol for coordinating distributed transactions atomicity co compliance each such data manager is co compliant or some co variant compliant see below then the entire distributed system guarantees distributed co and serializability and data access based distributed deadlocks deadlocks involving two or more data managers with at least one non materialized conflict are resolved automatically furthermore the data managers being co compliant is a necessary condition for distributed serializability in a system meeting conditions 1 2 above when the data managers are autonomous i e do not share concurrency control information beyond unmodified messages of atomic commitment protocol this theorem also means that when ss2pl or any other co variant is used locally in each transactional data manager and each data manager has exclusive control of its data no distributed lock manager which is often utilized to enforce distributed ss2pl is needed for distributed ss2pl and serializability it is relevant to a wide range of distributed transactional applications which can be easily designed to meet the theorem s conditions distributed optimistic co doco for implementing distributed optimistic co doco the generic local co algorithm is utilized in all the atomic commitment protocol participants in the system with no data access blocking and thus with no local deadlocks the previous theorem has the following corollary the distributed optimistic co doco theorem if doco is utilized then no local deadlocks occur and global voting deadlocks are resolved automatically and all are serializability related with non blocking conflicts rather than locking related with blocking and possibly also non blocking conflicts thus no deadlock handling is needed examples distributed ss2pl a distributed database system that utilizes two phase locking strong strict two phase locking ss2pl resides on two remote nodes a and b the database system has two transactional data managers resource managers one on each node and the database data are partitioned between the two data managers in a way that each has an exclusive control of its own local to the node portion of data each handles its own data and locks without any knowledge on the other manager s for each distributed transaction such data managers need to execute the available atomic commitment protocol two distributed transactions math t 1 math and math t 2 math are running concurrently and both access data x and y x is under the exclusive control of the data manager on a b s manager cannot access x and y under that on b math t 1 math reads x on a and writes y on b i e math t 1 r 1a x math math w 1b y math when using notation common for concurrency control math t 2 math reads y on b and writes x on a i e math t 2 r 2b y math math w 2a x math the respective local sub transactions on a and b the portions of math t 1 math and math t 2 math on each of the nodes are the following class wikitable style text align center local sub transactions transaction node a b math t 1 math math t 1a r 1a x math math t 1b w 1b y math math t 2 math math t 2a w 2a x math math t 2b r 2b y math the database system s schedule computer science schedule at a certain point in time is the following math r 1a x math math r 2b y math also math r 2b y math math r 1a x math is possible math t 1 math holds a read lock on x and math t 2 math holds read locks on y thus math w 1b y math and math w 2a x math are blocked by the two phase locking data access locks lock compatibility rules of ss2pl and cannot be executed this is a distributed deadlock situation which is also a voting deadlock see below with a distributed global cycle of length 2 number of edges conflicts 2 is the most frequent length the local sub transactions are in the following states math t 1a math is ready execution has ended and voted in atomic commitment math t 1b math is running and blocked a non materialized conflict situation no vote on it can occur math t 2b math is ready and voted math t 2a math is running and blocked a non materialized conflict no vote since the atomic commitment protocol cannot receive votes for blocked sub transactions a voting deadlock it will eventually abort some transaction with a missing vote s by timeout computing timeout either math t 1 math or math t 2 math or both if the timeouts fall very close this will resolve the global deadlock the remaining transaction will complete running be voted on and committed an aborted transaction is immediately restarted and re executed comments the data partition x on a y on b is important since without it for example x can be accessed directly from b if a transaction math t 3 math is running on b concurrently with math t 1 math and math t 2 math and directly writes x then without a distributed lock manager the read lock for x held by math t 1 math on a is not visible on b and cannot block the write of math t 3 math or signal a materialized conflict for a non blocking co variant see below thus serializability can be violated due to data partition x cannot be accessed directly from b however functionality is not limited and a transaction running on b still can issue a write or read request of x not common this request is communicated to the transaction s local sub transaction on a which is generated if does not exist already which issues this request to the local data manager on a variations in the scenario above both conflicts are non materialized and the global voting deadlock is reflected as a cycle in the global wait for graph but not in the global conflict graph see commitment ordering exact characterization of voting deadlocks by global cycles exact characterization of voting deadlocks by global cycles above however the database system can utilize any co variant with exactly the same conflicts and voting deadlock situation and same resolution conflicts can be either materialized or non materialized depending on co variant used for example if commitment ordering strict co sco sco below is used by the distributed database system instead of ss2pl then the two conflicts in the example are materialized all local sub transactions are in ready states and vote blocking occurs in the two transactions one on each node because of the co voting rule applied independently on both a and b due to conflicts math t 2a w 2a x math is not voted on before math t 1a r 1a x math ends and math t 1b w 1b y math is not voted on before math t 2b r 2b y math ends which is a voting deadlock now the conflict graph has the global cycle all conflicts are materialized and again it is resolved by the atomic commitment protocol and distributed serializability is maintained unlikely for a distributed database system but possible in principle and occurs in a multi database a can employ ss2pl while b employs sco in this case the global cycle is neither in the wait for graph nor in the serializability graph but still in the augmented conflict graph the union of the two the various combinations are summarized in the following table class wikitable style text align center voting deadlock situations case node br a node br b possible schedule materialized br conflicts br on cycle non br materialized br conflicts math t 1a math br math r 1a x math math t 1b math br math w 1b y math math t 2a math br math w 2a x math math t 2b math br math r 2b y math 1 ss2pl ss2pl math r 1a x math math r 2b y math 0 2 ready br voted running br blocked running br blocked ready br voted 2 ss2pl sco math r 1a x math math r 2b y math math w 1b y math 1 1 ready br voted ready br vote blocked running br blocked ready br voted 3 sco ss2pl math r 1a x math math r 2b y math math w 2a x math 1 1 ready br voted running br blocked ready br vote blocked ready br voted 4 sco sco math r 1a x math math r 2b y math math w 1b y math math w 2a x math 2 0 ready br voted ready br vote blocked ready br vote blocked ready br voted comments conflicts and thus cycles in the augmented conflict graph are determined by the transactions and their initial scheduling only independently of the concurrency control utilized with any variant of co any global cycle i e spans two databases or more causes a voting deadlock different co variants may differ on whether a certain conflict is materialized or non materialized some limited operation order changes in the schedules above are possible constrained by the orders inside the transactions but such changes do not change the rest of the table as noted above only case 4 describes a cycle in the regular conflict graph which affects serializability cases 1 3 describe cycles of locking based global deadlocks at least one lock blocking exists all cycle types are equally resolved by the atomic commitment protocol case 1 is the common distributed ss2pl utilized since the 1980s however no research article except the co articles is known to notice this automatic locking global deadlock resolution as of 2009 such global deadlocks typically have been dealt with by dedicated mechanisms case 4 above is also an example for a typical voting deadlock when commitment ordering distributed optimistic co doco distributed optimistic co doco is used i e case 4 is unchanged when optimistic co oco see below replaces sco on both a and b no data access blocking occurs and only materialized conflicts exist hypothetical multi single threaded core music environment comment while the examples above describe real recommended utilization of co this example is hypothetical for demonstration only certain experimental distributed memory resident databases advocate multi single threaded core music transactional environments single threaded refers to transaction thread computer science threads only and to serial execution of transactions the purpose is possible orders of magnitude gain in performance e g michael stonebraker h store and voltdb h store ref name stone08 robert kallman hideaki kimura jonathan natkins andrew pavlo alex rasin stanley zdonik evan jones yang zhang samuel madden michael stonebraker john hugg daniel abadi 2008 http portal acm org citation cfm id 1454211 h store a high performance distributed main memory transaction processing system proceedings of the 2008 vldb pages 1496 1499 auckland new zealand august 2008 ref and voltdb relatively to conventional transaction execution in multiple threads on a same core in what described below music is independent of the way the cores are distributed they may reside in one integrated circuit chip or in many chips possibly distributed geographically in many computers in such an environment if recoverable transactional data are partitioned among threads cores and it is implemented in the conventional way for distributed co as described in previous sections then doco and strictness exist automatically however downsides exist with this straightforward implementation of such environment and its practicality as a general purpose solution is questionable on the other hand tremendous performance gain can be achieved in applications that can bypass these downsides in most situations comment the music straightforward implementation described here which uses for example as usual in distributed co voting and transaction thread blocking in atomic commitment protocol when needed is for demonstration only and has no connection to the implementation in h store or any other project in a music environment local schedules are serial thus both local optimistic co oco see below and the global co enforcement vote ordering strategy condition for the atomic commitment protocol are met automatically this results in both distributed co compliance and thus distributed serializability and automatic global voting deadlock resolution furthermore also local strictness follows automatically in a serial schedule by theorem 5 2 in raz1992 raz 1992 page 307 when the co vote ordering strategy is applied also global strictness is guaranteed note that serial locally is the only mode that allows strictness and optimistic no data access blocking together the following is concluded the music theorem in music environments if recoverable transactional data are partitioned among cores threads then both oco and implied serializability i e doco and distributed serializability strictness allowing effective recovery 1 and 2 implying strict co see sco below and voting deadlock resolution automatically exist globally with unbounded scalability in number of cores used comment however two major downsides which need special handling may exist local sub transactions of a global transaction are blocked until commit which makes the respective cores idle this reduces core utilization substantially even if scheduling of the local sub transactions attempts to execute all of them in time proximity almost together it can be overcome by detaching execution from commit with some atomic commitment protocol for global transactions at the cost of possible cascading aborts increasing the number of cores for a given amount of recoverable data database size decreases the average amount of partitioned data per core this may make some cores idle while others very busy depending on data utilization distribution also a local to a core transaction may become global multi core to reach its needed data with additional incurred overhead thus as the number of cores increases the amount and type of data assigned to each core should be balanced according to data usage so a core is neither overwhelmed to become a bottleneck nor becoming idle too frequently and underutilized in a busy system another consideration is putting in a same core partition all the data that are usually accessed by a same transaction if possible to maximize the number of local transactions and minimize the number of global distributed transactions this may be achieved by occasional data re partition among cores based on load balancing data access balancing and patterns of data usage by transactions another way to considerably mitigate this downside is by proper physical data replication among some core partitions in a way that read only global transactions are possibly depending on usage patterns completely avoided and replication changes are synchronized by a dedicated commit mechanism co variants interesting special cases and generalizations special case schedule property classes e g ss2pl and sco below are strictly contained in the co class the generalizing classes eco and mvco strictly contain the co class i e include also schedules that are not co compliant the generalizing variants also guarantee global serializability without distributing local concurrency control information each database has the generalized autonomy property it uses only local information while relaxing co constraints and utilizing additional local information for better concurrency and performance eco uses knowledge about transactions being local i e confined to a single database and mvco uses availability of data versions values like co both generalizing variants are non blocking do not interfere with any transaction s operation scheduling and can be seamlessly combined with any relevant concurrency control mechanism the term co variant refers in general to co eco mvco or a combination of each of them with any relevant concurrency control mechanism or property including multi version based eco mveco no other interesting generalizing variants which guarantee global serializability with no local concurrency control information distribution are known but may be discovered strong strict two phase locking ss2pl main two phase locking strong strict two phase locking ss2pl also referred to as rigorousness or rigorous scheduling means that both read and write locks of a transaction are released only after the transaction has ended either committed or aborted the set of ss2pl schedules is a proper subset of the set of co schedules this property is widely utilized in database systems and since it implies co databases that use it and participate in global transactions generate together a serializable global schedule when using any atomic commitment protocol which is needed for atomicity in a multi database environment no database modification or addition is needed in this case to participate in a co distributed solution the set of undecided transactions to be aborted before committing in the commitment ordering the algorithm local generic co algorithm above is empty because of the locks and hence such an algorithm is unnecessary in this case a transaction can be voted on by a database system immediately after entering a ready state i e completing running its task locally its locks are released by the database system only after it is decided by the atomic commitment protocol and thus the condition in the global co enforcing theorem above is kept automatically interestingly if a local timeout mechanism is used by a database system to resolve local ss2pl deadlocks then aborting blocked transactions breaks not only potential local cycles in the global conflict graph real cycles in the augmented conflict graph but also database system s potential global cycles as a side effect if the atomic commitment protocol s abort mechanism is relatively slow such independent aborts by several entities typically may result in unnecessary aborts for more than one transaction per global cycle the situation is different for a local wait for graph based mechanisms such cannot identify global cycles and the atomic commitment protocol will break the global cycle if the resulting voting deadlock is not resolved earlier in another database local ss2pl together with atomic commitment implying global serializability can also be deduced directly all transactions including distributed obey the two phase locking 2pl ss2pl rules the atomic commitment protocol mechanism is not needed here for consensus on commit but rather for the end of phase two synchronization point probably for this reason without considering the atomic commitment voting mechanism automatic global deadlock resolution has not been noticed before co strict co sco image sco vs ss2pl jpg thumb 450px read write conflict sco vs ss2pl duration of transaction t2 is longer with ss2pl than with sco ss2pl delays write operation w2 x of t2 until t1 commits due to a lock on x by t1 following read operation r1 x if t time units are needed for transaction t2 after starting write operation w2 x in order to reach ready state than t2 commits t time units after t1 commits however sco does not block w2 x and t2 can commit immediately after t1 commits raz1991c raz 1991c strict commitment ordering sco raz1991c raz 1991c is the intersection of schedule computer science strict strictness a special case of recoverability and co and provides an upper bound for a schedule s concurrency when both properties exist it can be implemented using blocking mechanisms locking similar to those used for the popular ss2pl with similar overheads unlike ss2pl sco does not block on a read write conflict but possibly blocks on commit instead sco and ss2pl have identical blocking behavior for the other two conflict types write read and write write as a result sco has shorter average blocking periods and more concurrency e g performance simulations of a single database for the most significant variant of locks with ordered sharing which is identical to sco clearly show this with approximately 100 gain for some transaction loads also for identical transaction loads sco can reach higher transaction rates than ss2pl before lock thrashing computer science thrashing occurs more concurrency means that with given computing resources more transactions are completed in time unit higher transaction rate throughput and the average duration of a transaction is shorter faster completion see chart the advantage of sco is especially significant during lock contention the sco vs ss2pl performance theorem sco provides shorter average transaction completion time than ss2pl if read write conflicts exist sco and ss2pl are identical otherwise have identical blocking behavior with write read and write write conflicts sco is as practical as ss2pl since as ss2pl it provides besides serializability also strictness which is widely utilized as a basis for efficient recovery of databases from failure an ss2pl mechanism can be converted to an sco one for better performance in a straightforward way without changing recovery methods a description of a sco implementation can be found in perrizo and tatarinov 1998 ref cite conference first1 william last1 perrizo first2 igor last2 tatarinov title a semi optimistic database scheduler based on commit ordering citeseerx 10 1 1 53 7318 conference 1998 int l conference on computer applications in industry and engineering pages 75 79 location las vegas date november 11 1998 ref see also the history of commitment ordering semi optimistic database scheduler semi optimistic database scheduler ss2pl is a proper subset of sco which is another explanation why sco is less constraining and provides more concurrency than ss2pl optimistic co oco for implementing optimistic commitment ordering oco the generic local co algorithm is utilized without data access blocking and thus without local deadlocks oco without transaction or operation scheduling constraints covers the entire co class and is not a special case of the co class but rather a useful co variant and mechanism characterization extended co eco general characterization of eco extended commitment ordering eco raz1993a raz 1993a generalizes co when local transactions transactions confined to a single database can be distinguished from global distributed transactions transactions that span two databases or more commitment order is applied to global transactions only thus for a local to a database schedule to have the eco property the chronological partial order of commit events of global transactions only unimportant for local transactions is consistent with their order on the respective local conflict graph definition extended commitment ordering let math t 1 t 2 math be two committed global transactions in a schedule such that a directed path of unaborted transactions exists in the conflict graph precedence graph from math t 1 math to math t 2 math math t 1 math precedes math t 2 math possibly transitive relation transitively indirectly the schedule has the extended commitment ordering eco property if for every two such transactions math t 1 math commits before math t 2 math commits a distributed algorithm to guarantee global eco exists as for co the algorithm needs only unmodified atomic commitment protocol messages in order to guarantee global serializability each database needs to guarantee also the conflict serializability of its own transactions by any local concurrency control mechanism the eco and global serializability theorem local which implies global eco together with local conflict serializability is a sufficient condition to guarantee global conflict serializability when no concurrency control information beyond atomic commitment messages is shared outside a database autonomy and local transactions can be identified it is also a necessary condition see a necessity proof in raz1993a raz 1993a this condition eco with local serializability is weaker than co and allows more concurrency at the cost of a little more complicated local algorithm however no practical overhead difference with co exists when all the transactions are assumed to be global e g if no information is available about transactions being local eco reduces to co the eco algorithm before a global transaction is committed a generic local to a database eco algorithm aborts a minimal set of undecided transactions neither committed nor aborted either local transactions or global that run locally that can cause later a cycle in the conflict graph this set of aborted transactions not unique contrary to co can be optimized if each transaction is assigned with a weight that can be determined by transaction s importance and by the computing resources already invested in the running transaction optimization can be carried out for example by a reduction from the max flow in networks problem raz1993a raz 1993a like for co such a set is time dependent and becomes empty eventually practically almost in all needed implementations a transaction should be committed only when the set is empty and no set optimization is applicable the local to the database concurrency control mechanism separate from the eco algorithm ensures that local cycles are eliminated unlike with co which implies serializability by itself however practically also for co a local concurrency mechanism is utilized at least to ensure recoverability local transactions can be always committed concurrently even if a precedence relation exists unlike co when the overall transactions local partial order which is determined by the local conflict graph now only with possible temporary local cycles since cycles are eliminated by a local serializability mechanism allows also global transactions can be voted on to be committed concurrently when all their transitively indirect preceding via conflict global transactions are committed while transitively preceding local transactions can be at any state this in analogy to the distributed co algorithm s stronger concurrent voting condition where all the transitively preceding transactions need to be committed the condition for guaranteeing global eco can be summarized similarly to co the global eco enforcing vote ordering strategy theorem let math t 1 t 2 math be undecided neither committed nor aborted global transactions in a database system that ensures serializability locally such that a directed path of unaborted transactions exists in the local conflict graph that of the database itself from math t 1 math to math t 2 math then having math t 1 math ended either committed or aborted before math t 2 math is voted on to be committed in every such database system in a multidatabase environment is a necessary and sufficient condition for guaranteeing global eco the condition guarantees global eco which may be violated without it global eco all global cycles in the global conflict graph are eliminated by atomic commitment together with local serializability i e each database system maintains serializability locally all local cycles are eliminated imply global serializability all cycles are eliminated this means that if each database system in a multidatabase environment provides local serializability by any mechanism and enforces the vote ordering strategy in the theorem above a generalization of co s vote ordering strategy then global serializability is guaranteed no local co is needed anymore similarly to co as well the eco voting deadlock situation can be summarized as follows the eco voting deadlock theorem let a multidatabase environment comprise database systems that enforce each both global eco using the condition in the theorem above and local conflict serializability which eliminates local cycles in the global conflict graph then a voting deadlock occurs if and only if a global cycle spans two or more databases exists in the global augmented conflict graph also blocking by a data access lock is represented by an edge if the cycle does not break by any abort then all the global transactions on it are involved with the respective voting deadlock and eventually each has its vote blocked either directly or indirectly by a data access lock if a local transaction resides on the cycle it may be in any unaborted state running ready or committed unlike co no local commit blocking is needed as with co this means that also global deadlocks due to data access locking with at least one lock blocking are voting deadlocks and are automatically resolved by atomic commitment multi version co mvco multi version commitment ordering mvco raz1993b raz 1993b is a generalization of co for databases with multiversion concurrency control multi version resources with such resources read only transactions do not block or being blocked for better performance utilizing such resources is a common way nowadays to increase concurrency and performance by generating a new version of a database object each time the object is written and allowing transactions read operations of several last relevant versions of each object mvco implies one copy serializability 1ser or 1sr which is the generalization of serializability for multi version resources like co mvco is non blocking and can be combined with any relevant multi version concurrency control mechanism without interfering with it in the introduced underlying theory for mvco conflicts are generalized for different versions of a same resource differently from earlier multi version theories for different versions conflict chronological order is replaced by version order and possibly reversed while keeping the usual definitions for conflicting operations results for the regular and augmented conflict graphs remain unchanged and similarly to co a distributed mvco enforcing algorithm exists now for a mixed environment with both single version and multi version resources now single version is a special case of multi version as for co the mvco algorithm needs only unmodified atomic commitment protocol messages with no additional communication overhead locking based global deadlocks translate to voting deadlocks and are resolved automatically in analogy to co the following holds the mvco and global one copy serializability theorem mvco compliance of every autonomous database system or transactional object in a mixed multidatabase environment of single version and multi version databases is a necessary condition for guaranteeing global one copy serializability 1ser mvco compliance of every database system is a sufficient condition for guaranteeing global 1ser locking based global deadlocks are resolved automatically comment now a co compliant single version database system is automatically also mvco compliant mvco can be further generalized to employ the generalization of eco mveco example co based snapshot isolation cosi co based snapshot isolation cosi is the intersection of snapshot isolation si with mvco si is a multiversion concurrency control method widely utilized due to good performance and similarity to serializability 1ser in several aspects the theory in raz 1993b for mvco described above is utilized later in fekete et al 2005 and other articles on si e g cahill et al 2008 ref name cahill08 michael j cahill uwe r\xc3\xb6hm alan d fekete 2008 http portal acm org citation cfm id 1376690 serializable isolation for snapshot databases proceedings of the 2008 acm sigmod international conference on management of data pp 729 738 vancouver canada june 2008 isbn 978 1 60558 102 6 sigmod 2008 best paper award ref see also snapshot isolation making snapshot isolation serializable making snapshot isolation serializable and the references there for analyzing conflicts in si in order to make it serializable the method presented in cahill et al 2008 serializable snapshot isolation serializablesi a low overhead modification of si provides good performance results versus si with only small penalty for enforcing serializability a different method by combining si with mvco cosi makes si serializable as well with a relatively low overhead similarly to combining the generic co algorithm with single version mechanisms furthermore the resulting combination cosi being mvco compliant allows cosi compliant database systems to inter operate and transparently participate in a co solution for distributed global serializability see below besides overheads also protocols behaviors need to be compared quantitatively on one hand all serializable si schedules can be made mvco by cosi by possible commit delays when needed without aborting transactions on the other hand serializablesi is known to unnecessarily abort and restart certain percentages of transactions also in serializable si schedules co and its variants are transparently interoperable for global serializability with co and its variants e g ss2pl sco oco eco and mvco above global serializability is achieved via atomic commitment protocol based distributed algorithms for co and all its variants atomic commitment protocol is the instrument to eliminate global cycles cycles that span two or more databases in the global augmented and thus also regular conflict graph implicitly no global data structure implementation is needed in cases of either incompatible local commitment orders in two or more databases when no global partial order can embedding embed the respective local partial orders together or a data access locking related voting deadlock both implying a global cycle in the global augmented conflict graph and missing votes the atomic commitment protocol breaks such cycle by aborting an undecided transaction on it see commitment ordering the distributed co algorithm the distributed co algorithm above differences between the various variants exist at the local level only within the participating database systems each local co instance of any variant has the same role to determine the position of every global transaction a transaction that spans two or more databases within the local commitment order i e to determine when it is the transaction s turn to be voted on locally in the atomic commitment protocol thus all the co variants exhibit the same behavior in regard to atomic commitment this means that they are all interoperable via atomic commitment using the same software interfaces typically provided as service systems architecture service s some already international standard standardized for atomic commitment primarily for the two phase commit protocol e g x open xa and transparently can be utilized together in any distributed environment while each co variant instance is possibly associated with any relevant local concurrency control mechanism type in summary any single global transaction can participate simultaneously in databases that may employ each any possibly different co variant while concurrently running processes in each such database and running concurrently with local and other global transactions in each such database the atomic commitment protocol is indifferent to co and does not distinguish between the various co variants any global cycle generated in the augmented global conflict graph may span databases of different co variants and generate if not broken by any local abort a voting deadlock that is resolved by atomic commitment exactly the same way as in a single co variant environment local cycles now possibly with mixed materialized and non materialized conflicts both serializability and data access locking deadlock related e g sco are resolved locally each by its respective variant instance s own local mechanisms vote ordering vo or generalized co gco raz2009 raz 2009 the union of co and all its above variants is a useful concept and global serializability technique to comply with vo local serializability in it most general form commutativity based and including multi versioning and the vote order strategy voting by local precedence order are needed combining results for co and its variants the following is concluded the co variants interoperability theorem in a multi database environment where each database system transactional object is compliant with some co variant property vo compliant any global transaction can participate simultaneously in databases of possibly different co variants and global serializability is guaranteed sufficient condition for global serializability and global one copy serializability 1ser for a case when a multi version database exists if only local to a database system concurrency control information is utilized by every database system each has the generalized autonomy property a generalization of autonomy then compliance of each with some any co variant property vo compliance is a necessary condition for guaranteeing global serializability and global 1ser otherwise they may be violated furthermore in such environment data access locking related global deadlocks are resolved automatically each such deadlock is generated by a global cycle in the augmented conflict graph i e a voting deadlock see above involving at least one data access lock non materialized conflict and two database systems thus not a cycle in the regular conflict graph and does not affect serializability references citation first yoav last raz url http www vldb org conf 1992 p292 pdf title the principle of commitment ordering or guaranteeing serializability in a heterogeneous environment of multiple autonomous resource managers using atomic commitment work proceedings of the eighteenth international conference on very large data bases pages 292 312 place vancouver canada date august 1992 also dec tr 841 digital equipment corporation november 1990 citation first yoav last raz title serializability by commitment ordering work information processing letters volume 51 number 5 pages 257 264 date september 1994 doi 10 1016 0020 0190 94 90005 1 citation first yoav last raz url http sites google com site yoavraz2 home theory of commitment ordering title theory of commitment ordering summary date june 2009 accessdate november 11 2011 citation first yoav last raz url http yoavraz googlepages com dec co memo 90 11 16 pdf title on the significance of commitment ordering publisher digital equipment corporation date november 1990 cite id raz1991a yoav raz 1991a us patents http patft1 uspto gov netacgi nph parser sect1 pto2 sect2 hitoff p 1 u 2fnetahtml 2fpto 2fsearch bool html r 3 f g l 50 co1 and d ptxt s1 22commitment ordering 22 ti os ttl 5 504 899 eco http patft1 uspto gov netacgi nph parser sect1 pto2 sect2 hitoff p 1 u 2fnetahtml 2fpto 2fsearch bool html r 2 f g l 50 co1 and d ptxt s1 22commitment ordering 22 ti os ttl 5 504 900 co http patft1 uspto gov netacgi nph parser sect1 pto2 sect2 hitoff p 1 u 2fnetahtml 2fpto 2fsearch bool html r 1 f g l 50 co1 and d ptxt s1 22commitment ordering 22 ti os ttl 5 701 480 mvco cite cite id raz1991b yoav raz 1991b the commitment order coordinator coco of a resource manager or architecture for distributed commitment ordering based concurrency control dec tr 843 digital equipment corporation december 1991 cite cite id raz1991c yoav raz 1991c locking based strict commitment ordering or how to improve concurrency in locking based resource managers dec tr 844 december 1991 cite cite id raz1993a yoav raz 1993a http portal acm org citation cfm id 153858 extended commitment ordering or guaranteeing global serializability by applying commitment order selectivity to global transactions proceedings of the twelfth acm symposium on principles of database systems pods washington dc pp 83 96 may 1993 also dec tr 842 november 1991 cite cite id raz1993b yoav raz 1993b http ieeexplore ieee org xpl freeabs all jsp arnumber 281924 commitment ordering based distributed concurrency control for bridging single and multi version resources proceedings of the third ieee international workshop on research issues on data engineering interoperability in multidatabase systems ride ims vienna austria pp 189 198 april 1993 also dec tr 853 july 1992 cite footnotes reflist external links http sites google com site yoavraz2 the principle of co yoav raz s commitment ordering page defaultsort commitment ordering category data management category databases category transaction processing category concurrency control category distributed algorithms'
b'notability date november 2014 rainbow storage is a developing paper based data storage device data storage technique first demonstrated by indian student sainul abideen in november 2006 ref name arabnews http www arabnews com page 4 section 0 article 88962 d 18 m 11 y 2006 data can now be stored on paper by m a siraj arabnews published november 18 2006 accessed november 29 2006 ref abideen received his master of computer applications mca from mes college of engineering mes engineering college in kuttipuram in kerala s malappuram district initial newspaper reports of the technology were disputed by multiple technical sources although abideen says those reports were based on a misunderstanding of the technology the paper meant to demonstrate the capability of storing relatively large amounts of data and not necessarily in the gigabyte range using textures and diagrams ref name theinq http www theinquirer net default aspx article 36294 paper storage man misunderstood mdash the inquirer article 12 december 2006 retrieved 15 december 2006 ref the rainbow data storage technology claims to use geometry geometric shapes such as triangles circles and squares of various colors to store a large amount of data on ordinary paper or plastic surfaces this would provide several advantages over current forms of optical disc optical or magnetic storage magnetic data storage device data storage like less environmental pollution due to the biodegradability of paper low cost and high capacity data could be stored on rainbow versatile disk rvd or plastic paper cards of any form factor like sim cards ref name techworld com http www techworld com storage news index cfm newsid 7424 store 256gb on an a4 sheet by chris mellor techworld published november 24 2006 accessed november 29 2006 ref criticism following the wide media attention this news received some of the claims have been disputed by various experts ref name itsoup http itsoup blogspot com 2006 11 scam of indian student developing html it soup scam of indian student developing technology to store 450 gb of data on a sheet of paper by itsoup published november 25 2006 accessed november 25 2006 ref ref name arstechnica http arstechnica com news ars post 20061126 8288 html can you get 256gb on an a4 sheet no way by chris mellor techworld published november 24 2006 accessed november 29 2006 ref printing at 1 200 dots per inch dpi leads to a theoretical maximum of 1 440 000 colored dots per square inch if a scanner can reliably distinguish between 256 unique colors thus encoding one byte per dot the maximum possible storage is approximately 140 megabytes for a sheet of a4 paper ndash much lower when the necessary error correction is employed if the scanner were able to accurately distinguish between 16 777 216 colors 24 bits or 3 bytes per dot the capacity would triple but it still falls well below the media stories claims of several hundred gigabytes printing this quantity of unique colors would require specialized equipment to generate many spot color s the process color model used by most printers provides only four colors with additional colors simulated by a halftone halftone pattern at least one of three things must be true for the claim to be valid the paper must be printed and scanned at a much higher resolution than 1 200 dpi the printer and scanner must be able to accurately produce and distinguish between an extraordinary number of distinct color values or the compression scheme must be a revolutionary lossless compression algorithm the theory is if rainbow s geometric algorithm is to be encoded and decoded by a computer it would equally viable to store the compressed data on a conventional disk rather than printing it to paper or other non digital medium printing something as dots on a page rather than bits on a disk will not change the underlying compression ratio so a lossless compression algorithm that could store 250 gigabytes within a few hundred megabytes of data would be revolutionary indeed likewise data can be compressed with any algorithm and subsequently printed to paper as colored dots the amount of data that can be reliably stored in this way is limited by the printer and scanner as described above however sainul abideen says that the articles are based on misunderstandings he claims it as a method to store data in the form of colour in any medium where colour can be represented not only paper density of storage in paper will be very small and the density will be depends on the storage medium capacity of colour representation and retrieval methods etc demonstrations sainul abdeen demonstrated his technology to the college and members of the indian press in the mes college of engineering computer lab kerala and was able to compress 450 sheets plain text from foolscap folio foolscap paper into a 1 inch square he also demonstrated a 45 second audio clip compressed using this technology on to an iso 216 a4 sheet depending on the sampling frequency bit depth and audio compression if any a 45 second audio clip can consist of anywhere from a few kilobytes to a few megabytes of data abideen claimed that the technology could be extended to 250 gigabytes by using specific materials and devices fact date june 2009 this technology is based on two principles principle i every color or color combinations can be converted into some values and from the values the colors or color combinations can be regenerated principle ii every different color or color combinations will produce different values references reflist absolute rainbow dots absolute rainbow dots are used to detect errors caused by scratches and whether any fading has occurred absolute rainbow dots are predefined dots carrying a unique value these dots can be inserted in the rainbow picture in pre specified areas if fading occurs these dot values will change accordingly and at the reproduction stage this can be checked and corrected absolute rainbow dots will be microscopically small so that they occupy very little space in the rainbow picture these will be colored differently so that each dot will have its own fixed unique value external links http www kerlontech com randd html sainul abideen s home page dead http www deccanherald com deccanherald sep62006 cyberspace163748200695 asp deccan herald s article on rainbow storage dead http www dailytech com article aspx newsid 5052 article in dailytech http itsoup blogspot com 2006 11 scam of indian student developing html it soup scam of indian student developing technology to store 450 gb of data on a sheet of paper http www theregister co uk 2006 11 23 rvd system article in the register http www idm net au storypages storydata asp id 7749 idm paper storage claims a hoax dead category data management category vaporware'
b'refimprove date october 2013 use british english date march 2014 operational historian refers to a database software application that logs or historizes time based process data ref cite web title a practical guide to process data historians and process information systems url http www tappi org downloads unsorted untitled pcei99339pdf aspx publisher tappi accessdate 14 september 2012 author r h rick meeker jr format pdf date 13 january 1999 ref historian software is used to record trends and historical information about industrial processes for future reference it captures plant management information about production status performance monitoring quality assurance tracking and genealogy and product delivery with enhanced data capture data compression and data presentation capabilities ref name globalspecarticle cite web url http www globalspec com learnmore industrial engineering software industrial controls software trending historian software title globalspec historian article accessdate 12 jul 2012 ref operational historians are like enterprise historians but differ in that they are used by engineers on the plant floor rather than by business processes citation needed date february 2015 they are typically cheaper lighter in weight and easier to use and reconfigure than enterprise historians having an operational historian enables at the source analysis of the historical data that is not typically possible with enterprise historians typically these applications offer two layers of data access through a dedicated sdk standard development kit sometimes in two different flavours full administration api application programming interface and high speed read write api as well as user front end tools for instance administration panels engineering consoles or portal like web clients because these applications are designed to fulfil specific operation time requirements their marketing materials often indicate that these are real time database systems ref name example http software schneider electric com products wonderware production information management historian wonderware historian example of naming the operational historian the real time database ref however since such performance measurements are often executed for atomic operations especially write operations not necessarily whole transactions not all of the operational historians must be in fact real time databases usual challenges the operational historians must address are as follows data collection from real time external systems storage and archiving of very large volumes of data tag organisation typically time series database time series where a single sample contains the information about the time stamp the value and the sample quality basic data limit monitoring alarms and user prompts messages performance of read and write operations data access as opposed to enterprise historians the data access layer in the operational historian is designed to offer sophisticated data fetching modes without complex information analysis facilities the following settings are typically available for data access operations data scope single point history based on time range history based on sample count request modes raw data last known value aggregation interpolation sampling single point all points without sampling all points with interval sampling data omission based on the sample quality based on the sample value based on the count even though the operational historians are rarely relational database management system s they often offer sql based interfaces to query the database in most of such implementations the dialect does not follow the sql standard in order to provide syntax for specifying data access operations parameters notable software commercial abb decathlon history aspen technology infoplus 21 ref citation url http www aspentech com aspenone mes brochure pdf format pdf publisher aspen technology inc title aspenone mes brochure page 2 year 2014 accessdate 30 july 2014 ref enea ab enea s polyhedra historian a module of polyhedra dbms ref citation url http developer polyhedra com polyhedra features historian publisher enea ab title handling time series data in polyhedra imdb date 11 may 2012 accessdate 30 july 2014 ref ge intelligent platforms proficy historian ref citation url http www ge ip com account prepsend file proficy historian 5 5 pdf format pdf publisher ge intelligent platforms inc title datasheet proficy historian 5 5 year 2013 accessdate 30 july 2014 ref honeywell uniformance phd ref citation url http www honeywellprocess com library marketing notes uniformance phd pin pdf publisher honeywell international inc title uniformance phd product information note format pdf year 2013 accessdate 30 july 2014 ref iconics http www iconics com home products historians hyper historian aspx hyper historian inductive automation ignition scada sql bridge sql bridge module of ignition scada ref citation url http inductiveautomation com scada software scada modules sqlbridge publisher inductive automation title high powered data acquisition year 2014 accessdate 30 july 2014 ref national instruments citadel used in labview dsc and other products ref citation url http www ni com white paper 6579 en publisher national instruments corp title logging data with national instruments citadel date 19 july 2012 accessdate 30 july 2014 ref osisoft pi system schneider electric instep software http www instepsoftware com instep software products edna enterprise data historian edna real time historian schneider electric wonderware historian ref citation url http global wonderware com en pdf 20library datasheet wonderware historian pdf publisher invensys systems inc title wonderware historian software datasheet format pdf year 2013 accessdate 30 july 2014 ref yokogawa exaquantum historian ref citation url http www yokogawa com eu pims pdf bu 20gmscs0102 02e 20exaquantum 20bulletin 2072ppi pdf publisher yokogawa marex limited title exaquantum delivers production excellence format pdf year 2013 accessdate 30 july 2014 ref jaaji technologies insis historian ref citation url http www jaajitech com infoview publisher jaaji software technologies private limited title find view and analyze your process data from everywhere year 2014 accessdate 30 july 2014 ref see also time series database relational database management system references reflist category data management'
b'project workforce management is the practice of combining the coordination of all logistic elements of a project through a single software application or workflow engine this includes planning and tracking of schedules and mileposts cost and revenue resource allocation as well as overall management of these project elements efficiency is improved by eliminating manual processes like spreadsheet tracking ref cite web author seema haji title business intelligence cures the spreadsheet problem url http www refresher com asmhbi html publisher refresher publications inc year 2009 accessdate october 30 2009 ref to monitor project progress it also allows for at a glance status updates and ideally integrates with existing legacy applications in order to unify ongoing projects enterprise resource planning erp and broader organizational goals ref cite web author rudolf melik title the rise of the project workforce url http www projectworkforcebook com publisher wiley new york ny year 2007 accessdate october 30 2009 ref there are a lot of logistic elements in a project different team members are responsible for managing each element and often the organisation may have a mechanism to manage some logistic areas as well by coordinating these various components of project management workforce management and financials through a single solution the process of configuring and changing project and workforce details is simplified introduction ref citation title project workforce management url http www google com patents us20030236692 accessdate 2015 11 04 ref a project workforce management system defines project tasks project positions and assigns personnel to the project positions the project tasks and positions are correlated to assign a responsible project position or even multiple positions to complete each project task because each project position may be assigned to a specific person the qualifications and availabilities of that person can be taken into account when determining the assignment by associating project tasks and project positions a manager can better control the assignment of the workforce and complete the project more efficiently when it comes to project workforce management it is all about managing all the logistic aspects of a project or an organisation through a software application usually this software has a workflow engine defined therefore all the logistic processes take place in the workflow engine about technical field this invention relates to project management systems and methods more particularly to a software based system and method for project and workforce management ref citation title project workforce management technical field url http www google com patents us20030236692 accessdate 2015 11 04 ref software usage due to the software usage all the project workflow management tasks can be fully automated without leaving many tasks for the project managers this returns high efficiency to the project management when it comes to project tracking proposes in addition to different tracking mechanisms project workforce management software also offer a dashboard for the project team through the dashboard the project team has a glance view of the overall progress of the project elements most of the times project workforce management software can work with the existing legacy software systems such as erp enterprise resource planning systems this easy integration allows the organisation to use a combination of software systems for management purposes ref citation title project workforce management software use url http www google com patents us20030236692 accessdate 2015 11 04 ref background good project management is an important factor for the success of a project a project may be thought of as a collection of activities and tasks designed to achieve a specific goal of the organisation with specific performance or quality requirements while meeting any subject time and cost constraints project management refers to managing the activities that lead to the successful completion of a project furthermore it focuses on finite deadlines and objectives a number of tools may be used to assist with this as well as with assessment project management may be used when planning personnel resources and capabilities the project may be linked to the objects in a professional services life cycle and may accompany the objects from the opportunity over quotation contract time and expense recording billing period end activities to the final reporting naturally the project gets even more detailed when moving through this cycle ref citation title project workforce management background url http www google com patents us20030236692 accessdate 2015 11 04 ref for any given project several project tasks should be defined project tasks describe the activities and phases that have to be performed in the project such as writing of layouts customising testing what is needed is a system that allows project positions to be correlated with project tasks project positions describe project roles like project manager consultant tester etc project positions are typically arranged linearly within the project by correlating project tasks with project positions the qualifications and availability of personnel assigned to the project positions may be considered benefits of project management ref cite web title the advantages of project management and how it can help your business url https www nibusinessinfo co uk content advantages project management and how it can help your business website nibusinessinfo co uk accessdate 2015 11 04 last migrator ref good project management should reduce the chance of a project failing ensure a minimum level of quality and that results meet requirements and expectations free up other staff members to get on with their area of work and increase efficiency both on the project and within the business make things simpler and easier for staff with a single point of contact running the overall project encourage consistent communications amongst staff and suppliers keep costs timeframes and resources to budget workflow engine when it comes to project workforce management it is all about managing all the logistic aspects of a project or an organisation through a software application usually this software has a workflow engine defined in them so all the logistic processes take place in the workflow engine the regular and most common types of tasks handled by project workforce management software or a similar workflow engine are planning and monitoring the project schedules and milestones regularly monitoring your project s schedule performance can provide early indications of possible activity coordination problems resource conflicts and possible cost overruns to monitor schedule performance collecting information and evaluating it ensure a project accuracy ref cite web title how to monitor project schedule performance for dummies url http www dummies com how to content how to monitor schedule performance html website www dummies com accessdate 2015 11 04 ref tracking the cost and revenue aspects of projects the importance of tracking actual costs and resource usage in projects depends upon the project situation tracking actual costs and resource usage is an essential aspect of the project control function ref cite web title why track actual costs and resource usage on projects url http www projecttimes com articles why track actual costs and resource usage on projects html website www projecttimes com accessdate 2015 11 04 ref resource utilisation and monitoring organisational profitability is directly connected to project management efficiency and optimal resource utilisation to sum up organisations that struggle with either or both of these core competencies typically experience cost overruns schedule delays and unhappy customers ref cite web title resource utilization in project management url https www clarizen com work resource utilization in project management website www clarizen com accessdate 2015 11 04 ref the focus for project management is the analysis of project performance to determine whether a change is needed in the plan for the remaining project activities to achieve the project goals ref cite web title project management guru monitoring and controlling tools url http www projectmanagementguru com controlling html website www projectmanagementguru com accessdate 2015 11 04 ref other management aspects of the project management ref cite web title project management guide how to manage a project teamgantt url http teamgantt com guide to project management website teamgantt com accessdate 2015 11 04 ref project risk management risk identification consists of determining which risks are likely to affect the project and documenting the characteristics of each project communication management project communication management is about how communication is carried out during the course of the project project quality management it is of no use completing a project within the set time and budget if the final product is of poor quality the project manager has to ensure that the final product meets the quality expectations of the stakeholders this is done by good \xc2\x83 quality planning identifying what quality standards are relevant to the project and determining how to meet them quality assurance evaluating overall project performance on a regular basis to provide confidence that the project will satisfy the relevant quality standards quality control monitoring specific project results to determine if they comply with relevant quality standards and identifying ways to remove causes of poor performance project workforce management vs traditional management there are three main differences between project workforce management and traditional project management and workforce management disciplines and solutions ref cite web author rudolf melik title the rise of the project workforce url https books google co uk books id 0b2rb81rqyqc pg pa121 lpg pa121 dq the rise of project workforce pdf source bl ots io xyqd2q sig 4ko0i1gr5m xoybvwjhqfp0enhk hl en sa x ved 0cdiq6aewbgovchmimjoz3a33yaivq70ach1ygaq4 v onepage q the 20rise 20of 20project 20workforce 20pdf f false publisher wiley new york ny year 2007 accessdate november 4 2015 ref workflow driven all project and workforce processes are designed controlled and audited using a built in graphical workflow engine users can design control and audit the different processes involved in the project the graphical workflow is quite attractive for the users of the system and allows the users to have a clear idea of the workflow engine ref cite book title flexibility of data driven process structures url http link springer com chapter 10 1007 11837862 19 publisher springer berlin heidelberg date 2006 09 04 isbn 978 3 540 38444 1 pages 181 192 series lecture notes in computer science first dominic last m\xc3\xbcller first2 manfred last2 reichert first3 joachim last3 herbst editor first johann editor last eder editor first2 schahram editor last2 dustdar ref organisation and work breakdown structures project workforce management provides organization and work breakdown structures to create manage and report on functional and approval hierarchies and to track information at any level of detail users can create manage edit and report work breakdown structures work breakdown structures have different abstraction levels so the information can be tracked at any level usually project workforce management has approval hierarchies each workflow created will go through several records before it becomes an organisational or project standard this helps the organisation to reduce the inefficiencies of the process as it is audited by many stakeholders ref cite web title organisational breakdown structure url http www successful project management com organisational breakdown structure html website www successful project management com accessdate 2015 11 04 ref connected project workforce and financial processes unlike traditional disconnected project workforce and billing management systems that are solely focused on tracking it projects internal workforce costs or billable projects project workforce management is designed to unify the coordination of all project and workforce processes whether internal shared it or billable summary a project workforce management system defines project tasks project positions and assigns personnel to the project positions the project tasks and project positions are correlated to assign a responsible project position or positions to complete each project task because each project position may be assigned to a specific person the qualification and availabilities of the person can be taken into account when determining the assignment by correlating the project tasks and project positions a manager can better control the assignment of the workforce and complete projects more efficiently ref citation title project workforce management abstract url http www google com patents us20030236692 accessdate 2015 11 04 ref project workflow management is one of the best methods for managing different aspects of project if the project is complex then the outcomes for the project workforce management could be more effective for simple projects or small organisations project workflow management may not add much value but for more complex projects and big organisations managing project workflow will make a big difference this is because that small organisations or projects do not have a significant overhead when it comes to managing processes there are many project workforce management but many organisations prefer to adopt unique solutions therefore organisation gets software development companies to develop custom project workflow managing systems for them this has proved to be the most suitable way of getting the best project workforce management system acquired for the company literature cite book first rudolf last melik authorlink year 2007 title the rise of the project workforce edition publisher willey location new york ny isbn 0 470 12430 x references wikiquote reflist category data management category erp software category project management category workflow technology'
b'a bitmap index is a special kind of index database database index that uses bit array bitmap s bitmap indexes have traditionally been considered to work well for low cardinality columns which have a modest number of distinct values either absolutely or relative to the number of records that contain the data the extreme case of low cardinality is boolean data e g does a resident in a city have internet access which has two values true and false bitmap indexes use bit array s commonly called bitmaps and answer queries by performing bitwise operation bitwise logical operation s on these bitmaps bitmap indexes have a significant space and performance advantage over other structures for query of such data their drawback is they are less efficient than the traditional b tree indexes for columns whose data is frequently updated consequently they are more often employed in read only systems that are specialized for fast query e g data warehouses and generally unsuitable for online transaction processing applications some researchers argue that bitmap indexes are also useful for moderate or even high cardinality data e g unique valued data which is accessed in a read only manner and queries access multiple bitmap indexed columns using the and or or xor operators extensively ref name sharma http www oracle com technetwork articles sharma indexes 093638 html bitmap index vs b tree index which and when vivek sharma oracle technical network ref bitmap indexes are also useful in data warehousing applications for joining a large fact table to smaller dimension table s such as those arranged in a star schema bitmap based representation can also be used for representing a data structure which is labeled and directed attributed multigraph used for queries in graph databases code http www researchgate net publication 236593640 efficient graph management based on bitmap indices efficient graph management based on bitmap indices code article shows how bitmap index representation can be used to manage large dataset billions of data points and answer queries related to graph efficiently example continuing the internet access example a bitmap index may be logically viewed as follows class wikitable style text align center float left rowspan 2 identifier rowspan 2 hasinternet colspan 2 bitmaps y n 1 yes 1 0 2 no 0 1 3 no 0 1 4 unspecified 0 0 5 yes 1 0 on the left identifier refers to the unique number assigned to each resident hasinternet is the data to be indexed the content of the bitmap index is shown as two columns under the heading bitmaps each column in the left illustration is a bitmap in the bitmap index in this case there are two such bitmaps one for has internet yes and one for has internet no it is easy to see that each bit in bitmap y shows whether a particular row refers to a person who has internet access this is the simplest form of bitmap index most columns will have more distinct values for example the sales amount is likely to have a much larger number of distinct values variations on the bitmap index can effectively index this data as well we briefly review three such variations note many of the references cited here are reviewed at johnwu2007 john wu 2007 ref cite web ref johnwu2007 author john wu year 2007 title annotated references on bitmap index url http www cs umn edu kewu annotated html ref for those who might be interested in experimenting with some of the ideas mentioned here many of them are implemented in open source software such as fastbit ref http codeforge lbl gov projects fastbit fastbit ref the lemur bitmap index c library ref https code google com p lemurbitmapindex lemur bitmap index c library ref the roaring bitmap java library ref http roaringbitmap org roaring bitmaps ref the apache hive data warehouse system and luciddb clear compression software can data compression compress each bitmap in a bitmap index to save spaces there has been considerable amount of work on this subject ref cite book author t johnson editor1 malcolm p atkinson editor2 maria or\xc5\x82owska maria e orlowska editor3 patrick valduriez editor4 stanley b zdonik editor5 michael l brodie title vldb 99 proceedings of 25th international conference on very large data bases september 7 10 1999 edinburgh scotland uk publisher morgan kaufmann year 1999 isbn 1 55860 615 7 chapter performance measurements of compressed bitmap indices pages 278 89 url http www vldb org conf 1999 p29 pdf ref ref cite web vauthors wu k otoo e shoshani a title on the performance of bitmap indices for high cardinality attributes date march 5 2004 url http www osti gov energycitations servlets purl 822860 lozkmz native 822860 pdf ref though there are exceptions such as roaring bitmaps ref name roaring cite journal last1 chambi first1 s last2 lemire first2 d last3 kaser first3 o last4 godin first4 r title better bitmap performance with roaring bitmaps doi 10 1002 spe 2325 journal software practice experience volume 46 pages 5 year 2016 pmid pmc ref bitmap compression algorithms typically employ run length encoding such as the byte aligned bitmap code ref us patent 5363098 byte aligned data compression ref the word aligned hybrid code ref us patent 6831575 word aligned bitmap compression method data structure and apparatus ref the partitioned word aligned hybrid pwah compression ref cite conference url http dl acm org citation cfm doid 1989323 1989419 title a memory efficient reachability data structure through bit vector compression last1 van schaik first1 sebastiaan last2 de moor first2 oege year 2011 publisher acm booktitle proceedings of the 2011 international conference on management of data pages 913 924 location athens greece doi 10 1145 1989323 1989419 conference sigmod 11 isbn 978 1 4503 0661 4 ref the position list word aligned hybrid ref name doi 10 1145 1739041 1739071 cite book chapter position list word aligned hybrid optimizing space and performance for compressed bitmaps vauthors deli\xc3\xa8ge f pedersen tb editor1 ioana manolescu editor2 stefano spaccapietra editor3 jens teubner editor4 masaru kitsuregawa editor5 alain leger editor6 felix naumann editor7 anastasia ailamaki editor8 fatma ozcan title edbt 10 proceedings of the 13th international conference on extending database technology publisher acm location new york ny usa year 2010 pages 228 39 isbn 978 1 60558 945 9 doi 10 1145 1739041 1739071 url http alpha uhasselt be icdt edbticdt2010proc edbt papers p0228 deliege pdf ref the compressed adaptive index compax ref name autogenerated1382 cite journal author1 f fusco author2 m stoecklin author3 m vlachos title net fli on the fly compression archiving and indexing of streaming network traffic date september 2010 volume 3 issue 1 2 pages 1382 93 journal proc vldb endow url http www comp nus edu sg vldb2010 proceedings files papers i01 pdf ref enhanced word aligned hybrid ewah ref name ewah cite journal last1 lemire first1 d last2 kaser first2 o last3 aouiche first3 k title sorting improves word aligned bitmap indexes doi 10 1016 j datak 2009 08 006 journal data knowledge engineering volume 69 pages 3 year 2010 pmid pmc ref and the compressed n composable integer set ref http ricerca mat uniroma3 it users colanton concise html concise compressed n composable integer set webarchive url https web archive org web 20110528033714 http ricerca mat uniroma3 it users colanton concise html date may 28 2011 ref ref name doi 10 1016 j ipl 2010 05 018 these compression methods require very little effort to compress and decompress more importantly bitmaps compressed with bbc wah compax plwah ewah and concise can directly participate in bitwise operation s without decompression this gives them considerable advantages over generic compression techniques such as lz77 bbc compression and its derivatives are used in a commercial database management system bbc is effective in both reducing index sizes and maintaining database query query performance bbc encodes the bitmaps in bytes while wah encodes in words better matching current cpu s on both synthetic data and real application data the new word aligned schemes use only 50 more space but perform logical operations on compressed data 12 times faster than bbc ref cite book vauthors wu k otoo ej shoshani a editor1 henrique paques editor2 ling liu editor3 david grossman chapter a performance comparison of bitmap indexes year 2001 title cikm 01 proceedings of the tenth international conference on information and knowledge management publisher acm location new york ny usa pages 559 61 isbn 1 58113 436 3 doi 10 1145 502585 502689 url http crd lbl gov kewu ps lbnl 48975 pdf ref plwah bitmaps were reported to take 50 of the storage space consumed by wah bitmaps and offer up to 20 faster performance on logical operation s ref name doi 10 1145 1739041 1739071 similar considerations can be done for concise ref name doi 10 1016 j ipl 2010 05 018 cite journal vauthors colantonio a di pietro r title concise compressed n composable integer set journal information processing letters volume 110 issue 16 date 31 july 2010 doi 10 1016 j ipl 2010 05 018 url http ricerca mat uniroma3 it users colanton docs concise pdf pages 644 50 ref and enhanced word aligned hybrid ref name ewah the performance of schemes such as bbc wah plwah ewah compax and concise is dependent on the order of the rows a simple lexicographical sort can divide the index size by 9 and make indexes several times faster ref cite journal author1 d lemire author2 o kaser author3 k aouiche title sorting improves word aligned bitmap indexes journal data knowledge engineering volume 69 issue 1 date january 2010 arxiv 0901 3751 doi 10 1016 j datak 2009 08 006 pages 3 28 ref the larger the table the more important it is to sort the rows reshuffling techniques have also been proposed to achieve the same results of sorting when indexing streaming data ref name autogenerated1382 encoding basic bitmap indexes use one bitmap for each distinct value it is possible to reduce the number of bitmaps used by using a different encoding method ref name autogenerated355 cite book chapter bitmap index design and evaluation author1 c y chan author2 y e ioannidis lastauthoramp yes year 1998 title proceedings of the 1998 acm sigmod international conference on management of data sigmod 98 editor1 ashutosh tiwary editor2 michael franklin publisher acm location new york ny usa pages 355 6 doi 10 1145 276304 276336 url http www comp nus edu sg chancy sigmod98 pdf ref ref cite book chapter an efficient bitmap encoding scheme for selection queries author1 c y chan author2 y e ioannidis lastauthoramp yes year 1999 title proceedings of the 1999 acm sigmod international conference on management of data sigmod 99 publisher acm location new york ny usa pages 215 26 doi 10 1145 304182 304201 url http www ist temple edu vucetic cis616spring2005 papers p4 20p215 chan pdf ref for example it is possible to encode c distinct values using log c bitmaps with binary encoding ref cite journal author1 p e o neil author2 d quass lastauthoramp yes chapter improved query performance with variant indexes title proceedings of the 1997 acm sigmod international conference on management of data sigmod 97 year 1997 editor1 joan m peckman editor2 sudha ram editor3 michael franklin publisher acm location new york ny usa pages 38 49 doi 10 1145 253260 253268 ref this reduces the number of bitmaps further saving space but to answer any query most of the bitmaps have to be accessed this makes it potentially not as effective as scanning a vertical projection of the base data also known as a materialized view or projection index finding the optimal encoding method that balances arbitrary query performance index size and index maintenance remains a challenge without considering compression chan and ioannidis analyzed a class of multi component encoding methods and came to the conclusion that two component encoding sits at the kink of the performance vs index size curve and therefore represents the best trade off between index size and query performance ref name autogenerated355 binning for high cardinality columns it is useful to bin the values where each bin covers multiple values and build the bitmaps to represent the values in each bin this approach reduces the number of bitmaps used regardless of encoding method ref cite book chapter space efficient bitmap indexing title proceedings of the ninth international conference on information and knowledge management cikm 00 year 2000 author n koudas publisher acm location new york ny usa pages 194 201 doi 10 1145 354756 354819 ref however binned indexes can only answer some queries without examining the base data for example if a bin covers the range from 0 1 to 0 2 then when the user asks for all values less than 0 15 all rows that fall in the bin are possible hits and have to be checked to verify whether they are actually less than 0 15 the process of checking the base data is known as the candidate check in most cases the time used by the candidate check is significantly longer than the time needed to work with the bitmap index therefore binned indexes exhibit irregular performance they can be very fast for some queries but much slower if the query does not exactly match a bin history the concept of bitmap index was first introduced by professor israel spiegler and rafi maayan in their research storage and retrieval considerations of binary data bases published in 1985 ref cite journal title storage and retrieval considerations of binary data bases author1 spiegler i author2 maayan r journal information processing and management an international journal volume 21 issue 3 year 1985 doi 10 1016 0306 4573 85 90108 6 pages 233 54 ref the first commercial database product to implement a bitmap index was computer corporation of america s model 204 patrick o neil published a paper about this implementation in 1987 ref name model204 cite conference last o neil first patrick title model 204 architecture and performance booktitle proceedings of the 2nd international workshop on high performance transaction systems pages 40 59 publisher springer verlag year 1987 location london uk editor dieter gawlick editor2 mark n haynie editor3 andreas reuter eds ref this implementation is a hybrid between the basic bitmap index without compression and the list of row identifiers rid list overall the index is organized as a b tree when the column cardinality is low each leaf node of the b tree would contain long list of rids in this case it requires less space to represent the rid lists as bitmaps since each bitmap represents one distinct value this is the basic bitmap index as the column cardinality increases each bitmap becomes sparse and it may take more disk space to store the bitmaps than to store the same content as rid lists in this case it switches to use the rid lists which makes it a b tree index ref cite conference title bit sliced index arithmetic booktitle proceedings of the 2001 acm sigmod international conference on management of data sigmod 01 year 2001 author1 d rinfret p o neil author2 e o neil lastauthoramp yes editor timos sellis publisher acm location new york ny usa pages 47 57 doi 10 1145 375663 375669 ref ref cite conference author1 e o neil author2 p o neil author3 k wu title bitmap index design choices and their performance implications booktitle 11th international database engineering and applications symposium ideas 2007 year 2007 pages 72 84 url http crd lbl gov kewu ps lbnl 62756 pdf isbn 0 7695 2947 x doi 10 1109 ideas 2007 19 ref in memory bitmaps one of the strongest reasons for using bitmap indexes is that the intermediate results produced from them are also bitmaps and can be efficiently reused in further operations to answer more complex queries many programming languages support this as a bit array data structure for example java has the code http download oracle com javase 6 docs api java util bitset html bitset code class some database systems that do not offer persistent bitmap indexes use bitmaps internally to speed up query processing for example postgresql versions 8 1 and later implement a bitmap index scan optimization to speed up arbitrarily complex logical operation s between available indexes on a single table for tables with many columns the total number of distinct indexes to satisfy all possible queries with equality filtering conditions on either of the fields grows very fast being defined by this formula math mathbf c n left frac n 2 right equiv frac n left n left frac n 2 right right left frac n 2 right math ref cite web author alex bolenok date 2009 05 09 title creating indexes url http explainextended com 2009 05 09 creating indexes ref ref cite web author egor timoshenko title on minimal collections of indexes url http explainextended com files index en pdf ref a bitmap index scan combines expressions on different indexes thus requiring only one index per column to support all possible queries on a table applying this access strategy to b tree indexes can also combine range queries on multiple columns in this approach a temporary in memory bitmap is created with one bit for each row in the table 1 mib can thus store over 8 million entries next the results from each index are combined into the bitmap using bitwise operation s after all conditions are evaluated the bitmap contains a 1 for rows that matched the expression finally the bitmap is traversed and matching rows are retrieved in addition to efficiently combining indexes this also improves locality of reference of table accesses because all rows are fetched sequentially from the main table ref cite web author tom lane date 2005 12 26 title re bitmap indexes etc publisher postgresql mailing lists url http archives postgresql org pgsql performance 2005 12 msg00623 php accessdate 2007 04 06 ref the internal bitmap is discarded after the query if there are too many rows in the table to use 1 bit per row a lossy bitmap is created instead with a single bit per disk page in this case the bitmap is just used to determine which pages to fetch the filter criteria are then applied to all rows in matching pages references notes reflist 30em bibliography cite journal last o connell first s year 2005 title advanced databases course notes location southampton publisher university of southampton postscript bot inserted parameter either remove it or change its value to for the cite to end in a as necessary inconsistent citations cite journal last1 o neil first1 p last2 o neil first2 e year 2001 title database principles programming and performance location san francisco publisher morgan kaufmann publishers postscript bot inserted parameter either remove it or change its value to for the cite to end in a as necessary inconsistent citations cite journal last1 zaker first1 m last2 phon amnuaisuk first2 s last3 haw first3 s c year 2008 issue 2 volume 2 title an adequate design for large data warehouse systems bitmap index versus b tree index journal international journal of computers and communications url http www universitypress org uk journals cc cc 21 pdf accessdate 2010 01 07 postscript bot inserted parameter either remove it or change its value to for the cite to end in a as necessary inconsistent citations defaultsort bitmap index category bit data structures index category data management category database index techniques'
b'about large size databases international conference on very large databases vldb a very large database or vldb is a database that contains an extremely high number of tuple s database rows or occupies an extremely large physical filesystem storage space the most common definition of vldb is a database that occupies more than 1 terabyte or contains several billion rows although naturally this definition changes over time citation needed date march 2013 very large databases are often but not necessarily a core component in big data analysis references reflist database defaultsort very large database category data management category types of databases'
b'unreferenced date december 2006 a storage model is a model that captures key physical aspects of data structure in a data store on the other hand a data model is a model that captures key logical aspects of data structure in a database category data management compu storage stub'
b'orphan date february 2009 a technical data management system tdms is essentially a document management system dms pertaining to the management of technical and engineering drawing s and documents often the data are contained in records of various forms such as on paper microfilms or digital media hence technical data management is also concerned with record management involving technical data proper technical document management system management systems are essential for executions within large organisations with large scale projects involving engineering for example tdms is a vital function for the successful management of integrated steel plants isp automobile factories aero space facilities infrastructure companies city corporations research organisations etc in such organisations technical archives or technical documentation centres are created as central facilities for effective management of technical data and records file information processing system english svg alt a simplified example of information flow within a technical data management system thumb a simplified example of information flow within a technical data management system tdms functions are similar to that of conventional archive functions in concepts except that the archived materials in this case are essentially engineering drawings survey maps specification technical specifications plant and equipment data sheets feasibility reports project reports operation and maintenance manuals standards etc document registration indexing repository management reprography etc are parts of tdms various kinds of sophisticated technologies such as document scanners microfilming and digitization camera units wide format printers digital plotters software etc are available now making tdms functions an easier process than previous times crucial constituents of a technical data management system technical data refers to both scientific and technical information recorded and presented in any form or manner excluding financial and management information ref cite web url http www businessdictionary com definition technical data html title what is technical data definition and meaning date 2015 11 03 accessdate 2015 11 03 website businessdictionary com publisher webfinance inc last first ref a technical data management system is created within an organisation for archiving and sharing information such as technical specifications datasheets and drawings similar to other types of data management system a technical data management system consists of the 4 crucial constituents mentioned below data planning data plans long term or short term are constructed as the first essential step of a proper and complete tdms it is created to ultimately help with the 3 other constituents data acquisition data management and data sharing a proper data plan should not exceed 2 pages and should address the following basics ref cite web url https www libraries psu edu psul pubcur what is dm html data planning title data planning date 2015 11 03 accessdate 2015 11 03 website data curation publisher penn state university libraries last first ref types of data samples experiment results reports drawings etc and metadata data that summarizes and describes other data in this case it refers to details such as sample sizes experiment conditions and procedures dates of reports explanations of drawings etc ref cite web url http whatis techtarget com definition metadata title metadata date july 2014 accessdate 2015 11 03 website whatis com publisher search engine optimization seo last rouse first margaret ref means of researches and collections of data field works experiments in production lines etc costs of researches policies for access sharing re use within the organisation and re distribution to the public proposals for archiving data and maintaining access to it data acquisition raw data is collected from primary sites of the organisations through the use of modern technologies ref name 0 cite web url http sine ni com cs app doc p id cs 13019 title by using powerful default components tdm ni datafinder and diadem and without using a database we considerably reduced our creation and maintenance costs date 2015 11 03 accessdate 2015 11 03 website national instruments publisher a solution gmbh last finkl first karl ref please reference the table below for examples ref name 0 class wikitable organisations raw data primary sites technologies integrated steel plants automobile factories feasibility reports equipment datasheets etc test rigs and controls transiting software to digitize data and input software for recording report results and details on datasheets aero space facilities engineering drawings operation manuals maintenance logs etc engineering labs scanners for engineering drawings input software for maintenance logs city corporations survey maps population reports etc city to be mapped and city that involves the research digital cameras for survey maps input software for statistics of population the data collected is then transferred to technical data centres for data management data management after data acquisition data is sorted out whilst useful data is archived unwanted data is disposed when managing and archiving data the features below of the data are considered ref cite web url https www libraries psu edu psul pubcur what is dm html data management title data management date 2015 11 03 accessdate 2015 11 03 website data curation publisher penn state university libraries last first ref names labels values and descriptions for variables and records in the case of tdms one example is names of equipments on an equipment datasheet derived data from the original data with code algorithm or command file used to create them in the case of tdms one example is an expectation report derived from the analysis of an equipment datasheet metadata associates with the data being archived data sharing archived and managed data are accessible to rightful entities a proper and complete tdms should share data to a suitable extent under suitable security in order to achieve optimal usage of data within the organisation it aims for easy access when reused by other researchers and hence it enhances other research processes data is often referred in other tests and specification technical standard technical specifications where new analysis is generated managed and archived again as a result data is flowing within the organisation under effective management through the use of tdms ref cite web url https www libraries psu edu psul pubcur what is dm html data sharing title data sharing date 2015 11 03 accessdate 2015 11 03 website data curation publisher penn state university libraries last first ref advantages and disadvantages of usage of technical data management systems there are strengths and weakness when using technical data management systems tdms to archive data some of the advantages and disadvantages are listed below ref cite web url https razorleaf com solutions technologies product data management title product data management technical data management pdm tdm date 2015 11 03 accessdate 2015 11 03 website razorleaf solutions publisher razorleaf corporation last first ref ref cite web url http arxiv org ftp arxiv papers 1008 1008 1321 pdf title contributions of pdm systems in organiza tional technical data management date 2015 11 03 accessdate 2015 11 03 website publisher mechanical engineering informatics and virtual product development division mivp vienna university of technology last1 ahmed first1 zeeshan last2 gerhard first2 detlef ref ref cite web url http www flosim com calcium aspx title calcium technical data management date 2015 11 03 accessdate 2015 11 03 website flow simulation publisher flow simulation ltd last first ref advantages 1 faster and easier data management since tdms is integrated into the organisation s systems whenever workers develop data files solidworks autocad microsoft word etc they can also archive and manage data linking what they need to their current work at the same time they can also update the archives with useful data this speeds up working processes and makes them more efficient 2 increased security all data files are centralized hence internal and external data leakages are less likely to happen and the data flow is more closely monitored as a result data in the organisation is more secured 3 increased collaboration within the organisation since the data files are centralized and the data flow within the organisation increases researchers and workers within the organisation are able to work on joint projects more complex tasks can be performed for higher yields 4 compatible to various formats of data tdms is compatible to many formats of data from basic data like microsoft words to complex data like voice data this enhances the quality of the management of data archived disadvantages 1 higher financial costs implementing tdms into the organisation s systems involves monetary costs maintenance costs certain amount of human resources and money as well these resources involve opportunity costs as they can be utilized in other aspects 2 lower stability since tdms manages and centralizes all the data the organisation processes it links the working processes within the whole organisation together it also increases the vulnerability of the organisation data network if tdms is not stable enough or when it is exposed to hacker and virus attacks the organisation s data flow might shut down completely affecting the work in an organisation wide scale and leading to a lower stability as results comparison between traditional data management approaches and technical data management systems test engineers and researchers are facing great challenges in turning complex test results and simulation data into usable information for higher yields of firms these challenges are listed below ref cite web url http www ni com white paper 7389 en title from raw data to engineering results the ni technical data management solution date 2015 10 13 accessdate 2015 11 03 website publisher national instruments last first ref increase in complication of designs reduced in time and budgets available higher quality is demanded file logo oracle jpg alt a company logo for oracle thumb a company logo for oracle traditional data management approaches many organisations are still applying the conventional file management systems due to the difficulty in building a proper and complete archives for data management the first approach is the simple file folder system this costs the problem of ineffectiveness as workers and researchers have to manually go through numerous layers of systems and files for the target data moreover the target data may contain files with different formats and these files may not be stored in the same machine these files are also easily lost if renamed or moved to another location the second approach is conventional databases such as oracle these databases are capable of enabling easy search and access of data however a great drawback is that huge effort for preparing and modeling the data is required for large scale projects huge monetary costs are induced and extra it human resources must be employed for constant handling expanding and maintaining the inflexible system which is custom for specific tasks instead of all tasks in the long term it is not cost effective technical data management systems tdms tdms is developed based on 3 principles flexible and organized file storage self scaling hybrid data index and an interactive post processing environment the system in practical mainly consists of 3 components data files with essential and relevant metadata data finders for organizing and managing data regardless of files formats and a software of searching analyzing and reporting with metadata attached to original data files the data finder can identify different related data files during searches even if they are in different file formats tdms hence allows researchers to search for data like browsing the internet last but not least it can adapt to changes and update itself according to the changes unlike databases comparison between strong information systems and weak information systems complex organizations may need large amounts of technical information which can be distributed among several independent archives existing approaches span from no integration to strong integration that is based on a common database or product model the so called weak information systems wis ref cite conference url http www marcolazzari it publications weak information systems for technical data management preprint pdf title weak information systems for technical data management first last1 salvaneschi first1 paolo last2 lazzari first2 marco year 1997 conference worldwide ecce symposium on computers in the practice of building and civil engineering location lahti finland pages 310 314 access date 2015 11 29 ref lie somewhere in the middle their basic concept is to add to the pre existing information a new layer of multiple partial models of products and processes so that it is possible to reuse existing databases to reduce the development from scratch and to provide evolutionary paths relevant for the development of the wis each partial model may include specific knowledge and it acts as a way to structure and access the information according to a specific user view the comparison between strong and weak information systems may be summarized as follows class wikitable strong information systems weak information systems common data model multiple specific integration models database oriented architecture integration of multiple data sources by adding integration layers one shot design growing process redesign of legacy systems integration of legacy systems the architecture of a weak information system is composed of information sources databases computational programs the integration layer the integration layer comprises the following sub layers abstraction layer information models communication layer between models and information sources communication layer between models and humans human computer interface technical data management systems in terms of regulations in different countries in some countries such as in the us record and document management are considered very vital functions and much stress is given in the management of technical archives records and documents coming under the public domain are governed by appropriate laws ref cite web url http apps americanbar org lpm lpt articles tch01093 shtml title document management in the digital law office date january 2009 accessdate 2015 11 03 website law practice today publisher american bar association last1 best last2 foster first1 steven j first2 debbie ref however this has not been so in many underdeveloped and developing country developing nations for example india enacted the public records act ref cite web url http nationalarchives nic in writereaddata html en files html public records93 html title the public records act 1993 india date 1993 12 22 accessdate 2015 11 03 website publisher government of india last mohanpuria first k l ref in 1993 however many in the country are not aware of the existence of such a law or its importance applications and examples of technical data management systems technical data management systems tdms are widely applied across the globe in different sectors some of the examples are listed below voith hydro tests models of the power plant turbines including 4 main program parts engine characteristics values oscillation and cavitation and transfer data from 1 program part to the next one using tdms ref name 0 danburykline created a knowledge and data platform soros which is following the wiki based approach it aims to represent data in accessible and simple forms ref cite web url http danburykline co uk dkwp page id 967 title knowledge technical data management date 2015 11 03 accessdate 2015 11 03 website publisher danburykline last first ref berghof develops and provides a tdms to simplify and manage data for development of firms including automobile firms this tdms enables reserve of data centralization of data volumes on an online server it is also compatible to windows pc and many other systems ref cite web url http www berghof com en products test engineering technical data management title data availability date 2015 11 03 accessdate 2015 11 03 website test engineering technical data management publisher berghof last first ref this journal proposes the use of cloud database cloud tdms in third world countries for higher education purposes republic of sudan is the model in this journal some of the solutions mentioned include online course delivery and online assignments and tests for greater class participation weaknesses mentioned include high financial costs and the fact that underdeveloped countries have not enough infrastructure to support such proposal ref cite journal url http airccse org journal ijdms papers 7315ijdms02 pdf title cloud computing architecture for higher education in the third world countries republic of the sudan as model last adrees first mohmed sirelkhtem date june 2015 journal international journal of database management systems ijdms doi 10 5121 ijdms 2015 7302 pmid access date 2015 11 03 volume 7 last3 sheta last2 omer first2 majzoob kamal aldein first3 osama e issue 3 ref this journal is about text simplification the purpose of this text simplification project in the journal is to simplify high level knowledge in english so that students in high level studies who do not have sufficient english foundations can learn about these knowledge and data more easily the method to do so suggested by the journal is to introduce a tdms that can transform complicated english words into easier words a problem with this project is that the internet is flooded with useless information and it is very difficult to sort out useful information for simplification ref cite journal url http airccse org journal ijdms papers 7415ijdms01 pdf title software feasibility study to transform complex scientific written knowledge to a clear rationale and simple language last khandelwal first manoj date august 2015 journal international journal of database management system ijdms doi 10 5121 ijdms 2015 7401 pmid access date 2015 11 03 last2 jafarabad first2 mohammad issue 4 volume 7 ref this journal mentions about river basin river basin information system rbis which monitors data of different parts of a river basin in order to identify which parts of the basin are gauging data is dynamic and lots of information has to be taken which is impossible to do it manually rbis can help with this but one current weakness is that there are only 2 synoptic stations working kara and niamtougou whilst the rest are out of order ref cite journal url http airccse org journal ijdms papers 7115ijdms02 pdf title an information for integrated land and water resources management in the kara river basin togo and benin last badjana first h\xc3\xa8ou mal\xc3\xa9ki date february 2015 journal international journal of database management system ijdms doi 10 5121 ijdms 2015 7102 pmid access date 2015 11 03 last2 zander first2 franziska issue 1 volume 7 last3 kralisch first3 sven last4 helmschrot first4 j\xc3\xb6rg last5 fl\xc3\xbcgel first5 wolfgang albert ref data mining data mining is an important criteria in constructing a technical data management system for example in building a e commence platform tdms is needed to search and display information about the products ref cite journal url http airccse org journal ijdms papers 7115ijdms01 pdf title web mining on indonesia e commerce site lazada and rakuten last simanjuntak first humasak date february 2015 journal international journal of database management system ijdms doi 10 5121 ijdms 2015 7101 pmid access date 2015 11 03 last2 sibarani first2 novitasari issue 1 volume 7 last3 inaga first3 bambang last4 hutabarat first4 novalina ref it indicates that it is essential to gather information from other systems to archive and manage it properly and finally to share it to users see also data management system data management system data mining database information systems research further reading http airccse org journal ijdms papers 7115ijdms03 pdf ref cite journal url http airccse org journal ijdms papers 7115ijdms03 pdf title mining closed sequential patterns in large sequence databases last raju first v purushothama date february 2015 journal international journal of database management system ijdms doi 10 5121 ijdms 2015 7103 pmid access date 2015 11 03 last2 varma first2 g p saradhi issue 1 volume 7 ref https seer lcc ufmg br index php jidm ref cite journal url https seer lcc ufmg br index php jidm title journal of information and data management date february 2015 journal journal of information and data management doi pmid access date 2015 11 03 volume 6 issue 1 editor last traina junior editor first caetano editor2 last cordeiro editor2 first robson l f editor3 last amo editor3 first sandra de display editors 3 editor4 last davis editor4 first clodoveu issn 2178 7107 ref external links http airccse org journal ijdms editorialboard html references references references nowiki category data management category document management systems category systems engineering'
b'multiple issues weasel date december 2010 orphan date february 2012 cleanup date december 2010 the enterprise bus matrix is a data warehouse planning tool and model created by ralph kimball and is part of the data warehouse bus architecture the matrix is the logical definition of one of the core concepts of kimball s approach to dimensional modeling conformed dimensions ref cite web url http www kimballgroup com 2003 09 15 design tip 49 off the bench title design tip 49 off the bench publisher kimball group date 2003 09 15 accessdate 2015 05 22 dead link date december 2016 bot internetarchivebot fix attempted yes ref the bus matrix defines part of the data warehouse bus architecture and is an output of the business requirements phase in the kimball lifecycle it is applied in the following phases of dimensional modeling and development of the data warehouse the matrix can be categorized as a hybrid model being part technical design tool part project management tool and part communication tool ref name kimball kimball ralph ross margy the data warehouse toolkit the complete guide to dimensional modeling 2nd edition john wiley sons 2002 ref background the enterprise bus matrix stems from the issue of how one goes about creating the overall data warehouse environment historically there has been the structure of the centralized and planned approach and the more loosely defined department specific solutions developed in a more independent matter autonomous projects can result in a range of isolated stove pipe data marts naturally each approach has its issues the overall visionary approach often struggles with long delivery cycles and lack of reaction time as the formalities and scope issues is evident on the other hand the development of isolated data marts leading to stovepipe system s that lacks synergy in development over time this approach will lead to a so called data mart in a box architecture ref http www mimno com avoiding mistakes3 html 6 webarchive url https web archive org web 20100704220014 http www mimno com avoiding mistakes3 html 6 date july 4 2010 ref where interoperability and lack of cohesion is apparent and can hinder the realization of an overall enterprise data warehouse as an attempt to handle this matter ralph kimball introduced the enterprise bus bus matrix the bus matrix purpose is one of high abstraction and visionary planning on the data warehouse architectural level by dictating coherency in the development and implementation of an overall data warehouse the bus architecture approach enables an overall vision of the broader enterprise integration and consistency while at the same time dividing the problem into more manageable parts ref name kimball all in a technology and software independent manner ref cite web url http www b eye network com view 713 title data warehouse ralph kimball s vision by katherine drewek publisher beyenetwork date 2005 03 16 accessdate 2015 05 22 ref the bus matrix and architecture builds upon the concept of conformed dimensions creating a structure of common dimensions that ideally can be used across the enterprise by all business processes related to the dw and the corresponding fact tables from which they derive their context according to kimball and margy ross s article differences of opinion ref cite web url http intelligent enterprise informationweek com showarticle jhtml jsessionid 0ovjnehmprxgrqe1ghrskh4atmy32jvn articleid 17800088 title enterprise software news analysis advice informationweek publisher intelligent enterprise informationweek com date accessdate 2015 05 22 ref the enterprise data warehouse built on the bus architecture identifies and enforces the relationship between business process metrics facts and descriptive attributes dimensions the concept of a bus computing bus is well known in the language of information technology and is what reflects the conformed dimension concept in the data warehouse creating the skeletal structure where all parts of a system connect ensuring interoperability and consistency of data and at the same time considers future expansion this makes the conformed dimensions act as the integration glue creating a robust backbone of the enterprise data warehouse ref cite web url http intelligent enterprise informationweek com showarticle jhtml jsessionid gms3h4sobfqbbqe1ghoskh4atmy32jvn articleid 17800088 pgno 2 title enterprise software news analysis advice informationweek publisher intelligent enterprise informationweek com date accessdate 2015 05 22 ref establishment and applicability figure 1 ref cite web url http www widama us documents kimball dimensionalmodeling pdf format pdf title dimensional modeling overview author bob becker publisher widama is accessdate 2015 05 22 deadurl yes archiveurl https web archive org web 20130322224742 http www widama us 80 documents kimball dimensionalmodeling pdf archivedate 2013 03 22 df ref shows the base for a single document planning tool for the whole of the dw implementation a graphical overview of the enterprises core business processes or events each correspond to a measurement table of facts that typically is complemented by a major source system in the horizontal rows in the vertical columns the groups of contextual data is found as the common conformed dimensions in this way the shared dimensions are defined as each process indicates what dimensions it applies to through the cells figure 2 ref name kimball by this definition and coordination of conformed dimensions and processes the development of the overall data dw bus architecture is realized ref name kimball the matrix identifies the shared dimensions related to processes and fact tables and can be a tool for planning prioritizing what needs to be approached coordinating implementation and communicating the importance for conformed dimensions kimball extends the matrix bus in detail as seen in figure 3 ref name kimball by introducing the other steps of the datawarehouse methodology the fact tables granularity and at last the description of the needed facts description of the fact tables granularity and fact instances of each process structuring and specifying what is needed across the enterprise in a more specific matter further exemplifying how the matrix can be used as a planning tool references reflist defaultsort enterprise bus matrix category business intelligence category data management category data warehousing category information technology management'
b'technical date january 2017 in concurrency control of database s transaction processing transaction management and other transactional distributed computing distributed applications global serializability or modular serializability is a property of a global schedule of database transaction transactions a global schedule is the unified schedule computer science schedule of all the individual database and other transactional object schedules in a multidatabase environment e g federated database complying with global serializability means that the global schedule is serializable databases serializable has the serializability property while each component database module has a serializable schedule as well in other words a collection of serializable components provides overall system serializability which is usually incorrect a need in correctness across databases in multidatabase systems makes global serializability a major goal for global concurrency control or modular concurrency control with the proliferation of the internet cloud computing grid computing and small portable powerful computing devices e g smartphone s as well as increase in systems management sophistication the need for atomic distributed transactions and thus effective global serializability techniques to ensure correctness in and among distributed transactional applications seems to increase in a federated database system or any other more loosely defined multidatabase system which are typically distributed in a communication network transactions span multiple and possibly distributed database distributed databases enforcing global serializability in such system where different databases may use different types of concurrency control is problematic even if every local schedule of a single database is serializable the global schedule of a whole system is not necessarily serializable the massive communication exchanges of conflict information needed between databases to reach serializability view and conflict serializability conflict serializability globally would lead to unacceptable performance primarily due to computer and communication latency engineering latency achieving global serializability effectively over different types of concurrency control has been open problem open for several years commitment ordering or commit ordering co a serializability technique publicly introduced in 1991 by yoav raz from digital equipment corporation dec provides an effective general solution for global serializability view and conflict serializability conflict serializability across any collection of database systems and other transactional object s with possibly different concurrency control mechanisms co does not need the distribution of conflict information but rather utilizes the already needed unmodified atomic commitment protocol messages without any further communication between databases it also allows optimistic concurrency control optimistic non blocking implementations co generalizes two phase locking strong strict two phase locking ss2pl which in conjunction with the two phase commit protocol two phase commit 2pc protocol is the de facto standard for achieving global serializability across ss2pl based database systems as a result co compliant database systems with any different concurrency control types can transparently join existing ss2pl based solutions for global serializability the same applies also to all other multiple transactional object systems that use atomic transactions and need global serializability for correctness see examples above nowadays such need is not smaller than with database systems the origin of atomic transactions the most significant aspects of co that make it a uniquely effective general solution for global serializability are the following seamless low overhead integration with any concurrency control mechanism with neither changing any transaction s operation scheduling or blocking it nor adding any new operation heterogeneity global serializability is achieved across multiple transactional objects e g database management system s with different any concurrency control mechanisms without interfering with the mechanisms operations modularity transactional objects can be added and removed transparently commitment ordering co is a necessary condition for global serializability across autonomous database systems autonomy of transactional objects no need of conflict or equivalent information distribution e g local precedence relations locks timestamps or tickets no object needs other object s information scalability with normal global transactions computer network size and number of transactional objects can increase unboundedly with no impact on performance and automatic global deadlock resolution all these aspects except the first two are also possessed by the popular two phase locking ss2pl which is a constrained blocking special case of co and inherits many of co s qualities the global serializability problem problem statement the difficulties described above translate into the following problem find an efficient high performance and fault tolerant method to enforce global serializability global conflict serializability in a heterogeneous distributed environment of multiple autonomous database systems the database systems may employ different concurrency control methods no limitation should be imposed on the operations of either local transactions confined to a single database system or distributed transaction global transactions span two or more database systems quotations lack of an appropriate solution for the global serializability problem has driven researchers to look for alternatives to serializability as a correctness criterion in a multidatabase environment e g see global serializability relaxing global serializability relaxing global serializability below and the problem has been characterized as difficult and open problem open the following two quotations demonstrate the mindset about it by the end of the year 1991 with similar quotations in numerous other articles without knowledge about local as well as global transactions it is highly unlikely that efficient global concurrency control can be provided additional complications occur when different component dbmss database management systems and the fdbmss federated database management systems support different concurrency mechanisms it is unlikely that a theoretically elegant solution that provides conflict serializability without sacrificing performance i e concurrency and or response time and availability exists ref amit sheth james larson 1990 http www informatik uni trier de ley db journals csur shethl90 html federated database systems for managing distributed heterogeneous and autonomous databases acm computing surveys vol 22 no 3 pp 183 236 september 1990 quotation from page 227 ref commitment ordering ref name raz1992 ref name raz1994 publicly introduced in may 1991 see below provides an efficient elegance elegant general solution from both practical ref name raz1990 ref name raz1991 and theory theoretical ref name raz2009 points of view to the global serializability problem across database systems with possibly different concurrency control mechanisms it provides conflict serializability with no negative effect on availability and with no worse performance than the de facto standard for global serializability co s special case two phase locking strong strict two phase locking strong strict two phase locking ss2pl it requires knowledge about neither local nor global transactions transaction management in a heterogeneous distributed database system is a difficult issue the main problem is that each of the local database management systems may be using a different type of concurrency control scheme integrating this is a challenging problem made worse if we wish to preserve the local autonomy of each of the local databases and allow local and global transactions to execute in parallel one simple solution is to restrict global transactions to retrieve only access however the issue of reliable transaction management in the general case where global and local transactions are allowed to both read and write data is open problem still open ref abraham silberschatz michael stonebraker and jeffrey ullman 1991 http www informatik uni trier de ley db journals cacm silberschatzsu91 html database systems achievements and opportunities communications of the acm vol 34 no 10 pp 110 120 october 1991 quotation from page 120 ref the commitment ordering solution comprises effective integration of autonomous database management systems with possibly different concurrency control mechanisms this while local and global transactions execute in parallel without restricting any read or write operation in either local or global transactions and without compromising the systems autonomy even in later years after the public introduction of the commitment ordering general solution in 1991 the problem still has been considered by many unsolvable we present a transaction model for multidatabase systems with autonomous component systems coined heterogeneous 3 level transactions it has become evident that in such a system the requirements of guaranteeing full acid properties and full local autonomy can not be reconciled ref peter muth 1997 http portal acm org citation cfm id 264226 application specific transaction management in multidatabase systems distributed and parallel databases volume 5 issue 4 pp 357 403 october 1997 issn 0926 8782 quotation from the article s abstract ref the quotation above is from a 1997 article proposing a relaxed global serializability solution see global serializability relaxing global serializability relaxing global serializability below and referencing commitment ordering co articles the co solution supports effectively both full acid properties and full local autonomy as well as meeting the other requirements posed above in the global serializability problem statement problem statement section and apparently has been misunderstood similar thinking we see also in the following quotation from a 1998 article the concept of serializability has been the traditionally accepted correctness criterion in database systems however in multidatabase systems mdbss ensuring global serializability is a difficult task the difficulty arises due to the heterogeneity of the concurrency control protocols used by the participating local database management systems dbmss and the desire to preserve the autonomy of the local dbmss in general solutions to the global serializability problem result in executions with a low degree of concurrency the alternative relaxed serializability may result in data inconsistency ref name shar1998 sharad mehrotra rajeev rastogi henry korth abraham silberschatz 1998 http portal acm org citation cfm id 277629 ensuring consistency in multidatabases by preserving two level serializability acm transactions on database systems tods vol 23 no 2 pp 199 230 june 1998 quotation from the article s abstract ref also the above quoted article proposes a relaxed global serializability solution while referencing the co work the co solution for global serializability both bridges between different concurrency control protocols with no substantial concurrency reduction and typically minor if at all and maintains the autonomy of local dbmss evidently also here co has been misunderstood this misunderstanding continues to 2010 in a textbook by some of the same authors where the same relaxed global serializability technique two level serializability is emphasized and described in detail and co is not mentioned at all ref name silber2010 abraham silberschatz avi silberschatz henry f korth s sudarshan 2010 http highered mcgraw hill com sites 0073523321 database system concepts 6th edition mcgraw hill isbn 0 07 295886 3 ref on the other hand the following quotation on co appears in a 2009 book ref name bern2009 phil bernstein philip a bernstein eric newcomer 2009 http www elsevierdirect com product jsp isbn 9781558606234 principles of transaction processing 2nd edition morgan kaufmann elsevier june 2009 isbn 978 1 55860 623 4 quotation from page 145 ref not all concurrency control algorithms use locks three other techniques are timestamp ordering serialization graph testing and commit ordering timestamp ordering assigns each transaction a timestamp and ensures that conflicting operations execute in timestamp order serialization graph testing tracks conflicts and ensures that the serialization graph is acyclic commit ordering ensures that conflicting operations are consistent with the relative order in which their transactions commit which can enable interoperability of systems using different concurrency control mechanisms comments beyond the common locking based algorithm ss2pl which is a co variant itself also additional variants of co that use locks exist see below however generic or pure co does not use locks since co mechanisms order the commit events according to conflicts that already have occurred it is better to describe co as commit ordering ensures that the relative order in which transactions commit is consistent with the order of their respective conflicting operations the characteristics and properties of the co solution are discussed below proposed solutions several solutions some partial have been proposed for the global serializability problem among them global serializability testing conflict serializability conflict graph serializability graph precedence graph checking distributed two phase locking distributed 2pl distributed timestamp based concurrency control timestamp ordering tickets local logical timestamps which define local total orders and are propagated to determine global partial order of transactions commitment ordering technology perspective the problem of global serializability has been a quite intensively researched subject in the late 1980s and early 1990s commitment ordering co has provided an effective general solution to the problem insight into it and understanding about possible generalizations of serializability common mechanism ss2pl strong strict two phase locking ss2pl which practically and almost exclusively has been utilized in conjunction with the two phase commit protocol 2pc since the 1980s to achieve global serializability across databases an important side benefit of co is the automatic global deadlock resolution that it provides this is applicable also to distributed ss2pl though global deadlocks have been an important research subject for ss2pl automatic resolution has been overlooked except in the co articles until today 2009 at that time quite many commercial database system types existed many non relational and databases were relatively very small multi database systems were considered a key for database scalability by database systems interoperability and global serializability was urgently needed since then the tremendous progress in computing power storage and communication networks resulted in order of magnitude orders of magnitude increases in both centralized databases sizes transaction rates and remote access to database capabilities as well as blurring the boundaries between centralized computing and distributed one over fast low latency local networks e g infiniband these together with progress in database vendors distributed solutions primarily the popular ss2pl with 2pc based a de facto standard that allows interoperability among different vendors ss2pl based databases both ss2pl and 2pc technologies have gained substantial expertise and efficiency workflow management systems and database replication technology in most cases have provided satisfactory and sometimes better information technology solutions without multi database atomic distributed transaction s over databases with different concurrency control bypassing the problem above as a result the sense of urgency that existed with the problem at that period and in general with high performance distributed atomic transactions over databases with different concurrency control types has reduced however the need in concurrent distributed atomic transactions as a fundamental element of reliability exists in distributed systems also beyond database systems and so the need in global serializability as a fundamental correctness criterion for such transactional systems see also serializability distributed serializability distributed serializability in serializability with the proliferation of the internet cloud computing grid computing small portable powerful computing devices e g smartphone s and sophisticated systems management the need for effective global serializability techniques to ensure correctness in and among distributed transactional applications seems to increase and thus also the need in commitment ordering including the popular for databases special case ss2pl ss2pl though does not meet the requirements of many other transactional objects the commitment ordering solution pov section commitment ordering date november 2011 main commitment ordering main the history of commitment ordering commitment ordering ref name raz1992 yoav raz 1992 http www informatik uni trier de ley db conf vldb raz92 html the principle of commitment ordering or guaranteeing serializability in a heterogeneous environment of multiple autonomous resource managers using atomic commitment webarchive url https web archive org web 20070523182950 http www informatik uni trier de ley db conf vldb raz92 html date may 23 2007 proc of the eighteenth int conf on very large data bases vldb pp 292 312 vancouver canada august 1992 also dec tr 841 digital equipment corporation november 1990 ref ref name raz1994 yoav raz 1994 http linkinghub elsevier com retrieve pii 0020019094900051 serializability by commitment ordering information processing letters http www informatik uni trier de ley db journals ipl ipl51 html raz94 volume 51 number 5 pp 257 264 september 1994 received august 1991 ref or commit ordering co is the only high performance fault tolerant serializability view and conflict serializability conflict serializability providing solution that has been proposed as a fully distributed no central computing component or data structure are needed general mechanism that can be combined seamlessly with any local to a database concurrency control mechanism see commitment ordering summary technical summary since the co property of a schedule is a necessary condition for global serializability of commitment ordering co is a necessary condition for global serializability across autonomous database systems autonomous databases in the context of concurrency control it provides the only general solution for autonomous databases i e if autonomous databases do not comply with co then global serializability may be violated seemingly by sheer luck the co solution possesses many attractive properties does not interfere with any transaction s operation particularly neither block restrict nor delay any data access operation read or write for either local or distributed transaction global transactions and thus does not cause any extra aborts thus allows seamless integration with any concurrency control mechanism allows optimistic concurrency control optimistic implementations non blocking i e non data access blocking allows heterogeneity global serializability is achieved across multiple transactional objects with different any concurrency control mechanisms without interfering with the mechanisms operations allows modularity transactional objects can be added and removed transparently allows full acid transaction support maintains each database s commitment ordering co is a necessary condition for global serializability across autonomous database systems autonomy and does not need any concurrency control information distribution e g local precedence relations locks timestamps or tickets does not need any knowledge about the transactions requires no communication overhead since it only uses already needed unmodified atomic commitment protocol messages any such protocol using fault tolerant atomic commitment protocols and database systems makes the co solution fault tolerant automatically resolves global deadlock s due to lock computer science locking scalability scales up effectively with computer network size and number of databases almost without any negative impact on performance since each global transaction is typically confined to certain relatively small numbers of databases and network nodes requires no additional artificial transaction access operations e g take timestamp based concurrency control timestamp or take ticket which typically result in additional artificial conflicts that reduce concurrency requires low overhead the only overhead incurred by the co solution is locally detecting conflicts which is already done by any known serializability mechanism both pessimistic and optimistic and locally ordering in each database system both the local commits of local transactions and the voting for atomic commitment of global transactions such overhead is low the net effect of co may be some delays of commit events but never more delay than ss2pl and on the average less this makes co instrumental for global concurrency control of multidatabase systems e g federated database system s the underlying theory of commitment ordering ref name raz2009 yoav raz 2009 http sites google com site yoavraz2 home theory of commitment ordering theory of commitment ordering summary googlesites site of yoav raz retrieved 1 feb 2011 ref part of serializability theory is both sound and scientific method hypothesis development elegant and even mathematical beauty mathematically beautiful referring to structure and dynamics of conflicts graph cycles and deadlocks with interesting implications for transactional distributed computing distributed applications all the qualities of co in the list above except the first three are also possessed by ss2pl which is a special case of co but blocking and constraining this partially explains the popularity of ss2pl as a solution practically the only solution for many years for achieving global serializability however property 9 above automatic resolution of global deadlocks has not been noticed for ss2pl in the database research literature until today 2009 except in the co publications this since the phenomenon of voting deadlocks in such environments and their automatic resolution by the atomic commitment protocol has been overlooked most existing database systems including all major commercial database systems are serializability common mechanism ss2pl strong strict two phase locking ss2pl based and already co compliant thus they can participate in a commitment ordering summary co based solution for global serializability in multidatabase environments without any modification except for the popular multiversion concurrency control multiversioning where additional co aspects should be considered achieving global serializability across ss2pl based databases using atomic commitment primarily using two phase commit 2pc has been employed for many years i e using the same co solution for a specific special case however no reference is known prior to co that notices this special case s automatic global deadlock resolution by the atomic commitment protocol s commitment ordering exact characterization of voting deadlocks by global cycles augmented conflict graph global cycle elimination process virtually all existing distributed transaction processing environments and supporting products rely on ss2pl and provide 2pc as a matter of fact ss2pl together with 2pc have become a de facto standard this solution is a homogeneous concurrency control one suboptimal when both serializability and schedule computer science strict strictness are needed see commitment ordering strict co sco strict commitment ordering sco but still quite effective in most cases sometimes at the cost of increased computing power needed relatively to the optimum however for better performance serializability relaxing serializability relaxed serializability is used whenever applications allow it allows inter operation among ss2pl compliant different database system types i e allows heterogeneity in aspects other than concurrency control ss2pl is a very constraining schedule property and takes over when combined with any other property for example when combined with any concurrency control concurrency control mechanisms optimistic property the result is not optimistic anymore but rather characteristically ss2pl on the other hand co does not change data access scheduling patterns at all and any combined property s characteristics remain unchanged since also co uses atomic commitment e g 2pc for achieving global serializability as ss2pl does any co compliant database system or transactional object can transparently join existing ss2pl based environments use 2pc and maintain global serializability without any environment change this makes co a straightforward natural generalization of ss2pl for any conflict serializability based database system for all practical purposes commitment ordering has been quite widely known inside the transaction processing and database s communities at digital equipment corporation dec since 1990 it has been under company confidentiality due to patent ing ref name raz1990 yoav raz 1990 http yoavraz googlepages com dec co memo 90 11 16 pdf on the significance of commitment ordering call for patenting memorandum digital equipment corporation november 1990 ref ref name raz1991 yoav raz us patents http patft1 uspto gov netacgi nph parser sect1 pto2 sect2 hitoff p 1 u 2fnetahtml 2fpto 2fsearch bool html r 3 f g l 50 co1 and d ptxt s1 22commitment ordering 22 ti os ttl 5 504 899 http patft1 uspto gov netacgi nph parser sect1 pto2 sect2 hitoff p 1 u 2fnetahtml 2fpto 2fsearch bool html r 2 f g l 50 co1 and d ptxt s1 22commitment ordering 22 ti os ttl 5 504 900 http patft1 uspto gov netacgi nph parser sect1 pto2 sect2 hitoff p 1 u 2fnetahtml 2fpto 2fsearch bool html r 1 f g l 50 co1 and d ptxt s1 22commitment ordering 22 ti os ttl 5 701 480 ref processes co was disclosed outside of dec by lectures and technical reports distribution to database researches in may 1991 immediately after its first patent filing it has been misunderstood by many database researchers years after its introduction which is evident by the quotes above from articles in 1997 1998 referencing commitment ordering articles on the other hand co has been utilized extensively as a solution for global serializability in works on transactional processes ref heiko schuldt hans j\xc3\xb6rg schek and gustavo alonso 1999 http portal acm org citation cfm id 853907 transactional coordination agents for composite systems in proceedings of the 3rd international database engineering and applications symposium ideas 99 ieee computer society press montrteal canada pp 321 331 ref ref klaus haller heiko schuldt can t\xc3\xbcrker 2005 http portal acm org citation cfm doid 1099554 1099563 decentralized coordination of transactional processes in peer to peer environments proceedings of the 2005 acm cikm international conference on information and knowledge management pp 28 35 bremen germany october 31 november 5 2005 isbn 1 59593 140 6 ref and more recently in the related re gridit ref laura cristiana voicu heiko schuldt fuat akal yuri breitbart hans j\xc3\xb6rg schek 2009 http dbis cs unibas ch publications 2009 grid2009 dbis publication view re gridit coordinating distributed update transactions on replicated data in the grid 10th ieee acm international conference on grid computing grid 2009 banff canada 2009 10 ref ref laura cristiana voicu and heiko schuldt 2009 http dbis cs unibas ch publications 2009 clouddb09 dbis publication view how replicated data management in the cloud can benefit from a data grid protocol the re gridit approach proceedings of the 1st international workshop on cloud data management clouddb 2009 hong kong china 2009 11 ref which is an approach for transaction management in the converging grid computing and cloud computing see more in the history of commitment ordering relaxing global serializability some techniques have been developed for relaxed global serializability i e they do not guarantee global serializability see also serializability relaxing serializability relaxing serializability among them with several publications each quasi serializability ref name du1989 weimin du and ahmed k elmagarmid 1989 http www informatik uni trier de ley db conf vldb due89 html quasi serializability a correctness criterion for global concurrency control in interbase proceedings of the fifteenth international conference on very large data bases vldb august 22 25 1989 amsterdam the netherlands pp 347 355 morgan kaufmann isbn 1 55860 101 5 ref two level serializability ref name shar1998 while local to a database system relaxed serializability methods compromise serializability for performance gain and are utilized only when the application can tolerate possible resulting inaccuracies or its integrity is unharmed it is unclear that various proposed relaxed global serializability methods which compromise global serializability provide any performance gain over commitment ordering which guarantees global serializability typically the declared intention of such methods has not been performance gain over effective global serializability methods which apparently have been unknown to the inventors but rather correctness criteria alternatives due to lack of a known effective global serializability method oddly some of them were introduced years after co had been introduced and some even quote co without realizing that it provides an effective global serializability solution and thus without providing any performance comparison with co to justify them as alternatives to global serializability for some applications e g two level serializability ref name shar1998 two level serializability is even presented as a major global concurrency control method in a 2010 edition of a text book on databases ref name silber2010 authored by two of the original authors of two level serializability where one of them abraham silberschatz avi silberschatz is also an author of the original the history of commitment ordering aeso is modified to strong recoverability co strong recoverability articles this book neither mentions co nor references it and strangely apparently does not consider co a valid global serializability solution another common reason nowadays for global serializability relaxation is the requirement of availability of internet products and internet service provider services this requirement is typically answered by large scale data replication computer science replication the straightforward solution for synchronizing replicas updates of a same database object is including all these updates in a single atomic distributed transaction however with many replicas such a transaction is very large and may span several computer s and computer network networks that some of them are likely to be unavailable thus such a transaction is likely to end with abort and miss its purpose ref name gray1996 cite conference author jim gray computer scientist gray j author2 helland p author3 patrick o neil o neil p author4 dennis shasha shasha d year 1996 title the dangers of replication and a solution conference proceedings of the 1996 acm sigmod international conference on management of data pages 173 182 url ftp ftp research microsoft com pub tr tr 96 17 pdf doi 10 1145 233269 233330 ref consequently optimistic replication lazy replication is often utilized e g in many products and services by google amazon com amazon yahoo and alike while global serializability is relaxed and compromised for eventual consistency in this case relaxation is done only for applications that are not expected to be harmed by it classes of schedules defined by relaxed global serializability properties either contain the global serializability class or are incomparable with it what differentiates techniques for relaxed global conflict serializability rgcsr properties from those of relaxed conflict serializability rcsr properties that are not rgcsr is typically the different way global cycles span two or more databases in the global conflict graph are handled no distinction between global and local cycles exists for rcsr properties that are not rgcsr rcsr contains rgcsr typically rgcsr techniques eliminate local cycles i e provide local serializability which can be achieved effectively by regular known concurrency control methods however obviously they do not eliminate all global cycles which would achieve global serializability references reflist 33em defaultsort global serializability category data management category databases category transaction processing category concurrency control'
b'record linkage rl refers to the task of finding record database records in a data set that refer to the same entity across different data sources e g data files books websites databases record linkage is necessary when join sql joining data sets based on entities that may or may not share a common identifier e g relational model database key uniform resource identifier uri national identification number as may be the case due to differences in record shape storage location and or curator style or preference a data set that has undergone rl oriented reconciliation may be referred to as being cross linked record linkage is called data linkage in many jurisdictions but is the same process history the initial idea of record linkage goes back to halbert l dunn in his 1946 article titled record linkage published in the american journal of public health ref cite journal first halbert l last dunn authorlink halbert l dunn title record linkage journal american journal of public health date december 1946 volume 36 issue 12 pages pp 1412 ndash 1416 url http www ajph org cgi reprint 36 12 1412 format pdf accessdate 2008 05 31 doi 10 2105 ajph 36 12 1412 ref howard borden newcombe laid the probabilistic foundations of modern record linkage theory in a 1959 article in science journal science ref cite journal last newcombe first h b author2 j m kennedy author3 s j axford author4 a p james title automatic linkage of vital records journal science date october 1959 volume 130 issue 3381 pages 954 959 doi 10 1126 science 130 3381 954 pmid 14426783 ref which were then formalized in 1969 by ivan fellegi and alan sunter who proved that the probabilistic decision rule they described was optimal when the comparison attributes were conditionally independent their pioneering work a theory for record linkage ref name fellegisunter cite journal first ivan last fellegi authorlink ivan fellegi author2 sunter alan title a theory for record linkage journal journal of the american statistical association date december 1969 volume 64 issue 328 pages pp 1183 ndash 1210 jstor 2286061 doi 10 2307 2286061 url http courses cs washington edu courses cse590q 04au papers felligi69 pdf format pdf ref remains the mathematical foundation for many record linkage applications even today since the late 1990s various machine learning techniques have been developed that can under favorable conditions be used to estimate the conditional probabilities required by the fellegi sunter fs theory several researchers have reported that the conditional independence assumption of the fs algorithm is often violated in practice however published efforts to explicitly model the conditional dependencies among the comparison attributes have not resulted in an improvement in record linkage quality citation needed date may 2007 on the other hand machine learning or neural network algorithms that do not rely on these assumptions often provide far higher accuracy when sufficient labeled training data is available ref name referencea cite conference first d randall last wilson d randall title beyond probabilistic record linkage using neural networks and complex features to improve genealogical record linkage conference proceedings of international joint conference on neural networks location san jose california usa date july 31 august 5 2011 url http axon cs byu edu randy pubs wilson ijcnn2011 beyondprl pdf ref record linkage can be done entirely without the aid of a computer but the primary reasons computers are often used for record linkage are to reduce or eliminate manual review and to make results more easily reproducible computer matching has the advantages of allowing central supervision of processing better quality control speed consistency and better reproducibility of results ref cite web last winkler first william e title matching and record linkage url http www census gov srd papers pdf rr93 8 pdf publisher u s bureau of the census accessdate 12 november 2011 ref naming conventions record linkage is the term used by statisticians epidemiologists and historians among others to describe the process of joining records from one data source with another that describe the same entity commercial mail and database applications refer to it as merge purge processing or list washing computer science computer scientists often refer to it as data matching or as the object identity problem other names used to describe the same concept include coreference entity identity name record resolution entity disambiguation linking duplicate detection deduplication record matching reference reconciliation object identification data information integration and conflation ref http homes cs washington edu pedrod papers icdm06 pdf ref this profusion of terminology has led to few cross references between these research communities ref http datamining anu edu au linkage html cristen p t febrl freely extensible biomedical record linkage manual release 0 3 p 9 ref ref cite journal first ahmed last elmagarmid author2 panagiotis g ipeirotis author3 vassilios verykios title duplicate record detection a survey journal ieee transactions on knowledge and data engineering date january 2007 volume 19 issue 1 pages pp 1 ndash 16 url http www cs purdue edu homes ake pub tkde 0240 0605 1 pdf format pdf accessdate 2009 03 30 doi 10 1109 tkde 2007 9 ref while they share similar names record linkage and linked data are two separate concepts whereas record linkage focuses on the more narrow task of identifying matching entities across different data sets linked data focuses on the broader methods of structuring and publishing data to facilitate the discovery of related information methods data preprocessing record linkage is highly sensitive to the quality of the data being linked so all data sets under consideration particularly their key identifier fields should ideally undergo a data quality assessment prior to record linkage many key identifiers for the same entity can be presented quite differently between and even within data sets which can greatly complicate record linkage unless understood ahead of time for example key identifiers for a man named william j smith might appear in three different data sets as so class wikitable data set name date of birth city of residence data set 1 william j smith 1 2 73 berkeley california data set 2 smith w j 1973 1 2 berkeley ca data set 3 bill smith jan 2 1973 berkeley calif in this example the different formatting styles lead to records that look different but in fact all refer to the same entity with the same logical identifier values most if not all record linkage strategies would result in more accurate linkage if these values were first normalized or standardized into a consistent format e g all names are surname given name and all dates are yyyy mm dd standardization can be accomplished through simple rule based data transformation s or more complex procedures such as lexicon based tokenization lexical analysis tokenization and probabilistic hidden markov models ref cite journal last churches first tim author2 peter christen author3 kim lim author4 justin xi zhu title preparation of name and address data for record linkage using hidden markov models journal bmc medical informatics and decision making date 13 december 2002 volume 2 doi 10 1186 1472 6947 2 9 url http www biomedcentral com 1472 6947 2 9 pages 9 ref several of the packages listed in the software implementations section provide some of these features to simplify the process of data standardization entity resolution entity resolution is an operational intelligence process typically powered by an entity resolution engine or middleware whereby organizations can connect disparate data sources with a opinion view to understanding possible entity matches and non obvious relationships across multiple data silos it analyzes all of the information relating to individuals and or entities from multiple sources of data and then applies likelihood and probability scoring to determine which identities are a match and what if any non obvious relationships exist between those identities entity resolution engines are typically used to uncover risk fraud and conflicts of interest but are also useful tools for use within customer data integration cdi and master data management mdm requirements typical uses for entity resolution engines include terrorist screening insurance fraud detection usa patriot act compliance organized retail crime ring detection and applicant screening for example across different data silos employee records vendor data watch lists etc an organization may have several variations of an entity named abc which may or may not be the same individual these entries may in fact appear as abc1 abc2 or abc3 within those data sources by comparing similarities between underlying attributes such as address geography address date of birth or social security number the user can eliminate some possible matches and confirm others as very likely matches entity resolution engines then apply rules based on common sense logic to identify hidden relationships across the data in the example above perhaps abc1 and abc2 are not the same individual but rather two distinct people who share common attributes such as address or phone number data matching while entity resolution solutions include data matching technology many data matching offerings do not fit the definition of entity resolution here are four factors that distinguish entity resolution from data matching according to john talburt director of the ualr ualr center for advanced research in entity resolution and information quality works with both structured and unstructured records and it entails the process of extracting references when the sources are unstructured or semi structured uses elaborate business rules and concept models to deal with missing conflicting and corrupted information utilizes non matching asserted linking associate information in addition to direct matching uncovers non obvious relationships and association networks i e who s associated with whom in contrast to data quality products more powerful identity resolution engines also include a rules engine and workflow process which apply business intelligence to the resolved identities and their relationships these advanced technologies make automated decisions and impact business processes in real time limiting the need for human intervention deterministic record linkage the simplest kind of record linkage called deterministic or rules based record linkage generates links based on the number of individual identifiers that match among the available data sets ref cite journal last roos first ll author2 wajda a title record linkage strategies part i estimating information and evaluating approaches journal methods of information in medicine date april 1991 volume 30 issue 2 pages 117 123 pmid 1857246 ref two records are said to match via a deterministic record linkage procedure if all or some identifiers above a certain threshold are identical deterministic record linkage is a good option when the entities in the data sets are identified by a common identifier or when there are several representative identifiers e g name date of birth and sex when identifying a person whose quality of data is relatively high as an example consider two standardized data sets set a and set b that contain different bits of information about patients in a hospital system the two data sets identify patients using a variety of identifiers social security number ssn name date of birth dob sex and zip code zip the records in two data sets identified by the column are shown below class wikitable data set ssn name dob sex zip rowspan 4 set a 1 000956723 smith william 1973 01 02 male 94701 style background f0f0f0 2 000956723 smith william 1973 01 02 male 94703 3 000005555 jones robert 1942 08 14 male 94701 style background f0f0f0 4 123001234 sue mary 1972 11 19 female 94109 rowspan 2 set b 1 000005555 jones bob 1942 08 14 style background f0f0f0 2 smith bill 1973 01 02 male 94701 the most simple deterministic record linkage strategy would be to pick a single identifier that is assumed to be uniquely identifying say ssn and declare that records sharing the same value identify the same person while records not sharing the same value identify different people in this example deterministic linkage based on ssn would create entities based on a1 and a2 a3 and b1 and a4 while a1 a2 and b2 appear to represent the same entity b2 would not be included into the match because it is missing a value for ssn handling exceptions such as missing identifiers involves the creation of additional record linkage rules one such rule in the case of missing ssn might be to compare name date of birth sex and zip code with other records in hopes of finding a match in the above example this rule would still not match a1 a2 with b2 because the names are still slightly different standardization put the names into the proper surname given name format but could not discern bill as a nickname for william running names through a phonetic algorithm such as soundex nysiis or metaphone can help to resolve these types of problems though it may still stumble over surname changes as the result of marriage or divorce but then b2 would be matched only with a1 since the zip code in a2 is different thus another rule would need to be created to determine whether differences in particular identifiers are acceptable such as zip code and which are not such as date of birth as this example demonstrates even a small decrease in data quality or small increase in the complexity of the data can result in a very large increase in the number of rules necessary to link records properly eventually these linkage rules will become too numerous and interrelated to build without the aid of specialized software tools in addition linkage rules are often specific to the nature of the data sets they are designed to link together one study was able to link the social security death master file with two hospital registries from the midwestern united states using ssn nysiis encoded first name birth month and sex but these rules may not work as well with data sets from other geographic regions or with data collected on younger populations ref cite journal last grannis first sj author2 overhage jm author3 mcdonald cj title analysis of identifier performance using a deterministic linkage algorithm journal proc amia symp year 2002 pages 305 9 pmid 12463836 pmc 2244404 ref thus continuous maintenance testing of these rules is necessary to ensure they continue to function as expected as new data enter the system and need to be linked new data that exhibit different characteristics than was initially expected could require a complete rebuilding of the record linkage rule set which could be a very time consuming and expensive endeavor probabilistic record linkage probabilistic record linkage sometimes called fuzzy matching also probabilistic merging or fuzzy merging in the context of merging of databases takes a different approach to the record linkage problem by taking into account a wider range of potential identifiers computing weights for each identifier based on its estimated ability to correctly identify a match or a non match and using these weights to calculate the probability that two given records refer to the same entity record pairs with probabilities above a certain threshold are considered to be matches while pairs with probabilities below another threshold are considered to be non matches pairs that fall between these two thresholds are considered to be possible matches and can be dealt with accordingly e g human reviewed linked or not linked depending on the requirements whereas deterministic record linkage requires a series of potentially complex rules to be programmed ahead of time probabilistic record linkage methods can be trained to perform well with much less human intervention many probabilistic record linkage algorithms assign match non match weights to identifiers by means of two probabilities called u and m the u probability is the probability that an identifier in two non matching records will agree purely by chance for example the u probability for birth month where there are twelve values that are approximately uniformly distributed is 1 12 \xe2\x89\x88 0 083 identifiers with values that are not uniformly distributed will have different u probabilities for different values possibly including missing values the m probability is the probability that an identifier in matching pairs will agree or be sufficiently similar such as strings with high jaro winkler distance or low levenshtein distance this value would be 1 0 in the case of perfect data but given that this is rarely if ever true it can instead be estimated this estimation may be done based on prior knowledge of the data sets by manually identifying a large number of matching and non matching pairs to train the probabilistic record linkage algorithm or by iteratively running the algorithm to obtain closer estimations of the m probability if a value of 0 95 were to be estimated for the m probability then the match non match weights for the birth month identifier would be class wikitable outcome proportion of links proportion of non links frequency ratio weight match m 0 95 u \xe2\x89\x88 0 083 m u \xe2\x89\x88 11 4 ln m u ln 2 \xe2\x89\x88 3 51 non match 1\xe2\x88\x92 m 0 05 1 u \xe2\x89\x88 0 917 1 m 1 u \xe2\x89\x88 0 0545 ln 1 m 1 u ln 2 \xe2\x89\x88 4 20 the same calculations would be done for all other identifiers under consideration to find their match non match weights then every identifier of one record would be compared with the corresponding identifier of another record to compute the total weight of the pair the match weight is added to the running total whenever a pair of identifiers agree while the non match weight is added i e the running total decreases whenever the pair of identifiers disagrees the resulting total weight is then compared to the aforementioned thresholds to determine whether the pair should be linked non linked or set aside for special consideration e g manual validation ref name prl cite journal last blakely first tony author2 salmond clare title probabilistic record linkage and a method to calculate the positive predictive value journal international journal of epidemiology date december 2002 volume 31 issue 6 pages 1246 1252 doi 10 1093 ije 31 6 1246 pmid 12540730 url http ije oxfordjournals org content 31 6 1246 full ref determining where to set the match non match thresholds is a balancing act between obtaining an acceptable sensitivity and specificity sensitivity sensitivity or recall the proportion of truly matching records that are linked by the algorithm and positive predictive value or precision the proportion of records linked by the algorithm that truly do match various manual and automated methods are available to predict the best thresholds and some record linkage software packages have built in tools to help the user find the most acceptable values because this can be a very computationally demanding task particularly for large data sets a technique known as blocking is often used to improve efficiency blocking attempts to restrict comparisons to just those records for which one or more particularly discriminating identifiers agree which has the effect of increasing the positive predictive value precision at the expense of sensitivity recall ref name prl for example blocking based on a phonetically coded surname and zip code would reduce the total number of comparisons required and would improve the chances that linked records would be correct since two identifiers already agree but would potentially miss records referring to the same person whose surname or zip code was different due to marriage or relocation for instance blocking based on birth month a more stable identifier that would be expected to change only in the case of data error would provide a more modest gain in positive predictive value and loss in sensitivity but would create only twelve distinct groups which for extremely large data sets may not provide much net improvement in computation speed thus robust record linkage systems often use multiple blocking passes to group data in various ways in order to come up with groups of records that should be compared to each other machine learning in recent years a variety of machine learning techniques have been used in record linkage it has been recognized ref name referencea that a classic algorithm for probabilistic record linkage is equivalent to the naive bayes algorithm in the field of machine learning ref quass dallan and starkey paul record linkage for genealogical databases acm sigkdd 03 workshop on data cleaning record linkage and object consolidation august 24 27 2003 washington d c ref and suffers from the same assumption of the independence of its features an assumption that is typically not true ref langley pat wayne iba and kevin thompson an analysis of bayesian classifiers in proceedings of the 10th national conference on artificial intelligence aaai 92 aaai press mit press cambridge ma pp 223 228 1992 ref ref michie d d spiegelhalter and c taylor machine learning neural and statistical classification ellis horwood hertfordshire england book 19 1994 ref higher accuracy can often be achieved by using various other machine learning techniques including a single layer perceptron ref name referencea mathematical model in an application with two files a and b denote the rows records by math alpha a math in file a and math beta b math in file b assign math k math characteristics to each record the set of records that represent identical entities is defined by math m left a b a b a in a b in b right math and the complement of set math m math namely set math u math representing different entities is defined as math u a b a neq b a in a b in b math a vector math gamma math is defined that contains the coded agreements and disagreements on each characteristic math gamma left alpha a beta b right gamma 1 left alpha a beta b right gamma k left alpha a beta b right math where math k math is a subscript for the characteristics sex age marital status etc in the files the conditional probabilities of observing a specific vector math gamma math given math a b in m math math a b in u math are defined as math m gamma p left gamma left alpha a beta b right a b in m right sum a b in m p left gamma left alpha a beta b right right cdot p left a b m right math and math u gamma p left gamma left alpha a beta b right a b in u right sum a b in u p left gamma left alpha a beta b right right cdot p left a b u right math respectively ref name fellegisunter applications master data management most master data management mdm products use a record linkage process to identify records from different sources representing the same real world entity this linkage is used to create a golden master record containing the cleaned reconciled data about the entity the techniques used in mdm are the same as for record linkage generally mdm expands this matching not only to create a golden master record but to infer relationships also i e a person has a same similar surname and same similar address this might imply they share a household relationship data warehousing and business intelligence record linkage plays a key role in data warehousing and business intelligence data warehouses serve to combine data from many different operational source systems into one logical data model which can then be subsequently fed into a business intelligence system for reporting and analytics each operational source system may have its own method of identifying the same entities used in the logical data model so record linkage between the different sources becomes necessary to ensure that the information about a particular entity in one source system can be seamlessly compared with information about the same entity from another source system data standardization and subsequent record linkage often occur in the transform portion of the extract transform load etl process historical research record linkage is important to social history research since most data sets such as census census records and parish registers were recorded long before the invention of national identification number s when old sources are digitized linking of data sets is a prerequisite for longitudinal study this process is often further complicated by lack of standard spelling of names family names that change according to place of dwelling changing of administrative boundaries and problems of checking the data against other sources record linkage was among the most prominent themes in the history and computing field in the 1980s but has since been subject to less attention in research citation needed date november 2011 medical practice and research any experts out there record linkage is an important tool in creating data required for examining the health of the public and of the health care system itself it can be used to improve data holdings data collection quality assessment and the dissemination of information data sources can be examined to eliminate duplicate records to identify under reporting and missing cases e g census population counts to create person oriented health statistics and to generate disease registries and health surveillance systems some cancer registries link various data sources e g hospital admissions pathology and clinical reports and death registrations to generate their registries record linkage is also used to create health indicators for example fetal and infant mortality is a general indicator of a country s socioeconomic development public health and maternal and child services if infant death records are matched to birth records it is possible to use birth variables such as birth weight and gestational age along with mortality data such as cause of death in analyzing the data linkages can help in follow up studies of cohorts or other groups to determine factors such as vital status residential status or health outcomes tracing is often needed for follow up of industrial cohorts clinical trials and longitudinal surveys to obtain the cause of death and or cancer an example of a successful and long standing record linkage system allowing for population based medical research is the rochester epidemiology project based in rochester minnesota ref name data resource profile cite journal author1 st sauver jl author2 grossardt br author3 yawn bp author4 melton lj 3rd author5 pankratz jj author6 brue sm author7 rocca wa title data resource profile the rochester epidemiology project rep medical records linkage system journal int j epidemiol volume 41 issue 6 pages 1614 24 year 2012 pmid 23159830 doi 10 1093 ije dys195 url http ije oxfordjournals org content 41 6 1614 long ref criticism of existing software implementations the main reasons cited are project costs costs typically in the hundreds of thousands of dollars time lack of enough time to deal with large scale data cleansing software security concerns over sharing information giving an application access across systems and effects on legacy systems see also capacity optimization content addressable storage data deduplication delta encoding entity linking entity attribute value model identity resolution linked data named entity recognition open data schema matching single instance storage notes and references reflist 2 external links http pike psu edu linkage data linkage project at penn state usa http www datadecision com datadecision data matching online tool http www nameapi org en demos name matcher nameapi name matcher http sourceforge net projects oysterer oyster entity resolution http datamining anu edu au febrl freely extensible biomedical record linkage http infolab stanford edu serf stanford entity resolution framework http dbs uni leipzig de de research projects large scale object matching dedoop deduplication with hadoop https sourceforge net projects erframework blockingframework a framework for blocking based entity resolution http www ipdln org international population data linkage network https github com yahoo fel yahoo fast entity linker core defaultsort record linkage category data management'
b'refimprove date april 2012 in business master data management mdm comprises the processes governance policies standards and tools that consistently define and manage the critical data of an organization to provide a single point of reference ref what is master data searchdatamanagement techtarget 22 november 2010 http searchdatamanagement techtarget com definition master data management ref the data that is mastered may include reference data ndash the business objects for transactions and the dimensions for analysis analytical data ndash supports decision making ref introduction to master data management mark rittman director rittman mead consulting 9 may 2008 https s3 amazonaws com rmc docs introduction 20to 20oracle 20master 20data 20management pdf ref ref http www b eye network com view 2918 defining master data david loshin beyenetwork may 2006 ref in computing a master data management tool can be used to support master data management by removing duplicates standardizing data mass maintaining and incorporating rules to eliminate incorrect data from entering the system in order to create an authoritative source of master data master data are the products accounts and parties for which the business transactions are completed the root cause problem stems from business unit and product line segmentation in which the same customer will be serviced by different product lines with redundant data being entered about the customer a k a party in the role of customer and account in order to process the transaction the redundancy of party and account data is compounded in the front to back office life cycle where the authoritative single source for the party account and product data is needed but is often once again redundantly entered or augmented master data management has the objective of providing processes for collecting aggregating matching consolidating quality assuring persisting and distributing such data throughout an organization to ensure consistency and control in the ongoing maintenance and application use of this information the term recalls the concept of a master file from an earlier computing era definition master data management mdm is a comprehensive method of enabling an enterprise to link all of its critical data to one file called a master file that provides a common point of reference when properly done master data management streamlines data sharing among personnel and departments in addition master data management can facilitate computing in multiple system architectures platforms and applications ref cite web title master data management url http www ibm com software data master data management overview html publisher ibm ref at its core master data management mdm can be viewed as a discipline for specialized quality improvement ref dama dmbok guide 2010 dama international ref defined by the policies and procedures put in place by a data governance organization the ultimate goal being to provide the end user community with a trusted single version of the truth from which to base decisions issues at a basic level master data management seeks to ensure that an organization does not use multiple potentially consistency database systems inconsistent versions of the same master data in different parts of its operations which can occur in large organizations a typical example of poor master data management is the scenario of a bank at which a customer has taken out a mortgage loan mortgage and the bank begins to send mortgage solicitations to that customer ignoring the fact that the person already has a mortgage account relationship with the bank this happens because the customer information used by the marketing section within the bank lacks integration with the customer information used by the customer services section of the bank thus the two groups remain unaware that an existing customer is also considered a sales lead the process of record linkage is used to associate different records that correspond to the same entity in this case the same person other problems include for example issues with the data quality quality of data consistent classification and identification of data and data validation and reconciliation data reconciliation issues master data management of disparate data systems requires data transformation s as the data extracted from the disparate source data system is transformed and loaded into the master data management hub to synchronize the disparate source master data the managed master data extracted from the master data management hub is again transformed and loaded into the disparate source data system as the master data is updated as with other extract transform load based data movement these processes are expensive and inefficient to develop and to maintain which greatly reduces the return on investment for the master data management product one of the most common reasons some large corporations experience massive issues with master data management is growth through merger s or takeover acquisitions any organizations which merge will typically create an entity with duplicate master data since each likely had at least one master database of its own prior to the merger ideally database administrator s resolve this problem through data deduplication deduplication of the master data as part of the merger in practice however reconciling several master data systems can present difficulties because of the dependencies that existing applications have on the master databases as a result more often than not the two systems do not fully merge but remain separate with a special reconciliation process defined that ensures consistency between the data stored in the two systems over time however as further mergers and acquisitions occur the problem multiplies more and more master databases appear and data reconciliation processes become extremely complex and consequently unmanageable and unreliable because of this trend one can find organizations with 10 15 or even as many as 100 separate poorly integrated master databases which can cause serious operational problems in the areas of customer satisfaction operational efficiency decision support and regulatory compliance another problem concerns determining the proper degree of detail and normalization to include in the master data schema for example in a federated hr environment the enterprise may focus on storing people data as a current status adding a few fields to identify date of hire date of last promotion etc however this simplification can introduce business impacting errors into dependent systems for planning and forecasting the stakeholders of such systems may be forced to build a parallel network of new interfaces to track onboarding of new hires planned retirements and divestment which works against one of the aims of master data management solutions processes commonly seen in master data management include source identification data collection data transformation database normalization normalization rule administration error detection and correction data consolidation data storage device data storage data distribution data classification taxonomy services item master creation schema mapping product codification data enrichment and data governance the selection of entities considered for master data management depends somewhat on the nature of an organization in the common case of commercial enterprises master data management may apply to such entities as customer customer data integration product product information management employee and vendor master data management processes identify the sources from which to collect descriptions of these entities in the course of transformation and normalization administrators adapt descriptions to conform to standard formats and data domains making it possible to remove duplicate instances of any entity such processes generally result in an organizational master data management repository from which all requests for a certain entity instance produce the same description irrespective of the originating sources and the requesting destination the tools include data networks file systems a data warehouse data mart s an operational data store data mining data analysis data visualization federated database system data federation and data virtualization one of the newest tools virtual master data management utilizes data virtualization and a persistent metadata server to implement a multi level automated master data management hierarchy transmission of master data there are several ways in which master data may be collated and distributed to other systems ref http dama ny com images meeting 101509 damanyc mdmprint pdf creating the golden record better data through chemistry dama slide 26 donald j soulsby 22 october 2009 ref this includes data consolidation the process of capturing master data from multiple sources and integrating into a single hub operational data store for replication to other destination systems federated database system data federation the process of providing a single virtual view of master data from one or more sources to one or more destination systems data propagation the process of copying master data from one system to another typically through point to point interfaces in legacy systems see also reference data master data record linkage data steward data visualization customer data integration data integration product information management identity resolution enterprise information integration linked data semantic web data governance operational data store single customer view references reflist external links http msdn2 microsoft com en us library bb190163 aspx mdm04 topic4 microsoft the what why and how of master data management http msdn microsoft com en us library bb410798 aspx microsoft master data management mdm hub architecture http mike2 openmethodology org wiki master data management solution offering open methodology for master data management http www semarchy com overview why do i need mdm semarchy why do i need mdm video http www mdmalliancegroup com mdm community http www stibosystems com global explore stibo systems master data management aspx multidomain master data management http blogs gartner com andrew white 2014 06 05 reprise when is master data and mdm not master data or mdm reprise when is master data and mdm not master data or mdm http www orchestranetworks com mdm master data management multidomain data warehouse databases defaultsort master data management category business intelligence category data management category data warehousing category information technology management'
b'orphan date april 2011 social information architecture is a sub domain of information architecture which deals with the social aspects of conceptualizing modeling and organizing information social information architecture also known as social ia ref http sweetinformationarchitecture net social information architecture 20 sweet information architecture ref has become more relevant because of the rise of social media and web 2 0 in recent times approach there are different approaches to the explanation of social ia a the architecture model internal space architects designing a physical community space have to consider how the architecture will shape social interactions a long hallway of offices creates an utterly different dynamic than desks with arranged in an open space one might foster individuality privacy propriety the other collaboration distraction communalism still physical spaces can be flexibly repurposed and worked around if the inhabitants desire a social dynamic not instantly afforded by the space office doors can be left open to invite easier interaction partitions can be raised between adjacent desks to limit distraction and increase privacy that s physical architecture the information architectures of online communities are far more deterministic and far less flexible they literally define the social architecture by pre specifying in immutable computer code what information you have access to who you can talk to where you can go in the online world information architecture social architecture ref http www steinbock org ref b the social dialogue and information model external space all major brands use information architecture to market their products online it is then commonly wrapped under the umbrella phrase digital strategy information architecture used for strategic purposes encompasses brand seo strategic placement of virals social media presence etc charities news outlets and social dialogue forums can make a much more specific use of the same tools for positive and important social purposes social information architecture is perceived as the socially conscious wing of commercial information architecture ref http www sweetinformationarchitecture net ref and function to exchange information and ideas between people and groups social ia can pick up on conflicting issues that are treated with misunderstanding between cultures and leaves individuals and societies vulnerable to exploitation and manipulation since the net has such a far reach it is obvious to use it for meaningful and coordinated social dialogue example of such issues are faith environment politics climate change war injustice and other social challenges information architecture can help create frameworks in which sharing information brings people together inspires and encourages them to participate in a forward thinking and unfragmented way one of its core activities is to spread messages that bring people from opposite sites of social and cultural spectrums together and to confront uncomfortable subject head on how does social information architecture work social ia utilizes a variety of web2 0 applications to filter relevant or valuable information and weave them in appropriate information repository or provide feedback to interesting channels social ia makes strategic use of search engines social media google algorithms as well as websites video news channels it reads or listens to social conversations and search engine queries and engages with the net actively to gather clues about the world s pulse on the internet it assesses data social political trends and respond with targeted campaigns to give people ideas as well as help people with making sense of information principals dan brown in his paper 8 principals of social information architecture ref http socialinformationarchitecture org uk paper 8principal infoarchi pdf eight principles of information architecture dan brown published in the bulletin of the american society for information science and technology august september 2010 volume 36 number 6 ref enlists the following principals br 1 the principle of objects treat content as a living breathing thing with a lifecycle behaviors and attributes br 2 the principle of choices create pages that offer meaningful choices to users keeping the range of choices available focused on a particular task br 3 the principle of disclosure show only enough information to help people understand what kinds of information they ll find as they dig deeper br 4 the principle of exemplars describe the contents of categories by showing examples of the contents br 5 the principle of front doors assume at least half of the website s visitors will come through some page other than the home page br 6 the principle of multiple classification offer users several different classification schemes to browse the site s content br 7 the principle of focused navigation don t mix apples and oranges in your navigation scheme br 8 the principle of growth assume the content you have today is a small fraction of the content you will have tomorrow what can social information architecture achieve social information architecture has many potentials in terms of fostering social connections and how information is shared in social spaces on the web references reflist see also wodtke christina and govella austin information architecture blueprints for the web 2009 second edition published by new riders semantic web category information architects information architecture category world wide web category data management category information science category information technology category digital technology category new media'
b'unreferenced date march 2011 a state transition network is a diagram that is developed from a set of data and charts the data flow flow of data from particular data points called states or nodes to the next in a probabilistic manner use state transition networks are used in both academic and industry industrial fields examples state transition networks are a general construct with more specific examples being augmented transition networks recursive transition networks and augmented recursive networks among others see also state transition system markov network history monoid references reflist category data management'
b'in relational database s the log trigger or history trigger is a mechanism for automatic recording of information about changes inserting or and updating or and deleting row database rows in a table database database table it is a particular technique for change data capture change data capturing and in data warehousing for dealing with slowly changing dimension s definition suppose there is a table database table which we want to audit this table database table contains the following column database columns code column1 column2 columnn code the column database column code column1 code is assumed to be the primary key these column database columns are defined to have the following types code type1 type2 typen code the log trigger works writing the changes insert sql insert update sql update and delete sql delete operations on the table database table in another history table defined as following syntaxhighlight lang sql create table historytable column1 type1 column2 type2 columnn typen startdate datetime enddate datetime syntaxhighlight as shown above this new table database table contains the same column database columns as the original table database table and additionally two new column database columns of type code datetime code code startdate code and code enddate code this is known as tuple versioning tuple versioning these two additional column database columns define a period of time of validity of the data associated with a specified entity the entity of the primary key or in other words it stores how the data were in the period of time between the code startdate code included and code enddate code not included for each entity distinct primary key on the original table database table the following structure is created in the history table database table data is shown as example file example log trigger png center example notice that if they are shown chronologically the code enddate code column database column of any row database row is exactly the code startdate code of its successor if any it does not mean that both row database rows are common to that point in time since by definition the value of code enddate code is not included there are two variants of the log trigger depending how the old values delete update and new values insert update are exposed to the trigger it is rdbms dependent old and new values as fields of a record data structure syntaxhighlight lang sql create trigger historytable on originaltable for insert delete update as declare now datetime set now getdate deleting section update historytable set enddate now where enddate is null and column1 old column1 inserting section insert into historytable column1 column2 columnn startdate enddate values new column1 new column2 new columnn now null syntaxhighlight old and new values as rows of virtual tables syntaxhighlight lang sql create trigger historytable on originaltable for insert delete update as declare now datetime set now getdate deleting section update historytable set enddate now from historytable deleted where historytable column1 deleted column1 and historytable enddate is null inserting section insert into historytable column1 column2 columnn startdate enddate select column1 column2 columnn now null from inserted syntaxhighlight compatibility notes the function code getdate code is used to get the system date and time a specific relational database management system rdbms could either use another function name or get this information by another way several relational database management system rdbms db2 mysql do not support that the same trigger can be attached to more than one operation insert sql insert delete sql delete update sql update in such a case a trigger must be created for each operation for an insert sql insert operation only the inserting section must be specified for a delete sql delete operation only the deleting section must be specified and for an update sql update operation both sections must be present just as it is shown above the deleting section first then the inserting section because an update sql update operation is logically represented as a delete sql delete operation followed by an insert sql insert operation in the code shown the record data structure containing the old and new values are called code old code and code new code on a specific relational database management system rdbms they could have different names in the code shown the virtual tables are called code deleted code and code inserted code on a specific relational database management system rdbms they could have different names another relational database management system rdbms db2 even let the name of these logical tables be specified in the code shown comments are in c c style they could not be supported by a specific relational database management system rdbms or a different syntax should be used several relational database management system rdbms require that the body of the trigger is enclosed between code begin code and code end code keywords data warehousing according with the slowly changing dimension management methodologies the log trigger falls into the following slowly changing dimension type 2 type 2 tuple versioning tuple versioning variant slowly changing dimension type 4 type 4 use of history tables implementation in common rdbms ibm db2 ref database fundamentals by nareej sharma et al first edition copyright ibm corp 2010 ref a trigger cannot be attached to more than one operation insert sql insert delete sql delete update sql update so a trigger must be created for each operation the old and new values are exposed as fields of a record data structures the names of these records can be defined in this example they are named as code o code for old values and code n code for new values syntaxhighlight lang sql trigger for insert create trigger database tableinsert after insert on database originaltable referencing new as n for each row mode db2sql begin declare now timestamp set now current timestamp insert into database historytable column1 column2 columnn startdate enddate values n column1 n column2 n columnn now null end trigger for delete create trigger database tabledelete after delete on database originaltable referencing old as o for each row mode db2sql begin declare now timestamp set now current timestamp update database historytable set enddate now where column1 o column1 and enddate is null end trigger for update create trigger database tableupdate after update on database originaltable referencing new as n old as o for each row mode db2sql begin declare now timestamp set now current timestamp update database historytable set enddate now where column1 o column1 and enddate is null insert into database historytable column1 column2 columnn startdate enddate values n column1 n column2 n columnn now null end syntaxhighlight microsoft sql server ref microsoft sql server 2008 database development by thobias thernstr\xc3\xb6m et al microsoft press 2009 ref the same trigger can be attached to all the insert sql insert delete sql delete and update sql update operations old and new values as rows of virtual tables named code deleted code and code inserted code syntaxhighlight lang sql create trigger tabletrigger on originaltable for delete insert update as declare now datetime set now current timestamp update historytable set enddate now from historytable deleted where historytable columnid deleted columnid and historytable enddate is null insert into historytable columnid column2 columnn startdate enddate select columnid column2 columnn now null from inserted syntaxhighlight mysql a trigger cannot be attached to more than one operation insert sql insert delete sql delete update sql update so a trigger must be created for each operation the old and new values are exposed as fields of a record data structures called code old code and code new code syntaxhighlight lang sql delimiter trigger for insert create trigger historytableinsert after insert on originaltable for each row begin declare n datetime set n now insert into historytable column1 column2 columnn startdate enddate values new column1 new column2 new columnn n null end trigger for delete create trigger historytabledelete after delete on originaltable for each row begin declare n datetime set n now update historytable set enddate n where column1 old column1 and enddate is null end trigger for update create trigger historytableupdate after update on originaltable for each row begin declare n datetime set n now update historytable set enddate n where column1 old column1 and enddate is null insert into historytable column1 column2 columnn startdate enddate values new column1 new column2 new columnn n null end syntaxhighlight oracle database oracle the same trigger can be attached to all the insert sql insert delete sql delete and update sql update operations the old and new values are exposed as fields of a record data structures called code old code and code new code it is necessary to test the nullity of the fields of the code new code record that define the primary key when a delete sql delete operation is performed in order to avoid the insertion of a new row with null values in all columns syntaxhighlight lang sql create or replace trigger tabletrigger after insert or update or delete on originaltable for each row declare now timestamp begin select current timestamp into now from dual update historytable set enddate now where enddate is null and column1 old column1 if new column1 is not null then insert into historytable column1 column2 columnn startdate enddate values new column1 new column2 new columnn now null end if end syntaxhighlight historic information typically database dump database backups are used to store and retrieve historic information a database dump database backup is a security mechanism more than an effective way to retrieve ready to use historic information a full database dump database backup is only a snapshot of the data in specific points of time so we could know the information of each snapshot but we can know nothing between them information in database dump database backups is discrete in time using the log trigger the information we can know is not discrete but continuous we can know the exact state of the information in any point of time only limited to the granularity of time provided with the code datetime code data type of the relational database management system rdbms used advantages it is simple it is not a commercial product it works with available features in common relational database management system rdbms it is automatic once it is created it works with no further human intervention it is not required to have good knowledge about the tables of the database or the data model changes in current programming are not required changes in the current table database tables are not required because log data of any table database table is stored in a different one it works for both programmed and ad hoc statements only changes insert sql insert update sql update and delete sql delete operations are registered so the growing rate of the history tables are proportional to the changes it is not necessary to apply the trigger to all the tables on database it can be applied to certain table database tables or certain column database columns of a table database table disadvantages it does not automatically store information about the user producing the changes information system user not database user this information might be provided explicitly it could be enforced in information systems but not in ad hoc queries examples of use getting the current version of a table syntaxhighlight lang sql select column1 column2 columnn from historytable where enddate is null syntaxhighlight it should return the same resultset of the whole original table database table getting the version of a table in a certain point of time suppose the code date code variable contains the point or time of interest syntaxhighlight lang sql select column1 column2 columnn from historytable where date startdate and date enddate or enddate is null syntaxhighlight getting the information of an entity in a certain point of time suppose the code date code variable contains the point or time of interest and the code key code variable contains the primary key of the entity of interest syntaxhighlight lang sql select column1 column2 columnn from historytable where column1 key and date startdate and date enddate or enddate is null syntaxhighlight getting the history of an entity suppose the code key code variable contains the primary key of the entity of interest syntaxhighlight lang sql select column1 column2 columnn startdate enddate from historytable where column1 key order by startdate syntaxhighlight getting when and how an entity was created suppose the code key code variable contains the primary key of the entity of interest syntaxhighlight lang sql select h2 column1 h2 column2 h2 columnn h2 startdate from historytable as h2 left outer join historytable as h1 on h2 column1 h1 column1 and h2 column1 key and h2 startdate h1 enddate where h2 enddate is null syntaxhighlight immutability of primary key s since the trigger requires that primary key being the same throughout time it is desirable to either ensure or maximize its immutability if a primary key changed its value the entity it represents would break its own history there are several options to achieve or maximize the primary key immutability use of a surrogate key surrogate key as a primary key since there is no reason to change a value with no meaning other than identity and uniqueness it would never change use of an immutable natural key as a primary key in a good database design a natural key which can change should not be considered as a real primary key use of a mutable natural key as a primary key it is widely discouraged where changes are propagated in every place where it is a foreign key in such a case the history table should be also affected alternatives sometimes the slowly changing dimension is used as a method this diagram is an example file scd model png frame right scd model see also rdbms relational database primary key natural key surrogate key change data capture slowly changing dimension tuple versioning tuple versioning notes the log trigger was written by laurence ruiz ugalde laurence r ugalde to automatically generate history of transactional databases references references defaultsort log trigger category computer data category data management category data modeling category data warehousing'
b'customer data management cdm is the ways in which businesses keep track of their customer information and survey their customer base in order to obtain feedback cdm embraces a range of software or cloud computing applications designed to give large organizations rapid and efficient access to customer data surveys and data can be centrally located and widely accessible within a company as opposed to being warehoused in separate departments cdm encompasses the collection analysis organizing reporting and sharing of customer information throughout an organization businesses need a thorough understanding of their customers needs if they are to retain and increase their customer base efficient cdm solutions provide companies with the ability to deal instantly with customer issues and obtain immediate feedback as a result customer retention and customer satisfaction can show dramatic improvement according to a recent study by aberdeen group inc above average and best in class companies attain greater than 20 annual improvement in retention rates revenues data accuracy and partner customer satisfaction rates ref smalltree hannah 2006 http searchcrm techtarget com news 1212337 best practices in managing customer data ref customer data management and cloud computing cloud computing offers an attractive choice for cdm in many companies due to its accessibility and cost efficiency cost effectiveness businesses can decide who within their company should have the ability to create adjust analyze or share customer information in december 2010 52 of information technology information technology it professionals worldwide were deploying or planning to deploy cloud computing ref cisco com december 2010 http newsroom cisco com dlls 2010 prod 120810 html ref this percentage is far higher in many countries uses for management customer data management should provide a cost effective user friendly solution for marketing research sales human resources and it departments enables companies to create and email online surveys reports and newsletters encompasses and simplifies customer relationship management crm and customer feedback management services customer feedback management cfm background customer data management as a term was coined in the 1990s pre dating the alternative term enterprise feedback management efm customer data management cdm was introduced as a software solution that would replace earlier disc based or paper based surveys and spreadsheet data initially cdm solutions were marketed to businesses as software specific to one company and often to one department within that company this was superseded by application service provider s asps where software was hosted for end user organizations thus avoiding the necessity for it professionals to deploy and support software however asps with their single tenancy architecture were in turn superseded by software as a service saas engineered for multi tenancy by 2007 saas applications giving businesses on demand access to their customer information were rapidly gaining popularity compared with asps cloud computing now includes saas and many prominent cdm providers offer cloud based applications to their clients in recent years there has been a push away from the term efm with many of those working in this area advocating the slightly updated use of cdm the return to the term cdm is largely based on the greater need for clarity around the solutions offered by companies and on the desire to retire terminology veering on techno jargon that customers may have a hard time understanding ref insitesystems com december 2010 http www insitesystems com systems blogs the problem with efm html ref references see wikipedia footnotes on how to create references using ref ref tags which will then appear here automatically reflist categories category articles created via the article wizard category data management'
b'a data management plan or dmp is a formal document that outlines how you will handle your data both during your research and after the project is completed ref http www2 lib virginia edu brown data plan html ref the goal of a data management plan is to consider the many aspects of data management metadata generation data preservation and analysis before the project begins this ensures that data are well managed in the present and prepared for preservation in the future importance preparing a data management plan before data are collected ensures that data are in the correct format organized well and better annotated ref http libraries mit edu data management plan why ref this saves time in the long term because there is no need to re organize re format or try to remember details about data it also increases research efficiency since both the data collector and other researchers will be able to understand and use well annotated data in the future one component of a good data management plan is data archiving and preservation by deciding on an archive ahead of time the data collector can format data during collection to make its future submission to a database easier if data are preserved they are more relevant since they can be re used by other researchers it also allows the data collector to direct requests for data to the database rather than address requests individually data that are preserved have the potential to lead to new unanticipated discoveries and they prevent duplication of scientific studies that have already been conducted data archiving also provides insurance against loss by the data collector funding agencies are beginning to require data management plans as part of the proposal and evaluation process ref http www nsf gov bfa dias policy dmpfaqs jsp ref major components information about data data format include a description of data to be produced by the project ref cite web title elements of a data management plan url http www icpsr umich edu icpsrweb content datamanagement dmp elements html website www icpsr umich edu accessdate 2015 09 30 ref this might include but is not limited to data that are experimental observational raw or derived physical collections models simulations curriculum materials software images how will the data be acquired when and where will they be acquired after collection how will the data be processed include information about software used algorithms workflow scientific workflows describe the file formats that will be used justify those formats and describe the naming conventions used identify the quality assurance quality control measures that will be taken during sample collection analysis and processing if existing data are used what are their origins how will the data collected be combined with existing data what is the relationship between the data collected and existing data how will the data be managed in the short term consider the following version control for files backing up data and data products security protection of data and data products who will be responsible for management metadata content and format metadata are the contextual details including any information important for using data this may include descriptions of temporal and spatial details instruments parameters units files etc metadata is commonly referred to as data about data ref michener wk and jw brunt 2000 ecological data design management and processing blackwell science 180p ref consider the following what metadata are needed include any details that make data meaningful how will the metadata be created and or captured examples include lab notebooks gps hand held units auto saved files on instruments etc what format will be used for the metadata consider the metadata standards commonly used in the scientific discipline that contains your work there should be justification for the format chosen policies for access sharing and re use describe any obligations that exist for sharing data collected these may include obligations from funding agencies institutions other professional organizations and legal requirements include information about how data will be shared including when the data will be accessible how long the data will be available how access can be gained and any rights that the data collector reserves for using data address any ethical or privacy issues with data sharing address intellectual property copyright issues who owns the copyright what are the institutional publisher and or funding agency policies associated with intellectual property are there embargoes for political commercial or patent reasons describe the intended future uses users for the data indicate how the data should be cited by others how will the issue of persistent citation be addressed for example if the data will be deposited in a public archive will the dataset have a digital object identifier doi assigned to it long term storage and data management researchers should identify an appropriate archive for long term preservation of their data by identifying the archive early in the project the data can be formatted transformed and documented appropriately to meet the requirements of the archive researchers should consult colleagues and professional societies in their discipline to determine the most appropriate database and include a backup archive in their data management plan in case their first choice goes out of existence early in the project the primary researcher should identify what data will be preserved in an archive usually preserving the data in its most raw form is desirable although data derivatives and products can also be preserved an individual should be identified as the primary contact person for archived data and ensure that contact information is always kept up to date in case there are requests for data or information about data budget data management and preservation costs may be considerable depending on the nature of the project by anticipating costs ahead of time researchers ensure that the data will be properly managed and archived potential expenses that should be considered are personnel time for data preparation management documentation and preservation hardware and or software needed for data management backing up security documentation and preservation costs associated with submitting the data to an archive the data management plan should include how these costs will be paid nsf data management plan all grant proposals submitted to national science foundation nsf must include a data management plan that is no more than two pages ref http www nsf gov pubs policydocs pappguide nsf11001 gpg 2 jsp dmp ref this is a supplement not part of the 15 page proposal and should describe how the proposal will conform to the award and administration guide policy see below it may include the following the types of data the standards to be used for data and metadata format and content policies for access and sharing policies and provisions for re use plans for archiving data policy summarized from the national science foundation nsf award and administration guide section 4 dissemination and sharing of research results ref http www nsf gov bfa dias policy dmp jsp ref promptly publish with appropriate authorship share data samples physical collections and supporting materials with others within a reasonable time frame share software and inventions investigators can keep their legal rights over their intellectual property but they still have to make their results data and collections available to others policies will be implemented via proposal review award negotiations and conditions support incentives esrc data management plan since 1995 the uk s economic and social research council esrc have had a research data policy in place the current esrc research data policy states that research data created as a result of esrc funded research should be openly available to the scientific community to the maximum extent possible through long term preservation and high quality data management ref http www esrc ac uk about esrc information data policy aspx esrc research data policy 2010 ref esrc requires a data management plan for all research award applications where new data are being created such plans are designed to promote a structured approach to data management throughout the data lifecycle resulting in better quality data that is ready to archive for sharing and re use the uk data service the esrc s flagship data service provides practical guidance on research data management planning suitable for social science researchers in the uk and around the world ref http ukdataservice ac uk manage data aspx prepare and manage data guidance from the uk data service ref ref http www sagepub com books book240297 sage handbook managing and sharing data a guide to good practice ref esrc has a longstanding arrangement with the uk data archive based at the university of essex as a place of deposit for research data with award holders required to offer data resulting from their research grants via the uk data service ref http www data archive ac uk deposit who uk data archive who can deposit data ref the archive enables data re use by preserving data and making them available to the research and teaching communities references reflist further reading cite book title delivering research data management services last pryor first graham publisher facet publishing year 2014 isbn 9781856049337 location pages external links http www sagepub com books book240297 subject b00 sortby defaultpubdate 20desc fs 1 sage handbook managing and sharing research data a guide to good practice http dmp cdlib org dmptool guidance and resources for data management plans http www cdlib org services uc3 dmp index html california digital library university of california curation center uc3 http www dataone org plans dataone http www2 lib virginia edu brown data plan html university of virginia library https dmponline dcc ac uk dmponline http www dcc ac uk resources data management plans digital curation centre http www lib umich edu research data management and publishing support nsf data management plans directorate guide university of michigan library http www nsf gov pubs policydocs pappguide nsf11001 gpg 2 jsp dmp nsf grant proposal guidelines http www icpsr umich edu icpsrweb icpsr dmp index jsp inter university consortium for political and social research http lno lternet edu node 269 lter blog how to write a data management plan http www gesis org en archive and data management training and information center research data management data management plan more information about data management plans at gesis leibniz institute for the social sciences http ukdataservice ac uk manage data aspx uk data service prepare and manage data guidance and tools for social science researchers http www consorciomadrono es pagoda plan de gesti\xc3\xb3n de datos pagoda dmp toolkit of the consortium of universities of the region of madrid and the uned for library cooperation madro\xc3\xb1o spain categories category articles created via the article wizard category data management'
b'the australian national data service ands was established in 2008 to help address the challenges of storing and managing australia s research data and making it discoverable and accessible for validation and reuse it is a joint collaboration between monash university the australian national university and csiro background ands is funded by the australian department of education the funding has been provided through australian government s national collaborative research infrastructure strategy ncris as part of the platforms for collaboration investment plan ref cite web url http ncris innovation gov au capabilities pages pfc aspx ands title platforms for collaboration accessdate 2011 06 02 deadurl yes archiveurl https web archive org web 20110702094824 http ncris innovation gov au 80 capabilities pages pfc aspx archivedate 2011 07 02 df ref the ncris roadmap emphasized the vital importance of eresearch infrastructure to australian future research competitiveness ref cite web title eresearch infrastructure url http www pfc org au bin view main publisher ncris accessdate 25 june 2011 dead link date october 2016 bot internetarchivebot fix attempted yes ref in mid 2009 ands was further funded by the education investment fund eif for the establishment of the australian research data commons under the australian government s super science initiative ref cite web title super science initiative url http www innovation gov au science researchinfrastructure pages superscience aspx publisher diisr accessdate 25 june 2011 deadurl yes archiveurl https web archive org web 20110601193908 http www innovation gov au science researchinfrastructure pages superscience aspx archivedate 1 june 2011 df ref research data australia research data australia formerly the ands collections registry is an online discovery service run by ands ref http researchdata ands org au home about research data australia ref ref http www ands org au resource registry html ands collections registry webarchive url https web archive org web 20140302113241 http www ands org au resource registry html date march 2 2014 ref it allows researchers to publicise the existence of their research data and enable prospective users of that data to find it research data australia makes use of the iso 2146 based rif cs metadata standard ref http ands org au guides cpguide cpgrifcs html about rif cs ref external links official website http www ands org au http researchdata ands org au research data australia references references category data management australia org stub'
b'image supercolumn data store png 300px thumb the super column consists of a unique super column name and a number of columns a super column is a tuple a pair with a binary super column name and a value that maps it to many columns ref cite web accessdate 2011 03 18 author arin sarkissian date 2009 09 01 location http arin me post 40054651676 wtf is a supercolumn cassandra data model publisher arin sarkissian title wtf is a supercolumn an intro to the cassandra data model quote a supercolumn is a tuple with a binary name a value which is a map containing an unbounded number of columns keyed by the column s name url ref they consist of a key value pairs where the values are columns theoretically speaking super columns are sorting algorithm sorted associative array of columns ref cite web accessdate 2011 03 18 location http wiki apache org cassandra datamodel publisher apache cassandra title cassandra wiki data model super columns url http wiki apache org cassandra datamodel ref similar to a regular column family where a row is a sorted map of column names and column values a row in a super column family is a sorted map of super column names that maps to column names and column values a super column is part of a keyspace data model together with other super columns and column families and columns code example written in the json like syntax a super column definition can be like this source lang sql mccv tags cassandra incubator url http incubator apache org cassandra jira url http issues apache org jira browse cassandra thrift jira url http issues apache org jira browse thrift source see also column data store keyspace nosql references reflist external links http wiki apache org cassandra datamodel the apache cassandra data model interwikies category data management categories database stub'
b'infobox standardref title cloud data management interface status published year started 2009 version 1 1 1 organization storage networking industry association base standards hypertext transfer protocol related standards network file system abbreviation cdmi domain cloud computing license website http www snia org cloud cdmi technical working group the cloud data management interface cdmi is a http www snia org snia standard that specifies a protocol for self provisioning administering and accessing cloud storage ref cite web title cloud data management interface url http www snia org cdmi publisher snia accessdate 26 june 2011 ref cdmi defines rest ful http operations for assessing the capabilities of the cloud storage system allocating and accessing containers and objects managing users and groups implementing access control attaching metadata making arbitrary queries using persistent queues specifying retention intervals and holds for compliance purposes using a logging facility billing moving data between cloud systems and exporting data via other protocols such as iscsi and network file system protocol nfs transport security is obtained via transport layer security tls capabilities compliant implementations must provide access to a set of configuration parameters known as capabilities these are either boolean values that represent whether or not a system supports things such as queues export via other protocols path based storage and so on or numeric values expressing system limits such as how much metadata may be placed on an object as a minimal compliant implementation can be quite small with few features clients need to check the cloud storage system for a capability before attempting to use the functionality it represents containers a cdmi client may access objects including containers by either name or object id oid assuming the cdmi server supports both methods when storing objects by name it is natural to use nested named containers the resulting structure corresponds exactly to a traditional filesystem directory structure objects objects are similar to files in a traditional file system but are enhanced with an increased amount of and capacity for metadata as with containers they may be accessed by either name or oid when accessed by name clients use uniform resource locator urls that contain the full pathname of objects to create read update and delete them when accessed by oid the url specifies an oid string in the cdmi objectid container this container presents a flat name space conformant with standard object storage system semantics subject to system limits objects may be of any size or type and have arbitrary user supplied metadata attached to them systems that support query allow arbitrary queries to be run against the metadata domains users and groups cdmi supports the concept of a domain similar in concept to a domain in the windows active directory model users and groups created in a domain share a common administrative database and are known to each other on a first name basis i e without reference to any other domain or system domains also function as containers for usage and billing summary data access control cdmi exactly follows the access control list acl and access control entry ace model used for file authorization operations by nfsv4 nfsv4 nfsv4 this makes it also compatible with microsoft windows systems metadata cdmi draws much of its metadata model from the xam specification objects and containers have storage system metadata data system metadata and arbitrary user specified metadata in addition to the metadata maintained by an ordinary filesystem atime etc queries cdmi specifies a way for systems to support arbitrary queries against cdmi containers with a rich set of comparison operators including support for regular expression s queues cdmi supports the concept of persistent fifo computing and electronics fifo first in first out queues these are useful for job scheduling order processing and other tasks in which lists of things must be processed in order compliance both retention intervals and retention holds are supported by cdmi a retention interval consists of a start time and a retention period during this time interval objects are preserved as immutable and may not be deleted a retention hold is usually placed on an object because of judicial action and has the same effect objects may not be changed nor deleted until all holds placed on them are removed logging cdmi clients can sign up for logging of system security and object access events on servers that support it this feature allows clients to see events locally as the server logs them billing summary information suitable for billing clients for on demand services can be obtained by authorized users from systems that support it serialization serialization of objects and containers allows export of all data and metadata on a system and importation of that data into another cloud system foreign protocols cdmi supports export of containers as nfs or cifs shares clients that mount these shares see the container hierarchy as an ordinary filesystem directory hierarchy and the objects in the containers as normal files metadata outside of ordinary filesystem metadata may or may not be exposed provisioning of iscsi luns is also supported client sdks http www snia org forums csi programs cdmiportal cdmi reference implementation https github com scality droplet droplet https github com livenson libcdmi java libcdmi java https github com livenson libcdmi python libcdmi python https github com projectpvg1 net sdk net sdk see also comparison of cdmi server implementations references reflist external links http www iso org iso iso catalogue catalogue tc catalogue detail htm csnumber 40874 iso 8601 international organization for standardization data elements and interchange formats information interchange representation of dates and times iso 8601 20044 http www itu int itu t publications recs html itu t509 international telecommunications union telecommunication standardization sector itu t recommendation x 509 information technology open systems interconnection the directory public key and attribute certificate frameworks may 2000 specification and technical corrigenda http www unix org version3 ieee std html posix ere the open group base specifications issue 6 ieee std 1003 1 2004 edition http www cloudplugfest org cloud interoperability plugfest project category cloud storage category data management'
b'lowercase title purexml is the native xml storage feature in the ibm db2 data server purexml provides query language s storage technologies indexing technologies and other features to support xml data the word pure in purexml was chosen to indicate that db2 natively stores and natively processes xml data in its inherent hierarchical structure as opposed to treating xml data as plain text or converting it into a relational format ref http www ibm com developerworks blogs page datastudioteam entry purexml and purequery what s ref technical information db2 includes two distinct storage mechanisms one for efficiently managing traditional sql data types and another for managing xml data the underlying storage mechanism is transparent to users and applications they simply use sql including sql with xml extensions or sql xml or xquery to work with the data xml data is stored in columns of db2 tables that have the xml data type xml data is stored in a parsed format that reflects the hierarchical nature of the original xml data as such purexml uses trees and nodes as its model for storing and processing xml data if you instruct db2 to validate xml data against an xml schema prior to storage db2 annotates all nodes in the xml hierarchy with information about the schema types otherwise it will annotate the nodes with default type information upon storage db2 preserves the internal structure of xml data converting its tag names and other information into integer values doing so helps conserve disk space and also improves the performance of queries that use navigational expressions however users aren t aware of this internal representation finally db2 automatically splits xml nodes across multiple database pages as needed xml schemas specify which xml elements are valid in what order these elements should appear in xml data which xml data types are associated with each element and so on purexml allows you to validate the cells in a column of xml data against no schema one schema or multiple schemas purexml also provides tools to support evolving xml schemas ibm has enhanced its programming language interfaces to support access to its xml data these enhancements span java programming language java jdbc c programming language c embedded sql and call level interface cobol embedded sql php and microsoft s net framework through the db2 net provider history purexml was first included in the db2 9 for linux unix and microsoft windows release which was codenamed viper in june 2006 ref cite web url http www 03 ibm com press us en pressrelease 19781 wss title ibm news room 2006 06 08 ibm transforms database market with introduction of db2 united states archiveurl https web archive org web 20121011235127 http www 03 ibm com press us en pressrelease 19781 wss archivedate 2012 10 11 ref it was available on db2 9 for z os in march 2007 ref cite web url http www 03 ibm com press us en pressrelease 21189 wss title ibm news room 2007 03 06 ibm unveils db2 viper for the mainframe united states archiveurl https web archive org web 20121011235143 http www 03 ibm com press us en pressrelease 21189 wss archivedate 2012 10 11 ref in october 2007 ibm released db2 9 5 with improved xml data transaction performance and improved storage savings ref cite web url http www 03 ibm com press us en pressrelease 22455 wss title ibm news room 2007 10 15 ibm extends data server technology lead with introduction of db2 quot viper 2 quot united states archiveurl https web archive org web 20121011235149 http www 03 ibm com press us en pressrelease 22455 wss archivedate 2012 10 11 ref in june 2009 ibm released db2 9 7 with xml supported for database partitioned range partitioned and multi dimensionally clustered tables as well as compression of xml data and indices ref cite web url http www 03 ibm com press us en pressrelease 27279 wss title ibm news room 2009 04 22 ibm database software improves operational efficiency and cuts storage costs by up to 75 united states archiveurl https web archive org web 20121121014600 http www 03 ibm com press us en pressrelease 27279 wss archivedate 2012 11 21 ref competition see also xml database db2 is a hybrid data server it offers data management for traditional relational data as well as providing native xml data management other vendors that offer data management for both relational data and native xml storage include oracle corporation oracle with its oracle database 11g product and microsoft with its microsoft sql server sql server product purexml also competes with native xml databases like basex database basex exist marklogic or sedna database sedna user groups the international db2 users group idug is an independent not for profit association of it professionals who use ibm db2 idug provides education technical resources peer networking opportunities online resources and other programs for db2 users books ibm international technical support organization itso has published the following books which are available in print or as free e books http www redbooks ibm com abstracts sg247298 html open db2 9 purexml overview and fast start http www redbooks ibm com abstracts sg247315 html open db2 9 purexml guide the following books are also available for purchase http www amazon com db2 purexml cookbook master hybrid dp 0138150478 db2 purexml cookbook master the power of ibm hybrid data server education and training the following purexml classroom and online courses are available from ibm education http www 304 ibm com jct03001c services learning ites wss us en pagetype course description coursecode cg130 query and manage xml data with db2 9 ibm course cg130 classroom duration 4 days http www 304 ibm com jct03001c services learning ites wss us en pagetype course description coursecode cg100 query xml data with db2 9 ibm course cg100 classroom duration 2 days first 2 days of cg130 managing xml data in db2 9 ibm course cg160 classroom duration 2 days last 2 days of cg130 http www 304 ibm com jct03001c services learning ites wss us en pagetype course search sortby 5 searchtype 1 sortdirection 9 includenotscheduled 15 rowstart 0 rowstoreturn 20 maxsearchresults 200 searchstring ct140 language en country us db2 purexml ibm course ct140 self paced study plus live virtual classroom see also ibm db2 xml database references reflist external links official website http www ibm com software data db2 xml http www ibm com developerworks wikis display db2xml home purexml wiki http www ibm com developerworks forums forum jspa forumid 1423 purexml forum http www ibm com developerworks blogs page purexml purexml team blog http www nativexmldatabase com native xml database blog http blog 4loeser net blog with purexml topics online communities online communities allow purexml users to network with fellow professionals http www linkedin com groups gid 129185 purexml group on linkedin ibm db2 product family category xml software category data management category data modeling category ibm db2 category ibm software category xml databases'
b'file linnaeus regnum animale 1735 png thumb linnaean taxonomy a metadata system used historically for grouping animals in zoos first published in 1735 file 6123034166 card catalog jpg thumb card catalog and digital media access point meta data management also known as metadata management without the hyphen involves managing data about other data whereby this other data is generally referred to as content data the term is used most often in relation to digital media but older forms of metadata are catalogs dictionaries and taxonomies for example the dewey decimal classification is a metadata management system for books developed in 1876 for libraries metadata schema metadata management can be defined as the end to end process and governance framework for creating controlling enhancing attributing defining and managing a metadata schema model or other structured aggregation system either independently or within a repository and the associated supporting processes often to enable the management of content for web based systems uniform resource locator url s images video etc may be referenced from a triples table of object attribute and value scope with specific knowledge domain s the boundaries of the metadata for each must be managed since a general ontology is not useful to experts in one field whose language is knowledge domain specific metadata manager if one is in the process of making a knowledge management solution creating a metadata schema and developing a system in which metadata is managed are very important in such a project a dedicated metadata manager may be appointed in order to maintain adherence to metadata and information management standards citation needed date january 2011 this is a person who will be responsible for the metadata strategy and possibly the implementation a metadata manager does not need to know about and be involved with everything concerning the solution but it does help to have an understanding of as much of the process as possible to make sure a relevant schema is developed metadata management over time managing the metadata in a knowledge management solution is an important step in a metadata strategy it is part of the strategy to make sure that the metadata are complete current and correct at any given time managing a metadata project is also about making sure that users of the system are aware of the possibilities allowed by a well designed metadata system and how to maximize the benefits of metadata regularly monitoring the metadata to ensure that the schema remains relevant is advised wikipedia metadata wikipedia is a project that actively manages metadata for its articles and files for example volunteer editors carefully curate new biographical articles based on the notability claim to fame name birth and or death dates ref see the internal wikipedia project on the english wikipedia called wikipedia wikiproject biography ref similarly volunteer editors carefully curate new architectural articles based on name municipality or geo coordinates ref see wikipedia wikiproject architecture ref when new articles with a valid alternate spelling are added to wikipedia that match up to existing articles based on metadata these are then manually checked and if needed tagged for merging ref see wikipedia wikiproject merge ref when new articles are added that are considered out of scope or otherwise unfit for wikipedia these are nominated for deletion ref see wikipedia articles for deletion ref to help keep track of metadata on wikipedia the new wikimedia project wikidata was established in 2012 click on the pictures to view more metadata about these images gallery file sta eulalia jpg this picture of the barcelona cathedral was uploaded to the english wikipedia in 2003 to illustrate its wikipedia article and was transferred to wikimedia commons in 2007 so it could be used in other language versions of wikipedia file article catedral pantalla estreta png this screenprint of the catalan wikipedia page on the cathedral features several photos including this one the screenprint was uploaded to wikimedia commons in 2007 soon after the photo was available there but ca catedral de barcelona that article on the catalan wikipedia has since been expanded gallery see also data defined storage metadata discovery metadata publishing metadata registry iso iec 11179 dublin core references reflist defaultsort meta data management category metadata category data management'
b'infobox software name h store logo file h store logo png 80px h store logo screenshot caption developer brown university brown carnegie mellon university cmu massachusetts institute of technology mit yale university yale latest release version june 2016 latest release date start date and age 2016 06 03 programming language c java programming language java operating system linux mac os x genre database management system license bsd license gpl website url hstore cs brown edu h store is an experimental database management system dbms designed for online transaction processing applications that is being developed by a team at brown university carnegie mellon university the massachusetts institute of technology and yale university ref cite web url http hstore cs brown edu title h store next generation oltp dbms research accessdate 2011 08 07 ref ref cite web url http www dbms2 com 2008 02 18 mike stonebraker calls for the complete destruction of the old dbms order title stonebraker s h store there s something happenin here first david last van couvering date 2008 02 18 is this one just plain wrong publication date 2011 03 11 accessdate 2012 07 18 ref the system s design was developed in 2007 by database researchers michael stonebraker samuel madden mit sam madden andy pavlo and daniel abadi ref cite conference authorlink michael stonebraker first mike last stonebraker title the end of an architectural era it s time for a complete rewrite booktitle vldb 07 proceedings of the 33rd international conference on very large data bases location vienna austria year 2007 url http hstore cs brown edu papers hstore endofera pdf format pdf display authors etal ref ref cite journal last1 kallman first1 robert last2 kimura first2 hideaki last3 natkins first3 jonathan last4 pavlo first4 andrew last5 rasin first5 alexander last6 zdonik first6 stanley authorlink6 stan zdonik last7 jones first7 evan p c last8 madden first8 samuel authorlink8 samuel madden mit last9 stonebraker first9 michael authorlink9 michael stonebraker last10 zhang first10 yang last11 hugg first11 john last12 abadi first12 daniel j title h store a high performance distributed main memory transaction processing system journal proc vldb endowment year 2008 volume 1 series 2 pages 1496 1499 url http hstore cs brown edu papers hstore demo pdf issn 2150 8097 ref ref cite web url http www dbms2 com 2008 02 18 mike stonebraker calls for the complete destruction of the old dbms order title mike stonebraker calls for the complete destruction of the old dbms order first curt last monash year 2008 publication date 2008 02 18 accessdate 2012 07 18 ref architecture the significance of the h store is that it is the first implementation of a new class of parallel database parallel database management systems called newsql ref cite web url http www cs brown edu courses cs227 papers newsql aslett newsql pdf title how will the database incumbents respond to nosql and newsql first matthew last aslett publisher 451 group publication date 2011 04 04 year 2010 accessdate 2012 07 06 deadurl yes archiveurl https web archive org web 20120127202623 http www cs brown edu courses cs227 papers newsql aslett newsql pdf archivedate january 27 2012 ref good link just not supporting h store directly is supporting voltdb that is related but doesn not state the connection ref cite web url http cacm acm org blogs blog cacm 109710 new sql an alternative to nosql and old sql for new oltp apps fulltext title newsql an alternative to nosql and old sql for new oltp apps first michael last stonebraker publisher communications of the acm publication date 2011 06 16 accessdate 2012 07 06 ref that provide the high throughput and high availability of nosql systems but without giving up the acid transactional guarantees of a traditional dbms ref cite web url http preferisco blogspot com 2008 03 h store new architectural era or just html title h store a new architectural era or just a toy first nigel last thomas date 2008 03 01 accessdate 2012 07 05 ref such systems are able to scale out horizontally across multiple machines to improve throughput as opposed to moving to a more powerful more expensive machine for a single node system ref cite web url http blogs the451group com information management 2008 03 04 is h store the future of database management systems title is h store the future of database management systems first matthew last aslett date 2008 03 04 accessdate 2012 07 05 ref h store is able to execute transaction processing with high throughput by forgoing much of legacy architecture of ibm system r system r like systems for example h store was designed as a parallel database parallel row storage relational dbms that runs on a cluster of shared nothing architecture shared nothing main memory executor nodes ref cite web url http hstore cs brown edu documentation architecture overview title h store architecture overview accessdate 2011 08 07 ref the database is partition database partitioned into disjoint subsets that are assigned to a single threaded execution engine assigned to one and only one multi core processor core on a node each engine has exclusive access to all of the data at its partition because it is single threaded only one transaction at a time is able to access the data stored at its partition thus there are no physical locks or latches in the system and no transaction will stall waiting for another transaction once it is started ref cite web url http www zdnet com blog btl h store complete destruction of the old dbms order 8055 title h store complete destruction of the old dbms order first larry last dignan year 2008 accessdate 2012 07 05 ref licensing h store is licensed under the bsd license and gpl licenses the commercial version of h store s design is voltdb ref cite web url http www dbms2 com 2009 06 22 h store horizontica voltdb title h store is now voltdb first curt last monash year 2009 accessdate 2011 07 14 postscript ref see also portal free software voltdb c store transaction processing references reflist external links category data management category distributed data stores category free database management systems category newsql'
b'multiple issues original research date february 2011 more footnotes date september 2009 in computing data deduplication is a specialized data compression technique for eliminating duplicate copies of repeating data related and somewhat synonymous terms are intelligent data compression and single instance storage single instance data storage this technique is used to improve storage utilization and can also be applied to network data transfers to reduce the number of bytes that must be sent in the deduplication process unique chunks of data or byte patterns are identified and stored during a process of analysis as the analysis continues other chunks are compared to the stored copy and whenever a match occurs the redundant chunk is replaced with a small reference that points to the stored chunk given that the same byte pattern may occur dozens hundreds or even thousands of times the match frequency is dependent on the chunk size the amount of data that must be stored or transferred can be greatly reduced ref http www druva com blog 2009 01 09 understanding data deduplication understanding data deduplication druva 2009 retrieved 2013 2 13 ref this type of deduplication is different from that performed by standard file compression tools such as lz77 and lz78 whereas these tools identify short repeated substrings inside individual files the intent of storage based data deduplication is to inspect large volumes of data and identify large sections such as entire files or large sections of files that are identical in order to store only one copy of it this copy may be additionally compressed by single file compression techniques for example a typical email system might contain 100 instances of the same 1 mb megabyte file attachment each time the email platform is backed up all 100 instances of the attachment are saved requiring 100 mb storage space with data deduplication only one instance of the attachment is actually stored the subsequent instances are referenced back to the saved copy for deduplication ratio of roughly 100 to 1 benefits storage based data deduplication reduces the amount of storage needed for a given set of files it is most effective in applications where many copies of very similar or even identical data are stored on a single disk a surprisingly common scenario in the case of data backups which routinely are performed to protect against data loss most data in a given backup remain unchanged from the previous backup common backup systems try to exploit this by omitting or hard link ing files that haven t changed or storing data differencing differences between files neither approach captures all redundancies however hard linking does not help with large files that have only changed in small ways such as an email database differences only find redundancies in adjacent versions of a single file consider a section that was deleted and later added in again or a logo image included in many documents network data deduplication is used to reduce the number of bytes that must be transferred between endpoints which can reduce the amount of bandwidth required see wan optimization for more information virtual servers benefit from deduplication because it allows nominally separate system files for each virtual server to be coalesced into a single storage space at the same time if a given server customizes a file deduplication will not change the files on the other servers something that alternatives like hard links or shared disks do not offer backing up or making duplicate copies of virtual environments is similarly improved deduplication overview deduplication may occur in line as data is flowing or post process after it has been written post process deduplication with post process deduplication new data is first stored on the storage device and then a process at a later time will analysis analyze the data looking for duplication the benefit is that there is no need to wait for the hash calculations and lookup to be completed before storing the data thereby ensuring that store performance is not degraded implementations offering policy based operation can give users the ability to defer optimization on active files or to process files based on type and location one potential drawback is that duplicate data may be unnecessarily stored for a short time which can be problematic if the system is nearing full capacity in line deduplication alternatively deduplication hash calculations can be done in real time as data enters the target device if the storage system identifies a block which it has already stored only a reference to the existing block is stored rather than the whole new block the advantage of in line deduplication over post process deduplication is that it requires less storage since duplicate data is never stored on the negative side it is frequently argued by whom date august 2016 that because hash calculations and lookups take so long data ingestion can be slower thereby reducing the backup throughput of the device however certain vendors with in line deduplication have demonstrated equipment with similar performance to their post process deduplication counterparts according to whom date april 2015 data coming in is stored into lining space before it hits real storage blocks on ssd disks lining space is provided using non volatile random access memory nvram which is not cost efficient according to whom date august 2016 post process and in line deduplication methods are often heavily debated ref cite web url http www backupcentral com content view 134 47 title in line or post process de duplication updated 6 08 publisher backup central date accessdate 2009 10 16 deadurl yes archiveurl https web archive org web 20091206035054 http www backupcentral com 80 content view 134 47 archivedate 2009 12 06 df ref ref cite web url http searchdatabackup techtarget com tip 0 289483 sid187 gci1315295 00 html title inline vs post processing deduplication appliances publisher searchdatabackup techtarget com date accessdate 2009 10 16 ref data formats snia dictionary identifies two methods content agnostic data deduplication a data deduplication method that does not require awareness of specific application data formats content aware data deduplication a data deduplication method that leverages knowledge of specific application data formats source versus target deduplication another way to classify data deduplication methods is according to where they occur deduplication occurring close to where data is created is often referred to according to whom date august 2016 as source deduplication when it occurs near where the data is stored it is commonly called target deduplication source deduplication ensures that data on the data source is deduplicated this generally takes place directly within a file system ref cite web url http www microsoft com windowsserver2008 en us wss08 sis aspx title windows server 2008 windows storage server 2008 publisher microsoft com date accessdate 2009 10 16 deadurl yes archiveurl https web archive org web 20091004073508 http www microsoft com 80 windowsserver2008 en us wss08 sis aspx archivedate 2009 10 04 df ref ref cite web url http www netapp com us products platform os dedupe html title products platform os publisher netapp date accessdate 2009 10 16 ref the file system will periodically scan new files creating hashes and compare them to hashes of existing files when files with same hashes are found then the file copy is removed and the new file points to the old file unlike hard links however duplicated files are considered to be separate entities and if one of the duplicated files is later modified then using a system called copy on write a copy of that file or changed block is created the deduplication process is transparent to the users and backup applications backing up a deduplicated file system will often cause duplication to occur resulting in the backups being bigger than the source data target deduplication is the process of removing duplicates when the data was not generated at that location example of this would be a server connected to a san nas the san nas would be a target for the server target deduplication the server is not aware of any deduplication the server is also the point of data generation a second example would be backup if you have a backup system with deduplication generally this will be a backup store such as a data repository or a virtual tape library deduplication methods one of the most common forms of data deduplication implementations works by comparing chunks of data to detect duplicates for that to happen each chunk of data is assigned an identification calculated by the software typically using cryptographic hash functions in many implementations the assumption is made that if the identification is identical the data is identical even though this cannot be true in all cases due to the pigeonhole principle other implementations do not assume that two blocks of data with the same identifier are identical but actually verify that data with the same identification is identical ref an example of an implementation that checks for identity rather than assuming it is described in http appft1 uspto gov netacgi nph parser sect1 pto2 sect2 hitoff p 1 u 2fnetahtml 2fpto 2fsearch bool html r 1 f g l 50 co1 and d pg01 s1 shnelvar os shnelvar rs shnelvar us patent application 20090307251 ref if the software either assumes that a given identification already exists in the deduplication namespace or actually verifies the identity of the two blocks of data depending on the implementation then it will replace that duplicate chunk with a link once the data has been deduplicated upon read back of the file wherever a link is found the system simply replaces that link with the referenced data chunk the deduplication process is intended to be transparent to end users and applications commercial deduplication implementations differ by their chunking methods and architectures chunking in some systems chunks are defined by physical layer constraints e g 4kb block size in write anywhere file layout wafl in some systems only complete files are compared which is called single instance storage or sis the most intelligent but cpu intensive method to chunking is generally considered to be sliding block in sliding block a window is passed along the file stream to seek out more naturally occurring internal file boundaries client backup deduplication this is the process where the deduplication hash calculations are initially created on the source client machines files that have identical hashes to files already in the target device are not sent the target device just creates appropriate internal links to reference the duplicated data the benefit of this is that it avoids data being unnecessarily sent across the network thereby reducing traffic load primary storage and secondary storage by definition primary storage systems are designed for optimal performance rather than lowest possible cost the design criteria for these systems is to increase performance at the expense of other considerations moreover primary storage systems are much less tolerant of any operation that can negatively impact performance also by definition secondary storage systems contain primarily duplicate or secondary copies of data these copies of data are typically not used for actual production operations and as a result are more tolerant of some performance degradation in exchange for increased efficiency to date data deduplication has predominantly been used with secondary storage systems the reasons for this are two fold first data deduplication requires overhead to discover and remove the duplicate data in primary storage systems this overhead may impact performance the second reason why deduplication is applied to secondary data is that secondary data tends to have more duplicate data backup application in particular commonly generate significant portions of duplicate data over time data deduplication has been deployed successfully with primary storage in some cases where the system design does not require significant overhead or impact performance drawbacks and concerns whenever data is transformed concerns arise about potential loss of data by definition data deduplication systems store data differently from how it was written as a result users are concerned with the integrity of their data the various methods of deduplicating data all employ slightly different techniques however the integrity of the data will ultimately depend upon the design of the deduplicating system and the quality used to implement the algorithms as the technology has matured over the past decade the integrity of most of the major products has been well proven citation needed date november 2012 one method for deduplicating data relies on the use of cryptographic hash function s to identify duplicate segments of data if two different pieces of information generate the same hash value this is known as a collision computer science collision the probability of a collision depends upon the hash function used and although the probabilities are small they are always non zero thus the concern arises that data corruption can occur if a hash collision occurs and additional means of verification are not used to verify whether there is a difference in data or not both in line and post process architectures may offer bit for bit validation of original data for guaranteed data integrity ref citation url http www evaluatorgroup com document data de duplication e2 80 93why when where and how infostor article by russ fellows title data deduplication why when where and how publisher evaluator group accessdate 2011 07 05 ref the hash functions used include standards such as sha 1 sha 256 and others the computational resource intensity of the process can be a drawback of data deduplication however this is rarely an issue for stand alone devices or appliances as the computation is completely offloaded from other systems this can be an issue when the deduplication is embedded within devices providing other services to improve performance many systems utilize both weak and strong hashes weak hashes are much faster to calculate but there is a greater risk of a hash collision systems that utilize weak hashes will subsequently calculate a strong hash and will use it as the determining factor to whether it is actually the same data or not note that the system overhead associated with calculating and looking up hash values is primarily a function of the deduplication workflow the reconstitution of files does not require this processing and any incremental performance penalty associated with re assembly of data chunks is unlikely to impact application performance another area of concern with deduplication is the related effect on snapshot computer storage snapshots backup and archival especially where deduplication is applied against primary storage for example inside a network attached storage nas filer elucidate date december 2011 reading files out of a storage device causes full reconstitution of the files also known as rehydration so any secondary copy of the data set is likely to be larger than the primary copy in terms of snapshots if a file is snapshotted prior to deduplication the post deduplication snapshot will preserve the entire original file this means that although storage capacity for primary file copies will shrink capacity required for snapshots may expand dramatically another concern is the effect of compression and encryption although deduplication is a version of compression it works in tension with traditional compression deduplication achieves better efficiency against smaller data chunks whereas compression achieves better efficiency against larger chunks the goal of encryption is to eliminate any discernible patterns in the data thus encrypted data cannot be deduplicated even though the underlying data may be redundant deduplication ultimately reduces redundancy if this was not expected and planned for this may ruin the underlying reliability of the system compare this for example to the lockss storage architecture that achieves reliability through multiple copies of data scaling has also been a challenge for deduplication systems because ideally the scope of deduplication needs to be shared across storage devices if there are multiple disk backup devices in an infrastructure with discrete deduplication then space efficiency is adversely affected a deduplication shared across devices preserves space efficiency but is technically challenging from a reliability and performance perspective citation needed date december 2011 although not a shortcoming of data deduplication there have been data breaches citation needed date august 2016 when insufficient security and access validation procedures are used with large repositories of deduplicated data in some systems as typical with cloud storage citation needed date august 2016 an attacker can retrieve data owned by others by knowing or guessing the hash value of the desired data ref cite journal title a cloud you can trust publisher ieee work ieee spectrum accessdate 2011 12 21 url http spectrum ieee org computing networks a cloud you can trust author1 christian cachin author2 matthias schunter date december 2011 ref see also capacity optimization cloud storage single instance storage content addressable storage delta encoding linked data pointer computer programming pointer record linkage identity resolution convergent encryption references reflist 30em external links biggar heidi 2007 12 11 http wayback archive org web 20120325005645 http www infostor com webcast display webcast cfm id 540 webcast the data deduplication effect fellows russ evaluator group inc http www evaluatorgroup com document data de duplication e2 80 93why when where and how infostor article by russ fellows data deduplication why when where and how http wayback archive org web 20120328022229 http www tacoma washington edu tech docs research gradresearch mspiz pdf using latent semantic indexing for data deduplication http www forbes com 2009 08 08 exagrid storage data technology cio network tape html a better way to store data http www eweek com c a database what is the difference between data deduplication file deduplication and data compression what is the difference between data deduplication file deduplication and data compression database from eweek http www snia org forums dmf programs data protect init ddsrsig snia ddsr sig http wayback archive org web 20120322084240 http www snia org forums dmf knowledge white papers and reports understanding data deduplication ratios 20080718 pdf understanding data deduplication ratios http public dhe ibm com common ssi ecm en tsu12345usen tsu12345usen pdf data footprint reduction technology whitepaper http www itnext in content doing more less html doing more with less by jatinder singh http www sersc org journals ijsia vol7 no5 2013 38 pdf byte index chunking algorithm for data deduplication defaultsort data deduplication category data management category data compression'
b'data monetization a form of monetization is generating revenue from available data sources or real time streamed data by instituting the discovery capture storage analysis dissemination and use of that data said differently it is the process by which data producers data aggregators and data consumers large and small exchange sell or trade data data monetization leverages data generated through business operations as well as data associated with individual actors and with electronic devices and sensors participating in the internet of things the ubiquity of the internet of things is generating location data and other data from sensors and mobile devices at an ever increasing rate when this data is collated against traditional databases the value and utility of both sources of data increases leading to tremendous potential to mine data for social good research and discovery and achievement of business objectives closely associated with data monetization are the emerging data as a service models for transactions involving data by the data item there are three ethical and regulatory vectors involved in data monetization due to the sometimes conflicting interests of actors involved in the data supply chain the individual data creator who generates files and records through his own efforts or owns a device such as a sensor or a mobile phone that generates data has a claim to ownership of data the business entity that generates data in the course of its operations such as its transactions with financial institutions or risk factors discovered through feedback from customers also has a claim on data captured through their systems and platforms however the person that contributed the data may also have a legitimate claim on the data internet platforms and service providers such as google or facebook that require a user to forgo some ownership interest in their data in exchange for use of the platform also have a legitimate claim on the data thus the practice of data monetization although common since 2000 is now getting increasing attention from regulators the european union and the united states congress have begun to address these issues for instance in the financial services industry regulations involving data are included in the gramm leach bliley act and dodd frank some individual creators of data are shifting to using personal data vaults ref http www freepatentsonline com y2014 0032267 html ref and implementing vendor relationship management ref vendor relationship management ref concepts as a reflection of an increasing resistance to their data being federated or aggregated and resold without compensation groups such as the personal data ecosystem consortium ref http personaldataecosystem org ref patient privacy rights ref http patientprivacyrights org ref and others are also challenging corporate cooptation of data without compensation financial services companies are a relatively good example of an industry focused on generating revenue by leveraging data credit card issuers and retail banks use customer transaction data to improve targeting of cross sell offers partners are increasingly promoting merchant based reward programs which leverage a bank s data and provide discounts to customers at the same time steps identification of available data sources this includes data currently available for monetization as well as other external data sources that may enhance the value of what s currently available connect aggregate attribute validate authenticate and exchange data this allows data to be converted directly into actionable or revenue generating insight or services set terms and prices and facilitate data trading methods for data vetting storage and access for example many global corporations have locked and siloed data storage infrastructures which stymies efficient access to data and cooperative and real time exchange perform research and analytics draw predictive insights from existing data as a basis for using data for to reduce risk enhance product development or performance or improve customer experience or business outcomes action and leveraging the last phase of monetizing data includes determining alternative or improved datacentric products ideas or services examples may include real time actionable triggered notifications or enhanced channels such as web or mobile response mechanisms pricing variables and factors a fee for use of a platform to connect buyers and sellers a fee for use of a platform to configure organize and otherwise process data included in a data trade a fee for connecting or including a device or sensor into a data supply chain a fee for connecting and credentialing a creator of a data source and a data buyer often through a federated identity a fee for connecting a data source to other data sources to be included into a data supply chain a fee for use of an internet service or other transmission service for uploading and downloading data sometimes for an individual through a personal cloud a price or exchange or other trade value assigned by a data creator or generator to a data item or a data source a price or exchange or other trade value offered by a data buyer to a data creator a price or exchange or other trade value assigned by a data buyer for a data item or a data source formatted according to criteria set by a data buyer an incremental fee assigned by a data buyer for a data item or a data set scaled to the reputation of the data creator a fee for use of encrypted keys to achieve secure data transfer a fee for use of a search algorithm specifically designed to tag data sources that contain data points of value to the data buyer a fee for linking a data creator or generator to a data collection protocol or form a fee for server actions such as a notification triggered by an update to a data item or data source included into a data supply chain benefits improved decision making that leads to real time media real time crowd sourced research improved profits decreased costs reduced risk and improved compliance more impactful decisions e g make real time decisions more timely lower latency decisions e g a vendor making purchase recommendations while the customer is still on the phone or in the store a customer connecting with multiple vendors to discover a best price triggered notifications when thresholds are reached for data values more granular decisions e g localized pricing decisions at an individual or device or sensor level versus larger aggregates frameworks there are a wide variety of industries firms and business models related to data monetization the following frameworks have been offered to help understand the types of business models that are used doug laney of gartner a leading it research and advisory firm has posited a model for a range of data monetization methods indirect data monetization using data to improve efficiencies using data to measurably reduce risks using data to develop new products markets using data to build and solidify partner relationships publishing branded indices direct data monetization bartering or trading with information information enhanced products or services selling raw data through brokers offering data report subscriptions he also suggests a set of feasibility tests and questions for any data monetization ideas being considered class wikitable type of feasibility feasibility question practical is the idea utilitarian or merely interesting cool is it usable marketable would the idea have sufficiently broad appeal internally or externally scalable can the idea be developed and implemented to the extent required or intended manageable do you have the skills to oversee the development implementation of the idea technological do you have the tools information and skills to develop and rollout the idea economical will the idea require too much investment or generate sufficient return on investment legal does the idea conform to local laws where it will be used or implemented ethical will the idea be something that has the potential for customer user public backlash example will the idea cause significant positive vs negative impact on the environment roger ehrenberg of ia ventures a vc firm that invests in this space has defined three basic types of data product firms contributory databases the magic of these businesses is that a customer provides their own data in exchange for receiving a more robust set of aggregated data back that provides insight into the broader marketplace or provides a vehicle for expressing a view give a little get a lot back in return a pretty compelling value proposition and one that frequently results in a payment from the data contributor in exchange for receiving enriched aggregated data once these contributory databases are developed and customers become reliant on their insights they become extremely valuable and persistent data assets data processing platforms these businesses create barriers through a combination of complex data architectures proprietary algorithms and rich analytics to help customers consume data in whatever form they please often these businesses have special relationships with key data providers that when combined with other data and processed as a whole create valuable differentiation and competitive barriers bloomberg is an example of a powerful data processing platform they pull in data from a wide array of sources including their own home grown data integrate it into a unified stream make it consumable via a dashboard or through an api and offer a robust analytics suite for a staggering number of use cases needless to say their scale and profitability is the envy of the industry data creation platforms these businesses solve vexing problems for large numbers of users and by their nature capture a broad swath of data from their customers as these data sets grow they become increasingly valuable in enabling companies to better tailor their products and features and to target customers with highly contextual and relevant offers customers don t sign up to directly benefit from the data asset the product is so valuable that they simply want the features offered out of the box as the product gets better over time it just cements the lock in of what is already a successful platform mint was an example of this kind of business people saw value in the core product but the product continued to get better as more customer data was collected and analyzed there weren t network effects per se but the sheer scale of the data asset that was created was an essential element of improving the product over time ref cite web last ehrenberg first roger title creating competitive advantage through data url http www iaventures com creating competitive advantage through data publisher ia ventures blog accessdate 23 november 2013 deadurl yes archiveurl https web archive org web 20131203023719 http www iaventures com creating competitive advantage through data archivedate 3 december 2013 df ref selvanathan and zuk ref big data realized developing new data driven products and services to drive growth perspective ref offer a framework that includes monetization methods that are outside the bounds of the traditional value capture systems employed by an enterprise tuned to match the context and consumption models for the target customer they offer examples of four distinct approaches platforms applications data as a service and professional services ethan mccallum and ken gleason published an o rielly ebook titled business models for the data economy collect supply store host filter refine enhance enrich simplify access analyze obscure consult advise ref cite book last gleason first ken title business models for the data economy year 2013 publisher o reilly isbn 978 1 449 37223 1 url http www oreilly com data free business models for the data economy csp ref examples packaging of data with analytics to be resold to customers for things such as wallet share market share and benchmarking integration of data with analytics into new products as a value added differentiator such as on star for general motors cars gps enabled smartphones geolocation based offers and location discounts such as those offered by facebook ref https www theguardian com technology 2011 jan 31 facebook places deals uk europe ref and groupon ref http mashable com 2011 05 10 groupon now launches ref are other prime examples of data monetization leveraging new emerging channels crm based ad targeting and media attribution such as those offered by circulate intellectual property landscape some of the patents issued since 2010 by the uspto for monetizing data generated by individuals include 8 271 346 8 612 307 8 560 464 8 510 176 and 7 860 760 these are usually in the class 705 related to electronic commerce data processing and cost and price determination some of these patents use the term the data supply chain to reflect emerging technology to federate and aggregate data in real time from many individuals and devices linked together through the internet of things another emerging term is information banking an unexplored but potentially disruptive arena for data monetization is the use of bitcoin micropayments for data transactions because bitcoins are emerging as competitors with payment services like visa or paypal that can readily enable and reduce or eliminate transaction costs transactions for as little as a single data item can be facilitated consumers as well as enterprises who desire to monetize their participation in a data supply chain may soon be able to access social network enabled bitcoin exchanges and platforms ref lomas natasha techcrunch august 18 2014 ref clickbait and data hijacking may wither as micropayments for data are ubiquitous and enabled potentially even the current need to build out data broker managed data trading exchanges may be bypassed stanley smith ref http www linkedin com pub stan smith 9 3ab b37 ref who introduced the notion of the data supply chain has said that simple micropayments for data monetization are the key to evolution of ubiquitous implementation of user configurable data supply schemata enabling data monetization on a universal scale for all data creators including the burgeoning internet of things presentations and publications 2016 https www gartner com doc 3267517 how cios and cdos can use infonomics to identify justify and fund initiatives douglas laney and michael smith gartner 29 march 2016 http www wsj com articles accountings 21st century challenge how to value intangible assets 1458605126 accounting s 21st century challenge how to value intangible assets wsj cfo journal 22 march 2016 https s3 amazonaws com files technologyreview com whitepapers mit oracle report the rise of data capital pdf the rise of data capital oracle corporation oracle and mit technology review custom 2016 http www gartner com smarterwithgartner treating information as an asset treating information as an asset christy pettey douglas laney and michael m moran smarter with gartner 17 february 2016 http www reuters com article us europe data competition iduskcn0vf0kv german competition watchdog wants big data hoards considered in merger probes reuters 6 feb 2016 https www gartner com doc 3188917 shift from a project to an asset perspective to properly value and fund it investments michael smith and douglas laney gartner 16 january 2016 http www iri com blog iri business infonomics and you infonomics and you eric leohner iri cosort january 2016 http www nowozin net sebastian blog the fair price to pay a spy an introduction to the value of information html the fair price to pay a spy an introduction to the value of information sebastian nowozin nowozin net blog 9 january 2016 2015 https www gartner com doc 3173343 measure your information yield to maximize return on information and analytics investments frank buytendijk andrew white douglas laney and thomas w oestreich gartner 1 december 2015 https www gartner com doc 3162520 ibm storms information iot markets by buying the weather company douglas laney gartner 4 november 2015 https www gartner com doc 3158117 how to adopt open data for business data and analytics and why you should alan d duncan douglas laney gartner 28 october 2015 https www gartner com doc 3151321 seven steps to monetizing your information assets douglas laney gartner 15 october 2015 http www gartner com smarterwithgartner why and how to value your information as an asset why and how to value your information as an asset heather levy douglas laney smarter with gartner blog 3 september 2015 https www gartner com doc 3106721 hackers know the value of health information so why don t hdos appreciate healthcare infonomics laura craft douglas laney gartner 5 august 2015 in august 20 2015 gartner analyst doug laney gave a publicly available webinar with replay available on http www gartner com webinar 3098518 methods for monetizing your data this is a reprise of the presentation he has given at various gartner summits and symposia around the world https www gartner com doc 3106721 hackers know the value of health information so why don t hdos appreciate healthcare infonomics laura craft douglas laney gartner 5 august 2015 https www gartner com doc 3106719 why and how to measure the value of your information assets douglas laney gartner 5 august 2015 http prezi com xirqf54fix68 utm campaign share utm medium copy rc ex0share applied infonomics measuring the economic value of information assets http www mitcdoiq org mit chief data officer symposium doug laney gartner 22 july 2015 http www kpmg com us en topics data analytics documents kpmg d a main report for web 28 june 2015 pdf data and analytics a new driver of performance and valuation institutional investor research and kpmg 28 june 2015 http www thesummits org watch htm id 132356411 the convergence of information economics and economic information corp development summit presentation replay doug laney gartner 1 july 2015 http smartdatacollective com rk paleru 319941 data opportunity are you monetizing information data opportunity but are you monetizing information smart data collective rk paleru 28 may 2015 http blogs gartner com doug laney keeping busy with data strategy keeping busy with data strategy gartner blog network doug laney 26 may 2015 http blogs wsj com cio 2015 05 20 dollar value of data radioshack other bankrupt firms auction customer data to pay debt dollar value of data radioshack other bankrupt firms auction customer data to pay debt wall street journal kim nash 20 may 2015 https www gartner com doc 3024417 the benefits and risks of using open data doug laney gartner 8 april 2015 http goodstrat com 2015 01 30 consider this does all data have value consider this does all data have value good strategy blog martyn jones 30 january 2015 http www rsd com en resources white papers theory infonomics valuating corporate information assets the theory of infonomics valuating corporate information assets white paper rsd company rsd january 2015 http www firstpost com business customer data valuable asset treat way 2046119 html customer data is a valuable asset why not treat it that way f business ajay kelkar 14 january 2015 https www youtube com watch v du4yvpu4vhe the rise of data capital video oracle corporation oracle 8 january 2015 2014 http www cmswire com cms information management quantifying the value of your data 026674 php quantifying the value of your data cms wire bassam zarkout 30 september 2014 http www rsd com en blog 201409 what infonomics what is infonomics ed hallock rsd company rsd blog 9 september 2014 http cisr mit edu blog documents 2014 08 21 2014 0801 datamonetization wixom pdf cashing in on your data mit sloan center for information systems research barbara h wixom volume xiv number 8 august 2014 https www gartner com doc 2813227 increase the return on your information investments with the information yield curve gartner andrew white and douglas laney 31 july 2014 http www forbes com sites gartnergroup 2014 07 21 the hidden shareholder boost from information assets the hidden shareholder boost from information assets forbes doug laney 21 july 2014 http searchcio bitpipe com data demandengage action resid 1401817861 376 cio decisions the new infonomics reality determining the value of data techtarget searchcio june 2014 http searchcio techtarget com opinion putting a price on information the nascent field of infonomics putting a price on information the nascent field of infonomics techtarget searchcio linda tucci 13 may 2014 http searchcio techtarget com feature six ways to measure the value of your information assets six ways to measure the value of your information assets techtarget searchcio nicole laskowski 13 may 2014 http searchcio techtarget com feature infonomics treats data as a business asset infonomics treats data as a business asset techtarget searchcio nicole laskowski 13 may 2014 http pv tl blog 2014 04 13 the economics of information management utm content buffer29c67 utm medium social utm source twitter com utm campaign buffer the economics of information management pvtl blog felix barbalet 13 april 2014 http www forbes com sites gartnergroup 2014 03 27 the hidden tax advantage of monetizing your data the hidden tax advantage of monetizing your data forbes doug laney 27 march 2014 http blogs teradata com anz the chief data officer managing the value of data comment 2147 the chief data officer managing the value of data teradata anz blog renato manongdo march 2014 https www gartner com doc 2677518 how organizations can monetize customer data gartner olive huang doug laney 6 march 2014 https www gartner com doc 2677515 ref quicksearch sthkw infonomics improving the value of customer data through applied infonomics gartner research publication douglas laney olive huang 6 march 2014 http blogs gartner com andrew white 2014 02 14 information value accrual and its asymmetry information value accrual and its asymmetry gartner blog network andrew white 14 february 2014 http blogs gartner com andrew white 2014 01 29 does information utility suffer a half life does information utility suffer a half life gartner blog network andrew white 29 january 2014 2013 http www rsd com en blog 201312 what information information governance what is the information in information governance rsd company rsd blog james amsler 30 december 2013 http blogs gartner com doug laney to twitter youre worth 101 70 to twitter you re worth 101 70 gartner blog network by douglas laney 12 november 2013 http www economistgroup com leanback big data 2 treat data like money treat data like money cmo s advice marketers must develop an investment strategy for data the economist group jim davis svp cmo sas october 2013 http www ft com intl cms s 0 205ddf5c 1bf0 11e3 b678 00144feab7de html axzz2g8pcorv3 infonomics the new economics of information the financial times doug laney vp research gartner september 2013 https www youtube com watch v mqk 5q3vjv4 value of information gigaom presentation by dave mccrory svp at warner music group july 2013 http www bankingtech com 147432 accounting for the value of big data accounting for the value of big data banking technology magazine david bannister 11 june 2013 http searchcio techtarget com opinion putting a price on information the nascent field of infonomics putting a price on information the nascent field of infonomics searchcio journal linda tucci may 2013 on march 19 2013 the chicago chapter of the product development and management association pdma held an event titled monetizing data an evening with eight of chicago s data product management leaders ref http www builtinchicago org blog check out ppt deck monetizing data evening eight chicagos data product management leaders ref 2012 http www informationweek com big data news big data analytics whats your big data worth 240144449 what s your big data worth informationweek ellis booker 17 december 2012 https www gartner com doc 2278915 future of money infonomics monetizing value in big data information assets mary knox gartner 14 december 2012 http www information age com technology information management 2134803 an introduction to infonomics an introduction to infonomics informationage pete swabey 26 november 2012 https www gartner com doc 2186116 the birth of infonomics the new economics of information gartner research publication douglas laney 3 october 2012 public summary full text available to gartner clients http blogs gartner com doug laney tobins q a evidence of informations real market value 2 tobin s q a evidence of information s real market value gartner blog network douglas laney 14 aug 2012 http www ft com intl cms s 0 27476ad4 a6a5 11e1 968b 00144feabdc0 html axzz1vzocxvyw extracting value from information financial times interview with douglas laney by paul taylor 25 may 2012 free registration required http www forbes com sites gartnergroup 2012 05 22 infonomics the practice of information economics infonomics the practice of information economics forbes by douglas laney 22 may 2012 http blogs wsj com cio 2012 05 03 to facebook youre worth 80 95 mod wsjcio hps cioreport to facebook you re worth 80 95 wall street journal by douglas laney 3 may 2012 https www gartner com doc 1958016 introducing infonomics valuing information as a corporate asset gartner research publication douglas laney 21 march 2012 public summary full text available to gartner clients http www ijikm org volume7 ijikmv7p177 199evans0650 pdf barriers to the effective deployment of information assets an executive management perspective interdisciplinary journal of information knowledge and management nina evans and james price volume 7 2012 older http imcue com wp content uploads 2011 06 what is eim pdf what is enterprise information management eim by john ladley morgan kaufmann 2010 http blogs informatica com perspectives 2010 01 26 data as an asset comment 1072 data as an asset blog series by john schmidt 2010 http www amazon com 2finformation driven business information maximum advantage 2fdp 2f0470625775 2fref 3dsr 1 1 3fie 3dutf8 26s 3dbooks 26qid 3d1267263302 26sr 3d8 1 information driven business how to manage data and information for maximum advantage by rob hillard wiley 2010 http www amazon com 2fhow measure anything intangibles business 2fdp 2f0470539399 2fref 3dsr 1 1 3fs 3dbooks 26ie 3dutf8 26qid 3d1316207185 26sr 3d1 1 how to measure anything finding the value of intangibles in business by douglas w hubbard wiley 2010 http www amazon com 2fintangible assets valuation economic benefit 2fdp 2f0471671312 2fref 3dsr 1 3 3fs 3dbooks 26ie 3dutf8 26qid 3d1316206576 26sr 3d1 3 intangible assets valuation and economic benefit by jeffrey a cohen wiley 2005 http www amazon com 2fvalue driven intellectual capital intangible 2fdp 2f0471351040 2fref 3dsr 1 1 3fs 3dbooks 26ie 3dutf8 26qid 3d1316206774 26sr 3d1 1 value driven intellectual capital how to convert intangible corporate assets into market value by patrick h sullivan wiley 2000 http www vldb org conf 1998 p641 pdf bank of america case study the information currency advantage teradata felipe carino and mark jahnke proceedings of the 24th vldb conference new york ny 1998 http www amazon com information payoff transformation work electronic dp 0029317207 information payoff the transformation of work in the electronic age by paul a strassmann the free press 1985 see also infonomics monetization business intelligence analytics bitcoin data as a service references reflist 33em category articles created via the article wizard category data management'
b'contrast set learning is a form of association rule learning that seeks to identify meaningful differences between separate groups by reverse engineering the key predictors that identify for each particular group for example given a set of attributes for a pool of students labeled by degree type a contrast set learner would identify the contrasting features between students seeking bachelor s degrees and those working toward phd degrees overview a common practice in data mining is to statistical classification classify to look at the attributes of an object or situation and make a guess at what category the observed item belongs to as new evidence is examined typically by feeding a training set to a learning algorithm these guesses are re\xef\xac\x81ned and improved contrast set learning works in the opposite direction while classi\xef\xac\x81ers read a collection of data and collect information that is used to place new data into a series of discrete categories contrast set learning takes the category that an item belongs to and attempts to reverse engineer the statistical evidence that identifies an item as a member of a class that is contrast set learners seek rules associating attribute values with changes to the class distribution ref name bay01 cite journal author1 stephen bay author2 michael pazzani year 2001 title detecting group differences mining contrast sets journal data mining and knowledge discovery volume 5 issue 3 pages 213 246 url http wotan liu edu docis lib musl rclis dbl dmiknd 2001 5 253a3 253c213 253adgdmcs 253e www isle org 252f sbay 252fpapers 252fstucco dmkd pdf ref they seek to identify the key predictors that contrast one classification from another for example an aerospace engineer might record data on test launches of a new rocket measurements would be taken at regular intervals throughout the launch noting factors such as the trajectory of the rocket operating temperatures external pressures and so on if the rocket launch fails after a number of successful tests the engineer could use contrast set learning to distinguish between the successful and failed tests a contrast set learner will produce a set of association rules that when applied will indicate the key predictors of each failed tests versus the successful ones the temperature was too high the wind pressure was too high etc contrast set learning is a form of association rule learning ref name webb03 cite conference author1 gi webb author2 s butler author3 d newlands year 2003 title on detecting differences between groups conference kdd 03 proceedings of the ninth acm sigkdd international conference on knowledge discovery and data mining url http portal acm org citation cfm id 956781 ref association rule learners typically offer rules linking attributes commonly occurring together in a training set for instance people who are enrolled in four year programs and take a full course load tend to also live near campus instead of \xef\xac\x81nding rules that describe the current situation contrast set learners seek rules that differ meaningfully in their distribution across groups and thus can be used as predictors for those groups ref name bay99 cite conference author1 stephen bay author2 michael pazzani year 1999 title detecting change in categorical data mining contrast sets conference kdd 99 proceedings of the fifth acm sigkdd international conference on knowledge discovery and data mining ref for example a contrast set learner could ask what are the key identifiers of a person with a bachelor s degree or a person with a phd and how do people with phd s and bachelor s degrees differ standard classification in machine learning classifier algorithms such as c4 5 have no concept of class importance that is they do not know if a class is good or bad such learners cannot bias or filter their predictions towards certain desired classes as the goal of contrast set learning is to discover meaningful differences between groups it is useful to be able to target the learned rules towards certain classifications several contrast set learners such as minwal ref name cai98 cite conference author1 c h cai author2 a w c fu author3 c h cheng author4 w w kwong year 1998 title mining association rules with weighted items conference proceedings of international database engineering and applications symposium ideas 98 url http appsrv cse cuhk edu hk kdd assoc rule paper chcai pdf ref or the family of tar algorithms ref name hu03 ref name burlet07 cite conference author1 k gundy burlet author2 j schumann author3 t barrett author4 t menzies year 2007 title parametric analysis of antares re entry guidance algorithms using advanced test generation and data analysis conference in 9th international symposium on arti\xef\xac\x81cial intelligence robotics and automation in space ref ref name gay10 cite journal author1 gregory gay author2 tim menzies author3 misty davies author4 karen gundy burlet year 2010 title automatically finding the control variables for complex system behavior journal automated software engineering volume 17 issue 4 url http www greggay com pdf 10tar3 pdf ref assign weights to each class in order to focus the learned theories toward outcomes that are of interest to a particular audience thus contrast set learning can be though of as a form of weighted class learning ref name menzies03 cite journal author1 t menzies author2 y hu year 2003 title data mining for very busy people journal ieee computer volume 36 issue 11 pages 22 29 url http menzies us pdf 03tar2 pdf doi 10 1109 mc 2003 1244531 ref example supermarket purchases the differences between standard classification association rule learning and contrast set learning can be illustrated with a simple supermarket metaphor in the following small dataset each row is a supermarket transaction and each 1 indicates that the item was purchased a 0 indicates that the item was not purchased class wikitable hamburger potatoes foie gras onions champagne purpose of purchases 1 1 0 1 0 cookout 1 1 0 1 0 cookout 0 0 1 0 1 anniversary 1 1 0 1 0 cookout 1 1 0 0 1 frat party given this data association rule learning may discover that customers that buy onions and potatoes together are likely to also purchase hamburger meat classification may discover that customers that bought onions potatoes and hamburger meats were purchasing items for a cookout contrast set learning may discover that the major difference between customers shopping for a cookout and those shopping for an anniversary dinner are that customers acquiring items for a cookout purchase onions potatoes and hamburger meat and do not purchase foie gras or champagne treatment learning file treatmentlearningexample png thumb 200px example of a treatment produced based on data collected while riding a bicycle this treatment states that an optimal riding speed can be obtained while the hill slope is constrained to between \xe2\x88\x9210 and 0 and the cadence is between 1 4 and 2 5 deletable image caption date december 2011 treatment learning is a form of weighted contrast set learning that takes a single desirable group and contrasts it against the remaining undesirable groups the level of desirability is represented by weighted classes ref name hu03 cite book author y hu year 2003 title treatment learning implementation and application type master s thesis publisher department of electrical engineering university of british columbia ref the resulting treatment suggests a set of rules that when applied will lead to the desired outcome treatment learning differs from standard contrast set learning through the following constraints rather than seeking the differences between all groups treatment learning specifies a particular group to focus on applies a weight to this desired grouping and lumps the remaining groups into one undesired category treatment learning has a stated focus on minimal theories in practice treatment are limited to a maximum of four constraints i e rather than stating all of the reasons that a rocket differs from a skateboard a treatment learner will state one to four major differences that predict for rockets at a high level of statistical significance this focus on simplicity is an important goal for treatment learners treatment learning seeks the smallest change that has the greatest impact on the class distribution ref name menzies03 conceptually treatment learners explore all possible subsets of the range of values for all attributes such a search is often infeasible in practice so treatment learning often focuses instead on quickly pruning and ignoring attribute ranges that when applied lead to a class distribution where the desired class is in the minority ref name gay10 example boston housing data the following example demonstrates the output of the treatment learner tar3 on a dataset of housing data from the city of boston a nontrivial public dataset with over 500 examples in this dataset a number of factors are collected for each house and each house is classified according to its quality low medium low medium high and high the desired class is set to high and all other classes are lumped together as undesirable the output of the treatment learner is as follows code baseline class distribution low 29 medlow 29 medhigh 21 high 21 suggested treatment ptratio 12 6 16 rm 6 7 9 78 new class distribution low 0 medlow 0 medhigh 3 high 97 code with no applied treatments rules the desired class represents only 21 of the class distribution however if one filters the data set for houses with 6 7 to 9 78 rooms and a neighborhood parent teacher ratio of 12 6 to 16 then 97 of the remaining examples fall into the desired class high quality houses algorithms there are a number of algorithms that perform contrast set learning the following subsections describe two examples stucco the stucco contrast set learner ref name bay01 ref name bay99 treats the task of learning from contrast sets as a tree traversal tree search problem where the root node of the tree is an empty contrast set children are added by specializing the set with additional items picked through a canonical ordering of attributes to avoid visiting the same nodes twice children are formed by appending terms that follow all existing terms in a given ordering the formed tree is searched in a breadth first manner given the nodes at each level the dataset is scanned and the support is counted for each group each node is then examined to determine if it is significant and large if it should be pruned and if new children should be generated after all significant contrast sets are located a post processor selects a subset to show to the user the low order simpler results are shown first followed by the higher order results which are surprising and significantly different ref name bay99 the support calculation comes from testing a null hypothesis that the contrast set support is equal across all groups i e that contrast set support is independent of group membership the support count for each group is a frequency value that can be analyzed in a contingency table where each row represents the truth value of the contrast set and each column variable indicates the group membership frequency if there is a difference in proportions between the contrast set frequencies and those of the null hypothesis the algorithm must then determine if the differences in proportions represent a relation between variables or if it can be attributed to random causes this can be determined through a chi squared test chi square test comparing the observed frequency count to the expected count nodes are pruned from the tree when all specializations of the node can never lead to a significant and large contrast set the decision to prune is based on the minimum deviation size the maximum difference between the support of any two groups bust be greater than a user specified threshold expected cell frequencies the expected cell frequencies of a contingency table can only decrease as the contrast set is specialized when these frequencies are too small the validity of the chi square test is violated math chi 2 math bounds an upper bound is kept on the distribution of a statistic calculated when the null hypothesis is true nodes are pruned when it is no longer possible to meet this cutoff tar3 the tar3 ref name burlet07 ref name schumann09 cite conference author1 j schumann author2 k gundy burlet author3 c pasareanu author4 t menzies author5 a barrett year 2009 title software v v support by parametric analysis of large software simulation systems conference proceedings of the 2009 ieee aerospace conference ref weighted contrast set learner is based on two fundamental concepts the lift and support of a rule set the lift of a set of rules is the change that some decision makes to a set of examples after imposing that decision i e how the class distribution shifts in response to the imposition of a rule tar3 seeks the smallest set of rules which induces the biggest changes in the sum of the weights attached to each class multiplied by the frequency at which each class occurs the lift is calculated by dividing the score of the set in which the set of rules is imposed by the score of the baseline set i e no rules are applied note that by reversing the lift scoring function the tar3 learner can also select for the remaining classes and reject the target class it is problematic to rely on the lift of a rule set alone incorrect or misleading data noise if correlated with failing examples may result in an overfitted rule set such an overfitted model may have a large lift score but it does not accurately re\xef\xac\x82ect the prevailing conditions within the dataset to avoid overfitting tar3 utilizes a support threshold and rejects all rules that fall on the wrong side of this threshold given a target class the support threshold is a user supplied value usually 0 2 which is compared to the ratio of the frequency of the target class when the rule set has been applied to the frequency of that class in the overall dataset tar3 rejects all sets of rules with support lower than this threshold by requiring both a high lift and a high support value tar3 not only returns ideal rule sets but also favors smaller sets of rules the fewer rules adopted the more evidence that will exist supporting those rules the tar3 algorithm only builds sets of rules from attribute value ranges with a high heuristic value the algorithm determines which ranges to use by \xef\xac\x81rst determining the lift score of each attribute s value ranges these individual scores are then sorted and converted into a cumulative probability distribution tar3 randomly selects values from this distribution meaning that low scoring ranges are unlikely to be selected to build a candidate rule set several ranges are selected and combined these candidate rule sets are then scored and sorted if no improvement is seen after a user defined number of rounds the algorithm terminates and returns the top scoring rule sets references reflist defaultsort contrast set learning category data management category data mining'
b'governance information governance or ig is the set of multi disciplinary structures policies procedures processes and controls implemented to manage information at an enterprise level supporting an organization s immediate and future regulatory legal risk environmental and operational requirements information governance should determine the balance point between two potentially divergent organizational goals extracting value from information and reducing the potential risk of information information governance reduces organizational risk in the fields of compliance operational transparency and reducing expenditures associated with e discovery and litigation response an organization can establish a consistent and logical framework for employees to handle data through their information governance policies and procedures these policies guide proper behavior regarding how organizations and their employees handle electronically stored information electronically stored information federal rules of civil procedure esi ref cite web url http blogs gartner com debra logan 2010 01 11 what is information governance and why is it so hard title what is information governance and why is it so hard debra logan date 11 january 2010 publisher ref ref kooper m maes r and roos lindgreen e 2011 on the governance of information introducing a new concept of governance to support the management of information international journal of information management 31 3 195 200 ref information governance encompasses more than traditional records management it incorporates information security and protection compliance data governance electronic discovery risk management privacy data storage and archiving knowledge management business operations and management audit analytics it management master data management enterprise architecture business intelligence big data data science and finance ref cite web url http iginitiative com igi publishes 2014 annual report title igi publishes 2014 annual report information governance initiative date 11 august 2014 publisher ref history records management records management deals with the creation retention and storage and disposition of records a record can either be a physical tangible object or digital information such as a database application data and e mail the records life cycle lifecycle was historically viewed as the point of creation to the eventual disposal of a record as data generation exploded in recent decades and regulations and compliance issues increased traditional records management failed to keep pace a more comprehensive platform for managing records and information became necessary to address all phases of the lifecycle which led to the advent of information governance ref http www arma org pdf whatisrim pdf ref in 2003 the department of health in england introduced the concept of broad based information governance into the national health service publishing version 1 of an online performance assessment tool with supporting guidance the nhs ig toolkit ref cite web url https www igt hscic gov uk title home publisher ref is now used by over 30 000 nhs and partner organisations supported by an e learning platform with some 650 000 users in 2008 arma international introduced the generally accepted recordkeeping principles\xc2\xae or the principles ref cite web url http www arma org principles title generally accepted recordkeeping principles publisher ref and the subsequent the principles information governance maturity model ref http www arma org principles metrics cfm ref the principles identify the critical hallmarks of information governance as such they apply to all sizes of organizations in all types of industries and in both the private and public sectors multi national organizations can also use the principles to establish consistent practices across a variety of business units arma international recognized that a clear statement of generally accepted recordkeeping principles\xc2\xae the principles would guide ceos in determining how to protect their organizations in the use of information assets legislators in crafting legislation meant to hold organizations accountable and records management professionals in designing comprehensive and effective records management programs information governance goes beyond retention and disposition to include privacy access controls and other compliance issues in electronic discovery or e discovery relevant data in the form of electronically stored information is searched for by attorneys and placed on legal hold ig includes consideration of how this data is held and controlled for e discovery and also provides a platform for defensible disposition and compliance additionally metadata often accompanies electronically stored data and can be of great value to the enterprise if stored and managed correctly with all of these additional considerations that go beyond traditional records management ig emerged as a platform for organizations to define policies at the enterprise level across multiple jurisdictions ig then also provides for the enforcement of these policies into the various repositories of information data and records a coalition of organizations known as electronic discovery reference model edrm which was founded in 2005 to address issues related to electronic discovery and information governance subsequently developed as one of its projects a resource called the information governance reference model igrm ref cite web author edrm url http www edrm net what is edrm title about edrm accessdate 2015 01 21 ref in 2011 edrm in collaboration with arma international published a white paper that describes how the information governance reference model igrm complements arma international s generally accepted recordkeeping principles the principles ref cite book last white paper title how the information governance reference model igrm complements arma international s generally accepted recordkeeping principles year 2011 publisher edrm and arma international pages 15 url http www edrm net wp content uploads downloads 2011 12 white paper edrm information governance reference model igrm and armas garp principles 12 7 2011 pdf editor last ledergerber editor first marcus ref the igrm illustrates the relationship between key stakeholders and the information lifecycle and highlights the transparency required to enable effective governance igrm v3 0 update privacy security officers as stakeholders ref http www edrm net download all projects igrm the final igrm v3 0update whitepaper oct 2012 pdf igrm v3 0 update privacy security officers as stakeholders ref universities and professional associations started to develop information governance training and education programmes in 2010 dr elizabeth lomas who had been aligning rm with information security assurance and risk management models throughout the 2000s authored distance learning materials for information governance modules delivered internationally through northumbria university arma subsequently started to deliver an information governance certification these initiatives have now been picked up by other universities e g san jose state university offers a graduate certificate in information governance information assurance and cyber security and has also incorporated a required course in information governance as part of their 100 online master of archives and records administration ref http ischool sjsu edu programs master archives records administration mara ref mara degree program in 2014 john wiley sons published the first textbook on information governance information governance concepts strategies and best practices ref cite book url http www wiley com wileycda wileytitle productcd 1118218302 html title information governance concepts strategies and best practices isbn 978 1 118 21830 3 date april 2014 publisher john wiley sons ref by robert smallwood also in 2014 the information governance conference ref information governance conference http www infogovcon com infogovcon com ref an annual conference on information governance best practices began and the information governance model ref information governance model http www infogovmodel com infogovmodel com ref was launched at the inaugural event it is now in use at over 1000 organizations worldwide organizational structure in the past records managers owned records management perhaps within a compliance department at an enterprise in order to address the broader issues surrounding records management several other key stakeholders must be involved legal it and compliance tend to be the departments that touch information governance the most though certainly other departments might seek representation many enterprises create information governance committees to ensure that all necessary constituents are represented and that all relevant issues are addressed ref cite web url http www law com jsp cc pubarticlefriendlycc jsp id 1202533945005 title from the experts information governance and its impact on litigation publisher ref tools to address retention and disposition records management and enterprise content management applications were developed sometimes detached search engines or homegrown policy definition tools were created these were often employed at a departmental or divisional level rarely were tools used across the enterprise while these tools were used to define policies they lacked the ability to enforce those policies monitoring for compliance with policies was increasingly challenging since information governance addresses so much more than traditional records management several software solutions have emerged to include the vast array of issues facing records managers other available tools include arma international www arma org nextlevel next level information governance assessment based upon the generally accepted recordkeeping principles arma generally accepted recordkeeping principles ref arma international http www arma org r2 generally accepted br recordkeeping principles the principles arma international ref edrm information governance reference model ref edrm http www edrm net projects igrm information governance reference model edrm ref information coalition information governance model ref information coalition http infocoalition com resources models methodologies information governance model infogovmodel the information governance model information coalition ref nhs information governance toolkit ref nhs https www igt hscic gov uk nhs information governance toolkit nhs ref laws and regulations key to ig are the regulations and laws that help to define corporate policies some of these regulations include the foreign account tax compliance act or foreign account tax compliance act fatca ref cite web url http www irs gov businesses corporations article 0 id 236667 00 html title foreign account tax compliance act publisher ref payment card industry data security standard or payment card industry data security standard pci compliance ref cite web url https www pcisecuritystandards org title official pci security standards council site verify pci compliance download data security and credit card security standards publisher ref health insurance portability and accountability act or health insurance portability and accountability act hipaa ref cite web url http www hhs gov hipaa title health information privacy date 26 august 2015 publisher ref financial services modernization act of 1999 or gramm leach bliley act glba ref cite web url https www congress gov bill 106th congress senate bill 900 title s 900 gramm leach bliley act ref sarbanes oxley act of 2002 or sarbanes oxley sarbox or sox ref cite web url https www sec gov about laws soa2002 pdf title sarbanes oxley act of 2002 ref federal rules of civil procedure guidelines moreq2 ref cite web url http www moreq2 eu title home moreq2 publisher ref moreq2010 ref cite web url http moreq2010 eu title account suspended publisher ref iso 15489 information and documentation records management iso 15489 information and documentation records management ref cite web url http www iso org iso catalogue detail csnumber 31908 title iso 15489 1 2001 information and documentation records management part 1 general publisher ref dod 5015 2 or design criteria standard for electronic records management software applications ref cite web url http www archives gov records mgmt initiatives dod standard 5015 2 html title dod standard 5015 2 publisher ref see also data defined storage data governance electronic discovery enterprise content management information management information technology governance knowledge management national archives records management references reflist 30em external links http www epa gov records what quest1 htm epa 10 reasons for rm http www druva com resources analyst reports governance takes central role enterprises shift to mobile category information governance category information technology management category content management systems category public records category data management'
b'machine readable data is data or metadata which is in a format that can be understood by a computer there are two types human readable data that is markup language marked up so that it can also be read by machines examples microformat s rdfa html or data file formats intended principally for processing by machines resource description framework rdf xml json machine readable is not synonymous with digitally accessible a digitally accessible document may be online making it easier for a human to access it via a computer but unless the relevant data is available in a machine readable format it will be much harder to use the computer to extract transform and process that data ref cite web url https www data gov developers blog primer machine readability online documents and data title a primer on machine readability for online documents and data work data gov date 2012 09 24 accessdate 2015 02 27 ref for purposes of implementation of the government performance and results act gpra modernization act the office of management and budget omb defines machine readable as follows format in a standard computer language not english text that can be read automatically by a web browser or computer system e g xml traditional word processing documents and portable document format pdf files are easily read by humans but typically are difficult for machines to interpret other formats such as extensible markup language xml json or spreadsheets with header columns that can be exported as comma separated values csv are machine readable formats as html is a structural markup language discreetly labeling parts of the document computers are able to gather document components to assemble tables of content outlines literature search bibliographies etc it is possible to make traditional word processing documents and other formats machine readable but the documents must include enhanced structural elements ref http www whitehouse gov sites default files omb assets a11 current year s200 pdf omb circular a 11 part 6 preparation and submission of strategic plans annual performance plans and annual program performance reports ref references reflist see also open data linked data machine readable documents human readable medium http xml fido gov stratml references pl111 532stratml htm sec10 section 10 of the government performance and results act gpra modernization act gprama which requires u s federal agencies to publish their strategic and performance plans and reports in machine readable format like strategy markup language stratml president obama s http xml fido gov stratml carmel eoomrdwstyle xml executive order making open and machine readable the new default for government information http xml fido gov stratml carmel m 13 13wstyle xml 78e85ef4 b91c 11e2 bf2b 79d279ad226c omb m 13 13 open data policy managing information as an asset which requires agencies to use open machine readable data format standards category data management comp stub'
b'no footnotes date march 2009 infobox software operating system microsoft windows genre software framework license proprietary software base class library bcl portion under mit license source code under ms rsl website url http msdn2 microsoft com en us library aa286484 aspx nonprofit ado net is a data access technology from the microsoft net framework which provides communication between relational and non relational systems through a common set of components ado net is a set of computer software components that programmers can use to access data and data services from the database it is a part of the base class library base class library that is included with the microsoft net framework it is commonly used by programmers to access and modify data stored in relational dbms relational database systems though it can also access data in non relational sources ado net is sometimes considered an evolution of activex data objects ado technology but was changed so extensively that it can be considered an entirely new product architecture main ado net data provider image dotnet3 0 svg thumb right 240px this technology forms a part of net framework 3 0 having been part of the framework since version 1 0 ado net is conceptually divided into ado net consumer consumers and ado net provider data providers the consumers are the applications that need access to the data and the providers are the software components that implement the interface and thereby provide the data to the consumer functionality exists in microsoft visual studio visual studio ide to create specialized subclasses of the dataset classes for a particular database schema allowing convenient access to each field through strongly typed property programming properties this helps catch more programming errors at compile time and enhances the ide s intellisense feature o r mapping main object relational mapping entity framework main entity framework entity framework ef is an open source object relational mapping orm framework for ado net part of net framework it is a set of technologies in ado net that support the development of data oriented software applications architects and developers of data oriented applications have typically struggled with the need to achieve two very different objectives the entity framework enables developers to work with data in the form of domain specific objects and properties such as customers and customer addresses without having to concern themselves with the underlying database tables and columns where this data is stored with the entity framework developers can work at a higher level of abstraction when they deal with data and can create and maintain data oriented applications with less code than in traditional applications linq to sql main linq to sql linq to sql formerly called dlinq allows linq to be used to query microsoft sql server databases including sql server compact databases since sql server data may reside on a remote server and because sql server has its own query engine it does not use the query engine of linq instead it converts a linq query to a sql query that is then sent to sql server for processing however since sql server stores the data as relational data and linq works with data encapsulated in objects the two representations must be mapped to one another for this reason linq to sql also defines a mapping framework the mapping is done by defining classes that correspond to the tables in the database and containing all or a certain subset of the columns in the table as data members see also comparison of ado and ado net external links ado net http msdn2 microsoft com en us library aa286484 aspx ado net overview on msdn http msdn2 microsoft com en us library ms973217 aspx ado net for the ado programmer http www devlist com connectionstringspage aspx ado net connection strings net framework microsoft apis windows software stub microsoft stub programming software stub defaultsort ado net category data management category net framework terminology category microsoft application programming interfaces category sql data access category ado net data access technologies'
b'sources date april 2012 dmaic an acronym for define measure analyze improve and control pronounced d\xc9\x99 may ick refers to a data driven improvement cycle used for improving optimizing and stabilizing business processes and designs the dmaic improvement cycle is the core tool used to drive six sigma projects however dmaic is not exclusive to six sigma and can be used as the framework for other improvement applications steps dmaic is an abbreviation of the five improvement steps it comprises define measure analyze improve and control all of the dmaic process steps are required and always proceed in the given order file dmaicwebdingsii png thumbnail right 400px the five steps of dmaic define the purpose of this step is to clearly articulate the business problem goal potential resources project scope and high level project timeline this information is typically captured within project charter document write down what you currently know seek to clarify facts set objectives and form the project team define the following a problem the customer s voice of the customer voc and critical to quality ctqs what are the critical process outputs measure the purpose of this step is to objectively establish current baselines as the basis for improvement this is a data collection step the purpose of which is to establish process performance baselines the performance metric baseline s from the measure phase will be compared to the performance metric at the conclusion of the project to determine objectively whether significant improvement has been made the team decides on what should be measured and how to measure it it is usual for teams to invest a lot of effort into assessing the suitability of the proposed measurement systems good data is at the heart of the dmaic process analyze the purpose of this step is to identify validate and select root cause for elimination a large number of potential root causes process inputs x of the project problem are identified via root cause analysis for example a ishikawa diagram fishbone diagram the top 3 4 potential root causes are selected using multi voting or other consensus tool for further validation a data collection plan is created and data are collected to establish the relative contribution of each root causes to the project metric y this process is repeated until valid root causes can be identified within six sigma often complex analysis tools are used however it is acceptable to use basic tools if these are appropriate of the validated root causes all or some can be list and prioritize potential causes of the problem prioritize the root causes key process inputs to pursue in the improve step identify how the process inputs xs affect the process outputs ys data are analyzed to understand the magnitude of contribution of each root cause x to the project metric y statistical tests using p values accompanied by histograms pareto charts and line plots are often used to do this detailed process maps can be created to help pin point where in the process the root causes reside and what might be contributing to the occurrence improve the purpose of this step is to identify test and implement a solution to the problem in part or in whole this depends on the situation identify creative solutions to eliminate the key root causes in order to fix and prevent process problems use brainstorming or techniques like six thinking hats and random stimulus random word some projects can utilize complex analysis tools like doe design of experiments but try to focus on obvious solutions if these are apparent however the purpose of this step can also be to find solutions without implementing them create focus on the simplest and easiest solutions test solutions using pdca plan do check act pdca cycle based on pdca results attempt to anticipate any avoidable risks associated with the improvement using failure mode and effects analysis fmea create a detailed implementation plan deploy improvements control the purpose of this step is to sustain the gains monitor the improvements to ensure continued and sustainable success create a control plan update documents business process and training records as required a control chart can be useful during the control stage to assess the stability of the improvements over time by serving as 1 a guide to continue monitoring the process and 2 provide a response plan for each of the measures being monitored in case the process becomes unstable replicate and thank the teams this is additional to the standard dmaic steps but it should be considered think about replicating the changes in other processes share your new knowledge within and outside of your organization it is very important to always provide positive morale support to team members in an effort to maximize the effectiveness of dmaic replicating the improvements sharing your success and thanking your team members helps build buy in for future dmaic or improvement initiatives additional steps some organizations add a r ecognize step at the beginning which is to recognize the right problem to work on thus yielding an rdmaic methodology ref name webberwallace2006p43 cite book first1 larry last1 webber first2 michael last2 wallace title quality control for dummies url https books google com books id 9bwkxto2fcec pg pa43 accessdate 2012 05 16 date 15 december 2006 publisher for dummies isbn 978 0 470 06909 7 pages 42 43 ref see also design for six sigma dfss industrial engineering kaizen pdca six sigma references reflist category data management category six sigma'
b'an electronic lab notebook also known as electronic laboratory notebook or eln is a computer program designed to replace paper lab notebook laboratory notebook s lab notebooks in general are used by scientist s engineer s and technician s to document research experiment s and procedures performed in a laboratory a lab notebook is often maintained to be a legal document and may be used in a court of law as evidence law evidence similar to an inventor s notebook the lab notebook is also often referred to in patent prosecution and intellectual property litigation electronic lab notebooks are a fairly new technology and offer many benefits to the user as well as organizations for example electronic lab notebooks are easier to search upon simplify data copying and backups and support collaboration amongst many users ref cite conference title a collaborative electronic notebook first james last myers author2 elena mendoza author3 bonnie hoopes journal proceedings of the iasted international conference on internet and multimedia systems and applications year 2001 ref elns can have fine grained access controls and can be more secure than their paper counterparts ref cite conference last myers first james year 2003 journal proceedings of the 2003 international symposium on collaborative technologies and systems title collaborative electronic notebooks as electronic records design issues for the secure electronic laboratory notebook eln url http collaboratory emsl pnl gov resources publications papers seceln final1 1 22nov pdf ref they also allow the direct incorporation of data from instruments replacing the practice of printing out data to be stapled into a paper notebook ref cite journal last1 perkel first1 j m title coding your way out of a problem journal nature methods volume 8 issue 7 pages 541 543 year 2011 pmid 21716280 doi 10 1038 nmeth 1631 ref types elns can be divided into two categories specific elns contain features designed to work with specific applications scientific instrumentation or data types cross disciplinary elns or generic elns are designed to support access to all data and information that needs to be recorded in a lab notebook solutions range from specialized programs designed from the ground up for use as an eln to modifications or direct use of more general programs examples of using more general software include using openwetware a mediawiki install running the same software that wikipedia uses as an eln or the use of general note taking software such as onenote as an eln ref cite journal last1 perkel first1 j m title coding your way out of a problem journal nature methods volume 8 issue 7 pages 541 543 year 2011 pmid 21716280 doi 10 1038 nmeth 1631 ref eln s come in many different forms they can be standalone programs use a client server model or be entirely web based some use a lab notebook approach others resemble a blog a good many variations on the eln acronym have appeared ref cite web url http cerf notebook com articles eln glossary title lab notebook eln glossary cerf date 2016 02 16 language en us access date 2016 08 20 ref differences between systems with different names are often subtle with considerable functional overlap between them examples include ern electronic research notebook erms electronic resource or research or records management system or software and sdms scientific data or document management system or software ultimately these types of systems all strive to do the same thing capture record centralize and protect scientific data in a way that is highly searchable historically accurate and legally stringent and which also promotes secure collaboration greater efficiency reduced mistakes and lowered total research costs objectives a good electronic laboratory notebook should offer a secure environment to protect the integrity of both data and process whilst also affording the flexibility to adopt new processes or changes to existing processes without recourse to further software development the package architecture should be a modular design so as to offer the benefit of minimizing validation costs of any subsequent changes that you may wish to make in the future as your needs change a good electronic laboratory notebook should be an out of the box solution that as standard has fully configurable forms to comply with the requirements of regulated analytical groups through to a sophisticated eln for inclusion of structures spectra chromatograms pictures text etc where a preconfigured form is less appropriate all data within the system may be stored in a database e g mysql ms sql oracle and be fully searchable the system should enable data to be collected stored and retrieved through any combination of forms or eln that best meets the requirements of the user the application should enable secure forms to be generated that accept laboratory data input via pcs and or laptops palmtops and should be directly linked to electronic devices such as laboratory balances ph meters etc networked or wireless communications should be accommodated for by the package which will allow data to be interrogated tabulated checked approved stored and archived to comply with the latest regulatory guidance and legislation a system should also include a scheduling option for routine procedures such as equipment qualification and study related timelines it should include configurable qualification requirements to automatically verify that instruments have been cleaned and calibrated within a specified time period that reagents have been quality checked and have not expired and that workers are trained and authorized to use the equipment and perform the procedures regulatory and legal aspects the laboratory accreditation criteria found in the iso 17025 standard needs to be considered for the protection and computer backup of electronic records these criteria can be found specifically in clause 4 13 1 4 of the standard ref iso iec 17025 2005 general requirements for the competence of testing and calibration laboratories iso international organization for standardization web 16 nov 2011 http www iso org iso catalogue detail csnumber 39883 ref electronic lab notebooks used for development or research in regulated industries such as medical devices or pharmaceuticals are expected to comply with fda regulations related to software validation the purpose of the regulations is to ensure the integrity of the entries in terms of time authorship and content unlike elns for patent protection fda is not concerned with patent interference proceedings but is concerned with avoidance of falsification typical provisions related to software validation are included in the medical device regulations at 21 cfr 820 et seq ref united states food and drug administration department of health and human resources 1 food and drugs subchapter h medical devices part 820 system regcode of federal regulations title 2ulation fda gov 7 oct 1996 web http www accessdata fda gov scripts cdrh cfdocs cfcfr cfrsearch cfm cfrpart 820 ref and title 21 cfr part 11 ref united states food and drug administration department of health and human resources code of federal regulations title 21 part 11 electronic records electronic signatures fda gov authority 21 u s c 321 393 42 u s c 262 20 mar 1997 web 16 nov 2011 http www accessdata fda gov scripts cdrh cfdocs cfcfr cfrsearch cfm cfrpart 11 ref essentially the requirements are that the software has been designed and implemented to be suitable for its intended purposes evidence to show that this is the case is often provided by a software requirements specification srs setting forth the intended uses and the needs that the eln will meet one or more testing protocols that when followed demonstrate that the eln meets the requirements of the specification and that the requirements are satisfied under worst case conditions security audit trails prevention of unauthorized changes without substantial collusion of otherwise independent personnel i e those having no interest in the content of the eln such as independent quality unit personnel and similar tests are fundamental finally one or more reports demonstrating the results of the testing in accordance with the predefined protocols are required prior to release of the eln software for use if the reports show that the software failed to satisfy any of the srs requirements then corrective and preventive action capa must be undertaken and documented such capa may extend to minor software revisions or changes in architecture or major revisions capa activities need to be documented as well aside from the requirements to follow such steps for regulated industry such an approach is generally a good practice in terms of development and release of any software to assure its quality and fitness for use there are standards related to software development and testing that can be applied see ref see also list of eln software packages data management laboratory informatics scientific management references reflist further reading cite journal last1 taylor first1 k t title the status of electronic laboratory notebooks for chemistry and biology journal current opinion in drug discovery development volume 9 issue 3 pages 348 353 year 2006 pmid 16729731 cite journal last1 rubacha first1 m last2 rattan first2 a k last3 hosselet first3 s c doi 10 1016 j jala 2009 01 002 title a review of electronic laboratory notebooks available in the market today journal journal of laboratory automation volume 16 issue 1 pages 90 98 year 2011 pmid 21609689 pmc defaultsort electronic lab notebook category electronic lab notebook category research category science software category scientific documents category notebooks category electronic documents category data management category content management systems category data management software'
b'refimprove date september 2014 a database server is a computer program that provides database services to other computer programs or to computer s as defined by the client server software modeling model cn date january 2017 the term may also refer to a computer dedicated to running such a program database management system s frequently provide database server functionality and some database management system s dbmss such as mysql rely exclusively on the client server model for database access users access a database server either through a front and back ends front end running on the user s computer which displays requested data or through the front and back ends back end which runs on the server and handles tasks such as data analysis and storage in a master slave technology master slave model database master servers are central and primary locations of data while database slave servers are synchronized backups of the master acting as proxy server proxies most database servers respond to a query language each database understands its query language and converts each submitted query disambiguation query to server readable form and executes it to retrieve results examples of proprietary database servers include oracle database oracle ibm db2 db2 informix and microsoft sql server examples of gnu general public licence database servers include ingres database ingres and mysql every server uses its own query logic and structure the sql structured query language query language is more or less the same on all relational database servers db engines lists over 200 dbmss in its ranking ref cite web url http db engines com en ranking title db engines ranking publisher db engines com date 2013 12 01 accessdate 2013 12 28 ref history the foundations for modeling large sets of data were first introduced by charles bachman in 1969 ref name dbhist http knol google com k databases history early development databases history early development ref bachman introduced data structure diagram data structure diagrams dsds as a means to graphically represent data dsds provided a means to represent the relationships between different data entities in 1970 edgar f codd codd introduced the concept that users of a database should be ignorant of the inner workings of the database ref name dbhist codd proposed the relational view of data which later evolved into the relational model which most databases use today in 1971 the database task report group of codasyl the driving force behind the development of the programming language cobol first proposed a data description language for describing a database a data description language for describing that part of the data base known to a program and a data manipulation language ref name dbhist most of the research and development of databases focused on the relational model during the 1970s in 1975 bachman demonstrated how the relational model and the data structure set were similar and congruent ways of structuring data while working for the honeywell ref name dbhist the entity relationship model was first proposed in its current form by peter chen in 1976 while he was conducting research at mit ref http citeseerx ist psu edu viewdoc summary doi 10 1 1 123 1085 the entity relationship model toward a unified view of data 1976 ref this model became the most frequently used model to describe relational databases chen was able to propose a model that was superior to the navigational model and was more applicable to the real world than the relational model proposed by codd ref name dbhist references reflist see also replication computer science database replication database replication database category data management category servers computing category databases'
b'in computing linked data often capitalized as linked data is a method of publishing structured data so that it can be interlinked and become more useful through semantic query semantic queries it builds upon standard world wide web web technologies such as hypertext transfer protocol http resource description framework rdf and uniform resource identifier uris but rather than using them to serve web pages for human readers it extends them to share information in a way that can be read automatically by computers this enables data from different sources to be connected and queried ref name linkeddatastorysofar cite journal url http tomheath com papers bizer heath berners lee ijswis linked data pdf title linked data mdash the story so far last bizer first christian last2 heath first2 tom last3 berners lee first3 tim author3 link tim berners lee year 2009 accessdate 2010 12 18 doi 10 4018 jswis 2009081901 issn 1552 6283 journal international journal on semantic web and information systems volume 5 issue 3 pages 1 22 solving semantic interoperability conflicts in cross border e government services ref tim berners lee director of the world wide web consortium w3c coined the term in a 2006 design note about the semantic web project ref name designissues cite web url http www w3 org designissues linkeddata html title linked data work design issues author tim berners lee authorlink tim berners lee date 2006 07 27 publisher w3c accessdate 2010 12 18 ref principles tim berners lee outlined four principles of linked data in his linked data note of 2006 ref name designissues paraphrased along the following lines blockquote use uniform resource identifier uris to name identify things use hypertext transfer protocol http uris so that these things can be looked up interpreted dereferenced provide useful information about what a name identifies when it s looked up using open standards such as resource description framework rdf sparql etc refer to other things using their http uri based names when publishing data on the web blockquote tim berners lee gave a presentation on linked data at the ted conference ted 2009 conference ref cite web url http www ted com talks tim berners lee on the next web html title tim berners lee on the next web ref in it he restated the linked data principles as three extremely simple rules blockquote all kinds of conceptual things they have names now that start with http if i take one of these http names and i look it up i will get back some data in a standard format which is kind of useful data that somebody might like to know about that thing about that event when i get back that information it s not just got somebody s height and weight and when they were born its got relationships and when it has relationships whenever it expresses a relationship then the other thing that it s related to is given one of those names that starts with http blockquote components uniform resource identifier uri s http structured data using controlled vocabulary terms and dataset definitions expressed in resource description framework serialization formats such as rdfa rdf xml notation 3 n3 turtle syntax turtle or json ld linked data platform linked open data linked open data is linked data that is open content ref cite web url http linkeddata org faq title frequently asked questions faqs linked data connect distributed data across the web publisher ref ref cite web url https www coar repositories org activities repository observatory second edition linked open data 7 things you should know about open data title coar \xc2\xbb 7 things you should know about\xe2\x80\xa6linked data publisher ref ref cite web url http openorg ecs soton ac uk wiki linked data basics for techies open linked data title linked data basics for techies publisher ref tim berners lee gives the clearest definition of linked open data in differentiation with linked data quote text linked open data lod is linked data which is released under an open licence which does not impede its reuse for free author tim berners lee title linked data ref name designissues ref cite web url http 5stardata info en title 5 star open data ref large linked open data sets include dbpedia and freebase history the term linked open data has been in use since at least february 2007 when the linking open data mailing list ref cite web url http lists w3 org archives public public lod title public lod w3 org mail archives publisher ref was created ref cite web url http www w3 org wiki sweoig taskforces communityprojects linkingopendata newsarchive title sweoig taskforces communityprojects linkingopendata newsarchive publisher ref the mailing list was initially hosted by the simile project ref cite web url http simile mit edu mail html title simile project mailing lists publisher ref at the massachusetts institute of technology linking open data community project file lod cloud 2014 svg thumb 400px the above diagram shows which linking open data datasets are connected as of august 2014 this was produced by the linked open data cloud project which was started in 2007 some sets may include copyrighted data which is freely available ref linking open data cloud diagram 2014 by max schmachtenberg christian bizer anja jentzsch and richard cyganiak http lod cloud net ref the goal of the w3c semantic web education and outreach group s linking open data community project is to extend the web with a knowledge commons data commons by publishing various open knowledge open dataset s as rdf on the web and by setting resource description framework rdf links between data items from different data sources in october 2007 datasets consisted of over two billion rdf triples which were interlinked by over two million rdf links ref http esw w3 org topic sweoig taskforces communityprojects linkingopendata linking open data ref ref cite book last1 fensel first1 dieter last2 facca first2 federico michele last3 simperl first3 elena last4 ioan first4 toma title semantic web services year 2011 publisher springer isbn 3642191924 pages 99 ref by september 2011 this had grown to 31 billion rdf triples interlinked by around 504 million rdf links a detailed statistical breakdown was published in 2014 ref http linkeddatacatalog dws informatik uni mannheim de state ref european union projects there are a number of european union projects definition date june 2013 involving linked data these include the linked open data around the clock latc project ref http latc project eu linked open data around the clock latc ref the planetdata project ref http planet data eu planetdata ref the dapaas data and platform as a service project ref http project dapaas eu dapaas ref and the linked open data 2 lod2 project ref http lod2 eu linking open data 2 lod2 ref ref cite web url http cordis europa eu fetch caller proj ict action d cat proj rcn 95562 publisher european commission title cordis fp7 ict projects lod2 date 2010 04 20 ref ref cite web url http static lod2 eu deliverables lod2 d12 5 1 project fact sheet version pdf title lod2 project fact sheet project summary date 2010 09 01 accessdate 2010 12 18 ref data linking is one of the main goals of the eu open data portal which makes available thousands of datasets for anyone to reuse and link datasets dbpedia a dataset containing extracted data from wikipedia it contains about 3 4 million concepts described by 1 billion semantic triple triples including abstracts in 11 different languages foaf software foaf a dataset describing persons their properties and relationships geonames provides rdf descriptions of more than formatnum 7500000 geographical features worldwide umbel a lightweight reference structure of formatnum 20000 subject concept classes and their relationships derived from opencyc which can act as binding classes to external data also has links to 1 5 million named entities from dbpedia and yago ontology yago wikidata a collaboratively created linked dataset that acts as central storage for the structured data of its wikimedia sister projects dataset instance and class relationships clickable diagrams that show the individual datasets and their relationships within the dbpedia spawned lod cloud as shown by the figures to the right are available ref http www4 wiwiss fu berlin de bizer pub lod datasets 2009 07 14 html instance relationships amongst datasets ref ref http web archive org web 20110828103804 http umbel org sites umbel org lod lod constellation html class relationships amongst datasets ref see also authority control about controlled headings in library catalogs citation analysis for citations between scholarly articles hyperdata linked data page network model an older type of database management system schema org web ontology language references reflist 30em further reading ref begin 2 ahmet soylu felix m\xc3\xb6dritscher and patrick de causmaecker 2012 http www ahmetsoylu com wp content uploads 2013 10 soylu icae2012 pdf ubiquitous web navigation through harvesting embedded semantic data a mobile scenario integrated computer aided engineering 19 1 93 109 http linkeddatabook com book linked data evolving the web into a global data space 2011 by tom heath and christian bizer synthesis lectures on the semantic web theory and technology morgan claypool note this resources supersedes the tutorial http www4 wiwiss fu berlin de bizer pub linkeddatatutorial how to publish linked data on the web by bizer cyganiak and heath http wifo5 03 informatik uni mannheim de bizer pub linkeddatatutorial how to publish linked data on the web by chris bizer richard cyganiak and tom heath linked data tutorial at freie universit\xc3\xa4t berlin germany 27 july 2007 http www scientificamerican com article cfm id berners lee linked data the web turns 20 linked data gives people power part 1 of 4 by mark fischetti scientific american 2010 october 23 http knoesis wright edu library publications linkedai2010 submission 13 pdf linked data is merely more data prateek jain pascal hitzler peter z yeh kunal verma and amit p sheth in dan brickley vinay k chaudhri harry halpin and deborah mcguinness linked data meets artificial intelligence technical report ss 10 07 aaai press menlo park california 2010 pp nbsp 82 86 http knoesis org library resource php id 1718 moving beyond sameas with plato partonomy detection for linked data prateek jain pascal hitzler kunal verma peter z yeh amit sheth in proceedings of the 23rd acm hypertext and social media conference ht 2012 milwaukee wi usa june 25 28 2012 freitas andr\xc3\xa9 edward curry jo\xc3\xa3o gabriel oliveira and sean o riain 2012 http www edwardcurry org publications freitas ic 12 pdf querying heterogeneous datasets on the linked data web challenges approaches and trends ieee internet computing 16 1 24 33 http www2008 org papers pdf p1265 bizer pdf linked data on the web chris bizer tom heath kingsley uyi idehen tim berners lee in proceedings www2008 beijing china http sites wiwiss fu berlin de suhl bizer pub linkingopendata pdf interlinking open data on the web chris bizer tom heath danny ayers yves raimond in proceedings poster track eswc2007 innsbruck austria http knoesis wright edu library publications iswc10 paper218 pdf ontology alignment for linked open data prateek jain pascal hitzler amit sheth kunal verma peter z yeh in proceedings of the 9th international semantic web conference iswc 2010 shanghai china http www ncbi nlm nih gov pmc articles pmc3121711 linked open drug data for pharmaceutical research and development j cheminform 2011 3 19 samwald jentzsch bouton kalles\xc3\xb8e willighagen hajagos marshall prud hommeaux hassenzadeh pichler and stephens may 2011 http www community of knowledge de beitrag the hype the hope and the lod2 soeren auer engaged in the next generation lod interview with s\xc3\xb6ren auer head of the lod2 project about the continuation of lod2 in 2011 june 2011 http www semantic web at lod theessentials pdf linked open data the essentials florian bauer and martin kaltenb\xc3\xb6ck january 2012 http semanticweb com the flap of a butterfly wing b26808 the flap of a butterfly wing semanticweb com richard wallis february 2012 ref end external links http www w3 org wiki linkeddata linkeddata at the w3c wiki http linkeddata org linkeddata org http virtuoso openlinksw com white papers openlink software white papers http demo openlinksw com demo customers customerid alfki 23this data from northwind sql schema as linked data use case demo http nomisma org linked data for the discipline of numismatics use case demo http en lodlive it interactive lod demo http americanartcollaborative org american art collaborative consortium of us art museums committed to establishing a critical mass of linked open data on american art semantic web open data navbox authority control category cloud standards category data management category distributed computing architecture category hypermedia category internet terminology category open data category world wide web category semantic web'
b'the national information governance board for health and social care nigb was established in the united kingdom under section 157 ref http www legislation gov uk ukpga 2008 14 section 157 section 157 of the health and social care act 2008 ref of the health and social care act 2008 with effect from october 2008 with a range of advisory functions relating to information governance from january 2009 the nigb also gained functions under section 251 ref http www legislation gov uk ukpga 2006 41 section 251 section 251 of the nhs act 2006 ref of the nhs act 2006 which had previously been held by the patient information advisory group piag until its abolition these functions were to advise the secretary of state for health on the use of powers to set aside the common law duty of confidentiality in england where identifiable patient information is needed and where consent is not practicable from 1 april 2013 the nigb s functions for monitoring and improving information governance practice have transferred to the care quality commission which established a national information governance committee to oversee this work functions relating to section 251 of the national health service act 2006 nhs act 2006 access to people s personal and confidential information for research purposes were transferred to the health research authority s confidentiality advisory group ref http webarchive nationalarchives gov uk 20130513181011 http www nigb nhs uk webarchive page of the national information governance board for health and social care ref terms of reference the key functions of the nigb excerpted from the legislation were ol type a li to monitor the practice followed by relevant bodies in relation to the processing of relevant information li li to keep the secretary of state for health and such bodies as the secretary of state for health may designate by direction informed about the practice being followed by relevant bodies in relation to the processing of relevant information li li to publish guidance on the practice to be followed in relation to the processing of relevant information li li to advise the secretary of state for health on particular matters relating to the processing of relevant information by any person and li li to advise persons who process relevant information on such matters relating to the processing of relevant information by them as the secretary of state for health may from time to time designate by direction li ol the definition of relevant information in the legislation covers patient information any other information obtained or generated in the course of the provision of the health service and any information obtained or generated in the course of the exercise by a local social services authority in england of its adult social services functions ethics and confidentiality committee some areas of nigb functions d and e above had been delegated to the nigb s ethics and confidentiality committee ecc these functions primarily related to applications to use identifiable patient information without consent in specific circumstances within the bounds of section 251 of the nhs act 2006 these applications which had been considered by piag before the nigb passed on to the health research authority s confidentiality advisory group cag on 1 april 2013 care record development board the nigb had also replaced the care record development board crdb ref http www connectingforhealth nhs uk crdb care record development board archive page hosted by nhs connecting for health ref which had closed in september 2007 the nigb had subsequently maintained the nhs care record guarantee which was originally developed by the crdb and developed a companion social care record guarantee members the nigb had consisted of a chair a number of public members appointed by the nhs appointments commission and a number of representative members appointed by the secretary of state for health from a range of stakeholder organisations representatives of several other stakeholder organisations had served as corresponding advisers to the nigb but had not typically attended meetings regular observers at meetings had included representatives from the information commissioner s office and the devolved uk administrations the ecc had consisted of a chair and a number of members all of whom had been appointed by the nigb with advice from an nhs appointments commission approved independent assessor the ecc chair and two ecc members had also been nigb members between 1 june 2011 and 31 march 2013 dame fiona caldicott ref http www connectingforhealth nhs uk newsroom news nigbchair appointment of fiona caldicott as new nigb chair press release on nhs connecting for health website june 2011 ref had been chair of the nigb succeeding harry cayton who had chaired the nigb since its inception geography members of the nigb and ecc had been widely distributed nationally but had attended meetings at the nigb office since september 2011 this had been based at skipton house london se1 the nigb s staff team had been predominantly based at this office abolition as a result of the health and social care act 2012 the nigb was abolished with effect from 1 april 2013 the functions delegated to the ecc with respect to research transferred to the health research authority ref http www hra nhs uk news 2012 12 17 further update on transfer of s251 function from nigb to hra transfer of s251 function from nigb to hra ref the nhs commissioning board is now responsible for providing advice and guidance to nhs bodies other functions were transferred to the national information governance committee hosted by the care quality commission references reflist external links http webarchive nationalarchives gov uk 20130513181011 http www nigb nhs uk nigb website archived http www hra nhs uk nhs health research authority http www cqc org uk care quality commission http www nres nhs uk national research ethics service http www commissioningboard nhs uk nhs commissioning board category data management category medical privacy category 2008 establishments in the united kingdom category organizations established in 2008 category governance in the united kingdom category social care in the united kingdom'
b'multiple issues refimprove date february 2008 citation style date september 2013 file data warehouse overview jpg thumb 200px data warehouse overview in computing a data warehouse dw or dwh also known as an enterprise data warehouse edw is a system used for business reporting reporting and data analysis and is considered a core component of business intelligence ref dedi\xc4\x87 n and stanier c 2016 an evaluation of the challenges of multilingualism in data warehouse development in 18th international conference on enterprise information systems iceis 2016 p 196 ref dws are central repositories of integrated data from one or more disparate sources they store current and historical data and are used for creating analytical reports for knowledge workers throughout the enterprise examples of reports could range from annual and quarterly comparisons and trends to detailed daily sales analysis the data stored in the warehouse is upload ed from the operational system s such as marketing or sales the data may pass through an operational data store for additional operations before it is used in the dw for reporting types of systems data mart a data mart is a simple form of a data warehouse that is focused on a single subject or functional area hence they draw data from a limited number of sources such as sales finance or marketing data marts are often built and controlled by a single department within an organization the sources could be internal operational systems a central data warehouse or external data ref cite web url http docs oracle com html e10312 01 dm concepts htm title data mart concepts publisher oracle year 2007 ref denormalization is the norm for data modeling techniques in this system given that data marts generally cover only a subset of the data contained in a data warehouse they are often easier and faster to implement class wikitable difference between data warehouse and nowrap data mart data warehouse data mart enterprise wide data department wide data multiple subject areas single subject area difficult to build easy to build takes more time to build less time to build larger memory limited memory types of data marts dependent data mart independent data mart hybrid data mart online analytical processing olap olap is characterized by a relatively low volume of transactions queries are often very complex and involve aggregations for olap systems response time is an effectiveness measure olap applications are widely used by data mining techniques olap databases store aggregated historical data in multi dimensional schemas usually star schemas olap systems typically have data latency of a few hours as opposed to data marts where latency is expected to be closer to one day the olap approach is used to analyze multidimensional data from multiple sources and perspectives the three basic operations in olap are roll up consolidation drill down and slicing dicing ref name dwh cite web url https intellipaat com tutorial data warehouse tutorial title data warehousing tutorial for beginners publisher intellipaat ref online transaction processing oltp oltp is characterized by a large number of short on line transactions insert update delete oltp systems emphasize very fast query processing and maintaining data integrity in multi access environments for oltp systems effectiveness is measured by the number of transactions per second oltp databases contain detailed and current data the schema used to store transactional databases is the entity model usually third normal form 3nf ref cite web url http datawarehouse4u info oltp vs olap html title oltp vs olap year 2009 website datawarehouse4u info quote we can divide it systems into transactional oltp and analytical olap in general we can assume that oltp systems provide source data to data warehouses whereas olap systems help to analyze it ref normalization is the norm for data modeling techniques in this system predictive analysis predictive analysis is about pattern recognition finding and quantifying hidden patterns in the data using complex mathematical models that can be used to prediction predict future outcomes predictive analysis is different from olap in that olap focuses on historical data analysis and is reactive in nature while predictive analysis focuses on the future these systems are also used for crm customer relationship management software tools the typical extract transform load extract transform load etl based data warehouse uses staging data staging data integration and access layers to house its key functions the staging layer or staging database stores raw data extracted from each of the disparate source data systems the integration layer integrates the disparate data sets by transforming the data from the staging layer often storing this transformed data in an operational data store ods database the integrated data are then moved to yet another database often called the data warehouse database where the data is arranged into hierarchical groups often called dimensions and into facts and aggregate facts the combination of facts and dimensions is sometimes called a star schema the access layer helps users retrieve data ref name ijca96patil cite journal url http www ijcaonline org proceedings icwet number9 2131 db195 author1 patil preeti s author2 srikantha rao author3 suryakant b patil title optimization of data warehousing system simplification in reporting and analysis work ijca proceedings on international conference and workshop on emerging trends in technology icwet year 2011 volume 9 issue 6 pages 33 37 publisher foundation of computer science ref this definition of the data warehouse focuses on data storage the main source of the data is cleaned transformed catalogued and made available for use by managers and other business professionals for data mining olap online analytical processing market research and decision support ref marakas o brien 2009 ref however the means to retrieve and analyze data to extract transform load extract transform and load data and to manage the data dictionary are also considered essential components of a data warehousing system many references to data warehousing use this broader context thus an expanded definition for data warehousing includes business intelligence tools tools to extract transform and load data into the repository and tools to manage and retrieve metadata benefits a data warehouse maintains a copy of information from the source transaction systems this architectural complexity provides the opportunity to integrate data from multiple sources into a single database and data model mere congregation of data to single database so a single query engine can be used to present data is an ods mitigate the problem of database isolation level lock contention in transaction processing systems caused by attempts to run large long running analysis queries in transaction processing databases maintain provenance data provenance data history even if the source transaction systems do not integrate data from multiple source systems enabling a central view across the enterprise this benefit is always valuable but particularly so when the organization has grown by merger improve data quality by providing consistent codes and descriptions flagging or even fixing bad data present the organization s information consistently provide a single common data model for all data of interest regardless of the data s source restructure the data so that it makes sense to the business users restructure the data so that it delivers excellent query performance even for complex analytic queries without impacting the operational system s add value to operational business applications notably customer relationship management crm systems make decision support queries easier to write optimized data warehouse architectures allow data scientists to organize and disambiguate repetitive data ref cite web url https www idera com resourcecentral whitepapers modern data architecture title modern data architecture idera last first date website www idera com publisher access date 2016 09 18 ref generic environment the environment for data warehouses and marts includes the following source systems that provide data to the warehouse or mart data integration technology and processes that are needed to prepare the data for use different architectures for storing data in an organization s data warehouse or data marts different tools and applications for the variety of users metadata data quality and governance processes must be in place to ensure that the warehouse or mart meets its purposes in regards to source systems listed above rainer clarify reason who is rainer date december 2014 states a common source for the data in data warehouses is the company s operational databases which can be relational databases ref name rainer2012 cite book last rainer first r kelly title introduction to information systems enabling and transforming business 4th edition kindle edition date 2012 05 01 publisher wiley pages 127 128 130 131 133 ref regarding data integration rainer states it is necessary to extract data from source systems transform them and load them into a data mart or warehouse ref name rainer2012 rainer discusses storing data in an organization s data warehouse or data marts ref name rainer2012 metadata are data about data it personnel need information about data sources database table and column names refresh schedules and data usage measures ref name rainer2012 today the most successful companies are those that can respond quickly and flexibly to market changes and opportunities a key to this response is the effective and efficient use of data and information by analysts and managers ref name rainer2012 a data warehouse is a repository of historical data that are organized by subject to support decision makers in the organization ref name rainer2012 once data are stored in a data mart or warehouse they can be accessed history the concept of data warehousing dates back to the late 1980s ref cite web url http www computerworld com databasetopics data story 0 10801 70102 00 html title the story so far date 2002 04 15 accessdate 2008 09 21 deadurl yes archiveurl https web archive org web 20080708182105 http www computerworld com databasetopics data story 0 10801 70102 00 html archivedate 2008 07 08 df ref when ibm researchers barry devlin and paul murphy developed the business data warehouse in essence the data warehousing concept was intended to provide an architectural model for the flow of data from operational systems to decision support system decision support environments the concept attempted to address the various problems associated with this flow mainly the high costs associated with it in the absence of a data warehousing architecture an enormous amount of redundancy was required to support multiple decision support environments in larger corporations it was typical for multiple decision support environments to operate independently though each environment served different users they often required much of the same stored data the process of gathering cleaning and integrating data from various sources usually from long term existing operational systems usually referred to as legacy system s was typically in part replicated for each environment moreover the operational systems were frequently reexamined as new decision support requirements emerged often new requirements necessitated gathering cleaning and integrating new data from data mart s that were tailored for ready access by users key developments in early years of data warehousing were 1960s general mills and dartmouth college in a joint research project develop the terms dimensions and facts ref name kimball16 kimball 2002 pg 16 ref 1970s acnielsen and iri provide dimensional data marts for retail sales ref name kimball16 1970s bill inmon begins to define and discuss the term data warehouse citation needed date june 2014 1975 sperry univac introduces mapper maintain prepare and produce executive reports is a database management and reporting system that includes the world s first fourth generation programming language 4gl first platform designed for building information centers a forerunner of contemporary enterprise data warehousing platforms 1983 teradata introduces a database management system specifically designed for decision support 1984 metaphor computer systems founded by david liddle and don massaro releases data interpretation system dis dis was a hardware software package and gui for business users to create a database management and analytic system 1988 barry devlin and paul murphy publish the article an architecture for a business and information system where they introduce the term business data warehouse ref cite journal url http ieeexplore ieee org stamp stamp jsp tp arnumber 5387658 title an architecture for a business and information system journal ibm systems journal doi 10 1147 sj 271 0060 volume 27 pages 60 80 ref 1990 red brick systems founded by ralph kimball introduces red brick warehouse a database management system specifically for data warehousing 1991 prism solutions founded by bill inmon introduces prism warehouse manager software for developing a data warehouse 1992 bill inmon publishes the book building the data warehouse ref cite book last inmon first bill title building the data warehouse year 1992 publisher wiley isbn 0 471 56960 7 ref 1995 the data warehousing institute a for profit organization that promotes data warehousing is founded 1996 ralph kimball publishes the book the data warehouse toolkit ref name 0 cite book title the data warehouse toolkit last kimball first ralph publisher wiley year 2011 isbn 9780470149775 page 237 ref 2012 bill inmon developed and made public technology known as textual disambiguation textual disambiguation applies context to raw text and reformats the raw text and context into a standard data base format once raw text is passed through textual disambiguation it can easily and efficiently be accessed and analyzed by standard business intelligence technology textual disambiguation is accomplished through the execution of textual etl textual disambiguation is useful wherever raw text is found such as in documents hadoop email and so forth information storage facts a fact is a value or measurement which represents a fact about the managed entity or system facts as reported by the reporting entity are said to be at raw level e g in a mobile telephone system if a bts base transceiver station received 1 000 requests for traffic channel allocation it allocates for 820 and rejects the remaining then it would report 3 facts or measurements to a management system tch req total 1000 tch req success 820 tch req fail 180 facts at the raw level are further aggregated to higher levels in various dimension data warehouse dimensions to extract more service or business relevant information from it these are called aggregates or summaries or aggregated facts for instance if there are 3 btss in a city then the facts above can be aggregated from the bts to the city level in the network dimension for example math tch req success city tch req success bts1 tch req success bts2 tch req success bts3 math math avg tch req success city tch req success bts1 tch req success bts2 tch req success bts3 3 math dimensional versus normalized approach for storage of data there are three or more leading approaches to storing data in a data warehouse nbsp the most important approaches are the dimensional approach and the normalized approach the dimensional approach refers to ralph kimball s approach in which it is stated that the data warehouse should be modeled using a dimensional model star schema the normalized approach also called the third normal form 3nf model third normal form refers to bill inmon s approach in which it is stated that the data warehouse should be modeled using an e r model normalized model in a star schema dimensional approach transaction data are partitioned into facts which are generally numeric transaction data and dimension data warehouse dimensions which are the reference information that gives context to the facts for example a sales transaction can be broken up into facts such as the number of products ordered and the total price paid for the products and into dimensions such as order date customer name product number order ship to and bill to locations and salesperson responsible for receiving the order a key advantage of a dimensional approach is that the data warehouse is easier for the user to understand and to use also the retrieval of data from the data warehouse tends to operate very quickly ref name 0 dimensional structures are easy to understand for business users because the structure is divided into measurements facts and context dimensions facts are related to the organization s business processes and operational system whereas the dimensions surrounding them contain context about the measurement kimball ralph 2008 another advantage offered by dimensional model is that it does not involve a relational database every time thus this type of modeling technique is very useful for end user queries in data warehouse ref name dwh the main disadvantages of the dimensional approach are the following in order to maintain the integrity of facts and dimensions loading the data warehouse with data from different operational systems is complicated it is difficult to modify the data warehouse structure if the organization adopting the dimensional approach changes the way in which it does business in the normalized approach the data in the data warehouse are stored following to a degree database normalization rules tables are grouped together by subject areas that reflect general data categories e g data on customers products finance etc the normalized structure divides data into entities which creates several tables in a relational database when applied in large enterprises the result is dozens of tables that are linked together by a web of joins furthermore each of the created entities is converted into separate physical tables when the database is implemented kimball ralph 2008 citation needed date november 2013 the main advantage of this approach is that it is straightforward to add information into the database some disadvantages of this approach are that because of the number of tables involved it can be difficult for users to join data from different sources into meaningful information and to access the information without a precise understanding of the sources of data and of the data structure of the data warehouse both normalized and dimensional models can be represented in entity relationship diagrams as both contain joined relational tables the difference between the two models is the degree of normalization also known as database normalization normal forms normal forms these approaches are not mutually exclusive and there are other approaches dimensional approaches can involve normalizing data to a degree kimball ralph 2008 in information driven business ref cite book last hillard first robert title information driven business year 2010 publisher wiley isbn 978 0 470 62577 4 ref robert hillard proposes an approach to comparing the two approaches based on the information needs of the business problem the technique shows that normalized models hold far more information than their dimensional equivalents even when the same fields are used in both models but this extra information comes at the cost of usability the technique measures information quantity in terms of entropy information theory information entropy and usability in terms of the small worlds data transformation measure ref cite web url http mike2 openmethodology org wiki small worlds data transformation measure title information theory business intelligence strategy small worlds data transformation measure mike2 0 the open source methodology for information development publisher mike2 openmethodology org date accessdate 2013 06 14 ref design methods refimprove section date july 2015 bottom up design in the bottom up approach data mart s are first created to provide reporting and analytical capabilities for specific business process es these data marts can then be integrated to create a comprehensive data warehouse the data warehouse bus architecture is primarily an implementation of the bus a collection of dimension data warehouse types conformed dimension s and facts data warehouse types conformed fact s which are dimensions that are shared in a specific way between facts in two or more data marts ref cite web url http decisionworks com 2003 09 the bottom up misnomer title the bottom up misnomer decisionworks consulting website decisionworks consulting language en us access date 2016 03 06 ref top down design the top down approach is designed using a normalized enterprise data model data element atomic data that is data at the greatest level of detail are stored in the data warehouse dimensional data marts containing data needed for specific business processes or specific departments are created from the data warehouse ref name referencea gartner of data warehouses operational data stores data marts and data outhouses dec 2005 ref hybrid design data warehouses dw often resemble the hub and spokes architecture legacy system s feeding the warehouse often include customer relationship management and enterprise resource planning generating large amounts of data to consolidate these various data models and facilitate the extract transform load process data warehouses often make use of an operational data store the information from which is parsed into the actual dw to reduce data redundancy larger systems often store the data in a normalized way data marts for specific reports can then be built on top of the dw the dw database in a hybrid solution is kept on third normal form to eliminate data redundancy a normal relational database however is not efficient for business intelligence reports where dimensional modelling is prevalent small data marts can shop for data from the consolidated warehouse and use the filtered specific data for the fact tables and dimensions required the dw provides a single source of information from which the data marts can read providing a wide range of business information the hybrid architecture allows a dw to be replaced with a master data management solution where operational not static information could reside the data vault modeling components follow hub and spokes architecture this modeling style is a hybrid design consisting of the best practices from both third normal form and star schema the data vault model is not a true third normal form and breaks some of its rules but it is a top down architecture with a bottom up design the data vault model is geared to be strictly a data warehouse it is not geared to be end user accessible which when built still requires the use of a data mart or star schema based release area for business purposes versus operational system operational systems are optimized for preservation of data integrity and speed of recording of business transactions through use of database normalization and an entity relationship model operational system designers generally follow the codd s 12 rules codd rules of database normalization in order to ensure data integrity codd defined five increasingly stringent rules of normalization fully normalized database designs that is those satisfying all five codd rules often result in information from a business transaction being stored in dozens to hundreds of tables relational database s are efficient at managing the relationships between these tables the databases have very fast insert update performance because only a small amount of data in those tables is affected each time a transaction is processed finally in order to improve performance older data are usually periodically purged from operational systems data warehouses are optimized for analytic access patterns analytic access patterns generally involve selecting specific fields and rarely if ever select as is more common in operational databases because of these differences in access patterns operational databases loosely oltp benefit from the use of a row oriented dbms whereas analytics databases loosely olap benefit from the use of a column oriented dbms unlike operational systems which maintain a snapshot of the business data warehouses generally maintain an infinite history which is implemented through etl processes that periodically migrate data from the operational systems over to the data warehouse evolution in organization use these terms refer to the level of sophistication of a data warehouse offline operational data warehouse data warehouses in this stage of evolution are updated on a regular time cycle usually daily weekly or monthly from the operational systems and the data is stored in an integrated reporting oriented data offline data warehouse data warehouses at this stage are updated from data in the operational systems on a regular basis and the data warehouse data are stored in a data structure designed to facilitate reporting on time data warehouse online integrated data warehousing represent the real time data warehouses stage data in the warehouse is updated for every transaction performed on the source data integrated data warehouse these data warehouses assemble data from different areas of business so users can look up the information they need across other systems ref cite web url http www tech faq com data warehouse html title data warehouse ref see also colbegin 3 accounting intelligence anchor modeling business intelligence business intelligence tools data integration data mart data mining data presentation architecture data scraping data warehouse appliance database management system decision support system data vault modeling executive information system extract transform load master data management online analytical processing online transaction processing operational data store semantic warehousing snowflake schema software as a service star schema slowly changing dimension data warehouse automation colend references reflist 30em refs ref name ahsan cite journal last1 abdullah first1 ahsan title analysis of mealybug incidence on the cotton crop using adss olap online analytical processing tool volume 69 issue 1 journal computers and electronics in agriculture year 2009 pages 59 72 doi 10 1016 j compag 2009 07 003 volume 69 ref further reading davenport thomas h and harris jeanne g competing on analytics the new science of winning 2007 harvard business school press isbn 978 1 4221 0332 6 ganczarski joe data warehouse implementations critical implementation factors study 2009 vdm verlag isbn 3 639 18589 7 isbn 978 3 639 18589 8 kimball ralph and ross margy the data warehouse toolkit third edition 2013 wiley isbn 978 1 118 53080 1 linstedt graziano hultgren the business of data vault modeling second edition 2010 dan linstedt isbn 978 1 4357 1914 9 william inmon building the data warehouse 2005 john wiley and sons isbn 978 8 1265 0645 3 external links http www kimballgroup com html articles html ralph kimball articles http www ijcaonline org archives number3 77 172 international journal of computer applications http dwreview com dw overview html data warehouse introduction data data warehouse authority control defaultsort data warehouse category business intelligence category data management category data warehousing category information technology management'
b'file admsmodelv1 png thumb 300px adms uml model version 1 00 the asset description metadata schema adms is a common metadata vocabulary to describe standards so called interoperability assets on the web used in concert with web syndication web syndication technology adms helps people make sense of the complex multi publisher environment around standards and in particular the ones which are semantic assets such as ontologies data model s data dictionary data dictionaries code lists xml and resource description framework rdf schemas in spite of their importance standards are not easily discoverable on the web via search engines because metadata about them is seldom available navigating on the websites of the different publishers of standards is not efficient either key terminology a semantic asset is a specific type of standard which involves highly reusable metadata e g xml schemata generic data models and or reference data e g code lists taxonomies dictionaries vocabularies organisations use semantic assets to share information and knowledge within themselves and with others semantic assets are usually very valuable and reusable elements for the development of information systems in particular as part of machine to machine interfaces as enablers to interoperable information exchange semantic assets are usually created published and maintained by standardisation bodies nonetheless ict projects and groups of experts also create such assets there are therefore many publishers of semantic assets with different degrees of formalism what is adms adms ref name adms http joinup ec europa eu asset adms home adms homepage on joinup ref is a standardised metadata vocabulary created by the european union eu s interoperability solutions for european public administrations isa programme ref name isa http ec europa eu isa interoperability solutions for european public administrations isa programme ref of the european commission to help publishers of standards document what their standards are about their name their status theme version etc and where they can be found on the web adms descriptions can then be published on different websites while the standard itself remains on the website of its publisher i e syndication of content adms embraces the multi publisher environment and at the same time it provides the means for the creation of aggregated catalogues of standards and single points of access to them based on adms descriptions the commission will offer a single point of access to standards described using adms via its collaborative platform joinup ref name joinup https joinup ec europa eu link to joinup ref the federation ref name federation https joinup ec europa eu elibrary document adms enabled federation semantic asset repositories brochure link to the brochure of the federation of semantic asset repositories ref service will increase the visibility of standards described with adms on the web this will also stimulate their reuse by pan european initiatives adms working group more than 43 people of 20 eu member states as well as from the us and australia have participated in the https joinup ec europa eu asset adms document adms working group adms working group most of them were experts from standardisation bodies research centres and the eu commission the working group used a methodology based on w3c s processes and methods ref name corevocspm cite web url http joinup ec europa eu sites default files d3 1 process 20and 20methodology 20for 20core 20vocabularies v1 01 pdf title archived copy accessdate 2012 04 30 deadurl yes archiveurl https web archive org web 20130430164940 https joinup ec europa eu sites default files d3 1 process 20and 20methodology 20for 20core 20vocabularies v1 01 pdf archivedate 2013 04 30 df link to isa s core vocs methodology ref how to download adms adms version 1 was officially released in april 2012 ref name admsrelease https joinup ec europa eu news adms v100 officially released adms v1 release announcement ref version 1 00 of adms is available for download on joinup ref name joinup https joinup ec europa eu asset adms release 100 ref name adms1 https joinup ec europa eu link to adms v1 ref adms is offered under isa s open metadata licence v1 1 ref name openmetadatalicense https joinup ec europa eu category licence isa open metadata licence v11 isa open metadata licence v1 1 ref related work the adms specification reuses existing metadata vocabularies and core vocabularies including the dublin core metadata element set dcmes ref http dublincore org documents dces ref the data catalog vocabulary dcat ref http www w3 org tr vocab dcat ref the foaf software friend of a friend foaf ontology the vcard ontology ref http www w3 org tr vcard rdf representing vcard objects in rdf ref the future of adms adms v1 00 will be contributed to ref name admscontributed https joinup ec europa eu asset adms topic adms public review key specifications interoperability developed eus isa programme announcement that key specifications for interoperability developed by the eu s isa programme will become w3c standards ref w3c s government linked data gld working group ref name w3c gld http www w3 org 2011 gld wiki main page w3 government linked data gld working group ref this means that adms will be published by the gld working group as first public working drafts for further consultation within the context of the typical w3c standardization process the desired outcome of that process will be the publication of adms as a w3c recommendation available under w3c s royalty free license the adms rdfs vocabulary already has a w3 org namespace http www w3 org ns adms http www w3 org ns adms references reflist 30em category data management category metadata category technical communication'
b'file high level view data grid v1 jpg 200px right high level view data grid topology a data grid is an architecture or set of services that gives individuals or groups of users the ability to access modify and transfer extremely large amounts of geographically distributed data for research purposes ref allcock bill chervenak ann foster ian et al data grid tools enabling science on big distributed data ref data grids make this possible through a host of middleware application software applications and service systems architecture services that pull together data and resource computer science resources from multiple administrative domain s and then present it to users upon request the data in a data grid can be located at a single site or multiple sites where each site can be its own administrative domain governed by a set of security restrictions as to who may access the data ref venugopal srikumar buyya rajkumar ramamohanarao kotagiri a taxonomy of data grids for distributed data sharing management and processing p 37 ref likewise multiple replica s of the data may be distributed throughout the grid outside their original administrative domain and the security restrictions placed on the original data for who may access it must be equally applied to the replicas ref shorfuzzaman mohammad graham peter eskicioglu rasit adaptive replica placement in hierarchical data grids p 15 ref specifically developed data grid middleware is what handles the integration between users and the data they request by controlling access while making it available as efficiently as possible the diagram to the right depicts a high level view of a data grid middleware middleware provides all the services and applications necessary for efficient management of dataset s and computer file files within the data grid while providing users quick access to the datasets and files ref padala pradeep a survey of data middleware for grid systems p 1 ref there are a number of concepts and tools that must be available to make a data grid operationally viable however at the same time not all data grids require the same capabilities and services because of differences in access requirements security and location of resources in comparison to users in any case most data grids will have similar middleware services that provide for a universal namespace name space data transport service data access service data replication and resource management service when taken together they are key to the data grids functional capabilities universal namespace since sources of data within the data grid will consist of data from multiple separate systems and computer network networks using different file naming convention s it would be difficult for a user to locate data within the data grid and know they retrieved what they needed based solely on existing physical file names pfns a universal or unified name space makes it possible to create logical file names lfns that can be referenced within the data grid that map to pfns ref padala pradeep a survey of data middleware for grid systems ref when an lfn is requested or queried all matching pfns are returned to include possible replicas of the requested data the end user can then choose from the returned results the most appropriate replica to use this service is usually provided as part of a management system known as a storage resource broker srb ref arcot rajasekar wan michael moore reagan schroeder wayne kremenek storage resource broker managing distributed data in a grid ref information about the locations of files and mappings between the lfns and pfns may be stored in a metadata or replica catalogue ref venugopal srikumar buyya rajkumar ramamohanarao kotagiri a taxonomy of data grids for distributed data sharing management and processing p 11 ref the replica catalogue would contain information about lfns that map to multiple replica pfns data transport service another middleware service is that of providing for data transport or data transfer data transport will encompass multiple functions that are not just limited to the transfer of bits to include such items as fault tolerance and data access ref coetzee serena reference model for a data grid approach to address data in a dynamic sdi p 16 ref fault tolerance can be achieved in a data grid by providing mechanisms that ensures data transfer will resume after each interruption until all requested data is received ref venugopal srikumar buyya rajkumar ramamohanarao kotagiri a taxonomy of data grids for distributed data sharing management and processing p 21 ref there are multiple possible methods that might be used to include starting the entire transmission over from the beginning of the data to resuming from where the transfer was interrupted as an example gridftp provides for fault tolerance by sending data from the last acknowledged byte without starting the entire transfer from the beginning the data transport service also provides for the low level access and connections between host network hosts for file transfer ref allcock bill foster ian nefedova veronika chervenak ann deelman ewa kesselman carl high performance remote access to climate simulation data a challenge problem for data grid technologies ref the data transport service may use any number of modes to implement the transfer to include parallel data transfer where two or more data streams are used over the same channel communications channel or striped data transfer where two or more steams access different blocks of the file for simultaneous transfer to also using the underlying built in capabilities of the network hardware or specifically developed protocol object oriented programming protocols to support faster transfer speeds ref izmailov rauf ganguly samrat tu nan fast parallel file replication in data grid p 2 ref the data transport service might optionally include a network overlay function to facilitate the routing and transfer of data as well as file i o functions that allow users to see remote files as if they were local to their system the data transport service hides the complexity of access and transfer between the different systems to the user so it appears as one unified data source data access service data access services work hand in hand with the data transfer service to provide security access controls and management of any data transfers within the data grid ref raman vijayshankar narang inderpal crone chris hass laura malaika susan services for data access and data processing on grids ref security services provide mechanisms for authentication of users to ensure they are properly identified common forms of security for authentication can include the use of passwords or kerberos protocol authorization services are the mechanisms that control what the user is able to access after being identified through authentication common forms of authorization mechanisms can be as simple as file permissions however need for more stringent controlled access to data is done using access control list s acls role based access control rbac and tasked based authorization controls tbac ref thomas r k and sandhu r s task based authorization controls tbac a family of models for active and enterprise oriented authorization management ref these types of controls can be used to provide granular access to files to include limits on access times duration of access to granular controls that determine which files can be read or written to the final data access service that might be present to protect the confidentiality of the data transport is encryption ref sreelatha malempati grid based approach for data confidentiality p 1 ref the most common form of encryption for this task has been the use of transport layer security ssl while in transport while all of these access services operate within the data grid access services within the various administrative domains that host the datasets will still stay in place to enforce access rules the data grid access services must be in step with the administrative domains access services for this to work data replication service to meet the needs for scalability fast access and user collaboration most data grids support replication of datasets to points within the distributed storage architecture ref chervenak ann schuler robert kesselman carl koranda scott moe brian wide area data replication for scientific collaborations ref the use of replicas allows multiple users faster access to datasets and the preservation of bandwidth since replicas can often be placed strategically close to or within sites where users need them however replication of datasets and creation of replicas is bound by the availability of storage within sites and bandwidth between sites the replication and creation of replica datasets is controlled by a replica management system the replica management system determines user needs for replicas based on input requests and creates them based on availability of storage and bandwidth ref lamehamedi houda szymanski boleslaw shentu zujun deelman ewa data replication strategies in grid environments ref all replicas are then cataloged or added to a directory based on the data grid as to their location for query by users in order to perform the tasks undertaken by the replica management system it needs to be able to manage the underlying storage infrastructure the data management system will also ensure the timely updates of changes to replicas are propagated to all nodes replication update strategy there are a number of ways the replication management system can handle the updates of replicas the updates may be designed around a centralized model where a single master replica updates all others or a decentralized model where all peers update each other ref lamehamedi houda szymanski boleslaw shentu zujun deelman ewa data replication strategies in grid environments ref the topology of node placement may also influence the updates of replicas if a hierarchy topology is used then updates would flow in a tree like structure through specific paths in a flat topology it is entirely a matter of the peer relationships between nodes as to how updates take place in a hybrid topology consisting of both flat and hierarchy topologies updates may take place through specific paths and between peers replication placement strategy there are a number of ways the replication management system can handle the creation and placement of replicas to best serve the user community if the storage architecture supports replica placement with sufficient site storage then it becomes a matter of the needs of the users who access the datasets and a strategy for placement of replicas ref padala pradeep a survey of data middleware for grid systems ref there have been numerous strategies proposed and tested on how to best manage replica placement of datasets within the data grid to meet user requirements there is not one universal strategy that fits every requirement the best it is a matter of the type of data grid and user community requirements for access that will determine the best strategy to use replicas can even be created where the files are encrypted for confidentiality that would be useful in a research project dealing with medical files ref kranthi g and rekha d shashi protected data objects replication in data grid p 40 ref the following section contains several strategies for replica placement dynamic replication dynamic replication is an approach to placement of replicas based on popularity of the data ref belalem ghalem and meroufel bakhta management and placement of replicas in a hierarchical data grid ref the method has been designed around a hierarchical replication model the data management system keeps track of available storage on all nodes it also keeps track of requests hits for which data clients users in a site are requesting when the number of hits for a specific dataset exceeds the replication threshold it triggers the creation of a replica on the server that directly services the user s client if the direct servicing server known as a father does not have sufficient space then the father s father in the hierarchy is then the target to receive a replica and so on up the chain until it is exhausted the data management system algorithm also allows for the dynamic deletion of replicas that have a null access value or a value lower than the frequency of the data to be stored to free up space this improves system performance in terms of response time number of replicas and helps load balance across the data grid this method can also use dynamic algorithms that determine whether the cost of creating the replica is truly worth the expected gains given the location ref lamehamedi houda szymanski boleslaw shentu zujun deelman ewa data replication strategies in grid environments ref adaptive replication this method of replication like the one for dynamic replication has been designed around a hierarchical replication model found in most data grids it works on a similar algorithm to dynamic replication with file access requests being a prime factor in determining which files should be replicated a key difference however is the number and frequency of replica creations is keyed to a dynamic threshold that is computed based on request arrival rates from clients over a period of time ref shorfuzzaman mohammad graham peter eskicioglu rasit adaptive replica placement in hierarchical data grids ref if the number of requests on average exceeds the previous threshold and shows an upward trend and storage utilization rates indicate capacity to create more replicas more replicas may be created as with dynamic replication the removal of replicas that have a lower threshold that were not created in the current replication interval can be removed to make space for the new replicas fair share replication like the adaptive and dynamic replication methods before fair share replication is based on a hierarchical replication model also like the two before the popularity of files play a key role in determining which files will be replicated the difference with this method is the placement of the replicas is based on access load and storage load of candidate servers ref rasool qaisar li jianzhong oreku george s munir ehsan ullah fair share replication in data grid ref a candidate server may have sufficient storage space but be servicing many clients for access to stored files placing a replicate on this candidate could degrade performance for all clients accessing this candidate server therefore placement of replicas with this method is done by evaluating each candidate node for access load to find a suitable node for the placement of the replica if all candidate nodes are equivalently rated for access load none or less accessed than the other then the candidate node with the lowest storage load will be chosen to host the replicas similar methods to the other described replication methods are used to remove unused or lower requested replicates if needed replicas that are removed might be moved to a parent node for later reuse should they become popular again other replication the above three replica strategies are but three of many possible replication strategies that may be used to place replicas within the data grid where they will improve performance and access below are some others that have been proposed and tested along with the previously described replication strategies ref ranganathan kavitha and foster ian identifying dynamic replication strategies for a high performance data grid ref static uses a fixed replica set of nodes with no dynamic changes to the files being replicated best client each node records number of requests per file received during a preset time interval if the request number exceeds the set threshold for a file a replica is created on the best client one that requested the file the most stale replicas are removed based on another algorithm cascading is used in a hierarchical node structure where requests per file received during a preset time interval is compared against a threshold if the threshold is exceeded a replica is created at the first tier down from the root if the threshold is exceeded again a replica is added to the next tier down and so on like a waterfall effect until a replica is placed at the client itself plain caching if the client requests a file it is stored as a copy on the client caching plus cascading combines two strategies of caching and cascading fast spread also used in a hierarchical node structure this strategy automatically populates all nodes in the path of the client that requests a file tasks scheduling and resource allocation such characteristics of the data grid systems as large scale and heterogeneity require specific methods of tasks scheduling and resource allocation to resolve the problem majority of systems use extended classic methods of scheduling ref epimakhov igor hameurlain abdelkader dillon tharam morvan franck resource scheduling methods for query optimization in data grid systems ref others invite fundamentally different methods based on incentives for autonomous nodes like virtual money or reputation of a node another specificity of data grids dynamics consists in the continuous process of connecting and disconnecting of nodes and local load imbalance during an execution of tasks that can make obsolete or non optimal results of initial resource allocation for a task as a result much of the data grids utilize execution time adaptation techniques that permit the systems to reflect to the dynamic changes balance the load replace disconnecting nodes use the profit of newly connected nodes recover a task execution after faults resource management system rms the resource management system represents the core functionality of the data grid it is the heart of the system that manages all actions related to storage resources in some data grids it may be necessary to create a federated rms architecture because of different administrative policies and a diversity of possibilities found within the data grid in place of using a single rms in such a case the rmss in the federation will employ an architecture that allows for interoperability based on an agreed upon set of protocols for actions related to storage resources ref krauter klaus buyya rajkumar maheswaran muthucumaru a taxonomy and survey of grid resource management systems for distributed computing ref rms functional capabilities fulfillment of user and application requests for data resources based on type of request and policies rms will be able to support multiple policies and multiple requests concurrently scheduling timing and creation of replicas policy and security enforcement within the data grid resources to include authentication authorization and access support systems with different administrative policies to inter operate while preserving site autonomy support quality of service qos when requested if feature available enforce system fault tolerance and stability requirements manage resources i e disk storage network bandwidth and any other resources that interact directly or as part of the data grid manage trusts concerning resources in administrative domains some domains may place additional restrictions on how they participate requiring adaptation of the rms or federation supports adaptability extensibility and scalability in relation to the data grid topology file data grid multiple topologies 1 jpg right possible data grid topologies data grids have been designed with multiple topologies in mind to meet the needs of the scientific community on the right are four diagrams of various topologies that have been used in data grids ref zhu lichun metadata management in grid database federation ref each topology has a specific purpose in mind for where it will be best utilized each of these topologies is further explained below federation topology is the choice for institutions that wish to share data from already existing systems it allows each institution control over their data when an institution with proper authorization requests data from another institution it is up to the institution receiving the request to determine if the data will go to the requesting institution the federation can be loosely integrated between institutions tightly integrated or a combination of both monadic topology has a central repository that all collected data is fed into the central repository then responds to all queries for data there are no replicas in this topology as compared to others data is only accessed from the central repository which could be by way of a web portal one project that uses this data grid topology is the network for earthquake engineering simulation network for earthquake engineering simulation nees in the united states ref venugopal srikumar buyya rajkumar ramamohanarao kotagiri a taxonomy of data grids for distributed data sharing management and processing p 16 ref this works well when all access to the data is local or within a single region with high speed connectivity hierarchical topology lends itself to collaboration where there is a single source for the data and it needs to be distributed to multiple locations around the world one such project that will benefit from this topology would be cern that runs the large hadron collider that generates enormous amounts of data this data is located at one source and needs to be distributed around the world to organizations that are collaborating in the project hybrid topology is simply a configuration that contains an architecture consisting of any combination of the previous mentioned topologies it is used mostly in situations where researchers working on projects want to share their results to further research by making it readily available for collaboration history the need for data grids was first recognized by the scientific community concerning climate modeling where terabyte and petabyte sized data set s were becoming the norm for transport between sites ref allcock bill foster ian nefedova veronika chervenak ann deelman ewa kesselman carl high performance remote access to climate simulation data a challenge problem for data grid technologies ref more recent research requirements for data grids have been driven by the large hadron collider lhc at cern the ligo laser interferometer gravitational wave observatory ligo and the sloan digital sky survey sloan digital sky survey sdss these examples of scientific instruments produce large amounts of data that need to be accessible by large groups of geographically dispersed researchers ref allcock bill chervenak ann foster ian et al p 571 ref ref tierney brian l data grids and data grid performance issues p 7 ref other uses for data grids involve governments hospitals schools and businesses where efforts are taking place to improve services and reduce costs by providing access to dispersed and separate data systems through the use of data grids ref thibodeau p governments plan data grid projects ref from its earliest beginnings the concept of a data grid to support the scientific community was thought of as a specialized extension of the grid which itself was first envisioned as a way to link super computers into meta computers ref heingartner douglas the grid the next gen internet ref however that was short lived and the grid evolved into meaning the ability to connect computers anywhere on the web to get access to any desired files and resources similar to the way electricity is delivered over a grid by simply plugging in a device the device gets electricity through its connection and the connection is not limited to a specific outlet from this the data grid was proposed as an integrating architecture that would be capable of delivering resources for distributed computations it would also be able to service numerous to thousands of queries at the same time while delivering gigabytes to terabytes of data for each query the data grid would include its own management infrastructure capable of managing all aspects of the data grids performance and operation across multiple wide area networks while working within the existing framework known as the web ref heingartner douglas the grid the next gen internet ref the data grid has also been defined more recently in terms of usability what must a data grid be able to do in order for it to be useful to the scientific community proponents of this theory arrived at several criteria ref venugopal srikumar buyya rajkumar ramamohanarao kotagiri a taxonomy of data grids for distributed data sharing management and processing p 1 ref one users should be able to search and discover applicable resources within the data grid from amongst its many datasets two users should be able to locate datasets within the data grid that are most suitable for their requirement from amongst numerous replicas three users should be able to transfer and move large datasets between points in a short amount of time four the data grid should provide a means to manage multiple copies of datasets within the data grid and finally the data grid should provide security with user access controls within the data grid i e which users are allowed to access which data the data grid is an evolving technology that continues to change and grow to meet the needs of an expanding community one of the earliest programs begun to make data grids a reality was funded by the darpa defense advanced research projects agency darpa in 1997 at the university of chicago ref globus about the globus toolkit ref this research spawned by darpa has continued down the path to creating open source tools that make data grids possible as new requirements for data grids emerge projects like the globus toolkit will emerge or expand to meet the gap data grids along with the grid will continue to evolve notes reflist references cite journal last1 allcock first1 bill last2 chervenak first2 ann last3 foster first3 ian last4 kesselman first4 carl last5 livny first5 miron year 2005 title data grid tools enabling science on big distributed data journal journal of physics conference series volume 16 pages 571 575 publisher institute of physics publishing doi 10 1088 1742 6596 16 1 079 url http iopscience iop org 1742 6596 16 1 079 accessdate april 15 2012 cite journal last1 allcock first1 bill last2 foster first2 ian last3 nefedova first3 veronika l last4 chervenak first4 ann last5 deelman first5 ewa last6 kesselman first6 carl last7 lee first7 jason last8 sim first8 alex last9 shoshani first9 arie last10 drach first10 bob last11 williams first11 dean title high performance remote access to climate simulation data a challenge problem for data grid technologies work publisher acm press year 2001 citeseerx 10 1 1 64 6603 format doi accessdate april 20 2012 cite web last1 arcot first1 rajasekar last2 wan first2 michael last3 moore first3 reagan last4 schroeder first4 wayne last5 kremenek first5 george title storage resource broker managing distributed data in a grid work publisher date url http www npaci edu dice pubs csi paper sent doc format doi accessdate april 28 2012 deadurl yes archiveurl https web archive org web 20060507193028 http www npaci edu 80 dice pubs csi paper sent doc archivedate may 7 2006 df cite journal last1 belalem first1 ghalem last2 meroufel first2 bakhta year 2011 title management and placement of replicas in a hierarchical data grid journal international journal of distributed and parallel systems ijdps volume 2 issue 6 pages 23 30 location publisher doi 10 5121 ijdps 2011 2603 url http www scribd com doc 75105419 management and placement of replicas in a hierarchical data grid accessdate april 28 2012 cite journal last1 chervenak first1 a last2 foster first2 i last3 kesselman first3 c last4 salisbury first4 c last5 tuecke first5 s year 2001 title the data grid towards an architecture for the distributed management and analysis of large scientific datasets journal journal of network and computer applications volume 23 issue pages 187 200 location publisher doi 10 1006 jnca 2000 0110 url http www globus org alliance publications papers jncapaper pdf accessdate april 11 2012 cite web last1 chervenak first1 ann last2 schuler first2 robert last3 kesselman first3 carl last4 koranda first4 scott last5 moe first5 brian title wide area data replication for scientific collaborations work publisher ieee date november 14 2005 url http www globus org alliance publications papers chervenakgrid2005 pdf format doi accessdate april 25 2012 cite journal last1 coetzee first1 serena year 2012 title reference model for a data grid approach to address data in a dynamic sdi journal geoinformatica volume 16 issue 1 pages 111 129 location publisher doi 10 1007 s10707 011 0129 4 url http web up ac za sitefiles file 48 16053 coetzee 2011 referencemodelfordatagridapproach 2 pdf accessdate april 28 2012 dead link date december 2016 bot internetarchivebot fix attempted yes cite conference last1 epimakhov first1 igor last2 hameurlain first2 abdelkader last3 dillon first3 tharam last4 morvan first4 franck title resource scheduling methods for query optimization in data grid systems booktitle advances in databases and information systems 15th international conference adbis 2011 pages 185 199 publisher springer berlin heidelberg year 2011 location vienna austria url http link springer com chapter 10 1007 2f978 3 642 23737 9 14 doi 10 1007 978 3 642 23737 9 14 id accessdate september 20 2011 cite web last1 globus first1 title about the globus toolkit work publisher globus alliance globus year 2012 url http www globus org toolkit about html doi accessdate may 27 2012 cite news last1 heingartner first1 douglas title the grid the next gen internet work wired publisher date march 8 2001 url http www wired com science discoveries news 2001 03 42230 format doi accessdate may 13 2012 deadurl yes archiveurl https web archive org web 20120504035536 http www wired com 80 science discoveries news 2001 03 42230 archivedate may 4 2012 df cite web last1 izmailov first1 rauf last2 ganguly first2 samrat last3 tu first3 nan title fast parallel file replication in data grid work publisher year 2004 url http www cs huji ac il labs danss p2p resources fast parallel file replication on data grid pdf format doi accessdate may 10 2012 deadurl yes archiveurl https web archive org web 20120421081052 http www cs huji ac il labs danss p2p resources fast parallel file replication on data grid pdf archivedate april 21 2012 df cite journal last1 kranthi first1 g aruna last2 rekha first2 d shashi year 2012 title protected data objects replication in data grid journal international journal of network security its applications ijnsa volume 4 issue 1 pages 29 41 location publisher doi 10 5121 ijnsa 2012 4103 url http journaldatabase org articles protected data objects replication html issn 0975 2307 accessdate april 1 2012 deadurl yes archiveurl https web archive org web 20131008004646 http journaldatabase org articles protected data objects replication html archivedate october 8 2013 df cite journal last1 krauter first1 klaus last2 buyya first2 rajkumar last3 maheswaran first3 muthucumaru year 2002 title a taxonomy and survey of grid resource management systems for distributed computing journal software practice and experience spe volume 32 issue 2 pages 135 164 location publisher doi 10 1002 spe 432 citeseerx 10 1 1 38 2122 accessdate april 17 2012 cite conference last1 lamehamedi first1 houda last2 szymanski first2 boleslaw last3 shentu first3 zujun last4 deelman first4 ewa title data replication strategies in grid environments booktitle fifth international conference on algorithms and architectures for parallel processing ica3pp 02 pages 378 383 publisher press year 2002 location citeseerx 10 1 1 11 5473 doi accessdate april 5 2012 cite journal last1 padala first1 pradeep title a survey of data middleware for grid systems work publisher date citeseerx 10 1 1 114 1901 format doi accessdate april 28 2012 cite web last1 raman first1 vijayshankar last2 narang first2 inderpal last3 crone first3 chris last4 hass first4 laura last5 malaika first5 susan title services for data access and data processing on grids work publisher date february 9 2003 url http www ogf org documents gfd 14 pdf format doi accessdate may 10 2012 cite conference last1 ranganathan first1 kavitha last2 foster first2 ian title identifying dynamic replication strategies for a high performance data grid booktitle in proc of the international grid computing workshop pages 75 86 publisher year 2001 location citeseerx 10 1 1 20 6836 format doi 10 1007 3 540 45644 9 8 accessdate may 15 2012 cite journal last1 rasool first1 qaisar last2 li first2 jianzhong last3 oreku first3 george s last4 munir first4 ehsan ullah year 2008 title fair share replication in data grid journal information technology journal volume 7 issue 5 pages 776 782 publisher doi 10 3923 itj 2008 776 782 url http scialert net abstract doi itj 2008 776 782 accessdate april 27 2012 cite journal last1 shorfuzzaman first1 mohammad last2 graham first2 peter last3 eskicioglu first3 rasit year 2010 title adaptive replica placement in hierarchical data grids journal journal of physics conference series volume 256 issue 1 pages 1 18 location publisher iop publishing ltd doi 10 1088 1742 6596 256 1 012020 url http iopscience iop org 1742 6596 256 1 012020 accessdate april 15 2012 cite journal last1 sreelatha first1 malempati year 2011 title grid based approach for data confidentiality journal international journal of computer applications volume 25 issue 9 pages 1 5 location publisher doi 10 5120 3063 4186 issn 0975 8887 url http www ijcaonline org volume25 number9 pxc3874186 pdf accessdate april 28 2012 cite journal last1 thibodeau first1 p title governments plan data grid projects journal computerworld volume 39 issue 42 pages 14 location united states publisher computerworld date may 30 2005 url http www computerworld com s article 102119 governments plan data grid projects format doi issn 0010 4841 accessdate april 28 2012 cite web last1 thomas first1 r k last2 sandhu first2 r s title task based authorization controls tbac a family of models for active and enterprise oriented authorization management work publisher year 1997 url http profsandhu com confrnc ifip i97tbac pdf format doi accessdate april 28 2012 cite web last1 tierney first1 brian l title data grids and data grid performance issues work publisher year 2000 url http www didc lbl gov presentations csc2000 tierney pdf format doi accessdate april 28 2012 cite journal last1 venugopal first1 srikumar last2 buyya first2 rajkumar last3 ramamohanarao first3 kotagiri year 2006 title a taxonomy of data grids for distributed data sharing management and processing journal acm computing surveys volume 38 issue 1 pages 1 60 location new york publisher association for computing machinery doi url http www cloudbus org reports datagridtaxonomy pdf accessdate april 10 2012 cite web last1 zhu first1 lichun title metadata management in grid database federation work publisher date url http cs uwindsor ca richard cs510 lichun zhu survey pdf format doi accessdate may 15 2012 dead link date december 2016 bot internetarchivebot fix attempted yes further reading cite web last1 allcock first1 w authorlink w allcock title gridftp protocol extensions to ftp for the grid work publisher argonne national laboratory date april 2003 url http www globus org alliance publications papers gfd r 0201 pdf format doi accessdate april 20 2012 cite web last1 allcock first1 w last2 bresnahan first2 j last3 kettimuthu first3 r last4 link first4 m last5 dumitrescu first5 c last6 raicu first6 i last7 foster first7 i title the globus striped gridftp framework and server work publisher acm press date november 2005 url http www globus org alliance publications papers gridftp final pdf format doi accessdate april 20 2012 cite journal last1 foster first1 ian last2 kesselman first2 carl last3 tuecke first3 steven year 2001 title the anatomy of the grid enabling scalable virtual organizations journal international journal of high performance computing applications volume 15 issue 3 pages 200 222 location thousand oaks publisher sage publications doi 10 1177 109434200101500302 url http www globus org alliance publications papers anatomy pdf accessdate april 10 2012 cite web last1 foster first1 ian last2 kesselman first2 carl last3 nick first3 jeffrey m last4 tuecke first4 steven title the physiology of the grid an open grid services architecture for distributed systems integration work publisher date june 22 2002 url http forge gridforum org sf go doc13483 nav 1 format doi accessdate may 10 2012 deadurl yes archiveurl https web archive org web 20080322035911 http forge gridforum org 80 sf go doc13483 nav 1 archivedate march 22 2008 df cite journal last1 hancock first1 b year 2009 title a simple data grid using the inferno operating system journal library hi tech volume 27 issue 3 pages 382 392 location publisher emerald group publishing limited doi 10 1108 07378830910988513 url accessdate april 10 2012 cite web last1 hoschek first1 w last2 mccance first2 g title grid enabled relational database middleware work publisher global grid forum date october 10 2001 url http ppewww ph gla ac uk preprints 2001 11 ggf3rome2001 pdf format doi accessdate april 22 2012 deadurl yes archiveurl https web archive org web 20060128234459 http ppewww ph gla ac uk 80 preprints 2001 11 ggf3rome2001 pdf archivedate january 28 2006 df cite web last1 kunszt first1 peter z last2 guy first2 leanne p title the open grid services architecture and data grids work publisher date july 7 2002 url http www computing surrey ac uk courses csm23 papers data grid pdf format doi accessdate may 10 2012 cite web last1 moore first1 reagan w title evolution of data grid concepts work publisher date url http www nesc ac uk events ggf10 da programme papers 06 moore grid evolution pdf format doi accessdate may 10 2012 cite conference last1 rajkumar first1 kettimuthu last2 allcock first2 william last3 liming first3 lee last4 navarro first4 john paul last5 foster first5 ian title gridcopy moving data fast on the grid booktitle international parallel and distributed processing symposium ipdps 2007 pages 1 6 publisher ieee international date march 30 2007 location long beach url http www globus org alliance publications papers gridcopy pdf doi id accessdate april 29 2012 cite journal last1 thenmozhi first1 n last2 madheswaran first2 m year 2011 title content based data transfer mechanism for efficient bulk data transfer in grid computing environment journal international journal of grid computing applications ijgca volume 2 issue 4 pages 49 62 location publisher doi 10 5121 ijgca 2011 2405 issn 2229 3949 url http www scribd com doc 78611092 content based data transfer mechanism for efficient bulk data transfer in grid computing environment accessdate april 28 2012 cite journal last1 tu first1 manghui last2 li first2 peng last3 i ling first3 yen last4 thuraisingham first4 bhavani last5 khan first5 latifur year 2010 title secure data objects replication in data grid journal ieee transactions on dependable and secure computing volume 7 issue 1 pages 50 64 publisher ieee doi 10 1109 tdsc 2008 19 url http www utdallas edu lkhan papers secure data objects replication in data grid pdf accessdate april 26 2012 dead link date december 2016 bot internetarchivebot fix attempted yes category data management'
b'multiple issues unreferenced date december 2006 orphan date february 2009 a database clone is a complete and separate copy of a database system that includes the business data the dbms software and any other application tiers that make up the environment cloning is a different kind of operation to data replication replication and backup s in that the cloned environment is both fully functional and separate in its own right additionally the cloned environment may be modified at its inception due to configuration changes or data subsetting the cloning refers to the replication of the server in order to have a backup to upgrade the environment defaultsort clone database category data management category databases'
b'refimprove date june 2012 classora is a knowledge base for the internet oriented to data analysis from a practical point of view classora is a digital repository that stores structured information and allows it to be displayed in multiple formats analytically graphically geographically through maps as well as carry out olap analysis the information contained in classora comes from public sources ref http revista mundo r com contido e2 80 9cclassora evita tener que bucear entre resultados google o wikipedia e2 80 9d interview in r technological magazine spanish ref and is uploaded into the system through bots and extract transform load etl processes the knowledge base has a commercial api ref http blog classora com 2012 03 05 api de classora para desarrolladores classora api in official weblog ref for semantic enhancement and an open web ref http www classora com open web of classora knowledge base ref through which any user can access to part of the information collected it also allows users to complete data and share opinions internally classora is organized into knowledge units and reports a \xc2\xabknowledge unit\xc2\xbb is any element of the world about which information may be stored and presented in the form of a data sheet a person a company a country etc a \xc2\xabreport\xc2\xbb is a group of knowledge units a ranking of companies a sport classification table a survey about people etc in fact one of the technical capabilities of classora is that it allows the comparison of reports and knowledge units gathered from different sources thereby generating an added value for the media in which this information is published digital media interactive tv etc key definitions knowledge unit the units of knowledge also known as entries in classora are data sheets that have a certain semantic equivalence with the articles on the wikipedia they store information about any element of the world be it a film a country a company or an animal however they differ from wikipedia in that classora stores structured information enriched with a metadata layer and therefore it is able to automatically interpret the meaning of each unit of knowledge data report a report is a group of units of knowledge in which the repetition of elements is not allowed this definition includes any list poll ranking etc and in general any consultation that involves more than one unit of knowledge classora excels at the reports management due to its visualization capabilities being able to display data in the form of tables graphs and maps types of reports sports scores sports competitions results sanctioned by the competent institution rankings and lists all types of interesting and curious lists whether they have an implicit order or not polls units of knowledge that are ranked according to users votes queries to the knowledge base questions from users using contextual query language cql networks of connections automatically calculated from the reports and the taxonomy of each knowledge unit organizational taxonomy an organizational taxonomy also referred to as entry type is a data sheet that brings together the common attributes of a set of units of knowledge for instance the organizational taxonomy f1 driver displays attributes such as date of debut team etc and the organizational taxonomy football club presents attributes such as city stadium etc in classora taxonomies are hierarchically organized so that they inherit attributes from their parent taxonomies for instance f1 driver is a subsidiary taxonomy of sportsperson which is a subsidiary taxonomy of person which in turn is a subsidiary taxonomy of organism the simplest type of entry in classora is classora object all the other taxonomies are its subsidiaries and inherit its attributes in fact the only attribute classora object possesses is name all units of knowledge are required to have one name at least architecture of classora data extraction module the data extraction module consists of a set of robots coordinated by software that also manages the potential incidents most of the information available in classora is automatically uploaded through those robots which connect to the main online public sources to gather all types of data there are three categories of robots extraction robots responsible for the massive uploading of reports from official public sources fifa cia imf eurostat they are used for either absolute or incremental data uploading data scanner robots responsible for looking for and updating the data of a unit of knowledge they use specific sources to perform this task wikipedia imdb world bank etc content aggregators they don t connect to external sources instead they generate new information using classora s internal database participatory module in classora s open website internet users may participate providing their knowledge as they would on the wikipedia there are different ways to participate adding or correcting data in the knowledge base voting in surveys participatory rankings and creating new knowledge units and data reports connectivity module the knowledge base is designed to be embedded in multi platform multi channel systems thus enabling its integration into mobile devices tablets interactive tv etc this integration may be carried out through specific plugins for navigators or other devices or an api rest that provides content in xml or json formats the api is divided into three blocks of operations the first one is the block of general utility tools ranging from autosuggest components about geographical hierarchies to operations to obtain the list of today s celebrity birthdays using contextual query language cql the second one is the block of operations for widget generation graphs maps rankings using information from the knowledge base finally there is a block of operations designed for the publication of free source content ref http blog classora com 2012 03 05 api de classora para desarrolladores post about api in classora official weblog ref project statistics as of april 2012 2 000 000 knowledge units 15 000 reports around 10 000 maps and several million potential comparative analyses had been added to classora according to the site of web metrics alexa classora open website is ranked at 100 557 globally and at 2 880 in the spanish traffic ranking ref http www alexa com siteinfo http 3a 2f 2fwww classora com alexa metrics for classora open web ref users spend an average of 9 \xc2\xbd minutes in classora external links http www classora com open website of classora knowledge base references references category knowledge bases category data management category semantic web category knowledge representation software'
b'merge to business continuity planning date june 2015 a disaster recovery plan drp is a documented process or set of procedures to recover and protect a business information technology it infrastructure in the event of a disaster ref name 5 tips cite web url http www smallbusinesscomputing com news itmanagement 5 tips to build an effective disaster recovery plan html title 5 tips to build an effective disaster recovery plan publisher small business computing date 14 june 2012 accessdate 9 august 2012 first1 bill last1 abram ref such a plan ordinarily documented in written form specifies procedures an organization is to follow in the event of a disaster it is a comprehensive statement of consistent actions to be taken before during and after a disaster ref name dr journal cite web url http www drj com new2dr w2 002 htm title disaster recovery planning process first1 geoffrey h last1 wold work disaster recovery journal series adapted from volume 5 1 publisher disaster recovery world year 1997 accessdate 8 august 2012 ref the disaster could be natural disaster natural environmental disaster environmental or anthropogenic hazard man made man made disasters could be intentional for example an act of a terrorist or unintentional that is accidental such as the breakage of a man made dam given organizations increasing dependency on information technology to run their operations a disaster recovery plan sometimes erroneously called a continuity of operations plan coop is increasingly associated with the recovery of information technology data assets and facilities objectives organizations cannot always avoid disasters but with careful planning the effects of a disaster can be minimized the objective of a disaster recovery plan is to minimize downtime and data loss ref http www comp soln com drp whitepaper pdf an overview of the disaster recovery planning process from start to finish comprehensive consulting solutions inc disaster recovey planning an overview white paper march 1999 retrieved 8 august 2012 ref the primary objective is to protect the organization in the event that all or part of its operations and or computer services are rendered unusable the plan minimizes the disruption of operations and ensures that some level of organizational stability and an orderly recovery after a disaster will prevail ref name dr journal minimizing downtime and data loss is measured in terms of two concepts the recovery time objective rto and the recovery point objective rpo the recovery time objective is the time within which a business process must be restored after a disaster major incident mi has occurred in order to avoid unacceptable consequences associated with a break in business continuity the recovery point objective rpo is the age of files that must be recovered from backup storage for normal operations to resume if a computer system or network goes down as a result of a mi the rpo is expressed backwards in time that is into the past starting from the instant at which the mi occurs and can be specified in seconds minutes hours or days ref http whatis techtarget com definition recovery point objective rpo definition recovery point objective rpo retrieved 10 august 2012 ref the recovery point objective rpo is thus the maximum acceptable amount of data loss measured in time it is the age of the files or data in backup storage required to resume normal operations after the mi ref cite web url http www techopedia com definition 1032 recovery point objective rpo title recovery point objective rpo definition what does recovery point objective rpo mean work techopedia publisher janalta interactive inc year 2012 accessdate 10 august 2012 ref file schematic itsc and rto rpo mi jpg frame left a dr plan illustrating the chronology of the color bd00e0 rpo and the color ff7f7c rto with respect to the color fe0000 mi clear relationship to the business continuity plan according to the sans institute the business continuity planning business continuity plan bcp is a comprehensive organizational plan that includes the disaster recovery plan the institute further states that a business continuity plan bcp consists of the five component plans ref name the disaster recovery plan http www sans org reading room whitepapers recovery disaster recovery plan 1164 the disaster recovery plan chad bahan gsec practical assignment version 1 4b sans institute infosec reading room june 2003 retrieved 24 august 2012 ref business resumption plan occupant emergency plan continuity of operations plan incident management plan disaster recovery plan the institute states that the first three plans business resumption occupant emergency and continuity of operations plans do not deal with the it infrastructure they further state that the incident management plan imp does deal with the it infrastructure but since it establishes structure and procedures to address cyber attacks against an organization s it systems it generally does not represent an agent for activating the disaster recovery plan leaving the disaster recovery plan as the only bcp component of interest to it ref name the disaster recovery plan disaster recovery institute international states that disaster recovery is the area of business continuity that deals with technology recovery as opposed to the recovery of business operations ref https www drii org glossary php ref benefits like every insurance plan there are benefits that can be obtained from the drafting of a disaster recovery plan some of these benefits are ref name dr journal providing a sense of security minimizing risk of delays guaranteeing the reliability of standby systems providing a standard for testing the plan minimizing decision making during a disaster reducing potential legal liabilities lowering unnecessarily stressful work environment types of plans there is no one right type of disaster recovery plan ref name msu cite web url http www drp msu edu documentation stepbystepguide htm publisher michigan state university title disaster recovery planning step by step guide accessdate 9 may 2014 ref nor is there a one size fits all disaster recovery plan ref name 5 tips ref name msu however there are three basic strategies that feature in all disaster recovery plans 1 preventive measures 2 detective measures and 3 corrective measures ref cite web url http emailarchivingandremotebackup com backup disaster recovery html title backup disaster recovery publisher email archiving and remote backup year 2010 accessdate 9 may 2014 ref preventive measures will try to prevent a disaster from occurring these measures seek to identify and reduce risks they are designed to mitigate or prevent an event from happening these measures may include keeping data backed up and off site using surge protectors installing generators and conducting routine inspections detective measures are taken to discover the presence of any unwanted events within the it infrastructure their aim is to uncover new potential threats they may detect or uncover unwanted events these measures include installing fire alarms using up to date antivirus software holding employee training sessions and installing server and network monitoring software corrective measures are aimed to restore a system after a disaster or otherwise unwanted event takes place these measures focus on fixing or restoring the systems after a disaster corrective measures may include keeping critical documents in the disaster recovery plan or securing proper insurance policy insurance policies after a lessons learned brainstorming session ref name 5 tips ref cite web url http www stonecrossingsolutions com technical solutions disaster recovery title disaster recovery business continuity plans publisher stone crossing solutions date 2012 accessdate 9 august 2012 deadurl yes archiveurl https web archive org web 20120823045007 http www stonecrossingsolutions com technical solutions disaster recovery archivedate 23 august 2012 df ref a disaster recovery plan must answer at least three basic questions 1 what is its objective and purpose 2 who will be the people or teams who will be responsible in case any disruptions happen and 3 what will these people do the procedures to be followed when the disaster strikes ref cite web url http www continuitycompliance org disaster recovery planning on virtual and cloud platforms survey results now available title disaster recovery benefits of getting disaster planning software and template and contracting with companies offering data disaster recovery plans solutions and services why would you need a disaster recovery plan publisher continuity compliance date 7 june 2011 accessdate 14 august 2012 archivedate 9 may 2014 archiveurl http www webcitation org 6pqaoed5g url http www continuitycompliance org disaster recovery planning on virtual and cloud platforms survey results now available deadurl yes df ref types of disasters image sh 60b helicopter flies over sendai jpg thumb right 200px the tsunami that affected japan in 2011 a type of natural disaster image ua flight 175 hits wtc south tower 9 11 edit jpeg thumb right 200px september 11 2001 in new york city a type of man made disaster it caused pollution loss of lives property damage and considerable data loss disasters can be natural disaster natural or anthropogenic hazard man made man made disasters could be intentional for example sabotage or an act of terrorism or unintentional that is accidental such as the breakage of a man made dam disasters may encompass more than weather they may involve internet threats or take on other man made manifestations such as theft ref name 5 tips natural disaster main article natural disaster a natural disaster is a major adverse event resulting from the earth s natural hazards examples of natural disasters are flood s tsunami s tornado es hurricane hurricanes cyclones volcanic eruption s earthquake s heat wave s and landslide s other types of disasters include the more end time cosmic scenario of an impact event asteroid hitting the earth man made disasters main article man made disasters man made disasters are the consequence of technological or human hazards examples include stampede s fire urban fires industrial accident s oil spill s nuclear explosion s nuclear radiation and acts of war other types of man made disasters include the more cosmic scenarios of catastrophic global warming nuclear war and bioterrorism the following table categorizes some disasters and notes first response initiatives note that whereas the sources of a disaster may be natural for example heavy rains or man made for example a broken dam the results may be similar flooding ref http www nten org sites nten files sample 20disaster 20recovery 20plan doc business continuity planning bcp sample plan for nonprofit organizations wayback url http www nten org sites nten files sample 20disaster 20recovery 20plan doc date 20100602065521 pages 11 12 retrieved 8 august 2012 ref class wikitable rowspan 16 natural colspan 3 disaster bgcolor cccccc example profile first response avalanche the sudden drastic flow of snow down a slope occurring when either natural triggers such as loading from new snow or rain or artificial triggers such as explosives or backcountry skiers overload the snowpack shut off utilities evacuate building if necessary determine impact on the equipment and facilities and any disruption blizzard a severe snowstorm characterized by very strong winds and low temperatures power off all equipment listen to blizzard advisories evacuate area if unsafe assess damage earthquake the shaking of the earth s crust caused by underground volcanic forces of breaking and shifting rock beneath the earth s surface shut off utilities evacuate building if necessary determine impact on the equipment and facilities and any disruption fire fire wild fires that originate in uninhabited areas and which pose the risk to spread to inhabited areas attempt to suppress fire in early stages evacuate personnel on alarm as necessary notify fire department shut off utilities monitor weather advisories flood flash flooding small creeks gullies dry streambeds ravines culverts or even low lying areas flood quickly monitor flood advisories determine flood potential to facilities pre stage emergency power generating equipment assess damage freezing rain rain occurring when outside surface temperature is below freezing monitor weather advisories notify employees of business closure home arrange for snow and ice removal heat wave a prolonged period of excessively hot weather relative to the usual weather pattern of an area and relative to normal temperatures for the season listen to weather advisories power off all servers after a graceful shutdown if there is imminent potential of power failure shut down main electric circuit usually located in the basement or the first floor hurricane heavy rains and high winds power off all equipment listen to hurricane advisories evacuate area if flooding is possible check gas water and electrical lines for damage do not use telephones in the event of severe lightning assess damage landslide geological phenomenon which includes a range of ground movement such as rock falls deep failure of slopes and shallow debris flows shut off utilities evacuate building if necessary determine impact on the equipment and facilities and any disruption lightning strike an electrical discharge caused by lightning typically during thunderstorms power off all equipment listen to hurricane advisories evacuate area if flooding is possible check gas water and electrical lines for damage do not use telephones in the event of severe lightning assess damage limnic eruption the sudden eruption of carbon dioxide from deep lake water shut off utilities evacuate building if necessary determine impact on the equipment and facilities and any disruption tornado violent rotating columns of air which descent from severe thunderstorm cloud systems monitor tornado advisories power off equipment shut off utilities power and gas assess damage once storm passes tsunami a series of water waves caused by the displacement of a large volume of a body of water typically an ocean or a large lake usually caused by earthquakes volcanic eruptions underwater explosions landslides glacier calvings meteorite impacts and other disturbances above or below water power off all equipment listen to tsunami advisories evacuate area if flooding is possible check gas water and electrical lines for damage assess damage volcanic eruption the release of hot magma volcanic ash and or gases from a volcano shut off utilities evacuate building if necessary determine impact on the equipment and facilities and any disruption rowspan 6 man made bioterrorism the intentional release or dissemination of biological agents as a means of coercion get information immediately from your public health officials via the news media as to the right course of action if you think you have been exposed quickly remove your clothing and wash off your skin also put on a hepa to help prevent inhalation of the agent ref http answers webmd com answers 1176206 what should i do if there what should i do if there has been a bioterrorism attack edmond a hooker webmd 9 october 2007 retrieved 18 september 2012 ref civil unrest a disturbance caused by a group of people that may include sit in s and other forms of obstructions riots sabotage and other forms of crime and which is intended to be a demonstration to the public and the government but can escalate into general chaos contact local police or law enforcement ref http www usfa fema gov downloads pdf publications fa 142 pdf report of the joint fire police task force on civil unrest fa 142 recommendations for organization and operations during civil disturbance page 55 fema retrieved 21 october 2012 ref ref http www xanaboo com bcp 20 20developing 20a 20strategy 20to 20minimize 20risk 20and 20maintain 20operations pdf business continuity planning developing a strategy to minimize risk and maintain operations wayback url http www xanaboo com bcp 20 20developing 20a 20strategy 20to 20minimize 20risk 20and 20maintain 20operations pdf date 20140327234742 adam booher retrieved 19 september 2012 ref fire fire urban even with strict building fire codes people still perish needlessly in fires attempt to suppress fire in early stages evacuate personnel on alarm as necessary notify fire department shut off utilities monitor weather advisories hazardous material hazardous material spills the escape of solids liquids or gases that can harm people other living organisms property or the environment from their intended controlled environment such as a container leave the area and call the local fire department for help ref http www tnema org public hazmat html hazardous materials wayback url http www tnema org public hazmat html date 20121011150052 tennessee emergency management office retrieved 7 september 2012 ref if anyone was affected by the spill call the your local emergency medical services line ref http www atsdr cdc gov mhmi index asp managing hazardous materials incidents mhmis center for disease control retrieved 7 september 2012 ref nuclear and radiation accidents nuclear and radiation accidents an event involving significant release of radioactivity to the environment or a reactor core meltdown and which leads to major undesirable consequences to people the environment or the facility recognize that a cbrn incident has or may occur gather assess and disseminate all available information to first responders establish an overview of the affected area provide and obtain regular updates to and from first responders ref http www nato int docu cep cep cbrn response e pdf guidelines for first response to a cbrn incident project on minimum standards and non binding guidelines for first responders regarding planning training procedure and equipment for chemical biological radiological and nuclear cbrn incidents nato emergency management retrieved 21 october 2012 ref power failure caused by summer or winter storms lightning or construction equipment digging in the wrong location wait 5 10 minutes power off all servers after a graceful shutdown do not use telephones in the event of severe lightning shut down main electric circuit usually located in the basement or the first floor in the realm of information technology per se disasters may also be the result of a computer security exploit some of these are computer virus es cyberattack s denial of service attack s hacker computer security hacking and malware exploits these are ordinarily attended to by information security experts planning methodology according to geoffrey h wold of the disaster recovery journal the entire process involved in developing a disaster recovery plan consists of 10 steps ref name dr journal obtaining top management commitment for a disaster recovery plan to be successful the central responsibility for the plan must reside on management top level managers top management management is responsible for coordinating the disaster recovery plan and ensuring its effectiveness within the organization it is also responsible for allocating adequate time and resources required in the development of an effective plan resources that management must allocate include both financial considerations and the effort of all personnel involved establishing a planning committee a plan ning committee is appointed to oversee the development and implementation of the plan the planning committee includes representatives from all functional areas of the organization key committee members customarily include the operations manager and the data processing manager the committee also defines the scope of the plan performing a risk assessment the planning committee prepares a probabilistic risk assessment risk analysis and a business impact analysis bia that includes a range of possible disasters including natural technical and human threats each functional area of the organization is analyzed to determine the potential consequence and impact associated with several disaster scenarios the risk assessment process also evaluates the safety of critical documents and vital records traditionally fire has posed the greatest threat to an organization intentional human destruction however should also be considered a thorough plan provides for the worst case situation destruction of the main building it is important to assess the impacts and consequences resulting from loss of information and services the planning committee also analyzes the costs related to minimizing the potential exposures establishing priorities for processing and operations at this point the critical needs of each department within the organization are evaluated in order to prioritize them establishing wiktionary priority priorities is important because no organization possesses infinite resources and criteria must be set as to where to allocate resources first some of the areas often reviewed during the prioritization process are functional operations key personnel and their functions information flow processing systems used services provided existing documentation historical records and the department s policies and procedures processing and operations are analyzed to determine the maximum amount of time that the department and organization can operate without each critical system this will later get mapped into the recovery time objective a critical system is defined as that which is part of a system or procedure necessary to continue operations should a department computer center main facility or a combination of these be destroyed or become inaccessible a method used to determine the critical needs of a department is to document all the functions performed by each department once the primary functions have been identified the operations and processes are then ranked in order of priority essential important and non essential determining recovery strategies during this phase the most practical alternatives for processing in case of a disaster are researched and evaluated all aspects of the organization are considered including building physical facilities computer hardware and software communications link s data file s and database s customer service s provided user operations the overall management information system s mis structure end user systems and any other processing operations alternatives dependent upon the evaluation of the computer function may include hot site s warm site s cold site s reciprocal agreement disaster preparedness reciprocal agreements the provision of more than one data center the installation and deployment of multiple computer system duplication of service center consortium arrangements lease of equipment and any combinations of the above written contract agreements for the specific recovery alternatives selected are prepared specifying contract duration termination conditions system testing cost any special security procedures procedure for the notification of system changes hours of operation the specific hardware and other equipment required for processing personnel requirements definition of the circumstances constituting an emergency process to negotiate service extensions guarantee of computer compatibility compatibility availability non mainframe resource requirements priorities and other contractual issues collecting data in this phase data collection takes place among the recommended data gathering materials and documentation often included are various lists employee backup position listing critical telephone numbers list master call list master vendor list notification checklist inventories communications equipment documentation office equipment forms insurance policy insurance policies workgroup and data center computer hardware microcomputer hardware and software office supplies office supply off site storage location equipment telephones etc distribution register software and data files backup retention schedules temporary location specifications any other such other lists materials inventories and documentation pre formatted forms are often used to facilitate the data gathering process organizing and documenting a written plan next an outline of the plan s contents is prepared to guide the development of the detailed procedures top management reviews and approves the proposed plan the outline can ultimately be used for the table of contents after final revision other four benefits of this approach are that 1 it helps to organize the detailed procedures 2 identifies all major steps before the actual writing process begins 3 identifies redundant procedures that only need to be written once and 4 provides a plan road map for developing the procedures it is often considered best practice to develop a standard format for the disaster recovery plan so as to facilitate the writing of detailed procedures and the documentation of other information to be included in the plan later this helps ensure that the disaster plan follows a consistent format and allows for its ongoing future maintenance standardization is also important if more than one person is involved in writing the procedures it is during this phase that the actual written plan is developed in its entirety including all detailed procedures to be used before during and after a disaster the procedures include methods for maintaining and updating the plan to reflect any significant internal external or systems changes the procedures allow for a regular review of the plan by key personnel within the organization the disaster recovery plan is structured using a team approach specific responsibilities are assigned to the appropriate team for each functional area of the organization teams responsible for administrative functions building facilities logistics user support backup computer backup restoration and other important areas in the organization are identified the structure of the contingency organization may not be the same as the existing organization chart the contingency organization is usually structured with teams responsible for major functional areas such as administrative functions facilities logistics user support computer backup restoration and any other important area the management team is especially important because it coordinates the recovery process the team assesses the disaster activates the recovery plan and contacts team managers the management team also oversees documents and monitors the recovery process it is helpful when management team members are the final decision makers in setting priorities policies and procedures each team has specific responsibilities that are completed to ensure successful execution of the plan the teams have an assigned manager and an alternate in case the team manager is not available other team members may also have specific assignments where possible developing testing criteria and procedures best practices dictate that dr plans be thoroughly tested and evaluated on a regular basis at least annually thorough dr plans include documentation with the procedures for testing the plan the tests will provide the organization with the assurance that all necessary steps are included in the plan other reasons for testing include determining the feasibility and compatibility of backup facilities and procedures identifying areas in the plan that need modification providing training to the team managers and team members demonstrating the ability of the organization to recover providing motivation for maintaining and updating the disaster recovery plan testing the plan after testing procedures have been completed an initial dry run testing dry run of the plan is performed by conducting a structured walk through test the test will provide additional information regarding any further steps that may need to be included changes in procedures that are not effective and other appropriate adjustments these may not become evident unless an actual dry run test is performed the plan is subsequently updated to correct any problems identified during the test initially testing of the plan is done in sections and after normal business hours to minimize disruptions to the overall operations of the organization as the plan is further polished future tests occur during normal business hours types of tests include checklist tests simulation tests parallel tests and full interruption tests obtaining plan approval once the disaster recovery plan has been written and tested the plan is then submitted to management for approval it is top management s ultimate responsibility that the organization has a documented and tested plan management is responsible for 1 establishing the policies procedures and responsibilities for comprehensive contingency plan ning and 2 reviewing and approving the contingency plan annually documenting such reviews in writing organizations that receive information processing from service bureau s will in addition also need to 1 evaluate the adequacy of contingency plans for its service bureau and 2 ensure that its contingency plan is compatible with its service bureau s plan caveats controversies due to its high cost disaster recovery plans are not without critics cormac foster has identified five common mistakes organizations often make related to disaster recovery planning ref https web archive org web 20130116112225 http content dell com us en enterprise d large business mistakes that kill disaster aspx five mistakes that can kill a disaster recovery plan in archive org cormac foster dell corporation 25 october 2010 retrieved 8 august 2012 ref lack of buy in one factor is the perception by executive management that dr planning is just another fake earthquake drill or ceos that fail to make dr planning and preparation a priority are often significant contributors to the failure of a dr plan incomplete rtos and rpos another critical point is failure to include each and every important business process or a block of data every item in your dr plan requires a recovery time objective rto defining maximum process downtime or a recovery point objective rpo noting an acceptable restore point anything less creates ripples that can extend the disaster s impact as an example payroll accounting and the weekly customer newsletter may not be mission critical in the first 24 hours but left alone for several days they can become more important than any of your initial problems systems myopia a third point of failure involves focusing only on dr without considering the larger business continuity needs data and systems restoration after a disaster are essential but every business process in your organization will need it support and that support requires planning and resources as an example corporate office space lost to a disaster can result in an instant pool of teleworkers which in turn can overload a company s vpn overnight overwork the it support staff at the blink of an eye and cause serious bottlenecks and monopolies with the dial in pbx system lax security when there is a disaster an organization s data and business processes become vulnerable as such security can be more important than the raw speed involved in a disaster recovery plan s rto the most critical consideration then becomes securing the new data pipelines from new vpns to the connection from offsite backup services another security concern includes documenting every step of the recovery process something that is especially important in highly regulated industries government agencies or in disasters requiring post mortem forensics locking down or remotely wiping lost handheld devices is also an area that may require addressing outdated plans another important aspect that is often overlooked involves the frequency with which dr plans are updated yearly updates are recommended but some industries or organizations require more frequent updates because business processes evolve or because of quicker data growth to stay relevant disaster recovery plans should be an integral part of all business analysis processes and should be revisited at every major corporate acquisition at every new product launch and at every new system development milestone see also disaster recovery business continuity planning federal emergency management agency backup rotation scheme seven tiers of disaster recovery references reflist 2 defaultsort disaster recovery plan category disaster recovery category data management category backup category it risk management category planning'
b'newsql is a class of modern relational database management system relational database management system s that seek to provide the same scalable performance of nosql systems for online transaction processing oltp read write workloads while still maintaining the acid guarantees of a traditional database system ref name aslett2012 cite web url http cs brown edu courses cs227 archives 2012 papers newsql aslett newsql pdf title how will the database incumbents respond to nosql and newsql first matthew last aslett publisher 451 group publication date 2011 04 04 year 2011 accessdate 2012 07 06 ref ref cite web url http cacm acm org blogs blog cacm 109710 new sql an alternative to nosql and old sql for new oltp apps fulltext title newsql an alternative to nosql and old sql for new oltp apps first michael last stonebraker publisher communications of the acm blog publication date 2011 06 16 accessdate 2012 07 06 ref ref name highscalability cite web url http highscalability com blog 2012 9 24 google spanners most surprising revelation nosql is out and html title google spanner s most surprising revelation nosql is out and newsql is in first todd last hoff publication date 2012 09 24 accessdate 2012 10 07 ref history the term was first used by 451 group analyst matthew aslett in a 2011 research paper discussing the rise of new database systems as challengers to established vendors ref name aslett2010 many enterprise systems that handle high profile data e g financial and order processing systems also need to be able to scale but are unable to use nosql solutions because they cannot give up strong transactional and consistency requirements ref name aslett2010 cite web url http blogs the451group com information management 2011 04 06 what we talk about when we talk about newsql title what we talk about when we talk about newsql first matthew last aslett publisher 451 group publication date 2011 04 06 year 2010 accessdate 2012 10 07 ref ref cite web url http berlinbuzzwords de sessions keynote 0 title building spanner first alex last lloyd publisher berlin buzzwords publication date 2012 06 05 year 2012 accessdate 2012 10 07 ref the only options previously available for these organizations were to either purchase a more powerful single node machine or develop custom middleware that distributes queries over traditional dbms nodes both approaches are prohibitively expensive and thus are not an option for many thus in this paper aslett discusses how newsql upstarts are poised to challenge the supremacy of commercial vendors in particular oracle database oracle systems although newsql systems vary greatly in their internal architectures the two distinguishing features common amongst them is that they all support the relational model relational data model and use sql as their primary interface ref cite journal last1 cattell first1 r title scalable sql and nosql data stores doi 10 1145 1978915 1978919 journal acm sigmod record volume 39 issue 4 pages 12 year 2011 url http cattell net datastores datastores pdf pmid pmc ref the applications targeted by these newsql systems are characterized as having a large number of transactions that 1 are short lived i e no user stalls 2 touch a small subset of data using index lookups i e no full table scans or large distributed joins and 3 are repetitive i e executing the same queries with different inputs ref cite conference authorlink michael stonebraker first mike last stonebraker title the end of an architectural era it s time for a complete rewrite booktitle vldb 07 proceedings of the 33rd international conference on very large data bases location vienna austria year 2007 url http hstore cs brown edu papers hstore endofera pdf format pdf display authors etal ref these newsql systems achieve high performance and scalability by eschewing much of the legacy architecture of the original ibm system r design such as heavyweight algorithms for recovery and isolation exploiting semantics recovery or concurrency control algorithms ref cite journal last1 stonebraker first1 m last2 cattell first2 r doi 10 1145 1953122 1953144 title 10 rules for scalable performance in simple operation datastores journal communications of the acm volume 54 issue 6 pages 72 year 2011 pmid pmc ref one of the first known newsql systems is the h store parallel database parallel database system ref cite web url http blogs the451group com information management 2008 03 04 is h store the future of database management systems title is h store the future of database management systems first matthew last aslett year 2008 publication date 2008 03 04 accessdate 2012 07 05 ref ref cite web url http www zdnet com blog btl h store complete destruction of the old dbms order 8055 title h store complete destruction of the old dbms order first larry last dignan year 2008 accessdate 2012 07 05 ref newsql systems can be loosely grouped into three categories ref cite web url http www linuxforu com 2012 01 newsql handle big data title newsql the new way to handle big data first prasanna last venkatesh year 2012 publication date 2012 01 30 accessdate 2012 10 07 ref ref cite web url http www scalebase com the story of newsql title the newsql market breakdown first doron last levari year 2011 accessdate 2012 04 08 ref new architectures the first type of newsql systems are completely new database platforms these are designed to operate in a distributed cluster of shared nothing architecture shared nothing nodes in which each node owns a subset of the data these databases are often written from scratch with a distributed architecture in mind and include components such as distributed concurrency control flow control and distributed query processing example systems in this category are google spanner clustrix voltdb memsql pivotal labs pivotal s gemfire xd sap hana ref cite web title sap hana url http www sap com pc tech data management software extreme transaction oltp index html publisher sap accessdate 17 july 2014 ref nuodb tidb and trafodion ref cite web url http www trafodion org title trafodion transactional sql on hbase year 2014 ref sql engines the second category are highly optimized database engine storage engines for sql these systems provide the same programming interface as sql but scale better than built in engines such as innodb examples of these new storage engines include mysql cluster infobright tokudb and the now defunct infinidb transparent sharding these systems provide a shard database architecture sharding middleware layer to automatically split databases across multiple nodes scalebase is an example of this type of system see also transaction processing partition database references reflist 30em databases categories category data management category distributed data stores category newsql'
b'a single customer view is an aggregated consistent and holistic representation of the data known by an organisation about its customers ref http www experian co uk assets about us white papers single customer view whitepaper pdf exploiting the single customer view to maximise the value of customer relationships ref ref http www marketingweek co uk driving value from the single customer view 3015497 article driving value from the single customer view ref that can be viewed in one place such as a single page ref https spotlessdata com blog data driven marketing data driven marketing ref the advantage to an organisation of attaining this unified view comes from the ability it gives to analyse past behaviour in order to better target and personalise future customer interactions ref http www atominsight com about us blog single customer view essential why a single customer view is essential ref a single customer view is also considered especially relevant where organisations engage with customers through multichannel marketing since customers expect those interactions to reflect a consistent understanding of their history and preferences ref http econsultancy com uk blog 9612 the impact of a single customer view on consumer behaviour infographic the impact of a single customer view on consumer behaviour infographic ref however some commentators have challenged the idea that a single view of customers across an entire organisation is either natural or meaningful proposing that the priority should instead be consistency between the multiple views that arise in different contexts where representations of a customer are held in more than one data set achieving a single customer view can be difficult firstly because customer identity must be traceable between the records held in those systems and secondly because anomalies or discrepancies in the customer data must be data cleansing data cleansed ref http www atominsight com about us blog single customer view hard why building a single customer view isn t as easy as you might think ref as such the acquisition by an organisation of a single customer view is one potential outcome of successful master data management since 31 december 2010 maintaining a single customer view has become mandatory for united kingdom banks and other deposit takers due to new rules introduced by the financial services compensation scheme ref https www fscs org uk industry single customer view single customer view ref references reflist category identity management category business intelligence category data management category data warehousing category information technology management'
b'about mainframe computer file a general meaning in computing field data set refimprove date september 2014 in the context of ibm mainframe computer s a data set ibm preferred or dataset is a computer file having a record oriented file record organization use of this term began with os 360 and is still used by its successors including the current z os documentation for these systems historically preferred this term rather than computer file file a data set is typically stored on a direct access storage device dasd or magnetic tape however unit record devices such as punch card readers card punch and line printers can provide input output i o for a data set file ref http publib boulder ibm com infocenter zvm v5r4 index jsp topic com ibm zvm v54 hcpa7 hcse7b3050 htm ref data sets are not unstructured streams of byte s but rather are organized in various logical record and block structures determined by the code dsorg code data set organization code recfm code record format and other parameters these parameters are specified at the time of the data set allocation creation for example with job control language code dd code statements inside a job they are stored in the data control block dcb which is a data structure used to access data sets for example using access method s data set organization for os 360 the dcb s dsorg parameter specifies how the data set is organized it may be physically sequential ps indexed sequential is partitioned po or direct access da data sets on tape may only be dsorg ps the choice of organization depends on how the data is to be accessed and in particular how it is to be updated programmers utilize various access method s such as queued sequential access method qsam or vsam in programs for reading and writing data sets access method depends on the given data set organization record format recfm regardless of organization the physical structure of each record is essentially the same and is uniform throughout the data set this is specified in the dcb code recfm code parameter code recfm f code means that the records are of fixed length specified via the code lrecl code parameter and code recfm v code specifies a variable length record v records when stored on media are prefixed by a record descriptor word rdw containing the integer length of the record in bytes with code recfm fb code and code recfm vb code multiple logical records are grouped together into a single block data storage physical block on tape or disk fb and vb are code fixed blocked code and code variable blocked code respectively the code blksize code parameter specifies the maximum length of the block code recfm fbs code could be also specified meaning code fixed blocked standard code meaning all the blocks except the last one were required to be in full code blksize code length code recfm vbs code or code variable blocked spanned code means a logical record could be spanned across two or more blocks with flags in the rdw indicating whether a record segment is continued into the next block and or was continued from the previous one this mechanism eliminates the need for using any delimiter byte value to separate records thus data can be of any type including binary integers floating point or characters without introducing a false end of record condition the data set is an abstraction of a collection of records in contrast to files as unstructured streams of bytes anchor partitioned datasets partitioned data sets a partitioned data set pds is a data set containing multiple members each of which holds a separate sub data set similar to a directory file systems directory in other types of file system s this type of data set is often used to hold executable programs load modules source program libraries especially assembler macro definitions and job control language a pds may be compared to a zip file format zip file or com structured storage a partitioned data set can only allocate on a single volume with the maximum size of 65535 tracks besides members a pds consists also of their directory each member can be accessed directly using the directory structure once a member is located the data stored in that member is handled in the same manner as a ps sequential data set whenever a member is deleted the space it occupied is unusable for storing other data likewise if a member is re written it is stored in a new spot at the back of the pds and leaves wasted dead space in the middle the only way to recover dead space is to perform frequent file compression that moves all members to the front of the data space and leaves free usable space at the back note that in modern parlance this kind of operation might be called defragmentation or garbage collection computer science garbage collection data compression nowadays refers to a different more complicated concept pds files can only reside on disk in order to use the directory structure to access individual members not on tape they are most often used for storing multiple jcl files utility control statements and executable modules an improvement of this scheme is a partitioned data set extended pdse or pds e sometimes just libraries introduced with mvs xa system pds e structure is similar to pds and is used to store the same types of data however pds e files have a better directory structure which does not require pre allocation of directory blocks when the pds e is defined and therefore does not run out of directory blocks if not enough were specified also pds e automatically stores members in such a way that compression operation is not needed to reclaim dead space pds e files can only reside on disk in order to use the directory structure to access individual members see also volume table of contents vtoc a structure describing data sets stored on the disk distributed data management architecture references reflist http publib b boulder ibm com redbooks nsf redbookabstracts sg246366 html introduction to the new mainframe z os basics ch 5 working with data sets march 29 2011 isbn 0738435341 mainframe i o access methods defaultsort data set ibm mainframe category data management category ibm mainframe operating systems category computer file systems category computer files'
b'refimprove date september 2014 data access typically refers to software and activities related to storing retrieving or acting on data housed in a database or other information repository repository two fundamental types of data access exist sequential access as in magnetic tape data storage magnetic tape for example random access as in indexed digital media media data access crucially involves authorization to access different data repositories data access can help distinguish the abilities of administrators and users for example administrators may have the ability to remove edit and add data while general users may not even have read rights if they lack access to particular information historically each repository including each different database file system etc might require the use of different method computer science methods and languages and many of these repositories stored their content in different and incompatible formats over the years standardized languages methods and formats have developed to serve as interfaces between the often proprietary and always idiosyncratic specific languages and methods such standards include sql 1974 odbc ca 1990 jdbc xquery api for java xqj ado net xml xquery xpath 1999 and web services some of these standards enable translation of data from unstructured data unstructured such as html or free text files to structured data structured such as xml or sql structures such as connection string s and dburls ref cite web url http www quickprogrammingtips com java connecting to oracle database in java html title connecting to oracle database in java accessdate 2014 07 18 quote dburl is of the form jdbc oracle thin machinename 1521 databasename ref can attempt to standardise methods of database connection connecting to databases references reflist defaultsort data access category data management category data access technologies database stub'
b'merge from data consistency date november 2014 about consistency in distributed systems as defined in the cap theorem cap theorem consistency in database systems refers to the requirement that any given database transaction must change affected data only in allowed ways any data written to the database must be valid according to all defined rules including integrity constraints constraints cascading rollback cascades database trigger triggers and any combination thereof this does not guarantee correctness of the transaction in all ways the application programmer might have wanted that is the responsibility of application level code but merely that any programming errors cannot result in the violation of any defined rules as an acid guarantee consistency is one of the four guarantees that define acid database transaction transactions however significant ambiguity exists about the nature of this guarantee it is defined variously as the guarantee that any transactions started in the future necessarily see the effects of other transactions committed in the past ref name cap theorem paper http webpages cs luc edu pld 353 gilbert lynch brewer proof pdf brewer s conjecture and the feasibility of consistent available partition tolerant web services ref ref name ports et al cite journal url http drkp net papers txcache osdi10 pdf title transactional consistency and automatic management in an application data cache author1 ports d r k author2 clements a t author3 zhang i author4 madden s author5 liskov b journal mit csail ref the guarantee that relational database constraints database constraints are not violated particularly once a transaction commits ref name haerder reuter cite journal url http www minet uni jena de dbis lehre ws2005 dbs1 haerderreuter83 pdf title principles of transaction oriented database recovery author1 haerder t author2 reuter a journal computing surveys date december 1983 volume 15 issue 4 pages 287 317 ref ref cite web url http databases about com od specificproducts a acid htm title the acid model author mike chapple work about ref ref cite web url http msdn microsoft com en us library aa480356 aspx title acid properties publisher ref ref cite web url http www techopedia com definition 23949 atomicity consistency isolation durability acid title what is acid in databases definition from techopedia author cory janssen work techopedia com ref the guarantee that operations in transactions are performed accurately correctly and with validity with respect to application semantics ref cite web url http www iso org iso home store catalogue ics catalogue detail ics htm csnumber 27614 title iso iec 10026 1 1998 information technology open systems interconnection distributed transaction processing part 1 osi tp model publisher ref as these various definitions are not mutually exclusive it is possible to design a system that guarantees consistency in every sense of the word as most relational database management system s in common use today arguably do as a cap trade off the cap theorem is based on three trade offs one of which is atomic consistency shortened to consistency for the acronym about which the authors note discussing atomic consistency is somewhat different than talking about an acid database as database consistency refers to transactions while atomic consistency refers only to a property of a single request response operation sequence and it has a different meaning than the atomic in acid as it subsumes the database notions of both atomic and consistent ref name cap theorem paper see also consistency model cap theorem eventual consistency references reflist defaultsort consistency database systems category data management category transaction processing'
b'context date july 2014 file data flow diagram example jpg thumb 360px data flow diagram example ref john azzolini 2000 http ses gsfc nasa gov ses data 2000 000712 azzolini ppt introduction to systems engineering practices july 2001 ref a data flow diagram dfd is a graphical representation of the flow of data through an information system modelling its process aspects a dfd is often used as a preliminary step to create an overview of the system which can later be elaborated ref bruza p d van der weide th p the semantics of data flow diagrams university of nijmegen 1993 ref dfds can also be used for the data visualization visualization of data processing structured design a dfd shows what kind of information will be input to and output from the system where the data will come from and go to and where the data will be stored it does not show information about the timing of process or information about whether processes will operate in sequence or in parallel which is shown on a flowchart history larry constantine the original developer of structured design ref w stevens g myers l constantine http domino watson ibm com tchjr journalindex nsf d9f0a910ab8b637485256bc80066a393 a801ae3750be70ac85256bfa00685ded opendocument structured design ibm systems journal 13 2 115 139 1974 ref based on martin and estrin s data flow graph model of computation starting in the 1970s data flow diagrams dfd became a popular way to visualize the major steps and data involved in software system processes dfds were usually used to show data flow in a computer system although they could in theory be applied to business process modeling dfd were useful to document the major data flows or to explore a new high level design in terms of data flow ref craig larman applying uml and patterns pearson education isbn 978 81 7758 979 5 ref theory file dataflowdiagram example png thumb 360px data flow diagram example file data flow diagram notation svg thumb 160px data flow diagram edward yourdon yourdon tom demarco demarco notation data flow diagrams are also known as bubble charts ref http www orm net pdf jcm13 pdf introduced by clive finkelstein in australia caci in the uk and later writers such as james martin ref dfd is a designing tool used in the top down approach to systems design this context level dfd is next exploded to produce a level 1 dfd that shows some of the detail of the system being modeled the level 1 dfd shows how the system is divided into sub systems processes each of which deals with one or more of the data flows to or from an external agent and which together provide all of the functionality of the system as a whole it also identifies internal data stores that must be present in order for the system to do its job and shows the flow of data between the various parts of the system data flow diagrams are one of the three essential perspectives of the structured systems analysis and design method ssadm the sponsor of a project and the end users will need to be briefed and consulted throughout all stages of a system s evolution with a data flow diagram users are able to visualize how the system will operate what the system will accomplish and how the system will be implemented the old system s dataflow diagrams can be drawn up and compared with the new system s data flow diagrams to draw comparisons to implement a more efficient system data flow diagrams can be used to provide the end user with a physical idea of where the data they input ultimately has an effect upon the structure of the whole system from order to dispatch to report how any system is developed can be determined through a data flow diagram model in the course of developing a set of levelled data flow diagrams the analyst designer is forced to address how the system may be decomposed into component sub systems and to identify the transaction data in the data model data flow diagrams can be used in both analysis and design phase of the systems development life cycle sdlc there are different notations to draw data flow diagrams yourdon coad and chris gane computer scientist gane trish sarson sarson ref chris gane computer scientist chris gane and trish sarson structured systems analysis tools and techniques mcdonnell douglas systems integration company 1977 ref defining different visual representations for processes data stores data flow and external entities ref http www smartdraw com tutorials software dfd tutorial 01 htm how to draw data flow diagrams ref physical vs logical dfd a logical dfd captures the data flows that are necessary for a system to operate it describes the processes that are undertaken the data required and produced by each process and the stores needed to hold the data on the other hand a physical dfd shows how the system is actually implemented either at the moment current physical dfd or how the designer intends it to be in the future required physical dfd thus a physical dfd may be used to describe the set of data items that appear on each piece of paper that move around an office and the fact that a particular set of pieces of paper are stored together in a filing cabinet it is quite possible that a physical dfd will include references to data that are duplicated or redundant and that the data stores if implemented as a set of database table s would constitute an un normalised or de normalised relational database in contrast a logical dfd attempts to capture the data flow aspects of a system in a form that has neither redundancy nor duplication see also activity diagram business process model and notation control flow diagram data island dataflow directed acyclic graph drakon drakon chart functional flow block diagram function model idef0 logical data flow pipeline software pipeline structured analysis and design technique structure chart system context diagram value stream mapping workflow references reflist further reading scott w ambler http www agilemodeling com artifacts dataflowdiagram htm the object primer 3rd edition agile model driven development with uml 2 external links commons inline data model authority control defaultsort data flow diagram1 category information systems category data management category diagrams category visualization graphic category systems analysis'
b'infobox programming language name sql psm logo paradigm multi paradigm programming language multi paradigm year 1996 designer developer latest release version sql 2011 latest release date latest preview version latest preview date turing complete yes typing implementations mysql br ibm s sql pl influenced by ada programming language ada ref citation url http ocelot ca blog blog 2015 01 15 stored procedures critiques and defences title stored procedures critiques and defences year 2015 first1 peter last1 gulutzan ref influenced operating system cross platform cross platform multi platform license website file ext dialects wikibooks sql psm sql persistent stored modules is an iso standard mainly defining an extension of sql with a procedural language for use in stored procedure s initially published in 1996 as an extension of sql 92 iso iec 9075 4 1996 a version sometimes called psm 96 or even sql 92 psm ref cite journal last1 eisenberg first1 a title new standard for stored procedures in sql doi 10 1145 245882 245907 journal acm sigmod record volume 25 issue 4 pages 81 88 year 1996 pmid pmc ref sql psm was later incorporated into the multi part sql 1999 standard and has been part 4 of that standard since then most recently in sql 2011 the sql 1999 part 4 covered less than the original psm 96 because the sql statements for defining managing and invoking routines were actually incorporated into part 2 sql foundation leaving only the procedural language itself as sql psm ref cite book first1 jim last1 melton first2 alan r last2 simon title sql 1999 year 2002 publisher morgan kaufmann isbn 978 1 55860 456 8 pages 541 42 ref the sql psm facilities are still optional as far as the sql standard is concerned most of them are grouped in features p001 p008 sql psm standardizes syntax and semantics for control flow exception handling called condition handling in sql psm local variables assignment of expressions to variables and parameters and procedural use of cursor databases cursors it also defines an information schema metadata for stored procedures sql psm is one language in which method computer programming methods for the sql 1999 structured type s can be defined the other is java via sql jrt in practice mysql s procedural language and ibm s sql pl used in db2 are closest to the sql psm standard ref name harrisonfeuerstein2008 cite book first1 guy last1 harrison first2 steven last2 feuerstein title mysql stored procedure programming url https books google com books id ypep0ok0co4c pg pt75 year 2008 publisher o reilly isbn 978 0 596 10089 6 page 49 ref sql psm resembles and inspired by pl sql as well as pl pgsql so they are similar languages with postgresql v9 some sql psm features like overloading of sql invoked functions and procedures ref citation publisher postgresql title sql standard features edition 9 contribution url http www postgresql org docs 9 0 static features sql standard html contribution feature t322 ref are now supported a postgresql addon implements sql psm ref citation url https github com okbob plpsm0 format git type repository title plpsm0 ref ref citation publisher postgresql url http www postgresql org message id 1305291347 14548 13 camel jara office nic cz date may 2011 title announce ref ref http www postgresql org message id cafj8prdwfdcjnsnwqb 3j1 rmo6b8 tmltnbvdcsprrow2dfeg mail gmail com 2012 2 s proposal pl pgpsm announce ref ref citation title sql psm format wiki url http postgres cz wiki sql psm manual publisher postgresql type manual year 2008 ref alongside its own procedural language although it is not part of the core product ref citation contribution url http www postgresql org docs 9 2 static features html publisher postgresql title documentation edition 9 2 contribution sql conformance ref see also the following implementations adopt the standard but they are not 100 compatible to sql psm open source hsqldb stored procedures and functions ref name sql psm routines http hsqldb org doc 2 0 guide sqlroutines chapt html src psm routines ref mysql stored procedures ref name harrisonfeuerstein2008 postgresql pl pgsql proprietary oracle pl sql microsoft and sybase transact sql references reflist further reading jim melton understanding sql s stored procedures a complete guide to sql psm morgan kaufmann publishers 1998 isbn 1 55860 461 8 sql notoc defaultsort sql psm category data management category sql category data centric programming languages category programming languages created in 1996 compu lang stub database stub'
b'pl perl procedural language perl is a procedural language supported by the postgresql rdbms pl perl as an imperative programming language allows more control than the relational algebra of sql programs created in the pl perl language are called functions and can use most of the features that the perl perl programming language provides including common flow control structures and syntax that has incorporated regular expressions directly these functions can be evaluated as part of a sql statement or in response to a database trigger trigger or constraint database rule the design goals of pl perl were to create a loadable procedural language that can be used to create functions and trigger procedures adds control structures to the sql language can perform complex computations can be defined to be either http www postgresql org docs current static plperl trusted html trusted or untrusted by the server is easy to use pl perl is one of many pl languages available for postgresql pl pgsql http gborg postgresql org project pljava projdisplay php pl java http plphp commandprompt com plphp http www postgresql org docs current interactive plpython html pl python http www joeconway com plr pl r http raa ruby lang org list rhtml name pl ruby pl ruby http plsh projects postgresql org pl sh and http www postgresql org docs current interactive pltcl html pl tcl references http www postgresql org docs current static plperl html postgresql pl perl documentation defaultsort pl perl category data management category postgresql category data centric programming languages'
b'multiple issues refimprove date august 2011 update date march 2015 an xml database is a data persistence software system that allows data to be specified and sometimes stored in xml format this data can be xquery queried transformed exported and returned to a calling system xml databases are a flavor of document oriented database s which are in turn a category of nosql database rationale for xml in databases there are a number of reasons to directly specify data in xml or other document formats such as json for xml in particular they include ref name nicola2010 cite web last1 nicola first1 matthias title 5 reasons for storing xml in a database url http nativexmldatabase com 2010 09 28 5 reasons for storing xml in a database website native xml database accessdate 17 march 2015 date 28 september 2010 ref ref name feldman2013 cite conference last1 feldman first1 damon title moving from relational modeling to xml and marklogic data models url http www marklogic com resources slides moving from relational modeling to xml and marklogic data models resource download presentations conference marklogic world conferenceurl http world marklogic com date 11 april 2013 accessdate 17 march 2015 ref an enterprise may have a lot of xml in an existing standard format data may need to be exposed or ingested as xml so using another format such as relational forces double modeling of the data xml is very well suited to sparse data deeply nested data and mixed content such as text with embedded markup tags xml is human readable whereas relational tables require expertise to access metadata is often available as xml semantic web data is available as rdf xml steve o connell gives one reason for the use of xml in databases the increasingly common use of xml for transport layer data transport which has meant that data is extracted from databases and put into xml documents and vice versa ref name oconnell2005 cite report author o connell steve work advanced databases course notes title section 9 2 type syllabus date 2005 publisher university of southampton location southampton england ref update inline date march 2015 it may prove more efficient in terms of conversion costs and easier to store the data in xml format in content based applications the ability of the native xml database also minimizes the need for extraction or entry of metadata to support searching and navigation xml enabled databases xml enabled databases typically offer one or more of the following approaches to storing xml within the traditional relational structure xml is stored into a clob character large object xml is shredded into a series of tables based on a schema ref name oracle cite book title oracle xml db developer s guide 10 g release 2 date august 2005 publisher oracle corporation chapter url http docs oracle com cd b19306 01 appdev 102 b14259 xdb05sto htm accessdate 17 march 2015 chapter xml schema storage and query basic section http docs oracle com cd b19306 01 appdev 102 b14259 xdb05sto htm i1042421 creating xmltype tables and columns based on xml schema ref xml is stored into a native xml type as defined by iso standard 9075 14 ref name iso9075 2011 cite web title iso iec 9075 14 2011 information technology database languages sql part 14 xml related specifications sql xml url http www iso org iso home store catalogue ics catalogue detail ics htm csnumber 53686 publisher international organization for standardization accessdate 17 march 2015 date 2011 ref rdbms that support the iso xml type are ibm db2 purexml ref name db2purexml cite web title purexml overview db2 as an xml database url http www 01 ibm com support knowledgecenter ssepgg 10 1 0 com ibm db2 luw xml doc doc c0022308 html website ibm knowledge center publisher ibm accessdate 17 march 2015 ref microsoft sql server ref name sqlserver2005 cite web title using xml in sql server url https msdn microsoft com en us library ms190936 aspx website microsoft developer network publisher microsoft corporation accessdate 17 march 2015 ref oracle database ref name oracle2 cite book title oracle xml db developer s guide 10 g release 2 date august 2005 publisher oracle corporation chapter url http docs oracle com cd b19306 01 appdev 102 b14259 xdb04cre htm accessdate 17 march 2015 chapter xmltype operations ref postgresql ref name postgresql cite book title postgresql 9 0 19 documentation chapter url http www postgresql org docs 9 0 static datatype xml html accessdate 17 march 2015 chapter 8 13 xml type ref ref http www postgresql org docs 9 0 static datatype xml html postgresql data types xml type ref typically an xml enabled database is best suited where the majority of data are non xml for datasets where the majority of data are xml a native xml databases native xml database is better suited example of xml type query in ibm db2 sql source lang sql select id vol xmlquery j name passing journal as j as name from journals where xmlexists j licence creativecommons passing journal as j source native xml databases these databases are typically better when much of the data is in xml or other non relational formats fact date august 2015 basex berkeley db xml edition exist marklogic server qizx sedna database sedna all the above databases uses xml as an interface to specify documents as tree structured data that may contain unstructured text but on disk the data is stored as optimized binary files this makes query and retrieval faster for marklogic it also allows xml and json to co exist in one binary format ref cite book last1 siegel first1 erik last2 retter first2 adam title exist date december 2014 publisher o reilly associates isbn 978 1 4493 3710 0 url https www safaribooksonline com library view exist 9781449337094 ch04 html accessdate 18 march 2015 chapter 4 architecture ref key features of native xml databases include has an xml document as at least one fundamental unit of logical storage just as a relational database has a row in a table as a fundamental unit of logical storage need not have any particular underlying physical storage model for example nxds can use optimized proprietary storage formats this is a key aspect of xml databases managing xml as large strings is inefficient due to the extra markup in xml compressing and indexing xml allows the illusion of directly accessing querying and transforming xml while gaining the performance advantages of working with optimized binary tree structures ref name kellogg2010 cite web last1 kellogg first1 dave title yes virginia marklogic is a nosql system url http kellblog com 2010 04 11 yes virginia marklogic is a nosql system website kellblog accessdate 18 march 2015 date 11 april 2010 ref the standards for xml querying per w3c recommendation are xquery 1 0 and xquery 3 0 citation needed date march 2015 xquery includes xpath as a sub language and xml itself is a valid sub syntax of xquery in addition to xpath xml databases support xslt as a method of transforming documents or query results retrieved from the database xslt provides a declarative language written using an xml grammar it aims to define a set of xpath filter software filter s that can transform documents in part or in whole into other formats including plain text xml or html but big picture xml persistence describes only one format in the larger faster moving nosql movement at this time many databases support xml plus other formats even if xml is internally stored as an optimized high performance format and is a first class citizen within the database see google trends link above to see relative popularity of terms language features class wikitable sortable name license native language xquery 3 0 xquery update xquery full text expath extensions exquery extensions xslt 2 0 basex bsd license java yes yes yes yes yes yes exist lgpl lgpl license java partial proprietary proprietary no yes yes marklogic server commercial software commercial c partial proprietary proprietary no no yes qizx commercial software commercial java yes yes yes no no yes supported apis class wikitable sortable name xquery api for java xqj xml db representational state transfer restful restxq webdav basex yes yes yes yes yes exist yes yes yes yes yes marklogic server yes no yes yes yes qizx no no yes no no sedna yes yes no no no references reflist refbegin external links https web archive org web 20150906171257 http db engines com en ranking native xml dbms db engines ranking of native xml dbms by popularity updated monthly http www cfoster net articles xmldb business case xml databases the business case charles foster june 2008 talks about the current state of databases and data persistence how the current relational database model is starting to crack at the seams and gives an insight into a strong alternative for today s requirements http urn kb se resolve urn urn nbn se liu diva 3717 an xml based database of molecular pathways 2005 06 02 speed performance comparisons of exist x hive sedna and qizx open https web archive org web 20070922082133 http swing felk cvut cz index php option com docman task doc view gid 5 itemid 62 xml native database systems review of sedna ozone neocorexms 2006 http csdl2 computer org persagen dlabstoc jsp resourcepath dl mags ic toc comp mags ic 2005 02 w2toc xml doi 10 1109 mic 2005 48 xml data stores emerging practices bhargava p rajamani h thaker s agarwal a 2005 xml enabled relational databases texas the university of texas at austin https web archive org web 20070113224941 http xmldb org sourceforge net initiative for xml databases http www rpbourret com xml xmlanddatabases htm xml and databases ronald bourret september 2005 https web archive org web 20071011101718 http cafe elharo com xml the state of native xml databases the state of native xml databases elliotte rusty harold august 13 2007 official website https www qualcomm com qizx qualcomm qizx official website dead link date july 2016 bot internetarchivebot fix attempted yes refend database models databases category xml category data management category data modeling category xml databases'
b'in computer science an inverted index also referred to as postings file or inverted file is an index database index data structure storing a mapping from content such as words or numbers to its locations in a table database database file or in a document or a set of documents named in contrast to a forward index which maps from documents to content the purpose of an inverted index is to allow fast full text search es at a cost of increased processing when a document is added to the database the inverted file may be the database file itself rather than its index database index it is the most popular data structure used in document retrieval systems ref harvnb zobel moffat ramamohanarao 1998 ref none ref used on a large scale for example in search engine s additionally several significant general purpose mainframe computer mainframe based database management systems have used inverted list architectures including adabas datacom db and model 204 there are two main variants of inverted indexes a record level inverted index or inverted file index or just inverted file contains a list of references to documents for each word a word level inverted index or full inverted index or inverted list additionally contains the positions of each word within a document ref name isbn0 201 39829 x p192 harvnb baeza yates ribeiro neto 1999 p 192 ref byr99 ref the latter form offers more functionality like phrase search es but needs more processing power and space to be created applications the inverted index data structure is a central component of a typical index search engine search engine indexing algorithm a goal of a search engine implementation is to optimize the speed of the query find the documents where word x occurs once a search engine indexing the forward index forward index is developed which stores lists of words per document it is next inverted to develop an inverted index querying the forward index would require sequential iteration through each document and to each word to verify a matching document the time memory and processing resources to perform such a query are not always technically realistic instead of listing the words per document in the forward index the inverted index data structure is developed which lists the documents per word with the inverted index created the query can now be resolved by jumping to the word id via random access in the inverted index in pre computer times concordance publishing concordances to important books were manually assembled these were effectively inverted indexes with a small amount of accompanying commentary that required a tremendous amount of effort to produce in bioinformatics inverted indexes are very important in the sequence assembly of short fragments of sequenced dna one way to find the source of a fragment is to search for it against a reference dna sequence a small number of mismatches due to differences between the sequenced dna and reference dna or errors can be accounted for by dividing the fragment into smaller fragments at least one subfragment is likely to match the reference dna sequence the matching requires constructing an inverted index of all substrings of a certain length from the reference dna sequence since the human dna contains more than 3 billion base pairs and we need to store a dna substring for every index and a 32 bit integer for index itself the storage requirement for such an inverted index would probably be in the tens of gigabytes see also index search engine reverse index vector space model bibliography cite book last knuth first d e authorlink donald knuth title the art of computer programming publisher addison wesley edition third year 1997 origyear 1973 location reading massachusetts isbn 0 201 89685 0 ref knu97 chapter 6 5 retrieval on secondary keys cite journal last1 zobel first1 justin last2 moffat first2 alistair last3 ramamohanarao first3 kotagiri date december 1998 title inverted files versus signature files for text indexing journal acm transactions on database systems volume 23 issue 4 pages 453 490 publisher association for computing machinery location new york doi 10 1145 296854 277632 url accessdate cite journal last1 zobel first1 justin last2 moffat first2 alistair date july 2006 title inverted files for text search engines journal acm computing surveys volume 38 issue 2 page 6 publisher association for computing machinery location new york doi 10 1145 1132956 1132959 url accessdate cite book last baeza yates first ricardo authorlink ricardo baeza yates author2 ribeiro neto berthier title modern information retrieval publisher addison wesley longman location reading massachusetts year 1999 isbn 0 201 39829 x oclc doi ref byr99 page 192 cite journal last salton first gerard author2 fox edward a author3 wu harry title extended boolean information retrieval publisher acm year 1983 journal commun acm volume 26 issue 11 doi 10 1145 182 358466 page 1022 cite book title information retrieval implementing and evaluating search engines url http www ir uwaterloo ca book publisher mit press year 2010 location cambridge massachusetts isbn 978 0 262 02651 2 author8 stefan b uuml ttcher charles l a clarke and gordon v cormack references reflist external links https xlinux nist gov dads html invertedindex html nist s dictionary of algorithms and data structures inverted index http mg4j di unimi it managing gigabytes for java a free full text search engine for large document collections written in java http lucene apache org java docs lucene apache lucene is a full featured text search engine library written in java http sphinxsearch com sphinx search open source high performance full featured text search engine library used by craigslist and others employing an inverted index http rosettacode org wiki inverted index example implementations on rosetta code http www vision caltech edu malaa software research image search caltech large scale image search toolbox a matlab toolbox implementing inverted file bag of words image search category data management category search algorithms category database index techniques category substring indices'
b'infobox website logo 250px url url http data europa eu euodp commercial no type public services public service web portal portal and br institutional information registration not required language 24 official languages of the eu owner flag european union content license open author publications office european union eu publications office launch date december 2012 the eu open data portal is the single point of access to a wide range of data held by eu institutions agencies and other bodies the portal is a key element of eu open data strategy legal basis and launch date launched in december 2012 in beta mode the portal was formally established by commission decision of 12 december 2011 2011 833 eu on the reuse of commission documents to promote accessibility and reuse ref name r1 cite news url http eur lex europa eu legal content en txt uri celex 32011d0833 title commission decision of 12 december 2011 2011 833 eu ref while the operational management of the portal is the task of the publications office of the european union implementation of eu open data policy is the responsibility of the directorate general for communications networks content and technology of the european commission features the portal allows anyone to easily search explore link download and reuse the data for commercial or non commercial purposes through a catalogue of common metadata through this catalogue users access data stored on the websites of the eu institutions agencies and other bodies semantic technologies offer new functionalities the metadata catalogue can be searched via an interactive search engine data tab and through sparql queries linked data tab there is also a showcase of visualisation applications from various eu institutions agencies and other bodies users can suggest data they would like the portal to be linked to give feedback on the quality of data obtainable and share information with other users about how they have used it the interface is in 24 eu official languages while most metadata are currently available in a limited number of languages english french and german some of the metadata e g names of the data providers geographical coverage are in 24 languages following the translation of controlled vocabulary lists that are used by the portal ref name r3 cite news url http publications europa eu mdr authority index html title eu controlled vocabularies ref terms of reuse most data accessible via the eu open data portal are covered by the europa legal notice ref name r6 cite news url http ec europa eu geninfo legal notices en htm title europa legal notice ref and can be reused free of charge for commercial and non commercial purposes provided that the source is acknowledged specific conditions on reuse related mostly to the protection of third party intellectual property rights apply for a very limited amount of data data available the portal contains a very wide variety of high value open data across eu policy domains as also more recently identified by the g8 open data charter these include the economy employment science environment and education the number of data providers which include eurostat the european environment agency and the joint research centre continues to grow so far around 56 eu institutions bodies or departments e g eurostat the european environment agency the joint research centre and other european commission directorates general and eu agencies have made datasets available making a total of over 7 800 in addition to giving access to datasets the portal also is an easy entry point to a whole range of visualisation applications using eu data the applications are displayed as much for their information value as for giving examples of what applications can be made using the data architecture of the portal the portal is built using open source solutions such as the drupal content management system and ckan the data catalogue software developed by the open knowledge foundation it uses virtuoso as an resource description framework rdf database and has a sparql endpoint its metadata catalogue is built on the basis of international standards such as dublin core the data catalogue vocabulary dcat and the asset description metadata schema adms ref name r5 cite news url http ec europa eu digital agenda en open data portals title open data portals in europe ref see also open data institutions of the european union agencies of the european union bodies of the european union european data portal references reflist external links http ec europa eu europe2020 index en htm europe 2020 official eu site http ec europa eu digital agenda digital agenda for europe http ec europa eu digital agenda en open data 0 open data section of above site https joinup ec europa eu community ods description joinup community on eu open data http eur lex europa eu legal content en txt uri celex 52011dc0882 communication open data an engine for innovation growth and transparent governance https ec europa eu digital agenda en legislative measures legal rules on public services information http okfn org open knowledge foundation http dublincore org dublin core http latc project eu publication and usage of linked data on the web http datacatalogs org group eu official data catalogues http 5stardata info open data classification by tim berners lee http opendatachallenge org open data challenge now over category european commission category open data category transparency behavior category open government category semantic web category data management category creative commons'
b'distinguish data cluster redirect2 network filesystem parallel file system the sun nfs protocol network file system the ibm gpfs protocol ibm general parallel file system multiple refimprove date december 2015 cleanup date december 2013 reason merges need to be smoothed over a clustered file system is a file system which is shared by being simultaneously mount computing mounted on multiple server computing servers there are several approaches to computer cluster clustering most of which do not employ a clustered file system only direct attached storage for each node clustered file systems can provide features like location independent addressing and redundancy which improve reliability or reduce the complexity of the other parts of the cluster parallel file systems are a type of clustered file system that spread data across multiple storage nodes usually for redundancy or performance ref http www dell com downloads global power ps2q05 20040179 saify oe pdf ref anchor shared disk shared disk file system a shared disk file system uses a storage area network storage area network san to allow multiple computers to gain direct disk access at the block data storage block level access control and translation from file level operations that applications use to block level operations used by the san must take place on the client node the most common type of clustered file system the shared disk file system mdash by adding mechanisms for concurrency control mdash provides a consistent and serialization serializable view of the file system avoiding corruption and unintended data loss even when multiple clients try to access the same files at the same time shared disk file systems commonly employ some sort of fencing computing fencing mechanism to prevent data corruption in case of node failures because an unfenced device can cause data corruption if it loses communication with its sister nodes and tries to access the same information other nodes are accessing the underlying storage area network may use any of a number of block level protocols including scsi iscsi hyperscsi ata over ethernet aoe fibre channel network block device and infiniband there are different architectural approaches to a shared disk filesystem some distribute file information across all the servers in a cluster fully distributed others utilize a centralized metadata server both achieve the same result of enabling all servers to access all the data on a shared storage device citation needed date december 2009 examples div col 25em blue whale clustered file system bwfs silicon graphics sgi clustered file system cxfs veritas cluster file system dataplow nasan file system ibm general parallel file system gpfs lizardfs lustre file system lustre microsoft cluster shared volumes csv ocfs oracle cluster file system ocfs polyserve storage solutions quantum corporation quantum stornext file system stornext file system snfs ex adic ex centravision file system cvfs red hat global file system gfs sun qfs terrascale technologies terrafs versity vsm vmware vmfs xsan div col end anchor distributed fs distributed file systems distributed file systems do not share block level access to the same storage but use a network protocol computing protocol ref silberschatz galvin 1994 operating system concepts chapter 17 distributed file systems addison wesley publishing company isbn 0 201 59292 4 ref ref name ostep 1 citation title sun s network file system url http pages cs wisc edu remzi ostep dist nfs pdf publisher arpaci dusseau books date 2014 first1 remzi h last1 arpaci dusseau first2 andrea c last2 arpaci dusseau ref these are commonly known as network file systems even though they are not the only file systems that use the network to send data citation needed date march 2013 distributed file systems can restrict access to the file system depending on access list s or capability based security capabilities on both the servers and the clients depending on how the protocol is designed the difference between a distributed file system and a distributed data store is that a distributed file system allows files to be accessed using the same interfaces and semantics as local files snd for example mounting unmounting listing directories read write at byte boundaries system s native permission model distributed data stores by contrast require using a different api or library and have different semantics most often those of a database cn date june 2016 a distributed file system may also be created by software implementing ibm s distributed data management architecture ddm in which programs running on one computer use local interfaces and semantics to create manage and access files located on other networked computers all such client requests are trapped and converted to equivalent messages defined by the ddm using protocols also defined by the ddm these messages are transmitted to the specified remote computer on which a ddm server program interprets the messages and uses the file system interfaces of that computer to locate and interact with the specified file design goals distributed file systems may aim for transparency in a number of aspects that is they aim to be invisible to client programs which see a system which is similar to a local file system behind the scenes the distributed file system handles locating files transporting data and potentially providing other features listed below access transparency is that clients are unaware that files are distributed and can access them in the same way as local files are accessed location transparency a consistent name space exists encompassing local as well as remote files the name of a file does not give its location concurrency transparency all clients have the same view of the state of the file system this means that if one process is modifying a file any other processes on the same system or remote systems that are accessing the files will see the modifications in a coherent manner failure transparency the client and client programs should operate correctly after a server failure heterogeneity file service should be provided across different hardware and operating system platforms scalability the file system should work well in small environments 1 machine a dozen machines and also scale gracefully to huge ones hundreds through tens of thousands of systems replication transparency to support scalability we may wish to replicate files across multiple servers clients should be unaware of this migration transparency files should be able to move around without the client s knowledge history the incompatible timesharing system used virtual devices for transparent inter machine file system access in the 1960s more file servers were developed in the 1970s in 1976 digital equipment corporation created the file access listener fal an implementation of the data access protocol as part of decnet phase ii which became the first widely used network file system in 1985 sun microsystems created the file system called network file system protocol network file system nfs which became the first widely used internet protocol based network file system ref name ostep 1 other notable network file systems are andrew file system afs apple filing protocol afp netware core protocol ncp and server message block smb which is also known as common internet file system cifs in 1986 ibm announced client and server support for distributed data management architecture ddm for the system 36 system 38 and ibm mainframe computers running cics this was followed by the support for ibm personal computer as 400 ibm mainframe computers under the mvs and vse operating system vse operating systems and flexos ddm also became the foundation for drda distributed relational database architecture also known as drda examples main list of file systems distributed file systems l1 list of distributed file systems div col 25em beegfs fraunhofer ceph software ceph inktank red hat suse distributed file system microsoft windows distributed file system dfs microsoft infinit file system infinit gfarm file system gfarmfs glusterfs red hat google file system gfs google inc hadoop distributed file system hdfs apache software foundation interplanetary file system ipfs irods lizardfs skytechnology moose file system moosefs core technology gemius objectivefs onefs emc isilon openio orangefs clemson university omnibond systems formerly parallel virtual file system panasas panfs panasas parallel virtual file system clemson university argonne national laboratory ohio supercomputer center rozofs rozo systems torus coreos xtreemfs div col end network attached storage main network attached storage network attached storage nas provides both storage and a file system like a shared disk file system on top of a storage area network san nas typically uses file based protocols as opposed to block based protocols a san would use such as network file system protocol nfs popular on unix systems smb cifs server message block server message block common internet file system used with ms windows systems apple filing protocol afp used with macintosh apple macintosh computers or ncp used with novell open enterprise server oes and netware novell netware design considerations avoiding single point of failure the failure of disk hardware or a given storage node in a cluster can create a single point of failure that can result in data loss or unavailability fault tolerance and high availability can be provided through replication computing data replication of one sort or another so that data remains intact and available despite the failure of any single piece of equipment for examples see the lists of list of file systems distributed fault tolerant file systems distributed fault tolerant file systems and list of file systems distributed parallel fault tolerant file systems distributed parallel fault tolerant file systems performance a common performance measurement of a clustered file system is the amount of time needed to satisfy service requests in conventional systems this time consists of a disk access time and a small amount of central processing unit cpu processing time but in a clustered file system a remote access has additional overhead due to the distributed structure this includes the time to deliver the request to a server the time to deliver the response to the client and for each direction a cpu overhead of running the communication protocol software concurrency concurrency control becomes an issue when more than one person or client is accessing the same file or block and want to update it hence updates to the file from one client should not interfere with access and updates from other clients this problem is more complex with file systems due to concurrent overlapping writes where different writers write to overlapping regions of the file concurrently ref pessach yaniv 2013 distributed storage concepts algorithms and implementations isbn 978 1482561043 ref this problem is usually handled by concurrency control or lock computer science locking which may either be built into the file system or provided by an add on protocol history ibm mainframes in the 1970s could share physical disks and file systems if each machine had its own channel connection to the drives control units in the 1980s digital equipment corporation s tops 20 and openvms clusters vax alpha ia64 included shared disk file systems citation needed date may 2016 see also div col 25em network attached storage storage area network shared resource direct attached storage peer to peer file sharing disk sharing distributed data store distributed file system for cloud global file system gopher protocol list of file systems distributed file systems list of distributed file systems cachefs raid div col end references reflist further reading http www cloudbus org reports distributedstoragetaxonomy pdf a taxonomy of distributed storage systems http trac nchc org tw grid raw attachment wiki jazz 09 05 22 a taxonomy and survey on distributed file systems pdf a taxonomy and survey on distributed file systems http www cis upenn edu bcpierce courses dd papers satya89survey ps a survey of distributed file systems http www snia europe org objects store christian bandulet sniatutorial 20basics evolutionfilesystems pdf the evolution of file systems file systems state collapsed category computer file systems category shared disk file systems category storage area networks category distributed file systems category data management category distributed data storage category network file systems'
b'orphan date may 2014 the goal of content oriented workflow models is to articulate workflow progression by the presence of content units like data records objects documents most content oriented workflow approaches provide a life cycle model for content units such that workflow progression can be qualified by conditions on the state of the units most approaches are research and work in progress and the content models and life cycle models are more or less formalized the term content oriented workflows is an umbrella term for several scientific workflow approaches namely data driven resource driven artifact centric object aware and document oriented thus the meaning of content ranges from simple data attributes to self contained documents the term content oriented workflows appeared at first in ref name neumann2010 as an umbrella term such general term independent from a specific approach is necessary to contrast the content oriented modelling principle with traditional activity oriented workflow models like petri net s or business process model and notation bpmn where a workflow is driven by a control flow and where the content production perspective is neglected or even missing the term content was chosen to subsume the different levels in granularity of the content units in the respective workflow models it was also chosen to make associations with content management both terms artifact centric and data driven would also be good candidates for an umbrella term but each is closely related to a specific approach of a single working group the artifact centric group itself i e ibm research has generalized the characteristics of their approach and has used information centric as an umbrella term in ref name kumaran2008 yet the term information is too unspecific in the context of computer science thus content orientated workflows is considered as good compromise workflow model approaches data driven the data driven process structures provides a sophisticated workflow model being specialized on hierarchical write and review processes the approach provides interleaved synchronization of sub processes and extends activity diagrams unfortunately the corepro prototype implementation is not publicly available research on the project had been ceased the general idea has been continued by reichert in form of the object aware approach synonyms data driven process structures data driven modeling and coordination protagonists dr dominic m\xc3\xbcller university of twente joachim herbst daimlerchrysler research and manfred reichert at this time http wwwhome cs utwente nl reichertm index01 htm assoc prof at univ of twente currently http www uni ulm de en in institute of databases and information systems staff manfred reichert html prof at ulm univ organization s university of twente daimlerchrysler period 2005 2007 selected publications ref name mueller2006 ref name mueller2007 implementation http www utwente nl ewi is research completed projects completed projects corepro doc corepro resource driven the resource driven workflow system is an early approach that considered workflows from a content oriented perspective and emphasizes on the missing support for plain document driven processes by traditional activity oriented workflow engines the resource driven approach demonstrated the application of database triggers for handling workflow events still the system implementation is centralized and the workflow schema is statically defined the project appeared in 2005 but many aspects are considered future work by the authors research did not continue on the project wang completed his phd thesis in 2009 yet his thesis does not mention the resource driven approach to workflow modelling but is about discrete event simulation synonyms resource based workflows document driven workflow systems protagonists jianrui wang and http www personal psu edu axk41 prof akhil kumar organization pennsylvania state university period 2005 today selected publications ref name wang2005 ref name kumar2010 implementation n a artifact centric see also artifact centric business process model the artifact centric approach appears as a mature framework for general purpose content oriented workflows the distribution of the enterprise application landscape with its business services is considered yet the workflow engine itself seems to be centralized the process enactment seems to be tightly coupled with a technically pre integrated database management system infrastructure the latter makes it most suitable for manufacturing process or for organizational processes within a well defined institutional scope the approach remains work in progress still it is a relatively old and established project on content oriented workflows funded by ibm it has comparably high number of developers it is a promising approach synonyms artifact centric business process models artifact based business process acp artifact centric workflows protagonists http domino research ibm com comm research people nsf pages hull index html richard hull and dr kamal bhattacharya as well as cagdas e gerede and jianwen su organization ibm t j watson research center ny period 2007 today selected publications ref name bhattacharya2007 ref name calvanese2009 implementation http domino research ibm com comm research projects nsf pages artifact index html artifact object aware the object aware approach manages a set of object types and generates forms for creating object instances the form completion flow is controlled by transitions between object configurations each describing a progressing set of mandatory attributes each object configuration is named by an object state the data production flow is user shifting and it is discrete by defining a sequence of object states the discussion is currently limited to a centralized system without any workflows across different organizations however the approach is of great relevance to many domains like concurrent engineering finally the object aware approach and its philharmonicflows system are going to provide general purpose workflow systems for generic enactment of data production processes synonyms object aware process management datenorientiertes prozess management system protagonists http www uni ulm de en in institute of databases and information systems staff vera kuenzle html vera k\xc3\xbcnzle and http www uni ulm de en in institute of databases and information systems staff manfred reichert html prof manfred reichert organization ulm university period 2009 today selected publications ref name kuenzle2009 ref name kuenzle2010 implementation http www uni ulm de en in institute of databases and information systems research projects philharmonic flows html philharmonicflows distributed document oriented distributed document oriented process management ddpm enables distributed case handling in heterogeneous system environments and it is based on document oriented semantic integration integration the workflow model reflects the paper based working practice in inter institutional healthcare scenarios it targets distributed knowledge driven ad hoc workflows wherein distributed information systems are required to coordinate work with initially unknown sets of actors and activities the distributed workflow engine supports process planning process history as well as participant management and process template creation with import export the workflow engine embeds a functional fusion of 1 group based instant messaging 2 with a shared work list editor 3 with version control the software implementation of ddpm is \xce\xb1 flow which is available as open source ddpm and \xce\xb1 flow provide a content oriented approach to schema less workflows the complete distributed case handling application is provided in form of a single active document alpha doc the \xce\xb1 doc is a case file as information carrier with an embedded workflow engine in form of active properties inviting process participants is equivalent to providing them with a copy of an \xce\xb1 doc copying it like an ordinary desktop file all \xce\xb1 docs that belong to the same case can synchronize each other based on the participant management electronic postboxes store and forward messaging and an offline capable synchronization protocol synonyms distributed document oriented process management ddpm distributed case handling via active documents protagonists http www6 informatik uni erlangen de people cpn christoph p neumann and http www6 informatik uni erlangen de people lenz prof richard lenz organization friedrich alexander universit auml t erlangen n uuml rnberg period 2009 2012 selected publications ref name neumann2011 ref name neumann2012 and a phd thesis ref name dissneumann2012 implementation https github com cpnatwork alphaflow dev alpha flow open source related concepts content management the bandwidth of content management system s cms reaches from web content management system s wcms and document management system dms to enterprise content management ecm mature dms products support document production workflows in a basic form primarily focusing on review cycle workflows concerning a single document market leaders are alfresco software alfresco exo platform and emc with documentum groupware and computer supported cooperative work groupware focuses on messaging like e mail chat and instant messaging shared calendars e g lotus notes microsoft outlook with exchange server and conferencing e g skype groupware overlaps with computer supported cooperative work cscw that originated from shared multimedia editors for live drawing sketching and synchronous multi user applications like desktop sharing the extensive conceptual claim of cswc must be put into perspective by its actual solution scope that is available as the cscw cscw matrix cscw matrix case handling the case handling paradigm stems from prof van der aalst and gained momentum in 2005 the core features are a provide all information available i e present the case as a whole rather than showing bits and pieces b decide about activities on the basis of the information available rather than the activities already executed c separate work distribution from authorization and allow for additional types of roles not just the execute role and d allow workers to view and add modify data before or after the corresponding activities have been executed in healthcare the flow of a patient between healthcare professionals is considered as a workflow with activities that include all kinds of diagnostic or therapeutic treatments the workflow is considered as a case and workflow management in healthcare is to handle these cases case handling is orthogonal to content oriented workflows some content oriented workflow approaches are not related to case handling but for example to automated manufacturing in contrast systems that are considered to be case handling systems chs but which do not apply a content oriented workflow model are for example bpmone formerly protos and flower from pallas athena echo from digital cmdt from icl and vectus from london bridge group in conclusion those content oriented workflow approaches that are tightly related to case handling are the resource driven workflow model and the distributed document oriented workflow model protagonists http wwwis win tue nl wvdaalst prof wil van der aalst and associate professor dr http reijers com hajo reijers with focus on healthcare organization univ of technology eindhoven period 2001 today selected publication ref name reijers2003 ref name aalst2005 see also process philosophy workflow adam smith process control process management business process business process automation business process management business process model and notation advanced case management content management system web content management system document management system enterprise content management references references ref name neumann2010 christoph p neumann and richard lenz http ieeexplore ieee org xpls abs all jsp arnumber 5541988 the alpha flow use case of breast cancer treatment modeling inter institutional healthcare workflows by active documents in proc of the 8th int l workshop on agent based computing for enterprise collaboration acec at the 19th int l workshops on enabling technologies infrastructures for collaborative enterprises wetice 2010 larissa greece june 2010 http www6 informatik uni erlangen de publications public 2010 acec2010 neumann alphauc pdf pdf ref ref name kumaran2008 kumaran s r liu and f wu http www springerlink com content a837212173812011 on the duality of information centric and activity centric models of business processes in advanced information systems engineering 2008 p 32 47 ref ref name mueller2006 dominic m\xc3\xbcller manfred reichert und joachim herbst http www springerlink com content kpm9454157514825 flexibility of data driven process structures in business process management workshops lecture notes in computer science 2006 volume 4103 2006 181 192 http dbis eprints uni ulm de 111 2 mueller06 dpm pdf pdf ref ref name mueller2007 dominic m\xc3\xbcller manfred reichert und joachim herbst http www springerlink com content 2771670210653747 data driven modeling and coordination of large process structures on the move to meaningful internet systems 2007 coopis doa odbase gada and is lecture notes in computer science 2007 volume 4803 2007 131 149 http dbis eprints uni ulm de 116 1 mueller07 coopis pdf pdf ref ref name bhattacharya2007 kamal bhattacharya cagdas gerede richard hull rong liu and jianwen su 2007 http dl acm org citation cfm id 1793141 towards formal analysis of artifact centric business process models in proceedings of the 5th international conference on business process management bpm 07 gustavo alonso peter dadam and michael rosemann eds springer verlag berlin heidelberg 288 304 pages 289ff in http alumni cs ucsb edu gerede research papers bghls bpm07 artifact pdf pdf ref ref name calvanese2009 diego calvanese giuseppe de giacomo richard hull und jianwen su http www springerlink com content t58342lj86807111 artifact centric workflow dominance lecture notes in computer science 2009 volume 5900 2009 130 143 http www dis uniroma1 it degiacom papers 2009 icsoc09 pdf pdf ref ref name kuenzle2009 vera k\xc3\xbcnzle und manfred reichert http www springerlink com content n6448q47g0474242 towards object aware process management systems issues challenges benefits in enterprise business process and information systems modeling lecture notes in business information processing 2009 volume 29 part 1 part 5 197 210 doi 10 1007 978 3 642 01862 6 17 http dbis eprints uni ulm de 526 1 bpmds09 kuenzle reichert pdf pdf ref ref name kuenzle2010 k\xc3\xbcnzle vera and reichert manfred 2010 http dbis eprints uni ulm de 647 herausforderungen bei der integration von benutzern in datenorientierten prozess management systemen emisa forum 30 1 pp 11 28 issn 1610 3351 http dbis eprints uni ulm de 647 2 kure10 pdf pdf ref ref name wang2005 wang j and a kumar http www springerlink com content 79k8v7terwchn5ct a framework for document driven workflow systems in business process management 2005 p 285 301 http php scripts psu edu faculty a x axk41 bpm05 jerry reprint pdf pdf ref ref name kumar2010 akhil kumar und jianrui wang http www springerlink com content n218t085521q1347 a framework for designing resource driven workflows in handbook on business process management 1 international handbooks on information systems 2010 part iii 419 440 ref ref name neumann2011 christoph p neumann peter k schwab andreas m wahl and richard lenz alpha adaptive evolutionary workflow metadata in distributed document oriented process management in proc of the 4th int l workshop on process oriented information systems in healthcare prohealth 11 in conjunction with the 9th int l conf on business process management bpm 11 clermont ferrand france august 2011 http www6 informatik uni erlangen de publications public 2011 prohealth2011 neumann pdf pdf ref ref name neumann2012 christoph p neumann and richard lenz the alpha flow approach to inter institutional process support in healthcare international journal of knowledge based organizations igi global 2012 ref ref name dissneumann2012 christoph p neumann http www dr hut verlag de 978 3 8439 0919 8 html distributed case handling phd thesis german dissertation friedrich alexander universit auml t erlangen n uuml rnberg 2012 ref ref name reijers2003 hajo reijers jaap rigter wil van der aalst http www worldscinet com ijcis 12 1203 s0218843003000784 html the case handling case international journal of cooperative information systems ijcis volume 12 issue 3 2003 pp 365 391 http is tm tue nl staff hreijers h a 20reijers 20bestanden chc pdf pdf ref ref name aalst2005 wil m p van der aalst mathias weske dolf gr\xc3\xbcnbauer http www sciencedirect com science article pii s0169023x04001296 case handling a new paradigm for business process support in data amp knowledge engineering volume 53 issue 2 may 2005 pages 129 162 issn 0169 023x 10 1016 j datak 2004 07 003 http www imamu edu sa scientific selections abstracts abstratctit1 case 20handling 20a 20new 20paradigm 20for 20business 20process 20support pdf pdf ref references defaultsort content oriented workflows categories category workflow technology category data management'
b'approximate inference methods make it possible to learn realistic models from big data by trading off computation time for accuracy when exact learning and inference are computationally intractable major methods classes variational bayesian method s expectation propagation markov random field s bayesian network s variational message passing loopy and generalized belief propagation ref cite journal url http academic research microsoft com paper 14666 aspx title approximate inference and constrained optimization journal uncertainty in artificial intelligence uai pages 313 320 year 2003 ref ref cite web url http mlg eng cam ac uk zoubin approx html title approximate inference accessdate 2013 07 15 ref see also statistical inference fuzzy logic data mining references reflist external links cite web url http videolectures net mlss09uk minka ai title machine learning summer school mlss cambridge 2009 approximate inference author tom minka microsoft research date nov 2 2009 type video lecture category data management'
b'in computer science in the field of database s write write conflict also known as overwriting commit data management uncommitted data is a computational anomaly associated with interleaved execution of database transaction transactions given a schedule computer science schedule s math s begin bmatrix t1 t2 w a w b w b com w a com end bmatrix math note that there is no read in this schedule the writes are called blind writes we have a lost update any attempts to make this schedule serial would give off two different results either t1 s version of a and b is shown or t2 s version of a and b is shown and would not be the same as the above schedule this schedule would not be serializability serializable strict two phase locking strict 2pl overcomes this inconsistency by locking t1 out from b unfortunately deadlock s are something strict 2pl does not overcome all the time see also concurrency control read write conflict write read conflict references reflist unreferenced date august 2009 defaultsort write write conflict category data management category transaction processing'
b'unreferenced date june 2008 in computer science in the field of database s read write conflict also known as unrepeatable reads is a computational anomaly associated with interleaved execution of transactions given a schedule s math s begin bmatrix t1 t2 r a r a w a com r a w a com end bmatrix math in this example t1 has read the original value of a and is waiting for t2 to finish t2 also reads the original value of a overwrites a and commits however when t1 reads to a it discovers two different versions of a and t1 would be forced to abort computing abort because t1 would not know what to do this is an unrepeatable read this could never occur in a serial schedule strict two phase locking strict 2pl prevents this conflict real world example alice and bob are using a website to book tickets for a specific show only one ticket is left for the specific show alice signs on first to see that only one ticket is left and finds it expensive alice takes time to decide bob signs on and also finds one ticket left and orders it instantly bob purchases and logs off alice decides to buy a ticket to find there are no tickets this is a typical read write conflict situation see also concurrency control write read conflict write write conflict defaultsort read write conflict category data management category transaction processing'
b'a system of record sor or source system of record ssor is a data management data management term for an information storage system commonly implemented on a computer system that is the authoritative data source for a given data element or piece of information the need to identify systems of record can become acute in organizations where management information system s have been built by taking output data from multiple source systems re processing this data and then re presenting the result for a new business use in these cases multiple information systems may disagree about the same piece of information these disagreements may stem from semantic differences differences in opinion use of different sources differences in the timing of the extract transform load etl extracts that create the data they report against or may simply be the result of bugs the integrity and validity of any data set is open to question when there is no tracing software traceable connection to a good source such as a known system of record where the integrity of the data is vital if there is an agreed system of record the data element must either be linked to or extracted directly from it in other cases the provenance and estimated data quality should be documented the system of record approach is a good fit for environments where both there is a single authority over all data consumers and all consumers have similar needs in diverse environments one instead needs to support the presence of multiple opinions consumers may accept different authorities or may differ on what constitutes an authoritative source researchers may prefer carefully vetted data while tactical military systems may require the most recent credible report see also single source of truth practice of using one source for a particular data element privacy act of 1974 united states law including requirement for agencies to publish system of records notices sorn in the federal register to identify the system and describe the use of individuals data master data management defining the handling of master data systems of engagement more decentralized systems that incorporate technologies which encourage peer interactions references cite web title the system of record in the global data warehouse url http www information management com issues 20030501 6645 1 html publisher information management accessdate 2007 12 18 author bill inmon date may 2003 work cite web title the golden copy url http adam goucher ca p 72 last goucher first adam date 2006 04 26 accessdate 2013 04 30 cite web title the move from systems of record to systems of engagement url http www forbes com sites joshbersin 2012 08 16 the move from systems of record to systems of engagement last bersin first josh date 2012 08 16 accessdate 2013 04 30 category information systems category data management compu stub'
b'unreferenced date december 2009 in information systems design and theory single source of truth ssot is the practice of structuring information models and associated database schema schemata such that every data element is stored exactly once e g in no more than a single row of a single table any possible linkages to this data element possibly in other areas of the relational schema or even in distant federated database federated databases are by reference computer science reference only because all other locations of the data just refer back to the primary source of truth location updates to the data element in the primary location propagate to the entire system without the possibility of a duplicate value somewhere being forgotten deployment of an ssot architecture is becoming increasingly important in enterprise settings where incorrectly linked duplicate or de normalized data elements a direct consequence of intentional or unintentional denormalization of any explicit data model poses a risk for retrieval of outdated and therefore incorrect information a common example would be the electronic health record where it is imperative to accurately validate patient identity against a single referential repository which serves as the ssot duplicate representations of data within the enterprise would be implemented by the use of pointer computer programming pointer s rather than duplicate database tables rows or cells this ensures that data updates to elements in the authoritative location are comprehensively distributed to all federated database constituencies in the larger overall enterprise architecture fact date july 2012 ssot systems provide data that is authentic relevant and referable ref ibm smarter planet operational risk management for financial services http www ibm com smarterplanet za en banking technology nextsteps solution z017038z16405r75 html ref implementation the ideal implementation of ssot as described above is rarely possible in most enterprises this is because many organisations have multiple information systems each of which needs access to data relating to the same entities e g customer often these systems are purchased off the shelf from vendors and cannot be modified in non trivial ways each of these various systems therefore needs to store its own version of common data or entities and therefore each system must retain its own copy of a record hence immediately violating the ssot approach defined above for example an erp enterprise resource planning system such as sap or oracle e business suite may store a customer record the crm customer relationship management system also needs a copy of the customer record or part of it and the warehouse despatch system might also need a copy of some or all of the customer data e g shipping address in cases where vendors do not support such modifications it is not always possible to replace these records with pointers to the ssot for organisations with more than one information system wishing to implement a single source of truth without modifying all but one master system to store pointers to other systems for all entities three supporting technologies are commonly used fact date july 2012 enterprise service bus esb master data management mdm data warehouse dw enterprise service bus esb an enterprise service bus esb allows any number of systems in an organisation to receive updates of data that has changed in another system to implement a single source of truth a single source system of correct data for any entity must be identified changes to this entity creates updates and deletes are then published via the esb other systems which need to retain a copy of that data subscribe to this update and update their own records accordingly for any given entity the master source must be identified sometimes called the golden record it should be noted that any given system could publish be the source of truth for information on a particular entity e g customer and also subscribe to updates from another system for information on some other entity e g product fact date july 2012 an alternative approach is point to point data updates but these become exponentially more expensive to maintain as the number of systems increases and this approach is increasingly out of favour as an it architecture fact date july 2012 master data management mdm an mdm system can act as the source of truth for any given entity that might not necessarily have an alternative source of truth in another system typically the mdm acts as a hub for multiple systems many of which could allow be the source of truth for updates to different aspects of information on a given entity for example the crm system may be the source of truth for most aspects of the customer and is updated by a call centre operator however a customer may for example also update their address via a customer service web site with a different back end database from the crm system the mdm application receives updates from multiple sources acts as a broker to determine which updates are to be regarded as authoritative the golden record and then syndicates this updated data to all subscribing systems the mdm application normally requires an esb to syndicate its data to multiple subscribing systems ref bayt job site june 2014 http www bayt com en specialties q 7370 what are the top business processes and applications that need master data management ref customer data integration cdi as a common application of master data management is sometimes abbreviated cdi mdm fact date july 2012 data warehouse dw while the primary purpose of a data warehouse is to support reporting and analysis of data that has been combined from multiple sources the fact that such data has been combined according to business logic embedded in the extract transform load data transformation and integration processes means that the data warehouse is often used as a de facto ssot generally however the data available from the data warehouse is not used to update other systems rather the dw becomes the single source of truth for reporting to multiple stakeholders in this context the data warehouse is more correctly referred to as a single version of the truth since other versions of the truth exist in its operational data sources no data originates in the dw it is simply a reporting mechanism for data loaded from operational systems fact date july 2012 see also don t repeat yourself dry solid object oriented design database normalization single version of the truth system of record references reflist external links defaultsort single source of truth category data modeling category database normalization category data management'
b'in computer science xsa better known as cross server attack is a networking security intrusion method which allows for a malicious client to compromise security over a website or service on a server by using implemented services on the server that may not be secure in general xsa is demonstrated against websites yet sometimes it is used in conjunction with other services located on the same server basics xsa is a method that allows for a malicious client to use services that a remote server implements in order to attack another service on the same server or network most website hosting companies that offer hosting for large or even little amounts of separate websites are vulnerable to this method of attack because of the amount of access services such as php and the webserver itself give to a client that allows the client to access other website configurations files passwords and the like history the term xsa was first coined by deadlydata a prominent computer hacker during the early 2000s over the voice communications software teamspeak while he had not invented or pioneered this method of intrusion he coined it as a shortened term to describe the act of performing cross server attacks xsas it was then used further in the community and now supports for most of the methods and subsets of the method that give both computer hacker and malicious individuals the terminology to attack websites using software that is located on the same server see also portal software testing sql injection sqli cross site scripting xss cross site request forgery csrf buffer overflow defaultsort xsa category data management category computer security exploits category computer network security category world wide web category web development'
b'about a concept used universally in computing addressing specifically the main memory memory address refimprove date december 2011 in computing an address space defines a range of discrete addresses each of which may correspond to a network host peripheral device disk sector a computer data storage memory cell or other logical or physical entity for software programs to save and retrieve stored data each unit of data must have an address where it can be individually located or else the program will be unable to find and manipulate the data the number of address spaces available will depend on the underlying address structure and these will usually be limited by the computer architecture being used address spaces are created by combining enough uniquely identified qualifiers to make an address unambiguous within a particular address space for a person s physical address the address space would be a combination of locations such as a neighborhood town city or country some elements of an address space may be the same but if any element in the address is different than addresses in said space will reference different entities an example could be that there are multiple buildings at the same address of 32 main street but in different towns demonstrating that different towns have different although similarly arranged street address spaces an address space usually provides or allows a partitioning to several regions according to the mathematical structure it has in the case of total order as for memory address es these are simply interval mathematics chunks some nested domains hierarchy appears in the case of arborescence graph theory directed ordered tree as for the domain name system or a directory structure this is similar to the hierarchical design of postal address es in the internet for example the internet assigned numbers authority iana allocates ranges of ip address es to various registries in order to enable them to each manage their parts of the global internet address space ref cite web url http www iana org assignments ipv4 address space title ipv4 address space registry date march 11 2009 publisher internet assigned numbers authority iana accessdate september 1 2011 ref examples uses of addresses include but are not limited to the following memory address es for main memory memory mapped i o as well as for virtual memory device addresses on an expansion bus disk sector sector addressing for disk drive s file name s on a particular volume computing volume various kinds of network host addresses in computer network s uniform resource locator s in the internet address mapping and translation file cnftl9 jpg thumb illustration of translation from logical block addressing to physical geometry another common feature of address spaces are map mathematics mappings and translations often forming numerous layers this usually means that some higher level address must be translated to lower level ones in some way for example file system on a logical disk operates one dimensional array linear sector numbers which have to be translated to absolute logical block addressing lba sector addresses in simple cases via addition of the partition s first sector address then for a disk drive connected via parallel ata each of them must be converted to logical means fake cylinder head sector address due to the interface historical shortcomings it is converted back to lba by the disk controller computing controller and then finally to physical cylinder disk drive cylinder disk head head and track disk drive sector numbers the domain name system maps its names to and from network specific addresses usually ip addresses which in turn may be mapped to link layer network addresses via address resolution protocol also network address translation may occur on the edge of different ip spaces such as a local area network and the internet file virtual address space and physical address space relationship svg thumb virtual address space and physical address space relationship an iconic example of virtual to physical address translation is virtual memory where different page computer memory pages of virtual address space map either to paging page file or to main memory physical address space it is possible that several numerically different virtual addresses all refer to one physical address and hence to the same physical byte of random access memory ram it is also possible that a single virtual address maps to zero one cpu cache homonym and synonym problems or more than one physical address see also linear address space name space virtualization references reflist category computing terminology category data management category computer architecture'
b'refimprove date march 2015 data rooms are spaces used for housing data usually of a secure or privileged nature they can be physical data rooms virtual data room s or data centers ref cite web title data room entry url http financial dictionary thefreedictionary com data room website financial dictionary the free dictionary by farlex ref ref cite web title data room entry url http www nasdaq com investing glossary d data room website nasdaq ref they are used for a variety of purposes including data storage document exchange file sharing financial transactions legal transactions and more in mergers and acquisitions the traditional data room will literally be a physically secure continually monitored room normally in the vendor s offices or those of their lawyers which the bidders and their advisers will visit in order to inspect and report on the various documents and other data made available often only one bidder at a time will be allowed to enter and if new documents or new versions of documents are required these will have to be brought in by courier as hardcopy teams involved in large due diligence processes will typically have to be flown in from many regions or countries and remain available throughout the process such teams often comprise a number of experts in different fields and so the overall cost of keeping such groups on call near to the data room is often extremely high combating the significant cost of physical datarooms is the virtual data room which provides for the secure online dissemination of confidential information a virtual data room vdr is essentially a website with limited controlled access using a secure log on supplied by the vendor authority which can be disabled at any time by the vendor authority if a bidder withdraws to which the bidders and their advisers are given access much of the information released will be confidential and restrictions should be applied to the viewers ability to release this to third parties by forwarding copying or printing digital rights management is sometimes applied to control information detailed auditing must be provided for legal reasons so that a record is kept of who has seen which version of each document data rooms are commonly used by legal accounting investment banking and private equity companies performing mergers and acquisitions fundraising insolvency corporate restructuring and joint ventures including bio technology and tender processes references reflist http www imaa institute org docs kummer sliskovic do 20virtual 20data 20rooms 20add 20value 20to 20the 20mergers 20and 20acquisitions 20process pdf a report about the advantages and disadvantages of virtual vs physical data rooms defaultsort data room category data management'
b'online analytical processing or olap ipac en \xcb\x88 o\xca\x8a l \xc3\xa6 p is an approach to answering multi dimensional analytical mda queries swiftly in computing ref name codd1993 cite web url http www sgpnyc com us products dataquest whitepapers olap wp efcodd pdf title providing olap on line analytical processing to user analysts an it mandate publisher codd date inc author1 codd e f author2 codd s b author3 salley c t last author amp yes year 1993 accessdate 2008 03 05 ref olap is part of the broader category of business intelligence which also encompasses relational database report writing and data mining ref cite book url https books google com books id m uoe1cp9oec title business intelligence for telecommunications publisher crc press author deepak pareek year 2007 pages 294 pp isbn 0 8493 8792 2 accessdate 2008 03 18 ref typical applications of olap include business reporting for sales marketing management reporting business process management bpm ref cite book url http www google com products q 9783639222166 title business process management a data cube to analyze business process simulation data for decision making publisher vdm verlag vdm verlag dr m\xc3\xbcller e k author apostolos benisis year 2010 pages 204 pp isbn 978 3 639 22216 6 ref budget ing and forecasting forecast ing financial reporting and similar areas with new applications coming up such as agriculture ref name ahsan the term olap was created as a slight modification of the traditional database term online transaction processing oltp ref cite web url http www symcorp com downloads olap councilwhitepaper pdf format pdf title olap council white paper publisher olap council year 1997 accessdate 2008 03 18 ref olap tools enable users to analyze multidimensional data interactively from multiple perspectives olap consists of three basic analytical operations consolidation roll up drill down and slicing and dicing ref o brien marakas 2011 p 402 403 ref consolidation involves the aggregation of data that can be accumulated and computed in one or more dimensions for example all sales offices are rolled up to the sales department or sales division to anticipate sales trends by contrast the drill down is a technique that allows users to navigate through the details for instance users can view the sales by individual products that make up a region s sales slicing and dicing is a feature whereby users can take out slicing a specific set of data of the olap cube and view dicing the slices from different viewpoints these viewpoints are sometimes called dimensions such as looking at the same sales by salesperson or by date or by customer or by product or by region etc database s configured for olap use a multidimensional data model allowing for complex analytical and ad hoc queries with a rapid execution time ref cite web url http www dwreview com olap introduction olap html title introduction to olap slice dice and drill publisher data warehousing review author hari mailvaganam year 2007 accessdate 2008 03 18 ref they borrow aspects of navigational database s hierarchical database s and relational databases olap is typically contrasted to online transaction processing oltp online transaction processing which is generally characterized by much less complex queries in a larger volume to process transactions rather than for the purpose of business intelligence or reporting whereas olap systems are mostly optimized for read oltp has to processes all kinds of queries read insert update and delete overview of olap systems at the core of any olap system is an olap cube also called a multidimensional cube or a hypercube it consists of numeric facts called measures that are categorized by dimension data warehouse dimensions the measures are placed at the intersections of the hypercube which is spanned by the dimensions as a vector space the usual interface to manipulate an olap cube is a matrix interface like pivot table s in a spreadsheet program which performs projection operations along the dimensions such as aggregation or averaging the cube metadata is typically created from a star schema or snowflake schema or fact constellation of tables in a relational database measures are derived from the records in the fact table and dimensions are derived from the dimension table s each measure can be thought of as having a set of labels or meta data associated with it a dimension is what describes these labels it provides information about the measure a simple example would be a cube that contains a store s sales as a measure and date time as a dimension each sale has a date time label that describes more about that sale for example sale fact table sale amount time id time dimension 2008 10 1234 time id timestamp 1234 20080902 12 35 43 multidimensional databases multidimensional structure is defined as a variation of the relational model that uses multidimensional structures to organize data and express the relationships between data ref o brien marakas 2009 pg 177 ref the structure is broken into cubes and the cubes are able to store and access data within the confines of each cube each cell within a multidimensional structure contains aggregated data related to elements along each of its dimensions ref o brien marakas 2009 pg 178 ref even when data is manipulated it remains easy to access and continues to constitute a compact database format the data still remains interrelated multidimensional structure is quite popular for analytical databases that use online analytical processing olap applications ref o brien marakas 2009 ref analytical databases use these databases because of their ability to deliver answers to complex business queries swiftly data can be viewed from different angles which gives a broader perspective of a problem unlike other models ref williams c garza v r tucker s marcus a m 1994 january 24 multidimensional models boost viewing options infoworld 16 4 ref aggregations it has been claimed that for complex queries olap cubes can produce an answer in around 0 1 of the time required for the same query on oltp relational data ref cite web author microstrategy incorporated year 1995 title the case for relational olap url http www cs bgu ac il onap052 uploads seminar relational 20olap 20microstrategy pdf format pdf accessdate 2008 03 20 ref ref cite journal author1 surajit chaudhuri author2 umeshwar dayal lastauthoramp yes title an overview of data warehousing and olap technology journal sigmod rec publisher association for computing machinery acm volume 26 issue 1 year 1997 pages 65 url http doi acm org 10 1145 248603 248616 doi 10 1145 248603 248616 accessdate 2008 03 20 ref the most important mechanism in olap which allows it to achieve such performance is the use of aggregations aggregations are built from the fact table by changing the granularity on specific dimensions and aggregating up data along these dimensions the number of possible aggregations is determined by every possible combination of dimension granularities the combination of all possible aggregations and the base data contains the answers to every query which can be answered from the data ref cite journal last1 gray first1 jim author1 link jim gray computer scientist last2 chaudhuri first2 surajit last3 layman first3 andrew last4 reichart first4 don last5 venkatrao first5 murali last6 pellow first6 frank last7 pirahesh first7 hamid title data cube a relational aggregation operator generalizing group by cross tab and sub totals journal j data mining and knowledge discovery volume 1 issue 1 pages 29 53 year 1997 url http citeseer ist psu edu gray97data html accessdate 2008 03 20 ref because usually there are many aggregations that can be calculated often only a predetermined number are fully calculated the remainder are solved on demand the problem of deciding which aggregations views to calculate is known as the view selection problem view selection can be constrained by the total size of the selected set of aggregations the time to update them from changes in the base data or both the objective of view selection is typically to minimize the average time to answer olap queries although some studies also minimize the update time view selection is np complete many approaches to the problem have been explored including greedy algorithm s randomized search genetic algorithm s and a search algorithm types olap systems have been traditionally categorized using the following taxonomy ref name pendse2006 cite web url http www olapreport com architectures htm title olap architectures publisher olap report author nigel pendse date 2006 06 27 accessdate 2008 03 17 deadurl yes archiveurl https web archive org web 20080124155954 http www olapreport com architectures htm archivedate january 24 2008 ref multidimensional olap molap molap multi dimensional online analytical processing is the classic form of olap and is sometimes referred to as just olap molap stores this data in an optimized multi dimensional array storage rather than in a relational database some molap tools require the pre computation and storage of derived data such as consolidations the operation known as processing such molap tools generally utilize a pre calculated data set referred to as a data cube the data cube contains all the possible answers to a given range of questions as a result they have a very fast response to queries on the other hand updating can take a long time depending on the degree of pre computation pre computation can also lead to what is known as data explosion other molap tools particularly those that implement the functional database model functional database model do not pre compute derived data but make all calculations on demand other than those that were previously requested and stored in a cache advantages of molap fast query performance due to optimized storage multidimensional indexing and caching smaller on disk size of data compared to data stored in relational database due to compression techniques automated computation of higher level aggregates of the data it is very compact for low dimension data sets array models provide natural indexing effective data extraction achieved through the pre structuring of aggregated data disadvantages of molap within some molap solutions the processing step data load can be quite lengthy especially on large data volumes this is usually remedied by doing only incremental processing i e processing only the data which have changed usually new data instead of reprocessing the entire data set some molap methodologies introduce data redundancy products examples of commercial products that use molap are cognos powerplay oracle olap oracle database olap option microstrategy microsoft analysis services essbase applix tm1 jedox and iccube relational olap rolap rolap works directly with relational databases and does not require pre computation the base data and the dimension tables are stored as relational tables and new tables are created to hold the aggregated information it depends on a specialized schema design this methodology relies on manipulating the data stored in the relational database to give the appearance of traditional olap s slicing and dicing functionality in essence each action of slicing and dicing is equivalent to adding a where clause in the sql statement rolap tools do not use pre calculated data cubes but instead pose the query to the standard relational database and its tables in order to bring back the data required to answer the question rolap tools feature the ability to ask any question because the methodology does not limit to the contents of a cube rolap also has the ability to drill down to the lowest level of detail in the database while rolap uses a relational database source generally the database must be carefully designed for rolap use a database which was designed for oltp will not function well as a rolap database therefore rolap still involves creating an additional copy of the data however since it is a database a variety of technologies can be used to populate the database advantages of rolap note to editors please review the discussion page before making changes to the advantages or disadvantages thank you rolap is considered to be more scalable in handling large data volumes especially models with dimension data warehouse dimensions with very high cardinality i e millions of members with a variety of data loading tools available and the ability to fine tune the extract transform load etl code to the particular data model load times are generally much shorter than with the automated multidimensional olap 28molap 29 molap loads the data are stored in a standard relational database and can be accessed by any sql reporting tool the tool does not have to be an olap tool rolap tools are better at handling non aggregatable facts e g textual descriptions multidimensional olap 28molap 29 molap tools tend to suffer from slow performance when querying these elements by decoupling electronics decoupling the data storage from the multi dimensional model it is possible to successfully model data that would not otherwise fit into a strict dimensional model the rolap approach can leverage database authorization controls such as row level security whereby the query results are filtered depending on preset criteria applied for example to a given user or group of users sql where clause disadvantages of rolap note to editors please review the discussion page before making changes to the advantages or disadvantages thank you there is a consensus in the industry that rolap tools have slower performance than molap tools however see the discussion below about rolap performance the loading of aggregate tables must be managed by custom extract transform load etl code the rolap tools do not help with this task this means additional development time and more code to support when the step of creating aggregate tables is skipped the query performance then suffers because the larger detailed tables must be queried this can be partially remedied by adding additional aggregate tables however it is still not practical to create aggregate tables for all combinations of dimensions attributes rolap relies on the general purpose database for querying and caching and therefore several special techniques employed by molap tools are not available such as special hierarchical indexing however modern rolap tools take advantage of latest improvements in sql language such as cube and rollup operators db2 cube views as well as other sql olap extensions these sql improvements can mitigate the benefits of the molap tools since rolap tools rely on sql for all of the computations they are not suitable when the model is heavy on calculations which don t translate well into sql examples of such models include budgeting allocations financial reporting and other scenarios performance of rolap in the olap industry rolap is usually perceived as being able to scale for large data volumes but suffering from slower query performance as opposed to multidimensional olap 28molap 29 molap the http www olapreport com survey htm olap survey the largest independent survey across all major olap products being conducted for 6 years 2001 to 2006 have consistently found that companies using rolap report slower performance than those using molap even when data volumes were taken into consideration however as with any survey there are a number of subtle issues that must be taken into account when interpreting the results the survey shows that rolap tools have 7 times more users than multidimensional olap 28molap 29 molap tools within each company systems with more users will tend to suffer more performance problems at peak usage times there is also a question about complexity of the model measured both in number of dimensions and richness of calculations the survey does not offer a good way to control for these variations in the data being analyzed downside of flexibility some companies select rolap because they intend to re use existing relational database tables these tables will frequently not be optimally designed for olap use the superior flexibility of rolap tools allows this less than optimal design to work but performance suffers multidimensional olap 28molap 29 molap tools in contrast would force the data to be re loaded into an optimal olap design hybrid olap holap the undesirable trade off between additional extract transform load etl cost and slow query performance has ensured that most commercial olap tools now use a hybrid olap holap approach which allows the model designer to decide which portion of the data will be stored in multidimensional olap 28molap 29 molap and which portion in rolap there is no clear agreement across the industry as to what constitutes hybrid olap except that a database will divide data between relational and specialized storage ref name ieee cite cite journal last1 bach pedersen first1 torben last2 s jensen title multidimensional database technology journal distributed systems online volume issue issn 0018 9162 pages 40 46 publisher ieee location date december 2001 url http ieeexplore ieee org stamp stamp jsp arnumber 00970558 doi id accessdate first2 christian ref for example for some vendors a holap database will use relational tables to hold the larger quantities of detailed data and use specialized storage for at least some aspects of the smaller quantities of more aggregate or less detailed data holap addresses the shortcomings of multidimensional olap 28molap 29 molap and relational olap 28rolap 29 rolap by combining the capabilities of both approaches holap tools can utilize both pre calculated cubes and relational data sources vertical partitioning in this mode holap stores aggregations in multidimensional olap 28molap 29 molap for fast query performance and detailed data in relational olap 28rolap 29 rolap to optimize time of cube processing horizontal partitioning in this mode holap stores some slice of data usually the more recent one i e sliced by time dimension in multidimensional olap 28molap 29 molap for fast query performance and older data in relational olap 28rolap 29 rolap moreover we can store some dices in multidimensional olap 28molap 29 molap and others in relational olap 28rolap 29 rolap leveraging the fact that in a large cuboid there will be dense and sparse subregions ref owen kaser and daniel lemire http arxiv org abs cs db 0702143 attribute value reordering for efficient hybrid olap information sciences volume 176 issue 16 pages 2279 2438 2006 ref products the first product to provide holap storage was holos but the technology also became available in other commercial products such as microsoft analysis services oracle olap oracle database olap option microstrategy and sap ag bi accelerator the hybrid olap approach combines rolap and molap technology benefiting from the greater scalability of rolap and the faster computation of molap for example a holap server may allow large volumes of detail data to be stored in a relational database while aggregations are kept in a separate molap store the microsoft sql server 7 0 olap services supports a hybrid olap server comparison each type has certain benefits although there is disagreement about the specifics of the benefits between providers some molap implementations are prone to database explosion a phenomenon causing vast amounts of storage space to be used by molap databases when certain common conditions are met high number of dimensions pre calculated results and sparse multidimensional data molap generally delivers better performance due to specialized indexing and storage optimizations molap also needs less storage space compared to rolap because the specialized storage typically includes data compression compression techniques ref name ieee cite rolap is generally more scalable ref name ieee cite however large volume pre processing is difficult to implement efficiently so it is frequently skipped rolap query performance can therefore suffer tremendously since rolap relies more on the database to perform calculations it has more limitations in the specialized functions it can use holap encompasses a range of solutions that attempt to mix the best of rolap and molap it can generally pre process swiftly scale well and offer good function support other types the following acronyms are also sometimes used although they are not as widespread as the ones above wolap web based olap dolap desktop computer desktop olap rtolap rtolap real time olap apis and query languages unlike relational databases which had sql as the standard query language and widespread application programming interface api s such as odbc jdbc and oledb there was no such unification in the olap world for a long time the first real standard api was ole db for olap specification from microsoft which appeared in 1997 and introduced the multidimensional expressions mdx query language several olap vendors both server and client adopted it in 2001 microsoft and hyperion solutions corporation hyperion announced the xml for analysis specification which was endorsed by most of the olap vendors since this also used mdx as a query language mdx became the de facto standard ref cite web url http www olapreport com comment apis htm title commentary olap api wars publisher olap report author nigel pendse date 2007 08 23 accessdate 2008 03 18 deadurl yes archiveurl https web archive org web 20080528220113 http www olapreport com comment apis htm archivedate may 28 2008 ref since september 2011 linq can be used to query microsoft analysis services ssas olap cubes from microsoft net ref cite web url http www agiledesignllc com products title ssas entity framework provider for linq to ssas olap ref products history the first product that performed olap queries was express which was released in 1970 and acquired by oracle corporation oracle in 1995 from information resources ref cite web title the origins of today s olap products url http olapreport com origins htm publisher olap report date 2007 08 23 author nigel pendse accessdate november 27 2007 deadurl yes archiveurl https web archive org web 20071221044811 http www olapreport com origins htm archivedate december 21 2007 ref however the term did not appear until 1993 when it was coined by edgar f codd who has been described as the father of the relational database codd s paper ref name codd1993 resulted from a short consulting assignment which codd undertook for former arbor software later hyperion solutions and in 2007 acquired by oracle as a sort of marketing coup the company had released its own olap product essbase a year earlier as a result codd s twelve laws of online analytical processing were explicit in their reference to essbase there was some ensuing controversy and when computerworld learned that codd was paid by arbor it retracted the article olap market experienced strong growth in late 90s with dozens of commercial products going into market in 1998 microsoft released its first olap server microsoft analysis services which drove wide adoption of olap technology and moved it into mainstream product comparison main comparison of olap servers olap clients olap clients include many spreadsheet programs like excel web application sql dashboard tools etc market structure below is a list of top olap vendors in 2006 with figures in millions of us dollar s ref cite web url http www olapreport com market htm title olap market publisher olap report author nigel pendse year 2006 accessdate 2008 03 17 ref class wikitable sortable bgcolor cccccc align center vendor global revenue consolidated company microsoft corporation 1 806 microsoft hyperion solutions corporation 1 077 oracle cognos 735 ibm business objects company business objects 416 sap microstrategy 416 microstrategy sap ag 330 sap cartesis sap ag sap 210 sap applix 205 ibm infor 199 infor oracle corporation 159 oracle others 152 others total 5 700 open source druid open source data store is a popular open source distributed data store for olap queries that is used at scale in production by various organizations apache kylin is a distributed data store for olap queries originally developed by ebay cubes olap server is another light weight open source toolkit implementation of olap functionality in the python programming language python programming language with built in rolap linkedin pinot is used at linkedin to deliver scalable real time analytics with low latency ref cite news last yegulalp first serdar date 2015 06 11 title linkedin fills another sql on hadoop niche url http www infoworld com article 2934506 olap linkedins pinot fills another sql on hadoop niche html magazine infoworld access date 2016 11 19 ref it can ingest data from offline data sources such as hadoop and flat files as well as online sources such as kafka pinot is designed to scale horizontally see also portal computer science comparison of olap servers data warehouse online transaction processing oltp business analytics predictive analytics data mining thomsen diagrams functional database model bibliography cite web url http www daniel lemire com olap title data warehousing and olap a research oriented bibliography author daniel lemire date december 2007 cite book title olap solutions building multidimensional information systems 2nd edition publisher john wiley sons series year 1997 isbn 978 0 471 14931 6 author erik thomsen ling liu and tamer m \xc3\xb6zsu eds 2009 http www springer com computer database management information retrieval book 978 0 387 49616 0 encyclopedia of database systems 4100 p nbsp 60 illus isbn 978 0 387 49616 0 o brien j a marakas g m 2009 management information systems 9th ed boston ma mcgraw hill irwin references reflist 30em refs ref name ahsan cite journal last1 abdullah first1 ahsan title analysis of mealybug incidence on the cotton crop using adss olap online analytical processing tool journal computers and electronics in agriculture date november 2009 volume 69 issue 1 pages 59 72 doi 10 1016 j compag 2009 07 003 ref data warehouse authority control defaultsort online analytical processing category online analytical processing category data management category information technology management'
b'a content repository or content store is a database of digital content with an associated set of data management search and access methods allowing application independent access to the content rather like a digital library but with the ability to store and modify content in addition to searching and retrieving the content repository acts as the storage engine for a larger application such as a content management system or a document management system which adds a user interface on top of the repository s application programming interface ref http openacs org doc acs content repository design html content repository design http openacs org doc acs content repository acs content repository http openacs org openacs org ref advantages provided by repositories common rules for data access allow many applications to work with the same content without interrupting the data they give out signals when changes happen letting other applications using the repository know that something has been modified which enables collaborative data management developers can deal with data using programs that are more compatible with the desktop programming environment the data model is scriptable when users use a content repository content repository features a content repository may provide functionality such as add edit delete content hierarchy and sort order management query search versioning access control import export locking life cycle management retention and holding records management examples apache jackrabbit modeshape applications content management document management system document management digital asset management records management revision control social collaboration web content management system web content management standards and specification content repository api for java webdav content management interoperability services see also information repository content media references reflist external links http db engines com en ranking content store db engines ranking of content stores by popularity updated monthly category data management category content management systems'
b'multiple issues orphan date november 2013 unreferenced date november 2013 this article pertains to a new technology used in business intelligence managed memory computing uses aggregated data for in memory analytics aggregated data cubes are the most effective form of storage of aggregated or summarized data for quick analysis this technology is driven by online analytical processing online analytical processing technology utilizing these data cubes involves intense disk i o operations this at times lowers the speed for users of data conventional in memory processing in memory processing does not rely on stored and summarized or aggregated data but brings all the relevant data to the memory this technology then utilizes intense processing and large amounts of memory to perform all calculations and aggregations while in memory managed memory computing blends the best of both methods allowing users to define data cubes with per structured and aggregated data providing a logical business layer to users and offering in memory computation these features make the response time for user interactions far superior and enable the most balanced approach between disk i o and in memory processing the hybrid approach of managed memory computing provides analysis dashboards graphical interaction ad hoc querying presentation and discussion driven analytic at blazing speeds making the business intelligence business intelligence tool ready for everything from an interactive session in the boardroom to a production planning meeting on the factory floor references http www cioreview in magazine elegantj bi managed memory computing business intelligence redefined czsi499492332 html introduction of managed memory computing in cioreview category business intelligence category financial data analysis category data management category computer architecture'
b'unreferenced date september 2013 data thinking is the generic mental pattern observed during the processes of picking a subject to start with identifying its parts or components organizing and describing them in an informative fashion that is relevant to what motivated and initiated the whole processes the term was created by mario faria and rogerio panigassi in 2013 when they were writing a book about data science data analysis data analytics data management and how data practitioners were able to achieve their goals mario faria is one of the first chief data officer s in the world category data management'
b'iso iec jtc 1 sc 32 data management and interchange is a standardization subcommittee of the joint technical committee iso iec jtc1 iso iec jtc 1 of the international organization for standardization iso and the international electrotechnical commission iec which develops and facilitates standards within the field of data management and interchange the international secretariat administrative office secretariat of iso iec jtc 1 sc 32 is the american national standards institute ansi located in the united states ref name countries cite web title iso iec jtc 1 sc 32 data management and interchange url http www iso org iso home standards development list of iso technical committees iso technical committee participation htm commid 45342 author iso accessdate 2013 10 03 ref history iso iec jtc 1 sc 32 was formed in 1997 as a combination of the following three iso iec jtc 1 subgroups iso iec jtc 1 sc 21 wg 3 database iso iec jtc 1 sc 14 data elements and iso iec jtc 1 sc 30 open edi the new subcommittee was established with the intention of developing and facilitating the development of standards for data management within local and distributed information system environments ref name briefing cite news title information technology iso iec jtc 1 sc 32 data management and interchange type briefings author1 cannan stephen author2 melton jim journal iso bulletin date january 2000 url http jtc1sc32 org doc n0601 0650 32n0607 pdf pages 3 4 volume 31 issue 1 ref iso iec jtc 1 sc 32 was originally made up of five working groups wgs though iso iec jtc 1 sc 32 wg 5 database access and interchange was disbanded in march 2002 ref cite report type business plan draft title draft business plan for iso iec jtc 1 sc32 data management and interchange author mann douglas accessdate 2013 10 04 url http jtc1sc32 org doc n0751 0800 32n0783 pdf date 2002 04 04 page 4 ref the four other original working groups of the subcommittee are currently active although the title of iso iec jtc 1 sc 32 wg 1 was changed from open edi to its current title e business ref name briefing scope the scope of iso iec jtc 1 sc 32 is standards for data management within and among local and distributed information systems environments sc 32 provides enabling technologies to promote harmonization of data management facilities across sector specific areas specifically sc32 standards include ref name business2012 cite report type business plan url http jtc1info org wp content uploads 2013 03 sc 32 business plan 2012 pdf accessdate 2013 10 03 author melton jim date 2012 10 02 title business plan for jtc1 sc32 2012 2013 ref reference models and frameworks for the coordination of existing and emerging standards definition of data domains data types and data structures and their associated semantics languages services and protocols for persistent storage concurrent access and concurrent update and interchange of data methods languages services and protocols to structure organize and register metadata and other information resources associated with sharing and interoperability including electronic commerce structure iso iec jtc 1 sc 32 is made up of four active working groups each of which carries out specific tasks in standards development within the field of data management and interchange as a response to changing standardization needs working groups of iso iec jtc 1 sc 32 can be disbanded if their area of work is no longer applicable or established if new working areas arise the focus of each working group is described in the group s terms of reference active working groups of iso iec jtc 1 sc 32 are ref name business2012 ref cite web title iso iec jtc 1 sc 32 data management and interchange author iso accessdate 2013 10 03 url http www iso org iso home standards development list of iso technical committees iso technical committee htm commid 45342 ref class wikitable width 60 width 20 working group width 40 working area iso iec jtc 1 sc 32 wg 1 electronic business e business iso iec jtc 1 sc 32 wg 2 metadata iso iec jtc 1 sc 32 wg 3 database database languages database languages iso iec jtc 1 sc 32 wg 4 sql multimedia and application packages collaborations iso iec jtc 1 sc 32 works in close collaboration with a number of other organizations or subcommittees both internal and external to iso or iec in order to avoid conflicting or duplicative work organizations internal to iso or iec that collaborate with or are in liaison to iso iec jtc 1 sc 32 include ref name business2012 ref cite web url http www iso org iso home standards development list of iso technical committees iso technical committee htm commid 45342 title iso iec jtc 1 sc 32 data management and interchange accessdate 2013 10 03 author iso ref ref cite web title sc32 liaison organizations accessdate 2013 10 03 author iso iec jtc 1 sc 32 url http jtc1sc32 org ref iso iec jtc 1 sc 7 software and systems engineering iso iec jtc 1 sc 25 interconnection of information technology equipment iso iec jtc 1 sc 38 cloud computing and distributed platforms iso tc 12 quantities and units iso tc 37 terminology and other language and content resources iso tc 37 sc 2 terminographical and lexicographical working methods iso tc 37 sc 3 systems to manage terminology knowledge and content iso tc 37 sc 4 language resource management iso tc 46 sc 4 technical interoperability iso tc 46 sc 11 archives records management iso tc 68 sc 2 financial services security iso tc 127 earth moving machinery iso tc 154 processes data elements and documents in commerce industry and administration iso tc 184 automation systems and integration iso tc 184 sc 4 iso tc 184 sc 4 industrial data iso tc 204 intelligent transport systems iso tc 211 geographic information geomatics iso tc 215 health informatics iso tc 232 learning services outside formal education some organizations external to iso or iec that collaborate with or are in liaison to iso iec jtc 1 sc 32 include conf\xc3\xa9d\xc3\xa9ration internationale des soci\xc3\xa9t\xc3\xa9s d\xc2\xb4auteurs et compositeurs international confederation of societies of authors and composers cisac dublin core metadata initiative dcmi eurostat international telecommunications satellite organization itso international telecommunication union itu infoterm object management group omg society for worldwide interbank financial telecommunication swift un cefact united nations economic commission for europe unece world meteorological organization wmo w3c member countries countries pay a fee to iso to be members of subcommittees ref cite manual url http www iso org iso iso membership manual 2012 pdf pages 18 chapter iii what help can i get from the iso central secretariat title iso membership manual author iso date june 2012 accessdate 2013 07 12 publisher iso ref the 14 p participating members of iso iec jtc 1 sc 32 are canada china czech republic c\xc3\xb4te d ivoire egypt finland germany india japan republic of korea portugal russian federation united kingdom and united states ref name countries the 22 o observing members of iso iec jtc 1 sc 32 are australia austria belgium bosnia and herzegovina france ghana hungary iceland indonesia islamic republic of iran ireland italy kazakhstan luxembourg netherlands norway poland romania serbia spain switzerland and turkey published standards iso iec jtc 1 sc 32 standards are meant to structure organize and register metadata and other information resources associated with sharing and interoperability including electronic commerce ref name briefing iso iec jtc 1 sc 32 currently has 74 published standards within the field of data management and interchange including ref cite web title iso iec jtc 1 sc 32 url http www iso org iso home store catalogue tc catalogue tc browse htm commid 45342 published on accessdate 2013 10 03 author iso ref ref cite web publisher iso url http standards iso org ittf publiclyavailablestandards index html title freely available standards accessdate 2013 09 26 ref class wikitable sortable width 100 data sort type number width 14 iso iec standard width 29 title width 6 status width 49 description width 2 wg data sort value 14662 iso iec 14662 http standards iso org ittf licence html free information technology open edi reference model published 2010 specifies the framework for coordinating the integration of existing international standards and the development of future international standards for the interworking of open edi parties through open edi ref cite journal title iso iec 14662 date 2010 02 15 author iso edition 3 page 1 ref ref cite web title iso iec 14662 2010 accessdate 2013 10 04 url http www iso org iso home store catalogue tc catalogue detail htm csnumber 55290 date 2010 02 02 author iso ref 1 data sort value 15944 iso iec 15944 1 http standards iso org ittf licence html free information technology business operational view part 1 operational aspects of open edi for implementation published 2011 allows constraints including legal requirements commercial and or international trade and contract terms public policy and laws and regulations to be defined and integrated into open edi through the business operational view bov ref cite journal title iso iec 15944 1 date 2011 08 01 author iso edition 2 page 1 ref ref cite web title iso iec 15944 1 2011 accessdate 2013 10 04 url http www iso org iso home store catalogue tc catalogue detail htm csnumber 55289 date 2011 07 21 author iso ref 1 data sort value 11179 iso iec 11179 3 http standards iso org ittf licence html free information technology metadata registry metadata registries mdr part 3 registry metamodel and basic attributes published 2013 specifies the structure of a metadata registry in the form of a conceptual data model and specifies basic attributes which are required to describe metadata items ref cite journal page 1 date 2003 02 15 title iso iec 11179 3 author iso edition 2 ref ref cite web title iso iec 11179 3 2013 accessdate 2013 10 03 url http www iso org iso home store catalogue tc catalogue detail htm csnumber 50340 date 2013 02 12 author iso ref 2 data sort value 20943 iso iec tr 20943 1 http standards iso org ittf licence html free information technology procedures for achieving metadata registry content consistency part 1 data elements published 2003 describes a set of procedures for the consistent registration of data elements and their attributes in a registry ref cite journal title iso iec tr 20943 1 author iso page 1 date 2003 08 01 edition 1 ref ref cite web accessdate 2013 10 04 url http www iso org iso home store catalogue tc catalogue detail htm csnumber 34343 date 2012 12 19 title iso iec tr 20943 1 2003 author iso ref 2 data sort value 20944 iso iec 20944 1 information technology metadata registries interoperability and bindings mdr ib part 1 framework common vocabulary and common provisions for conformance published 2013 contains the overview framework common vocabulary and common provisions for conformance for the iso iec 20944 series which provides the bindings and their interoperability for mdrs ref cite web author iso url http www iso org iso home store catalogue tc catalogue detail htm csnumber 51914 date 2013 01 08 accessdate 2013 10 04 title iso iec 20944 1 2013 ref 2 data sort value 19502 iso iec 19502 information technology meta object facility meta object facility mof published 2005 defines a metamodel using mof and a set of interfaces using open distributed processing odp that can be used to define and manipulate a set of interoperable metamodels and their corresponding models ref cite web title iso iec 19502 2005 accessdate 2013 10 04 url http www iso org iso home store catalogue tc catalogue detail htm csnumber 32621 date 2011 03 17 author iso ref 2 data sort value 19773 iso iec 19773 information technology metadata registries mdr modules published 2011 specifies small modules of data to be used or reused in applications ref cite web url http www iso org iso iso catalogue catalogue tc catalogue detail htm csnumber 41769 date 2011 09 01 accessdate 2013 10 04 author iso title iso iec 19773 2011 ref 2 data sort value 09075 iso iec 9075 1 http standards iso org ittf licence html free information technology database languages sql standardization sql part 1 framework sql framework published 2011 defines the conceptual framework to specify the grammar of sql and the result of processing statements in that language by an sql implementation ref cite journal title iso iec 9075 1 page 1 date 2008 07 15 edition 3 author iso ref ref cite web url http www iso org iso home store catalogue tc catalogue detail htm csnumber 53681 date 2013 02 04 accessdate 2013 10 04 author iso title iso iec 9075 1 2011 ref 3 data sort value 13249 iso iec 13249 3 information technology database languages sql multimedia and application packages part 3 spatial published 2011 defines spatial user defined types routines and schemas for generic spatial data handling ref cite web date 2011 08 22 url http www iso org iso home store catalogue tc catalogue detail htm csnumber 53698 accessdate 2013 10 04 author iso title iso iec 13249 3 2011 ref 4 see also iso iec jtc1 list of international organization for standardization standards list of iso standards american national standards institute international organization for standardization international electrotechnical commission references reflist 2 external links http www iso org iso home standards development list of iso technical committees iso technical committee htm commid 45342 iso iec jtc 1 sc 32 page at iso defaultsort iso iec jtc1 sc32 category iso iec jtc1 subcommittees 032 category standards organizations category data management category data interchange standards'
b'refimprove date june 2011 quote box quote the essence of abstractions is preserving information that is relevant in a given context and forgetting information that is irrelevant in that context source john v guttag ref cite book edition spring 2013 publisher the mit press isbn 9780262519632 last guttag first john v title introduction to computation and programming using python location cambridge massachusetts date 2013 01 18 ref width 25 in software engineering and computer science abstraction is a technique for arranging complexity of computer systems it works by establishing a level of complexity on which a person interacts with the system suppressing the more complex details below the current level the programmer works with an idealized interface usually well defined and can add additional levels of functionality that would otherwise be too complex to handle for example a programmer writing code that involves numerical operations may not be interested in the way numbers are represented in the underlying hardware e g whether they re 16 bit or 32 bit integer computer science integers and where those details have been suppressed it can be said that they were abstracted away leaving simply numbers with which the programmer can work in addition a task of sending an email message across continents would be extremely complex if the programmer had to start with a piece of fiber optic cable and basic hardware components by using layers of complexity that have been created to abstract away the physical cables and network layout and presenting the programmer with a virtual data channel this task is manageable abstraction can apply to control or to data control abstraction is the abstraction of actions while data abstraction is that of data structures control abstraction involves the use of subroutine s and control flow abstractions data abstraction allows handling pieces of data in meaningful ways for example it is the basic motivation behind the datatype the notion of an object computer science object in object oriented programming can be viewed as a way to combine abstractions of data and code the same abstract definition can be used as a common interface computer science interface for a family of objects with different implementations and behaviors but which share the same meaning the inheritance computer science inheritance mechanism in object oriented programming can be used to define an class computer science abstract abstract class as the common interface the recommendation that programmers use abstractions whenever suitable in order to avoid duplication usually code duplication of code is known as the abstraction principle computer programming abstraction principle the requirement that a programming language provide suitable abstractions is also called the abstraction principle rationale computing mostly operates independently of the concrete world the hardware implements a model of computation that is interchangeable with others the software is structured in software architecture architecture s to enable humans to create the enormous systems by concentrating on a few issues at a time these architectures are made of specific choices of abstractions greenspun s tenth rule is an aphorism on how such an architecture is both inevitable and complex a central form of abstraction in computing is language abstraction new artificial languages are developed to express specific aspects of a system modeling languages help in planning computer language s can be processed with a computer an example of this abstraction process is the generational development of programming language s from the first generation programming language machine language to the second generation programming language assembly language and the third generation programming language high level language each stage can be used as a stepping stone for the next stage the language abstraction continues for example in scripting language s and domain specific programming language s within a programming language some features let the programmer create new abstractions these include subroutine s module programming modules polymorphism computer science polymorphism and software component s some other abstractions such as software design pattern s and software architecture architecture examples architectural styles remain invisible to a translator computing translator and operate only in the design of a system some abstractions try to limit the range of concepts a programmer needs to be aware of by completely hiding the abstractions that they in turn are built on the software engineer and writer joel spolsky has criticised these efforts by claiming that all abstractions are leaky abstraction leaky that they can never completely hide the details below ref cite web last1 spolsky first1 joel title the law of leaky abstractions url http www joelonsoftware com articles leakyabstractions html ref however this does not negate the usefulness of abstraction some abstractions are designed to interoperate with other abstractions for example a programming language may contain a foreign function interface for making calls to the lower level language language features programming languages main programming language different programming languages provide different types of abstraction depending on the intended applications for the language for example in object oriented programming language s such as c object pascal or java programming language java the concept of abstraction has itself become a declarative statement using the keyword computer programming keyword s code virtual code in c or code abstract code ref name oracle java abstract cite web title abstract methods and classes url http docs oracle com javase tutorial java iandi abstract html website the java\xe2\x84\xa2 tutorials publisher oracle accessdate 4 september 2014 ref and code interface code ref name oracle java interface cite web title using an interface as a type url http docs oracle com javase tutorial java iandi interfaceastype html website the java\xe2\x84\xa2 tutorials publisher oracle accessdate 4 september 2014 ref in java programming language java after such a declaration it is the responsibility of the programmer to implement a class computer science class to instantiate the object computer science object of the declaration functional programming language s commonly exhibit abstractions related to functions such as lambda abstraction s making a term into a function of some variable and higher order function s parameters are functions this has to be merged in the following sections modern members of the lisp programming language family such as clojure scheme programming language scheme and common lisp support macro computer science syntactic macros macro systems to allow syntactic abstraction other programming languages such as scala programming language scala also have macros or very similar metaprogramming features for example haskell programming language haskell has template haskell and ocaml has metaocaml these can allow a programmer to eliminate boilerplate code abstract away tedious function call sequences implement new control flow control flow structures and implement domain specific language domain specific languages dsls which allow domain specific concepts to be expressed in concise and elegant ways all of these when used correctly improve both the programmer s efficiency and the clarity of the code by making the intended purpose more explicit a consequence of syntactic abstraction is also that any lisp dialect and in fact almost any programming language can in principle be implemented in any modern lisp with significantly reduced but still non trivial in some cases effort when compared to more traditional programming languages such as python programming language python c programming language c or java programming language java specification methods main formal specification analysts have developed various methods to formally specify software systems some known methods include abstract model based method vdm z algebraic techniques larch clear obj act one casl process based techniques lotos sdl estelle trace based techniques special tam knowledge based techniques refine gist specification languages main specification language specification languages generally rely on abstractions of one kind or another since specifications are typically defined earlier in a project and at a more abstract level than an eventual implementation the unified modeling language uml specification language for example allows the definition of abstract classes which in a waterfall project remain abstract during the architecture and specification phase of the project control abstraction main control flow programming languages offer control abstraction as one of the main purposes of their use computer machines understand operations at the very low level such as moving some bits from one location of the memory to another location and producing the sum of two sequences of bits programming languages allow this to be done in the higher level for example consider this statement written in a pascal programming language pascal like fashion code a 1 2 5 code to a human this seems a fairly simple and obvious calculation one plus two is three times five is fifteen however the low level steps necessary to carry out this evaluation and return the value 15 and then assign that value to the variable a are actually quite subtle and complex the values need to be converted to binary representation often a much more complicated task than one would think and the calculations decomposed by the compiler or interpreter into assembly instructions again which are much less intuitive to the programmer operations such as shifting a binary register left or adding the binary complement of the contents of one register to another are simply not how humans think about the abstract arithmetical operations of addition or multiplication finally assigning the resulting value of 15 to the variable labeled a so that a can be used later involves additional behind the scenes steps of looking up a variable s label and the resultant location in physical or virtual memory storing the binary representation of 15 to that memory location etc without control abstraction a programmer would need to specify all the register binary level steps each time they simply wanted to add or multiply a couple of numbers and assign the result to a variable such duplication of effort has two serious negative consequences it forces the programmer to constantly repeat fairly common tasks every time a similar operation is needed it forces the programmer to program for the particular hardware and instruction set structured programming main structured programming structured programming involves the splitting of complex program tasks into smaller pieces with clear flow control and interfaces between components with reduction of the complexity potential for side effects in a simple program this may aim to ensure that loops have single or obvious exit points and where possible to have single exit points from functions and procedures in a larger system it may involve breaking down complex tasks into many different modules consider a system which handles payroll on ships and at shore offices the uppermost level may feature a menu of typical end user operations within that could be standalone executables or libraries for tasks such as signing on and off employees or printing checks within each of those standalone components there could be many different source files each containing the program code to handle a part of the problem with only selected interfaces available to other parts of the program a sign on program could have source files for each data entry screen and the database interface which may itself be a standalone third party library or a statically linked set of library routines either the database or the payroll application also has to initiate the process of exchanging data with between ship and shore and that data transfer task will often contain many other components these layers produce the effect of isolating the implementation details of one component and its assorted internal methods from the others object oriented programming embraces and extends this concept data abstraction main abstract data type data abstraction enforces a clear separation between the abstract properties of a data type and the concrete details of its implementation the abstract properties are those that are visible to client code that makes use of the data type the interface to the data type while the concrete implementation is kept entirely private and indeed can change for example to incorporate efficiency improvements over time the idea is that such changes are not supposed to have any impact on client code since they involve no difference in the abstract behaviour for example one could define an abstract data type called lookup table which uniquely associates keys with values and in which values may be retrieved by specifying their corresponding keys such a lookup table may be implemented in various ways as a hash table a binary search tree or even a simple linear list computing list of key value pairs as far as client code is concerned the abstract properties of the type are the same in each case of course this all relies on getting the details of the interface right in the first place since any changes there can have major impacts on client code as one way to look at this the interface forms a contract on agreed behaviour between the data type and client code anything not spelled out in the contract is subject to change without notice this makes no sense to me user takuyamurata taku 07 13 19 june 2005 utc languages that implement data abstraction include ada programming language ada and modula 2 object oriented languages are commonly claimed by whom date march 2009 to offer data abstraction however their inheritance computer science inheritance concept tends to put information in the interface that more properly belongs in the implementation thus changes to such information ends up impacting client code leading directly to the fragile binary interface problem abstraction in object oriented programming main object computer science in object oriented programming theory abstraction involves the facility to define objects that represent abstract actors that can perform work report on and change their state and communicate with other objects in the system the term encapsulation object oriented programming encapsulation refers to the hiding of state computer science state details but extending the concept of data type from earlier programming languages to associate behavior most strongly with the data and standardizing the way that different data types interact is the beginning of abstraction when abstraction proceeds into the operations defined enabling objects of different types to be substituted it is called polymorphism computer science polymorphism when it proceeds in the opposite direction inside the types or classes structuring them to simplify a complex set of relationships it is called delegation object oriented programming delegation or inheritance computer science inheritance various object oriented programming languages offer similar facilities for abstraction all to support a general strategy of polymorphism computer science polymorphism in object oriented programming which includes the substitution of one type in object oriented programming type for another in the same or similar role although not as generally supported a configuration in object oriented programming configuration or image or package may predetermine a great many of these name binding bindings at compile time link time or loadtime this would leave only a minimum of such bindings to change at run time program lifecycle phase run time common lisp object system or self programming language self for example feature less of a class instance distinction and more use of delegation for polymorphism in object oriented programming polymorphism individual objects and functions are abstracted more flexibly to better fit with a shared functional heritage from lisp programming language lisp c exemplifies another extreme it relies heavily on generic programming templates and method overloading overloading and other static bindings at compile time which in turn has certain flexibility problems although these examples offer alternate strategies for achieving the same abstraction they do not fundamentally alter the need to support abstract nouns in code all programming relies on an ability to abstract verbs as functions nouns as data structures and either as processes consider for example a sample java programming language java fragment to represent some common farm animals to a level of abstraction suitable to model simple aspects of their hunger and feeding it defines an code animal code class to represent both the state of the animal and its functions source lang java public class animal extends livingthing private location loc private double energyreserves public boolean ishungry return energyreserves 2 5 public void eat food food consume food energyreserves food getcalories public void moveto location location move to new location this loc location source with the above definition one could create objects of type tt animal tt and call their methods like this source lang java thepig new animal thecow new animal if thepig ishungry thepig eat tablescraps if thecow ishungry thecow eat grass thecow moveto thebarn source in the above example the class code animal code is an abstraction used in place of an actual animal code livingthing code is a further abstraction in this case a generalisation of code animal code if one requires a more differentiated hierarchy of animals to differentiate say those who provide milk from those who provide nothing except meat at the end of their lives that is an intermediary level of abstraction probably dairyanimal cows goats who would eat foods suitable to giving good milk and meatanimal pigs steers who would eat foods to give the best meat quality such an abstraction could remove the need for the application coder to specify the type of food so s he could concentrate instead on the feeding schedule the two classes could be related using inheritance computer science inheritance or stand alone and the programmer could define varying degrees of polymorphism computer science polymorphism between the two types these facilities tend to vary drastically between languages but in general each can achieve anything that is possible with any of the others a great many operation overloads data type by data type can have the same effect at compile time as any degree of inheritance or other means to achieve polymorphism the class notation is simply a coder s convenience object oriented design main object oriented design decisions regarding what to abstract and what to keep under the control of the coder become the major concern of object oriented design and domain analysis mdash actually determining the relevant relationships in the real world is the concern of object oriented analysis or legacy analysis in general to determine appropriate abstraction one must make many small decisions about scope domain analysis determine what other systems one must cooperate with legacy analysis then perform a detailed object oriented analysis which is expressed within project time and budget constraints as an object oriented design in our simple example the domain is the barnyard the live pigs and cows and their eating habits are the legacy constraints the detailed analysis is that coders must have the flexibility to feed the animals what is available and thus there is no reason to code the type of food into the class itself and the design is a single simple animal class of which pigs and cows are instances with the same functions a decision to differentiate dairyanimal would change the detailed analysis but the domain and legacy analysis would be unchanged mdash thus it is entirely under the control of the programmer and we refer to abstraction in object oriented programming as distinct from abstraction in domain or legacy analysis considerations when discussing formal semantics of programming languages formal methods or abstract interpretation abstraction refers to the act of considering a less detailed but safe definition of the observed program behaviors for instance one may observe only the final result of program executions instead of considering all the intermediate steps of executions abstraction is defined to a concrete more precise model of execution abstraction may be exact or faithful with respect to a property if one can answer a question about the property equally well on the concrete or abstract model for instance if we wish to know what the result of the evaluation of a mathematical expression involving only integers \xc3\x97 is worth modular arithmetic modulo n we need only perform all operations modulo n a familiar form of this abstraction is casting out nines abstractions however though not necessarily exact should be sound that is it should be possible to get sound answers from them mdash even though the abstraction may simply yield a result of undecidable problem undecidability for instance we may abstract the students in a class by their minimal and maximal ages if one asks whether a certain person belongs to that class one may simply compare that person s age with the minimal and maximal ages if his age lies outside the range one may safely answer that the person does not belong to the class if it does not one may only answer i don t know the level of abstraction included in a programming language can influence its overall usability the cognitive dimensions framework includes the concept of abstraction gradient in a formalism this framework allows the designer of a programming language to study the trade offs between abstraction and other characteristics of the design and how changes in abstraction influence the language usability abstractions can prove useful when dealing with computer programs because non trivial properties of computer programs are essentially undecidable problem undecidable see rice s theorem as a consequence automatic methods for deriving information on the behavior of computer programs either have to drop termination on some occasions they may fail crash or never yield out a result soundness they may provide false information or precision they may answer i don t know to some questions abstraction is the core concept of abstract interpretation model checking generally takes place on abstract versions of the studied systems levels of abstraction main abstraction layer computer science commonly presents levels or less commonly layers of abstraction wherein each level represents a different model of the same information and processes but uses a system of expression involving a unique set of objects and compositions that apply only to a particular domain ref luciano floridi http www cs ox ac uk activities ieg research reports ieg rr221104 pdf levellism and the method of abstraction ieg research report 22 11 04 ref each relatively abstract higher level builds on a relatively concrete lower level which tends to provide an increasingly granular representation for example gates build on electronic circuits binary on gates machine language on binary programming language on machine language applications and operating systems on programming languages each level is embodied but not determined by the level beneath it making it a language of description that is somewhat self contained database systems main database management system since many users of database systems lack in depth familiarity with computer data structures database developers often hide complexity through the following levels image data abstraction levels png thumb data abstraction levels of a database system physical level the lowest level of abstraction describes how a system actually stores data the physical level describes complex low level data structures in detail logical level the next higher level of abstraction describes what data the database stores and what relationships exist among those data the logical level thus describes an entire database in terms of a small number of relatively simple structures although implementation of the simple structures at the logical level may involve complex physical level structures the user of the logical level does not need to be aware of this complexity this referred to as physical data independence database administrator s who must decide what information to keep in a database use the logical level of abstraction view level the highest level of abstraction describes only part of the entire database even though the logical level uses simpler structures complexity remains because of the variety of information stored in a large database many users of a database system do not need all this information instead they need to access only a part of the database the view level of abstraction exists to simplify their interaction with the system the system may provide many view database view s for the same database layered architecture main abstraction layer the ability to provide a design of different levels of abstraction can simplify the design considerably enable different role players to effectively work at various levels of abstraction support the portability of software artifact s model based ideally systems design and business process modeling business process design can both use this some software modeling design processes specifically generate designs that contain various levels of abstraction layered architecture partitions the concerns of the application into stacked groups layers it is a technique used in designing computer software hardware and communications in which system or network components are isolated in layers so that changes can be made in one layer without affecting the others see also abstraction principle computer programming abstraction inversion for an anti pattern of one danger in abstraction abstract data type for an abstract description of a set of data algorithm for an abstract description of a computational procedure bracket abstraction for making a term into a function of a variable data modeling for structuring data independent of the processes that use it encapsulation object oriented programming encapsulation for abstractions that hide implementation details greenspun s tenth rule for an aphorism about an the optimum point in the space of abstractions higher order function for abstraction where functions produce or consume other functions lambda abstraction for making a term into a function of some variable list of abstractions computer science program refinement refinement for the opposite of abstraction in computing references reflist foldoc further reading refbegin cite book author1 harold abelson author2 gerald jay sussman author3 julie sussman title structure and interpretation of computer programs url http mitpress mit edu sicp full text book book z h 10 html accessdate 22 june 2012 edition 2 date 25 july 1996 publisher mit press isbn 978 0 262 01153 2 cite web last spolsky first joel title the law of leaky abstractions url http www joelonsoftware com articles leakyabstractions html work joel on software date 11 november 2002 http www cs cornell edu courses cs211 2006sp lectures l08 abstraction 08 abstraction html abstraction information hiding cs211 course cornell university cite book author eric s roberts title programming abstractions in c a second course in computer science date 1997 cite web last palermo first jeffrey title the onion architecture url http jeffreypalermo com blog the onion architecture part 1 work jeffrey palermo date 29 july 2008 refend external links https sites google com site simulationarchitecture simarch example of layered architecture for distributed simulation systems use dmy dates date june 2011 defaultsort abstraction computer science category data management category programming paradigms category articles with example java code category abstraction'
b'introduction the term small data did not exist before the word big data which came into use in the 1990 s what we once called data is now called small data small data was not made useless by the advent of big data most data we see in our life is small data and it should not be overlooked in any field this article helps to define small data and to give examples in marketing and recruiting to help in understanding definition small data sm a\xc4\x93 \xc4\x81ll dh \xc9\x99 ta is data that is small enough for human comprehension ref cite news author rufus pollock url https www theguardian com news datablog 2013 apr 25 forget big data small data revolution title forget big data small data is the real revolution 124 news newspaper the guardian date accessdate 2016 10 02 ref ref cite web url http jwork org main node 18 title small data never heard this term website jwork org date accessdate 2016 10 02 ref it is data in a volume and format that makes it accessible informative and actionable ref cite web url http whatis techtarget com definition small data title what is small data definition from whatis com website whatis techtarget com date 2016 08 18 accessdate 2016 10 02 ref the term big data is about machines and small data is about people ref cite web author eric lundquist url http www eweek com enterprise apps small data analysis the next big thing advocates assert html title small data analysis the next big thing advocates assert website eweek com date 2013 09 10 accessdate 2016 10 02 ref this is to say that eye witness observations or five pieces of related data could be small data small data is what we used to think of as data the only way to comprehend big data is to reduce the data into small visually appealing objects representing various aspects of large data sets such as histogram chart s and scatter plots so sometimes big data is simplified to be like small data a formal definition of small data has been proposed by allen bonde vp of innovation at actuate corporation actuate small data connects people with timely meaningful insights derived from big data and or local sources organized and packaged often visually to be accessible understandable and actionable for everyday tasks ref cite web url http smalldatagroup com 2013 10 18 defining small data title defining small data publisher small data group date accessdate 2016 10 02 ref another definition of small data is the small set of specific attributes produced by the internet of things these are typically a small set of sensor data such as temperature wind speed vibration and status ref cite web author url http www forbes com sites mikekavis 2015 02 25 forget big data small data is driving the internet of things 4a72ffad661b title forget big data small data is driving the internet of things website forbes com date accessdate 2016 10 02 ref some examples of uses in business marketing bonde has written extensively about the topic for forbes ref cite web author url http www forbes com sites markfidelman 2012 10 30 these smart social apps bring big data down to size title these smart social apps bring big data down to size website forbes com date accessdate 2016 10 02 ref direct marketing news ref cite web url http www dmnews com why small data is the next big thing for marketers article 308376 title why small data is the next big thing for marketers dmn website dmnews com date 2013 08 22 accessdate 2016 10 02 ref cmo com ref cite web last bonde first allen url http www cmo com features articles 2013 11 20 think small time for html title think small time for marketers to move beyond the big data hype website cmo com date 2013 12 12 accessdate 2016 10 02 ref and other publications according to martin lindstrom in his book small data the tiny clues that uncover huge trends small data in customer research small data is seemingly insignificant behavioral observations containing very specific attributes pointing towards an unmet customer need small data is the foundation for break through ideas or completely new ways to turnaround brands ref cite web url https www martinlindstrom com small data title small data martin lindstrom bestselling author publisher martin lindstrom date accessdate 2016 10 02 ref conclusion small data is what we used to call data the hype about big data should not cause us to look down on small data small data s practicality and depth of insight is often better than big data references reflist category data management'
b'datafication is a modern technological trend turning many aspects of our life into computerised data ref name cukiermayer schoenberger2013 cite journal last cukier first kenneth last2 mayer schoenberger first2 viktor title the rise of big data journal foreign affairs issue may june pages 28 40 date 2013 url http www foreignaffairs com articles 139104 kenneth neil cukier and viktor mayer schoenberger the rise of big data accessdate 24 january 2014 ref and transforming this information into new forms of value ref name schuttoneil2014 cite book last o neil first cathy last2 schutt first2 rachel title doing data science publisher o reilly media date 2013 pages 406 isbn 978 1 4493 5865 5 ref examples of datafication as applied to social and communication media are how twitter datafies stray thoughts or datafication of human resource management hr by linkedin and others alternative examples are diverse and include aspects of the built environment and design via engineering and or other tools that tie data to formal functional or other physical media outcomes of which formsolver ref https www formsolver com ref is an example file shape optimization for buildings by formsolver jpg thumbnail right example datafication of the skin and form of a building to assist engineers designers and architects determine the performance of particular building geometries example provided courtesy of formsolver com file emerging shape optimization families for buildings by formsolver jpg thumbnail right example shape families resulting from differing goals when data is used for the purposes of shape optimization example provided courtesy of formsolver com see also big data references reflist category information science category technology forecasting category data management category big data category information society tech stub'
b'orphan date january 2016 quality of data qod is a designation coined by l veiga that specifies and describes the required quality of service of a distributed storage system from the consistency point of view of its data it can be used to support big data big data management frameworks workflow management and hpc systems mainly for data replication and consistency it takes into account data semantics namely time interval of data freshness sequence of tolerable number of outstanding versions of the data read before refresh and value divergence allowed before displaying it initially it was based in a model from an existing research work regarding vector field consistency ref cite conference author1 nuno santos author2 lu\xc3\xads veiga author3 paulo ferreira year 2007 title vector field consistency for adhoc gaming booktitle acm ifip usenix middleware conference 2007 url http www gsd inesc id pt lveiga msc 08 09 vfc middleware 07 pdf format pdf ref awarded the best paper prize in the acm ifip usenix middleware conference 2007 and later enhanced for increased scalability and fault tolerance ref cite conference author1 lu\xc3\xads veiga author2 andr\xc3\xa9 negr\xc3\xa3o author3 nuno santos author4 paulo ferreira year 2010 title unifying divergence bounding and locality awareness in replicated systems with vector field consistency booktitle jisa journal of internet services and applications volume 1 number 2 95 115 springer 2010 url http www gsd inesc id pt lveiga vfc jisa 2010 pdf format pdf ref this consistency model has been successfully applied and proven in big data key value store apache hbase ref group nb url https hbase apache org ref initially designed as a middleware ref cite conference author1 sergio est\xc3\xa9ves author2 jo\xc3\xa3o silva author3 lu\xc3\xads veiga last author amp yes year 2013 title quality of service for consistency of data geo replication in cloud computing booktitle euro par 2012 parallel processing springer berlin heidelberg 2012 285 297 url http www gsd inesc id pt sesteves papers vfc3 europar12 pdf format pdf ref module seating between clusters from separate data centres the hbase qod coupling ref cite conference author1 \xc3\xa1lvaro garc\xc3\xada recuero author2 sergio est\xc3\xa9ves author3 lu\xc3\xads veiga year 2013 title quality of data for consistency levels in geo replicated cloud data stores booktitle ieee cloudcom 2013 url http www inesc id pt ficheiros publicacoes 9253 pdf format pdf ref minimises bandwidth usage and optimises resources allocation during replication achieving the desired consistency level at a more fine grained level qod is defined by the three dimensions of vector k \xce\xb8 \xcf\x83 \xce\xbd but with a broader view of the issue applicable also to large scale data management techniques in regards to their timely delivery ref group nb sub url http www 01 ibm com software data quality sub ref other descriptions quality of data should not be confused with other definitions for data quality such as ref cite conference author1 richard y wang year 1992 title toward quality data an attribute based approach booktitle decision support systems 13 mit url http web mit edu tdqm www tdqmpub toward 20quality 20data pdf format pdf ref ref cite conference author1 george a mihaila author2 louiqa raschid author3 mar\xc3\xada esther vidal year 2000 title using quality of data metadata for source selection and ranking booktitle url http citeseerx ist psu edu viewdoc summary doi 10 1 1 34 9361 format ref completeness validity accuracy notes references group nb references references category data management'
b'geospatial metadata also geographic metadata or simply metadata when used in a geographic context is a type of metadata that is applicable to objects that have an explicit or implicit geography geographic extent i e are associated with some position on the surface of the globe such objects may be stored in a geographic information system gis or may simply be documents data sets images or other objects services or related items that exist in some other native environment but whose features may be appropriate to describe in a geographic metadata catalog may also be known as a data directory data inventory etc definition i so 19115 2013 geographic information metadata ref name 0 cite web url https www iso org obp ui iso std iso 19115 1 ed 1 v1 en title iso 19115 1 2014 en last international organization for standardization first date 2014 04 01 website iso publisher access date 2016 04 01 ref from iso tc 211 the industry standard for geospatial metadata describes its scope as follows this standard provides information about the identification the extent the quality the spatial and temporal aspects the content the spatial reference the portrayal distribution and other properties of digital geographic data and services ref name 0 iso 19115 2013 also provides for non digital mediums t hough this part of https www iso org obp ui iso std iso 19115 en iso 19115 is applicable to digital data and services its principles can be extended to many other types of resources such as maps charts and textual documents as well as non geographic data ref name 0 the u s federal geographic data committee fgdc describes geospatial metadata as follows a metadata record is a file of information usually presented as an xml document which captures the basic characteristics of a data or information resource it represents the who what when where why and how of the resource geospatial metadata commonly document geographic digital data such as geographic information system gis files geospatial databases and earth imagery but can also be used to document geospatial resources including data catalogs mapping applications data models and related websites metadata records include core library catalog elements such as title abstract and publication data geographic elements such as geographic extent and projection information and database elements such as attribute label definitions and attribute domain values ref cite web url http www fgdc gov metadata title geospatial metadata federal geographic data committee website www fgdc gov access date 2016 04 01 ref history the growing appreciation of the value of geospatial metadata through the 1980s and 1990s led to the development of a number of initiatives to collect metadata according to a variety of formats either within agencies communities of practice or countries groups of countries for example nasa s dif metadata format was developed during an earth science and applications data systems workshop in 1987 ref http gcmd nasa gov user difguide whatisadif html gene major and lola olsen a short history of the dif on gcmd website visited 16 october 2006 ref and formally approved for adoption in 1988 similarly the u s fgdc developed its geospatial metadata standard over the period 1992 1994 ref http libraries mit edu guides subjects metadata standards fgdc html mit libraries guide federal geographic data committee fgdc metadata on mit libraries website visited 16 october 2006 ref the spatial information council of australia and new zealand anzlic ref cite web url http anzlic gov au sites default files files anzlicmetadataprofileguidelines v1 2 pdf title anzlic metadata profile guidelines version 1 2 july 2011 year 2011 publisher anzlic accessdate 2011 04 11 quote anzlic the spatial information council of australia and new zealand formerly known as the australia new zealand land information council ref a combined body representing spatial data interests in australia and new zealand released version 1 of its metadata guidelines in 1996 ref http anzlic gov au resources anzlic metadata profile anzlic metadata guidelines core metadata elements for geographic data in australia and new zealand version 2 february 2001 ref iso tc 211 undertook the task of harmonizing the range of formal and de facto standards over the approximate period 1999 2002 resulting in the release of iso 19115 geographic information metadata in 2003 and a subsequent revision in 2013 as of 2011 individual countries communities of practice agencies etc have started re casting their previously used metadata standards as profiles or recommended subsets of iso 19115 occasionally with the inclusion of additional metadata elements as formal extensions to the iso standard the growth in popularity of internet technologies and data formats such as extensible markup language xml during the 1990s led to the development of mechanisms for exchanging geographic metadata on the world wide web web in 2004 the open geospatial consortium released the current version 3 1 of geography markup language gml an xml grammar for expressing geospatial features and corresponding metadata with the growth of the semantic web in the 2000s the geospatial community has begun to develop ontology computer science ontologies for representing semantic geospatial metadata some examples include the http www ordnancesurvey co uk oswebsite ontology hydrology and administrative ontologies developed by the ordnance survey in the united kingdom iso 19115 geographic information metadata iso 19115 is a standard of the international organization for standardization iso ref iso 19115 geographic information metadata international organization for standardization iso geneva 2003 ref the standard is part of the iso tc 211 iso geographic information suite of standards 19100 series iso 19115 and its parts define how to describe geographical information and associated services including contents spatial temporal purchases data quality access and rights to use the objective of this international standard is to provide a clear procedure for the description of digital geographic data sets so that users will be able to determine whether the data in a holding will be of use to them and how to access the data by establishing a common set of metadata terminology definitions and extension procedures this standard promotes the proper use and effective retrieval of geographic data ref cite web title iso 19115 metadata factsheet url http www isotc211 org outreach overview factsheet 19115 pdf publisher ag outreach accessdate 2012 11 22 ref iso 19115 was revised in 2013 to accommodate growing use of the internet for metadata management as well as add many new categories of metadata elements referred to as codelists and the ability to limit the extent of metadata use temporally or by user ref cite web url https wiki earthdata nasa gov display nasaiso nasa metadata and the new iso 19115 1 capabilities title nasa metadata and the new iso 19115 1 capabilities nasa iso for eosdis earthdata wiki website wiki earthdata nasa gov access date 2016 04 01 ref expand section date june 2012 iso 19139 geographic information metadata xml schema implementation iso 19139 2012 ref cite web url https www iso org obp ui iso std iso ts 19139 2 ed 1 v1 en title iso ts 19139 2 2012 en last international organization for standardization first date 2012 12 15 website iso publisher access date 2016 04 01 ref provides the xml implementation schema for iso 19115 specifying the metadata record format and may be used to describe validate and exchange geospatial metadata prepared in xml ref http marinemetadata org references iso19139 iso 19139 geographic information metadata xml schema implementation marine metadata interoperability project ref the standard is part of the iso tc 211 iso geographic information suite of standards 19100 series and provides a spatial metadata xml spatial metadata extensible mark up language smxml encoding an xml schema implementation derived from iso 19115 geographic information metadata the metadata includes information about the identification constraint extent quality spatial and temporal reference distribution lineage and maintenance of the digital geographic data set expand section date june 2012 metadata directories also known as metadata catalogues or data directories need discussion of and subsections on gcmd fgdc metadata gateway asdd european and canadian initiatives etc etc http gisinventory net gis inventory national gis inventory system which is maintained by the us based national states geographic information council national states geographic information council nsgic as a tool for the entire us gis community its primary purpose is to track data availability and the status of geographic information system gis implementation in state and local governments to aid the planning and building of statewide spatial data infrastructures ssdi the random access metadata for online nationwide assessment ramona database is a critical component of the gis inventory ramona moves its fgdc compliant metadata csdgm standard for each data layer to a web folder and a catalog service for the web csw that can be harvested by federal programs and others this provides far greater opportunities for discovery of user information the gis inventory website was originally created in 2006 by nsgic under award na04nos4730011 from the coastal services center national oceanic and atmospheric administration u s department of commerce the department of homeland security has been the principal funding source since 2008 and they supported the development of the version 5 during 2011 2012 under order number hshqdc 11 p 00177 the federal emergency management agency and national oceanic and atmospheric administration have provided additional resources to maintain and improve the gis inventory some us federal programs require submission of csdgm compliant metadata for data created under grants and contracts that they issue the gis inventory provides a very simple interface to create the required metadata http gcmd nasa gov gcmd global change master directory s goal is to enable users to locate and obtain access to earth science data sets and services relevant to global change and earth science research the gcmd database holds more than 20 000 descriptions of earth science data sets and services covering all aspects of earth and environmental sciences http earthdata nasa gov echo echo the eos clearing house echo is a spatial and temporal metadata registry service registry and order broker it allows users to more efficiently search and access data and services through the http reverb earthdata nasa gov echo reverb client or application programmer interfaces apis echo stores metadata from a variety of science disciplines and domains totalling over 3400 earth science data sets and over 118 nbsp million granule records http www gogeo ac uk gogeo gogeo gogeo is a service run by edina university of edinburgh and is supported by jisc gogeo allows users to conduct geographically targeted searches to discover geospatial datasets gogeo searches many data portals from the he and fe community and beyond gogeo also allows users to create standards compliant metadata through its geodoc metadata editor geospatial metadata tools there are many commercial gis or geospatial products that support metadata viewing and editing on gis resources for example esri s arcgis desktop socet gxp autodesk s autocad map 3d 2008 arcitecta s mediaflux and intergraph s geomedia support geospatial metadata extensively http gisinventory net gis inventory is a free web based tool that provides a very simple interface to create geospatial metadata participants create a profile and document their data layers through a survey style interface the gis inventory produces metadata that is compliant with the federal content standard for digital geospatial metadata csdgm the gis inventory is also capably of ingesting already completed metadata through document upload and web server connectivity through the gis inventory web services metadata are automatically shared with us federal agencies http geonetwork opensource org geonetwork opensource is a comprehensive free and open source software solution to manage and publish geospatial metadata and services based on international metadata and catalog standards the software is part of the open source geospatial foundation s software stack http geocat net bridge geocat bridge allows to edit validate and directly publish metadata from arcgis desktop to http geonetwork opensource org geonetwork and generic csw catalogs and publishes data as map services on http geoserver org geoserver several metadata profiles are supported pycsw is an ogc csw server implementation written in python pycsw fully implements the opengis catalogue service implementation specification catalog service for the web catalogue service for the web the project is certified ogc compliant and is an ogc reference implementation http catmdedit sourceforge net catmdedit terracatalog arccatalog arcgis server portal http geonetwork opensource org geonetwork opensource http www conterra de en products sdi terracatalog index shtm ime http www intelec ca html en technologies m3cat html m3cat metad http www gigateway org uk metadata metagenie html metagenie parcs canada metadata editor mapit cadit nokis editor expand section date june 2008 references references http anzlic gov au sites default files files anzlicmetadataprofileguidelines v1 2 pdf anzlic metadata profile version 1 2 viewed july 2011 external links http www fgdc gov metadata fgdc metadata page http gcmd nasa gov global change master directory gcmd http wiki milcord com wiki geospatial exploitation of motion imagery geospatial exploitation of motion imagery is a geospatially aware and integrated intelligent video surveillance ivs software system targeted at real time and forensic video analytic and mining applications that require low resolution detection tracking and classification of moving objects people and vehicles in outdoor wide area scenes http www iso org iso en cataloguedetailpage cataloguedetail csnumber 26020 iso 19115 2003 geographic information metadata http www iso org iso iso catalogue catalogue tc catalogue detail htm csnumber 32557 geographic information metadata xml schema implementation http www earthdatamodels org designs metadata bgs html earthdatamodels design for metadata is a logical data model and physical implementation of a spatial metadata database based on iso19115 and is inspire compliant use dmy dates date january 2011 category data management category metadata category geographic data and information'
b'vinelink com vine is a national website in the united states that allows victims of crime and the general public to track the movements of prisoners held by the various us states states and territories of the united states territories the first four letters in the websites name vine are an acronym for victim information and notification everyday vinelink com displays information based on the information provided by the various states departments of correction and other law enforcement agencies on whether an inmate is in custody has been released has been granted parole or probation or has escaped from custody in some cases the website will reveal whether a defendant has been granted parole or probation but then subsequently violated conditions of their release and become a fugitive ref cite web url http www correct state ak us probation parole vine title automated victim notification system vine publisher alaska department of corrections accessdate 2014 05 20 ref information provided on vinelink com represents metadata in that the website lists a defendants custody status but does not list what the individual is charged with their criminal history or the amount of their bail if applicable internet users accessing the vinelink com website choose from a map of states and provinces within the united states where they wish to perform a search for an inmate the user may then search for an individual using the inmate or parolees name or by entering the inmates specific department of corrections inmate number if known when the inmates custody status changes users who have registered to be notified of such changes will be notified via email phone or both ref cite web url http www doj state or us victims pages vine aspx title victim information and notification everyday vine publisher oregon department of justice date 2013 02 03 accessdate 2014 05 20 ref this information is currently released upon request without the website requesting reasons for the users search or requiring payment as public records available to the general public inmate information is available for most states and for puerto rico on the website the states of arizona georgia u s state georgia massachusetts montana new hampshire and west virginia provide very limited information on the site the states of kansas maine and south dakota do not participate in the vine system ref cite web url http www theledger com article 20130203 news 130209793 title who to call vinelink publisher the ledger date 2010 accessdate 2014 05 20 ref the website does not provide data on prisoners detained by the united states federal government references external links http www vinelink com official website see also visit http www bop gov inmateloc the official federal bureau of prisons website to search for inmates being held by the united states federal bureau of prisons category data management category government services web portals in the united states category law enforcement websites category metadata category public records website stub crime stub'
b'multiple issues technical date june 2014 coi date july 2014 file patterns1024cities2 jpg thumb 500px 1024 cities that follow exactly zipf s law which implies that the first largest city is size 1 the second largest city is size 1 2 the third largest city is size 1 3 and the smallest city is size 1 1024 the left pattern is produced by head tail breaks while the right one by natural breaks also known as jenks natural breaks optimization head tail breaks is a new clustering algorithm scheme for data with a heavy tailed distribution such as power laws and lognormal distributions the heavy tailed distribution can be simply referred to the scaling pattern of far more small things than large ones or alternatively numerous smallest a very few largest and some in between the smallest and largest the classification is done through dividing things into large or called the head and small or called the tail things around the arithmetic mean or average and then recursively going on for the division process for the large things or the head until the notion of far more small things than large ones is no longer valid or with more or less similar things left only ref name jiang1 jiang bin 2013a head tail breaks a new classification scheme for data with a heavy tailed distribution the professional geographer 65 3 482 494 ref head tail breaks is not just for classification but also for visualization of big data by keeping the head since the head is self similar to the whole head tail breaks can be applied not only to vector data such as points lines and polygons but also to raster data like digital elevation model dem motivation the head tail breaks is mainly motivated by inability of conventional classification methods such as equal intervals quantiles geometric progressions standard deviation and natural breaks commonly known as jenks natural breaks optimization for revealing the underlying scaling pattern of far more small things than large ones note that the notion of far more small things than large one is not only referred to geometric property but also to topological and semantic properties in this connection the notion should be interpreted as far more unpopular or less connected things than popular or well connected ones or far more meaningless things than meaningful ones method given some variable x that demonstrates a heavy tailed distribution there are far more small x than large ones take the average of all xi and obtain the first mean m1 then calculate the second mean for those xi greater than m1 and obtain m2 in the same recursive way we can get m3 depending on whether the ending condition of no longer far more small x than large ones is met for simplicity we assume there are three means m1 m2 and m3 this classification leads to four classes minimum m1 m1 m2 m2 m3 m3 maximum in general it can be represented as a recursive function as follows recursive function head tail breaks break the input data around mean or average into the head and the tail the head for data values greater the mean the tail for data values less the mean while head 40 head tail breaks head end function the resulting number of classes is referred to as ht index an alternative index to fractal dimension for characterizing complexity of fractals or geographic features the higher the ht index the more complex the fractals ref name jiang2 jiang bin and yin junjun 2014 ht index for quantifying the fractal or scaling structure of geographic features annals of the association of american geographers 104 3 530 541 ref recently a more sensitive ht index namely crg index ref cite journal title crg index a more sensitive ht index for enabling dynamic views of geographic features url http dx doi org 10 1080 00330124 2015 1099448 journal the professional geographer date 2015 12 09 issn 0033 0124 pages 1 13 volume 0 issue 0 doi 10 1080 00330124 2015 1099448 first peichao last gao first2 zhao last2 liu first3 meihui last3 xie first4 kun last4 tian first5 gang last5 liu ref has been developed and it is able to capture slight changes which ht index is unable to thus while ht index is an integer crg index is a real number a postgresql function for calculating ht index can be found here ref cite journal title a postgresql function for calculating the ht index url https www researchgate net publication 287533541 a postgresql function for calculating the ht index channel doi linkid 56777e5b08aebcdda0e962fe showfulltext true date 2015 01 01 doi 10 13140 rg 2 1 3041 0324 first peichao gao last kun tian ref threshold or its sensitivity the criterion to stop the iterative classification process using the head tail breaks method is that the remaining data i e the head part are not heavy tailed or simply the head part is no longer a minority i e the proportion of the head part is no longer less than a threshold such as 40 this threshold is suggested to be 40 by jiang et al 2013 ref name jiang3 just as the codes above i e head 40 but sometimes a larger threshold for example 50 or more can be used as jiang and yin 2014 ref name jiang2 noted in another article this condition can be relaxed for many geographic features such as 50 percent or even more however all heads percentage on average must be smaller than 40 or 41 42 indicating far more small things than large ones this sensitivity issue deserves further research in the future rank size plot and ra index a good tool to display the scaling pattern or the heavy tailed distribution is the rank size plot which is a scatter plot to display a set of values according to their ranks with this tool a new index ref cite journal last gao first peichao last2 liu first2 zhao last3 tian first3 kun last4 liu first4 gang date 2016 03 10 title characterizing traf\xef\xac\x81c conditions from the perspective of spatial temporal heterogeneity url http www mdpi com 2220 9964 5 3 34 journal isprs international journal of geo information language en volume 5 issue 3 pages 34 doi 10 3390 ijgi5030034 ref termed as the ratio of areas ra in a rank size plot was defined to characterize the scaling pattern the ra index has been successfully used in the estimation of traffic conditions however it should be noted that the ra index can only be used as a complementary method to the ht index because it is ineffective to capture the scaling structure of geographic features applications instead of more or less similar things there are far more small things than large ones surrounding us given the ubiquity of the scaling pattern head tail breaks is found to be of use to statistical mapping map generalization cognitive mapping and even perception of beauty ref name jiang3 jiang bin liu xintao and jia tao 2013 scaling of geographic space as a universal rule for map generalization annals of the association of american geographers 103 4 844 855 ref ref name jiang4 jiang bin 2013b the image of the city out of the underlying scaling of city artifacts or locations annals of the association of american geographers 103 6 1552 1566 ref ref name jiang5 jiang bin and sui daniel 2014 a new kind of beauty out of the underlying scaling of geographic space the professional geographer 66 4 676 686 ref it helps visualize big data since big data are likely to show the scaling property of far more small things than large ones the visualization strategy is to recursively drop out the tail parts until the head parts are clear or visible enough ref name jiang6 jiang bin 2015 head tail breaks for visualization of city structure and dynamics cities 43 69 77 ref in addition it helps delineate cities or natural cities to be more precise from various geographic information such as street networks social media geolocation data and nighttime images characterizing the imbalance as the head tail breaks method can be used iteratively to obtain head parts of a data set this method actually captures the underlying hierarchy of the data set for example if we divide the array 19 8 7 6 2 1 1 1 0 with the head tail breaks method we can get two head parts i e the first head part 19 8 7 6 and the second head part 19 these two head parts as well as the original array form a three level hierarchy the 1st level 19 the 2nd level 19 8 7 6 and the 3rd level 19 8 7 6 2 1 1 1 0 the number of levels of the above mentioned hierarchy is actually a characterization of the imbalance of the example array and this number of levels has been termed as the ht index ref name jiang2 with the ht index we are able to compare degrees of imbalance of two data sets for example the ht index of the example array 19 8 7 6 2 1 1 1 0 is 3 and the ht index of another array 19 8 8 8 8 8 8 8 8 is 2 therefore the degree of imbalance of the former array is higher than that of the latter array file natural cities of germany created from points of interest jpg thumb 250px right the left panel pattern contains 50 000 natural cities which can be put into 7 hierarchical levels it looks like a hair ball instead of showing all the 7 hierarchical levels we show 4 top levels by dropping out 3 low levels now with the right panel the scaling pattern of far more small cities than large ones emerges it is important to note that the right pattern or the remaining part after dropping out the tails is self similar to the whole or the left pattern thus the right pattern reflects the underlying structure of the left one and enables us to see the whole file headtail breaks of american dem jpg thumb 250px the scaling pattern of us terrain surface is distorted by the natural breaks but revealed by the head tail breaks delineating natural cities the term natural cities refers to the human settlements or human activities in general on earth s surface that are naturally or objectively defined and delineated from massive geographic information based on head tail division rule a non recursive form of head tail breaks ref name jiang7 jiang bin and miao yufan 2015 the evolution of natural cities from the perspective of location based social media the professional geographer 67 2 295 306 ref ref name long long ying 2016 redefining chinese city system with emerging new data applied geography 75 36 48 ref such geographic information could be from various sources such as massive street junctions ref name long and street ends a massive number of street blocks nighttime imagery and social media users locations etc distinctive from conventional cities the adjective natural could be explained not only by the sources of natural cities but also by the approach to derive them natural cities are derived from a meaningful cutoff averaged from a massive amount of units extracted from geographic information ref name jiang6 those units vary according to different kinds of geographic information for example the units could be area units for the street blocks and pixel values for the nighttime images a http www arcgis com home item html id 47b1d6fdd1984a6fae916af389cdc57d natural cities model has been created using arcgis model builder ref name ren ren zheng 2016 natural cities model in arcgis http www arcgis com home item html id 47b1d6fdd1984a6fae916af389cdc57d ref it follows the same process of deriving natural cities from location based social media ref name jiang7 namely building up huge triangular irregular network tin based on the point features street nodes in this case and regarding the triangles which are smaller than a mean value as the natural cities color rendering dem current color renderings for dem or density map are essentially based on conventional classifications such as natural breaks or equal intervals so they disproportionately exaggerate high elevations or high densities as a matter of fact there are not so many high elevations or high density locations ref name jiang8 jiang bin 2015 geospatial analysis requires a different way of thinking the problem of spatial heterogeneity geojournal 80 1 1 13 ref it was found that coloring based head tail breaks is more favorable than those by other classifications ref name wu wu jou hsuan 2015 examining the new kind of beauty using the human being as a measuring instrument http www diva portal org smash get diva2 805296 fulltext01 pdf ref software implementations the following implementations are available under free and open source software free open source software licenses https github com digmaa headtailbreaks ht calculator a winform application for obtaining related metrics of head tail breaks applying on a single data array http jsfiddle net mhkeller 5yatk ht in javascript a javascript implementation for applying head tail breaks on a single data array http fromto hig se bjg axwoman ht mapping tool a function in the free plug in axwoman 6 3 to arcmap 10 2 that conducts geo data symbolization automatically based on the head tail breaks classification https github com chad m head tail breaks algorithm ht in python python and javascript code for the head tail breaks algorithm it s works great for choropleth map coloring references one author date june 2014 reflist further reading further reading cleanup date june 2014 lin yue 2013 a comparison study on natural and head tail breaks involving digital elevation models http www diva portal org smash get diva2 658963 fulltext02 pdf wu jou hsuan 2015 the mirror of the self test http sharon19891101 wix com mirror of the self defaultsort head tail breaks category data management category cartography'
b'unreferenced date june 2014 file phuse computational science symposium 2016 26133831630 jpg thumb phuse computational science symposium 2016 phuse or pharmaceutical users software exchange is an independent not for profit organization that started in europe but now which serves as forum and global platform for clinical data management biostatistics and eclinical information technology professionals it provides three collaboration platforms for members a set online suits which implements worldwide collaboration tools a wiki a repository of videos phuse tube a blog a webforum and an archive phuse also publishes pharmaceutical programming an academic journal focusing on programming for drug regulation drug regulation environments of the pharmaceutical industry a quarterly newsletter phuse news in addition it organizes an annual meeting conference external links http www phuse eu official web site http www phusewiki org wiki index php title phuse wiki phuse wiki http www phuse eu blog blog aspx blog http www phuse eu society newsletters aspx newsletter http www phuse eu forum aspx forum http www phuse eu archive aspx archive http www phuse eu phusetube aspx phuse tube http www phuse eu publications aspx publications category pharmacy organizations category regulation therapeutic goods category biostatistics category data management'
b'refimprove date february 2012 data security means protecting data such as a database from destructive forces and from the unwanted actions of unauthorized users ref summers g 2004 data and databases in koehne h developing databases with access nelson australia pty limited p4 5 ref data security technologies disk encryption disk encryption refers to encryption technology that encrypts data on a hard disk drive disk encryption typically takes form in either software see disk encryption software or hardware see disk encryption hardware disk encryption is often referred to as on the fly encryption otfe or transparent encryption software versus hardware based mechanisms for protecting data software based security solutions encrypt the data to protect it from theft however a malicious program or a hacker could corrupt the data in order to make it unrecoverable making the system unusable hardware based security solutions can prevent read and write access to data and hence offer very strong protection against tampering and unauthorized access hardware based security or assisted computer security offers an alternative to software only computer security security token s such as those using pkcs 11 may be more secure due to the physical access required in order to be compromised access is enabled only when the token is connected and correct personal identification number pin is entered see two factor authentication however dongles can be used by anyone who can gain physical access to it newer technologies in hardware based security solves this problem offering full proof security for data working of hardware based security a hardware device allows a user to log in log out and set different privilege levels by doing manual actions the device uses biometric technology to prevent malicious users from logging in logging out and changing privilege levels the current state of a user of the device is read by controllers in peripheral devices such as hard disks illegal access by a malicious user or a malicious program is interrupted based on the current state of a user by hard disk and dvd controllers making illegal access to data impossible hardware based access control is more secure than protection provided by the operating systems as operating systems are vulnerable to malicious attacks by computer virus viruses and hackers the data on hard disks can be corrupted after a malicious access is obtained with hardware based protection software cannot manipulate the user privilege levels it is impossible for a hacker computer security hacker or a malicious program to gain access to secure data protected by hardware or perform unauthorized privileged operations this assumption is broken only if the hardware itself is malicious or contains a backdoor ref citation last1 waksman first1 adam last2 sethumadhavan first2 simha title silencing hardware backdoors volume pages periodical proceedings of the ieee symposium on security and privacy location oakland california url http www cs columbia edu simha preprint oakland11 pdf year 2011 issn doi isbn ref the hardware protects the operating system image and file system privileges from being tampered therefore a completely secure system can be created using a combination of hardware based security and secure system administration policies backups backup s are used to ensure data which is lost can be recovered from another source it is considered essential to keep a backup of any data in most industries and the process is recommended for any files of importance to a user data masking main data masking data masking of structured data is the process of obscuring masking specific data within a database table or cell to ensure that data security is maintained and sensitive information is not exposed to unauthorized personnel ref cite web title what is data obfuscation url http www dataobfuscation com au accessdate 1 march 2016 ref this may include masking the data from users for example so banking customer representatives can only see the last 4 digits of a customers national identity number developers who need real production data to test new software releases but should not be able to see sensitive financial data outsourcing vendors etc ref cite web url http searchsecurity techtarget com definition data masking title data masking accessdate 29 july 2016 ref data erasure data erasure is a method of software based overwriting that completely destroys all electronic data residing on a hard drive or other digital media to ensure that no sensitive data is leaked when an asset is retired or reused international laws and standards international laws in the united kingdom uk the data protection act 1998 data protection act is used to ensure that personal data is accessible to those whom it concerns and provides redress to individuals if there are inaccuracies ref cite web url https ico org uk for organisations guide to data protection principle 1 fair and lawful title data protection act accessdate 29 july 2016 ref this is particularly important to ensure individuals are treated fairly for example for credit checking purposes the data protection act states that only individuals and companies with legitimate and lawful reasons can process personal information and cannot be shared data privacy day is an international holiday started by the council of europe that occurs every january 28 ref name dataprivacyday cite web url http googleblog blogspot com 2008 01 celebrating data privacy html title celebrating data privacy author peter fleischer jane horvath shuman ghosemajumder publisher google blog accessdate 12 august 2011 year 2008 ref international standards the international standards iso iec 27001 2013 and iso iec 27002 2013 covers data security under the topic of information security and one of its cardinal principles is that all stored information i e data should be owned so that it is clear whose responsibility it is to protect and control access to that data the trusted computing group is an organization that helps standardize computing security technologies the pci dss payment card industry data security standard is a proprietary international information security standard for organizations that handle cardholder information for the major debit card debit credit card credit prepaid e purse cash machine atm and pos cards ref cite web title pci dss definition url http www pcmag com encyclopedia term 59104 pci dss accessdate 1 march 2016 ref industry and software there are several data security software available to be used by consumers and one of the most used data security software with a u s issued patent is folder lock folder lock see also copy protection data centric security data erasure data masking data recovery digital inheritance disk encryption comparison of disk encryption software identity based security information security it network assurance pre boot authentication privacy engineering secure usb drive security breach notification laws single sign on smart card trusted computing group notes and references reflist external links commons category data privacy portal bar computer security information technology category data security category data management'
b'orphan date june 2014 edi 834 files american national standards institute ansi 834 edi enrollment implementation file format format is a standard format for electronically exchanging health plan enrollment data between employers and health insurance carriers an 834 file contains a string of data elements and each data element represents a fact such as a subscriber s name hire date etc the entire string is called a data segment health insurance portability and accountability act the health insurance portability and accountability act hipaa requires that all health plans or health insurance carriers accept a standard enrollment format ansi 834a version 5010 the ansi 834a is the national standard for electronic enrollment and maintenance health plan the 834 is used to transfer enrollment information from the sponsor of the insurance coverage benefits or policy to a payer the intent of this implementation guide is to meet the health care industry s specific need for the initial enrollment and subsequent maintenance of individuals who are enrolled in insurance products this implementation guide specifically addresses the enrollment and maintenance of health care products only one or more separate guides may be developed for life flexible spending and retirement products an example layout of an ansi 834a version 5010 file is shown below sample file output br ins y 18 030 xn a e ft br ref of 152239999 br ref 1l blue br dtp 336 d8 20070101 br nm1 il 1 bluth lucille 34 152239999 br n3 224 n des plaines 6th floor br n4 chicago il 60661 usa br dmg d8 19720121 f m br hd 030 vis emp br dtp 348 d8 20111016 br ins n 19 030 xn a e n n br ref of 152239999 br ref 1l blue br dtp 357 d8 20111015 br nm1 il 1 bluth buster br n3 224 n des plaines 6th floor br n4 chicago il 60661 usa br dmg d 19911015 m hd 030 vis br dtp 348 d8 20110101 br dtp 349 d8 20111015 see also x12 document list references reflist http getworkforce com ansi 834 file layout ansi 834 file layout http getworkforce com ansi 834 file layout guardian electronic user guide 834 enrollment and maintenance http www 1edisource com transaction sets tset 834 edi 834 benefit enrollment and maintenance category data management'
b'file umbrello 1 png 320px thumb example of a case tool computer aided software engineering case is the domain of software tools used to design and implement applications case tools are similar to and were partly inspired by computer aided design cad tools used for designing hardware products case tools are used for developing high quality defect free and maintainable software ref kuhn d l 1989 selecting and effectively using a computer aided software engineering tool annual westinghouse computer symposium 6 7 nov 1989 pittsburgh pa u s doe project ref case software is often associated with methods for the development of information system s together with automated tools that can be used in the software development process ref p loucopoulos and v karakostas 1995 system requirements engineerinuality software which will perform effectively ref history the information system design and optimization system isdos project started in 1968 at the university of michigan initiated a great deal of interest in the whole concept of using computer systems to help analysts in the very difficult process of analysing requirements and developing systems several papers by daniel teichroew fired a whole generation of enthusiasts with the potential of automated systems development his problem statement language problem statement analyzer psl psa tool was a case tool although it predated the term ref cite journal last1 teichroew first1 daniel last2 hershey first2 ernest allen title psl psa a computer aided technique for structured documentation and analysis of information processing systems journal proceeding icse 76 proceedings of the 2nd international conference on software engineering date 1976 url http dl acm org citation cfm id 807641 publisher ieee computer society press ref another major thread emerged as a logical extension to the data dictionary of a database by extending the range of metadata held the attributes of an application could be held within a dictionary and used at runtime this active dictionary became the precursor to the more modern model driven engineering capability however the active dictionary did not provide a graphical representation of any of the metadata it was the linking of the concept of a dictionary holding analysts metadata as derived from the use of an integrated set of techniques together with the graphical representation of such data that gave rise to the earlier versions of case ref cite book last1 coronel first1 carlos last2 morris first2 steven title database systems design implementation management date february 4 2014 publisher cengage learning isbn 1285196147 pages 695 700 url https books google com books id qwlpagaaqbaj pg pa698 lpg pa698 dq case tools data dictionary source bl ots ejt gwyhgx sig mzemeswkjrgdczksez 6bnqdnay hl en sa x ei hnf0vovwdu3xigk5fq ved 0cfiq6aewcq v onepage q case 20tools 20data 20dictionary f false accessdate 25 november 2014 ref the term was originally coined by software company nastec corporation of southfield michigan in 1982 with their original integrated graphics and text editor graphitext which also was the first microcomputer based system to use hyperlinks to cross reference text strings in documents mdash an early forerunner of today s web page link graphitext s successor product designaid was the first microprocessor based tool to logically and semantically evaluate software and system design diagrams and build a data dictionary under the direction of albert f case jr vice president for product management and consulting and vaughn frick director of product management the designaid product suite was expanded to support analysis of a wide range of structured analysis and design technique structured analysis and design methodologies including those of ed yourdon and tom demarco chris gane computer scientist chris gane trish sarson ward mellor real time sa sd and warnier orr data driven ref cite journal last1 case first1 albert title computer aided software engineering case technology for improving software development productivity journal acm sigmis database date fall 1985 volume 17 issue 1 pages 35 43 url http dl acm org citation cfm id 1040698 ref the next entrant into the market was excelerator from index technology in cambridge mass while designaid ran on convergent technologies and later burroughs ngen networked microcomputers index launched excelerator on the ibm pc at platform while at the time of launch and for several years the ibm platform did not support networking or a centralized database as did the convergent technologies or burroughs machines the allure of ibm was strong and excelerator came to prominence hot on the heels of excelerator were a rash of offerings from companies such as knowledgeware james martin fran tarkenton and don addington texas instrument s information engineering facility ief and andersen consulting andersen consulting s foundation toolset design 1 install 1 fcp case tools were at their peak in the early 1990s ref cite news last1 yourdon first1 ed title can xp projects grow url https books google com books id faqto2febkc pg pa28 lpg pa28 dq case tools most interest 90 27s source bl ots 9wndaypu89 sig vc s1jtryowchccvydici5h9z7w hl en sa x ei lnd0vpr1de2rjak6o4d4da ved 0cdiq6aewaw v onepage q case 20tools 20most 20interest 2090 27s f false accessdate 25 november 2014 publisher computerworld date jul 23 2001 ref at the time ibm had proposed ad cycle which was an alliance of software vendors centered on ibm s software repository using ibm db2 in mainframe computer mainframe and os 2 the application development tools can be from several sources from ibm from vendors and from the customers themselves ibm has entered into relationships with bachman information systems index technology corporation and knowledgeware knowledgeware wherein selected products from these vendors will be marketed through an ibm complementary marketing program to provide offerings that will help to achieve complete life cycle coverage ref name adc saaa ad cycle strategy and architecture ibm systems journal vol 29 no 2 1990 p 172 ref with the decline of the mainframe ad cycle and the big case tools died off opening the market for the mainstream case tools of today many of the leaders of the case market of the early 1990s ended up being purchased by computer associates including iew ief adw cayenne and learmonth burchett management systems lbms the other trend that led to the evolution of case tools was the rise of object oriented methods and tools most of the various tool vendors added some support for object oriented methods and tools in addition new products arose that were designed from the bottom up to support the object oriented approach andersen developed its project eagle as an alternative to foundation several of the thought leaders in object oriented development each developed their own methodology and case tool set jacobsen rumbaugh grady booch booch etc eventually these diverse tool sets and methods were consolidated via standards led by the object management group omg the omg s unified modelling language uml is currently widely accepted as the industry standard for object oriented modeling case software a fuggetta classified case software into 3 categories ref name af 93 cite journal author alfonso fuggetta date december 1993 title a classification of case technology journal computer volume 26 issue 12 pages 25 38 doi 10 1109 2 247645 url http www2 computer org portal web csdl abs mags co 1993 12 rz025abs htm accessdate 2009 03 14 ref tools support specific tasks in the software life cycle workbenches combine two or more tools focused on a specific part of the software life cycle environments combine two or more tools or workbenches and support the complete software life cycle tools case tools supports specific tasks in the software development life cycle they can be divided into the following categories business and analysis modeling graphical modeling tools e g e r modeling object modeling etc development design and construction phases of the life cycle debugging environments e g gnu debugger verification and validation analyze code and specifications for correctness performance etc configuration management control the check in and check out of repository objects and files e g source code control system sccs cms metrics and measurement analyze code for complexity modularity e g no go to s performance etc project management manage project plans task assignments scheduling another common way to distinguish case tools is the distinction between upper case and lower case upper case tools support business and analysis modeling they support traditional diagrammatic languages such as er diagram s data flow diagram structure chart s decision tree s decision table s etc lower case tools support development activities such as physical design debugging construction testing component integration maintenance and reverse engineering all other activities span the entire life cycle and apply equally to upper and lower case ref software engineering tools principles and techniques by sangeeta sabharwal umesh publications ref workbenches workbenches integrate two or more case tools and support specific software process activities hence they achieve a homogeneous and consistent interface presentation integration seamless integration of tools and tool chains control and data integration an example workbench is microsoft s visual basic programming environment it incorporates several development tools a gui builder smart code editor debugger etc most commercial case products tended to be such workbenches that seamlessly integrated two or more tools workbenches also can be classified in the same manner as tools as focusing on analysis development verification etc as well as being focused on upper case lower case or processes such as configuration management that span the complete life cycle environments an environment is a collection of case tools or workbenches that attempts to support the complete software process this contrasts with tools that focus on one specific task or a specific part of the life cycle case environments are classified by fuggetta as follows ref name af 93 toolkits loosely coupled collections of tools these typically build on operating system workbenches such as the unix programmer s workbench or the vms vax set they typically perform integration via piping or some other basic mechanism to share data and pass control the strength of easy integration is also one of the drawbacks simple passing of parameters via technologies such as shell scripting can t provide the kind of sophisticated integration that a common repository database can fourth generation these environments are also known as 4gl standing for fourth generation language environments due to the fact that the early environments were designed around specific languages such as visual basic they were the first environments to provide deep integration of multiple tools typically these environments were focused on specific types of applications for example user interface driven applications that did standard atomic transactions to a relational database examples are informix 4gl and focus language centered environments based on a single often object oriented language such as the symbolics lisp genera environment or visualworks smalltalk from parcplace in these environments all the operating system resources were objects in the object oriented language this provides powerful debugging and graphical opportunities but the code developed is mostly limited to the specific language for this reason these environments were mostly a niche within case their use was mostly for prototyping and r d projects a common core idea for these environments was the model view controller user interface that facilitated keeping multiple presentations of the same design consistent with the underlying model the mvc architecture was adopted by the other types of case environments as well as many of the applications that were built with them integrated these environments are an example of what most it people tend to think of first when they think of case environments such as ibm s ad cycle andersen consulting s foundation the icl cades system and dec cohesion these environments attempt to cover the complete life cycle from analysis to maintenance and provide an integrated database repository for storing all artifacts of the software process the integrated software repository was the defining feature for these kinds of tools they provided multiple different design models as well as support for code in heterogenous languages one of the main goals for these types of environments was round trip engineering being able to make changes at the design level and have those automatically be reflected in the code and vice versa these environments were also typically associated with a particular methodology for software development for example the foundation case suite from andersen was closely tied to the andersen method 1 methodology process centered this is the most ambitious type of integration these environments attempt to not just formally specify the analysis and design objects of the software process but the actual process itself and to use that formal process to control and guide software projects examples are east enterprise ii process wise process weaver and arcadia these environments were by definition tied to some methodology since the software process itself is part of the environment and can control many aspects of tool invocation in practice the distinction between workbenches and environments was flexible visual basic for example was a programming workbench but was also considered a 4gl environment by many the features that distinguished workbenches from environments were deep integration via a shared repository or common language and some kind of methodology integrated and process centered environments or domain 4gl specificity ref name af 93 major case risk factors some of the most significant risk factors for organizations adopting case technology include inadequate standardization organizations usually have to tailor and adopt methodologies and tools to their specific requirements doing so may require significant effort to integrate both divergent technologies as well as divergent methods for example before the adoption of the uml standard the diagram conventions and methods for designing object oriented models were vastly different among followers of jacobsen booch rumbaugh unrealistic expectations the proponents of case technology especially vendors marketing expensive tool sets often hype expectations that the new approach will be a silver bullet that solves all problems in reality no such technology can do that and if organizations approach case with unrealistic expectations they will inevitably be disappointed inadequate training as with any new technology case requires time to train people in how to use the tools and to get up to speed with them case projects can fail if practitioners are not given adequate time for training or if the first project attempted with the new technology is itself highly mission critical and fraught with risk inadequate process control case provides significant new capabilities to utilize new types of tools in innovative ways without the proper process guidance and controls these new capabilities can cause significant new problems as well ref http ithandbook ffiec gov it booklets development and acquisition development procedures software development techniques computer aided software engineering aspx computer aided software engineering in ffiec it examination handbook infobase retrieved 3 mar 2012 ref see also data modeling domain specific modeling method engineering model driven architecture modeling language rapid application development model based architecture references reflist authority control category computer aided software engineering tools category data management'
b'orphan date august 2014 query rewriting is a technique used in mediation based data integration systems for translating the queries formulated over the mediated schema to a query over the various sources by making use of the view definitions ref name refone cite conference author alon y halevy title answering queries using views a survey booktitle the vldb journal year 2001 pages 270 294 ref mediation based data integration system hides from the end user the underlying heterogeneity of the various data providing sources linked to it by providing a uniform query interface in the form of a mediated schema this schema is also referred to as the global schema whereas the schema of the various data sources is collectively referred to as the local schema the local schema and the mediated schema are mapped to each other using view definitions the queries formulated on the mediated schema cannot be directly used to query the sources therefore query rewriting translates such a query formulated over the global schema to a query over the various data sources examples include bucket algorithm minicon algorithm inverse rules algorithm ref name refone this rewritten query is then evaluated to obtain the query response making use of the data obtained by querying the data sources see also data integration schema matching data virtualization references references defaultsort query rewriting category data management'
b'infobox company name personal inc logo file personal inc logo png type private company private foundation 2009 location washington d c us industry internet homepage url www personal com personal also referred to as personal com or personal inc was a consumer personal data service and identity management system for individuals to aggregate manage and reuse their own data it was re launched in may 2016 as a collaborative data management and security solution for the workplace called teamdata ref cite web url http www prnewswire com news releases personalcom becomes teamdata 300275063 html title personal com becomes teamdata last teamdata website www prnewswire com access date 2016 10 03 ref personal s consumer products included the data vault with cloud sync for secure management and sharing of data and documents between an individual and other individuals companies sites apps and devices and data imports to import information from third parties including social networking services social media services companies and the united states department of education u s department of education and the fill it app for automated completion of web and mobile forms logins and checkouts the personal platform supported user centric dataportability data management and portability for over 1 200 different types or fields of structured machine readable human readable data the platform also provided tools and apis for developers and companies to integrate fill it and the data vault into their websites and applications primarily to give data back to their customers so they can autofill web and mobile forms history personal was founded in 2009 in washington dc by the management team that built the map network which was acquired by nokia navteq navteq in 2006 ref name acquisition cite web url http www directionsmag com pressreleases navteq announces agreement to acquire the map network 110396 title navteq announces agreement to acquire the map network date 6 december 2006 website directions magazine accessdate 21 august 2014 ref founded in 1999 the map network previously called urhere com built the first platform for places and events to create and distribute digital online and mobile maps location data and content the map network served as the official mapping solution for over 100 cities and thousands of events and venues from the nfl super bowl to the democratic and republican national conventions to the smithsonian institution it also produced the most used interactive map of 9 11 relief and rescue efforts ref name acquisition ref cite web url http www prnewswire com news releases interactive relief and rescue map aids in nyc response 72052587 html title interactive relief and rescue map aids in nyc response date 17 september 2001 website pr newswire accessdate 25 august 2014 ref ref cite web url http spatialnews geocomm com dailynews 2001 sep 11 title the geospatial industry s response to terrorism date 11 september 2001 website geocommunity accessdate 26 august 2014 ref called a life management platform by the economist ref cite web url http www economist com blogs babbage 2011 11 personal data title a life management platform last l first g date 17 november 2011 website the economist accessdate 8 august 2014 ref and a personal encrypted cloud service by time for its user centric approach to data ref name time com cite web url http time com 3069834 how to take control of your personal data title how to take control of your personal data last stokes first natasha date 1 august 2014 website time inc accessdate 8 august 2014 ref the company has been associated with both the infomediary model originated in 1999 by john hagel iii and mark singer as well as the vendor relationship management vendor relationship management vrm model developed by doc searls personal closed 7 6m in funding in december 2010 including steve case s revolution ventures grotech ventures allen company ted leonsis neil ashe and jonathan miller businessman jonathan miller ref cite web url http techcrunch com 2011 01 06 personal raises 7m from steve case and others to help consumers protect their digital data title personal raises 7m from steve case and others to help consumers protect their digital data last rao first leena date 6 january 2011 website techcrunch accessdate 8 august 2014 ref personal was early to embrace small data which it defines as big data for the benefit of individuals ref cite web url http blog personal com 2012 03 the era of small data begins title the era of small data begins last green first shane date 6 march 2012 website personal accessdate 20 august 2014 ref the term small data may have been originally coined by jeremie miller of sing ly who mentioned it in a talk at the web 2 0 summit in november 2011 and is cited in the intention economy ref cite web url http siliconprairienews com 2011 11 watch jeremie miller present singly at the web 2 0 summit title watch jeremie miller present singly at the web 2 0 summitt first michael date 9 november 2011 website silicon prairie news accessdate 8 august 2014 ref in 2011 personal was a part of the first group of companies to join the personal data ecosystem consortium s startup circle ref cite web url http pde cc startup circle 2011 title members of the pdec startup circle website personal data ecosystem consortium accessdate 20 august 2014 ref a small data meetup website meetup group has also formed in new york city bringing together technology legal and business experts to exchange ideas about user centric and user driven models for internet products and services ref http www meetup com smalldata ref personal ultimately raised 24 million including 4 5m from bill miller of legg mason and esther dyson of edventures in october 2013 ref cite web url http www reuters com article 2013 10 15 idus412005883920131015 title personal raises 4 5 million to be the personal data vault we so desperately need date 15 october 2013 website reuters accessdate 8 august 2014 ref products and services overview the personal platform was a privacy and security by design platform for individuals to manage and reuse their own data and information the fill it app was a 1 click form filling solution for web and mobile logins checkouts and forms and the data vault app served as the main cloud based repository for a user s data personal helped individuals take control and benefit from their information while knowing that the information in their data vault remained legally theirs and could not be used without their permission ref cite web url http www zdnet com article intel execs on big data and privacy its a balancing act title intel execs on big data and privacy it s a balancing act zdnet last king first rachel website zdnet access date 2016 10 03 ref data vault with cloud sync personal spent two years building the personal platform before launching its data vault product in beta in november 2011 following privacy by design principles personal only enabled users to see or share the sensitive data and all the files they stored in their data vault such information was encrypted and could only be decrypted with a user s password only users could choose and know their passwords to their vault because personal did not store user passwords and therefore could not reset them without deleting a user s sensitive data and all files stored in their vault ref cite web url http www ipc on ca images resources pbd pde pdf title privacy by design and the emerging personal data ecosystem last cavoukian first ann last2 green first2 shane date october 2012 website office of the information and privacy commissioner accessdate 8 august 2014 ref all personal apps and services were linked to a user s private data vault the data vault featured automatic synchronization of data and files added on any device logged into personal it also featured a secure share function that created a live private network allowing registered users to share access to data and files through an exchange of encrypted keys without the risk of transmitting the data or files through non secure direct means it also allowed users to immediately update data across their own network and revoke access to it when they choose personal launched its android operating system android app on november 30 2011 ref name mashable cite web url http mashable com 2011 11 17 personal title never fill out a form again personal seeks to be the data vault for your private information last parr first ben date 17 november 2011 website mashable accessdate 8 august 2014 ref ref cite web url https www personal com s pages news personal android release title personal releases android app for its private personal network and data vault service date 30 november 2011 website personal inc accessdate 8 august 2014 ref the ios data vault app was released on may 7 2012 ref cite web url http techcrunch com 2012 05 07 personal takes its secure vault for all of your private digital data mobile with ios app title personal takes its secure vault for all of your private digital data mobile with ios app last rao first leena date 7 may 2012 website techcrunch accessdate 8 august 2014 ref personal officially launched its application programming interface application programming interface apis on october 2 2012 at the mashery business of apis conference ref cite web url https www personal com s pages news personal launches personal platform title personal launches personal platform at business of apis conference opening apis for developers date 2 october 2012 website personal inc accessdate 8 august 2014 ref a review by cnet highlighted the challenges of getting people to trust such a new service with their sensitive data and spending the time required entering enough data to make it useful ref cite web url http www cnet com news what hump personals private database faces challenges title what hump personal s private database faces challenges last needleman first rafe date 30 november 2011 website cnet accessdate 8 august 2014 ref fill it app and form index when the data vault was launched in november 2011 mashable posed the question never fill out a form again ref name mashable the world economic forum in its february 2013 report highlighted the possibility of saving 10 billion hours globally and improv ing the delivery of public and private sector services through automated form filling tools specifically citing personal s fill it app ref cite web url http www3 weforum org docs wef it unlockingvaluepersonaldata collectionusage report 2013 pdf title unlocking the value of personal data from collection to usage date february 2013 website world economic forum accessdate 8 august 2014 ref in january 2013 personal launched fill it in beta as a web bookmarklet for automatic form filling ref cite web url http www digitaltrends com web personal coms new fill it feature makes quick work of long web forms bbp6ij title personal com s new fill it app makes quick work of long online forms last couts first andrew date 16 january 2013 website digital trends accessdate 8 august 2014 ref on june 11 2014 personal released fill it as a web extension and announced that it was publishing an index of over 140 000 1 click online forms at www fillit com ref name extensionlaunch cite web url http tech co dc based startup personal launches fill it for quick and safe auto filling on online forms 2014 06 title dc based personal launches fill it for quick and safe auto filling on online forms last barba first ronald date 16 june 2014 website tech cocktail accessdate 8 august 2014 ref the company also announced that a mobile version of the product will launch later in the year according to a story in tech cocktail about the launch personal s web extension and mobile app are able to support over 1 200 different types of reusable data even enabling them to unlock more confidential information so they can complete longer forms including patient registrations job applications event registrations school admissions insurance and bank applications and government forms ref name extensionlaunch in november 2014 a mobile version of fill it was launched that could autofill mobile forms using apis ref cite web url http tech co personal launches fill it mobile 2014 11 title personal launches fill it mobile at pii2014 date 2014 11 13 language en us access date 2016 10 03 ref personal s form portal ultimately indexed more than 500 000 forms with three components which together allowed data to be captured and reused across any of the forms 1 a form graph which mapped individual form fields to the personal ontology 2 a semantic layer which determined how data was required on a form e g one field vs three fields for a u s telephone number and 3 a correlations graph which helped individuals match their specific data to a form without looking at the data value e g knowing which phone number is a mobile phone number which address is a billing address or that a person uses their middle name as a first name on most forms ref cite web url http tech co dc startup personal university data privacy security 2014 08 title personal launches personal university a video series on data privacy and security last barba first ronald date 8 august 2014 website tech cocktail accessdate 8 august 2014 ref monetizing personal data with the initial public offering of facebook in may 2012 there was media interest in the question of the monetary value of personal data and whether tools and services might emerge to help consumers monetize their own data personal was frequently cited as a company that could potentially offer such a service articles and pieces focusing on this subject have appeared in the new york times adweek the mit technology review and on cnn and national public radio ref cite web url http www nytimes com 2012 02 13 technology start ups aim to help users put a price on their personal data html r 0 title start ups seek to help users put a price on their personal data last brustein first joshua date 12 february 2012 website the new york times accessdate 8 august 2014 ref ref cite web url http www technologyreview com view 426235 is personal data the new currency title is personal data the new currency last zax first david date 30 november 2011 website mit technology review accessdate 8 august 2014 ref ref cite web url http edition cnn com 2012 02 24 tech web owning your data online title manage and make cash with your data online last gross first doug date 27 february 2012 website cnn accessdate 8 august 2014 ref company co founder and ceo shane green was quoted as saying that the average american consumer would soon be able to realize over 1 000 per year by granting limited anonymous access to their data to marketers but that figure was never supported by green or the company ref cite web url http www ft com cms s 2 61c4c378 60bd 11e2 a31a 00144feab49a html axzz39ptb1it4 title data mining offers rich seam last palmer first maija date 18 february 2013 website financial times accessdate 8 august 2014 ref reception and re launch as teamdata personal was the first online consumer facing company to be named an ambassador for privacy by design for its technical business and legal commitments to providing users with control over the data they store in personal s service ref cite web url http www privacybydesign ca index php ambassador personal com title personal com website privacy by design accessdate 15 august 2014 ref ref cite web url http www privacybydesign ca content uploads 2010 03 2011 10 24 personal com pdf title personal and privacy by design website privacy by design accessdate 20 august 2014 ref ref cite web url http www privacybydesign ca index php ambassador joshua p galper title joshua p galper website privacy by design accessdate 20 august 2014 ref ref cite web url http www privacybydesign ca index php ambassador shane green title shane green website privacy by design accessdate 20 august 2014 ref the company received recognition for its user agreement called the owner data agreement ref cite web url https www personal com owner data agreement title owner data agreement date 7 february 2014 website personal inc accessdate 8 august 2014 ref which acted like a reverse license agreement when data was shared between registered parties and emphasized that data ownership resides with the user doc searls wrote in the intention economy when customers take charge that the owner data agreement had no precedent and modeled a new legal position both for vendors and for intermediaries ref cite book last searls first doc date may 1 2012 title the intention economy when customers take charge publisher harvard business review press page 186 isbn 978 1422158524 ref fast company magazine fast company called the data vault a tool that will simplify our lives ref cite web url http www fastcompany com 1836521 personalcom creates online vault manage all your data title personal com creates an online vault to manage all your data last boyd first e b date 7 may 2012 website fast company accessdate 8 august 2014 ref personal has been included in case studies by ctrl shift and forrester regarding personal data stores and personal identity management ref cite web url https www ctrl shift co uk index php research product 64 title personal data stores website ctrl shift accessdate 20 august 2014 ref ref cite web url http blog personal com uploads 2011 10 forrester research personal identity management pdf last khatibloo first fatemeh last2 frankland first2 dave last3 maler first3 eve last4 smith first4 allison date 30 september 2011 title personal identity management website forrester accessdate 20 august 2014 ref in 2011 personal received the innovator spotlight award at privacy identity innovation conference pii2011 and participated in the technology showcase at pii2012 ref cite web url http www prweb com releases 2011 5 prweb8484188 htm last fonseca first natalie title personal and passtouch receive innovator spotlight award at privacy identity innovation conference pii2011 website prweb accessdate 20 august 2014 ref ref cite web url https www privacyidentityinnovation com pii2012 seattle pii2012 technology showcase title pii2012 technology showcase website privacy identity innovation accessdate 20 august 2014 ref in 2012 techhive named personal as one of the top five apps or web services of sxsw ref cite web url http www techhive com article 251744 hot apps and web services of sxsw html last sullivan first mark date 13 march 2012 title hot apps and web services of sxsw accessdate 20 august 2014 ref personal won the 2013 campus technology innovators award with lone star college in july 2013 ref cite web url http campustechnology com articles 2013 07 23 2013 innovators awards aspx last raths first david last2 namahoe first2 kanoe last3 lloyd first3 meg date 23 july 2013 title 2013 innovators awards website campus technology accessdate 20 august 2014 ref personal was included in a list of executive travel magazine s favorite travel apps for 2013 in its may june issue ref citation url http www executivetravelmagazine com articles ets favorite travel apps for 2013 last null first christopher title et s favorite travel apps of 2013 archiveurl https web archive org web 20131023184535 http www executivetravelmagazine com articles ets favorite travel apps for 2013 archivedate 2013 10 23 accessdate 20 august 2014 ref in 2013 personal was also included as part of nyu govlab s open data 500 and was named by j walter thompson as one of 100 things to watch for in 2014 ref cite web url http www opendata500 com us personal inc title personal inc website open data 500 accessdate 20 august 2014 ref ref cite web url http www jwtintelligence com 2013 12 100 watch 2014 axzz2qybvcrms last mack first ann date 26 december 2013 title 100 things to watch in 2014 website jwt intelligence accessdate 20 august 2014 ref in 2015 the national law journal named company chief policy officer and general counsel joshua p galper as one of their 50 cybersecurity privacy trailblazers ref cite web url http pdfserver amlaw com nlj flipbook cybersecurity trailblazers 2015 cyber security trailblazers 2015 web html p 253d14 2523p 8 title cybersecurity trailblazers website pdfserver amlaw com access date 2016 10 03 ref in may 2016 personal co founder and ceo shane green announced the launch of teamdata with one of the other co founders tarik kurspahic and new board chair eric c anderson teamdata focuses on the problem of securing and collaboratively managing data in the workplace and is based on the technology and platform of personal ref cite web url https medium com shanegreen why personal com graduated to teamdata today f75e0d539ba1 yrkhukyec title why personal com graduated to teamdata today last green first shane date 2016 05 20 access date 2016 10 03 ref onboardly included the new collaborative teamdata solution in its list of top 10 apps to keep your team on track and as part of its top 50 list of all time best content marketing tools ref cite web url http onboardly com content marketing all time best tools for content marketing teams title all time best tools for content marketing teams via onboardly date 2016 04 07 language en us access date 2016 10 03 ref references reflist 2 external links official website url www fillit com fill it homepage itunes preview app 493536192 personal itunes preview app 910517122 fill it google play com personal android personal google play com personal fillit fill it http tech co tag personal personal collected news and commentary at tech cocktail crunchbase personal personal category american websites category android operating system software category companies based in washington d c category companies established in 2009 category data management category data security category internet companies of the united states category internet properties established in 2009 category ios software'
b'ncsa brown dog is a research project to develop a method for easily accessing historic research data stored in order to maintain the long term viability of large bodies of scientific research it is supported by the national center for supercomputing applications ncsa that is funded by the national science foundation nsf ref name bdweb cite web title brown dog url http browndog ncsa illinois edu website ncsa brown dog accessdate 31 july 2014 ref history brown dog is part of the datanet datanet partners program funded by nsf in 2008 datanet was conceived to address the increasingly digital and data intensive nature of science engineering and education brown dog is part of a follow on effort called data infrastructure building blocks dibbs focused on building software to support datanet the project was proposed by researchers at ncsa and the university of illinois urbana champaign as well as researchers from boston university and the university of north carolina at chapel hill unstructured uncurated long tail data much scientific data is smaller unstructured data unstructured and uncurated and thus not easily shared such data is sometimes referred to as long tail data this borrows a term from statistics and refers to the tail of the distribution of project sizes the majority of smaller projects lack the resources to properly steward the data they produce this so called long tail data both past and present has the potential to inform future research in many study areas much of this data has become inaccessible due to obsolete software and file formats the resulting impossibility of reviewing data from older research disrupts the overall scientific research project ref cite web title dataup data curation for the long tail of science url http blogs msdn com b msr er archive 2012 10 02 dataup data curation for the long tail of science aspx website microsoft research connections blog publisher microsoft research connections team accessdate 7 august 2014 ref approach brown dog describes itself as the super mutt of software ref cite web last1 woodie first1 alex title ncsa project aims to create a dns like service for data url http www datanami com 2014 01 06 ncsa project aims to create a dns like service for data website datanami accessdate 7 august 2014 ref thus the name brown dog serving as a low level data infrastructure to interface digital data content across the internet its approach is to use every possible source of automated help i e software in existence in a robust and provenance preserving manner to create a service that can deal with as much of this data as possible ref cite web last1 pletz first1 john title u of i researchers get millions for super mutt to sniff out big data trends url http www chicagobusiness com article 20131202 blogs11 131129794 u of i researchers get millions for super mutt to sniff out big data trends website chicago business publisher crain communications inc accessdate 7 august 2014 ref the project sees the broader impact of its work in its potential to serve the general public as a sort of dns for data with the goal of making all data and all file formats as accessible as webpages are today technology brown dog seeks to address problems involving the use of uncurated and unstructured data collections through the development of two services the data access proxy dap to aid in the conversion of file formats and the data tilling services dts for the automatic extraction of metadata from file contents once developed researchers and general public users will be able to download browser plugins and other tools from the brown dog tool catalog ref name bdweb ref cite web last1 jewett first1 barbara title data set free url http www ncsa illinois edu news stories kentonmchenry website ncsa access magazine publisher ncsa accessdate 7 august 2014 ref data tilling service data tilling service dts will allow users to search data collections using an existing file to discover other similar files in a collection a dts search field will be appended to configured browsers where example files can be dropped this tells dts to search all the files under a given url for files similar to the dropped file for example while browsing an online image collection a user could drop an image of three people into the search field and the dts would return all images in the collection that also contain three people if dts encounters a foreign file format it will utilize dap to make the file accessible dts also indexes the data and extract and appends metadata to files and collections enabling users to gain some sense of the type of data they are encountering this service runs on port 9443 data access proxy data access proxy dap allows users to access data files that would otherwise be unreadable similar to an internet gateway or domain name system domain name service the dap configuration would be entered into a user s machine and browser settings data requests over http would first be examined by dap to determine if the native file format is readable on the client device if not dap converts the file into the best available format readable by the client machine alternatively the user could specify the desired format themselves this service runs on port 8184 use cases brown dog targets three use cases proposed by groups within the http earthcube org earthcube research communities developers and researchers from these communities will work together on use cases that span geoscience engineering biology and social science long tail vegetation data in ecology and global change biology this use case is led by http people bu edu dietze michael dietze http www bu edu boston university blockquote data on the abundance species composition and size structure of vegetation is critically important for a wide array of sub disciplines in ecology conservation natural resource management and global change biology however addressing many of the pressing questions in these disciplines will require that terrestrial biosphere and hydrologic models are able to assimilate the large amount of long tail data that exists but is largely inaccessible the brown dog team in cooperation with researches from dietze s lab will facilitate the capture of a huge body of smaller research oriented vegetation data sets collected over many decades and historical vegetation data embedded in public land survey data dating back to 1785 this data will be used as initial conditions for models to make sense of other large data sets and for model calibration and validation ref name bdweb ref name newswise cite web title bu scientist collaborators get 10 5 million grant to develop software for un curated data url http www newswise com articles bu scientist collaborators get 10 5 million grant to develop software for un curated data website www newswise com publisher boston university college of arts and sciences accessdate 7 august 2014 ref blockquote designing green infrastructure considering storm water and human requirements this use case is led by http eisa ncsa illinois edu barbara minsker http illinois edu university of illinois at urbana champaign http willsull net research william sullivan university of illinois at urbana champaign http cee illinois edu faculty arthurschmidt arthur schmidt university of illinois at urbana champaign blockquote this case study involves developing novel green infrastructure design criteria and models that integrate requirements for storm water management and ecosystem and human health and well being to address the scientific and social problems associated with the design of green spaces data accessibility and availability is a major challenge this study will focus on identified areas of the green healthy neighborhood planning region within the city of chicago where existing local sewer performance is most deficient and where changes in impervious area through green infrastructure would be beneficial to under served neighborhoods brown dog will be used to extract long tail experimental data on human landscape preferences and health impacts this data will be used to develop a human health impacts model that will then be linked together with a terrestrial biosphere model and a storm water model using brown dog technology ref name bdweb blockquote development and application for critical zone studies this use case is led by http hydrocomplexity net index html praveen kumar university of illinois at urbana champaign blockquote earth s critical zone critical zone cz is the skin of the earth that extends from the treetops to the bedrock that is created by life processes working at scales from microbes to biomes the critical zone supports all terrestrial living systems its upper part is the bio mantle this is where terrestrial biota live reproduce use and expend energy and where their wastes and remains accumulate and decompose it encompasses the soil which acts as a geomembrane through which water and solutes energy gases solids and organisms interact with the atmosphere biosphere hydrosphere and lithosphere a variety of drivers affect this bio dynamic zone ranging from climate and deforestation to agriculture grazing and human development understanding and predicting these effects is central to managing and sustaining vital ecosystem services such as soil fertility water purification and production of food resources and at larger scales global carbon cycling and carbon sequestration the cz provides a unifying framework for integrating terrestrial surface and near surface environments and reflects an intricate web of biological and chemical processes and human impacts occurring at vastly different temporal and spatial scales the nature of these data create significant challenges for inter disciplinary studies of the cz because integration of the variety and number of data products and models has been a barrier on the other hand cz data provides an excellent opportunity for defining testing and implementing brown dog technologies in this context unstructured data is viewed broadly as consisting of a collection of heterogeneous data with formats that reflect temporal and disciplinary legacies data from emerging low cost open hardware based sensors and embedded sensor networks that lack well defined metadata and sensor characteristics as well as data that are available as maps images and text ref name bdweb blockquote nsf award cif21 dibbs brown dog was awarded in the winter of 2013 with a start date of october 1 2013 estimated expiration date is september 30 2018 ref cite web title award 1261582 cif21 dibbs brown dog url http www nsf gov awardsearch showaward awd id 1261582 historicalawards false website nsf gov accessdate 31 july 2014 ref the award amount was 10 519 716 00 the largest dibb award the principal investigator is kenton mchenry of ncsa at the university of illinois at urbana champaign coleaders are jong lee ncsa uiuc barbara minsker civil and environmental engineering university of illinois at urbana champaign praveen kumar civil and environmental engineering university of illinois at urbana champaign michael dietze department of earth and environment boston university references reflist 30em external links official website http browndog ncsa illinois edu category data management category national science foundation category research projects'
b'refimprove date october 2014 government performance management gpm consists of a set of processes that help government organizations optimize their business performance it provides a framework for organizing automating and analyzing business methodologies metrics processes and systems that drive business performance ref cite web url http www information management com bissues 20070301 2600312 1 html title performance management for government author michael owellen date 28 february 2007 work bi review magazine accessdate 22 october 2014 ref some commentators who date october 2014 see gpm as the next generation of business intelligence bi for governments gpm helps governments to make use of their financial human material and other resources in the past owners have sought to drive strategy down and across their organizations they have struggled to transform strategies into actionable metrics and they have grappled with meaningful analysis to expose the cause and effect relationships that if understood could give profitable insight to their operational decision makers gpm software and methods allow a systematic integrated approach that links government strategy to core processes and activities running by the numbers now means something planning budgeting analysis and reporting can give the measurements that empower management decisions ref cite web url http www encyclopedia com doc 1o12 performancemanagement html accessdate february 7 2010 deadurl yes archiveurl https web archive org web 20090112223214 http www encyclopedia com doc 1o12 performancemanagement html archivedate january 12 2009 ref performance management pm market according to gartner citation needed date october 2014 the enterprise performance management epm suite market continues to experience strong momentum growing 19 during 2007 this is slightly in advance of their earlier market sizing and forecast analysis which anticipated 2007 revenue to be 1 836 million representing an 18 year over year growth in the latest forecast gartner believe that the market for epm will be more than 3 billion by 2011 representing a 14 4 compound annual growth rate several factors contributed to the continued significant growth in epm revenue during 2007 many organizations replaced difficult to maintain inflexible or outmoded spreadsheets and homegrown financial applications continued growth in large enterprises was fueled by desires to achieve greater transparency and adherence to governance and compliance legislation increased demand for applications that support strategic plans and operational activities drove new momentum in the deployment of scorecards there was increased demand from mid size enterprises representing one of the largest untapped and dynamic areas of the business application software sector advertising and pr from increasingly large vendors and system integrators are raising the epm profile and generating greater demand gartner also expects the business intelligence software market to reach 3 billion in 2009 companies around the world have purchased more than us 40 billion worth of enterprise applications including erp crm and hr during the past few years said colleen graham principal research analyst at gartner this has generated significant volumes of data in support of the operational processes they automate by investing in bi companies can further leverage their enterprise application investments and turn the torrent of data into meaningful insight to better measure performance respond more quickly to market changes and opportunities and comply with an increasingly complex regulatory environment ref cite web url http www gartner com press releases asset 144782 11 html title gartner news room publisher gartner com accessdate 22 october 2014 ref itworx government performance management gpm itworx gpm is a bilingual microsoft based framework that gives governments the capability to cascade share track and update strategies and plans organization wide it creates detailed views of multi source key performance indicator s kpis using customized balance scorecard s dashboards strategy maps statistical charts and reports as well as provides ad hoc analytical and reporting tools itworx gpm features merge to section yes itworx date october 2014 advert date october 2014 itworx gpm ref http www itworx com solutions microsoftgovernmentperformance pages default aspx webarchive url https web archive org web 20110311185821 http www itworx com solutions microsoftgovernmentperformance pages default aspx date march 11 2011 ref provides a top down approach in recording government strategy the strategy is cascaded and shared across government bodies to define objectives and balancing targets it also links strategy to execution operational plans are recorded linked to strategies assigned a time range for implementation broken down to initiatives and business activities approved and then propagated to all levels itworx gpm ref http www itworx com solutions microsoftgovernmentperformance pages default aspx webarchive url https web archive org web 20110311185821 http www itworx com solutions microsoftgovernmentperformance pages default aspx date march 11 2011 ref defines a time range for implementing an initiative its owners cost drivers budgets kpis and targets and links initiatives to strategic objectives business activities contributing in strategy execution are defined including their kpis and targets kpis can be entered and configured manually calculated using other kpis or extracted from external data sources itworx gpm ref http www itworx com solutions microsoftgovernmentperformance pages default aspx webarchive url https web archive org web 20110311185821 http www itworx com solutions microsoftgovernmentperformance pages default aspx date march 11 2011 ref enables the definition of government specific business rules such as kpi calculation formulas it also enables administration of system settings such as the configuration of the organization structure and definition of approval workflows for each organization unit furthermore administrators can manage user roles and groups as well as archive plans and approvals itworx gpm ref http www itworx com solutions microsoftgovernmentperformance pages default aspx webarchive url https web archive org web 20110311185821 http www itworx com solutions microsoftgovernmentperformance pages default aspx date march 11 2011 ref calculates measurement formulas compares actual values against targets and performs analysis color coded kpis are represented through strategic and customized scorecards dashboards strategy maps and statistical charts and graphs in addition to ad hoc analytical and reporting tools the solution enables communication throughout the decision making process by allowing users to post comments and discuss topics regarding a strategy kpi or report keeping a documented record of why and when decisions are made itworx gpm retains the history of contributions itworx gpm ref http www itworx com solutions microsoftgovernmentperformance pages default aspx webarchive url https web archive org web 20110311185821 http www itworx com solutions microsoftgovernmentperformance pages default aspx date march 11 2011 ref provides a mechanism for policy makers and strategy implementers to facilitate the strategic management process without compromising data government frontline officials are provided with a feedback channel to submit change requests and propositions to approved strategic plans targets and actual data while securing the validity and consistency of data frontline officials can monitor performance though consolidated views while detailed views are provided for department and executive levels based on privileges users can view rolled up kpis and drill down for root cause analysis or corrective actions references reflist external links http gpm itworx com gpm itworx com http www microsoft com downloads details aspx displaylang en familyid efdc60d3 2622 44f5 aa5d 2b79d10c93ab microsoft com http www pcmag mideast com gitex tag itworx pcmag mideast com http www itp net 578068 itworx releases new gpm suite itp net category business intelligence category data management'
b'use dmy dates date september 2016 use indian english date september 2016 orphan date september 2014 infobox company name analytix ds logo file analytix ds logo png type private limited company private location city chantilly virginia chantilly virginia location country united states founder unbulleted list mike boggs foundation 2006 area served worldwide key people madan k ceo br mike boggs chief technology officer cto founder br sam benedict vp sales marketing br john carter director of professional services industry software company services it business consulting and automation services homepage url www analytixds com intl yes analytix data services is a software vendor that provides specialized data mapping and etl conversion tools for data integration data management enterprise application integration and big data software and services ref cite web title mapping manager the missing link in moving data around url http www bloorresearch com analysis analytix mapping manager missing link moving data publisher bloor accessdate 17 september 2014 ref headquarter s based in chantilly virginia analytix ds has offices in dallas tx and hyderabad india and an international network of technical and services partners ref cite web title analytix ds partners url http analytixds com technology partners ref michael boggs the cto and founder of analytix ds coined the term pre etl mapping now it is widely used and accepted synonym for design phase of data integration mapping manager is the flagship product of analytix ds an agile unified platform designed to govern and accelerate data integration processes by eliminating manual processes and replacing them with software designed to automate govern and accelerate manual processes ref cite web title mapping manager url http analytixds com amm ref several versions of mapping manager were released since the release of its first version in 2006 analytix ds in april 2016 launched mapping manager version 7 0 a major release version which extends the unified platform for enterprise data mapping governance and automation ref cite web title analytix data services announces the launch of major release of mapping manager version 7 0 url http www pr com press release 680901 publisher pr accessdate 27 july 2016 ref the latest release has added plenty of features and revolutionary modules never before seen with every new release out of box features and functionalities were introduced later analytix ds began to add new modules including release management reference data manager code set manager catfx litespeed conversion code automation templates for data vault mapping manager big data edition data quality assessment manager dqam metadata management data vault code gen bundle and test manager which extends the tools capabilities above and beyond management of the data mapping process mapping manager big data edition ref cite web title mapping manager big data edition url http analytixds com mapping manager bigdata edition ref helps automate the big data mapping process and provides a bridge between structured and unstructured data to meet big data challenges release management ref cite web title analytix ds release manager url http analytixds com arm ref helps track the release process approvals audits and verifications through the approval process reference data manager ref cite web title analytix ds reference data manager url http analytixds com rdm ref helps create database like structure to maintain all reference data code set manager ref cite web title analytix ds codeset manager url http analytixds com code set manager ref helps drive the organization of user defined reference data and code sets across an enterprise customizable code automation framework catfx ref cite web title analytix ds customizable code automation framework catfx url http analytixds com catfx code automation templates ref helps automate manual coding and tasks for etl integration and data profiling testing automation and more litespeed conversion ref cite web title analytix ds litespeed conversion url http analytixds com litespeed conversion ref helps automate the conversion of etl tool platforms through an automated framework code automation templates for data vault ref cite web title analytix ds code automation templates for data vault url http analytixds com data vault automation ref helps automatically generate the hub etl code link etl code and the satellite etl code through your existing etl platform data quality assessment manager dqam ref cite web title analytix ds data quality assessment manager dqam url http analytixds com data quality assessment manager ref helps standardize and execute a formal data quality assessment methodology metadata management ref cite web title analytix ds metadata management url http analytixds com metadata management ref helps metadata in mapping manager unified platform to be ported into third party metadata environments data vault code gen bundle ref cite web title analytix ds data vault code gen bundle url http analytixds com data vault code gen bundle ref helps automate the code generation process for building the data vault through code automation templates cat s test manager ref cite web title analytix ds test manager url http analytixds com test manager ref helps test cases and test sql generation to be managed in a purpose built module for testing data mappings and etl processes recently analytix ds expanded its presence in the us with the opening of a new office in dallas analytix ds has been named to cio review s 20 most promising productivity tools solution providers 2015 ref cite web title cio review s 20 most promising productivity tools solution providers 2015 url http www pr com press release 662862 publisher pr accessdate 18 march 2016 ref analytix ds is a platinum sponsor for wwdvc 2016 ref cite web title analytix data services to platinum sponsor for wwdvc 2016 url http wwdvc com sponsors ref references reflist external links official website http www analytixds com youtube u analytixds analytix ds facebook analytix data services analytix ds category software companies of india category data management category extract transform load tools category data mapping category data warehousing products category international information technology consulting firms category multinational companies headquartered in the united states'
b'about semantic differences in data other uses heterogeneity disambiguation semantic heterogeneity is when database schema or data set datasets for the same domain are developed by independent parties resulting in differences in meaning and interpretation of data values ref cite journal title why your data won t mix author alon halevy journal queue volume 3 issue 8 year 2005 url http queue acm org detail cfm id 1103836 ref beyond data structure structured data the problem of semantic heterogeneity is compounded due to the flexibility of semi structured data and various tag metadata tagging methods applied to documents or unstructured data semantic heterogeneity is one of the more important sources of differences in heterogeneous database system heterogeneous datasets yet for multiple data sources to interoperability interoperate with one another it is essential to reconcile these semantics semantic differences decomposing the various sources of semantic heterogeneities provides a basis for understanding how to map and transform data to overcome these differences classification one of the first known classification schemes applied to semantic data model data semantics is from william kent more than two decades ago ref cite conference title the many forms of a single fact author william kent conference proceedings of the ieee compcon date february 27 march 3 1989 location san francisco number hpl sal 88 8 hewlett packard laboratories oct 21 1988 at 13 pp url http www bkent net doc manyform htm ref kent s approach dealt more with structural data mapping mapping issues than differences in meaning which he pointed to data dictionary data dictionaries as potentially solving one of the most comprehensive classifications is from pluempitiwiriyawej and hammer classification scheme for semantic and schematic heterogeneities in xml data sources ref cite news title a classification scheme for semantic and schematic heterogeneities in xml data sources author charnyote pluempitiwiriyawej and joachim hammer publisher university of florida at technical report tr00 004 location gainesville florida date september 2000 url https cise ufl edu tr doc rep 2000 396 pdf ref they classify heterogeneities into three broad classes data structure structural conflicts arise when the schema of the sources representing related or overlapping data exhibit discrepancies structural conflicts can be detected when comparing the underlying schema the class of structural conflicts includes generalization conflicts aggregation conflicts internal path discrepancy missing items element ordering constraint and type mismatch and naming conflicts between the element types and attribute names data domain domain conflicts arise when the semantics of the data sources that will be integrated exhibit discrepancies domain conflicts can be detected by looking at the information contained in the schema and using knowledge about the underlying data domains the class of domain conflicts includes schematic discrepancy scale or unit precision and data representation conflicts data conflicts refer to discrepancies among similar or related data values across multiple sources data conflicts can only be detected by comparing the underlying sources the class of data conflicts includes id value missing data incorrect spelling and naming conflicts between the element contents and the attribute values moreover mismatches or conflicts can occur between set elements a population mismatch or attributes a description mismatch michael bergman expanded upon this schema by adding a fourth major explicit category of language and also added some examples of each kind of semantic heterogeneity resulting in about 40 distinct potential categories ref cite web title sources and classification of semantic heterogeneities author m k bergman website ai3 adaptive information date 6 june 2006 accessdate 28 september 2014 url http www mkbergman com 232 sources and classification of semantic heterogeneities ref ref cite web title big structure and data interoperability author m k bergman website ai3 adaptive information date 12 august 2014 accessdate 28 september 2014 url http www mkbergman com 1782 big structure and data interoperability ref this table shows the combined 40 possible sources of semantic heterogeneities across sources style text align left width 100 border 1 cellpadding 3 cellspacing 0 style vertical align middle text align center font weight bold background efefef class style vertical align middle text align center font weight bold background efefef category style vertical align middle text align center font weight bold background efefef subcategory style vertical align middle text align center font weight bold background efefef examples rowspan 8 colspan 1 language rowspan 4 colspan 1 character encoding encoding ingest encoding mismatch for example us ascii ascii v utf 8 ingest encoding lacking mis recognition of tokens because not being parsed with the proper encoding query encoding mismatch for example ascii v utf 8 in search query encoding lacking mis recognition of search tokens because not being parsed with the proper encoding rowspan 4 colspan 1 languages script mismatch variations in how parsers handle say stemming white spaces or hyphens parsing morphological analysis errors many arabic languages right to left v romance languages left to right syntactical errors many ambiguous sentence references such as i m glad i m a man and so is lola lola song lola by ray davies and the kinks semantics errors many river bank v money bank v billiards bank shot rowspan 17 colspan 1 conceptual rowspan 5 colspan 1 naming case sensitivity uppercase v lower case v camel case synonym s united states v usa v america v uncle sam v great satan acronym s united states v usa v us homonym s such as when the same name refers to more than one concept such as name referring to a person v name referring to a book misspellings as stated rowspan 1 colspan 2 generalization specialization when single items in one schema are related to multiple items in another schema or vice versa for example one schema may refer to phone but the other schema has multiple elements such as home phone work phone and cell phone rowspan 2 colspan 1 aggregation intra aggregation when the same population is divided differently such as census v federal regions for states england v great britain v united kingdom or full person names v first middle last inter aggregation may occur when sums or counts are included as set members rowspan 1 colspan 2 internal path discrepancy can arise from different source target retrieval paths in two different schemas for example hierarchical structures where the elements are different levels of remove rowspan 4 colspan 1 missing item content discrepancy differences in set enumerations or including items or not say us territories in a listing of us states missing content differences in scope coverage between two or more datasets for the same concept attribute list discrepancy differences in attribute completeness between two or more datasets missing attribute differences in scope coverage between two or more datasets for the same attribute rowspan 2 colspan 2 item equivalence when two types classes or sets are asserted as being the same when the scope and reference are not for example berlin the city v states of germany subdivisions berlin the official city state when two individuals are asserted as being the same when they are actually distinct for example john f kennedy the president v uss john f kennedy cv 67 john f kennedy the aircraft carrier rowspan 1 colspan 2 type mismatch when the same item is characterized by different types such as a person being typed as an animal v human being v person rowspan 1 colspan 2 constraint mismatch when attributes referring to the same thing have different cardinalities or disjointedness assertions rowspan 9 colspan 1 domain of discourse domain rowspan 4 colspan 1 schematic discrepancy element value to element label mapping rowspan 4 colspan 1 one of four errors that may occur when attribute names say hair v fur may refer to the same attribute or when same attribute names say hair v hair may refer to different attribute scopes say hair v fur or where values for these attributes may be the same but refer to different actual attributes or where values may differ but be for the same attribute and putative value br br many of the other semantic heterogeneities herein also contribute to schema discrepancies attribute value to element label mapping element value to attribute label mapping attribute value to attribute label mapping rowspan 2 colspan 1 scale or units measurement type differences say in the metric v english measurement systems or currencies units differences say in meters v centimeters v millimeters rowspan 1 colspan 2 precision for example a value of 4 1 inches in one dataset v 4 106 in another dataset rowspan 2 colspan 1 data representation primitive data type confusion often arises in the use of literals v uniform resource identifier uris v object types data format delimiting decimals by period v commas various date formats using exponents or aggregate units such as thousands or millions rowspan 8 colspan 1 data rowspan 5 colspan 1 naming case sensitivity uppercase v lower case v camel case synonyms for example centimeters v cm acronyms for example currency symbols v currency names homonyms such as when the same name refers to more than one attribute such as name referring to a person v name referring to a book misspellings as stated rowspan 1 colspan 2 id mismatch or missing id uris can be a particular problem here due to actual mismatches but also use of name spaces or not and truncated uris rowspan 1 colspan 2 missing data a common problem more acute with closed world approaches than with open world assumption open world ones rowspan 1 colspan 2 element ordering set members can be ordered or unordered and if ordered the sequences of individual members or values can differ a different approach toward classifying semantics and integration approaches is taken by amit sheth sheth et al ref cite journal title semantics for the semantic web the implicit the formal and the powerful author1 amit p sheth author2 cartic ramakrishnan author3 christopher thomas journal int l journal on semantic web information systems volume 1 issue 1 pages 1 18 date 2005 url http www informatik uni trier de ley db journals ijswis ijswis1 html ref under their concept they split semantics into three forms implicit formal and powerful implicit semantics are what is either largely present or can easily be extracted formal languages though relatively scarce occur in the form of ontology information science ontologies or other description logic s and powerful soft semantics are fuzzy and not limited to rigid set based assignments sheth et al s main point is that first order logic fol or description logic is inadequate alone to properly capture the needed semantics relevant applications besides data interoperabiity relevant areas in information technology that depend on reconciling semantic heterogeneities include data mapping semantic integration and enterprise information integration among many others from the conceptual to actual data there are differences in perspective vocabularies measures and conventions once any two data sources are brought together explicit attention to these semantic heterogeneities is one means to get the information to integrate or interoperate a mere twenty years ago information technology systems expressed and stored data in a multitude of formats and systems the internet and web protocols have done much to overcome these sources of differences while there is a large number of categories of semantic heterogeneity these categories are also patterned and can be anticipated and corrected these patterned sources inform what kind of work must be done to overcome semantic differences where they still reside see also data integration data mapping enterprise information integration heterogeneous database system interoperability ontology based data integration schema matching semantic integration semantic matching semantics references references further reading http wiki opensemanticframework org index php classification of semantic heterogeneity classification of semantic heterogeneity category data management category interoperability category knowledge management category semantics'
b'the concept of systems of engagement is attributed to geoffrey moore a business author of such books as crossing the chasm ref crossing the chasm marketing and selling high tech products to mainstream customers 1991 revised 1999 and 2014 isbn 0 06 051712 3 ref in his paper for aiim org entitled systems of engagement and the future of enterprise it moore states amidst the texting and twittering and facebooking of a generation of digital natives the fundamentals of next generation communication and collaboration are being worked out for them it is clear there is no going back so at minimum if you expect these folks to be your customers your employees and your citizens and frankly where else could you look then you need to apply their expectations to the next generation of enterprise it systems systems of engagement \xe2\x80\xa6 will overlay and complement our deep investments in systems of record ref moore geoffrey 2011 systems of engagement and the future of enterprise it http www aiim org futurehistory ref cite news last moore first geoffrey date 2011 title systems of engagement and the future of enterprise it a sea change in enterprise it url http www aiim org futurehistory accessdate october 7 2014 since then systems of engagement has been adopted by organizations such as http blogs forrester com category systems of engagement forrester research hewlett packard hp ibm aiim and http www avoka com blog 2013 07 deliver a system of engagement avoka forrester defines systems of engagement as follows systems of engagement are different from the traditional systems of record that log transactions and keep the financial accounting in order they focus on people not processes these new systems harness a perfect storm of mobile social cloud and big data innovation to deliver apps and smart products directly in the context of the daily lives and real time workflows of customers partners and employees ref http blogs forrester com ted schadler 12 02 14 a billion smartphones require new systems of engagement ref cite news last schadler first ted date february 14 2012 title a billion smartphones require new systems of engagement url http blogs forrester com ted schadler 12 02 14 a billion smartphones require new systems of engagement accessdate october 7 2014 see also system of record conventional enterprise systems designed to contain the authoritative data source for a given piece of information references reflist category information systems category data management compu stub'
b'file bbc genome logo png thumb bbc genome logo the bbc genome project is a digitised searchable database of programme listings from the radio times from the first issue in 1923 to 2009 ref name about cite web title about this project url http genome ch bbc co uk about publisher bbc accessdate 21 october 2014 ref history prior bbc genome is not the bbc s first online searchable database in april 2006 the bbc gave the public access to infax the bbc s programme database infax contained around 900 000 entries but not every programme ever broadcast and it ceased operation in december 2007 ref name about infax cite web title about this prototype url http open bbc co uk cataloguemeta 2005 11 about this prototype html publisher bbc accessdate 2 february 2016 archiveurl https web archive org web 20060613100552 http open bbc co uk cataloguemeta 2005 11 about this prototype html archivedate 13 june 2006 ref the front page of the website is still available to see via the internet archive https web archive org web 20060512054648 http open bbc co uk catalogue infax here after infax ceased a message on the website said that it would be incorporating in the information into individual programme pages ref name prototype end cite web title this experimental prototype trial has now concluded url http www bbc co uk archive catalogue offline shtml publisher bbc accessdate 2 february 2016 ref in 2012 it was replaced by the database fabric but this is only for internal use within the bbc radio times file bbc genome ocr error jpg thumb screenshot of an ocr error since corrected in genome the text uza tarbuclc s christmas should read liza tarbuck s christmas in december 2012 the bbc completed a digitisation exercise scanning the listings from radio times of all bbc programmes 1923 2009 from an entire run of about 4 500 copies of the magazine ref name kelion cite web url http www bbc co uk news technology 20625884 title bbc finishes radio times archive digitisation effort last kelion first leo work bbc online accessdate 20 january 2013 ref they identified around five million programmes involving 8 5 million actors presenters writers and technical staff ref name kelion the listings are as published in advance and so do not include late changes or cancellations the issues were scanned at high resolution producing tif images and optical character recognition ocr was then used to turn the text from the page into searchable text on the genome database ref name about bbc genome was released for public use on 15 october 2014 ref name hilbish cite web url http www bbc co uk blogs aboutthebbc posts genome the radio times archive is now live title genome radio times archive now live last bishop first hilary work bbc online accessdate 15 october 2014 ref ref cite news last sweney first mark title bbc digitises radio times back issues url https www theguardian com media 2014 oct 16 bbc digitises radio times back issues genome project newspaper guardian date 16 october 2014 ref the aim of this project is to allow researchers to be able to find out information easier and to help bbc archives to build up a picture of what exists and what is currently missing from the archive ref cite web title bbc s genome project offers radio and tv archive listings url http www bbc co uk news technology 29643662 publisher bbc accessdate 21 october 2014 date 16 october 2014 ref ref cite web last1 sweney first1 mark title bbc digitises radio times back issues url https www theguardian com media 2014 oct 16 bbc digitises radio times back issues genome project publisher the guardian accessdate 21 october 2014 date 16 october 2014 ref corrections to ocr errors and changes to advertised schedules are being crowdsourcing crowdsourced ref name hilbish with over 180 000 user generated edits accepted as of january 2017 ref cite web url http genome ch bbc co uk schedules bbcone london 1964 04 20 title bbc one london 20 april 1964 bbc genome website genome ch bbc co uk access date 2017 01 09 ref each listing entry has a unique identifier which may be expressed as a url for example the very first screening of doctor who is http genome ch bbc co uk 8f81c193ba224e84981f353cae480d49 a broadcast programme may have more than one such identifier if it was screened and thus listed on repeat occasions see also bbc archives timeline of the bbc references reflist 2 external links wikidata property p1573 bbc genome identifiers http genome ch bbc co uk bbc genome website http www bbc co uk blogs genome bbc genome blog http twitter com bbcgenome bbc genome on twitter http www facebook com bbcgenome bbc genome on facebook bbc category bbc category bbc new media archives category data management category broadcasting websites category british websites category history of television in the united kingdom category history of radio category bbc history category databases in the united kingdom'
b'multiple issues unreferenced date october 2008 original research date october 2008 rtolap real time olap whilst many olap servers like microsoft analysis services store pre calculating consolidations and calculated elements to achieve rapid response times a real time olap server will calculate the values on the fly when they are required the essential characteristic of rtolap system is in holding all the data in ram it is a protocol which analyzes fly values when required it saves every bit of information in ram the calculations are executed in a right away manner which reduces the setback linked with information outburst since it only saves information under the ram size standard advantages since precalculated values aren t stored the size of a cube in an rtolap system is smaller than of an olap product which resorts to precalculation rtolap often reduces the problem which may be associated with data explosion by means of storing less data rtolap essentially performs calculations just in time by only calculating values when they are needed space can be saved since in a precalculated system a great deal of calculations will be stored which may well never be called up incremental updates are available once they are loaded and any modifications to data will flow through the system immediately with rtolap when a change is made everyone sees the result this isn t a unique characteristic of rtolap since other olap systems e g sas institute microsoft analysis services microstrategy behave the same way disadvantages since rtolap stores the entire cube in ram it doesn t scale to the data volumes larger than the ram size performance of queries can be slower since the values need to be calculated on the fly instead of being accessed from the precalculated storage category data management category information technology management category online analytical processing'
b'lead too short date october 2014 data management comprises all the list of academic disciplines disciplines related to managing data as a valuable resource overview the official definition provided by dama international the professional organization for those in the data management profession is data resource management is the development and execution of architectures policies practices and procedures that properly manage the full data lifecycle needs of an enterprise this definition is fairly broad and encompasses a number of professions which may not have direct technical contact with lower level aspects of data management such as relational database management file the data lifecycle jpg thumb the data lifecycle alternatively the definition provided in the dama data management body of knowledge ref https technicspub com dmbok dama dmbok ref is data management is the development execution and supervision of plans policies programs and practices that control protect deliver and enhance the value of data and information assets ref dama dmbok guide data management body of knowledge introduction project status note pdf no longer available online at https www dama org current version available for purchase ref the concept of data management arose in the 1980s as technology moved from sequential processing first cards then tape to random access processing since it was now technically possible to store a single fact in a single place and access that using random access disk those suggesting that data management was more important than process management used arguments such as a customer s home address is stored in 75 or some other large number places in our computer systems during this period random access processing was not competitively fast so those suggesting process management was more important than data management used batch processing time as their primary argument as applications moved into real time interactive applications it became obvious to most practitioners that both management processes were important if the data was not well defined the data would be mis used in applications if the process wasn t well defined it was impossible to meet user needs corporate data quality management corporate data quality management cdqm is according to the efqm european foundation for quality management and the competence center corporate data quality cc cdq university of st gallen the whole set of activities intended to improve corporate data quality both reactive and preventive main premise of cdqm is the business relevance of high quality corporate data cdqm comprises with following activity areas ref https benchmarking iwi unisg ch framework for cdqm pdf efqm iwi hsg efqm framework for corporate data quality management brussels efqm press 2011 ref strategy for corporate data quality as cdqm is affected by various business drivers and requires involvement of multiple divisions in an organization it must be considered a company wide endeavor corporate data quality controlling effective cdqm requires compliance with standards policies and procedures compliance is monitored according to previously defined metrics and performance indicators and reported to stakeholders corporate data quality organization cdqm requires clear roles and responsibilities for the use of corporate data the cdqm organization defines tasks and privileges for decision making for cdqm corporate data quality processes and methods in order to handle corporate data properly and in a standardized way across the entire organization and to ensure corporate data quality standard procedures and guidelines must be embedded in company s daily processes data architecture for corporate data quality the data architecture consists of the data object model which comprises the unambiguous definition and the conceptual model of corporate data and the data storage and distribution architecture applications for corporate data quality software applications support the activities of corporate data quality management their use must be planned monitored managed and continuously improved topics in data management topics in data management grouped by the dama dmbok framework ref http dama dach org dama dmbok functional framework dama dmbok functional framework v3 ref include colbegin 2 data governance data asset data governance data steward data architecture analysis and design data analysis data architecture data modeling database management data maintenance database administration database management system data security management data access data erasure data privacy data security data quality management data cleansing data integrity data enrichment data quality data quality assurance reference and master data management data integration master data management reference data data warehousing and business intelligence management business intelligence data mart data mining data movement extract transform load data warehouse document record and content management document management system records management meta data management meta data management metadata metadata discovery metadata publishing metadata registry contact data management business continuity planning marketing operations customer data integration identity management identity theft data theft erp software crm software address geography postal code email address telephone number colend body of knowledge the dama guide to the data management body of knowledge dama dmbok guide under the guidance of a new dama dmbok editorial board this publication is available from april 5 2009 usage in modern management fad management usage one can easily discern a trend away from the term data in composite expressions to the term information or even knowledge when talking in a non technical context thus there exists not only data management but also information management and knowledge management this is a misleading trend as it obscures that traditional data are managed or somehow data processing processed on second looks cn date june 2016 the distinction between data and derived values can be seen in the information ladder cn date june 2016 while data can exist as such information and knowledge are always in the eye or rather the brain of the beholder and can only be measured in relative units several organisations have established a data management centre dmc ref for example cite book last1 kumar first1 sangeeth last2 ramesh first2 maneesha vinodini chapter lightweight management framework lmf for a heterogeneous wireless network for landslide detection editor1 last meghanathan editor1 first natarajan editor2 last boumerdassi editor2 first selma editor3 last chaki editor3 first nabendu editor4 last nagamalai editor4 first dhinaharan title recent trends in networks and communications international conferences necom 2010 wimon 2010 west 2010 chennai india july 23 25 2010 proceedings url https books google com books id 8i5qcqaaqbaj series communications in computer and information science volume 90 publisher springer publication date 2010 page 466 isbn 9783642144936 accessdate 2016 06 16 quote 4 4 data management center dmc the data management center is the data center for all of the deployed cluster networks through the dmc the lmf allows the user to list the services in any cluster member belonging to any cluster ref for their operations integrated data management integrated data management idm is a tools approach to facilitate data management and improve performance idm consists of an integrated modular environment to manage enterprise application data and optimize data driven applications over its information lifecycle management lifetime ref http www ibm com developerworks data library techarticle dm 0807hayes s tact 105agx11 s cmp fp ibm content integrated data management managing data across its lifecycle by holly hayes ref ref http www ibmsystemsmagmainframedigital com nxtbooks ibmsystemsmag mainframe 20090708 index php 34 organizations thrive on data by eric naiburg ref ref http download boulder ibm com ibmdl pub software data sw library data management optim reports fragmented pdf fragmented management across the data life cycle increases cost and risk a commissioned study conducted by forrester consulting on behalf of ibm october 2008 ref ref http publib boulder ibm com infocenter idm v2r1 index jsp integrated ibm data management information center ref idm s purpose is to produce enterprise ready applications faster improve data access speed iterative testing empower collaboration between architects developers and dbas consistently achieve service level targets automate and simplify operations provide contextual intelligence across the solution stack support business growth accommodate new initiatives without expanding infrastructure simplify application upgrades consolidation and retirement facilitate alignment consistency and governance define business policies and standards up front share extend and apply throughout the lifecycle see also colbegin 2 open data information architecture information management enterprise architecture information design information system controlled vocabulary data curation data retention data governance data quality data modeling data management plan information lifecycle management computer data storage data proliferation digital preservation digital perpetuation document management enterprise content management hierarchical storage management information repository records management system integration colend references reflist external links dmoz computers software master data management articles defaultsort data management category data management category information technology management'
b'a relational data stream management system rdsms is a distributed in memory data stream management system dsms that is designed to use standards compliant sql queries to process unstructured and structured data streams in real time unlike sql queries executed in a traditional rdbms which return a result and exit sql queries executed in a rdsms do not exit generating results continuously as new data become available continuous sql queries in a rdsms use the sql window function to analyze join and aggregate data streams over fixed or sliding windows windows can be specified as time based or row based rdsms sql query examples continuous sql queries in a rdsms conform to the ansi sql standards the most common rdsms sql query is performed with the declarative code select code statement a continuous sql code select code operates on data across one or more data streams with optional keywords and clauses that include code from code with an optional code join code subclause to specify the rules for joining multiple data streams the code where code clause and comparison predicate to restrict the records returned by the query code group by code to project streams with common values into a smaller set code having code to filter records resulting from a code group by code and code order by code to sort the results the following is an example of a continuous data stream aggregation using a code select code query that aggregates a sensor stream from a weather monitoring station the code select code query aggregates the minimum maximum and average temperature values over a one second time period returning a continuous stream of aggregated results at one second intervals source lang sql select stream floor weatherstream rowtime to second as floor second min temp as min temp max temp as max temp avg temp as avg temp from weatherstream group by floor weatherstream rowtime to second source rdsms sql queries also operate on data streams over time or row based windows the following example shows a second continuous sql query using the code window code clause with a one second duration the code window code clause changes the behavior of the query to output a result for each new record as it arrives hence the output is a stream of incrementally updated results with zero result latency source lang sql select stream rowtime min temp over w1 as wmin temp max temp over w1 as wmax temp avg temp over w1 as wavg temp from weatherstream window w1 as range interval 1 second preceding source see also sql nosql newsql external links http www sqlstream com stream processing stream processing with sql http researcher watson ibm com researcher view group php id 2531 ibm system s http www mcjones org system r sql reunion 95 sqlr95 html 1995 sql reunion people projects and politics by paul mcjones ed transcript of a reunion meeting devoted to the personal history of relational databases sql system r category data management category relational model'
b'semantic queries allow for queries and analytics of associative and contextual nature semantic queries enable the retrieval of both explicitly and implicitly derived information based on syntactic semantic and structural information contained in data they are designed to deliver precise results possibly the distinctive selection of one single piece of information or to answer more fuzzy and wide open questions through pattern matching and reasoning system digital reasoning semantic queries work on named graphs linked data linked data or semantic triple triples this enables the query to process the actual relationships between information and infer the answers from the network of data this is in contrast to semantic search which uses semantics the science of meaning in unstructured data unstructured text to produce a better search result see natural language processing from a technical point of view semantic queries are precise relational type operations much like a sql database query they work on structured data and therefore have the possibility to utilize comprehensive features like operators e g and namespaces pattern matching type inheritance subclassing transitive relation s semantic web rule language semantic rules and contextual full text index full text search the semantic web technology stack of the w3c is offering sparql ref name xml com cite web url http www xml com pub a 2005 11 16 introducing sparql querying semantic web tutorial html title introducing sparql querying the semantic web publisher xml com date 2005 ref ref name w3c cite web url http www w3 org tr rdf sparql query title sparql query language for rdf publisher w3c date 2008 ref to formulate semantic queries in a syntax similar to sql semantic queries are used in triplestore s graph databases semantic wiki s natural language and artificial intelligence systems background relational database s contain all relationships between data in an implicit manner only ref name acm dl cite web url http portal acm org citation cfm id 1646157 title semantic queries in databases problems and challenges publisher acm digital library date 2009 ref ref name eswc cite web url http 2012 eswc conferences org sites default files eswc2012 submission 357 pdf title karma a system for mapping structured sources into the semantic web publisher eswc conferences org date 2012 ref for example the relationships between customers and products stored in two content tables and connected with an additional link table only come into existence in a query statement sql in the case of relational databases written by a developer writing the query demands exact knowledge of the database schema ref name ieee cite web url http www scf usc edu taheriya papers taheriyan14 icsc paper pdf title a scalable approach to learn semantic models of structured sources publisher 8th ieee international conference on semantic computing date 2014 ref ref name aaai cite web url http www isi edu integration papers knoblock13 sbd pdf title semantics for big data integration and analysis publisher aaai fall symposium on semantics for big data date 2013 ref linked data linked data contain all relationships between data in an explicit manner in the above example no query code needs to be written the correct product for each customer can be fetched automatically whereas this simple example is trivial the real power of linked data comes into play when a network of information is created customers with their geo spatial information like city state and country products with their categories within sub and super categories now the system can automatically answer more complex queries and analytics that look for the connection of a particular location with a product category the development effort for this query is omitted executing a semantic query is conducted by walking the network of information and finding matches also called data graph traversal another important aspect of semantic queries is that the type of the relationship can be used to incorporate intelligence into the system the relationship between a customer and a product has a fundamentally different nature than the relationship between a neighbourhood and its city the latter enables the semantic query engine to infer that a customer living in manhattan is also living in new york city whereas other relationships might have more complicated patterns and contextual analytics this process is called inference or reasoning and is the ability of the software to derive new information based on given facts articles cite web last velez first golda year 2008 url http www wallstreetandtech com data management showarticle jhtml articleid 208700210 pgno 2 title semantics help wall street cope with data overload publisher wallstreetandtech com cite web last zhifeng first xiao year 2009 url http adsabs harvard edu abs 2009spie 7492e 60x title spatial information semantic query based on sparql publisher international symposium on spatial analysis cite web last aquin first mathieu year 2010 url http www semantic web journal net sites default files swj96 1 pdf title watson more than a semantic web search engine publisher semantic web journal cite web last prudhommeaux first eric year 2010 url http www cambridgesemantics com semantic university sparql vs sql intro title sparql vs sql introduction publisher cambridge semantics cite web last dworetzky first tom year 2011 url http www ibtimes com how siri works iphones brain comes natural language processing stanford professors teach free online title how siri works iphone s brain comes from natural language processing publisher international business times cite web last horwitt first elisabeth year 2011 url http www computerworld com s article 9209118 the semantic web gets down to businessarticleid 208700210 pgno 2 title the semantic web gets down to business publisher computerworld com cite web last rodriguez first marko year 2011 url http markorodriguez com 2011 06 15 graph pattern matching with gremlin 1 1 title graph pattern matching with gremlin publisher markorodriguez com on graph computing cite web last sequeda first juan year 2011 url http www cambridgesemantics com semantic university sparql nuts and bolts title sparql nuts bolts publisher cambridge semantics cite web last freitas first andre year 2012 url https www deri ie sites default files publications freitas ic 12 pdf title querying heterogeneous datasets on the linked data web publisher ieee internet computing cite web last kauppinen first tomi year 2012 url http linkedscience org tools sparql package for r tutorial on sparql package for r title using the sparql package in r to handle spatial linked data publisher linkedscience org cite web last lorentz first alissa year 2013 url http www wired com 2013 04 with big data context is a big issue title with big data context is a big issue publisher wired see also dataspaces knowledge representation linked data ontology alignment semantic integration semantic publishing semantics of business vocabulary and business rules sparql references reflist external links http www w3 org standards semanticweb query w3c semantic web standards query category data management category query languages category semantic web'
b'peacock date may 2015 data lineage is defined as a data life cycle that includes the data s origins and where it moves over time ref http www techopedia com definition 28040 data lineage ref it describes what happens to data as it goes through diverse processes it helps provide visibility into the analytics pipeline and simplifies tracing errors back to their sources it also enables replaying specific portions or inputs of the dataflow for step wise debugging or regenerating lost output in fact database systems have used such information called data provenance to address similar validation and debugging challenges already ref name desoumyarupa de soumyarupa 2012 newt an architecture for lineage based replay and debugging in disc systems uc san diego b7355202 retrieved from https escholarship org uc item 3170p7zn ref data lineage provides a visual representation to discover the data flow movement from its source to destination via various changes and hops on its way in the enterprise environment data lineage represents how the data hops between various data points how the data gets transformed along the way how the representation and parameters change and how the data splits or converges after each hop easier representation of the data lineage can be shown with dots and lines where dot represents a data container for data point s and lines connecting them represents the transformation s the data point under goes between the data containers deleted image removed file datalineage dots lines png representation of data lineage broadly depends on scope of the meta data management metadata management and reference point of interest data lineage provides sources of the data and intermediate data flow hops from the reference point with backward data lineage leads to the final destination s data points and its intermediate data flows with forward data lineage these views can be combined with end to end lineage for a reference point that provides complete audit trail of that data point of interest from source s to its final destination s as the data points or hops increases the complexity of such representation becomes incomprehensible thus the best feature of the data lineage view would be to be able to simplify the view by temporarily masking unwanted peripheral data points tools that have the masking feature enables scalability of the view and enhances analysis with best user experience for both technical and business users alike scope of the data lineage determines the volume of metadata required to represent its data lineage usually data governance data governance and data management data management determines the scope of the data lineage based on their regulation s enterprise data management strategy data impact reporting attributes and critical data element s of the organization data lineage provides the audit trail of the data points at the lowest granular level but presentation of the lineage may be done at various zoom levels to simplify the vast information similar to the analytic web maps data lineage can be visualized at various levels based on the granularity of the view at a very high level data lineage provides what systems the data interacts before it reaches destination as the granularity increases it goes up to the data point level where it can provide the details of the data point and its historical behavior attribute properties and trends and data quality data quality of the data passed through that specific data point in the data lineage data governance data governance plays a key role in metadata management for guidelines strategies policies implementation data quality data quality and master data management master data management helps in enriching the data lineage with more business value even though the final representation of data lineage is provided in one interface but the way the metadata is harvested and exposed to the data lineage graphical user interface user interface ui could be entirely different thus data lineage can be broadly divided into three categories based on the way metadata is harvested data lineage involving software packages for structured data programming languages and big data data lineage expects to view at least the technical metadata involving the data points and its various transformations along with technical data data lineage may enrich the metadata with their corresponding data quality results reference data values data model data models glossary of business and management terms business vocabulary data steward people program management programs and enterprise system systems linked to the data points and transformations masking feature in the data lineage visualization allows the tools to incorporate all the enrichments that matter for the specific use case metadata normalization may be done in data lineage to represent disparate systems into one common view data provenance documents the inputs entities systems and processes that influence data of interest in effect providing a historical record of the data and its origins the generated evidence supports essential forensic activities such as data dependency analysis error compromise detection and recovery and auditing and compliance analysis lineage is a simple type of why provenance ref name desoumyarupa case for data lineage the world of big data is changing dramatically right before our eyes statistics say that ninety percent 90 of the world s data has been created in the last two years alone ref http newstex com 2014 07 12 thedataexplosionin2014minutebyminuteinfographic ref this explosion of data has resulted in the ever growing number of systems and automation at all levels in all sizes of organizations today distributed systems like google map reduce ref jeffrey dean and sanjay ghemawat mapreduce simplified data processing on large clusters commun acm 51 1 107 113 january 2008 ref microsoft dryad ref michael isard mihai budiu yuan yu andrew birrell and dennis fetterly dryad distributed data parallel programs from sequential building blocks in proceedings of the 2nd acm sigops eurosys european conference oncomputer systems 2007 eurosys 07 pages 59 72 new york ny usa 2007 acm ref apache hadoop ref apache hadoop http hadoop apache org ref an open source project and google pregel ref grzegorz malewicz matthew h austern aart j c bik james c dehnert ilan horn naty leiser and grzegorz czajkowski pregel a system for largescale graph processing in proceedings of the 2010 international conference on managementof data sigmod 10 pages 135 146 new york ny usa 2010 acm ref provide such platforms for businesses and users however even with these systems big data analytics can take several hours days or weeks to run simply due to the data volumes involved for example a ratings prediction algorithm for the netflix prize challenge took nearly 20 hours to execute on 50 cores and a large scale image processing task to estimate geographic information took 3 days to complete using 400 cores ref shimin chen and steven w schlosser map reduce meets wider varieties of applications technical report intel research 2008 ref the large synoptic survey telescope is expected to generate terabytes of data every night and eventually store more than 50 petabytes while in the bioinformatics sector the largest genome 12 sequencing houses in the world now store petabytes of data apiece ref the data deluge in genomics https www 304 ibm com connections blogs ibmhealthcare entry data overload in genomics3 lang de 2010 ref due to the humongous size of the big data there could be features in the data that are not considered in the machine learning algorithm possibly even outliers it is very difficult for a data scientist to trace an unknown or an unanticipated result big data debugging big data analytics is the process of examining large data sets to uncover hidden patterns unknown correlations market trends customer preferences and other useful business information they apply machine learning algorithms etc to the data which transform the data due to the humongous size of the data there could be unknown features in the data possibly even outliers it is pretty difficult for a data scientist to actually debug an unexpected result the massive scale and unstructured nature of data the complexity of these analytics pipelines and long runtimes pose significant manageability and debugging challenges even a single error in these analytics can be extremely difficult to identify and remove while one may debug them by re running the entire analytics through a debugger for step wise debugging this can be expensive due to the amount of time and resources needed auditing and data validation are other major problems due to the growing ease of access to relevant data sources for use in experiments sharing of data between scientific communities and use of third party data in business enterprises ref yogesh l simmhan beth plale and dennis gannon a survey of data prove nance in e science sigmod rec 34 3 31 36 september 2005 ref ref name ianfosterjensvockler ian foster jens vockler michael wilde and yong zhao chimera a virtual data system for representing querying and automating data derivation in 14th international conference on scientific and statistical database management july 2002 ref ref name benjamim luiz benjamin h sigelman luiz andr barroso mike burrows pat stephenson manoj plakal donald beaver saul jaspan and chandan shanbhag dapper a large scale distributed systems tracing infrastructure technical report google inc 2010 ref ref name peterbuneman peter buneman sanjeev khanna and wang chiew tan data provenance some basic issues in proceedings of the 20th conference on foundations of softwaretechnology and theoretical computer science fst tcs 2000 pages 87 93 london uk uk 2000 springer verlag ref these problems will only become larger and more acute as these systems and data continue to grow as such more cost efficient ways of analyzing data intensive computing data intensive scalable computing disc are crucial to their continued effective use challenges in big data debugging massive scale according to an emc idc study ref http www emc com about news press 2012 20121211 01 htm ref 2 8zb of data were created and replicated in 2012 the digital universe will double every two years between now and 2020 and there will be approximately 5 2tb of data for every man woman and child on earth in 2020 working with this scale of data has become very challenging unstructured data the phrase unstructured data usually refers to information that doesn t reside in a traditional row column database unstructured data files often include text and multimedia content examples include e mail messages word processing documents videos photos audio files presentations webpages and many other kinds of business documents note that while these sorts of files may have an internal structure they are still considered unstructured because the data they contain doesn t fit neatly in a database experts estimate that 80 to 90 percent of the data in any organization is unstructured and the amount of unstructured data in enterprises is growing significantly often many times faster than structured databases are growing big data can include both structured and unstructured data but idc estimates that 90 percent of big data is unstructured data ref webopedia http www webopedia com term u unstructured data html ref long runtime in today s hyper competitive business environment companies not only have to find and analyze the relevant data they need they must find it quickly the challenge is going through the sheer volumes of data and accessing the level of detail needed all at a high speed the challenge only grows as the degree of granularity increases one possible solution is hardware some vendors are using increased memory and powerful parallel processing to crunch large volumes of data extremely quickly another method is putting data in memory but using a grid computing approach where many machines are used to solve a problem both approaches allow organizations to explore huge data volumes even this level of sophisticated hardware and software few of the image processing tasks in large scale take a few days to few weeks ref sas http www sas com resources asset five big data challenges article pdf ref debugging of the data processing is extremely hard due to long run times complex platform big data platforms have a very complicated structure data is distributed among several machines typically the jobs are mapped into several machines and results are later combined by reduce operations debugging of a big data pipeline becomes very challenging because of the very nature of the system it will not be an easy task for the data scientist to figure out which machine s data has the outliers and unknown features causing a particular algorithm to give unexpected results proposed solution data provenance or data lineage can be used to make the debugging of big data pipeline easier this necessitates the collection of data about data transformations the below section will explain data provenance in more detail data provenance data provenance provides a historical record of the data and its origins the provenance of data which is generated by complex transformations such as workflows is of considerable value to scientists from it one can ascertain the quality of the data based on its ancestral data and derivations track back sources of errors allow automated re enactment of derivations to update a data and provide attribution of data sources provenance is also essential to the business domain where it can be used to drill down to the source of data in a data warehouse track the creation of intellectual property and provide an audit trail for regulatory purposes the use of data provenance is proposed in distributed systems to trace records through a dataflow replay the dataflow on a subset of its original inputs and debug data flows to do so one needs to keep track of the set of inputs to each operator which were used to derive each of its outputs although there are several forms of provenance such as copy provenance and how provenance ref name peterbuneman ref robert ikeda and jennifer widom data lineage a survey technical report stanford university 2009 ref the information we need is a simple form of why provenance or lineage as defined by cui et al ref name ycui y cui and j widom lineage tracing for general data warehouse transformations vldb journal 12 1 2003 ref lineage capture intuitively for an operator t producing output o lineage consists of triplets of form i t o where i is the set of inputs to t used to derive o capturing lineage for each operator t in a dataflow enables users to ask questions such as which outputs were produced by an input i on operator t and which inputs produced output o in operator t ref name desoumyarupa a query that finds the inputs deriving an output is called a backward tracing query while one that finds the outputs produced by an input is called a forward tracing query ref name robertikedahyunjung robert ikeda hyunjung park and jennifer widom provenance for generalized map and reduce workflows in proc of cidr january 2011 ref backward tracing is useful for debugging while forward tracing is useful for tracking error propagation ref name robertikedahyunjung tracing queries also form the basis for replaying an original dataflow ref name ianfosterjensvockler ref name ycui ref name robertikedahyunjung however to efficiently use lineage in a disc system we need to be able to capture lineage at multiple levels or granularities of operators and data capture accurate lineage for disc processing constructs and be able to trace through multiple dataflow stages efficiently disc system consists of several levels of operators and data and different use cases of lineage can dictate the level at which lineage needs to be captured lineage can be captured at the level of the job using files and giving lineage tuples of form if i m rjob of i lineage can also be captured at the level of each task using records and giving for example lineage tuples of form k rr v rr map k m v m the first form of lineage is called coarse grain lineage while the second form is called fine grain lineage integrating lineage across different granularities enables users to ask questions such as which file read by a mapreduce job produced this particular output record and can be useful in debugging across different operator and data granularities within a dataflow ref name desoumyarupa file map reduce job 1 png thumb center 500px map reduce job showing containment relationships to capture end to end lineage in a disc system we use the ibis model ref c olston and a das sarma ibis a provenance manager for multi layer systems in proc of cidr january 2011 ref which introduces the notion of containment hierarchies for operators and data specifically ibis proposes that an operator can be contained within another and such a relationship between two operators is called operator containment operator containment implies that the contained or child operator performs a part of the logical operation of the containing or parent operator ref name desoumyarupa for example a mapreduce task is contained in a job similar containment relationships exist for data as well called data containment data containment implies that the contained data is a subset of the containing data superset file containment hierarchy png thumb center 500px containment hierarchy prescriptive data lineage the concept of prescriptive data lineage combines both the logical model entity of how that data should flow with the actual lineage for that instance ref http info hortonworks com rs 549 qal 086 images hadoop governance white paper pdf ref data lineage and provenance typically refers to the way or the steps a dataset came to its current state data lineage as well as all copies or derivatives however simply looking back at only audit or log correlations to determine lineage from a forensic point of view is flawed for certain data management cases for instance it is impossible to determine with certainty if the route a data workflow took was correct or in compliance without the logic model only by combining the a logical model with atomic forensic events can proper activities be validated authorized copies joins or ctas operations mapping of processing to the systems that those process are run on ad hoc versus established processing sequences many certified compliance reports require provenance of data flow as well as the end state data for a specific instance with these types of situations any deviation from the prescribed path need to be accounted for and potentially remediated ref https www sec gov info smallbus secg bd small entity compliance guide htm sec small entity compliance guide ref this is marks a shift in thinking from purely a look back model to a framework which is better suited to capture compliance workflows active vs lazy lineage lazy lineage collection typically captures only coarse grain lineage at run time these systems incur low capture overheads due to the small amount of lineage they capture however to answer fine grain tracing queries they must replay the data flow on all or a large part of its input and collect fine grain lineage during the replay this approach is suitable for forensic systems where a user wants to debug an observed bad output active collection systems capture entire lineage of the data flow at run time the kind of lineage they capture may be coarse grain or fine grain but they do not require any further computations on the data flow after its execution active fine grain lineage collection systems incur higher capture overheads than lazy collection systems however they enable sophisticated replay and debugging ref name desoumyarupa actors an actor is an entity that transforms data it may be a dryad vertex individual map and reduce operators a mapreduce job or an entire dataflow pipeline actors act as black boxes and the inputs and outputs of an actor are tapped to capture lineage in the form of associations where an association is a triplet i t o that relates an input i with an output o for an actor t the instrumentation thus captures lineage in a dataflow one actor at a time piecing it into a set of associations for each actor the system developer needs to capture the data an actor reads from other actors and the data an actor writes to other actors for example a developer can treat the hadoop job tracker as an actor by recording the set of files read and written by each job ref name mainpaper dionysios logothetis soumyarupa de and kenneth yocum 2013 scalable lineage capture for debugging disc analytics in proceedings of the 4th annual symposium on cloud computing socc 13 acm new york ny usa article 17 15 pages ref associations association is a combination of the inputs outputs and the operation itself the operation is represented in terms of a black box also known as the actor the associations describe the transformations that are applied on the data the associations are stored in the association tables each unique actor is represented by its own association table an association itself looks like i t o where i is the set of inputs to the actor t and o is set of outputs given produced by the actor associations are the basic units of data lineage individual associations are later clubbed together to construct the entire history of transformations that were applied to the data ref name desoumyarupa architecture big data systems scale horizontally i e increase capacity by adding new hardware or software entities into the distributed system the distributed system acts as a single entity in the logical level even though it comprises multiple hardware and software entities the system should continue to maintain this property after horizontal scaling an important advantage of horizontal scalability is that it can provide the ability to increase capacity on the fly the biggest plus point is that horizontal scaling can be done using commodity hardware the horizontal scaling feature of big data systems should be taken into account while creating the architecture of lineage store this is essential because the lineage store itself should also be able to scale in parallel with the big data system the number of associations and amount of storage required to store lineage will increase with the increase in size and capacity of the system the architecture of big data systems makes the use of a single lineage store not appropriate and impossible to scale the immediate solution to this problem is to distribute the lineage store itself ref name desoumyarupa the best case scenario is to use a local lineage store for every machine in the distributed system network this allows the lineage store also to scale horizontally in this design the lineage of data transformations applied to the data on a particular machine is stored on the local lineage store of that specific machine the lineage store typically stores association tables each actor is represented by its own association table the rows are the associations themselves and columns represent inputs and outputs this design solves 2 problems it allows horizontal scaling of the lineage store if a single centralized lineage store was used then this information had to be carried over the network which would cause additional network latency the network latency is also avoided by the use of a distributed lineage store ref name mainpaper file selection 065 png thumb center 500px architecture of lineage systems data flow reconstruction the information stored in terms of associations needs to be combined by some means to get the data flow of a particular job in a distributed system a job is broken down into multiple tasks one or more instances run a particular task the results produced on these individual machines are later combined together to finish the job tasks running on different machines perform multiple transformations on the data in the machine all the transformations applied to the data on a machines is stored in the local lineage store of that machines this information needs to be combined together to get the lineage of the entire job the lineage of the entire job should help the data scientist understand the data flow of the job and he she can use the data flow to debug the big data pipeline the data flow is reconstructed in 3 stages association tables the first stage of the data flow reconstruction is the computation of the association tables the association tables exists for each actor in each local lineage store the entire association table for an actor can be computed by combining these individual association tables this is generally done using a series of equality joins based on the actors themselves in few scenarios the tables might also be joined using inputs as the key indexes can also be used to improve the efficiency of a join the joined tables need to be stored on a single instance or a machine to further continue processing there are multiple schemes that are used to pick a machine where a join would be computed the easiest one being the one with minimum cpu load space constraints should also be kept in mind while picking the instance where join would happen association graph the second step in data flow reconstruction is computing an association graph from the lineage information the graph represents the steps in the data flow the actors act as vertices and the associations act as edges each actor t is linked to its upstream and downstream actors in the data flow an upstream actor of t is one that produced the input of t while a downstream actor is one that consumes the output of t containment relationships are always considered while creating the links the graph consists of three types of links or edges explicitly specified links the simplest link is an explicitly specified link between two actors these links are explicitly specified in the code of a machine learning algorithm when an actor is aware of its exact upstream or downstream actor it can communicate this information to lineage api this information is later used to link these actors during the tracing query for example in the mapreduce architecture each map instance knows the exact record reader instance whose output it consumes ref name desoumyarupa logically inferred links developers can attach data flow archetypes to each logical actor a data flow archetype explains how the children types of an actor type arrange themselves in a data flow with the help of this information one can infer a link between each actor of a source type and a destination type for example in the mapreduce architecture the map actor type is the source for reduce and vice versa the system infers this from the data flow archetypes and duly links map instances with reduce instances however there may be several mapreduce jobs in the data flow and linking all map instances with all reduce instances can create false links to prevent this such links are restricted to actor instances contained within a common actor instance of a containing or parent actor type thus map and reduce instances are only linked to each other if they belong to the same job ref name desoumyarupa implicit links through data set sharing in distributed systems sometimes there are implicit links which are not specified during execution for example an implicit link exists between an actor that wrote to a file and another actor that read from it such links connect actors which use a common data set for execution the dataset is the output of the first actor and is the input of the actor following it ref name desoumyarupa topological sorting the final step in the data flow reconstruction is the topological sorting of the association graph the directed graph created in the previous step is topologically sorted to obtain the order in which the actors have modified the data this inherit order of the actors defines the data flow of the big data pipeline or task tracing replay this is the most crucial step in big data debugging the captured lineage is combined and processed to obtain the data flow of the pipeline the data flow helps the data scientist or a developer to look deeply into the actors and their transformations this step allows the data scientist to figure out the part of the algorithm that is generating the unexpected output a big data pipeline can go wrong in 2 broad ways the first is a presence of a suspicious actor in the data flow the second being the existence of outliers in the data the first case can be debugged by tracing the data flow by using lineage and data flow information together a data scientist can figure out how the inputs are converted into outputs during the process actors that behave unexpectedly can be caught either these actors can be removed from the data flow or they can be augmented by new actors to change the data flow the improved data flow can be replayed to test the validity of it debugging faulty actors include recursively performing coarse grain replay on actors in the data flow ref wenchao zhou qiong fei arjun narayan andreas haeberlen boon thau loo and micah sherr secure network provenance in proceedings of 23rd acm symposium on operating system principles sosp december 2011 ref which can be expensive in resources for long dataflows another approach is to manually inspect lineage logs to find anomalies ref name benjamim luiz ref rodrigo fonseca george porter randy h katz scott shenker and ion stoica x trace a pervasive network tracing framework in in proceedings of nsdi 07 2007 ref which can be tedious and time consuming across several stages of a data flow furthermore these approaches work only when the data scientist can discover bad outputs to debug analytics without known bad outputs the data scientist need to analyze the data flow for suspicious behavior in general however often a user may not know the expected normal behavior and cannot specify predicates this section describes a debugging methodology for retrospectively analyzing lineage to identify faulty actors in a multi stage data flow we believe that sudden changes in an actor s behavior such as its average selectivity processing rate or output size is characteristic of an anomaly lineage can reflect such changes in actor behavior over time and across different actor instances thus mining lineage to identify such changes can be useful in debugging faulty actors in a data flow file tracing anomalous actors png thumb center 400px tracing anomalous actors the second problem i e the existence of outliers can also be identified by running the data flow step wise and looking at the transformed outputs the data scientist finds a subset of outputs that are not in accordance to the rest of outputs the inputs which are causing these bad outputs are the outliers in the data this problem can be solved by removing the set of outliers from the data and replaying the entire data flow it can also be solved by modifying the machine learning algorithm by adding removing or moving actors in the data flow the changes in the data flow are successful if the replayed data flow does not produce bad outputs file tracing outliers in the data png thumb center 400px tracing outliers in the data challenges even though use data lineage is a novel way of debugging of big data pipelines the process is not simple the challenges are scalability of lineage store fault tolerance of the lineage store accurate capture of lineage for black box operators and many others these challenges must be considered carefully and trade offs between them need to be evaluated to make a realistic design for data lineage capture scalability disc systems are primarily batch processing systems designed for high throughput they execute several jobs per analytics with several tasks per job the overall number of operators executing at any time in a cluster can range from hundreds to thousands depending on the cluster size lineage capture for these systems must be able scale to both large volumes of data and numerous operators to avoid being a bottleneck for the disc analytics fault tolerance lineage capture systems must also be fault tolerant to avoid rerunning data flows to capture lineage at the same time they must also accommodate failures in the disc system to do so they must be able to identify a failed disc task and avoid storing duplicate copies of lineage between the partial lineage generated by the failed task and duplicate lineage produced by the restarted task a lineage system should also be able to gracefully handle multiple instances of local lineage systems going down this can achieved by storing replicas of lineage associations in multiple machines the replica can act like a backup in the event of the real copy being lost black box operators lineage systems for disc dataflows must be able to capture accurate lineage across black box operators to enable fine grain debugging current approaches to this include prober which seeks to find the minimal set of inputs that can produce a specified output for a black box operator by replaying the data flow several times to deduce the minimal set ref anish das sarma alpa jain and philip bohannon prober ad hoc debugging of extraction and integration pipelines technical report yahoo april 2010 ref and dynamic slicing as used by zhang et al ref mingwu zhang xiangyu zhang xiang zhang and sunil prabhakar tracing lineage beyond relational operators in proc conference on very large data bases vldb september 2007 ref to capture lineage for nosql operators through binary rewriting to compute dynamic slices although producing highly accurate lineage such techniques can incur significant time overheads for capture or tracing and it may be preferable to instead trade some accuracy for better performance thus there is a need for a lineage collection system for disc dataflows that can capture lineage from arbitrary operators with reasonable accuracy and without significant overheads in capture or tracing efficient tracing tracing is essential for debugging during which a user can issue multiple tracing queries thus it is important that tracing has fast turnaround times ikeda et al ref name robertikedahyunjung can perform efficient backward tracing queries for mapreduce dataflows but are not generic to different disc systems and do not perform efficient forward queries lipstick ref yael amsterdamer susan b davidson daniel deutch tova milo and julia stoyanovich putting lipstick on a pig enabling database style workflow provenance in proc of vldb august 2011 ref a lineage system for pig ref christopher olston benjamin reed utkarsh srivastava ravi kumar and andrew tomkins pig latin a not so foreign language for data processing in proc of acm sigmod vancouver canada june 2008 ref while able to perform both backward and forward tracing is specific to pig and sql operators and can only perform coarse grain tracing for black box operators thus there is a need for a lineage system that enables efficient forward and backward tracing for generic disc systems and dataflows with black box operators sophisticated replay replaying only specific inputs or portions of a data flow is crucial for efficient debugging and simulating what if scenarios ikeda et al present a methodology for lineage based refresh which selectively replays updated inputs to recompute affected outputs ref robert ikeda semih salihoglu and jennifer widom provenance based refresh in data oriented workflows in proceedings of the 20th acm international conference on information and knowledge management cikm 11 pages 1659 1668 new york ny usa 2011 acm ref this is useful during debugging for re computing outputs when a bad input has been fixed however sometimes a user may want to remove the bad input and replay the lineage of outputs previously affected by the error to produce error free outputs we call this exclusive replay another use of replay in debugging involves replaying bad inputs for step wise debugging called selective replay current approaches to using lineage in disc systems do not address these thus there is a need for a lineage system that can perform both exclusive and selective replays to address different debugging needs anomaly detection one of the primary debugging concerns in disc systems is identifying faulty operators in long dataflows with several hundreds of operators or tasks manual inspection can be tedious and prohibitive even if lineage is used to narrow the subset of operators to examine the lineage of a single output can still span several operators there is a need for an inexpensive automated debugging system which can substantially narrow the set of potentially faulty operators with reasonable accuracy to minimize the amount of manual examination required see also please do not list specific implementations here provenance big data topological sorting debugging nosql scalability directed acyclic graph references reflist 33em category data management category distributed computing problems category big data'
b'infobox software title scidb developer paradigm4 publisher genre database management system license affero general public license agpl v3 ref http www scidb org forum viewtopic php f 16 t 364 download licensing ref first last coauthors released 2008 website url http www paradigm4 com distinguish2 http scidb sourceforge net scidb chess database scidb ref cite web url http www scidb org title scidb website ref is an array database designed for multidimensional data management and analytics common to scientific geospatial financial and industrial applications it is developed by paradigm4 co founded by michael stonebraker history and characteristics michael stonebraker co created scidb where he claims arrays are 100 or so times faster than a rdbms on this class of problem ref cite web url http www theregister co uk 2010 09 13 michael stonebraker interview title stonebraker interview ref it is swapping rows and columns for mathematical arrays that put fewer restrictions on the data and can work in any number of dimensions unlike the conventionally widely used relational database management system model in which each relation database relation supports only one dimension of records according to a strata conference presentation on scidb ref cite web url http strataconf com stratany2011 public schedule detail 21376 title big data and big analytics scidb is not hadoop ref it natively supports an array data model for efficient storage and manipulation of larger than memory multi dimensional arrays data versioning and provenance to allow tracking results back to original supporting data what if modeling back testing and re analysis massive scale math on the arrays for linear algebra and analytics uncertainty can be modeled by associating error bars with data efficient storage see also please do not list specific implementations here comparison of object database management systems comparison of structured storage software references reflist 2 external links http www theregister co uk 2010 09 13 michael stonebraker interview michael stonebraker interview http itknowledgeexchange techtarget com soa talk stonebraker sees high programming overhead for nosql stonebraker sees high programming overhead for nosql category data management category distributed data stores category document oriented databases category distributed computing architecture category free database management systems category structured storage category nosql category software using the gnu agpl license'
b'most database management systems are organized around a single database model data model that determines how data can be organized stored and manipulated in contrast a multi model database is designed to support multiple data models against a single integrated backend ref name neither http blogs the451group com information management 2013 02 08 neither fish nor fowl the 451 group neither fish nor fowl the rise of multi model databases ref document graph relational and key value models are examples of data models that may be supported by a multi model database background the relational model relational data model became popular after its publication by edgar f codd in 1970 due to increasing requirements for scalability horizontal and vertical scaling horizontal scalability and fault tolerance nosql databases became prominent after 2009 nosql databases use a variety of data models with document oriented database document graph database graph and key value models being popular ref name rise http www infoworld com article 2861579 database the rise of the multimodel database html infoworld the rise of the multi model database ref a multi model database is a database that can store index and query data in more than one model for some time databases have primarily supported only one model such as relational database document oriented database graph database or triplestore a database that combines many of these is multi model for some time it was all but forgotten or considered irrelevant that there were any other database models besides relational the relational model and notion of third normal form were the de facto standard for all data storage however prior to the dominance of relational data modeling from about 1980 to 2005 the hierarchical database model was commonly used and since 2000 or 2010 many nosql models that are non relational including documents triples key value stores and graphs are popular arguably geospatial data temporal data and text data are also separate models though indexed queryable text data is generally termed a search engine rather than a database the first time the word multi model has been associated to the databases was on may 30 2012 in cologne germany during the luca garulli s key note nosql adoption what s the next step ref cite journal date 2012 06 01 title multi model storage 1 2 one product url http www slideshare net lvca no sql matters2012keynote 47 multimodel storage 12 one product ref ref cite web url https 2012 nosql matters org cgn index html p 1202 html luca garulli keynote title nosql matters conference 2012 nosql matters cgn 2012 website 2012 nosql matters org access date 2017 01 12 ref luca garulli envisioned the evolution of the 1st generation nosql products into new products with more features able to be used by multiple use cases a multi model database is most directly a response to the polyglot persistence approach of knitting together multiple database products each handing a different model to achieve a multi model capability as described by martin fowler ref name polyglot http martinfowler com bliki polyglotpersistence html polyglot persistence ref this strategy has two major disadvantages it leads to a significant increase in operational complexity and there is no support for maintaining data consistency across the separate data stores so multi model databases have begun to fill in this gap some stories of overcomplicated systems from un necessary frankenbeast database integrations are found on the web ref name frankenbeast http www marklogic com blog polyglot persistence done right marklogic avoiding the frankenbeast ref ref name boring http mcfunley com choose boring technology mckinley choose boring technology ref multi model databases are intended to offer the data modeling advantages of polyglot persistence ref name polyglot without its disadvantages operational complexity in particular is reduced through the use of a single data store ref name rise databases multi model databases include in alphabetic order arangodb document json graph key value couchbase relational sql document cratedb relational sql document lucene marklogic document xml and json graph rdf with owl rdfs text geospatial binary sql orientdb document json graph key value text geospatial binary sql reactive note that the level of support for the various models varies widely including the ability to query across models fully index the internal structure of a model transactional support and optimization or query planning across models the first multi model database was orientdb created in 2010 as an answer to the fragmented nosql environment with the goal of providing one product to replace multiple nosql databases architecture the main difference between the available multi model databases is related to their architectures multi model databases can support different models either within the engine or via different layers on top of the engine some products may provide an engine which supports documents and graphs while others provide layers on top of a key key store ref http blog foundationdb com 7 things that make google f1 and the foundationdb sql layer so strikingly similar layer ref with a layered architecture each data model is provided via its own component based software engineering component user defined data models in addition to offering multiple data models in a single data store some databases allow developers to easily define custom data models this capability is enabled by acid transactions with high performance and scalability in order for a custom data model to support concurrent updates the database must be able to synchronize updates across multiple keys acid transactions if they are sufficiently performant allow such synchronization ref name multiple http www odbms org wp content uploads 2014 04 multiple data models pdf odbms polyglot persistence or multiple data models ref json documents graphs and relational tables can all be implemented in a manner that inherits the horizontal scalability and fault tolerance of the underlying data store see also please do not list specific implementations here comparison of multi model databases acid nosql comparison of structured storage software database transaction distributed database distributed transaction document oriented database graph database relational model references reflist 2 external links http www orientechnologies com docs last orientdb wiki tutorial document and graph model html orientdb document and graph model https www arangodb com key features arangodb key features https foundationdb com try multi model foundationdb multi model architecture http martinfowler com bliki polyglotpersistence html polyglot persistence http blogs the451group com information management 2013 02 08 neither fish nor fowl the 451 group neither fish nor fowl the rise of multi model databases http www odbms org blog 2013 10 on multi model databases interview with martin schonert and frank celler odbms on multi model databases interview with martin sch\xc3\xb6nert and frank celler http www odbms org wp content uploads 2014 04 multiple data models pdf odbms polyglot persistence or multiple data models http www infoworld com article 2861579 database the rise of the multimodel database html infoworld the rise of the multi model database https crate io docs reference storage consistency html crate io storage and consistency http www marklogic com blog tag multi model database marklogic on multi model databases defaultsort multi model database category applications of distributed computing category databases category data management category distributed computing architecture category distributed data stores category nosql category structured storage category transaction processing'
b'infobox organization name open compute project image opencompute logo jpg mcaption formation 2011 type industry trade group purpose sharing designs of data center products headquarters membership website url opencompute org remarks file open compute server front jpg thumb open compute v2 server file open compute 1u drive tray bent jpg thumb open compute v2 drive tray br 2nd lower tray extended the open compute project ocp is an organization that shares designs of data center products among companies including facebook intel nokia google apple inc apple microsoft seagate technology dell rackspace ericsson cisco juniper networks goldman sachs fidelity investments fidelity lenovo and bank of america ref cite web url http www wired com 2015 03 facebook got even apple back open source hardware title how facebook changed the basic tech that runs the internet date 11 apr 2015 ref ref cite web url http www opencompute org about ocp incubation committee title incubation committee website open compute access date 2016 08 19 ref the open compute project s mission is to design and enable the delivery of the most efficient server storage and data center hardware designs for scalable computing we believe that openly sharing ideas specifications and other intellectual property is the key to maximizing innovation and reducing operational complexity in the scalable computing space ref cite web url http www opencompute org about mission and principles title mission and principles website open compute accessdate 2016 05 13 ref br all facebook data centers are 100 ocp prineville data center forest city data center altoona data center lule\xc3\xa5 data center sweden facebook data centers under construction fort worth data center clonee data center ireland ref cite web url http uk businessinsider com facebook eu data center open compute project 2016 1 first matt last weinberger title facebook s newest data center is going to make some big tech companies very nervous website open compute date january 25 2016 accessdate 2016 05 16 ref details the initiative was announced in april 2011 by jonathan heiliger ref cite news last1 heiliger first1 jonathan title why i started the open compute project url http www vertexventures com 2015 06 why i started the open compute project accessdate 18 june 2015 date 2015 06 15 ref at facebook to openly share designs of data center products ref cite web url http www datacenterknowledge com archives 2011 04 14 will open compute alter the data center market title will open compute alter the data center market date april 14 2011 first rich last miller work data center knowledge accessdate july 9 2013 ref the effort came out of a redesign of facebook s data center in prineville oregon ref cite web url http www facebook com notes facebook engineering building efficient data centers with the open compute project 10150144039563920 title building efficient data centers with the open compute project first jonathan last heiliger date april 7 2011 work facebook engineering s notes accessdate july 9 2013 ref after two years with regards to a more module server design it was admitted that the new design is still a long way from live data centers ref cite news title facebook shatters the computer server into tiny pieces date january 16 2013 first cade last metz work wired url http www wired com wiredenterprise 2013 01 facebook server pieces accessdate july 9 2013 ref however some aspects published were used in the prineville center to improve the energy efficiency as measured by the power usage effectiveness index defined by the green grid ref name stanford cite web title facebook s open compute project work stanford ee computer systems colloquium date february 15 2012 url http www stanford edu class ee380 abstracts 120215 html first amir last michael publisher stanford university http ee380 stanford edu cgi bin videologger php target 120215 ee380 300 asx video archive ref the open compute project foundation is a 501 c 6 non profit incorporated in the state of delaware corey bell serves as the foundation s ceo currently there are 7 members who serve on board of directors which is made up of two individual members and five organizational members jason taylor facebook is the foundation s president and chairman frank frankovsky formerly of facebook and past president and chairman and andy bechtolsheim are the two individual members in addition to jason taylor who represents facebook other organizations on the open compute board of directors include intel jason waxman goldman sachs don duet rackspace mark roenick and microsoft bill laing ref cite web title organization and board url http www opencompute org about organization and board website open compute accessdate 2015 09 12 ref on march 11 2015 apple inc apple cisco and juniper networks joined the project ref cite web title open compute apple cisco join while hp expands first charles last babcock date march 11 2015 url http www informationweek com cloud infrastructure as a service open compute apple cisco join while hp expands d d id 1319421 accessdate march 11 2015 ref on november 16 2015 nokia joined the project ref cite web title nokia networks joins open compute project to advance its airframe data center solution date november 16 2015 url http company nokia com en news press releases 2015 11 16 nokia networks joins open compute project to advance its airframe data center solution ref on february 23 2016 lenovo joined the project ref cite web title lenovo joins open compute project date february 23 2016 url http news lenovo com blog lenovo joins open compute projects htm ref on march 9 2016 google joined the project ref cite web title google joins the open compute project date march 9 2016 url http techcrunch com 2016 03 09 google joins the open compute project ref components of the open compute project include server compute nodes included one for intel processors and one for advanced micro devices amd processors in 2013 calxeda contributed a design with arm architecture processors ref cite web title arm server motherboard design for open vault chassis hardware v0 3 mb draco hesperides 0 3 first tom last schnell date january 16 2013 url http www opencompute org wp wp content uploads 2013 01 open compute project arm server specification v0 3 pdf accessdate july 9 2013 ref br several generations of server designs have been deployed so far being freedom intel spitfire amd windmill intel e5 2600 watermark amd winterfell intel e5 2600 v2 and leopard intel e5 2600 v3 ref cite web title guide to facebook s open source data center hardware author data center knowledge date april 28 2016 url http www datacenterknowledge com archives 2016 04 28 guide to facebooks open source data center hardware accessdate may 13 2016 ref ref cite web title facebook rolls out new web and database server designs first the last register date january 17 2013 url http www theregister co uk 2013 01 17 open compute facebook servers accessdate may 13 2016 ref open vault storage building blocks offer high disk densities with 30 drives in a 2u open rack chassis designed for easy disk drive replacement the 3 5 inch disks are stored in two drawers five across and three deep in each drawer with connections via serial attached scsi ref cite web title open vault storage hardware v0 7 or draco bueana 0 7 author mike yan and jon ehlen date january 16 2013 url http www opencompute org wp wp content uploads 2013 01 open compute project open vault storage specification v0 7 pdf accessdate july 9 2013 ref this storage is also called knox there is also a cold storage variant where the disks power down if not used to save energy consumption ref cite web title under the hood facebook s cold storage system date may 4 2015 url https code facebook com posts 1433093613662262 under the hood facebook s cold storage system accessdate may 13 2016 ref another design concept was contributed by hyve solutions a division of synnex in 2012 ref cite web title hyve solutions contributes storage design concept to ocp community work news release date january 17 2013 url http ir synnex com releasedetail cfm releaseid 733922 accessdate july 9 2013 ref ref cite web title torpedo design concept storage server for open rack hardware v0 3 st draco chimera 0 3 first conor last malone date january 15 2012 url http www opencompute org wp wp content uploads 2013 01 open compute project storage server for open rack specification v0 3 pdf accessdate july 9 2013 ref br at the ocp summit 2016 facebook together with taiwanese odm wistron s spin off wiwynn introduced lightning a flexible nvme jbof just a bunch of flash based on the existing open vault knox design ref cite web title introducing lightning a flexible nvme jbof first chris last petersen date march 9 2016 url https code facebook com posts 989638804458007 introducing lightning a flexible nvme jbof accessdate may 13 2016 ref ref cite web title wiwynn showcases all flash storage product with leading edge nvme technology date march 9 2016 url http www wiwynn com english company newsinfo 23 accessdate may 13 2016 ref mechanical mounting system open racks have the same outside width 600 nbsp mm and depth as standard 19 inch rack s but are designed to mount wider chassis with a 537 nbsp mm width about 21 inches this allows more equipment to fit in the same volume and improves air flow compute chassis sizes are defined in multiples of an openu which is 48 nbsp mm slightly larger than the typical rack unit data center designs for energy efficiency include 277 vac power distribution that eliminates one transformer stage in typical data centers a single voltage 12 5 vdc power supply designed to work with 277 vac input and 48 vdc battery backup ref name stanford on may 8 2013 an effort to define an open network switch was announced ref cite web title up next for the open compute project the network date may 8 2013 author jay hauser for frank frankovsky work open compute blog url http www opencompute org blog up next for the open compute project the network accessdate june 20 2014 ref the plan was to allow facebook to load its own operating system software onto the switch press reports predicted that more expensive and higher performance switches would continue to be popular while less expensive products treated more like a commodity using the buzzword top of rack might adopt the proposal ref cite news title can open compute change network switching first david last chernicoff work zdnet date may 9 2013 url http www zdnet com can open compute change network switching 7000015141 accessdate july 9 2013 ref br a similar project for a custom switch for the google platform had been rumored and evolved to use the openflow protocol ref cite news title facebook rattles networking world with open source gear date may 8 2013 first cade last metz work wired url http www wired com wiredenterprise 2013 05 facebook networking accessdate july 9 2013 ref ref cite news title going with the flow google s secret switch to the next wave of networking date april 17 2012 first steven last levy work wired url http www wired com wiredenterprise 2012 04 going with the flow google accessdate july 9 2013 ref br the first switch open sourced by facebook was designed together with taiwanese odm accton using broadcom trident ii chip and is called wedge the linux os that it runs is called fboss ref cite web url https code facebook com posts 681382905244727 introducing wedge and fboss the next steps toward a disaggregated network title introducing wedge and fboss the next steps toward a disaggregated network website meet the engineers who code facebook date june 18 2014 accessdate 2016 05 13 ref ref cite web url https code facebook com posts 843620439027582 facebook open switching system fboss and wedge in the open title facebook open switching system fboss and wedge in the open website meet the engineers who code facebook date march 10 2015 accessdate 2016 05 13 ref later switch contributions include 6 pack and wedge 100 based on broadcom tomahawk chips ref cite web url https code facebook com posts 203733993317833 opening designs for 6 pack and wedge 100 title opening designs for 6 pack and wedge 100 website meet the engineers who code facebook date march 9 2016 accessdate 2016 05 13 ref similar switch hardware designs have been contributed by edge core networks corporation accton spin off mellanox technologies interface masters technologies agema systems ref cite web url http www opencompute org wiki networking specsanddesigns title accepted or shared hardware specifications website open compute accessdate 2016 05 13 ref capable of running onie compatible operating systems such as cumulus linux big switch or pica8 ref cite web url http www opencompute org wiki networking onie nos status title current network operating system nos list website open compute accessdate 2016 05 13 ref providers the promoted vendors include ref http www opencompute org about open compute project solution providers open compute project solution providers ref amax information technologies circle b itochu techno solutions ctc hewlett packard enterprise hyperscale it synnex hyve solutions penguin computing nokia quanta computer racklive stack velocity wiwynn see also novena computing platform open source computing hardware openpower foundation telecom infra project facebook sister project focusing on optical networking optical broadband networks and open cellular network cellular networks references reflist 33em external links commons category data centers official website http opencompute org https www facebook com prinevilledatacenter prineville data center https www facebook com forestcitydatacenter forest city data center https www facebook com altoonadatacenter altoona data center https www facebook com luleadatacenter lule\xc3\xa5 data center sweden https www facebook com fortworthdatacenter fort worth data center https www facebook com cloneedatacenter clonee data center ireland videos youtube 2htfzumdaow hc23 t2 the open compute project hot chips 23 2011 2 5 hour tutorial youtube qttf9pdqxpc facebook open compute server facebook v1 open compute server youtube cknzwqhds60 facebook v2 windmill server youtube gbzqe3jo4hc hyve adapting facebook s servers for your data center open compute starts at 5 40 facebook navbox state collapsed category open source hardware category facebook category 2011 software category data centers category data management category servers computing category distributed data storage category distributed data storage systems category applications of distributed computing category cloud storage category computer networking category science and technology in the san francisco bay area'
b'about the mathematical concept the musical term tuplet redir octuple the type of rowing boat octuple scull redir duodecuple the term in music twelve tone technique a tuple is a finite ordered list of element mathematics elements in mathematics an math n tuple is a sequence or ordered list of math n elements where math n is a non negative integer there is only one 0 tuple an empty sequence an math n tuple is recursive definition defined inductively using the construction of an ordered pair tuples are usually written by listing the elements within parentheses math text math and separated by commas for example math 2 7 4 1 7 math denotes a 5 tuple sometimes other symbols are used to surround the elements such as square brackets or angle brackets braces are only used in defining arrays in some programming languages such as java programming language java but not in mathematical expressions as they are the standard notation for set mathematics sets tuples are often used to describe other mathematical objects such as vector mathematics and physics vectors in computer science tuples are directly implemented as product type s in most functional programming functional programming languages citation needed date january 2016 more commonly they are implemented as record computer science record types where the components are labeled instead of being identified by position alone citation needed date january 2016 this approach is also used in relational algebra tuples are also used in relation to programming the semantic web with the resource description framework rdf tuples are also used in linguistics ref cite web url http www oxfordreference com view 10 1093 acref 9780199202720 001 0001 acref 9780199202720 e 2276 title n\xe2\x80\x90tuple oxford reference work oxfordreference com accessdate 1 may 2015 ref and philosophy ref cite web url http www oxfordreference com view 10 1093 acref 9780199541430 001 0001 acref 9780199541430 e 2262 title ordered n tuple oxford reference work oxfordreference com accessdate 1 may 2015 ref etymology the term originated as an abstraction of the sequence single double triple quadruple quintuple sextuple septuple octuple math n \xe2\x80\x91tuple where the prefixes are taken from the latin names of the numerals the unique 0\xe2\x80\x91tuple is called the null tuple a 1\xe2\x80\x91tuple is called a singleton a 2\xe2\x80\x91tuple is called an ordered pair and a 3\xe2\x80\x91tuple is a triple or triplet math n can be any nonnegative integer for example a complex number can be represented as a 2\xe2\x80\x91tuple a quaternion can be represented as a 4\xe2\x80\x91tuple an octonion can be represented as an 8\xe2\x80\x91tuple and a sedenion can be represented as a 16\xe2\x80\x91tuple although these uses treat \xe2\x80\x91tuple as the suffix the original suffix was \xe2\x80\x91ple as in triple three fold or decuple ten\xe2\x80\x91fold this originates from medieval latin plus meaning more related to greek language greek \xe2\x80\x91\xcf\x80\xce\xbb\xce\xbf\xe1\xbf\xa6\xcf\x82 which replaced the classical and late antique \xe2\x80\x91plex meaning folded as in duplex ref oed s v triple quadruple quintuple decuple ref names for tuples of specific lengths class wikitable tuple length math n math name alternative names align right 0 empty tuple unit empty sequence align right 1 single singleton mathematics singleton monuple align right 2 double couple ordered pair dual twin product align right 3 triple treble triplet triad align right 4 quadruple quad align right 5 quintuple pentuple align right 6 sextuple hextuple align right 7 septuple heptuple align right 8 octuple align right 9 nonuple align right 10 decuple align right 11 undecuple hendecuple align right 12 duodecuple align right 13 tredecuple align right 14 quattuordecuple align right 15 quindecuple align right 16 sexdecuple align right 17 septendecuple align right 18 octodecuple align right 19 novemdecuple align right 20 vigintuple align right 21 unvigintuple align right 22 duovigintuple align right 23 trevigintuple align right 24 quattuorvigintuple align right 25 quinvigintuple align right 26 sexvigintuple align right 27 septenvigintuple align right 28 octovigintuple align right 29 novemvigintuple align right 30 trigintuple align right 31 untrigintuple align right 40 quadragintuple align right 50 quinquagintuple align right 60 sexagintuple align right 70 septuagintuple align right 80 octogintuple align right 90 nongentuple align right 100 centuple align right 1 000 milluple properties the general rule for the identity of two math n tuples is math a 1 a 2 ldots a n b 1 b 2 ldots b n math if and only if math a 1 b 1 text a 2 b 2 text ldots text a n b n math thus a tuple has properties that distinguish it from a set mathematics set a tuple may contain multiple instances of the same element so break tuple math 1 2 2 3 neq 1 2 3 math but set math 1 2 2 3 1 2 3 math tuple elements are ordered tuple math 1 2 3 neq 3 2 1 math but set math 1 2 3 3 2 1 math a tuple has a finite number of elements while a set or a multiset may have an infinite number of elements definitions there are several definitions of tuples that give them the properties described in the previous section tuples as functions if we are dealing with sets an math n tuple can be regarded as a function mathematics definition function math f whose domain is the tuple s implicit set of element indices math x and whose codomain math y is the tuple s set of elements formally math a 1 a 2 dots a n equiv x y f math where math begin align x 1 2 dots n y a 1 a 2 ldots a n f 1 a 1 2 a 2 ldots n a n end align math in slightly less formal notation this says math a 1 a 2 dots a n f 1 f 2 dots f n math tuples as nested ordered pairs another way of modeling tuples in set theory is as nested ordered pair s this approach assumes that the notion of ordered pair has already been defined thus a 2 tuple the 0 tuple i e the empty tuple is represented by the empty set math emptyset math an math n tuple with math n 0 can be defined as an ordered pair of its first entry and an math n \xe2\x88\x92 1 tuple which contains the remaining entries when math n 1 math a 1 a 2 a 3 ldots a n a 1 a 2 a 3 ldots a n math this definition can be applied recursively to the math n \xe2\x88\x92 1 tuple math a 1 a 2 a 3 ldots a n a 1 a 2 a 3 ldots a n emptyset ldots math thus for example math begin align 1 2 3 1 2 3 emptyset 1 2 3 4 1 2 3 4 emptyset end align math a variant of this definition starts peeling off elements from the other end the 0 tuple is the empty set math emptyset math for math n 0 math a 1 a 2 a 3 ldots a n a 1 a 2 a 3 ldots a n 1 a n math this definition can be applied recursively math a 1 a 2 a 3 ldots a n ldots emptyset a 1 a 2 a 3 ldots a n math thus for example math begin align 1 2 3 emptyset 1 2 3 1 2 3 4 emptyset 1 2 3 4 end align math tuples as nested sets using ordered pair kuratowski definition kuratowski s representation for an ordered pair the second definition above can be reformulated in terms of pure set theory the 0 tuple i e the empty tuple is represented by the empty set math emptyset math let math x math be an math n tuple math a 1 a 2 ldots a n math and let math x rightarrow b equiv a 1 a 2 ldots a n b math then math x rightarrow b equiv x x b math the right arrow math rightarrow math could be read as adjoined with in this formulation math begin array lclcl emptyset 1 rightarrow 1 1 emptyset emptyset 1 1 2 1 rightarrow 2 1 1 2 emptyset emptyset 1 emptyset emptyset 1 2 1 2 3 1 2 rightarrow 3 1 2 1 2 3 emptyset emptyset 1 emptyset emptyset 1 2 emptyset emptyset 1 emptyset emptyset 1 2 3 end array math anchor n tuple math n tuples of math m sets in discrete mathematics especially combinatorics and finite probability theory math n tuples arise in the context of various counting problems and are treated more informally as ordered lists of length math n ref harvnb d angelo west 2000 p 9 ref math n tuples whose entries come from a set of math m elements are also called arrangements with repetition permutations of a multiset and in some non english literature variation disambiguation mathematics variations with repetition the number of math n tuples of an math m set is math m sup n sup this follows from the combinatorial rule of product ref harvnb d angelo west 2000 p 101 ref if math s is a finite set of cardinality math m this number is the cardinality of the math n fold cartesian product cartesian power cartesian power math s \xc3\x97 s \xc3\x97 s tuples are elements of this product set type theory main product type in type theory commonly used in programming language s a tuple has a product type this fixes not only the length but also the underlying types of each component formally math x 1 x 2 ldots x n mathsf t 1 times mathsf t 2 times ldots times mathsf t n math and the projection mathematics projection s are term constructors math pi 1 x mathsf t 1 pi 2 x mathsf t 2 ldots pi n x mathsf t n math the tuple with labeled elements used in the relational model relational model has a record computer science record type both of these types can be defined as simple extensions of the simply typed lambda calculus ref name pierce2002 cite book last pierce first benjamin title types and programming languages publisher mit press year 2002 isbn 0 262 16209 1 pages 126 132 ref the notion of a tuple in type theory and that in set theory are related in the following way if we consider the natural model theory model of a type theory and use the scott brackets to indicate the semantic interpretation do not link that article needs to be a dab first then the model consists of some sets math s 1 s 2 ldots s n math note the use of italics here that distinguishes sets from types such that math mathsf t 1 s 1 mathsf t 2 s 2 ldots mathsf t n s n math and the interpretation of the basic terms is math x 1 in mathsf t 1 x 2 in mathsf t 2 ldots x n in mathsf t n math the math n tuple of type theory has the natural interpretation as an math n tuple of set theory ref steve awodey http www andrew cmu edu user awodey preprints stcsfinal pdf from sets to types to categories to sets 2009 preprint ref math x 1 x 2 ldots x n x 1 x 2 ldots x n math the unit type has as semantic interpretation the 0 tuple see also wiktionary tuple arity exponential object formal language multidimensional expressions mdx data types olap multidimensional expressions prime k tuple relation mathematics tuplespace notes reflist references refbegin citation first1 john p last1 d angelo first2 douglas b last2 west title mathematical thinking problem solving and proofs year 2000 edition 2nd publisher prentice hall isbn 978 0 13 014412 6 keith devlin the joy of sets springer verlag 2nd ed 1993 isbn 0 387 94094 4 pp nbsp 7 8 abraham adolf fraenkel yehoshua bar hillel azriel l\xc3\xa9vy https books google com books q foundations of set theory btng search books foundations of set theory elsevier studies in logic vol 67 edition 2 revised 1973 isbn 0 7204 2270 1 p nbsp 33 gaisi takeuti w m zaring introduction to axiomatic set theory springer graduate texts in mathematics gtm 1 1971 isbn 978 0 387 90024 7 p nbsp 14 george j tourlakis https books google com books as isbn 9780521753746 lecture notes in logic and set theory volume 2 set theory cambridge university press 2003 isbn 978 0 521 75374 6 pp nbsp 182 193 refend set theory interwikies categories authority control category data management category mathematical notation category sequences and series category basic concepts in set theory category type theory ar \xd8\xb2\xd9\x88\xd8\xac \xd9\x85\xd8\xb1\xd8\xaa\xd8\xa8'
b'a media aggregation platform or media aggregation portal map is an over the top service for distributing web based streaming media content from multiple sources to a large audience maps consist of networks of sources who host their own content which viewers can choose and access directly from a larger variety of content to choose from than a single source can offer ref cite web url https medium com bmobley over the top of ott need a map 9931096775c2 title over the top of ott\xe2\x80\xa6 need a map publisher medium com accessdate 23 march 2015 ref ref cite book title building next generation converged networks theory and practice publisher crc press author1 al sakib khan pathan author2 muhammad mostafa monowar author3 zubair md fadlullah year 2013 isbn 1466507616 ref the service is used by content providers looking to extend the reach of their content unlike multichannel video programming distributor mvpd or multiple system operator mso maps rely on the internet rather than cables or satellite as more network television channels have moved online in the early 21st century ref cite web url http www exchange4media com 59377 ott platforms to be key growth area for tv broadcasters html title ott platforms to be key growth area for tv broadcasters accessdate 23 march 2015 ref joining web native channels like netflix maps aggregates content the way that msos and mvpds have used cable and to a lesser extent satellite and iptv infrastructure there are companies that offer a similar service for free including yidio and tv com while others charge a subscription fee like as freecast inc s rabbit tv plus ref cite web url http rabbittvplus com title freecast inc rabbit tv plus accessdate 23 march 2015 ref when compared with msos and mvpds mpas network have much lower cost due to lack of physical infrastructure the majority of revenues from their services is retained by the content creators and revenues are from advertisements pay per view and subscription based content offerings instead of by licensing and reselling content maps service consumers directly with the content source and they purchase content directly from its source without the markup added by a middleman ref cite web url https www ncta com industry data title ncta industry data accessdate 23 march 2015 ref see also multichannel video programming distributor multichannel video programming distributor mvpd multiple system operator multiple system operator mso internet protocol television internet protocol television iptv over the top content over the top ott over the air television over the air ota video on demand video on demand vod broadcast networks broadcast networks internet television streaming media pay tv references reflist category data management'
b'more footnotes date march 2015 data recovery hardware was developed because data recovery software lacks the ability to deal with all lost or corrupted data files often the failures such as media files with bad sectors firmware failures pcb printed circuit board failures hard drive head failures etc cannot be fixed bad sectors the two types of bad sectors are physical and logical bad sectors or hard and soft bad sectors ref cite web url http www howtogeek com 173463 bad sectors explained why hard drives get bad sectors and what you can do about it title bad sectors explained why hard drives get bad sectors and what you can do about it date october 9 2013 accessdate march 26 2015 author chris hoffman website how to geek llc ref when a disk has physical bad sectors software cannot effectively offer soft reset hard reset power reset error handling read algorithm auto exchange nor skip sectors if a disk with bad physical sectors is connected to a personal computer pc the condition would potentially not be detected soft bad sectors can potentially be fixed by either data recovery software or hardware depending on the damaged condition some amount of bad sectors can be skipped using software while a severely corrupted disk with a large area of bad sectors may potentially only be repaired ref cite web url https www winxdvd com resource repair mp4 file free htm title how to repair corrupted mp4 video file date july 3 2015 accessdate august 24 2016 author estrella garcia website winxdvd ref bad sectors are areas on the hard drive that cannot be read even new hardrives sometimes contain bad sectors since manufacturers intensely compete to cram more space into disks systems operate close to the limit of that generation of technology ref cite web url https www grc com sr faq htm title grc spinrite 6 0 faq frequently asked questions website www grc com access date 2016 10 20 ref dead pcb when the drive has dead pcb printed circuit board users need to swap in a new pcb put one donor integrated circuit chip and write by chip reader with matching rom read only memory content get the dead drive spinning when the drive has physical head damage users need to open the drive in cleanroom environment and find donor heads or other donor components to swap data recovery hardware types disk image file extraction hardware firmware repair hardware rom chip reader head and platter swap tools see hard disk drive platter spindle release hardware other hardware see also portal computer security computing div col 20em data recovery firmware bad sector disk image printed circuit board cleanroom list of data recovery software comparison of file systems computer forensics continuous data protection data archaeology data loss error detection and correction file carving hidden file and hidden directory knowledge extraction undeletion div col end further reading tanenbaum a woodhull a s 1997 operating systems design and implementation 2nd ed new york prentice hall dmoz computers hardware storage data recovery data recovery cite web url https www grc com sr faq htm title grc nbsp nbsp spinrite 6 0 faq frequently asked questions publisher references reflist 30em defaultsort data recovery category data recovery category computer data category data management category transaction processing category hard disk software category backup recovery'
b'one source date january 2015 software intelligence si is software designed to analyze source code to better understand information technology information technology environments similarly to business intelligence business intelligence bi software intelligence is a set of software tools and techniques for the data mining mining of data into meaningful and useful information ref cite web last1 hassan first1 ahmed last2 xie first2 tao title software intelligence the future of mining software engineering data url http web engr illinois edu taoxie publications foser10 si pdf website http web engr illinois edu accessdate 19 january 2015 ref references reflist category data management category source code'
b'unreferenced date april 2015 a code commit code statement in sql ends a database transaction transaction within a relational database management system rdbms and makes all changes visible to other users the general format is to issue a code begin work sql begin work code statement one or more sql statements and then the code commit code statement alternatively a code rollback data management rollback code statement can be issued which undoes all the work performed since code begin work code was issued a code commit code statement will also release any existing savepoint s that may be in use in terms of transactions the opposite of commit is to discard the tentative changes of a transaction a rollback data management rollback see also commit data management atomic commit two phase commit protocol three phase commit protocol databases defaultsort commit data management category data management category sql category transaction processing comp sci stub'
b'expert subject computer science talk copyright issues and need for expert attention a data architect is a practitioner of data architecture an information technology discipline concerned with designing creating deploying and managing an organization s data architecture data architects define how the data will be stored consumed integrated and managed by different data entities and it systems as well as any applications using or processing that data in some way ref cite web title definition of data architect url http stage web techopedia com definition 29452 data architect website techopedia ref it is closely allied with business architecture and is considered to be one of the four domains of enterprise architecture role according to the data management body of knowledge ref cite web title data management body of knowledge url http www dama org content body knowledge publisher data management association ref the data architect provides a standard common business vocabulary expresses strategic data requirements outlines high level integrated designs to meet these requirements and aligns with enterprise strategy and related business architecture according to the open group architecture framework togaf a data architect is expected to set data architecture principles create models of data that enable the implementation of the intended business architecture create diagrams showing key data entities and create an inventory of the data needed to implement the architecture vision ref name togaf cite book title the open group architectural framework togaf 9 1 publisher the open group location chapter 10 data architecture url http pubs opengroup org architecture togaf9 doc arch chap10 html accessdate 1 march 2015 ref responsibilities organizes data at the macro level i e which subject areas are managed in which goldensources organizes data at the micro level data models for a new application provides a logical data model as a standard for the goldensource and for consuming applications to inherit provides a logical data model with elements and business rules needed for the creation of dq rules skills bob lambert director of data architecture at consulting firm captech describes the necessary skills of a data architect as follows ref cite web last1 lambert first1 bob title skills of a data architect url http www captechconsulting com blog bob lambert skills the data architect website captech ref foundation in systems development the data architect should understand the system development life cycle software project management approaches and requirements design and test techniques the data architect is asked to conceptualize and influence application and interface projects and therefore must understand what advice to give and where to plug in to steer toward desirable outcomes depth in data modeling and database design this is the core skill of the data architect and the most requested in data architect job descriptions the effective data architect is sound across all phases of data modeling from conceptualization to database optimization in his experience this skill extends to sql development and perhaps database administration breadth in established and emerging data technologies in addition to depth in established data management and reporting technologies the data architect is either experienced or conversant in emerging tools like columnar and nosql databases predictive analytics data visualization and unstructured data while not necessarily deep in all of these technologies the data architect hopefully is experienced in one or more and must understand them sufficiently to guide the organization in understanding and adopting them ability to conceive and portray the big data picture when the data architect initiates evaluates and influences projects he or she does so from the perspective of the entire organization the data architect maps the systems and interfaces used to manage data sets standards for data management analyzes current state and conceives desired future state and conceives projects needed to close the gap between current state and future goals ability to astutely operate in the organization well respected and influential able to emphasize methodology modeling and governance technologically and politically neutral articulate persuasive and a good salesperson and enthusiastic references reflist see also data architecture information architect enterprise architecture category data management category data modeling category data security'
b'orphan date july 2015 infobox company name maps system image file maps system logo png 250px industry public limited company plc founder thierry muller headquarters luxembourg area served france luxembourg germany switzerland belgium products product information management digital asset management master data management and business process management homepage http www maps system com maps s a is a software editor founded in 2011 by thierry muller which is headquartered in luxembourg its platform called maps system provides data management data management solutions for multichannel marketing history and funding in 1999 the founder had realized that certain challenges arose with several tools for customer relationship management customer relationship management public relations public relations and in particular marketing tools set in place complex data structures developed difficulties for organizations who lost focus of their dispersed data when wanting to operate and sell in an international and multichannel marketing multichannel environment the founder drafted his initial ideas on the topic of multichannel marketing and developed his first version of maps system under the agency prem1um s a in 2005 which in combination with the data management solution also provided various multimedia marketing activities in 2011 after being successful prem1um s a decided to enable the software maps system to operate independently under maps s a as a separate company and editor of the software the first financial supports were provided by malta ici a venture capital firm and the local partner chameleon invest a seed capital fund led by business angels who invested \xe2\x82\xac900 000 in a second investment round in 2014 led by newion investments a venture capital firm \xe2\x82\xac1 4 million were raised thus amounting to total assets of \xe2\x82\xac2 2 million products the services included in maps system range from the data centralization data governance to an optimized multichannel marketing the software today features more than 35 modules for master data management product information management digital asset management business process management including catalogue publishing features references official website http www maps system com maps s a newion invests in maps system http www newion investments com news newion invests in maps system 1 introducing maps system http www siliconluxembourg lu introducing maps system a centralized information management solution category data management software category data management category software companies category companies of luxembourg'
b'multiple issues underlinked date may 2016 orphan date may 2016 refimprove date april 2015 author name disambiguation is a type of record linkage that is applied to scholarly documents where the goal is to find all mentions of the same author and cluster them together authors of scholarly documents often share names which makes it hard to distinguish each author s work hence author name disambiguation aims to find all publications that belong to a given author and distinguish them from publications of other authors who share the same name there are multiple reasons that cause author names to be ambiguous among which individuals may publish under multiple names for variety of reasons including different spelling misspelling name change due to marriage or the use of middle names and initials ref cite journal authorlink smalheiser neil r and torvik vetle i title author name disambiguation journal annual review of information science and technology url http onlinelibrary wiley com doi 10 1002 aris 2009 1440430113 full accessdate 2015 04 20 doi 10 1002 aris 2009 1440430113 ref typical approach for author name disambiguation rely on information about the authors such as their affiliations email addresses year of publication co authors topic information to distinguish between authors these information can be used to learn a machine learning classifier that decides whether two mentions refer to the same author or not other approaches utilized heuristics to distinguish between authors references reflist category metadata category data management'
b'more footnotes date may 2015 data based decision making or data driven decision making refers to educator s ongoing process of collecting and analyzing different types of data including demographic student achievement test satisfaction process data to guide decisions towards improvement of educational process dddm becomes more important in education since federal and state test based accountability policies no child left behind act opens broader opportunities and incentives in using data by educational organizations by requiring schools and districts to analyze additional components of data as well as pressing them to increase student test scores information makes schools accountable for year by year improvement various student groups dddm helps to recognize the problem and who is affected by the problem therefore dddm can find a solution of the problem purpose the purpose of dddm is to help educators schools districts and states to use information they have to actionable knowledge to improve student outcomes dddm requires high quality data and possibly technical assistance otherwise data can misinform and lead to unreliable inferences data management techniques can improve teaching and learning in schools test scores are used by many principals to identify bubble kids students whose results are just below proficiency level in reading and mathematics ref name r1 cite journal last1 mandinach first1 ellen title a perfect time for data use journal educational psychologist date april 23 2012 volume 47 page 2 doi 10 1080 00461520 2012 667064 ref types of data used in education there are 4 major types of data used in education demographics data perceptions data student learning data and school processes data ref name bernhardt cite book last1 bernhardt first1 victoria title data analysis for continuous school improvement date 2013 publisher routledge location 711 third avenue new york 10017 isbn 978 1 59667 252 9 pages 27 80 ref 1 demographics data in educational organizations answers the question who are we demographics show the current context of the school and shows the trends trends help to predict and plan for the future along with seeing measures where leaders work towards continuous school improvement thorough demographic data explains the structure of school system and the leadership in education demographic data to the next items number of students in the school number of students with special needs number of english learners age or grade of students in cohorts socio economical status of students attendance rates ethnicity race human classification race religious beliefs graduation rates dropout rates experience information of teachers information about parents of students ref name bernhardt 2 perception data tells us what students staff and parents think about a school and answers the question how do we do business school culture climate and organizational processes are assessed by perception data perception data includes values beliefs perceptions opinions observations perception data is collected mostly questionnaires perception data can be differentiate by two groups 1 staff 2 students and parents staff are being asked if any changes in instruction or curriculum need to take place student and parent are questioned to report their interests how difficult it take them to learn how are they taught and treated ref name bernhardt 3 student learning data answers two questions how are our students doing and where are we now student learning data requires information from all subject areas disaggregated by demographic groups by teachers by grade level by cohorts over time and individual student growth this type of data helps to address additional help to students who are not proficient deepening into what they know and what they don t know to become proficient student learning data connects with curriculum teaching instruction and educational assessment assessment in order to improve outcomes student learning data can clearly state the effectiveness of a single educator or the entire school sld can be gathered by looking at diagnostic tests formative assessments performance assessments standardized tests non referenced tests summative assessments teacher assigned tests and others ref name bernhardt 4 school processes refer to actions of administrators and teachers to achieve the purpose of the school teachers habits customs knowledge and professionalism are the things leading towards progress inside organizations school processes data tell us what works what doesn t the results of educational process and answers the question what are our processes are school processes produce school and class results there are 4 major types of school processes 1 instructional processes 2 organizational processes 3 administrative processes 4 continuous school improvement processes ref name bernhardt use in educational organizations the u s department of education and the institute of education sciences require to use data and dddm in past decades to run educational organizations hard evidence and the use of data are emphasized to inform decisions the data in educational organizations means more than analyzing test scores educational data movement is considered as a sociotechnical revolution educational data systems involve technologies and evidence to explain districts schools classrooms tendencies dddm is used to explain complexity of education support collaboration creating new designs of teaching student performance is central in dddm nclb provided boost in the collection and use of educational information ref name piety cite book last1 piety first1 philip title assessing the educational data movement date 2013 publisher teachers college press location new york isbn 978 0 8077 5426 9 pages 1 20 ref for example in a rural area educators tried to understand why a particular subset of students were struggling academically data analysts collected students performance data medical records behavioral data attendance and other data less qualitative information after not finding direct correlation between collected data and student outcomes they decided to include transportation data into the research as result educators found that students who had longer way from houses to the school were struggling the most according to the finding administrators modified transportation arrangements to make the way shorter for students as well as installing internet access in buses so students could concentrate on doing homework dddm in this particular case helped to improve student results ref name r1 effects on schools effective schools showing outstanding gains in academic measures report that the wide and wise use of data has a positive effect on student achievement and progress dddm is suggested to be a main tool to move educational organizations towards school improvement and educator effectiveness data can be used to measure growth over time program evaluation along with identifying root causes of problems connected to education involving school teachers in data inquiry causes more collaborative work from staff data provides increasing communication and knowledge which has a positive effect on altering educator attitudes towards groups inside schools which are underperforming ref cite journal last1 wayman first1 jeffrey title involving teachers in data driven decision making using computer data systems to support teacher inquiry and reflection journal journal of education for students placed at risk date 2005 pages 296 300 ref notes reflist general references cite journal last1 spillane first1 james p title data in practice conceptualizing the data based decision making phenomena journal american journal of education date 2012 volume 118 issue 2 pages 113 141 jstor 10 1086 663283 doi 10 1086 663283 cite journal last1 reeves first1 patricia l last2 burt first2 walter l title challenges in data based decision making voices from principals journal educational horizons date 2006 volume 85 issue 1 pages 65 71 jstor 42925967 category data management category standards based education'
b'use dmy dates date may 2016 a data lake is a method of storing data within a system or repository in its natural format ref http blogs sas com content datamanagement 2016 11 21 growing import big data quality the growing importance of big data quality ref that facilitates the collocation of data in various schemata and structural forms usually object blobs or files invention james dixon then chief technology officer at pentaho coined the term ref name woods2011 cite news title big data requires a big architecture last woods first dan work forbes date 21 july 2011 department tech url http www forbes com sites ciocentral 2011 07 21 big data requires a big new architecture ref to contrast it with data mart which is a smaller repository of interesting attributes extracted from raw data ref name dixon2010 cite web last dixon first james title pentaho hadoop and data lakes url https jamesdixon wordpress com 2010 10 14 pentaho hadoop and data lakes website james dixon s blog publisher james accessdate 7 november 2015 quote if you think of a datamart as a store of bottled water cleansed and packaged and structured for easy consumption the data lake is a large body of water in a more natural state the contents of the data lake stream in from a source to fill the lake and various users of the lake can come to examine dive in or take samples ref he argued that data marts have several inherent problems and that data lakes are the optimal solution these problems are often referred to as information silo ing pricewaterhousecoopers said that data lakes could put an end to data silos ref name stein2014 cite report url http www pwc com en us us technology forecast 2014 cloud computing assets pdf pwc technology forecast data lakes pdf format pdf title data lakes and the promise of unsiloed data last2 morrison first2 alan last stein first brian publisher pricewaterhousecooper series technology forecast rethinking integration year 2014 ref in their study on data lakes they noted that enterprises were starting to extract and place data for analytics into a single hadoop based repository characteristics the idea of data lake is to have a single store of all data in the enterprise ranging from raw data which implies exact copy of source system data to transformed data which is used for various tasks including data reporting reporting data visualization visualization data analytics analytics and machine learning the data lake includes structured data from relational databases rows and columns semi structured data csv logs xml json unstructured data emails documents pdfs and even binary data images audio video thus creating a centralized data store accommodating all forms of data examples one example of a data lake is the distributed file system apache hadoop many companies also use cloud storage services such as amazon s3 ref name tuulos2015 cite web title petabyte scale data pipelines with docker luigi and elastic spot instances last tuulos first ville date 22 september 2015 url http tech adroll com blog data 2015 09 22 data pipelines docker html ref there is a gradual academic interest in the concept of data lakes for instance http www researchgate net publication 283053696 personal data lake with data gravity pull personal datalake ref http ieeexplore ieee org xpl abstractauthors jsp reload true arnumber 7310733 ref an ongoing project at cardiff university to create a new type of data lake which aims at managing big data of individual users by providing a single point of collecting organizing and sharing personal data ref http www researchgate net publication 283053696 personal data lake with data gravity pull ref the earlier data lake hadoop 1 0 had limited capabilities with its batch oriented processing map reduce and was the only processing paradigm associated with it interacting with the data lake meant you had to have expertise in java with map reduce and higher level tools like pig hive which by themselves were batch oriented with the dawn of hadoop 2 0 and separation of duties with resource management taken over by yarn yet another resource negotiator new processing paradigms like streaming interactive on line have become available via hadoop and the data lake criticism criticism section date december 2015 in june 2015 david needle characterized so called data lakes as one of the more controversial ways to manage big data ref name needle2015 cite news last needle first david title hadoop summit wrangling big data requires novel tools techniques date 10 june 2015 work eweek url http www eweek com enterprise apps hadoop summit wrangling big data requires novel tools techniques 2 html department enterprise apps access date 1 november 2015 quote walter maguire chief field technologist at hp s big data business unit discussed one of the more controversial ways to manage big data so called data lakes ref pricewaterhousecoopers were also careful to note in their research that not all data lake initiatives are successful they quote sean martin cto of cambridge semantics quote sign source we see customers creating big data graveyards dumping everything into hdfs hadoop distributed file system and hoping to do something with it down the road but then they just lose track of what s there ref name stein2014 they advise that the main challenge is not creating a data lake but taking advantage of the opportunities it presents ref name stein2014 they describe companies that build successful data lakes as gradually maturing their lake as they figure out which data and metadata are important to the organization references references category data management'
b'class wikitable implementation snia reference implementation cdmi serve cdmi proxy cdmi for openstack s swift cdmi z onedata version http www snia org forums csi programs cdmiportal 1 0e https github com koenbollen cdmi serve 238c28fc7c https github com livenson vcdm 0 1 https github com osaddon cdmi f0e3ad9bac 1 http packages onedata org oneprovider linux rpm 2 0 cdmi version 1 0 2 1 0 1 1 0 2 1 0 2 colspan 7 align center http features https yes yes basic authentication yes digest authentication yes x 509 x 509 authentication yes x 509 voms authentication yes token based authentication yes colspan 7 align center data access methods filesystem in userspace fuse yes gridftp no iscsi yes no webdav no network file system nfs no browser user interface bui yes colspan 7 align center system wide cdmi capabilities cdmi domains no false no false no false no false yes true no false cdmi export cifs no false no false no false no false no false no false cdmi dataobjects yes true yes true yes true yes true yes true yes true cdmi export iscsi no false no false no false no false no false no false cdmi export nfs no false no false no false no false no false no false cdmi export occi iscsi yes true no false no false no false no false no false cdmi export webdav no false no false no false no false no false no false cdmi metadata maxitems 1024 4096 1024 cdmi metadata maxsize 4096 4096 4096 cdmi metadata maxtotalsize \xe2\x88\x9e 1048576 1048576 cdmi notification no false no false no false no false no false no false cdmi logging no false no false no false no false no false no false cdmi query no false no false no false no false no false no false cdmi query regex no false no false no false no false no false no false cdmi query contains no false no false no false no false no false no false cdmi query tags no false no false no false no false no false no false cdmi query value no false no false no false no false no false no false cdmi queues no false no false no false no false yes true no false cdmi security access control no false no false no false no false no false yes true cdmi security audit no false no false no false no false no false no false cdmi security data integrity no false no false no false no false no false no false cdmi security encryption no false no false no false no false no false no false cdmi security immutability no false no false no false no false no false no false cdmi security sanitization no false no false no false no false no false no false cdmi serialization json no false no false no false no false no false no false cdmi snapshots no false no false no false no false no false no false cdmi references no false no false no false no false no false no false cdmi object move from local no false no false no false no false no false yes true cdmi object move from remote no false no false no false no false no false no false cdmi object move from id no false no false no false no false no false no false cdmi object move to id no false no false no false no false no false no false cdmi object copy from local no false no false no false no false no false yes true cdmi object copy from remote no false no false no false no false no false no false cdmi object access by id no false no false no false no false no false yes true cdmi post dataobject by id no false no false no false no false yes true no false cdmi post queue by id no false no false no false no false yes true no false cdmi deserialize dataobject by id no false no false no false no false no false no false cdmi deserialize queue by id no false no false no false no false no false no false cdmi serialize dataobject to id no false no false no false no false no false no false cdmi serialize domain to id no false no false no false no false no false no false cdmi serialize container to id no false no false no false no false no false no false cdmi serialize queue to id no false no false no false no false no false no false cdmi copy dataobject by id no false no false no false no false no false no false cdmi copy queue by id no false no false no false no false no false no false cdmi create reference by id no false no false no false no false yes true no false colspan 7 align center data object capabilities cdmi read value no false no false yes true no false no false yes true cdmi read value range no false no false no false no false no false yes true cdmi read metadata no false no false yes true no false no false yes true cdmi modify value no false no false yes true no false no false yes true cdmi modify value range no false no false no false no false no false yes true cdmi modify metadata no false no false yes true no false no false yes true cdmi modify deserialize dataobject no false no false no false no false no false no false cdmi delete dataobject yes true yes true yes true yes true yes true yes true cdmi acl no false no false no false no false no false yes true cdmi size no false no false no false no false no false yes true cdmi ctime no false no false no false no false no false yes true cdmi atime no false no false no false no false no false yes true cdmi mtime no false no false no false no false no false yes true cdmi acount no false no false no false no false no false no false cdmi mcount no false no false no false no false no false no false cdmi assignedsize no false no false no false no false no false no false cdmi data redundancy no no no no no no false cdmi data dispersion no false no false no false no false no false no false cdmi data retention no false no false no false no false no false no false cdmi data autodelete no false no false no false no false no false no false cdmi data holds no false no false no false no false no false no false cdmi encryption no no no no no no false cdmi geographic placement no false no false no false no false no false no false cdmi immediate redundancy no no no no no no false cdmi infrastructure redundancy no no no no no no false cdmi latency no false no false no false no false no false no false cdmi rpo no false no false no false no false no false no false cdmi rto no false no false no false no false no false no false cdmi sanitization method no no no no no no false cdmi throughput no false no false no false no false no false no false cdmi value hash no no no no no no false colspan 7 align center container capabilities cdmi list children yes true yes true yes true yes true yes true yes true cdmi list children range no false no false no false no false no false yes true cdmi read metadata no false no false yes true no false no false yes true cdmi modify metadata no false no false yes true no false no false yes true cdmi modify deserialize container no false no false no false no false no false no false cdmi snapshot no false no false no false no false no false no false cdmi serialize dataobject no false no false no false no false no false no false cdmi serialize container no false no false no false no false no false no false cdmi serialize queue no false no false no false no false no false no false cdmi serialize domain no false no false no false no false no false no false cdmi deserialize container no false no false no false no false no false no false cdmi deserialize queue no false no false no false no false no false no false cdmi deserialize dataobject no false no false no false no false no false no false cdmi create dataobject yes true yes true yes true yes true yes true yes true cdmi post dataobject no false no false no false no false yes true no false cdmi post queue no false no false no false no false yes true no false cdmi create container yes true yes true yes true yes true yes true yes true cdmi create queue no false no false no false no false yes true no false cdmi create reference no false no false no false no false no false no false cdmi export container cifs no false no false no false no false no false no false cdmi export container nfs no false no false no false no false no false no false cdmi export container iscsi no false no false no false no false no false no false cdmi export container occi no false no false no false no false no false no false cdmi export container webdav no false no false no false no false no false no false cdmi delete container yes true yes true yes true yes true yes true yes true cdmi move container no false no false no false no false no false yes true cdmi copy container no false no false no false no false no false yes true cdmi move dataobject no false no false no false no false no false yes true cdmi copy dataobject no false no false no false no false no false yes true cdmi acl no false no false no false no false no false yes true cdmi size no false no false no false no false no false yes true cdmi ctime no false no false no false no false no false yes true cdmi atime no false no false no false no false no false yes true cdmi mtime no false no false no false no false no false yes true cdmi acount no false no false no false no false no false no false cdmi mcount no false no false no false no false no false no false cdmi assignedsize no false no false no false no false no false no false cdmi data redundancy no no no no no no false cdmi data dispersion no false no false no false no false no false no false cdmi data retention no false no false no false no false no false no false cdmi data autodelete no false no false no false no false no false no false cdmi data holds no false no false no false no false no false no false cdmi encryption no no no no no no false cdmi geographic placement no false no false no false no false no false no false cdmi immediate redundancy no no no no no no false cdmi infrastructure redundancy no no no no no no false cdmi latency no false no false no false no false no false no false cdmi rpo no false no false no false no false no false no false cdmi rto no false no false no false no false no false no false cdmi sanitization method no no no no no no false cdmi throughput no false no false no false no false no false no false cdmi value hash no no no no no no false colspan 7 align center domain object capabilities cdmi create domain no n a no n a no n a no n a yes true no false cdmi delete domain no n a no n a no n a no n a yes true no false cdmi domain summary no n a no n a no n a no n a no false no false cdmi domain members no n a no n a no n a no n a no false no false cdmi list children no n a no n a no n a no n a yes true no false cdmi read metadata no n a no n a no n a no n a no false no false cdmi modify metadata no n a no n a no n a no n a no false no false cdmi modify deserialize domain no n a no n a no n a no n a no false no false cdmi copy domain no n a no n a no n a no n a no false no false cdmi deserialize domain no n a no n a no n a no n a no false no false cdmi acl no n a no n a no n a no n a no false no false cdmi size no n a no n a no n a no n a no false no false cdmi ctime no n a no n a no n a no n a no false no false cdmi atime no n a no n a no n a no n a no false no false cdmi mtime no n a no n a no n a no n a no false no false cdmi acount no n a no n a no n a no n a no false no false cdmi mcount no n a no n a no n a no n a no false no false cdmi assignedsize no n a no n a no n a no n a no false no false cdmi data redundancy no n a no n a no n a no n a no no false cdmi data dispersion no n a no n a no n a no n a no false no false cdmi data retention no n a no n a no n a no n a no false no false cdmi data autodelete no n a no n a no n a no n a no false no false cdmi data holds no n a no n a no n a no n a no false no false cdmi encryption no n a no n a no n a no n a no no false cdmi geographic placement no n a no n a no n a no n a no false no false cdmi immediate redundancy no n a no n a no n a no n a no no false cdmi infrastructure redundancy no n a no n a no n a no n a no no false cdmi latency no n a no n a no n a no n a no false no false cdmi rpo no n a no n a no n a no n a no false no false cdmi rto no n a no n a no n a no n a no false no false cdmi sanitization method no n a no n a no n a no n a no no false cdmi throughput no n a no n a no n a no n a no false no false cdmi value hash no n a no n a no n a no n a no no false colspan 7 align center queue object capabilities cdmi read value no n a no n a no n a no n a no false no false cdmi read metadata no n a no n a no n a no n a no false no false cdmi modify value no n a no n a no n a no n a no false no false cdmi modify metadata no n a no n a no n a no n a no false no false cdmi modify deserialize queue no n a no n a no n a no n a no false no false cdmi delete queue no n a no n a no n a no n a yes true no false cdmi move queue no n a no n a no n a no n a no false no false cdmi copy queue no n a no n a no n a no n a no false no false cdmi reference queue no n a no n a no n a no n a no false no false cdmi acl no n a no n a no n a no n a no false no false cdmi size no n a no n a no n a no n a no false no false cdmi ctime no n a no n a no n a no n a no false no false cdmi atime no n a no n a no n a no n a no false no false cdmi mtime no n a no n a no n a no n a no false no false cdmi acount no n a no n a no n a no n a no false no false cdmi mcount no n a no n a no n a no n a no false no false cdmi assignedsize no n a no n a no n a no n a no false no false cdmi data redundancy no n a no n a no n a no n a no no false cdmi data dispersion no n a no n a no n a no n a no false no false cdmi data retention no n a no n a no n a no n a no false no false cdmi data autodelete no n a no n a no n a no n a no false no false cdmi data holds no n a no n a no n a no n a no false no false cdmi encryption no n a no n a no n a no n a no no false cdmi geographic placement no n a no n a no n a no n a no false no false cdmi immediate redundancy no n a no n a no n a no n a no no false cdmi infrastructure redundancy no n a no n a no n a no n a no no false cdmi latency no n a no n a no n a no n a no false no false cdmi rpo no n a no n a no n a no n a no false no false cdmi rto no n a no n a no n a no n a no false no false cdmi sanitization method no n a no n a no n a no n a no no false cdmi throughput no n a no n a no n a no n a no false no false cdmi value hash no n a no n a no n a no n a no no false defaultsort cdmi server implementation comparison category cloud storage category data management'
b'infobox company logo file cleo company logo 2014 png name cleo type privately held company foundation 1976 location city loves park illinois location country united states of america key people mahesh rajasekharan small chief executive officer ceo small br sumit garg small president small num employees 200 industry managed file transfer data integration network management and secure file sharing homepage url http cleo com cleo is an enterprise software company that provides electronic data interchange edi and application to application a2a business to business b2b and big data integration services to organizations with managed file transfer needs the company formerly known as cleo communications was founded in 1976 cleo was acquired by investment firm globe equity partners in 2012 mahesh rajasekharan is cleo s ceo and sumit garg serves as cleo s president ref cite web author alex gary url http www rrstar com x1364621329 private equity firm acquires loves park company title private equity firm acquires loves park company blogs rockford register star publisher rrstar com date accessdate 2014 06 05 ref business cleo originally began as a division of phone 1 inc a voice data gathering systems manufacturer and built data concentrators and terminal emulator s multi bus computers modems and terminals to interface with ibm mainframes via binary synchronous communications bisynchronous communications the company then began developing mainframe middleware in the 1980s and with the rise of the personal computer pc moved into b2b data communications and file transfer software ref cite web url https books google com books id jnjwaaaamaaj q cleo 22phone 1 22 dq cleo 22phone 1 22 hl en sa x ei 6i65vliabc6yogtf0ikaba ved 0cecq6aewbtge title kelly grimes ibm pc compatible computer directory brian w kelly dennis j grimes google books publisher books google com date 2008 01 28 accessdate 2015 04 02 ref cleo s portfolio features big data extreme file transfer data transformation person to person collaboration and file sharing solutions ref http www channelworld in interviews high speed data transfer is more critical than ever 3a mahesh rajasekharan 2c cleo ref and its product line includes software for secure file transfer exchange and cloud collaboration collaboration secure email text and voice messaging and others ref http investing businessweek com research stocks private snapshot asp privcapid 204651549 cleo communications inc private company information businessweek bot generated title ref cleo products use the as2 specification and other protocols for connectivity and community management ref cite web author url http www filetransferconsulting com forresters managed file transfer good bad ugly title forrester s managed file transfer solutions good bad and ugly publisher filetransferconsulting com date 2011 07 14 accessdate 2014 06 05 ref cleo versalex is the engine behind its software offerings which include cleo lexicom ref http www itjungle com fhs fhs030408 story10 html four hundred stuff cleo updates b2b communications software bot generated title ref cleo vltrader ref http www itjungle com fhs fhs021610 story08 html four hundred stuff stonebranch taps cleo for b2b expertise bot generated title ref and cleo harmony which supports the streamlining of data integration ref http webcache googleusercontent com search q cache http lerablog org technology software ftp tools to help with large file transfers best ftp tools for large file transfers bot generated title dead link date november 2016 bot internetarchivebot fix attempted yes ref the company also developed the cleo unify and cleo trust secure file sharing and email messaging solutions that work independently or in conjunction with cleo s data integration platform ref cite web url http www rrstar com article 20150209 news 150209528 title cleo releases new software rockford register star publisher rrstar com date 2015 02 09 accessdate 2015 02 19 ref in 2015 cleo introduced the cleo jetsonic high speed data transfer software solution ref cite web url http www rrstar com article 20150708 news 150709580 1 json title cleo announces new data solution rockford register star publisher rrstar com date accessdate 2015 07 09 ref the city of atlanta adopted cleo s fax technology cleo streem in 2006 to accommodate its communication needs ref http citycouncil atlantaga gov 2013 images proposed 13r3556 pdf ref and the u s department of veterans affairs did the same in 2013 when in need of fips 140 2 compliant technology to protect information ref http www va gov trm toolpage asp tid 6568 one va technical reference model bot generated title ref cleo also serves u s transportation logistics company mercurygate international ref cite web url http www rrstar com article 20140820 entertainmentlife 140829886 10487 business title loves park business selected to provide online services rockford register star publisher rrstar com date 2014 08 20 accessdate 2015 02 19 ref as a customer and partners with hortonworks ref http hortonworks com blog secure reliable hadoop data transfer option cleo mft secure reliable hadoop data transfer with cleo mft hortonworks bot generated title ref for big data integration and tech data for software distribution ref http logistics cioreview com news cleo s data transfer solutions now available on the tech data online store nid 2119 cid 33 html cleo s data transfer solutions now available on the tech data online store bot generated title ref cleo software also powers the architecture for several major supply chain companies such as jda software and sap se sap ref cite web last grackin first ann url http searchmanufacturingerp techtarget com tip smart sensors bring the supply chain to life title smart sensors bring the supply chain to life publisher searchmanufacturingerp techtarget com date accessdate 2015 07 08 ref in 2009 cleo was added to the gartner magic quadrant for managed file transfer ref http www servicecatalog dts ca gov services sft docs mft quad 2009 axway 3183 pdf ref expansion in june 2014 cleo opened an office in chicago for members of its support and engineering teams ref http rockrivertimes com 2014 07 16 cleo continues to grow expands operations into chicago office cleo continues to grow expands operations into chicago office the rock river times bot generated title ref the company in 2014 hired jorge rodriguez as senior vice president of product development ref cite web author url http www marketwatch com story jorge rodriguez joins cleo as senior vice president of product development 2014 02 04 title jorge rodriguez joins cleo as senior vice president of product development publisher marketwatch date 2014 02 04 accessdate 2015 02 19 ref and john thielens as vice president of technology ref cite web author url http www marketwatch com story john thielens joins cleo as vice president of technology 2014 01 31 title john thielens joins cleo as vice president of technology publisher marketwatch date 2014 01 31 accessdate 2015 02 19 ref and in 2015 cleo hired dave brunswick as vice president of solutions for north america ref cite web url http www rrstar com article 20150705 news 150709917 title cleo announces new hire rockford register star publisher rrstar com date accessdate 2015 07 08 ref cleo also opened its center of innovation product development facility in bengaluru india in 2015 ref http www deccanherald com content 500091 cleo bengaluru centre plans co html ref in 2016 cleo acquired extol international extol international a pottsville pennsylvania pottsville pa based business and edi integration and data transformation company for an undisclosed amount the pottsville office will operate under the cleo name ref http www lvb com article 20160406 lvb01 160409929 pottsville tech firm acquired by illinois company ref certification cleo regularly submits its products to drummond group s interoperability software testing for as2 ref http www supplychainbrain com content technology solutions supplier relationship mgmt single article page article cleos versalex wins drummond certification for as2 interoperability cleo s versalex wins drummond certification for as2 interoperability bot generated title ref as3 ref cite web url http www drummondgroup com index php newsevents press releases 341 as3 secure messaging products are drummond certified in 1q14 interoperability test event title as3 secure messaging products are drummond certified\xe2\x84\xa2 in 1q14 interoperability test event publisher drummond group date 2014 02 19 accessdate 2014 06 05 ref and ebms 2 0 ref cite web url http www drummondgroup com index php newsevents press releases 339 newest ebms 20 secure messaging products are drummond certified title newest ebms 2 0 secure messaging products are drummond certified\xe2\x84\xa2 publisher drummond group date 2013 09 09 accessdate 2014 06 05 ref awards cleo has been given a xerox partner of the year award for each of the past five years the cleo streem solution integrates with xerox multi function products providing customers with comprehensive solutions for network fax and interactive messaging needs ref cite web url http www rrstar com article 20150330 news 150339927 10447 news title cleo wins xerox partner of the year award news rockford register star publisher rrstar com date accessdate 2015 04 02 ref references reflist 3 category edi software companies category software companies based in illinois category network management category managed file transfer category file transfer protocols category data management'
b'refimprove date september 2014 information integration ii also called referential integrity is the merging of information from heterogeneous sources with differing conceptual contextual and typographical representations it is used in data mining and consolidation of data from unstructured or semi structured resources typically information integration refers to textual representations of knowledge but is sometimes applied to rich media content information fusion which is a related term involves the combination of information into a new set of information towards reducing redundancy and uncertainty ref name dca m haghighat m abdel mottaleb w alhalabi 2016 http dx doi org 10 1109 tifs 2016 2569061 discriminant correlation analysis real time feature level fusion for multimodal biometric recognition ieee transactions on information forensics and security 11 9 1984 1996 ref examples of technology technologies available to integrate information include data deduplication deduplication and string metrics which allow the detection of similar text in different data sources by fuzzy string searching fuzzy matching a host of methods for these research areas are available such as those presented in the international society of information fusion see also data fusion is a subset of information integration sensor fusion data integration image fusion external links https github com mhaghighat dcafuse discriminant correlation analysis dca ref name dca ref http webcache googleusercontent com search q cache orncxopaxamj infolab stanford edu pub papers integration using views ps information integration cd 5 hl en ct clnk gl us client firefox a information integration using logical view lncs 1997 http www isif org international society of information fusion books liggins martin e david l hall and james llinas multisensor data fusion second edition theory and practice multisensor data fusion crc 2008 isbn 978 1 4200 5308 1 david l hall sonya a h mcmullen mathematical techniques in multisensor data fusion 2004 isbn 1 58053 335 3 springer information fusion in data mining 2003 isbn 3 540 00676 1 h b mitchell multi sensor data fusion an introduction 2007 springer verlag berlin isbn 978 3 540 71463 7 s das high level data fusion 2008 artech house publishers norwood ma isbn 978 1 59693 281 4 and 1596932813 erik p blasch eloi bosse and dale a lambert high level information fusion management and system design 2012 artech house publishers norwood ma isbn 1608071510 isbn 978 1608071517 references reflist defaultsort information integration category data management ar \xd8\xaa\xd9\x83\xd8\xa7\xd9\x85\xd9\x84 \xd8\xa7\xd9\x84\xd8\xa8\xd9\x8a\xd8\xa7\xd9\x86\xd8\xa7\xd8\xaa de informationsintegration'
b'infobox occupation caption database administrator official names database administrator database analyst activity sector information technology information system s competencies database design databases design and implementation computer programming programming skills database theory computer network networking basics analytical skill s critical thinking formation at least a academic certificate certificate with experience database administrators dbas use specialized software to store and organize data ref name bls dba cite web url http www bls gov ooh computer and information technology database administrators htm title database administrators publisher bureau of labor statistics work 11 04 2015 accessdate 4 november 2015 ref the role may include capacity planning installation computer programs installation computer configuration configuration database design data migration migration performance monitoring computer security security troubleshooting as well as backup and data recovery ref name techrepublic cite web url http www techrepublic com blog the enterprise cloud what does a dba do all day title what does a dba do all day publisher techrepublic com work 11 04 2015 accessdate 4 november 2015 ref skills list of skills required to become database administrators are ref cite web last1 spenik first1 mark last2 sledge first2 orryn date 2001 03 20 url http www developer com db article php 718491 what is a database administrator htm title what is a database administrator dba publisher developer com accessdate 2012 02 06 archiveurl https web archive org web 20110613101702 http www developer com db article php 718491 what is a database administrator htm archivedate 2011 06 13 ref ref http www dba oracle com oracle tips dba job skills htm ref ref http www orafaq com wiki roles and responsibilities ref communication skills knowledge of database theory knowledge of database design knowledge about the relational database management system rdbms itself e g microsoft sql server or mysql knowledge of sql structured query language sql e g sql psm or transact sql general understanding of distributed computing distributed computing architectures e g client server model general understanding of operating system e g microsoft windows windows or linux general understanding of computer data storage storage technologies and computer network networking general understanding of routine maintenance recovery and handling failover of a database database administrators benefit from a bachelor s degree or master s degree in computer science an associate degree or a academic certificate certificate may be sufficient with work experience ref name study com cite web url http study com articles database administrator job description and requirements html title database administrator job description and requirements publisher study com work 11 4 2015 accessdate 4 november 2015 ref certification there are many certifications available for becoming a certified database administrator many of these certifications are offered by database vendors themselves by passing a series of tests and sometimes other requirements you can earn a database administrator certification schools offering database administration degrees can also be found ref name learn org cite web url http learn org articles how do i become a certified database administrator html title how do i become a certified database administrator publisher learn org work learn org accessdate 4 november 2015 ref for example ibm certified advanced database administrator db2 10 1 for linux unix and windows ref name ibm com cite web url http www 03 ibm com certify index shtml title ibm professional certification program work ibm com publisher ibm accessdate 2014 08 10 ref ibm certified database administrator db2 10 1 for linux unix and windows ref name ibm com oracle database 11g administrator certified professional ref cite web url http education oracle com pls web prod plq dad db pages getpage page id 143 p org id 1001 lang us title oracle certification program work oracle com publisher oracle corporation accessdate 2011 06 18 ref oracle mysql 5 6 database administrator certified professional ref cite web url https education oracle com pls web prod plq dad ou product category getpage p cat id 159 p org id 15941 lang us tabs 3 title oracle certified professional mysql 5 6 database administrator work oracle com publisher oracle corporation accessdate 2016 09 18 ref mcsa sql server 2012 ref name mcsasql cite web url https www microsoft com en us learning mcsa sql certification aspx title mcsa sql server work microsoft com publisher microsoft accessdate 2015 11 03 ref mcse data platform solutions expert ref name microsoftsolutionsexpert cite web url https www microsoft com en us learning mcse sql data platform aspx title mcse data platform publisher microsoft com work 11 4 2015 accessdate 4 november 2015 ref duties a database administrator s responsibilities can include the following tasks ref cite web url http docs oracle com cd b10501 01 server 920 a96521 dba htm 852 title oracle dba responsibilities work oracle corporation accessdate 2012 02 06 ref installation computer programs installing and upgrade upgrading the database server and application tools allocating system storage and planning future storage requirements for the database system modifying the database structure as necessary from information given by application developers enrolling users and maintaining system computer security security ensuring compliance with database vendor license license agreement controlling and system monitoring monitoring user computing user access to the database monitoring and program optimization optimizing the performance of the database planning for backup and recovery of database information maintaining archive d data backing up and restoring databases contacting database vendor for technical support generating various reports by querying from database as per need see also comparison of database tools references reflist external links database administrators database use british english date june 2012 use dmy dates date june 2012 defaultsort database administrator category computer occupations category data management category database specialists'
b'embedded analytics is the technology designed to make data analysis and business intelligence more accessible by all kinds of application or user definition according to gartner analysts kurt schlegel traditional business intelligence were suffering in 2008 a lack of integration between the data and the business users ref cite web last kelly first jeff title gartner business intelligence summit embed bi within business processes publisher techtarget accessdate august 2015 url http searchbusinessanalytics techtarget com news 1507180 gartner business intelligence summit embed bi within business processes ref this technology intention is to be more pervasive by real time autonomy and self service of data visualization or customization meanwhile decision makers business users or even customers are doing their own daily workflow and tasks history first mentions of the concept were made by howard dresner consultant author former gartner analyst and inventor of the term business intelligence ref cite web last kelly first jeff title gartner business intelligence summit embed bi within business processes publisher techtarget accessdate august 2015 url http searchbusinessanalytics techtarget com news 1507180 gartner business intelligence summit embed bi within business processes ref consolidation of business intelligence doesn t mean the bi market has reached maturity ref cite web last dresner first howard title howard dresner predicts the future of business intelligence publisher techtarget accessdate august 2015 url http searchbusinessanalytics techtarget com podcast howard dresner predicts the future of business intelligence ref said howard dresner while he was working for hyperion solutions a company that oracle bought in 2007 oracle started then to use the term embedded analytics at their press release for oracle\xc2\xae rapid planning on 2009 ref cite web title oracle announces oracle\xc2\xae rapid planning publisher oracle accessdate august 2015 url http www oracle com us corporate press 040402 ref gartner group a company for which howard dresner has been working finally added the term to their it glossary on november 5 2012 ref cite web title gartner it glossary embedded analytics publisher gartner accessdate august 2015 url http www gartner com it glossary embedded analytics ref it was clear this was a mainstream technology when dresner advisory services published the 2014 embedded business intelligence market study as part of the wisdom of crowds\xc2\xae series of research including 24 vendors ref cite web title 2014 embedded business intelligence market study now available from dresner advisory services publisher market wired accessdate august 2015 url http www marketwired com press release 2014 embedded business intelligence market study now available from dresner advisory 1962227 htm ref tools colbegin 2 actuate corporation actuate dundas data visualization gooddata ibm iccube logi analytics pentaho qlik sap se sap sas software sas servicenow tableau software tableau thoughtspot tibco sisense colend references reflist category types of analytics category big data analytics category business intelligence category data management'
b'refimprove date june 2014 chunked transfer encoding is a data transfer mechanism in version 1 1 of the hypertext transfer protocol http in which data is sent in a series of chunks it uses the list of http header fields transfer encoding response header transfer encoding http header in place of the content length header which the earlier version of the protocol would otherwise require ref http tools ietf org html rfc1945 section 7 2 ref because the content length header is not used the sender does not need to know the length of the content before it starts transmitting a response to the receiver senders can begin transmitting dynamically generated content before knowing the total size of that content the size of each chunk is sent right before the chunk itself so that the receiver can tell when it has finished receiving data for that chunk the data transfer is terminated by a final chunk of length zero an early form of the chunked encoding was proposed in 1994 ref cite web last connolly first daniel title content transfer encoding packets for http url http 1997 webhistory org www lists www talk 1994q3 1147 html accessdate 13 september 2013 date 27 sep 1994 id lt 9409271503 aa27488 austin2 hal com gt ref later it was standardized in http 1 1 rationale the introduction of chunked encoding provided various benefits chunked transfer encoding allows a server to maintain an http persistent connection for dynamically generated content in this case the http content length header cannot be used to delimit the content and the next http request response as the content size is as yet unknown chunked encoding has the benefit that it is not necessary to generate the full content before writing the header as it allows streaming of content as chunks and explicitly signaling the end of the content making the connection available for the next http request response chunked encoding allows the sender to send additional header fields after the message body this is important in cases where values of a field cannot be known until the content has been produced such as when the content of the message must be digitally signed without chunked encoding the sender would have to buffer the content until it was complete in order to calculate a field value and send it before the content applicability for version 1 1 of the http protocol the chunked transfer mechanism is considered to be always and anyways acceptable even if not listed in the list of http header fields te request header te transfer encoding request header field and when used with other transfer mechanisms should always be applied last to the transferred data and never more than one time this transfer coding method also allows additional entity header fields to be sent after the last chunk if the client specified the trailers parameter as an argument of the te field the origin server of the response can also decide to send additional entity trailers even if the client did not specify the trailers option in the te request field but only if the metadata is optional i e the client can use the received entity without them whenever the trailers are used the server should list their names in the trailer header field 3 header field types are specifically prohibited from appearing as a trailer field list of http header fields transfer encoding response header transfer encoding list of http header fields content length response header content length and list of http header fields trailer response header trailer format if a tt transfer encoding tt field with a value of tt chunked tt is specified in an http message either a request sent by a client or the response from the server the body of the message consists of an unspecified number of chunks a terminating chunk trailer and a final crlf sequence i e carriage return followed by line feed each chunk starts with the number of octet computing octets of the data it embeds expressed as a hexadecimal number in ascii followed by optional parameters chunk extension and a terminating crlf sequence followed by the chunk data the chunk is terminated by crlf if chunk extensions are provided the chunk size is terminated by a semicolon and followed by the parameters each also delimited by semicolons each parameter is encoded as an extension name followed by an optional equal sign and value these parameters could be used for a running message digest or digital signature or to indicate an estimated transfer progress for instance the terminating chunk is a regular chunk with the exception that its length is zero it is followed by the trailer which consists of a possibly empty sequence of entity header fields normally such header fields would be sent in the message s header however it may be more efficient to determine them after processing the entire message entity in that case it is useful to send those headers in the trailer header fields that regulate the use of trailers are te used in requests and trailers used in responses use with compression http servers often use data compression compression to optimize transmission for example with tt content encoding gzip tt or tt content encoding deflate tt if both compression and chunked encoding are enabled then the content stream is first compressed then chunked so the chunk encoding itself is not compressed and the data in each chunk is not compressed individually the remote endpoint then decodes the stream by concatenating the chunks and uncompressing the result example encoded data in the following example three chunks of length 4 5 and 14 are shown the chunk size is transferred as a hexadecimal number followed by r n as a line separator followed by a chunk of data of the given size pre 4 r n wiki r n 5 r n pedia r n e r n in r n r n chunks r n 0 r n r n pre note the chunk size indicates the size of the chunk data and excludes the trailing crlf r n ref http skrb org ietf http errata html ref in this particular example the crlf following in is counted toward the chunk size of 0xe 14 the crlf in its own line is also counted toward the chunk size the period character at the end of chunks is the 14th character so it is the last data character in that chunk the crlf following the period is the trailing crlf so it is not counted toward the chunk size of 0xe 14 decoded data pre wikipedia in chunks pre see also list of http header fields references reflist refbegin see http tools ietf org html rfc7230 section 4 1 rfc 7230 section 4 1 for further details of chunked encoding the previous obsoleted version is at https tools ietf org html rfc2616 section 3 6 1 rfc 2616 section 3 6 1 refend defaultsort chunked transfer encoding category data management category hypertext transfer protocol category hypertext transfer protocol headers'
b'about large collections of data the band big data band file hilbert infogrowth png thumb right 400px growth of and digitization of global information storage capacity ref cite web url http www martinhilbert net worldinfocapacity html title the world s technological capacity to store communicate and compute information work martinhilbert net accessdate 13 april 2016 ref big data is a term for data set s that are so large or complex that traditional data processing applications are inadequate to deal with them challenges include data analysis analysis capture data curation search data sharing sharing computer data storage storage data transmission transfer data visualization visualization query language querying updating and information privacy the term big data often refers simply to the use of predictive analytics user behavior analytics or certain other advanced data analytics methods that extract value from data and seldom to a particular size of data set ref cite book url http link springer com 10 1007 978 3 319 21569 3 title new horizons for a data driven economy springer doi 10 1007 978 3 319 21569 3 ref there is little doubt that the quantities of data now available are indeed large but that s not the most relevant characteristic of this new data ecosystem ref cite journal last1 boyd first1 dana last2 crawford first2 kate title six provocations for big data journal social science research network a decade in internet time symposium on the dynamics of the internet and society date september 21 2011 doi 10 2139 ssrn 1926431 ref analysis of data sets can find new correlations to spot business trends prevent diseases combat crime and so on r economist scientists business executives practitioners of medicine advertising and government database governments alike regularly meet difficulties with large data sets in areas including web search engine internet search finance urban informatics and business informatics scientists encounter limitations in e science work including meteorology genomics ref cite journal title community cleverness required journal nature volume 455 issue 7209 page 1 date 4 september 2008 doi 10 1038 455001a url http www nature com nature journal v455 n7209 full 455001a html ref connectomics complex physics simulations biology and environmental research ref cite journal last1 reichman first1 o j last2 jones first2 m b last3 schildhauer first3 m p title challenges and opportunities of open data in ecology journal science volume 331 issue 6018 pages 703 5 year 2011 doi 10 1126 science 1197962 pmid 21311007 ref data sets grow rapidly in part because they are increasingly gathered by cheap and numerous information sensing mobile device s aerial remote sensing software logs digital camera cameras microphones radio frequency identification rfid readers and wireless sensor networks ref cite web author hellerstein joe title parallel programming in the age of big data date 9 november 2008 work gigaom blog url http gigaom com 2008 11 09 mapreduce leads the way for parallel programming ref ref cite book first1 toby last1 segaran first2 jeff last2 hammerbacher title beautiful data the stories behind elegant data solutions url https books google com books id zxnglqu1fkgc year 2009 publisher o reilly media isbn 978 0 596 15711 1 page 257 ref the world s technological per capita capacity to store information has roughly doubled every 40 months since the 1980s ref name martinhilbert net cite journal last1 hilbert first1 martin first2 priscila last2 l\xc3\xb3pez title the world s technological capacity to store communicate and compute information journal science volume 332 issue 6025 pages 60 65 year 2011 doi 10 1126 science 1200970 pmid 21310967 url http martinhilbert net worldinfocapacity html ref harv ref as of 2012 lc on every day 2 5 exabyte s 2 5\xc3\x9710 sup 18 sup of data are generated ref cite web url http www ibm com big data us en title ibm what is big data bringing big data to the enterprise publisher www ibm com accessdate 2013 08 26 ref one question for large enterprises is determining who should own big data initiatives that affect the entire organization ref oracle and fsn http www fsn co uk channel bi bpm cpm mastering big data cfo strategies to transform insight into opportunity uo2ac ttuys mastering big data cfo strategies to transform insight into opportunity december 2012 ref relational database management system s and desktop statistics and visualization packages often have difficulty handling big data the work may require massively parallel software running on tens hundreds or even thousands of servers ref cite web author jacobs a title the pathologies of big data date 6 july 2009 work acmqueue url http queue acm org detail cfm id 1563874 ref what counts as big data varies depending on the capabilities of the users and their tools and expanding capabilities make big data a moving target for some organizations facing hundreds of gigabytes of data for the first time may trigger a need to reconsider data management options for others it may take tens or hundreds of terabytes before data size becomes a significant consideration ref cite journal last1 magoulas first1 roger last2 lorica first2 ben title introduction to big data journal release 2 0 issue 11 date february 2009 url http radar oreilly com r2 release2 0 11 html publisher o reilly media location sebastopol ca ref definition file viegas useractivityonwikipedia gif thumb visualization of daily wikipedia edits created by ibm at multiple terabyte s in size the text and images of wikipedia are an example of big data the term has been in use since the 1990s with some giving credit to john mashey for coining or at least making it popular ref cite web title big data and the next wave of infrastress author john r mashey date 25 april 1998 publisher usenix work slides from invited talk url http static usenix org event usenix99 invited talks mashey pdf accessdate 28 september 2016 ref ref cite web title the origins of big data an etymological detective story author steve lohr date 1 february 2013 url http bits blogs nytimes com 2013 02 01 the origins of big data an etymological detective story publisher new york times accessdate 28 september 2016 ref big data usually includes data sets with sizes beyond the ability of commonly used software tools to data acquisition capture data curation curate manage and process data within a tolerable elapsed time ref name editorial cite journal last1 snijders first1 c last2 matzat first2 u last3 reips first3 u d year 2012 title big data big gaps of knowledge in the field of internet url http www ijis net ijis7 1 ijis7 1 editorial html journal international journal of internet science volume 7 issue pages 1 5 ref big data size is a constantly moving target as of 2012 lc on ranging from a few dozen terabytes to many petabyte s of data big data requires a set of techniques and technologies with new forms of integration to reveal insights from datasets that are diverse complex and of a massive scale ref cite journal last1 ibrahim first1 last2 targio hashem first2 abaker last3 yaqoob first3 ibrar last4 badrul anuar first4 nor last5 mokhtar first5 salimah last6 gani first6 abdullah last7 ullah khan first7 samee year 2015 title big data on cloud computing review and open research issues url journal information systems volume 47 issue pages 98 115 doi 10 1016 j is 2014 07 006 ref in a 2001 research report ref cite web first douglas last laney title 3d data management controlling data volume velocity and variety url http blogs gartner com doug laney files 2012 01 ad949 3d data management controlling data volume velocity and variety pdf publisher gartner accessdate 6 february 2001 ref and related lectures meta group now gartner analyst doug laney defined data growth challenges and opportunities as being three dimensional i e increasing volume amount of data velocity speed of data in and out and linktext variety range of data types and sources gartner and now much of the industry continue to use this 3vs model for describing big data ref cite web last beyer first mark title gartner says solving big data challenge involves more than just managing volumes of data url http www gartner com it page jsp id 1731916 publisher gartner accessdate 13 july 2011 archiveurl https web archive org web 20110710043533 http www gartner com it page jsp id 1731916 archivedate 10 july 2011 deadurl no ref in 2012 gartner updated its definition as follows big data is high volume high velocity and or high variety information assets that require new forms of processing to enable enhanced decision making insight discovery and process optimization gartner s definition of the 3vs is still widely used and in agreement with a consensual definition that states that big data represents the information assets characterized by such a high volume velocity and variety to require specific technology and analytical methods for its transformation into value ref name big data definition cite journal last1 de mauro first1 andrea last2 greco first2 marco last3 grimaldi first3 michele year 2016 title a formal definition of big data based on its essential features url http www emeraldinsight com doi abs 10 1108 lr 06 2015 0061 journal library review volume 65 issue pages 122 135 doi 10 1108 lr 06 2015 0061 ref additionally a new v veracity is added by some organizations to describe it ref cite web title what is big data url http www villanovau com university online programs what is big data publisher villanova university ref revisionism challenged by some industry authorities ref cite web last grimes first seth title big data avoid wanna v confusion url http www informationweek com big data big data analytics big data avoid wanna v confusion d d id 1111077 publisher informationweek accessdate 5 january 2016 ref the 3vs have been expanded to other complementary characteristics of big data ref name bd4d cite web last hilbert first martin title big data for development a review of promises and challenges development policy review url http www martinhilbert net big data for development work martinhilbert net accessdate 2015 10 07 ref ref name whatisbigdata volume big data doesn t sample it just observes and tracks what happens velocity big data is often available in real time variety big data draws from text images audio video plus it completes missing pieces through data fusion machine learning big data often doesn t ask why and simply detects patterns ref mayer sch\xc3\xb6nberger v cukier k 2013 big data a revolution that will transform how we live work and think london john murray ref digital footprint big data is often a cost free byproduct of digital interaction ref name whatisbigdata cite av media url https www youtube com watch v xrvih1h47sa index 51 list pltjbscvwcu3rnm46d3r85efm0hrzjuaig title dt sc 7 3 what is big data date 12 august 2015 publisher via youtube ref ref cite web url https canvas instructure com courses 949415 title digital technology social change publisher ref the growing maturity of the concept more starkly delineates the difference between big data and business intelligence ref http www bigdataparis com presentation mercredi pdelort pdf phpsessid tv7k70pcr3egpi2r6fi3qbjtj6 page 4 ref business intelligence uses descriptive statistics with data with high information density to measure things detect trends etc big data uses inductive statistics and concepts from nonlinear system identification ref name sab1 billings s a nonlinear system identification narmax methods in the time frequency and spatio temporal domains wiley 2013 ref to infer laws regressions nonlinear relationships and causal effects from large sets of data with low information density ref cite web url http www andsi fr tag dsi big data title le blog andsi \xc2\xbb dsi big data publisher ref to reveal relationships and dependencies or to perform predictions of outcomes and behaviors ref name sab1 ref cite web url http lecercle lesechos fr entrepreneur tendances innovation 221169222 big data low density data faible densite information com title les echos big data car low density data la faible densit\xc3\xa9 en information comme facteur discriminant archives author les echos date 3 april 2013 work lesechos fr ref characteristics big data can be described by the following characteristics ref name bd4d ref name whatisbigdata volume the quantity of generated and stored data the size of the data determines the value and potential insight and whether it can actually be considered big data or not variety the type and nature of the data this helps people who analyze it to effectively use the resulting insight velocity in this context the speed at which the data is generated and processed to meet the demands and challenges that lie in the path of growth and development variability inconsistency of the data set can hamper processes to handle and manage it veracity the quality of captured data can vary greatly affecting accurate analysis factory work and cyber physical system s may have a 6c system connection sensor and networks cloud computing and data on demand ref wu d liu x hebert s gentzsch w terpenny j 2015 performance evaluation of cloud based high performance computing for finite element analysis proceedings of the asme 2015 international design engineering technical conference computers and information in engineering conference idetc cie2015 boston massachusetts u s ref ref cite journal last1 wu first1 d last2 rosen first2 d w last3 wang first3 l last4 schaefer first4 d year 2015 title cloud based design and manufacturing a new paradigm in digital manufacturing and design innovation url journal computer aided design volume 59 issue 1 pages 1 14 doi 10 1016 j cad 2014 07 006 ref cyber model and memory content context meaning and correlation community sharing and collaboration customization personalization and value data must be processed with advanced tools analytics and algorithms to reveal meaningful information for example to manage a factory one must consider both visible and invisible issues with various components information generation algorithms must detect and address invisible issues such as machine degradation component wear etc on the factory floor ref name indin2014 cite journal last1 lee first1 jay last2 bagheri first2 behrad last3 kao first3 hung an title recent advances and trends of cyber physical systems and big data analytics in industrial informatics journal ieee int conference on industrial informatics indin 2014 date 2014 url https www researchgate net profile behrad bagheri publication 266375284 recent advances and trends of cyber physical systems and big data analytics in industrial informatics links 542dc0100cf27e39fa948a7d origin publication detail ref ref name mfgletters cite journal last1 lee first1 jay last2 lapira first2 edzel last3 bagheri first3 behrad last4 kao first4 hung an title recent advances and trends in predictive manufacturing systems in big data environment journal manufacturing letters volume 1 issue 1 pages 38 41 doi 10 1016 j mfglet 2013 09 005 url http www sciencedirect com science article pii s2213846313000114 ref architecture in 2000 seisint inc now lexisnexis lexisnexis group developed a c based distributed file sharing framework for data storage and query the system stores and distributes structured semi structured and unstructured data across multiple servers users can build queries in a c dialect computing dialect called ecl programming language ecl ecl uses an apply schema on read method to infer the structure of stored data when it is queried instead of when it is stored in 2004 lexisnexis acquired seisint inc ref cite web url http www washingtonpost com wp dyn articles a50577 2004jul14 html title lexisnexis to buy seisint for 775 million publisher washington post accessdate 15 july 2004 ref and in 2008 acquired choicepoint choicepoint inc ref cite web url http www washingtonpost com wp dyn content article 2008 02 21 ar2008022100809 html title lexisnexis parent set to buy choicepoint publisher washington post accessdate 22 february 2008 ref and their high speed parallel processing platform the two platforms were merged into hpcc or high performance computing cluster systems and in 2011 hpcc was open sourced under the apache v2 0 license quantcast file system was available about the same time ref cite web url http www datanami com 2012 10 01 quantcast opens exabyte ready file system title quantcast opens exabyte ready file system publisher www datanami com accessdate 1 october 2012 ref in 2004 google published a paper on a process called mapreduce that uses a similar architecture the mapreduce concept provides a parallel processing model and an associated implementation was released to process huge amounts of data with mapreduce queries are split and distributed across parallel nodes and processed in parallel the map step the results are then gathered and delivered the reduce step the framework was very successful ref bertolucci jeff http www informationweek com big data news software platforms hadoop from experiment to leading big d 240157176 hadoop from experiment to leading big data platform information week 2013 retrieved on 14 november 2013 ref so others wanted to replicate the algorithm therefore an implementation of the mapreduce framework was adopted by an apache open source project named apache hadoop hadoop ref webster john http research google com archive mapreduce osdi04 pdf mapreduce simplified data processing on large clusters search storage 2004 retrieved on 25 march 2013 ref mike2 0 methodology mike2 0 is an open approach to information management that acknowledges the need for revisions due to big data implications identified in an article titled big data solution offering ref cite web url http mike2 openmethodology org wiki big data solution offering title big data solution offering publisher mike2 0 accessdate 8 december 2013 ref the methodology addresses handling big data in terms of useful permutation s of data sources complexity in interrelationships and difficulty in deleting or modifying individual records ref cite web url http mike2 openmethodology org wiki big data definition title big data definition publisher mike2 0 accessdate 9 march 2013 ref 2012 studies showed that a multiple layer architecture is one option to address the issues that big data presents a list of file systems distributed parallel file systems distributed parallel architecture distributes data across multiple servers these parallel execution environments can dramatically improve data processing speeds this type of architecture inserts data into a parallel dbms which implements the use of mapreduce and hadoop frameworks this type of framework looks to make the processing power transparent to the end user by using a front end application server ref cite journal last boja first c author2 pocovnicu a author3 b\xc4\x83t\xc4\x83gan l title distributed parallel architecture for big data journal informatica economica year 2012 volume 16 issue 2 pages 116 127 ref big data analytics for manufacturing applications is marketed as a 5c architecture connection conversion cyber cognition and configuration ref cite web url http www imscenter net cyber physical platform title ims cps ims center publisher accessdate 16 june 2016 ref the data lake allows an organization to shift its focus from centralized control to a shared model to respond to the changing dynamics of information management this enables quick segregation of data into the data lake thereby reducing the overhead time ref http www hcltech com sites default files solving key businesschallenges with big data lake 0 pdf ref ref cite web url https secplab ppgia pucpr br files papers 2015 0 pdf title method for testing the fault tolerance of mapreduce frameworks publisher computer networks year 2015 ref technologies see enablers of big data a 2011 mckinsey company mckinsey global institute report characterizes the main components and ecosystem of big data as follows ref name mckinsey cite journal last1 manyika first1 james first2 michael last2 chui first3 jaques last3 bughin first4 brad last4 brown first5 richard last5 dobbs first6 charles last6 roxburgh first7 angela hung last7 byers title big data the next frontier for innovation competition and productivity publisher mckinsey global institute date may 2011 url http www mckinsey com insights mgi research technology and innovation big data the next frontier for innovation accessdate january 16 2016 ref techniques for analyzing data such as a b testing machine learning and natural language processing big data technologies like business intelligence cloud computing and databases visualization such as charts graphs and other displays of the data multidimensional big data can also be represented as tensor s which can be more efficiently handled by tensor based computation ref cite web title future directions in tensor based computation and modeling date may 2009 url http www cs cornell edu cv tenwork finalreport pdf ref such as multilinear subspace learning ref name mslsurvey cite journal first haiping last lu first2 k n last2 plataniotis first3 a n last3 venetsanopoulos url http www dsp utoronto ca haiping publication surveymsl pr2011 pdf title a survey of multilinear subspace learning for tensor data journal pattern recognition volume 44 number 7 pages 1540 1551 year 2011 doi 10 1016 j patcog 2011 01 004 ref additional technologies being applied to big data include massively parallel processing massive parallel processing mpp databases search based application s data mining ref cite web last1 pllana first1 sabri last2 janciak first2 ivan last3 brezany first3 peter last4 w\xc3\xb6hrer first4 alexander title a survey of the state of the art in data mining and integration query languages url http ieeexplore ieee org xpl articledetails jsp arnumber 6041580 website 2011 international conference on network based information systems nbis 2011 publisher ieee computer society accessdate 2 april 2016 ref distributed file system s distributed database s cloud computing cloud based infrastructure applications storage and computing resources and the internet citation needed date september 2011 some but not all massive parallel processing mpp relational databases have the ability to store and manage petabytes of data implicit is the ability to load monitor back up and optimize the use of the large data tables in the rdbms ref cite web author monash curt title ebay s two enormous data warehouses date 30 april 2009 url http www dbms2 com 2009 04 30 ebays two enormous data warehouses br cite web author monash curt title ebay followup nbsp greenplum out teradata 10 petabytes hadoop has some value and more date 6 october 2010 url http www dbms2 com 2010 10 06 ebay followup greenplum out teradata 10 petabytes hadoop has some value and more ref darpa s topological data analysis program seeks the fundamental structure of massive data sets and in 2008 the technology went public with the launch of a company called ayasdi ref cite web url http www ayasdi com resources title resources on how topological data analysis is used to analyze big data publisher ayasdi ref the practitioners of big data analytics processes are generally hostile to slower shared storage ref cite web title storage area networks need not apply author cnet news date 1 april 2011 url http news cnet com 8301 21546 3 20049693 10253464 html ref preferring direct attached storage direct attached storage das in its various forms from solid state drive ssd to high capacity serial ata sata disk buried inside parallel processing nodes the perception of shared storage architectures storage area network san and network attached storage nas is that they are relatively slow complex and expensive these qualities are not consistent with big data analytics systems that thrive on system performance commodity infrastructure and low cost real or near real time information delivery is one of the defining characteristics of big data analytics latency is therefore avoided whenever and wherever possible data in memory is good data on spinning disk at the other end of a fiber connector fc storage area network san connection is not the cost of a storage area network san at the scale needed for analytics applications is very much higher than other storage techniques there are advantages as well as disadvantages to shared storage in big data analytics but big data analytics practitioners as of 2011 lc on did not favour it ref cite web title how new analytic systems will impact storage date september 2011 url http www evaluatorgroup com document big data how new analytic systems will impact storage 2 ref applications file 2013 09 11 bus wrapped with sap big data parked outside idf13 9730051783 jpg thumb bus wrapped with sap ag sap big data parked outside intel developer forum idf13 big data has increased the demand of information management specialists so much so that software ag oracle corporation ibm microsoft sap ag sap emc corporation emc hewlett packard hp and dell have spent more than 15 nbsp billion on software firms specializing in data management and analytics in 2010 this industry was worth more than 100 nbsp billion and was growing at almost 10 nbsp percent a year about twice as fast as the software business as a whole r economist developed economies increasingly use data intensive technologies there are 4 6 nbsp billion mobile phone subscriptions worldwide and between 1 nbsp billion and 2 nbsp billion people accessing the internet r economist between 1990 and 2005 more than 1 nbsp billion people worldwide entered the middle class which means more people became more literate which in turn lead to information growth the world s effective capacity to exchange information through telecommunication networks was 281 petabytes in 1986 471 petabytes in 1993 2 2 exabytes in 2000 65 exabytes in 2007 ref name martinhilbert net and predictions put the amount of internet traffic at 667 exabytes annually by 2014 r economist according to one estimate one third of the globally stored information is in the form of alphanumeric text and still image data ref name hilbertcontent cite web url http www tandfonline com doi abs 10 1080 01972243 2013 873748 title an error occurred setting your user cookie publisher ref which is the format most useful for most big data applications this also shows the potential of yet unused data i e in the form of video and audio content while many vendors offer off the shelf solutions for big data experts recommend the development of in house solutions custom tailored to solve the company s problem at hand if the company has sufficient technical capabilities ref cite web url http www kdnuggets com 2014 07 interview amy gershkoff ebay in house bi tools html title interview amy gershkoff director of customer analytics insights ebay on how to design custom in house bi tools last1 rajpurohit first1 anmol date 11 july 2014 website kdnuggets accessdate 2014 07 14 quote dr amy gershkoff generally i find that off the shelf business intelligence tools do not meet the needs of clients who want to derive custom insights from their data therefore for medium to large organizations with access to strong technical talent i usually recommend building custom in house solutions ref government the use and adoption of big data within governmental processes is beneficial and allows efficiencies in terms of cost productivity and innovation ref cite web url http www computerworld com article 2472667 government it the government and big data use problems and potential html title the government and big data use problems and potential date 21 march 2012 publisher computerworld access date 12 september 2016 ref but does not come without its flaws data analysis often requires multiple parts of government central and local to work in collaboration and create new and innovative processes to deliver the desired outcome below are some examples of initiatives the governmental big data space united states of america in 2012 the presidency of barack obama obama administration announced the big data research and development initiative to explore how big data could be used to address important problems faced by the government ref name wh big data cite web last kalil first tom title big data is a big deal url http www whitehouse gov blog 2012 03 29 big data big deal publisher white house accessdate 26 september 2012 ref the initiative is composed of 84 different big data programs spread across six departments ref cite web last executive office of the president title big data across the federal government url http www whitehouse gov sites default files microsites ostp big data fact sheet final 1 pdf publisher white house accessdate 26 september 2012 date march 2012 ref big data analysis played a large role in barack obama s successful barack obama presidential campaign 2012 2012 re election campaign ref name infoworld bigdata cite web last lampitt first andrew title the real story of how big data analytics helped obama win url http www infoworld com d big data the real story of how big data analytics helped obama win 212862 work infoworld accessdate 31 may 2014 ref the united states federal government owns six of the ten most powerful supercomputer s in the world ref cite web last hoover first j nicholas title government s 10 most powerful supercomputers url http www informationweek com government enterprise applications image gallery governments 10 most powerf 224700271 work information week publisher ubm accessdate 26 september 2012 ref the utah data center has been constructed by the united states national security agency when finished the facility will be able to handle a large amount of information collected by the nsa over the internet the exact amount of storage space is unknown but more recent sources claim it will be on the order of a few exabyte s ref cite news last bamford first james title the nsa is building the country s biggest spy center watch what you say url http www wired com threatlevel 2012 03 ff nsadatacenter all 1 work wired magazine accessdate 2013 03 18 date 15 march 2012 ref ref cite web url http www nsa gov public info press room 2011 utah groundbreaking ceremony shtml title groundbreaking ceremony held for 1 2 billion utah data center publisher national security agency central security service accessdate 2013 03 18 ref ref cite news last hill first kashmir title tblueprints of nsa s ridiculously expensive data center in utah suggest it holds less info than thought url http www forbes com sites kashmirhill 2013 07 24 blueprints of nsa data center in utah suggest its storage capacity is less impressive than thought work forbes accessdate 2013 10 31 ref india big data analysis was in part responsible for the bharatiya janata party bjp to win the indian general election 2014 indian general election 2014 ref cite web url http www livemint com industry buqo8xq3gstsay5ii9lxok are indian companies making enough sense of big data html title news live mint date 23 june 2014 accessdate 2014 11 22 website are indian companies making enough sense of big data publisher live mint ref the government of india indian government utilizes numerous techniques to ascertain how the indian electorate is responding to government action as well as ideas for policy augmentation ref cite web url http decipherias com currentaffairs big data whats so big about it title big data what s so big about it date 18 march 2016 publisher decipher ias access date 12 september 2016 ref united kingdom examples of uses of big data in public services data on prescription drugs by connecting origin location and the time of each prescription a research unit was able to exemplify the considerable delay between the release of any given drug and a uk wide adaptation of the national institute for health and care excellence guidelines this suggests that new or most up to date drugs take some time to filter through to the general patient ref cite web url https www ijedr org papers ijedr1504022 pdf title survey on big data using data mining date 2015 publisher international journal of engineering development and research access date 14 september 2016 ref joining up data a local authority blended data about services such as road gritting rotas with services for people at risk such as meals on wheels the connection of data allowed the local authority to avoid any weather related delay ref cite web url https www researchgate net publication 297762848 recent advances delivered by mobile cloud computing and internet of things for big data applications a survey title recent advances delivered by mobile cloud computing and internet of things for big data applications a survey date 11 march 2016 publisher international journal of network management access date 14 september 2016 ref international development research on the effective usage of information and communication technologies for development also known as ict4d suggests that big data technology can make important contributions but also present unique challenges to international development ref cite web url http www unglobalpulse org projects bigdatafordevelopment title white paper big data for development opportunities challenges 2012 united nations global pulse publisher accessdate 13 april 2016 ref ref cite web title wef world economic forum vital wave consulting 2012 big data big impact new possibilities for international development work world economic forum accessdate 24 august 2012 url http www weforum org reports big data big impact new possibilities international development ref advancements in big data analysis offer cost effective opportunities to improve decision making in critical development areas such as health care employment economic productivity crime security and natural disaster and resource management ref name hilbertbigdata2013 ref cite web url http blogs worldbank org ic4d four ways to talk about big data title elena kvochko four ways to talk about big data information communication technologies for development series publisher worldbank org accessdate 2012 05 30 ref ref cite web title daniele medri big data business an on going revolution url http www statisticsviews com details feature 5393251 big data business an on going revolution html publisher statistics views date 21 october 2013 ref additionally user generated data offers new opportunities to give the unheard a voice ref cite web title responsible use of data author tobias knobloch and julia manske work d c development and cooperation date 11 january 2016 url http www dandc eu en article opportunities and risks user generated and automatically compiled data ref however longstanding challenges for developing regions such as inadequate technological infrastructure and economic and human resource scarcity exacerbate existing concerns with big data such as privacy imperfect methodology and interoperability issues ref name hilbertbigdata2013 manufacturing based on tcs 2013 global trend study improvements in supply planning and product quality provide the greatest benefit of big data for manufacturing ref name tcs big data study manufacturing cite web url http sites tcs com big data study manufacturing big data benefits challenges title manufacturing big data benefits and challenges work tcs big data study publisher tata consultancy services limited location mumbai india accessdate 2014 06 03 ref big data provides an infrastructure for transparency in manufacturing industry which is the ability to unravel uncertainties such as inconsistent component performance and availability predictive manufacturing as an applicable approach toward near zero downtime and transparency requires vast amount of data and advanced prediction tools for a systematic process of data into useful information ref cite journal last lee first jay author2 wu f author3 zhao w author4 ghaffari m author5 liao l title prognostics and health management design for rotary machinery systems reviews methodology and applications journal mechanical systems and signal processing date january 2013 volume 42 issue 1 ref a conceptual framework of predictive manufacturing begins with data acquisition where different type of sensory data is available to acquire such as acoustics vibration pressure current voltage and controller data vast amount of sensory data in addition to historical data construct the big data in manufacturing the generated big data acts as the input into predictive tools and preventive strategies such as prognostics and health management phm ref cite web url https www phmsociety org events conference phm europe 16 tutorials title tutorials publisher phm society accessdate 27 september 2016 ref ref cite web url https www itri org tw eng content msgpic01 contents aspx siteid 1 mmmid 620651706136357202 catid 620653256103620163 msid 654532365564567545 title prognostic and health management technology for mocvd equipment publisher industrial technology research institute accessdate 27 september 2016 ref cyber physical models current phm implementations mostly use data during the actual usage while analytical algorithms can perform more accurately when more information throughout the machine s lifecycle such as system configuration physical knowledge and working principles are included there is a need to systematically integrate manage and analyze machinery or process data during different stages of machine life cycle to handle data information more efficiently and further achieve better transparency of machine health condition for manufacturing industry with such motivation a cyber physical coupled model scheme has been developed the coupled model is a digital twin of the real machine that operates in the cloud platform and simulates the health condition with an integrated knowledge from both data driven analytical algorithms as well as other available physical knowledge it can also be described as a 5s systematic approach consisting of sensing storage synchronization synthesis and service the coupled model first constructs a digital image from the early design stage system information and physical knowledge are logged during product design based on which a simulation model is built as a reference for future analysis initial parameters may be statistically generalized and they can be tuned using data from testing or the manufacturing process using parameter estimation after that step the simulation model can be considered a mirrored image of the real machine able to continuously record and track machine condition during the later utilization stage finally with the increased connectivity offered by cloud computing technology the coupled model also provides better accessibility of machine condition for factory managers in cases where physical access to actual equipment or machine data is limited ref name mfgletters healthcare big data analytics has helped healthcare improve by providing personalized medicine and prescriptive analytics clinical risk intervention and predictive analytics waste and care variability reduction automated external and internal reporting of patient data standardized medical terms and patient registries and fragmented point solutions ref name ref135 cite journal doi 10 1016 j ijrobp 2015 10 060 title impending challenges for the use of big data ref some areas of improvement are more aspirational than actually implemented the level of data generated within healthcare systems is not trivial with the added adoption of mhealth ehealth and wearable technologies the volume of data will continue to increase this includes electronic health record data imaging data patient generated data sensor data and other forms of difficult to process data there is now an even greater need for such environments to pay greater attention to data and information quality ref cite journal url http doi acm org 10 1145 2378016 2378021 title data management within mhealth environments patient sensors mobile devices and databases first1 john last1 o donoghue first2 john last2 herbert date 1 october 2012 publisher volume 4 issue 1 pages 5 1 5 20 accessdate 16 june 2016 via acm digital library doi 10 1145 2378016 2378021 ref big data very often means dirty data and the fraction of data inaccuracies increases with data volume growth human inspection at the big data scale is impossible and there is a desperate need in health service for intelligent tools for accuracy and believability control and handling of information missed ref name mirkes2016 cite journal last1 mirkes first1 e m last2 coats first2 t j last3 levesley first3 j last4 gorban first4 a n title handling missing data in large healthcare dataset a case study of unknown trauma outcomes url https www researchgate net publication 300400110 handling missing data in large healthcare dataset a case study of unknown trauma outcomes journal computers in biology and medicine volume 75 issue pages 203 216 year 2016 doi 10 1016 j compbiomed 2016 06 004 ref while extensive information in healthcare is now electronic it fits under the big data umbrella as most is unstructured and difficult to use ref cite journal last murdoch first travis b last2 detsky first2 allan s date 2013 04 03 title the inevitable application of big data to health care url http jamanetwork com journals jama article abstract 1674245 journal jama language en volume 309 issue 13 doi 10 1001 jama 2013 393 issn 0098 7484 ref education a mckinsey company mckinsey global institute study found a shortage of 1 5 million highly trained data professionals and managers ref name mckinsey and a number of universities ref cite news url http www forbes com sites jmaureenhenderson 2013 07 30 degrees in big data fad or fast track to career success access date 2016 02 21 newspaper forbes title degrees in big data fad or fast track to career success ref including university of tennessee and uc berkeley have created masters programs to meet this demand private bootcamps have also developed programs to meet that demand including free programs like the data incubator or paid programs like general assembly ref cite news title ny gets new bootcamp for data scientists it s free but harder to get into than harvard newspaper venture beat access date 2016 02 21 url http venturebeat com 2014 04 15 ny gets new bootcamp for data scientists its free but harder to get into than harvard ref media to understand how the media utilises big data it is first necessary to provide some context into the mechanism used for media process it has been suggested by nick couldry and joseph turow that wikt practitioner practitioners in media and advertising approach big data as many actionable points of information about millions of individuals the industry appears to be moving away from the traditional approach of using specific media environments such as newspapers magazines or television shows and instead taps into consumers with technologies that reach targeted people at optimal times in optimal locations the ultimate aim is to serve or convey a message or content that is statistically speaking in line with the consumer s mindset for example publishing environments are increasingly tailoring messages advertisements and content articles to appeal to consumers that have been exclusively gleaned through various data mining activities ref cite journal last1 couldry first1 nick last2 turow first2 joseph title advertising big data and the clearance of the public realm marketers new approaches to the content subsidy journal international journal of communication date 2014 volume 8 pages 1710 1726 ref targeting of consumers for advertising by marketers data capture data journalism publishers and journalists use big data tools to provide unique and innovative insights and infographics internet of things iot tone section date september 2016 main article internet of things big data and the iot work in conjunction from a media perspective data is the key derivative of device inter connectivity and allows accurate targeting the internet of things with the help of big data therefore transforms the media industry companies and even governments opening up a new era of economic growth and competitiveness the intersection of people data and intelligent algorithms have far reaching impacts on media efficiency the wealth of data generated allows an elaborate layer on the present targeting mechanisms of the industry technology ebay com uses two data warehouses at 7 5 petabytes and 40pb as well as a 40pb hadoop cluster for search consumer recommendations and merchandising ref cite web last tay first liz url http www itnews com au news inside ebay8217s 90pb data warehouse 342615 title inside ebay s 90pb data warehouse publisher itnews accessdate 2016 02 12 ref amazon com handles millions of back end operations every day as well as queries from more than half a million third party sellers the core technology that keeps amazon running is linux based and as of 2005 they had the world s three largest linux databases with capacities of 7 8 tb 18 5 tb and 24 7 tb ref cite web last layton first julia url http money howstuffworks com amazon1 htm title amazon technology publisher money howstuffworks com accessdate 2013 03 05 ref facebook handles 50 nbsp billion photos from its user base ref cite web url https www facebook com notes facebook engineering scaling facebook to 500 million users and beyond 409881258919 title scaling facebook to 500 million users and beyond publisher facebook com accessdate 2013 07 21 ref as of august 2012 google was handling roughly 100 nbsp billion searches per month ref cite web url http searchengineland com google 1 trillion searches per year 212940 title google still doing at least 1 trillion searches per year date 16 january 2015 work search engine land accessdate 15 april 2015 ref oracle nosql database has been tested to past the 1m ops sec mark with 8 shards and proceeded to hit 1 2m ops sec with 10 shards ref cite web last lamb first charles url https blogs oracle com charleslamb entry oracle nosql database exceeds 1 title oracle nosql database exceeds 1 million mixed ycsb ops sec ref private sector information technology especially since 2015 big data has come to prominence within business operations as a tool to help employees work more efficiently and streamline the collection and distribution of information technology it the use of big data to attack it and data collection issues within an enterprise is called it operations analytics itoa ref name itoa1 cite web last1 solnik first1 ray title the time has come analytics delivers for it operations url http www datacenterjournal com time analytics delivers operations website data center journal accessdate june 21 2016 ref by applying big data principles into the concepts of machine intelligence and deep computing it departments can predict potential issues and move to provide solutions before the problems even happen ref name itoa1 in this time itoa businesses were also beginning to play a major role in systems management by offering platforms that brought individual data silos together and generated insights from the whole of the system rather than from isolated pockets of data retail walmart handles more than 1 million customer transactions every hour which are imported into databases estimated to contain more than 2 5 petabytes 2560 terabytes of data the equivalent of 167 times the information contained in all the books in the us library of congress r economist retail banking fico card detection system protects accounts worldwide ref name fico com cite web url http www fico com en products dmapps pages fico falcon fraud manager aspx title fico\xc2\xae falcon\xc2\xae fraud manager publisher fico com accessdate 2013 07 21 ref the volume of business data worldwide across all companies doubles every 1 2 years according to estimates ref name knowwpcarey com cite web url http research wpcarey asu edu managing it ebay study how to build trust and improve the shopping experience title ebay study how to build trust and improve the shopping experience publisher knowwpcarey com date 8 may 2012 accessdate 2015 12 20 ref ref http www statista com statistics 280444 global leading priorities for big data according to business and it executives leading priorities for big data for business and it emarketer october 2013 retrieved january 2014 ref real estate windermere real estate uses anonymous gps signals from nearly 100 million drivers to help new home buyers determine their typical drive times to and from work throughout various times of the day ref cite news last wingfield first nick url http bits blogs nytimes com 2013 03 12 predicting commutes more accurately for would be home buyers title predicting commutes more accurately for would be home buyers nytimes com publisher bits blogs nytimes com date 12 march 2013 accessdate 2013 07 21 ref science the large hadron collider experiments represent about 150 million sensors delivering data 40 nbsp million times per second there are nearly 600 nbsp million collisions per second after filtering and refraining from recording more than 99 99995 ref cite web last1 alexandru first1 dan title prof url https cds cern ch record 1504817 files cern thesis 2013 004 pdf website cds cern ch publisher cern accessdate 24 march 2015 ref of these streams there are 100 collisions of interest per second ref cite web title lhc brochure english version a presentation of the largest and the most powerful particle accelerator in the world the large hadron collider lhc which started up in 2008 its role characteristics technologies etc are explained for the general public url http cds cern ch record 1278169 ln en work cern brochure 2010 006 eng lhc brochure english version publisher cern accessdate 20 january 2013 ref ref cite web title lhc guide english version a collection of facts and figures about the large hadron collider lhc in the form of questions and answers url http cds cern ch record 1092437 ln en work cern brochure 2008 001 eng lhc guide english version publisher cern accessdate 20 january 2013 ref ref name nature cite news title high energy physics down the petabyte highway work nature date 19 january 2011 first geoff last brumfiel doi 10 1038 469282a volume 469 pages 282 83 url http www nature com news 2011 110119 full 469282a html ref as a result only working with less than 0 001 of the sensor stream data the data flow from all four lhc experiments represents 25 petabytes annual rate before replication as of 2012 this becomes nearly 200 petabytes after replication if all sensor data were recorded in lhc the data flow would be extremely hard to work with the data flow would exceed 150 million petabytes annual rate or nearly 500 exabyte s per day before replication to put the number in perspective this is equivalent to 500 quintillion 5\xc3\x9710 sup 20 sup bytes per day almost 200 times more than all the other sources combined in the world the square kilometre array is a radio telescope built of thousands of antennas it is expected to be operational by 2024 collectively these antennas are expected to gather 14 exabytes and store one petabyte per day ref http www zurich ibm com pdf astron cebit 202013 20background 20dome pdf ref ref cite web url http arstechnica com science 2012 04 future telescope array drives development of exabyte processing title future telescope array drives development of exabyte processing work ars technica accessdate 15 april 2015 ref it is considered one of the most ambitious scientific projects ever undertaken ref cite web url http theconversation com australias bid for the square kilometre array an insiders perspective 4891 title australia s bid for the square kilometre array an insider s perspective date 1 february 2012 publisher the conversation website the conversation accessdate 27 september 2016 ref science and research expand section date december 2016 when the sloan digital sky survey sdss began to collect astronomical data in 2000 it amassed more in its first few weeks than all data collected in the history of astronomy previously continuing at a rate of about 200 nbsp gb per night sdss has amassed more than 140 terabytes of information ref name economist cite news title data data everywhere url http www economist com node 15557443 newspaper the economist date 25 february 2010 accessdate 9 december 2012 ref when the large synoptic survey telescope successor to sdss comes online in 2020 its designers expect it to acquire that amount of data every five days r economist decoding the human genome project human genome originally took 10 years to process now it can be achieved in less than a day the dna sequencers have divided the sequencing cost by 10 000 in the last ten years which is 100 times cheaper than the reduction in cost predicted by moore s law ref http www oecd org sti ieconomy session 3 delort pdf page 6 delort p oecd iccp technology foresight forum 2012 ref the nasa center for climate simulation nccs stores 32 petabytes of climate observations and simulations on the discover supercomputing cluster ref cite web url http www nasa gov centers goddard news releases 2010 10 051 html title nasa nasa goddard introduces the nasa center for climate simulation publisher accessdate 13 april 2016 ref ref cite web last webster first phil title supercomputing the climate nasa s big data mission url http www csc com cscworld publications 81769 81773 supercomputing the climate nasa s big data mission work csc world publisher computer sciences corporation accessdate 2013 01 18 ref google s dnastack compiles and organizes dna samples of genetic data from around the world to identify diseases and other medical defects these fast and exact calculations eliminate any friction points or human errors that could be made by one of the numerous science and biology experts working with the dna dnastack a part of google genomics allows scientists to use the vast sample of resources from google s search server to scale social experiments that would usually take years instantly ref cite web url http www theglobeandmail com life health and fitness health these six great neuroscience ideas could make the leap from lab to market article21681731 title these six great neuroscience ideas could make the leap from lab to market date 20 november 2014 publisher the globe and mail accessdate 1 october 2016 ref ref cite web url https cloud google com customers dnastack title dnastack tackles massive complex dna datasets with google genomics publisher google cloud platform accessdate 1 october 2016 ref 23andme s dna database contains genetic information of over 1 000 000 people worldwide ref cite web title 23andme ancestry url https www 23andme com en int ancestry website 23andme com accessdate 29 december 2016 ref the company explores selling the anonymous aggregated genetic data to other researchers and pharmaceutical companies for research purposes if patients give their consent ref name verge1 cite web last1 potenza first1 alessandra title 23andme wants researchers to use its kits in a bid to expand its collection of genetic data url http www theverge com 2016 7 13 12166960 23andme genetic testing database genotyping research publisher the verge accessdate 29 december 2016 date 13 july 2016 ref ref cite web title this startup will sequence your dna so you can contribute to medical research url https www fastcompany com 3066775 innovation agents this startup will sequence your dna so you can contribute to medical resea publisher fast company accessdate 29 december 2016 date 23 december 2016 ref ref cite web last1 seife first1 charles title 23andme is terrifying but not for the reasons the fda thinks url https www scientificamerican com article 23andme is terrifying but not for the reasons the fda thinks publisher scientific american accessdate 29 december 2016 ref ref cite web last1 zaleski first1 andrew title this biotech start up is betting your genes will yield the next wonder drug url http www cnbc com 2016 06 22 23andme thinks your genes are the key to blockbuster drugs html publisher cnbc accessdate 29 december 2016 date 22 june 2016 ref ref cite web last1 regalado first1 antonio title how 23andme turned your dna into a 1 billion drug discovery machine url https www technologyreview com s 601506 23andme sells data for drug search publisher mit technology review accessdate 29 december 2016 ref ahmad hariri professor of psychology and neuroscience at duke university who has been using 23andme in his research since 2009 states that the most important aspect of the company s new service is that it makes genetic research accessible and relatively cheap for scientists ref name verge1 a study that identified 15 genome sites linked to depression in 23andme s database lead to a surge in demands to access the repository with 23andme fielding nearly 20 requests to access the depression data in the two weeks after publication of the paper ref cite web title 23andme reports jump in requests for data in wake of pfizer depression study fiercebiotech url http www fiercebiotech com it 23andme reports jump requests for data wake pfizer depression study website fiercebiotech com accessdate 29 december 2016 ref sports big data can be used to improve training and understanding competitors using sport sensors it is also possible to predict winners in a match using big data analytics ref cite web url http www itweb co za index php option com content view article id 147241 title data scientists predict springbok defeat author admire moyo work www itweb co za accessdate 12 december 2015 ref future performance of players could be predicted as well thus players value and salary is determined by data collected throughout the season ref cite web url http www itweb co za index php option com content view article id 147852 title predictive analytics big data transform sports author regina pazvakavambwa work www itweb co za accessdate 12 december 2015 ref the movie moneyball film moneyball demonstrates how big data could be used to scout players and also identify undervalued players ref cite web url http www datacenterknowledge com archives 2011 09 23 the lessons of moneyball for big data analysis title the lessons of moneyball for big data analysis author rich miller work www datecenterknowledge com accessdate 12 december 2015 ref in formula one races race cars with hundreds of sensors generate terabytes of data these sensors collect data points from tire pressure to fuel burn efficiency then this data is transferred to team headquarters in united kingdom through fiber optic cables that could carry data at the speed of light ref cite web url http www huffingtonpost com dave ryan sports where big data fin b 8553884 html title sports where big data finally makes sense author dave ryan work www huffingtonpost com accessdate 12 december 2015 ref based on the data engineers and data analysts decide whether adjustments should be made in order to win a race besides using big data race teams try to predict the time they will finish the race beforehand based on simulations using data collected over the season ref cite web url http www forbes com sites frankbi 2014 11 13 how formula one teams are using big data to get the inside edge title how formula one teams are using big data to get the inside edge author frank bi work www forbes com accessdate 12 december 2015 ref research activities encrypted search and cluster formation in big data was demonstrated in march 2014 at the american society of engineering education gautam siwach engaged at tackling the challenges of big data by mit computer science and artificial intelligence laboratory and dr amir esmailpour at unh research group investigated the key features of big data as formation of clusters and their interconnections they focused on the security of big data and the actual orientation of the term towards the presence of different type of data in an encrypted form at cloud interface by providing the raw definitions and real time examples within the technology moreover they proposed an approach for identifying the encoding technique to advance towards an expedited search over encrypted text leading to the security enhancements in big data ref cite conference url http asee ne org proceedings 2014 student 20papers 210 pdf title encrypted search cluster formation in big data last1 siwach first1 gautam last2 esmailpour first2 amir date march 2014 year conference asee 2014 zone i conference conference url http ubconferences org location university of bridgeport bridgeport connecticut us ref in march 2012 the white house announced a national big data initiative that consisted of six federal departments and agencies committing more than 200 nbsp million to big data research projects ref cite web title obama administration unveils big data initiative announces 200 million in new r d investments publisher the white house url http www whitehouse gov sites default files microsites ostp big data press release final 2 pdf ref the initiative included a national science foundation expeditions in computing grant of 10 million over 5 years to the amplab ref cite web url http amplab cs berkeley edu title amplab at the university of california berkeley publisher amplab cs berkeley edu accessdate 2013 03 05 ref at the university of california berkeley ref cite web title nsf leads federal efforts in big data date 29 march 2012 publisher national science foundation nsf url http www nsf gov news news summ jsp cntn id 123607 org nsf from news ref the amplab also received funds from darpa and over a dozen industrial sponsors and uses big data to attack a wide range of problems from predicting traffic congestion ref cite conference url https amplab cs berkeley edu publication scaling the mobile millennium system in the cloud 2 author1 timothy hunter date october 2011 author2 teodor moldovan author3 matei zaharia author4 justin ma author5 michael franklin author6 pieter abbeel author7 alexandre bayen title scaling the mobile millennium system in the cloud ref to fighting cancer ref cite news title computer scientists may have what it takes to help cure cancer author david patterson publisher the new york times date 5 december 2011 url http www nytimes com 2011 12 06 science david patterson enlist computer scientists in cancer fight html r 0 ref the white house big data initiative also included a commitment by the department of energy to provide 25 million in funding over 5 years to establish the scalable data management analysis and visualization sdav institute ref cite web title secretary chu announces new institute to help scientists improve massive data set research on doe supercomputers publisher energy gov url http energy gov articles secretary chu announces new institute help scientists improve massive data set research doe ref led by the energy department s lawrence berkeley national laboratory the sdav institute aims to bring together the expertise of six national laboratories and seven universities to develop new tools to help scientists manage and visualize data on the department s supercomputers the u s state of massachusetts announced the massachusetts big data initiative in may 2012 which provides funding from the state government and private companies to a variety of research institutions ref cite web title governor patrick announces new initiative to strengthen massachusetts position as a world leader in big data publisher commonwealth of massachusetts url http www mass gov governor pressoffice pressreleases 2012 2012530 governor announces big data initiative html ref the massachusetts institute of technology hosts the intel science and technology center for big data in the mit computer science and artificial intelligence laboratory combining government corporate and institutional funding and research efforts ref cite web url http bigdata csail mit edu title big data csail publisher bigdata csail mit edu date 22 february 2013 accessdate 2013 03 05 ref the european commission is funding the 2 year long big data public private forum through their seventh framework program to engage companies academics and other stakeholders in discussing big data issues the project aims to define a strategy in terms of research and innovation to guide supporting actions from the european commission in the successful implementation of the big data economy outcomes of this project will be used as input for horizon 2020 their next framework programmes for research and technological development framework program ref cite web url http cordis europa eu search index cfm fuseaction proj document pj rcn 13267529 title big data public private forum publisher cordis europa eu date 1 september 2012 accessdate 2013 03 05 ref the british government announced in march 2014 the founding of the alan turing institute named after the computer pioneer and code breaker which will focus on new ways to collect and analyse large data sets ref cite news url http www bbc co uk news technology 26651179 title alan turing institute to be set up to research big data publisher bbc news accessdate 2014 03 19 date 19 march 2014 ref at the university of waterloo stratford campus canadian open data experience code inspiration day participants demonstrated how using data visualization can increase the understanding and appeal of big data sets and communicate their story to the world ref cite web url http www betakit com event inspiration day at university of waterloo stratford campus title inspiration day at university of waterloo stratford campus publisher betakit com accessdate 2014 02 28 ref to make manufacturing more competitive in the united states and globe there is a need to integrate more american ingenuity and innovation into manufacturing therefore national science foundation has granted the industry university cooperative research center for intelligent maintenance systems ims at university of cincinnati to focus on developing advanced predictive tools and techniques to be applicable in a big data environment ref cite journal last lee first jay author2 lapira edzel author3 bagheri behrad author4 kao hung an title recent advances and trends in predictive manufacturing systems in big data environment journal manufacturing letters year 2013 volume 1 issue 1 url http www sciencedirect com science article pii s2213846313000114 doi 10 1016 j mfglet 2013 09 005 pages 38 41 ref in may 2013 ims center held an industry advisory board meeting focusing on big data where presenters from various industrial companies discussed their concerns issues and future goals in big data environment computational social sciences nbsp anyone can use application programming interfaces apis provided by big data holders such as google and twitter to do research in the social and behavioral sciences ref name pigdata cite journal last reips first ulf dietrich author2 matzat uwe title mining big data using big data services journal international journal of internet science year 2014 volume 1 issue 1 pages 1 8 url http www ijis net ijis9 1 ijis9 1 editorial pre html ref often these apis are provided for free ref name pigdata tobias preis et al used google trends data to demonstrate that internet users from countries with a higher per capita gross domestic product gdp are more likely to search for information about the future than information about the past the findings suggest there may be a link between online behaviour and real world economic indicators ref cite journal first1 tobias last1 preis first2 helen susannah last2 moat first3 h eugene last3 stanley first4 steven r last4 bishop title quantifying the advantage of looking forward journal scientific reports volume 2 page 350 year 2012 doi 10 1038 srep00350 pmid 22482034 pmc 3320057 ref ref cite web url http www newscientist com article dn21678 online searches for future linked to economic success html title online searches for future linked to economic success first paul last marks work new scientist date 5 april 2012 accessdate 9 april 2012 ref ref cite web url http arstechnica com gadgets news 2012 04 google trends reveals clues about the mentality of richer nations ars title google trends reveals clues about the mentality of richer nations first casey last johnston work ars technica date 6 april 2012 accessdate 9 april 2012 ref the authors of the study examined google queries logs made by ratio of the volume of searches for the coming year 2011 to the volume of searches for the previous year 2009 which they call the future orientation index ref cite web url http www tobiaspreis de bigdata future orientation index pdf title supplementary information the future orientation index is available for download author tobias preis date 24 may 2012 accessdate 2012 05 24 ref they compared the future orientation index to the per capita gdp of each country and found a strong tendency for countries where google users inquire more about the future to have a higher gdp the results hint that there may potentially be a relationship between the economic success of a country and the information seeking behavior of its citizens captured in big data tobias preis and his colleagues helen susannah moat and h eugene stanley introduced a method to identify online precursors for stock market moves using trading strategies based on search volume data provided by google trends ref cite web url http www nature com news counting google searches predicts market movements 1 12879 title counting google searches predicts market movements author philip ball work nature date 26 april 2013 accessdate 9 august 2013 ref their analysis of google search volume for 98 terms of varying financial relevance published in scientific reports ref cite journal author tobias preis helen susannah moat and h eugene stanley title quantifying trading behavior in financial markets using google trends journal scientific reports volume 3 pages 1684 year 2013 doi 10 1038 srep01684 pmid 23619126 pmc 3635219 ref suggests that increases in search volume for financially relevant search terms tend to precede large losses in financial markets ref cite news url http bits blogs nytimes com 2013 04 26 google search terms can predict stock market study finds title google search terms can predict stock market study finds author nick bilton work new york times date 26 april 2013 accessdate 9 august 2013 ref ref cite news url http business time com 2013 04 26 trouble with your investment portfolio google it title trouble with your investment portfolio google it author christopher matthews work time magazine date 26 april 2013 accessdate 9 august 2013 ref ref cite web url http www nature com news counting google searches predicts market movements 1 12879 title counting google searches predicts market movements author philip ball work nature journal nature date 26 april 2013 accessdate 9 august 2013 ref ref cite web url http www businessweek com articles 2013 04 25 big data researchers turn to google to beat the markets title big data researchers turn to google to beat the markets author bernhard warner work bloomberg businessweek date 25 april 2013 accessdate 9 august 2013 ref ref cite news url http www independent co uk news business comment hamish mcrae hamish mcrae need a valuable handle on investor sentiment google it 8590991 html title hamish mcrae need a valuable handle on investor sentiment google it author hamish mcrae work the independent date 28 april 2013 accessdate 9 august 2013 location london ref ref cite web url http www ft com intl cms s 0 e5d959b8 acf2 11e2 b27f 00144feabdc0 html title google search proves to be new word in stock market prediction author richard waters work financial times date 25 april 2013 accessdate 9 august 2013 ref ref cite news url http www forbes com sites davidleinweber 2013 04 26 big data gets bigger now google trends can predict the market title big data gets bigger now google trends can predict the market author david leinweber work forbes date 26 april 2013 accessdate 9 august 2013 ref ref cite news url http www bbc co uk news science environment 22293693 title google searches predict market moves author jason palmer work bbc date 25 april 2013 accessdate 9 august 2013 ref big data sets come with algorithmic challenges that previously did not exist hence there is a need to fundamentally change the processing ways ref e sejdi\xc4\x87 adapt current tools for use with big data nature vol vol 507 no 7492 pp 306 mar 2014 ref the workshops on algorithms for modern massive data sets mmds bring together computer scientists statisticians mathematicians and data analysis practitioners to discuss algorithmic challenges of big data ref stanford http web stanford edu group mmds mmds workshop on algorithms for modern massive data sets ref sampling big data an important research question that can be asked about big data sets is whether you need to look at the full data to draw certain conclusions about the properties of the data or is a sample good enough the name big data itself contains a term related to size and this is an important characteristic of big data but sampling statistics enables the selection of right data points from within the larger data set to estimate the characteristics of the whole population for example there are about 600 million tweets produced every day is it necessary to look at all of them to determine the topics that are discussed during the day is it necessary to look at all the tweets to determine the sentiment on each of the topics in manufacturing different types of sensory data such as acoustics vibration pressure current voltage and controller data are available at short time intervals to predict down time it may not be necessary to look at all the data but a sample may be sufficient big data can be broken down by various data point categories such as demographic psychographic behavioral and transactional data with large sets of data points marketers are able to create and utilize more customized segments of consumers for more strategic targeting there has been some work done in sampling algorithms for big data a theoretical formulation for sampling twitter data has been developed ref cite conference author1 deepan palguna author2 vikas joshi author3 venkatesan chakaravarthy author4 ravi kothari author5 l v subramaniam last author amp yes title analysis of sampling algorithms for twitter journal international joint conference on artificial intelligence year 2015 ref critique critiques of the big data paradigm come in two flavors those that question the implications of the approach itself and those that question the way it is currently done ref cite journal doi 10 1002 joe 21642 title big data and business intelligence debunking the myths journal global business and organizational excellence volume 35 issue 1 pages 23 34 year 2015 last1 kimble first1 c last2 milolidakis first2 g ref one approach to this criticism is the field of critical data studies critiques of the big data paradigm a crucial problem is that we do not know much about the underlying empirical micro processes that lead to the emergence of the se typical network characteristics of big data ref name editorial in their critique snijders matzat and ulf dietrich reips reips point out that often very strong assumptions are made about mathematical properties that may not at all reflect what is really going on at the level of micro processes mark graham has leveled broad critiques at chris anderson writer chris anderson s assertion that big data will spell the end of theory ref cite web url http www wired com science discoveries magazine 16 07 pb theory title the end of theory the data deluge makes the scientific method obsolete author chris anderson date 23 june 2008 work wired ref focusing in particular on the notion that big data must always be contextualized in their social economic and political contexts ref cite news author graham m title big data and the end of theory newspaper the guardian url https www theguardian com news datablog 2012 mar 09 big data theory location london date 9 march 2012 ref even as companies invest eight and nine figure sums to derive insight from information streaming in from suppliers and customers less than 40 of employees have sufficiently mature processes and skills to do so to overcome this insight deficit big data no matter how comprehensive or well analysed must be complemented by big judgment according to an article in the harvard business review ref cite web title good data won t guarantee good decisions harvard business review url http hbr org 2012 04 good data wont guarantee good decisions ar 1 work shah shvetank horne andrew capell\xc3\xa1 jaime publisher hbr org accessdate 8 september 2012 ref much in the same line it has been pointed out that the decisions based on the analysis of big data are inevitably informed by the world as it was in the past or at best as it currently is ref name hilbertbigdata2013 cite web url http papers ssrn com abstract 2205145 title big data for development from information to knowledge societies publisher ref fed by a large number of data on past experiences algorithms can predict future development if the future is similar to the past ref name hilberttedx https www youtube com watch v uxef6yfjzai big data requires big visions for big change hilbert m 2014 london tedxucl x independently organized ted talks ref if the systems dynamics of the future change if it is not a stationary process the past can say little about the future in order to make predictions in changing environments it would be necessary to have a thorough understanding of the systems dynamic which requires theory ref name hilberttedx as a response to this critique it has been suggested to combine big data approaches with computer simulations such as agent based model s ref name hilbertbigdata2013 and complex systems agent based models are increasingly getting better in predicting the outcome of social complexities of even unknown future scenarios through computer simulations that are based on a collection of mutually interdependent algorithms ref cite web url http www theatlantic com magazine archive 2002 04 seeing around corners 302471 title seeing around corners author jonathan rauch date 1 april 2002 work the atlantic ref ref epstein j m axtell r l 1996 growing artificial societies social science from the bottom up a bradford book ref in addition use of multivariate methods that probe for the latent structure of the data such as factor analysis and cluster analysis have proven useful as analytic approaches that go well beyond the bi variate approaches cross tabs typically employed with smaller data sets in health and biology conventional scientific approaches are based on experimentation for these approaches the limiting factor is the relevant data that can confirm or refute the initial hypothesis ref http www bigdataparis com documents pierre delort inserm pdf page 5 delort p big data in biosciences big data paris 2012 ref a new postulate is accepted now in biosciences the information provided by the data in huge volumes omics without prior hypothesis is complementary and sometimes necessary to conventional approaches based on experimentation ref cite web url http www cs cmu edu durand 03 711 2011 literature next gen genomics nrg 2010 pdf title next generation genomics an integrative approach date july 2010 publisher nature accessdate 18 october 2016 ref ref cite web url https www researchgate net publication 283298499 big data in biosciences title big data in biosciences date october 2015 publisher researchgate accessdate 18 october 2016 ref in the massive approaches it is the formulation of a relevant hypothesis to explain the data that is the limiting factor ref cite web url https next ft com content 21a6e7d8 b479 11e3 a09a 00144feabdc0 title big data are we making a big mistake date 28 march 2014 publisher financial times accessdate 20 october 2016 ref the search logic is reversed and the limits of induction glory of science and philosophy scandal c d broad 1926 are to be considered citation needed date april 2015 consumer privacy privacy advocates are concerned about the threat to privacy represented by increasing storage and integration of personally identifiable information expert panels have released various policy recommendations to conform practice to expectations of privacy ref cite web first paul last ohm title don t build a database of ruin publisher harvard business review url http blogs hbr org cs 2012 08 dont build a database of ruin html ref ref darwin bond graham http www counterpunch org 2013 12 03 iron cagebook iron cagebook the logical end of facebook s patents counterpunch org 2013 12 03 ref ref darwin bond graham http www counterpunch org 2013 09 11 inside the tech industrys startup conference inside the tech industry s startup conference counterpunch org 2013 09 11 ref critiques of big data execution big data has been called a fad in scientific research and its use was even made fun of as an absurd practice in a satirical example on pig data ref name pigdata researcher danah boyd has raised concerns about the use of big data in science neglecting principles such as choosing a sampling statistics representative sample by being too concerned about actually handling the huge amounts of data ref name danah cite web url http www danah org papers talks 2010 www2010 html title privacy and publicity in the context of big data author danah boyd work world wide web conference www 2010 conference date 29 april 2010 accessdate 2011 04 18 ref this approach may lead to results bias statistics bias in one way or another integration across heterogeneous data resources some that might be considered big data and others not presents formidable logistical as well as analytical challenges but many researchers argue that such integrations are likely to represent the most promising new frontiers in science ref cite journal last1 jones first1 mb last2 schildhauer first2 mp last3 reichman first3 oj last4 bowers first4 s title the new bioinformatics integrating ecological data from the gene to the biosphere journal annual review of ecology evolution and systematics volume 37 issue 1 pages 519 544 year 2006 doi 10 1146 annurev ecolsys 37 091305 110031 url http www pnamp org sites default files jones2006 arees pdf format pdf ref in the provocative article critical questions for big data ref name danah2 cite journal doi 10 1080 1369118x 2012 678878 title critical questions for big data journal information communication society volume 15 issue 5 pages 662 679 year 2012 last1 boyd first1 d last2 crawford first2 k ref the authors title big data a part of mythology large data sets offer a higher form of intelligence and knowledge with the aura of truth objectivity and accuracy users of big data are often lost in the sheer volume of numbers and working with big data is still subjective and what it quantifies does not necessarily have a closer claim on objective truth ref name danah2 recent developments in bi domain such as pro active reporting especially target improvements in usability of big data through automated filter software filtering of non useful data and correlations ref name big decisions white paper http www fortewares com administrator userfiles banner forte wares pro active reporting en pdf failure to launch from big data to big decisions forte wares ref big data analysis is often shallow compared to analysis of smaller data sets ref name kdnuggets berchthold cite web url http www kdnuggets com 2014 08 interview michael berthold knime research big data privacy part2 html title interview michael berthold knime founder on research creativity big data and privacy part 2 date 12 august 2014 author gregory piatetsky authorlink gregory i piatetsky shapiro publisher kdnuggets accessdate 2014 08 13 ref in many big data projects there is no large data analysis happening but the challenge is the extract transform load part of data preprocessing ref name kdnuggets berchthold big data is a buzzword and a vague term ref cite web last1 pelt first1 mason title big data is an over used buzzword and this twitter bot proves it url http siliconangle com blog 2015 10 26 big data is an over used buzzword and this twitter bot proves it website siliconangle com publisher siliconangle accessdate 4 november 2015 ref ref name ft harford cite web url http www ft com cms s 2 21a6e7d8 b479 11e3 a09a 00144feabdc0 html title big data are we making a big mistake last1 harford first1 tim date 28 march 2014 website financial times publisher financial times accessdate 2014 04 07 ref but at the same time an obsession ref name ft harford with entrepreneurs consultants scientists and the media big data showcases such as google flu trends failed to deliver good predictions in recent years overstating the flu outbreaks by a factor of two similarly academy awards and election predictions solely based on twitter were more often off than on target big data often poses the same challenges as small data and adding more data does not solve problems of bias but may emphasize other problems in particular data sources such as twitter are not representative of the overall population and results drawn from such sources may then lead to wrong conclusions google translate which is based on big data statistical analysis of text does a good job at translating web pages however results from specialized domains may be dramatically skewed on the other hand big data may also introduce new problems such as the multiple comparisons problem simultaneously testing a large set of hypotheses is likely to produce many false results that mistakenly appear significant ioannidis argued that most published research findings are false ref name ioannidis cite journal last1 ioannidis first1 j p a authorlink1 john p a ioannidis title why most published research findings are false journal plos medicine volume 2 issue 8 pages e124 year 2005 pmid 16060722 pmc 1182327 doi 10 1371 journal pmed 0020124 ref due to essentially the same effect when many scientific teams and researchers each perform many experiments i e process a big amount of scientific data although not with big data technology the likelihood of a significant result being actually false grows fast even more so when only positive results are published sorry this started overlapping with above section more and more merging is welcome i already dropped the intended subheadline hype cycle and inflated expectations furthermore big data analytics results are only as good as the model on which they are predicated in an example big data took part in attempting to predict the results of the 2016 u s presidential election ref cite news url http www nytimes com 2016 11 10 technology the data said clinton would win why you shouldnt have believed it html title how data failed us in calling an election last lohr first steve date 2016 11 10 last2 singer first2 natasha newspaper the new york times issn 0362 4331 access date 2016 11 27 ref with varying degrees of success forbes predicted if you believe in big data analytics it s time to begin planning for a hillary clinton presidency and all that entails ref cite news url http www forbes com sites jonmarkman 2016 08 08 big data and the 2016 election 4802f20846d7 title big data and the 2016 election last markman first jon newspaper forbes access date 2016 11 27 ref see also portal information technology category see also label for a list of companies and tools see also big data no companies or tool spam here that would be an endless list see also concepts not linked above big memory datafication data defined storage data journalism data lineage data philanthropy data science machine learning statistics small data urban informatics list of buzzwords references reflist 30em further reading cite magazine editors peter kinnaird inbal talgam cohen series xrds magazine xrds crossroads the acm magazine for students title big data issue 19 1 date 2012 publisher association for computing machinery issn 1528 4980 oclc 779657714 url http dl acm org citation cfm id 2331042 cite book title mining of massive datasets author1 jure leskovec author2 anand rajaraman author3 jeffrey d ullman year 2014 publisher cambridge university press url http mmds org isbn 9781107077232 oclc 888463433 cite book author1 viktor mayer sch\xc3\xb6nberger author2 kenneth cukier title big data a revolution that will transform how we live work and think date 2013 publisher houghton mifflin harcourt isbn 9781299903029 oclc 828620988 cite web url http www forbes com sites gilpress 2013 05 09 a very short history of big data title a very short history of big data first gil last press work forbes com date 2013 05 09 accessdate 2016 09 17 publisher forbes magazine location jersey city nj external links commonsinline wiktionary inline big data use dmy dates date december 2015 authority control category big data category data management category distributed computing problems category technology forecasting category transaction processing'
b'cleanup reorganize date june 2015 data availability ref http searchstorage techtarget com definition data availability ref is a term used by computer storage manufacturers and storage service providers ssps to describe products and services that ensure that data continues to be available at a required level of performance in situations ranging from normal through disastrous anytime a server loses power for example it has to reboot recover data and repair corrupted data the time it takes to recover known as the mean time to recover mtr could be minutes hours or days ref http blog schneider electric com datacenter 2012 10 12 understanding data center reliability availability and the cost of downtime ref data center standards the two organizations in the united states that publish data center standards are the telecommunications industry association tia and the uptime institute tia data center standards see wiki entry on tia 942 uptime institute data center tier standards tier i requirements ref http www firstcomm com overview of data center availability tiers ref single non redundant distribution path serving the it equipment non redundant capacity components basic site infrastructure with expected availability of 99 671 tier ii requirements meets or exceeds all tier i requirements redundant site infrastructure capacity components with expected availability of 99 741 tier iii requirements meets or exceeds all tier i and tier ii requirements multiple independent distribution paths serving the it equipment all it equipment must be dual powered and fully compatible with the topology of a site s architecture concurrently maintainable site infrastructure with expected availability of 99 982 tier iv requirements meets or exceeds all tier i tier ii and tier iii requirements all cooling equipment is independently dual powered including chillers and heating ventilating and air conditioning hvac systems fault tolerant site infrastructure with electrical power storage and distribution facilities with expected availability of 99 995 the uptime institute s tier system allows for the following minutes of downtime annually tier i 99 671 minimum uptime 1729 minutes maximum annual downtime tier ii 99 741 minimum uptime 1361 minutes maximum annual downtime tier iii 99 982 minimum uptime 95 minutes maximum annual downtime tier iv 99 995 minimum uptime 26 minutes maximum annual downtime see also data center tiers data center data center tiers data center tiers references reflist category data management category distributed data storage category distributed data storage systems'
b'redirect2 copyrighting copyrights the use of words to promote or advertise copywriting the wikipedia policy about copyright issues wikipedia copyrights pp move indef small yes intellectual property capitalism concepts use dmy dates date january 2011 use american english date january 2014 copyright is a natural and legal rights legal right created by the law of a country that grants the creator of an original work exclusive right s for its use and distribution this is usually only for a limited time the exclusive rights are not absolute but limited by limitations and exceptions to copyright law including fair use a major limitation on copyright is that copyright protects only the original expression of ideas and not the underlying ideas themselves ref cite web url http www bitlaw com copyright unprotected html ideas title works unprotected by copyright law publisher bitlaw author daniel a tysver ref ref cite web url http digital law online info lpdi1 0 treatise9 html title legal protection of digital information page chapter 1 an overview of copyright section ii e ideas versus expression author lee a hollaar ref copyright is a form of intellectual property applicable to certain forms of creative work some but not all jurisdictions require fixing copyrighted works in a tangible form it is often shared among multiple authors each of whom holds a set of rights to use or license the work and who are commonly referred to as rights holders ref citation title copyright publisher university of california year 2014 url http copyright universityofcalifornia edu ownership joint works html accessdate 2014 12 15 ref ref http www jetlaw org publish journal conventions ref ref https books google de books id kz1f6uahtaec pg pa81 dq 22rights holder 22 hl en sa x ved 0ahukewig4onuo87rahxqbcakhqgzad8q6aeihdaa v onepage q 22rights 20holder 22 f false ref ref https books google de books id xd ibwaaqbaj pg pt465 dq 22rights holder 22 hl en sa x ved 0ahukewig4onuo87rahxqbcakhqgzad8q6aeikdac v onepage q 22rights 20holder 22 f false ref these rights frequently include reproduction control over derivative work s distribution performing rights public performance and moral rights such as attribution ref citation title 17 u s c \xc2\xa7 106 publisher united states of america year 2011 url http www copyright gov title17 92chap1 html 106 accessdate 2014 12 15 ref copyrights are considered territorial rights which means that they do not extend beyond the territory of a specific jurisdiction while many aspects of national copyright laws have been standardized through international copyright agreements copyright laws vary by country ref name international copyright law survey cite web url http worldcopyrightlaw com copyrightsurvey title international copyright law survey publisher mincov law corporation ref typically the copyright term duration of a copyright spans the author s life plus 50 to 100 years that is copyright typically expires 50 to 100 years after the author dies depending on the jurisdiction some countries require certain copyright copyright formalities formalities to establishing copyright but most recognize copyright in any completed work without formal registration generally copyright is enforced as a civil law common law civil matter though some jurisdictions do apply criminal law criminal sanctions most jurisdictions recognize copyright limitations allowing fair exceptions to the creator s exclusivity of copyright and giving users certain rights the development of digital media and computer network technologies have prompted reinterpretation of these exceptions introduced new difficulties in enforcing copyright and inspired additional challenges to copyright law s philosophic basis simultaneously businesses with great economic dependence upon copyright such as those in the music business have advocated the extension and expansion of copyright and sought additional legal and technological enforcement history main history of copyright law background copyright came about with the invention of printing press the printing press and with wider literacy as a legal concept its origins in britain were from a reaction to printers monopolies at the beginning of the 18th nbsp century charles ii of england was concerned by the unregulated copying of books and passed the licensing of the press act 1662 by act of parliament ref copyright in historical perspective p 136 137 patterson 1968 vanderbilt univ press ref which established a register of licensed books and required a copy to be deposited with the worshipful company of stationers and newspaper makers stationers company essentially continuing the licensing of material that had long been in effect copyright laws allow products of creative human activities such as literary and artistic production to be preferentially exploited and thus incentivized different cultural attitudes social organizations economic models and legal frameworks are seen to account for why copyright emerged in europe and not for example in asia in the middle ages in europe there was generally a lack of any concept of literary property due to the general relations of production the specific organization of literary production and the role of culture in society the latter refers to the tendency of oral societies such as that of europe in the medieval period to view knowledge as the product and expression of the collective rather than to see it as individual property however with copyright laws intellectual production comes to be seen as a product of an individual with attendant rights the most significant point is that patent and copyright laws support the expansion of the range of creative human activities that can be commodified this parallels the ways in which capitalism led to the commodification of many aspects of social life that earlier had no monetary or economic value per nbsp se ref bettig ronald v 1996 copyrighting culture the political economy of intellectual property westview press p 9 17 isbn 0 8133 1385 6 ref copyright has grown from a legal concept regulating copying rights in the publishing of books and maps to one with a significant effect on nearly every modern industry covering such items as sound recording and reproduction sound recordings films photographs software and architectural works national copyrights see also statute of anne history of us copyright law file statute of anne jpg thumb left the statute of anne the copyright act 1709 came into force in 1710 often seen as the first real copyright law the 1709 british statute of anne gave the publishers rights for a fixed period after which the copyright expired ref name rethinking copyright history theory language cite book url https www google com books id dmyxq9v1jbqc dq statute of anne copyright lr as brr 3 source gbs navlinks s title rethinking copyright history theory language page 13 author ronan deazley isbn 978 1 84542 282 0 year 2006 publisher edward elgar publishing deadurl yes archiveurl https web archive org web 20111119042246 https www google com books id dmyxq9v1jbqc dq statute of anne copyright lr as brr 3 source gbs navlinks s archivedate 19 november 2011 ref the act also alluded to individual rights of the artist it began whereas printers booksellers and other persons have of late frequently taken the liberty of printing books and other writings without the consent of the authors to their very great detriment and too often to the ruin of them and their families ref cite web url http www copyrighthistory com anne html title statute of anne publisher copyrighthistory com accessdate 2012 06 08 ref a right to benefit financially from the work is articulated and court rulings and legislation have recognized a right to control the work such as ensuring that the integrity of it is preserved an irrevocable right to be recognized as the work s creator appears in some countries copyright laws the copyright clause of the united states constitution 1787 authorized copyright legislation to promote the progress of science and useful arts by securing for limited times to authors and inventors the exclusive right to their respective writings and discoveries that is by guaranteeing them a period of time in which they alone could profit from their works they would be enabled and encouraged to invest the time required to create them and this would be good for society as a whole a right to profit from the work has been the philosophical underpinning for much legislation extending the duration of copyright to the life of the creator and beyond to their heirs the original length of copyright in the united states was 14 nbsp years and it had to be explicitly applied for if the author wished they could apply for a second 14\xe2\x80\x91year monopoly grant but after that the work entered the public domain so it could be used and built upon by others copyright law was enacted rather copyright in germany late in german states and the historian eckhard h\xc3\xb6ffner argues that the absence of copyright laws in the early 19th century encouraged publishing was profitable for authors led to a proliferation of books enhanced knowledge and was ultimately an important factor in the ascendency of germany as a power during that century ref name thad cite web url http www spiegel de international zeitgeist 0 1518 710976 00 html author frank thadeusz title no copyright law the real reason for germany s industrial expansion publisher der spiegel date 18 august 2010 accessdate 11 april 2015 ref international copyright treaties see also international copyright agreements list of parties to international copyright agreements file joseph ferdinand keppler the pirate publisher puck magazine restoration by adam cuerden jpg the pirate publisher an international burlesque that has the longest run on record from puck magazine puck 1886 satirizes the then existing situation where a publisher could profit by simply stealing newly published works from one country and publishing them in another and vice versa thumb 300px the 1886 berne convention for the protection of literary and artistic works berne convention first established recognition of copyrights among sovereignty sovereign nations rather than merely bilaterally under the berne convention copyrights for creative works do not have to be asserted or declared as they are automatically in force at creation an author need not register or apply for a copyright in countries adhering to the berne convention ref name berne convention for the protection of literary and artistic works article 5 cite web url http www wipo int treaties en ip berne trtdocs wo001 html p109 16834 title berne convention for the protection of literary and artistic works article 5 accessdate 2011 11 18 publisher world intellectual property organization ref as soon as a work is fixed that is written or recorded on some physical medium its author is automatically entitled to all copyrights in the work and to any derivative works unless and until the author explicitly disclaims them or until the copyright expires the berne convention also resulted in foreign authors being treated equivalently to domestic authors in any country signed onto the convention the uk signed the berne convention in 1887 but did not implement large parts of it until 100 nbsp years later with the passage of the copyright designs and patents act of 1988 the united states did not sign the berne convention until 1989 ref garfinkle ann m fries janet lopez daniel possessky laura 1997 art conservation and the legal obligation to preserve artistic intent jaic 36 2 165 179 ref the united states and most latin america n countries instead entered into the buenos aires convention in 1910 which required a copyright notice on the work such as all rights reserved and permitted signatory nations to limit the duration of copyrights to shorter and renewable terms ref http www copyright gov circs circ38a pdf international copyright relations of the united states u s nbsp copyright office circular no nbsp 38a august nbsp 2003 ref ref http www unesco org culture copyright html eng ucc52ms pdf parties to the geneva act of the universal copyright convention as of 2000 01 01 the dates given in the document are dates of ratification not dates of coming into force the geneva act came into force on 16 september 1955 for the first twelve to have ratified which included four non members of the berne union as required by art nbsp 9 1 or three months after ratification for other countries webarchive url https web archive org web 20080625003242 http www unesco org culture copyright html eng ucc52ms pdf date 25 june 2008 ref ref http www copyright ht en 165 nbsp parties to the berne convention for the protection of literary and artistic works as of may 2012 ref the universal copyright convention was drafted in 1952 as another less demanding alternative to the berne convention and ratified by nations such as the soviet union and developing nations the regulations of the berne convention for the protection of literary and artistic works berne convention are incorporated into the world trade organization s agreement on trade related aspects of intellectual property rights trips agreement 1995 thus giving the berne convention effectively near global application ref name contemporary intellectual property law and policy cite book title contemporary intellectual property law and policy url https www google com books id iwcn4pt0ooc dq contemporary intellectual property source gbs navlinks s page 39 author1 macqueen hector l author2 charlotte waelde author3 graeme t laurie isbn 978 0 19 926339 4 year 2007 publisher oxford university press ref in 1961 the united international bureaux for the protection of intellectual property signed the rome convention for the protection of performers producers of phonograms and broadcasting organizations in 1996 this organization was succeeded by the founding of the world intellectual property organization which launched the 1996 wipo performances and phonograms treaty and the 2002 world intellectual property organization copyright treaty wipo copyright treaty which enacted greater restrictions on the use of technology to copy works in the nations that ratified it the trans pacific partnership includes trans pacific partnership intellectual property provisions intellectual property provisions relating to copyright copyright laws are standardized somewhat through these international conventions such as the berne convention for the protection of literary and artistic works berne convention and universal copyright convention these multilateral treaties have been ratified by nearly all countries and international organizations such as the european union or world trade organization require their member states to comply with them obtaining protection ownership the original holder of the copyright may be the employer of the author rather than the author himself if the work is a work for hire ref 17 u s c \xc2\xa7 201 b cmty for creative non violence v reid 490 u s 730 1989 ref for example in english law the copyright designs and patents act 1988 provides that if a copyrighted work is made by an employee in the course of that employment the copyright is automatically owned by the employer which would be a work for hire eligible works copyright may apply to a wide range of creative intellectual or artistic forms or works specifics vary by jurisdiction but these can include poem s theses drama plays and other book literary works film motion pictures choreography music musical compositions sound recording s painting s drawing s sculpture s photography photographs computer software radio and television broadcasting broadcasts and industrial design s graphic designs and industrial designs may have separate or overlapping laws applied to them in some jurisdictions ref name intellectual property and information wealth copyright and related rights cite book title intellectual property and information wealth copyright and related rights url https www google com books id tgk9bzcf5wgc dq statute of anne copyright lr as brr 3 source gbs navlinks s page 346 author peter k yu isbn 978 0 275 98883 8 year 2007 publisher greenwood publishing group ref ref cite web url http www wipo int freepublications en intproperty 909 wipo pub 909 pdf format pdf last world intellectual property organization title understanding copyright and related rights publisher wipo accessdate 11 august 2016 page 8 ref copyright does not cover ideas and information themselves only the form or manner in which they are expressed ref name art and copyright cite book title art and copyright url https www google com books id h xbqkiryaqc dq idea expression dichotomy lr as brr 3 source gbs navlinks s pages 48 49 author simon stokes isbn 978 1 84113 225 9 year 2001 publisher hart publishing ref for example the copyright to a mickey mouse cartoon restricts others from making copies of the cartoon or creating derivative work s based on the walt disney company disney s particular anthropomorphic mouse but does not prohibit the creation of other works about anthropomorphic mice in general so long as they are different enough to not be judged copies of disney s ref name art and copyright note additionally that mickey mouse is not copyrighted because characters cannot be copyrighted rather steamboat willie is copyrighted and mickey mouse as a character in that copyrighted work is afforded protection originality main threshold of originality typically a work must meet threshold of originality minimal standards of originality in order to qualify for copyright and the copyright expires after a set period of time some jurisdictions may allow this to be extended different countries impose different tests although generally the requirements are low in the united kingdom there has to be some skill labour and judgment that has gone into it ref express newspaper plc v news uk plc f s r 36 1991 ref in australia and the united kingdom it has been held that a single word is insufficient to comprise a copyright work however single words or a short string of words can sometimes be registered as a trademark instead copyright law recognizes the right of an author based on whether the work actually is an original creation rather than based on whether it is unique two authors may own copyright on two substantially identical works if it is determined that the duplication was coincidental and neither was copied from the other registration main copyright registration file fermat last theorem proof registered by ukraine officials jpg thumb right a copyright certificate for proof of the fermat theorem issued by the state department of intellectual property of ukraine in all countries where the berne convention for the protection of literary and artistic works berne convention standards apply copyright is automatic and need not be obtained through official registration with any government office once an idea has been reduced to tangible form for example by securing it in a fixed medium such as a drawing sheet music photograph a videotape or a computer file the copyright holder is entitled to enforce his or her exclusive rights ref name berne convention for the protection of literary and artistic works article 5 however while registration isn t needed to exercise copyright in jurisdictions where the laws provide for registration it serves as prima facie evidence of a valid copyright and enables the copyright holder to seek statutory damages for copyright infringement statutory damages and attorney s fees ref cite web title subject matter and scope of copyright url http copyright gov title17 92chap1 pdf website copyright gov accessdate 4 june 2015 ref in the usa registering after an infringement only enables one to receive actual damages and lost profits a widely circulated strategy to avoid the cost of copyright registration is referred to as the poor man s copyright it proposes that the creator send the work to himself in a sealed envelope by registered mail using the postmark to establish the date this technique has not been recognized in any published opinions of the united states courts note to editors the previously worded statement this technique has not been recognized by any united states court is overbroad because not all such cases are reported and it is impossible to know whether this is correct the united states copyright office says the technique is not a substitute for actual registration ref cite web title copyright in general faq url http www copyright gov help faq faq general html poorman publisher u s copyright office accessdate 11 aug 2016 ref the united kingdom intellectual property office discusses the technique and notes that the technique as well as commercial registries does not constitute dispositive proof that the work is original nor who the creator of the work is ref http www ipo gov uk copy c claim c register htm copyright registers united kingdom intellectual property office ref ref http www ipo gov uk types copy c about c auto htm automatic right united kingdom intellectual property office ref note to editors the previously worded statement the united kingdom intellectual property office discusses the technique but does not recommend its use overstates the uk ipo position the ipo does not recommend against the pmc approach fixing the berne convention allows member countries to decide whether creative works must be fixed to enjoy copyright article 2 section 2 of the berne convention states it shall be a matter for legislation in the countries of the union to prescribe that works in general or any specified categories of works shall not be protected unless they have been fixed in some material form some countries do not require that a work be produced in a particular form to obtain copyright protection for instance spain france and australia do not require fixation for copyright protection the united states and canada on the other hand require that most works must be fixed in a tangible medium of expression to obtain copyright protection ref name cyber law harvard edu see harvard law school http cyber law harvard edu copyrightforlibrarians module 3 the scope of copyright law fixation module 3 the scope of copyright law see also tyler t ochoa http digitalcommons law scu edu chtlj vol20 iss4 5 copyright derivative works and fixation is galoob a mirage or does the form gen of the alleged derivative work matter 20 smallcaps santa clara high tech l j 991 999 1002 2003 thus both the text of the act and its legislative history demonstrate that congress intended that a derivative work does not need to be fixed in order to infringe the legislative history of the 1976 copyright act says this difference was intended to address transitory works such as ballets pantomimes improvised performances dumb shows mime performances and dancing ref u s law requires that the fixation be stable and permanent enough to be perceived reproduced or communicated for a period of more than transitory duration similarly canadian courts consider fixation to require that the work be expressed to some extent at least in some material form capable of identification and having a more or less permanent endurance ref name cyber law harvard edu copyright notice main copyright notice file copyright svg thumb upright a copyright symbol used in copyright notice before 1989 united states law required the use of a copyright notice consisting of the copyright symbol \xc2\xa9 the letter c inside a circle the abbreviation copr or the word copyright followed by the year of the first publication of the work and the name of the copyright holder ref copyright act of 1976 uspl 94 553 90 stat 2541 \xc2\xa7 401 a 19 october 1976 ref ref the berne convention implementation act of 1988 bcia uspl 100 568 102 stat 2853 2857 one of the changes introduced by the bcia was to section nbsp 401 which governs copyright notices on published copies specifying that notices may be placed on such copies prior to the bcia the statute read that notices shall be placed on all such copies an analogous change was made in section nbsp 402 dealing with copyright notices on phonorecords ref several years may be noted if the work has gone through substantial revisions the proper copyright notice for sound recordings of musical or other audio works is a sound recording copyright symbol \xe2\x84\x97 the letter nbsp p inside a circle which indicates a sound recording copyright with the letter nbsp p indicating a phonorecord in addition the phrase all rights reserved was once required to assert copyright but that phrase is now legally obsolete in 1989 the united states enacted the berne convention for the protection of literary and artistic works berne convention implementation act amending the 1976 nbsp copyright act to conform to most of the provisions of the berne convention as a result the use of copyright notices has become optional to claim copyright because the berne convention makes copyright automatic ref cite web url http www copyright gov circs circ03 pdf title u s copyright office information circular format pdf accessdate 2012 07 07 ref however the lack of notice of copyright using these marks may have consequences in terms of reduced damages in an infringement lawsuit nbsp using notices of this form may reduce the likelihood of a defense of innocent infringement being successful ref 17 u s c unitedstatescodesec 17 401 d ref enforcement copyrights are generally enforced by the holder in a civil law private law civil law court but there are also criminal infringement statutes in some jurisdictions while copyright registry central registries are kept in some countries which aid in proving claims of ownership registering does not necessarily prove ownership nor does the fact of copying even without permission necessarily legal proof prove that copyright was infringed criminal sanctions are generally aimed at serious counterfeiting activity but are now becoming more commonplace as copyright collectives such as the riaa are increasingly targeting the file sharing home internet user thus far however most such cases against file sharers have been settled out of court see legal aspects of file sharing in most jurisdictions the copyright holder must bear the cost of enforcing copyright this will usually involve engaging legal representation administrative and or court costs in light of this many copyright disputes are settled by a direct approach to the infringing party in order to settle the dispute out of court copyright infringement main copyright infringement for a work to be considered to infringe upon copyright its use must have occurred in a nation that has domestic copyright laws and or adheres to a bilateral treaty or established international convention such as the berne convention for the protection of literary and artistic works berne convention or world intellectual property organization copyright treaty wipo copyright treaty improper use of materials outside of legislation is deemed unauthorized edition not copyright infringement ref cite journal last1 owen first1 l doi 10 1087 09531510125100313 title piracy journal learned publishing volume 14 pages 67 70 year 2001 pmid pmc ref copyright infringement most often occurs to software film and music however infringement upon books and other text works remains common especially for educational reasons statistics regarding the effects of copyright infringement are difficult to determine studies have attempted to determine whether there is a monetary loss for industries affected by copyright infringement by predicting what portion of pirated works would have been formally purchased if they had not been freely available ref butler s piracy losses billboard 199 36 ref other reports indicate that copyright infringement does not have an adverse effect on the entertainment industry and can have a positive effect ref cite web url http www ejpd admin ch content ejpd de home dokumentation mi 2011 2011 11 30 html title urheberrechtsverletzungen im internet der bestehende rechtliche rahmen gen\xc3\xbcgt publisher ejpd admin ch ref in particular a 2014 university study concluded that free music content accessed on youtube does not necessarily hurt sales instead has the potential to increase sales ref cite journal publisher social science electronic publishing url http papers ssrn com sol3 papers cfm abstract id 2425386 title video killed the radio star online music videos and digital music sales issn 2042 2695 year 2014 authors tobias kretschmer christian peukert ref rights granted exclusive rights several exclusive rights typically attach to the holder of a copyright to produce copies or reproductions of the work and to sell those copies including typically electronic copies to import or export the work to create derivative work s works that adapt the original work to perform or display the work publicly to sell or cede these rights to others to transmit or display by radio or video ref name autogenerated1 cite book title intellectual property and information wealth copyright and related rights page 346 author peter k yu isbn 978 0 275 98883 8 year 2007 publisher greenwood publishing group ref the phrase exclusive right means that only the copyright holder is free to exercise those rights and others are prohibited from using the work without the holder s permission copyright is sometimes called a negative right as it serves to prohibit certain people e g readers viewers or listeners and primarily publishers and would be publishers from doing something they would otherwise be able to do rather than permitting people e g authors to do something they would otherwise be unable to do in this way it is similar to the unregistered design right in english law and european law the rights of the copyright holder also permit him her to not use or exploit their copyright for some or all of the term there is however a critique which rejects this assertion as being based on a philosophy of copyright philosophical interpretation of copyright law that is not universally shared there is also debate on whether copyright should be considered a property right or a moral rights copyright law moral right ref tom g palmer http www tomgpalmer com wp content uploads papers morallyjustified pdf are patents and copyrights morally justified accessed 5 february 2013 ref if a pictorial graphic or sculptural work is a useful article it is copyrighted only if its aesthetic features are separable from its utilitarian features a useful article is an article having an intrinsic utilitarian function that is not merely to portray the appearance of the article or to convey information they must be separable from the functional aspect to be copyrighted ref name u s copyright office copyright law chapter 1 cite web url http www copyright gov title17 92chap1 pdf title u s copyright office copyright law chapter 1 accessdate 2012 06 27 ref duration this section is linked from little nemo main copyright term list of countries copyright length file tom bell s graph showing extension of u s copyright term over time svg thumb 300px expansion of u s copyright law currently based on the date of creation or publication copyright subsists for a variety of lengths in different jurisdictions the length of the term can depend on several factors including the type of work e g musical composition novel whether the work has been publication published and whether the work was created by an individual or a corporation in most of the world the default length of copyright is the life of the author plus either 50 or 70 years in the united states the term for most existing works is a fixed number of years after the date of creation or publication under most countries laws for example the united states ref usc 17 305 ref and the united kingdom ref the duration of copyright and rights in performances regulations 1995 http www opsi gov uk si si1995 uksi 19953297 en 3 htm part ii amendments of the uk copyright designs and patents act 1988 ref copyrights expire at the end of the calendar year in question the length and requirements for copyright duration are subject to change by legislation and since the early 20th century there have been a number of adjustments made in various countries which can make determining the duration of a given copyright somewhat difficult for example the united states used to require copyrights to be renewed after 28 years to stay in force and formerly required a copyright notice upon first publication to gain coverage in italy and france there were post wartime extensions that could increase the term by approximately 6 years in italy and up to about 14 in france many countries have extended the length of their copyright terms sometimes retroactively international treaties establish minimum terms for copyrights but individual countries may enforce longer terms than those ref cite book title copyright sacred text technology and the dmca last nimmer first david publisher kluwer law international year 2003 isbn 978 90 411 8876 2 oclc 50606064 page 63 url https books google com books id ryfrcnxgpo4c ref in the united states all books and other works published before 1923 have expired copyrights and are in the public domain ref http copyright cornell edu resources publicdomain cfm copyright term and the public domain in the united states cornell university ref in addition works published before 1964 that did not have their copyrights renewed 28 years after first publication year also are in the public domain hirtle points out that the great majority of these works including 93 of the books were not renewed after 28 years and are in the public domain ref see peter b hirtle copyright term and the public domain in the united states 1 january 2015 https copyright cornell edu resources publicdomain cfm online at footnote 8 ref books originally published outside the us by non americans are exempt from this renewal requirement if they are still under copyright in their home country but if the intended exploitation of the work includes publication or distribution of derivative work such as a film based on a book protected by copyright outside the u s the terms of copyright around the world must be considered if the author has been dead more than 70 years the work is in the public domain in most but not all countries in 1998 the length of a copyright in the united states was increased by 20 years under the copyright term extension act this legislation was strongly promoted by corporations which had valuable copyrights which otherwise would have expired and has been the subject of substantial criticism on this point ref lawrence lessig copyright s first amendment 48 ucla l rev 1057 1065 2001 ref globalize us date september 2016 limitations and exceptions main limitations and exceptions to copyright traditional safety valves in many jurisdictions copyright law makes exceptions to these restrictions when the work is copied for the purpose of commentary or other related uses it should be noted that us copyright does not cover names title short phrases or listings such as ingredients recipes labels or formulas ref http copyright gov circs circ34 pdf 2012 copyright protection not available for names titles or short phrases u s copyright office ref however there are protections available for those areas copyright does not cover such as trademark s and patent s there are some exceptions to what copyright will protect copyright will not protect names of products names of businesses organizations or groups pseudonyms of individuals titles of works catchwords catchphrases mottoes slogans or short advertising expressions listings of ingredients in recipes labels and formulas though the directions can be copyrighted idea expression dichotomy and the merger doctrine main idea expression divide the idea expression divide differentiates between ideas and expression and states that copyright protects only the original expression of ideas and not the ideas themselves this principle first clarified in the 1879 case of baker v selden has since been codified by the copyright act of 1976 at 17 u s c \xc2\xa7 102 b the first sale doctrine and exhaustion of rights main first sale doctrine exhaustion of rights copyright law does not restrict the owner of a copy from reselling legitimately obtained copies of copyrighted works provided that those copies were originally produced by or with the permission of the copyright holder it is therefore legal for example to resell a copyrighted book or compact disc cd in the united states this is known as the first sale doctrine and was established by the court s to clarify the legality of reselling books in second hand bookstore s some countries may have parallel importation restrictions that allow the copyright holder to control the aftermarket merchandise aftermarket this may mean for example that a copy of a book that does not infringe copyright in the country where it was printed does infringe copyright in a country into which it is imported for retailing the first sale doctrine is known as exhaustion of rights in other countries and is a principle which also applies though somewhat differently to patent and trademark rights it is important to note that the first sale doctrine permits the transfer of the particular legitimate copy involved it does not permit making or distributing additional copies in kirtsaeng v john wiley sons inc ref cite web title john wiley sons inc v kirtsaeng url http www supremecourt gov opinions 12pdf 11 697 d1o2 pdf ref in 2013 the united states supreme court held in a 6 3 decision that the first sale doctrine applies to goods manufactured abroad with the copyright owner s permission and then imported into the us without such permission the case involved a plaintiff who imported asian editions of textbooks that had been manufactured abroad with the publisher plaintiff s permission the defendant without permission from the publisher imported the textbooks and resold on ebay the supreme court s holding severely limits the ability of copyright holders to prevent such importation in addition copyright in most cases does not prohibit one from acts such as modifying defacing or destroying his or her own legitimately obtained copy of a copyrighted work so long as duplication is not involved however in countries that implement moral rights copyright law moral rights a copyright holder can in some cases successfully prevent the mutilation or destruction of a work that is publicly visible fair use and fair dealing main fair use fair dealing copyright does not prohibit all copying or replication in the united states the fair use fair use doctrine codified by the united states copyright act of 1976 copyright act of 1976 as 17 u s c section 107 permits some copying and distribution without permission of the copyright holder or payment to same the statute does not clearly define fair use but instead gives four non exclusive factors to consider in a fair use analysis those factors are the purpose and character of one s use the nature of the copyrighted work what amount and proportion of the whole work was taken and the effect of the use upon the potential market for or value of the copyrighted work ref cite web url http www4 law cornell edu uscode 17 107 html title us code title 17 107 limitations on exclusive rights fair use publisher law cornell edu date 2009 05 20 accessdate 2009 06 16 ref in the united kingdom and many other commonwealth of nations commonwealth countries a similar notion of fair dealing was established by the court s or through legislation the concept is sometimes not well defined however in canada private copying for personal use has been expressly permitted by statute since 1999 in alberta education v canadian copyright licensing agency access copyright 2012 scc 37 the supreme court of canada concluded that limited copying for educational purposes could also be justified under the fair dealing exemption in australia the fair dealing exceptions under the copyright act 1968 cth are a limited set of circumstances under which copyrighted material can be legally copied or adapted without the copyright holder s consent fair dealing uses are research and study review and critique news reportage and the giving of professional advice i e legal advice under current law of australia australian law although it is still a breach of copyright to copy reproduce or adapt copyright material for personal or private use without permission from the copyright owner owners of a legitimate copy are permitted to format shift that work from one medium to another for personal private use or to time shift a broadcast work for later once and only once viewing or listening other technical exemptions from infringement may also apply such as the temporary reproduction of a work in machine readable form for a computer in the united states the ahra audio home recording act codified in section 10 1992 prohibits action against consumers making noncommercial recordings of music in return for royalties on both media and devices plus mandatory copy control mechanisms on recorders section 1008 prohibition on certain infringement actions no action may be brought under this title alleging infringement of copyright based on the manufacture importation or distribution of a digital audio recording device a digital audio recording medium an analog recording device or an analog recording medium or based on the noncommercial use by a consumer of such a device or medium for making digital musical recordings or analog musical recordings later acts amended us copyright law so that for certain purposes making 10 copies or more is construed to be commercial but there is no general rule permitting such copying indeed making one complete copy of a work or in many cases using a portion of it for commercial purposes will not be considered fair use the digital millennium copyright act prohibits the manufacture importation or distribution of devices whose intended use or only significant commercial use is to bypass an access or copy control put in place by a copyright owner ref name intellectual property and information wealth copyright and related rights an appellate court has held that fair use is not a defense to engaging in such distribution the copyright directive allows eu member states to implement a set of exceptions to copyright examples of those exceptions are photographic reproductions on paper or any similar medium of works excluding sheet music provided that the rightholders receives fair compensation reproduction made by libraries educational establishments museums or archives which are non commercial archival reproductions of broadcasts uses for the benefit of people with a disability for demonstration or repair of equipment for non commercial research or private study when used in parody accessible copies it is legal in several countries including the united kingdom and the united states to produce alternative versions for example in large print or braille of a copyrighted work to provide improved access to a work for blind and visually impaired persons without permission from the copyright holder ref http www copyright gov title17 92chap1 html 121 copyright law of the usa chapter 1 section 121 ref ref cite web url http www rnib org uk xpedio groups public documents publicwebsite public cvipsact2002 hcsp title copyright visually impaired persons act 2002 comes into force publisher royal national institute of blind people date 1 january 2011 accessdate 11 aug 2016 ref anchor transfer and licensing and assignment transfer assignment and licensing see also collective rights management extended collective licensing compulsory license copyright transfer agreement file all rights reserved jpg thumb right 300px dvd all rights reserved a copyright or aspects of it e g reproduction alone all but moral rights may be assigned or transferred from one party to another ref name wipo guide on the licensing of copyright and related rights cite book title wipo guide on the licensing of copyright and related rights url https www google com books id lvrrvxbii8mc dq copyright transfer and licensing as brr 3 source gbs navlinks s isbn 978 92 805 1271 7 year 2004 publisher world intellectual property organization page 15 ref for example a musician who records an album will often sign an agreement with a record company in which the musician agrees to transfer all copyright in the recordings in exchange for royalties and other considerations the creator and original copyright holder benefits or expects to from production and marketing capabilities far beyond those of the author in the digital age of music music may be copied and distributed at minimal cost through the internet however the record industry attempts to provide promotion and marketing for the artist and his or her work so it can reach a much larger audience a copyright holder need not transfer all rights completely though many publishers will insist some of the rights may be transferred or else the copyright holder may grant another party a non exclusive license to copy and or distribute the work in a particular region or for a specified period of time a transfer or licence may have to meet particular formal requirements in order to be effective ref name wipo guide on the licensing of copyright and related rights 2 cite book title wipo guide on the licensing of copyright and related rights url https www google com books id lvrrvxbii8mc dq copyright transfer and licensing as brr 3 source gbs navlinks s page 8 isbn 978 92 805 1271 7 year 2004 publisher world intellectual property organization ref for example under the australian copyright law of australia copyright act 1968 copyright act 1968 the copyright itself must be expressly transferred in writing under the u s copyright act a transfer of ownership in copyright must be memorialized in a writing signed by the transferor for that purpose ownership in copyright includes exclusive licenses of rights thus exclusive licenses to be effective must be granted in a written instrument signed by the grantor no special form of transfer or grant is required a simple document that identifies the work involved and the rights being granted is sufficient non exclusive grants often called non exclusive licenses need not be in writing under law of the united states u s law they can be oral or even implied by the behavior of the parties transfers of copyright ownership including exclusive licenses may and should be recorded in the u s copyright office information on recording transfers is available on the office s web site while recording is not required to make the grant effective it offers important benefits much like those obtained by recording a deed in a real estate transaction copyright may also be license d ref name wipo guide on the licensing of copyright and related rights some jurisdictions may provide that certain classes of copyrighted works be made available under a prescribed statutory license e g musical works in the united states used for radio broadcast or performance this is also called a compulsory license because under this scheme anyone who wishes to copy a covered work does not need the permission of the copyright holder but instead merely files the proper notice and pays a set fee established by statute or by an agency decision under statutory guidance for every copy made ref name wipo guide on the licensing of copyright and related rights 3 cite book title wipo guide on the licensing of copyright and related rights url https www google com books id lvrrvxbii8mc dq copyright transfer and licensing as brr 3 source gbs navlinks s page 16 isbn 978 92 805 1271 7 year 2004 publisher world intellectual property organization ref failure to follow the proper procedures would place the copier at risk of an infringement suit because of the difficulty of following every individual work copyright collective s or collecting societies and performance rights organisation performing rights organizations such as ascap broadcast music incorporated bmi and sesac have been formed to collect royalties for hundreds thousands and more works at once though this market solution bypasses the statutory license the availability of the statutory fee still helps dictate the price per work collective rights organizations charge driving it down to what avoidance of procedural hassle would justify free licences copyright licenses known as open or free license s seek to grant several rights to licensees either for a fee or not to an effect inspired by the public domain free in this context isn t much of a reference to price as it is to freedom what constitutes free licensing has been characterised in a number of similar definitions including by order of longevity the free software definition the debian free software guidelines the open source definition and the definition of free cultural works further refinements to these licenses have resulted in categories such as copyleft and permissive license permissive common examples of free licences are the gnu general public license bsd license s and some creative commons licenses founded in 2001 by james boyle academic james boyle lawrence lessig and hal abelson the creative commons cc is a non profit organization ref name cc cite web url http creativecommons org title creative commons website website creativecommons org accessdate 24 october 2011 ref which aims to facilitate the legal sharing of creative works to this end the organization provides a number of generic copyright license options to the public gratis versus libre gratis these licenses allow copyright holders to define conditions under which others may use a work and to specify what types of use are acceptable ref name cc terms of use have traditionally been negotiated on an individual basis between copyright holder and potential licensee therefore a general cc license outlining which rights the copyright holder is willing to waive enables the general public to use such works more freely six general types of cc licenses are available although some of them aren t properly free per the above definitions and per creative commons own advice these are based upon copyright holder stipulations such as whether he or she is willing to allow modifications to the work whether he or she permits the creation of derivative works and whether he or she is willing to permit commercial use of the work ref name rubin rubin r e 2010 foundations of library and information science third edition neal schuman publishers inc new york p 341 ref as of 2009 approximately 130 million individuals had received such licenses ref name rubin criticism some sources are critical of particular aspects of the copyright system this is known as a debate over copynorms particularly on the internet there is discussion about the copyright aspects of downloading and streaming the copyright aspects of hyperlinking and framing such concerns are often couched in the language of digital rights and database right s discussions include free culture book free culture a 2004 book by lawrence lessig lessig coined the term permission culture to describe a worst case system good copy bad copy documentary and rip a remix manifesto discuss copyright some suggest an alternative compensation system some groups reject copyright altogether taking an anti copyright stance the perceived inability to enforce copyright online leads some to advocate crypto anarchism ignoring legal statutes when on the web public domain main public domain copyright like other intellectual property rights is subject to a statutorily determined term once the term of a copyright has expired the formerly copyrighted work enters the public domain and may be freely used or exploited by anyone courts in common law countries such as the united states and the united kingdom have rejected the doctrine of a common law copyright public domain works should not be confused with works that are publicly available works posted in the internet for example are publicly available but are not generally in the public domain copying such works may therefore violate the author s copyright see also portal social and political philosophy law colbegin colwidth 15em adelphi charter artificial scarcity conflict of laws copyright alliance copyright in architecture in the united states copyright on the content of patents and in the context of patent prosecution copyright for creativity copyright infringement of software copyright on religious works creative barcode digital rights management digital watermarking entertainment law freedom of panorama intellectual property education intellectual property protection of typefaces list of copyright acts list of copyright case law model release paracopyright photography and the law pirate party private copying levy production music rent seeking reproduction fees samizdat software copyright threshold pledge system colend references reflist 30em further reading refbegin 30em cite book author dowd raymond j title copyright litigation handbook publisher thomson west edition 1st year 2006 isbn 0 314 96279 4 ref dowd litigation handbook ellis sara r copyrighting couture an examination of fashion design protection and why the dppa and idpppa are a step towards the solution to counterfeit chic 78 tenn l rev 163 2010 available at http ssrn com abstract 1735745 cite book author1 gantz john author2 rochester jack b title pirates of the digital millennium publisher financial times prentice hall year 2005 isbn 0 13 146315 2 ref gantz pirates shuman ghosemajumder ghosemajumder shuman http dspace mit edu handle 1721 1 8438 advanced peer based technology business models mit sloan school of management 2002 bruce lehman lehman bruce intellectual property and the national information infrastructure report of the working group on intellectual property rights 1995 lindsey marc copyright law on campus washington state university press 2003 isbn 978 0 87422 264 7 mazzone jason copyfraud http ssrn com abstract 787244 ssrn mcdonagh luke is creative use of musical works without a licence acceptable under copyright international review of intellectual property and competition law iic 4 2012 401 426 available at http papers ssrn com sol3 papers cfm abstract id 2521081 ssrn cite book last nimmer first melville authorlink melville nimmer author2 david nimmer title nimmer on copyright publisher matthew bender year 1997 isbn 0 8205 1465 9 cite book title copyright in historical perspective author patterson lyman ray year 1968 publisher vanderbilt university press isbn 0 8265 1373 5 version online version rife by martine courant convention copyright and digital writing southern illinois university press 2013 222 pages examines legal pedagogical and other aspects of online authorship cite book last rosen first ronald title music and copyright publisher oxford university press location oxford oxfordshire year 2008 isbn 0 19 533836 7 shipley david e http ssrn com abstract 1076789 thin but not anorexic copyright protection for compilations and other fact works uga legal studies research paper no 08 001 journal of intellectual property law vol 15 no 1 2007 silverthorne sean http hbswk hbs edu item jhtml id 4206 t innovation music downloads pirates or customers harvard business school harvard business school working knowledge 2004 sorce keller marcello originality authenticity and copyright sonus vii 2007 no 2 pp nbsp 77 85 cite book author1 steinberg s h author2 trevitt john title five hundred years of printing location london and new castle publisher the british library and oak knoll press edition 4th year 1996 isbn 1 884718 19 1 ref steinberg five hundred years cite book title the copy south dossier issues in the economics politics and ideology of copyright in the global south url http copysouth org en documents csdossier pdf editor1 story alan editor2 darch colin editor3 halbert deborah year 2006 publisher copy south research group isbn 978 0 9553140 1 8 http whynotaskme org whynotaskme org organization demanding democratic participation in copyright legislation and a moratorium on secret and fast tracked copyright negotiations refend external links wikisource1911enc copyright wikisource wikisource copyright law copyright law spoken wikipedia en copyright ogg 2008 12 30 library resources box wikiquote inline commons inline copyright dmoz society law legal information intellectual property copyrights http www wipo int clea en collection of laws for electronic access from wipo intellectual property laws of many countries http purl fdlp gov gpo gpo55676 compendium of copyright practices 3rd ed united states copyright office http ucblibraries colorado edu govpubs us copyrite htm copyright from ucb libraries govpubs http www ipo gov uk types copy htm about copyright at the uk intellectual property office http www lawtech jus unitn it index php copyright history bibliography a bibliography on the origins of copyright and droit d auteur http ocw mit edu courses electrical engineering and computer science 6 912 introduction to copyright law january iap 2006 6 912 introduction to copyright law taught by keith winstein mit opencourseware january iap 2006 http www wipo int treaties en showresults jsp country id all start year any end year any search what c treaty id 15 copyright berne convention country list list of the 164 members of the berne convention for the protection of literary and artistic works http www copyrightservice co uk copyright p01 uk copyright law uk copyright law fact sheet april 2000 a concise introduction to uk copyright legislation http www jisc ac uk whatwedo themes content contentalliance reports ipr aspx ipr toolkit an overview key issues and toolkit elements september 2009 by professor charles oppenheim and naomi korn at the http www jisc ac uk whatwedo themes content contentalliance aspx strategic content alliance http ocw mit edu courses electrical engineering and computer science 6 912 introduction to copyright law january iap 2006 mit opencourseware 6 912 introduction to copyright law free self study course with video lectures as offered during the january 2006 independent activities period iap http www loc gov rr rarebook coll 067 html early copyright records from the http www loc gov rr rarebook rare book and special collections division at the library of congress http copyright gov title17 copyright law of the united states documents us government copyright law by country intellectual property activism authority control category copyright law category data management category intellectual property law category metadata category monopoly economics category public records'
b'other uses2 data processing refimprove date july 2013 a data processing system is a combination of machines data processing is specific to machines there is no data processing in nature see the oed people and processes that for a set of inputs produces a defined set of outputs ref the first machines used for data processing were unit record equipment punched card machines now computer s are used ref the inputs and outputs are interpreted as data facts information depending on the interpreter s relation to the system a common synonymous term is information systems types of information systems information system ref name ralston cite book title encyclopedia of computer science 4th ed author anthony ralston et al ed year 2000 publisher nature publishing group page 865 ref a data processing system may involve some combination of data conversion conversion converting data to another format data validation validation ndash ensuring that supplied data is clean correct and useful sorting ndash arranging items in some sequence and or in different sets summary statistic summarization ndash reducing detail data to its main points aggregate data aggregation ndash combining multiple pieces of data statistical analysis analysis ndash the collection organization analysis interpretation and presentation of data reporting ndash list detail or summary data or computed information types of data processing systems by application area scientific data processing scientific data processing usually involves a great deal of computation arithmetic and comparison operations upon a relatively small amount of input data resulting in a small volume of output ref name reddy cite book last reddy first r j title business data processing computer applications year 2004 publisher a p h publishing corporation location new dehli isbn 8176486493 page 17 url https books google com books id flkoxcts9ssc lpg pa17 dq 22scientific 20data 20processing 22 pg pa17 v onepage q 22scientific 20data 20processing 22 f false ref commercial data processing commercial data processing involves a large volume of input data relatively few computational operations and a large volume of output ref name reddy accounting programs are the prototypical examples of data processing applications information systems information systems is is the field that studies such organizational computer systems data analysis data analysis is a body of methods that help to describe facts detect patterns develop explanations and test hypotheses ref cite web last dartmouth college title introduction what is data analysis url http www dartmouth edu mss data 20analysis volume 20i 20pdf 20 006 20intro 20 28what 20is 20the 20weal pdf accessdate july 5 2013 ref for example data analysis might be used to look at sales and customer data to identify connections between products to allow for cross selling campaigns ref cite book last1 berthold first1 m r last2 borgelt first2 c last3 h\xc5\x91ppner first3 f last4 klawonn first4 f title guide to intelligent data analysis year 2010 publisher springer isbn 978 1 84882 260 3 page 15 ref by service type ref name ralston transaction processing system transaction processing systems information retrieval information storage and retrieval systems command and control systems computing service systems control system process control systems message switching systems examples simple example a very simple example of a data processing system is the process of maintaining a check register transactions mdash checks and deposits mdash are recorded as they occur and the transactions are summarized to determine a current balance monthly the data recorded in the register is reconciled with a hopefully identical list of transactions processed by the bank a more sophisticated record keeping system might further identify the transactions mdash for example deposits by source or checks by type such as charitable contributions this information might be used to obtain information like the total of all contributions for the year the important thing about this example is that it is a system in which all transactions are recorded consistently and the same method of bank reconciliation is used each time real world example this is a flowchart of a data processing system combining manual and computerized processing to handle accounts receivable billing and general ledger file stockbridge system flowchart example jpg ref the highest acceleration of data processing the point of software ref references reflist see also data processing electronic data processing computational science scientific computing information processing system broader term further reading bourque linda b clark virginia a 1992 processing data the survey example quantitative applications in the social sciences no 07 085 sage publications isbn 0 8039 4741 0 category data management category data processing'
b'the skyline operator is used in a query and performs a filtering of results from a database so that it keeps only those objects that are not worse than any other this operator is an extension to sql proposed by b\xc3\xb6rzs\xc3\xb6nyi et al ref name borzsony2001skyline cite journal last1 borzsonyi first1 stephan last2 kossmann first2 donald last3 stocker first3 konrad title the skyline operator journal proceedings 17th international conference on data engineering date 2001 pages 421 430 doi 10 1109 icde 2001 914855 ref a classic example of application of the skyline operator involves selecting a hotel for a holiday the user wants the hotel to be both cheap and close to the beach however hotels that are close to the beach may also be expensive in this case the skyline operator would only present those hotels that are not worse than any other hotel in both price and distance to the beach proposed syntax to give an example in sql b\xc3\xb6rzs\xc3\xb6nyi et al ref name borzsony2001skyline proposed the following syntax for the skyline operator source lang sql select from where group by having skyline of distinct d1 min max diff dm min max diff order by source where d sub 1 sub d sub m sub denote the dimensions of the skyline and min max and diff specify whether the value in that dimension should be minimised maximised or simply be different implementation the skyline operator can be implemented directly in sql using current sql constructs however this has been shown to be very slow ref name borzsony2001skyline other algorithms have been proposed that make use of divide and conquer indices ref name borzsony2001skyline mapreduce ref cite journal last1 mullesgaard first1 kasper last2 pedersen first2 jens laurits last3 lu first3 hua last4 zhou first4 yongluan title efficient skyline computation in mapreduce journal proc 17th international conference on extending database technology edbt date 2014 pages 37 48 url http www openproceedings eu 2014 conf edbt mullesgaardplz14 pdf ref and general purpose computing on graphics processing units general purpose computing on graphics cards ref cite journal last1 b\xc3\xb8gh first1 kenneth s last2 assent first2 ira last3 magnani first3 matteo title efficient gpu based skyline computation journal proceedings of the ninth international workshop on data management on new hardware date 2013 pages 5 1 5 6 doi 10 1145 2485278 2485283 ref skyline queries on data streams i e continuous skyline queries have been studied in the context of parallel query processing on multicores owing to their wide diffusion in real time decision making problems and data streaming analytics ref cite journal last1 de matteis first1 tiziano last2 di girolamo first2 salvatore last3 mencagli first3 gabriele title continuous skyline queries on multicore architectures journal concurrency and computation practice and experience date 25 august 2016 volume 28 issue 12 pages 3503 3522 doi 10 1002 cpe 3866 ref references references category data management category query languages category relational database management systems category sql database software stub'
b'data warehouse automation or dwa refers to the process of accelerating and automating the data warehouse development cycles while assuring quality and consistency dwa is believed to provide automation of the entire lifecycle of a data warehouse from source systems analysis system analysis to software testing testing to documentation it helps improve productivity reduce cost and improve overall quality ref cite web title automate and accelerate your data transformations url http www attunity com products prepare data compose website www attunity com publisher attunity accessdate 7 december 2015 ref general data warehouse automation primarily focuses on automation of each and every step involved in the lifecycle of a data warehouse thus reducing the efforts required in managing it ref cite web title new buzzword data warehouse automation url http blogs jetreports com 2015 03 05 new buzzword data warehouse automation website blogs jetreports com publisher jetreports accessdate 7 december 2015 ref data warehouse automation works on the principles of design patterns it comprises a central repository of design patterns which encapsulate architectural standards as well as best practices for data design data management data integration and data usage ref cite web url https www wherescape com media 1988 data warehouse automation decision guide pdf title data warehouse automation a decision guide website www wherescape com publisher david l wells infocentric llc accessdate 7 december 2015 ref in november 2015 an analyst firm has published a guide which data warehouse automation tool is right for you covering four of the leading products in the dwa space ref cite web title which data warehouse automation tool is right for you url http eckerson com register content which data warehouse automation tool is right for you website eckerson com publisher wayne eckerson accessdate 9 december 2015 ref in november 2015 an international software and technology services company engaged in developing agile tools for the data integration industry was named by cio review as one of the 20 most promising productivity tools solution providers 2015 ref cite web title cio magazine award 20 most promising productivity tools url http analytixds com latest news analytix data services wins cio reviews 2015 website www analtyixds com publisher analytix ds accessdate 25 november 2016 ref benefits data warehouse automation can provide advantages like source data exploration warehouse data models etl generation test automation metadata management managed deployment scheduling change impact analysis and easier maintenance and modification of the data warehouse ref cite web title data warehouse automation dwa url http www timextender com software data warehouse automation business value website timextender com publisher timextender software 2015 accessdate 7 december 2015 ref more important than the technical features of dwa tools however is the ability to deliver projects faster and with less resources ref cite web title deliver faster url http kalido com products kalido information engine deliver faster website kalido com publisher magnitude software accessdate 9 december 2015 ref references reflist external links dmoz computers software databases data warehousing data warehouse automation data warehouse automation http analytixds com wp content uploads 2016 07 analytix data services top 20 cio review pdf cio magazine award 20 most promising productivity tools cio review november 10 2015 see also data warehouse data mart data warehouse appliance data integration data warehouse category data management category data warehousing'
b'online transaction processing or oltp is a class of information system s that facilitate and manage transaction oriented applications typically for data entry and retrieval transaction processing the term is somewhat ambiguous some understand a transaction in the context of computer or database transactions while others such as the transaction processing performance council define it in terms of business or financial transaction commercial transactions ref http www tpc org transaction processing performance council website ref oltp has also been used to refer to processing in which the system responds immediately to user requests an automated teller machine atm for a bank is an example of a commercial transaction processing application online transaction processing applications are high throughput and insert or update intensive in database management these applications are used concurrently by hundreds of users the key goals of oltp applications are availability speed concurrency and recoverability ref http docs oracle com cd a87860 01 doc server 817 a76992 ch3 eval htm 2680 application and system performance characteristics ref reduced paper trails and the faster more accurate forecast for revenues and expenses are both examples of how oltp makes things simpler for businesses however like many modern online information technology solutions some systems require offline maintenance which further affects the cost benefit analysis of on line transaction processing system oltp is typically contrasted to online analytical processing olap online analytical processing which is generally characterized by much more complex queries in a smaller volume for the purpose of business intelligence or reporting rather than to process transactions whereas oltp systems process all kinds of queries read insert update and delete olap is generally optimized for read only and might not even support other kinds of queries overview oltp system is a popular data processing system in today s enterprises some examples of oltp systems include order entry retail sales and financial transaction systems ref what is an oltp system http docs oracle com cd e11882 01 server 112 e25523 part oltp htm ref on line transaction processing system increasingly requires support for transactions that span a network and may include more than one company for this reason modern on line transaction processing software use client or server processing and brokering software that allows transactions to run on different computer platforms in a network in large applications efficient oltp may depend on sophisticated transaction management software such as cics and or database optimization tactics to facilitate the processing of large numbers of concurrent updates to an oltp oriented database for even more demanding decentralized database systems oltp brokering programs can distribute transaction processing among multiple computers on a computer network network oltp is often integrated into service oriented architecture soa and web service s on line transaction processing oltp involves gathering input information processing the information and updating existing information to reflect the gathered and processed information as of today most organizations use a database management system to support oltp oltp is carried in a client server system on line transaction process concerns about concurrency and atomicity concurrency controls guarantee that two users accessing the same data in the database system will not be able to change that data or the user has to wait until the other user has finished processing before changing that piece of data atomicity controls guarantee that all the steps in transaction are completed successfully as a group that is if any steps between the transaction fail all other steps must fail also ref http technet microsoft com en us library ms187669 v sql 105 aspx on line transaction processing vs decision support ref systems design to build an oltp system a designer must know that the large number of concurrent users does not interfere with the system s performance to increase the performance of oltp system designer must avoid the excessive use of indexes and clusters the following elements are crucial for the performance of oltp systems ref http docs oracle com cd a87860 01 doc server 817 a76992 ch3 eval htm 2680 application and system performance characteristics ref rollback segments rollback segments are the portions of database that record the actions of transactions in the event that a transaction is rolled back rollback segments provide read consistency roll back transactions and recover the database ref http docs oracle com cd a87860 01 doc server 817 a76956 rollbak htm rollback ref clusters a cluster is a database schema schema that contains one or more tables that have one or more columns in common clustering tables in database improves the performance of join sql join operation ref http www iselfschooling com mc4articles mc4cluster htm cluster table ref discrete transactions all changes to the data are deferred until the transaction commits during a discrete transaction it can improve the performance of short non distributed transaction ref http docs oracle com cd a57673 01 doc server doc a48506 transac htm discrete transactions ref block data storage size the data block size should be a multiple of the operating system s block size within the maximum limit to avoid unnecessary i o ref http docs oracle com cd b10500 01 server 920 a96524 c03block htm data block ref buffer cache size to avoid unnecessary resource consumption tune sql statements to use the database buffer cache ref http docs oracle com cd e16655 01 server 121 e15857 tune buffer cache htm tgdba294 database buffer cache ref dynamic allocation of space to tables and rollback segments transaction processing monitors and the multi threaded server a transaction processing monitor is used for coordination of services it is like an operating system and does the coordination at a high level of granularity and can span multiple computing devices ref http c2 com cgi wiki transactionprocessingmonitor transaction processing monitor ref partition database partition increases performance for sites that have regular transactions while still maintain availability and security ref partition database partition ref database tuning with database tuning oltp system can maximize its performance as efficiently and rapidly as possible contrasted to batch processing grid computing see also on line analytical processing olap transaction processing database transaction references references external links wiktionary oltp http hstore cs brown edu h store project architectural and application shifts affecting oltp performance http www ibm com cics ibm cics official website http www tpc org transaction processing performance council http dbms knowledgehills com what is online transaction processing oltp schema a32p2 oltp schema http www amazon com dp 1558601902 transaction processing concepts techniques management databases defaultsort on line transaction processing category data management category databases category transaction processing'
b'infobox software name couchbase server logo file couchbaselogo svg 224px screenshot couchbase server screenshot jpg caption developer couchbase inc released start date 2010 08 latest release version 4 5 latest release date release date 2016 06 22 status active programming language c erlang programming language erlang c programming language c ref cite web author damien katz url http damienkatz net 2013 01 the unreasonable effectiveness of c html title the unreasonable effectiveness of c date january 8 2013 accessdate september 30 2016 ref go programming language go operating system cross platform genre multi model database key value database distributed key value document oriented database license apache license open source edition proprietary software proprietary free community edition and paid enterprise edition website url http www couchbase com frequently updated yes couchbase server originally known as membase is an open source distributed shared nothing architecture multi model database multi model nosql document oriented database software package that is optimized for interactive applications these applications may serve many concurrent user s by creating storing retrieving aggregating manipulating and presenting data in support of these kinds of application needs couchbase server is designed to provide easy to scale key value or json document access with low latency and high sustained throughput it is designed to be cluster computing clustered from a single machine to very large scale deployments spanning many machines a version originally called couchbase lite was later marketed as couchbase mobile combined with other software couchbase server provided client protocol compatibility with memcached ref cite web url http code google com p memcached wiki newprotocols title newprotocols memcached klingon memcached google project hosting publisher code google com date 2011 08 22 accessdate 2013 06 04 ref but added disk persistence computer science persistence data replication live cluster reconfiguration rebalancing and multitenancy with partition database data partitioning product history membase was developed by several leaders of the memcached project who had founded a company northscale to develop a key value store with the simplicity speed and scalability of memcached but also the storage persistence and querying capabilities of a database the original membase source code was contributed by northscale and project co sponsors zynga and naver corporation then known as nhn to a new project on membase org in june 2010 ref cite book title professional nosql author shashank tiwari publisher john wiley sons pages 15 16 isbn 9781118167809 ref on february 8 2011 the membase project founders and membase inc announced a merger with couchone a company with many of the principal players behind couchdb with an associated project merger the merged company was called couchbase inc in january 2012 couchbase released couchbase server 1 8 in september 2012 orbitz said it had changed some of its systems to use couchbase ref cite web url http gigaom com cloud balancing oracle and open source at orbitz title balancing oracle and open source at orbitz publisher gigaom date september 21 2012 accessdate september 19 2016 ref on december 2012 couchbase server 2 0 announced in july 2011 was released and included a new json document store indexing and querying incremental mapreduce and replication computing replication across data center s ref name zd2 cite web url http www zdnet com couchbase 2 0 released implements json document store 7000008649 title couchbase 2 0 released implements json document store publisher zdnet author andrew brust date december 12 2012 ref ref cite web title couchbase goes 2 0 pushes sql for nosql author derrick harris date july 29 2011 work gigaom url https gigaom com 2011 07 29 couchbase 2 0 unql sql nosql accessdate september 19 2016 ref architecture every couchbase node consists of a data service index service query service and cluster manager component starting with the 4 0 release the three services can be distributed to run on separate nodes of the cluster if needed in the parlance of eric brewer s cap theorem couchbase is normally a cp type system meaning it provides consistency database systems consistency and network partitioning partition tolerance or it can be set up as an ap system with multiple clusters cluster manager the cluster manager supervises the configuration and behavior of all the servers in a couchbase cluster it configures and supervises inter node behavior like managing replication streams and re balancing operations it also provides metric aggregation and consensus functions for the cluster and a rest ful cluster management interface the cluster manager uses the erlang programming language erlang programming language and the open telecom platform replication and fail over data replication within the nodes of a cluster can be controlled with several parameters in december 2012 replication was also supported between different data center s ref name zd2 data manager the data manager stores and retries documents in response to data operations from applications it asynchronously writes data to disk after acknowledging to the client in version 1 7 and later applications can optionally ensure data is written to more than one server or to disk before acknowledging a write to the client parameters define item ages that affect when data is persisted and how max memory and migration from main memory to disk is handled it supports working sets greater than a memory quota per node or bucket external systems can subscribe to filtered data streams supporting for example full text search indexing data analytics or archiving ref cite web url http blog couchbase com want know what your memcached servers are doing tap them title want to know what your memcached servers are doing tap them author trond norbye work couchbase blog date march 15 2010 ref data format a document is the most basic unit of data manipulation in couchbase server documents are stored in json document format with no predefined schemas object managed cache couchbase server includes a built in multi threaded object managed cache that implements memcached compatible apis such as get set delete append prepend etc storage engine couchbase server has a tail append storage design that is immune to data corruption oom killer s or sudden loss of power data is written to the data file in an append only manner which enables couchbase to do mostly sequential writes for update and provide an optimized access patterns for disk i o performance a performance benchmark done by altoros in 2012 compared couchbase server with other technologies ref cite web url http www couchbase com nosql resources presentations benchmarking couchbase 5b2 5d html title benchmarking couchbase author frank weigel publisher couchbase date october 30 2012 accessdate september 30 2016 ref cisco systems published a benchmark that measured the latency and throughput of couchbase server with a mixed workload in 2012 ref cite web url http www cisco com en us prod collateral switches ps9441 ps9670 white paper c11 708169 pdf title cisco and solarflare achieve dramatic latency reduction for interactive web applications with couchbase a nosql database publisher cisco systems date june 18 2012 archivedate august 13 2012 archiveurl https web archive org web 20120813162214 http www cisco com en us prod collateral switches ps9441 ps9670 white paper c11 708169 pdf accessdate october 7 2016 ref licensing and support couchbase server is a packaged version of couchbase s open source software technology and is available in a community edition without recent bug fixes with apache 2 0 license ref cite web url http developer couchbase com open source projects title couchbase open source projects work couchbase web site accessdate october 7 2016 ref and an edition for commercial use ref cite web url http www couchbase com couchbase server editions title couchbase server editions publisher couchbase ref couchbase server builds are available for ubuntu debian red hat suse oracle linux microsoft windows and mac os x operating systems couchbase has supported software developers kits for the programming languages net framework net php ruby programming language ruby python programming language python c programming language c node js java programming language java and go programming language go n1ql a query language called the non first normal form query language n1ql pronounced nickel is used for manipulating the json data in couchbase just like sql manipulates data in rdbms it has select insert update delete merge statements to operate on json data it was announced in march 2015 as sql for documents ref cite web title ssssh don t tell anyone but couchbase is a serious contender couchbase live europe 2015 author andrew slater date march 24 2015 accessdate september 19 2016 ref the n1ql data model is database normalization non first normal form 28nf c2 b2 or n1nf 29 non first normal form n1nf with support for nested attributes and domain oriented database normalization normalization the n1ql data model is also a proper superset and generalization of the relational model example source lang json email testme gmail com friends name rick name cate source like query code 2 sql select from bucket where like gmail com array query code 2 sql 1 select from bucket where any x in friends satisfies x name cate end bibliography cite book last brown first mc editor first editor last title getting started with couchbase server 1st edition publisher o reilly media date june 22 2012 page 88 isbn 978 1449331061 citation first1 david last1 ostrovsky first2 mohammed last2 haji first3 yaniv last3 rodenski date november 26 2015 title pro couchbase server 2nd ed edition 2nd publisher apress page 349 isbn 978 1484211861 citation first1 henry last1 potsangbam date november 23 2015 title learning couchbase edition 1st publisher packt page 202 isbn 978 1785288593 citation first1 deepak last1 vohra date august 3 2015 title pro couchbase development a nosql platform for the enterprise edition 1st publisher apress page 331 isbn 978 1484214350 references reflist external links official website category free database management systems category distributed computing architecture category nosql category cross platform software category structured storage category client server database management systems category database related software for linux category applications of distributed computing category databases category data management category distributed data stores'
b'a national data repository ndr is a data bank that seeks to preserve and promote a country s natural resources data particularly data related to the petroleum exploration and production e p sector a national data repository is normally established by an entity that governs controls and supports the exchange capture transference and distribution of e p information with the final target to provide the state with the tools and information to assure the growth govern ability control independence and sovereignty of the industry the two fundamental reasons for a country to establish an ndr are to preserve data generated inside the country by the industry and to promote investments in the country by utilizing data to reduce the exploration production and transportation business risks countries take different approaches towards preserving and promoting their natural resources data the approach varies according to a country s natural resources policies level of openness and its attitude towards foreign investment data types ndrs store a vast array of data related to a country s natural resources this includes wells well logging well log data well reports core sample s seismic surveys seismic inversion post stack seismic resolution inversion post stack seismic field data tapes seismic acquisition processing reports oil production production data geological map s and reports license data and geologic modeling geological models funding models some ndrs are financed entirely by a country s government others are industry funded still some are hybrid systems funded in part by industry and government ndrs typically charge fees for data requests and for data loading the cost differs significantly between countries in some cases an annual membership is charged to oil companies to store and access the data in the ndr standards body energistics is the global energy standards resource center for the upstream oil and gas industry energistics national data repository work group the standards body is energistics ref http energistics org energistics standards directory energistics ref energistics standards directory global regulators of upstream oil and natural gas information including seismic drilling production and reservoir data formed the national data repository ndr work group in 2008 to collaborate on the development of data management standards and to assist emerging nations with hydrocarbon reserves to better collect maintain and deliver oil and gas data to the public and to the industry ten countries led by the netherlands norway and the united kingdom formed ndr to share best practices and to formalize the development and deployment of data management standards for regulatory agencies the other countries involved in the ndr work group s formation are australia canada india kenya new zealand south africa and the united states annual ndr conference approximately every 18 months energistics organizes a national data repository conference the purpose is to provide government and regulatory agencies from around the world an opportunity to attend a series of workshops dedicated to developing data exchange standards improving communications with the oil and gas industry and learning data management techniques for natural resources information ref http www energistics org regulatory national data repository ndr work group ndr meetings ndr conference page on the energistics website ref society of exploration geophysicists and the international oil and gas producers association the seg is the custodian of the seg standards which are used for the exchange retention and release of seismic data they are commonly used by national data repositories with the segd and segy being the field and processed exchange standards respectively ndrs around the world https www google com maps d viewer mid 1by9vddowwnzd0f8vnt le2tthtu click here to see a map of the ndrs around the world class wikitable sortable country name agency scope status purposes data types volumes standards used funding website flagcountry algeria banque de donn\xc3\xa9es nationale bdn agence nationale pour la valorisation des ressources en hydrocarbures alnaft onshore and offshore algeria ongoing project agency created by new law in 2005 custodian of all e p data of the country cultural seismic 2d 3d wells data wells wells report production facilities economical and fiscality interpretation physical assets index data drilling transcription vectorisation digitalization ascii segy ukooa las dlis lis pds bit rode pdf tif etc government funding agency revenue http www alnaft gov dz flagcountry colombia epis agencia nacional de hidrocarburos anh onshore and offshore colombia created originally for ecopetrol and transferred to anh when it was established in 2003 new system launched december 2009 promote and preserve all the technical e p information assets of the country wells surveys licenses seismic sections well reports maps rest web services government funding http www epis com co flagcountry canada cnsopb nova scotia offshore petroleum board geoscience research centre digital data management centre dmc offshore nova scotia canada operational since 2007 to provide an effective efficient system for the management of digital petroleum data assist explorers in easily obtaining access to large volumes of data via the web data preservation and data distribution wells well log curves well reports cores and samples field data tapes seismic acquisition processing reports production data interpretative maps and reports las dlis segy funded 50 50 by the federal and provincial governments with some funds from industry through cost recovery http www cnsopb ns ca flagcountry australia pims geoscience australia active various online and web based systems exist for e p geosciences wells well log curves well reports cores and samples field data tapes seismic acquisition processing reports production data interpretative maps and reports http dbforms ga gov au pls www npm pims web search flagcountry western australia wapims government of western australia active wapims is a petroleum geothermal and minerals exploration database contains data on titles wells geophysical surveys and other petroleum exploration and production data submitted to dmp by the petroleum industry http dmp wa gov au flagcountry new south wales government of new south wales active various online geoscience databases to assist new south wales including digs http www dpi nsw gov au minerals http digsopen minerals nsw gov au flagcountry northern territory government of northern territory active various online geoscience databases to assist northern territories wells well log curves well reports cores and samples field data tapes seismic acquisition processing reports production data interpretative maps and reports http www nt gov au d minerals energy index cfm header petroleum flagcountry queensland government of queensland active various online geoscience databases to assist queensland including q dex wells well log curves well reports cores and samples field data tapes seismic acquisition processing reports production data interpretative maps and reports https www dnrm qld gov au flagcountry south australia sarig government of south australia active various online geoscience databases to assist south australia such as pep sa wells well log curves well reports cores and samples field data tapes seismic acquisition processing reports production data interpretative maps and reports http petroleum statedevelopment sa gov au data and publications sarig https sarig pir sa gov au flagcountry tasmania various online geoscience databases to assist tasmania active http www mrt tas gov au portal page pageid 35 1 dad portal schema portal flagcountry china cnpc chinese national petroleum corporation various oil companies in china with cnpc the largest and parent of petrochina http www cnpc com cn en http www petrochina com cn ptr http www cnooc com cn http english sinopec com index shtml flagcountry russia sakhalin digc rdc various oil companies in russia the largest being rosneft which is state owned http www rosneft com http www lukoil com http www tnk bp com en http www surgutneftegas ru http www gazprom neft com http www tatneft ru wps wcm connect tatneft portal rus homepage flagcountry indonesia indonesia s national data centre ndc for petroleum energy and minerals data agency for research and development in the ministry of energy and mineral resources of the republic of indonesia onshore offshore in 1997 indonesia established migas data management mdm operated by pt patra nusa data pnd pnd manages and promotes petroleum investment opportunities by compiling and value adding available petroleum data and information http www patranusa com flagcountry new zealand new zealand online exploration database new zealand petroleum minerals ministry of business innovation employment new zealand onshore and offshore out to the outer continental shelf opened to public in april 2007 data preservation investment facilitation aid in monitoring regulatory compliance maximise the return to the nation by informing public policy and business strategy wells well log curves petroleum reports includes wells and surveys mineral reports coal reports cores and samples seismic surveys post stack seismic field data tapes seismic acquisition processing reports geophysical and geochemical data acquired in mineral and coal exploration incorporated as enclosures to reports vsp incorporated as enclosures to reports seismic survey observer logs gis data and projects minerals and coal estimated total ndr size 2 5 tb loaded 3 0 tb staged for loading 40 tb field data offline closely follow australian digital reporting standards no naming standards for wells and surveys 50 government funding 50 third party permit license fees paid by exploration companies https data nzpam govt nz flagcountry jordan nra jordan natural resources authority nra onshore active online data room allows users to browse and select large data set quickly in a controlled and secure environment reserves land records field data maps engineering seismic data geological studies and well files http www jordan gov jo flagcountry angola sonangol offshore angola active promotion organisation management of all exploration production e p data of angola wells surveys licenses seismic sections well reports maps norad ofd and npd assistance http www sonangol co ao flagcountry france beph french territory interactive maps of french territory of oil data are available to internet users which includes permits for petroleum exploration seismic exploration oil drilling data documents available wells surveys licenses seismic sections well reports maps http www beph net flagcountry s\xc3\xa3o tom\xc3\xa9 and pr\xc3\xadncipe anp stp national petroleum agency of s\xc3\xa3o tom\xc3\xa9 principe anp stp offshore norad ofd and npd assistance http www anp stp gov st flagcountry tanzania tpdc tanzania petroleum development corp began in the early 1990s with norwegian assistance an e p data archive centre geophysical survey data geological studies well drilling and completion reports cores and drill steam data norad ofd and npd assistance http www tpdc tz com flagcountry oman ogdr department of petroleum concession ministry of oil and gas onshore offshore operational tendering ogdr as a managed service fully outsourced june 2015 preservation of e p data support concession promotion well related data header deviation tops field and processed logs well documents seismic related data field and processed 2d 3d gravimag vsp ogdr data submission standard that uses industry standards where possible i e dlis seg ukooa government concession holders http www mog gov om english tabid 309 default aspx flagcountry netherlands dino the geological survey of the netherlands a division of netherlands organisation for applied scientific research tno the netherlands including offshore waters started in 2004 currently bro is being planned to succeed dino to archive subsurface data of the netherlands in one repository and provide easy access to the data to encourage multiple use of data wms web services dino uses own naming conventions 100 government funding http www nlog nl en home nlogportal html flagcountry india dgh directorate general of hydrocarbons dgh active scheduled operation by april 2015 establishing national data archival improving data quality and access for quality exploration covering large area under exploration and providing basis for long term energy policy formulation as well as support oalp wells well logs cores scanned core images seismic reports production technical reports government of india http www dghindia org datamanagement aspx flagcountry sri lanka prds ministry of petroleum and petroleum resources development active since 2009 the prds developed a website to disseminate petroleum data and information to public and to investors to assist promotion of offshore areas to attract investors for petroleum exploration wells surveys licenses seismic sections well reports maps data historic and current archived on different media paper mylar magnetic tape http www prds srilanka com data onlinedata faces flagcountry argentina enarsa energia argentina sa established in 2006 http www enarsa com ar http energia mecon gov ar upstream us pterminados asp flagcountry peru perupetro active http www perupetro com pe flagcountry kazakhstan ministry of energy and mineral resources of the republic of kazakhstan memr active http www petrodata kz flagcountry pakistan ppepdr directorate general petroleum concessions dgpc active since 2001 repository contains more than 10 terabytes of secure petrotechnical data http www ppepdr net flagcountry nigeria department of petroleum resources active since december 2003 preserve maintain the integrity and promote the national e p data assets with improved quality efficiency and accessibility in the most rapid secure and reliable manner international and petrobank data management standards funded by establishment costs one off funding by government and running costs subscription transaction fees by operators http ndr dprnigeria com flagcountry turkey petrobank mds turkish petroleum corporation tpao it is noc of turkey 36\xcb\x9a 42\xcb\x9a northern parallel and the 26 45\xcb\x9a eastern meridian operational since 2007 data assets preservation easy access to assets assets access controlling and auditing consolidation of assets national archive central management of all assets standardization of assets according to international standards and naming conventions working with the most convenient assets wells well log curves well reports cores and samples seismic surveys post stack seismic field data tapes and seismic acquisition processing reports international and petrobank data management standards funded fully by the turkish petroleum corporation service usage is free of charge http www tpao gov tr flagcountry norway diskos norwegian national data repository norwegian petroleum directorate npd and diskos group of oil companies norwegian continental shelf started in 1995 to ensure compliance with npd reporting regulations for digital e p data to reduce data redundancy to ensure that data is made generally available to the oil and gas industry and to society as a whole long term preservation of data wells well log curves seismic surveys field pre stack post stack seismic seismic reports production data monthly allocated size of ndr estimated at more than 3 petabytes seg d for seismic field data seg y for pre stack and post stack seismic data currently only limited amounts of field and pre stack data all relevant well data standards such as lis dlis las spla scal etc pdf and tif are also used costs are shared equally between all participating oil companies around 50 in the diskos consortium including the npd in addition reporting companies pay to submit and download data all norwegian universities have free access to public data in diskos non oil companies can apply for associated membership there are currently around 25 such members http www diskos no http www npd no flagcountry united kingdom cda cda common data access ltd uk offshore waters wells went live in 1995 infrastructure started operations in 2000 seismic went live in 2009 estimated ndr size 6 terabytes save costs for licenses improve access to data comply with regulations well log curves well reports post stack seismic seismic reports vsp deviation and test data estimated ndr size 6 terabytes cda has adopted decc s naming standards for wells and surveys and continues to work closely with decc and industry to identify a range of standards see the cda and decc websites for more on this owned by the uk oil and gas industry http www ukoilandgasdata com http www gov uk oil and gas petroleum operations notices http www cdal com flagcountry united kingdom ukogl uk onshore geophysical library uk onshore in operation since 1994 managed and operated by lynx information systems ltd on behalf of ukogl custodian of all uk onshore seismic data seismic well tops logs cultural current archive size approx 6tb segy ukooa las dlis self funded through data sales http www ukogl org uk http maps lynxinfo co uk ukogl live map html flagcountry brazil anp ag\xc3\xaancia nacional do petr\xc3\xb3leo anp bdep formed in may 2000 stores seismic well log post stack and pre stack seismic data and potential field data grav mag anp standards in place funded by members http www bdep gov br flagcountry mexico ditep pemex established in 2002 promotes and preserve all the technical e p information assets of the country http www pep pemex com index html flagcountry israel the ministry of national infrastructures exploratory http www mni gov il mni en us naturalresources oilandgasexploration oilmaps flagcountry cyprus mcit ministry of commerce industry and tourism energy service offshore promotional responsible for granting licences for prospecting exploration and exploitation of hydrocarbons http www mcit gov cy mcit mcit nsf dmlhexploration en dmlhexploration en opendocument flagcountry south africa petroleum agency of south africa active seismic data well data samples reports and diagrams standards formats segd segy lis las pdf and tiff media 3480 3590 dlt 8mm exabyte dat from 2010 funded by government http www petroleumagencysa com flagcountry kenya national data center ndc national oil corporation of kenya offshore and onshore began in 2007 system implemented in 2010 digital data preservation national archive to implement integrated data management systems provide easy access to quality controlled data for internal and external customers attract oil and gas exploration investment and to reduce data management costs wells well log curves well reports post stack seismic field data tapes seismic acquisition processing reports interpretive maps and reports seismic data segy 3590 or 3592 data cartridges 100 government funded http www nockenya co ke flagcountry united states boemre bureau of ocean energy management regulation and enforcement bureau of ocean energy management regulation and enforcement boemre gulf of mexico has replaced the former minerals management service mms http www gomr boemre gov homepg data center html flagcountry united states ngrds national geoscience data repository system ngdrs ngrds is a system of geoscience data repositories providing information about their respective holdings accessible through a web based supercatalog geologic geophysical and engineering data maps well logs and samples doe has provided funds for the ngdrs since 1993 http www agiweb org ngdrs index html http www energy gov http www agiweb org index html list of repositories in us listed also as directory http www agiweb org ngdrs overview datadirectory html flagcountry cambodia cnpa cambodia national petroleum authority onshore offshore promotion and preservation of technical e p information assets of the country norad ofd and npd assistance http www cnpa cambodia com flagcountry afghanistan mom ministry of mines of the islamic republic of afghanistan mom onshore promotion and preservation of technical e p information assets of the country norad ofd and npd assistance http mom gov af en news 1637 flagcountry bangladesh moemr hydrocarbon unit ministry of power energy and mineral resources moemr onshore offshore active and ongoing via hcu unit since 2005 a mini data bank has established in the hcu to handle production data resource data by using database gis software 2005 and promotion of technical e p information assets of the country funded assistance norad ofd and npd assistance http www hcu org bd http www petrobangla org bd http www bapex com bd flagcountry ethiopia mome ministry of mines and energy ethiopia active and ongoing promotion of technical e p information assets of the country http www mome gov et petroleum html flagcountry cameroon snh snh cameroon active ongoing preservation and promotion of technical e p information assets of the country http www snh cm flagcountry malaysia piri petronas yet to establish full ndr promotion and preservation of technical e p information assets of the country http www petronas com my flagcountry spain ath online gis databases to geophyscial information sigeof and ath archivo de hydrocarbures http www mityc es energia petroleo exploracion paginas estadisticas aspx http hidrocarburos mityc es ath http www igme es internet sigeof iniciosigeof htm flagcountry morocco onhym office national des hydrocarbures et des mines promotion and preservation of technical e p information assets of the country http www onhym com flagcountry madagascar omnis promotion and preservation of technical e p information assets of the country norad ofd and npd assistance flagcountry sudan pic petroleum information center ministry of oil and gas active since 2000 preserve and promote e p data managing oil museum wells well log well reports cores and samples seismic acquisition processing reports production data gis http www spc gov sd flagcountry morocco onhym office national des hydrocarbures et des mines promotion and preservation of technical e p information assets of the country http www onhym com flagcountry nicaragua mem active ongoing norad ofd and npd assistance http www ine gob ni http www mem gob ni flagcountry iraq moo ministry of oil republic of iraq active and ongoing since 2005 moo establishing a centralized data base and ndr for iraqi petroleum data and to ensure that data information from petroleum activities is made available and attract more investors by promoting the petroleum activities well logs maps magnetic tapes core cutting samples other geological and geophysical information norad ofd and npd assistance http www oil gov iq flagcountry latvia legmc latvian environment geology and meteorology centre offshore onshore an e p data archive centre which provides data available for purchase geological well and seismic data maps reports etc http mapx map vgd gov lv geo3 vgd oil page index htm flagcountry albania akbn national agency of natural resources generates and promotes exploration opportunities in albania maintains archive of e p data http www akbn gov al index php ak details cid 5 lng en flagcountry uganda pepd petroleum exploration production dept pepd onshore technical e p data archive and information norad ofd assistance http www statehouse go ug government php catid 10 http www energyandminerals go ug flagcountry zambia ministry of mines and minerals development geological survey department gsd onshore active and ongoing technical e p data archive and information technical records unit norad ofd npd assistance http www zambiageosurvey gov zm flagcountry ivory coast mme ministry of mines energy onshore offshore norad ofd and npd assistance http www cotedivoirepr ci http www petroci ci index php numlien 31 flagcountry romania national agency for mineral resources promotion and preservation of technical e p information assets of the country http www namr ro main en htm flagcountry fiji sopac mineral resources dept fiji created as sopac petroleum data bank a cooperative effort with geoscience australia sopac acts as custodian and primary point for e p data information preserved on behalf of pacific island member nations well logs maps magnetic tapes core cutting samples other geological and geophysical information in part externally managed http www mrd gov fj gfiji http www mrd gov fj gfiji petroleum petroleum html http www ga gov au energy projects pacific islands applied geoscience commission html http www sopac org index php member countries fiji islands flagcountry papua new guinea sopac department of petroleum and energy created as sopac petroleum data bank a cooperative effort with geoscience australia sopac acts as custodian and primary point for e p data information preserved on behalf of pacific island member nations well logs maps magnetic tapes core cutting samples other geological and geophysical information externally managed http www petroleum gov pg http www petrominpng com pg about html http www ga gov au energy projects pacific islands applied geoscience commission html http www sopac org index php member countries papua new guinea flagcountry solomon islands sopac created as sopac petroleum data bank a cooperative effort with geoscience australia sopac acts as custodian and primary point for e p data information preserved on behalf of pacific island member nations well logs maps magnetic tapes core cutting samples other geological and geophysical information externally managed http www ga gov au energy projects pacific islands applied geoscience commission html http www sopac org index php member countries solomon islands flagcountry tonga sopac created as sopac petroleum data bank a cooperative effort with geoscience australia sopac acts as custodian and primary point for e p data information preserved on behalf of pacific island member nations well logs maps magnetic tapes core cutting samples other geological and geophysical information externally managed http www ga gov au energy projects pacific islands applied geoscience commission html http www sopac org index php member countries tonga flagcountry vanuatu sopac created as sopac petroleum data bank a cooperative effort with geoscience australia sopac acts as custodian and primary point for e p data information preserved on behalf of pacific island member nations well logs maps magnetic tapes core cutting samples other geological and geophysical information externally managed http www ga gov au energy projects pacific islands applied geoscience commission html http www sopac org index php member countries vanuatu flagcountry guyana ggmc guyana geology and mines commission promotion and preservation of technical e p information assets of the country http www ggmc gov gy flagcountry syria spc syrian petroleum company http www spc sy com en aboutus aboutus1 en php flagcountry liberia nocal national oil company of liberia promotion and preservation of technical e p information assets of the country http www nocal lr com flagcountry chile enap national oil company of chile promotion and preservation of technical e p information assets of the country http www enap cl flagcountry thailand pttep ptt exploration and production public company ltd promotion and preservation of technical e p information assets of the country http www pttep com http www pttep com en index aspx flagcountry venezuela pdvsa petroleos de venezuela onshore offshore promotion and preservation of technical e p information assets of the country http www pdvsa com flagcountry trinidad trinidad ministry of energy and energy affairs promotion and preservation of technical e p information assets of the country http www energy gov tt energy industry php mid 31 http www petrotrin com petrotrin2007 upstreambusiness htm flagcountry mozambique napd established in 1999 under norad support to ensure that data information from petroleum activities is made available and attract more investors by promoting the petroleum activities well logs maps magnetic tapes core cutting samples other geological and geophysical information national budget and inp funds http www inp gov mz flagcountry denmark danish energy agency online gis service for wells and license data http www ens dk en us oilandgas sider oilandgas aspx flagcountry dominican republic directorate of hydrocarbons http www dgm gov do sdhidrocarburo index html flagcountry equatorial guinea exploration databank for equatorial guinea http www equatorialoil com http www equatorialoil com database html flagcountry faroe islands jardfeingi jardfeingi faorese earth and energy directorate promotion of exploration and licensing rounds http www jardfeingi fo flagcountry philippines pnoc philippine national oil company promotion of exploration and licensing rounds http www pnoc com ph http www pnoc ec com ph business php id 2 flagcountry greenland greenpetrodata mmr ministry of mineral resources web and gis system providing access to all released well and geophysical data https www greenpetrodata gl http govmin gl flagcountry iceland iceland continental shelf portal icsp orkustofnunn national energy authority offshore the iceland continental shelf portal icsp provides access to information about data pertaining to the icelandic continental shelf in particular initially to the northern dreki area to assist with licensing round promotion http www os is http www nea is oil and gas exploration flagcountry myanmar moge myanmar oil gas enterprise http www energy gov mm upstreampetroleumsubsector htm flagcountry yemen pepa petroleum exploration and production authority pepa http www pepa com ye flagcountry tunisia etap enterprise tunisienne d activities petrolieres promotion and preservation of technical e p information assets of the country http www etap com tn flagcountry gabon dgh direction generale des hydrocarbures dgh http www gabon industriel com les actions energie petrole flagcountry republic of the congo snpc soci\xc3\xa9t\xc3\xa9 nationale des p\xc3\xa9troles du congo flagcountry mali aurep not a typo autorite pour la promotion de la recherce des petroliere au mali databank service managing the geological and geophysical data relative to petroleum research http www aurep org http www aurep org htmlpages mali html flagcountry guatemala mem direcci\xc3\xb3n general de hidrocarbures online maps and images of wells seismic licenses protected areas exploration and production http www mem gob gt portal home aspx http www mem gob gt portal home aspx secid 25 flagcountry iran nioc national oil company of iran http www nioc ir flagcountry libya noc noc libya virtual data room in place for promotion of exploration and exploitation of hydrocarbons flagcountry united arab emirates adnoc abu dhabi national oil company http www adnoc ae flagcountry qatar qatar petroleum http www qp com qa flagcountry south korea knoc korea national petroleum corporation http www knoc co kr flagcountry seychelles snoc seychelles national oil company flagcountry saudi arabia saudi aramco http www saudiaramco com flagcountry belarus http geologiya org index php categoryid 14 http minpriroda by ru napravlenia minsyrbaza flagcountry east timor lafaek autoridade nacional do petr\xc3\xb3leo online gis with wells and licences norad ofd assistance http www anp tl org webs anptlweb nsf pgmaps see also norwegian petroleum directorate energistics professional petroleum data management association ppdm oil and gas industry in the united kingdom petroleum exploration in guyana notes reflist external links http www energistics org regulatory national data repository ndr work group energistics national data repository work group http www kadme com wp content uploads kadme oil and gas technology jan2011 pdf national data repositories the case for open data in the oil and gas industry http www seg org ts society of exploration geophysicists category data management category open standards category hydrocarbons category geophysics organizations'
b'astroinformatics is an interdisciplinary field of study involving the combination of astronomy data science informatics and information technology information communications technologies communications technologies ref name astroinfo ref name pdf http www math bas bg nkirov zip seedi astro presentation pdf astroinformatics and digitization of astronomical heritage nikolay kirov the fifth seedi international conference digitization of cultural and scientific heritage may 19 20 2010 sarajevo retrieved 1 november 2012 ref background astroinformatics is primarily focused on developing the tools methods and applications of computational science data science and statistics for research and education in data oriented astronomy ref name astroinfo cite web last1 borne first1 kirk title astroinformatics data oriented astronomy research and education url http link springer com article 10 1007 2fs12145 010 0055 2 website journal of earth science informatics june 2010 volume 3 issue 1 pp 5 17 publisher springer link netherlands accessdate 11 january 2016 ref early efforts in this direction included data discovery metadata standards development data modeling astronomical data dictionary development data access information retrieval ref cite arxiv last1 borne first1 kirk title science user scenarios for a virtual observatory design reference mission science requirements for data mining arxiv astro ph 0008307 ref data integration and data mining ref cite web last1 borne first1 kirk title scientific data mining in astronomy url https www crcpress com next generation of data mining kargupta han yu motwani kumar 9781420085860 website crc press pp 91 114 publisher taylor francis group accessdate 11 january 2016 ref in the astronomical virtual observatory initiatives ref cite web last1 borne first1 kirk title distributed data mining in the national virtual observatory url http proceedings spiedigitallibrary org proceeding aspx articleid 764620 website spie digital library publisher spie accessdate 11 january 2016 ref ref name vodm ref cite web last1 laurino first1 o title astroinformatics of galaxies and quasars a new general method for photometric redshifts estimation url http mnras oxfordjournals org content 418 4 2165 website monthly notices of the royal astronomical society vol 418 pp 2165 2195 publisher oxford journals accessdate 12 january 2016 display authors etal ref further development of the field along with astronomy community endorsement was presented to the national research council united states in 2009 in the astroinformatics state of the profession position paper for the 2010 astronomy and astrophysics decadal survey ref cite web last1 borne first1 kirk title astroinformatics a 21st century approach to astronomy url http adsabs harvard edu abs 2009astro2010p 6b website astrophysics data system publisher sao nasa accessdate 11 january 2016 ref that position paper provided the basis for the subsequent more detailed exposition of the field in the informatics journal paper astroinformatics data oriented astronomy research and education ref name astroinfo astroinformatics as a distinct field of research was inspired by work in the fields of bioinformatics and geoinformatics and through the escience work ref cite web title online science url http research microsoft com en us um people gray jimgraytalks htm website talks by jim gray publisher microsoft research accessdate 11 january 2015 ref of jim gray computer scientist at microsoft research whose legacy was remembered and continued through the jim gray escience awards ref cite web title jim gray escience award url http research microsoft com en us collaboration focus escience jim gray award aspx website microsoft research ref though the primary focus of astroinformatics is on the large worldwide distributed collection of digital astronomical databases image archives and research tools the field recognizes the importance of legacy data sets as well using modern technologies to preserve and analyze historical astronomical observations some astroinformatics practitioners help to digital data digitize historical and recent astronomical observations and images in a large database for efficient retrieval through world wide web web based interfaces ref name pdf ref http www casca ca lrp2010 docs lrpreports astroinformatics lrp pdf astroinformatics in canada nicholas m ball david schade retrieved 1 november 2012 ref another aim is to help develop new methods and software for astronomers as well as to help facilitate the process and analysis of the rapidly growing amount of data in the field of astronomy ref cite web title astroinformatics helps astronomers explore the sky url http phys org news 2013 10 astroinformatics astronomers exploring sky html website phys org publisher heidelberg university accessdate 11 january 2015 ref astroinformatics is described as the fourth paradigm of astronomical research ref cite web title the fourth paradigm data intensive scientific discovery url https www microsoft com en us research publication fourth paradigm data intensive scientific discovery website microsoft research ref there are many research areas involved with astroinformatics such as data mining machine learning statistics visualization scientific data management and semantic science ref name vodm cite web last1 borne first1 kirk title virtual observations data mining and astroinformatics url http link springer com referenceworkentry 10 1007 978 94 007 5618 2 9 website planets stars and stellar systems volume 2 astronomical techniques software and data pp 403 443 publisher springer link netherlands accessdate 11 january 2015 ref data mining and machine learning play significant roles in astroinformatics as a scientific method scientific research discipline due to their focus on knowledge discovery from data data mining kdd and learning from data ref cite web last1 ball first1 n m last2 brunner first2 r j title data mining and machine learrning in astronomy url http www worldscientific com doi abs 10 1142 s0218271810017160 website international journal of modern physics d publisher world scientific publishing accessdate 12 january 2016 ref ref cite web last1 borne first1 kirk title the lsst data mining research agenda url http scitation aip org content aip proceeding aipcp 10 1063 1 3059074 website classification and discovery in large astronomical surveys pp 347 351 publisher american institute of physics accessdate 12 january 2016 ref the amount of data collected from astronomical sky surveys has grown from gigabytes to terabytes throughout the past decade and is predicted to grow in the next decade into hundreds of petabytes with the large synoptic survey telescope and into the exabytes with the square kilometre array ref cite web last1 ivezi\xc4\x87 first1 \xc5\xbe title parametrization and classification of 20 billion lsst objects url http scitation aip org content aip proceeding aipcp 10 1063 1 3059076 website classification and discovery in large astronomical surveys pp 359 365 publisher american institute of physics accessdate 12 january 2016 display authors etal ref this plethora of new data both enables and challenges effective astronomical research therefore new approaches are required in part due to this data driven science is becoming a recognized academic discipline consequently astronomy and other scientific disciplines are developing sub disciplines information and data intensive to an extent that these sub disciplines are now becoming or have already become stand alone research disciplines and full fledged academic programs while many institutes of education do not boast an astroinformatics program the most likely will in the near future informatics has been recently defined as the use of digital data information and related services for research and knowledge generation however the usual or commonly used definition is informatics is the discipline of organizing accessing integrating and mining data from multiple sources for discovery and decision support therefore the discipline of astroinformatics includes many naturally related specialties including data modeling data organization etc it may also include transformation and normalization methods for data integration and information visualization as well as knowledge extraction indexing techniques information retrieval and data mining methods classification schemes e g taxonomy general taxonomies ontology information science ontologies folksonomy folksonomies and or collaborative tag metadata tagging ref cite web last1 borne first1 kirk title collaborative annotation for scientific data discovery and reuse url http www asis org bulletin apr 13 aprmay13 rdap borne html website bulletin of the asis t publisher american society for information science and technology accessdate 11 january 2016 ref plus astrostatistics will also be heavily involved citizen science projects such as galaxy zoo also contribute highly valued novelty discovery feature meta tagging and object characterization within large astronomy data sets all of these specialties enable scientific discovery across varied massive data collections collaborative research and data re use in both research and learning environments in 2012 two position papers ref cite web last1 borne first1 kirk title astroinformatics in a nutshell url https asaip psu edu articles astroinformatics in a nutshell website asaip psu edu publisher the astrostatistics and astroinformatics portal penn state university accessdate 11 january 2016 ref ref cite web last1 feigelson first1 eric title astrostatistics in a nutshell url https asaip psu edu articles astrostatistics in a nutshell website asaip psu edu publisher the astrostatistics and astroinformatics portal penn state university accessdate 11 january 2016 ref were presented to the council of the american astronomical society that led to the establishment of formal working groups in astroinformatics and astrostatistics for the profession of astronomy within the usa and elsewhere ref cite arxiv last1 feigelson first1 e last2 ivezi\xc4\x87 first2 \xc5\xbe last3 hilbe first3 j last4 borne first4 k title new organizations to support astroinformatics and astrostatistics arxiv 1301 3069 ref astroinformatics provides a natural context for the integration of education and research ref cite web last1 borne first1 kirk title the revolution in astronomy education data science for the masses url http adsabs harvard edu abs 2009astro2010p 7b website astrophysics data system publisher sao nasa accessdate 11 january 2016 ref the experience of research can now be implemented within the classroom to establish and grow data literacy through the easy re use of data ref cite web title using data in the classroom url http serc carleton edu usingdata index html website science education resource center at carleton college publisher national science digital library accessdate 11 january 2016 ref it also has many other uses such as repurposing archival data for new projects literature data links intelligent retrieval of information and many others ref cite book last1 borne first1 kirk title astroinformatics data oriented astronomy location george mason university usa url http www iccs meeting org iccs2009 posterpapers poster paper18 pdf accessdate january 21 2015 ref conferences class wikitable year place link 2016 sorrento italy http www iau org science meetings future symposia 1158 2015 dubrovnik dalmatia http iszd hr astroinfo2015 2014 university of chile http eventos cmm uchile cl astro2014 2013 australia telescope national facility csiro http www atnf csiro au research workshops 2013 astroinformatics 2012 microsoft research http www astro caltech edu ai12 2011 sorrento italy http dame dsf unina it astroinformatics2011 html 2010 caltech http www astro caltech edu ai10 see also astronomy and computing astrophysics data system astrophysics source code library astrostatistics galaxy zoo international astrostatistics association international virtual observatory alliance ivoa milkyway home virtual observatory worldwide telescope zooniverse citizen science project zooniverse external links http www adass org astronomical data analysis software and systems adass https asaip psu edu astrostatistics and astroinformatics portal https asaip psu edu organizations iaa iaa working group of cosmostatistics cosmostatistics initiative coin http www iau org science scientific bodies commissions b3 astroinformatics and astrostatistics commission of the international astronomical union references reflist category astronomy category astrophysics category big data category computational astronomy category data management category information science by discipline category applied statistics category computational fields of study'
b'about the software type deployed applications of the software type full text database primary sources date may 2012 a document oriented database or document store is a computer program designed for storing retrieving and managing document oriented information also known as semi structured model semi structured data document oriented databases are one of the main categories of nosql databases and the popularity of the term document oriented database has grown ref http db engines com en ranking categories db engines ranking per database model category ref with the use of the term nosql itself xml database s are a subclass of document oriented databases that are optimized to work with xml documents graph databases are similar but add another layer the relationship which allows them to link documents for rapid traversal document oriented databases are inherently a subclass of the key value database key value store another nosql database concept the difference lies in the way the data is processed in a key value store the data is considered to be inherently opaque to the database whereas a document oriented system relies on internal structure in the document in order to extract metadata that the database engine uses for further optimization although the difference is often moot due to tools in the systems efn to the point that document oriented and key value systems can often be interchanged in operation conceptually the document store is designed to offer a richer experience with modern programming techniques document databases efn and key value stores in general contrast strongly with the traditional relational database rdb relational databases generally store data in separate tables that are defined by the programmer and a single object may be spread across several tables document databases store all information for a given object in a single instance in the database and every stored object can be different from every other this makes mapping objects into the database a simple task normally eliminating anything similar to an object relational mapping this makes document stores attractive for programming web application s which are subject to continual change in place and where speed of deployment is an important issue documents the central concept of a document oriented database is the notion of a document while each document oriented database implementation differs on the details of this definition in general they all assume documents encapsulate and encode data or information in some standard formats or encodings encodings in use include xml yaml json and bson as well as binary forms like pdf and microsoft office documents ms word excel and so on documents in a document store are roughly equivalent to the programming concept of an object they are not required to adhere to a standard schema nor will they have all the same sections slots parts or keys generally programs using objects have many different types of objects and those objects often have many optional fields every object even those of the same class can look very different document stores are similar in that they allow different types of documents in a single store allow the fields within them to be optional and often allow them to be encoded using different encoding systems for example the following is a document encoded in json syntaxhighlight lang javascript firstname bob address 5 oak st hobby sailing syntaxhighlight a second document might be encoded in xml as syntaxhighlight lang xml contact firstname bob firstname lastname smith lastname phone type cell 123 555 0178 phone phone type work 890 555 0133 phone address type home type street1 123 back st street1 city boys city state ar state zip 32225 zip country us country address contact syntaxhighlight these two documents share some structural elements with one another but each also has unique elements the structure and text and other data inside the document are usually referred to as the document s content and may be referenced via retrieval or editing methods see below unlike a relational database where every record contains the same fields leaving unused fields empty there are no empty fields in either document record in the above example this approach allows new information to be added to some records without requiring that every other record in the database share the same structure document databases typically provide for additional metadata to be associated with and stored along with the document content that metadata may be related to facilities the datastore provides for organizing documents providing security or other implementation specific features crud operations the core operations a document oriented database supports on documents are similar to other databases and while the terminology isn t perfectly standardized most practitioners will recognize them as crud creation or insertion retrieval or query search finds update or edit deletion or removal keys documents are addressed in the database via a unique key that represents that document this key is a simple identifier or id typically a string computer science string a uri or a path computing path the key can be used to retrieve the document from the database typically the database retains an database index index on the key to speed up document retrieval and in some cases the key is required to create or insert the document into the database retrieval another defining characteristic of a document oriented database is that beyond the simple key to document lookup that can be used to retrieve a document the database offers an api or query language that allows the user to retrieve documents based on content or metadata for example you may want a query that retrieves all the documents with a certain field set to a certain value the set of query apis or query language features available as well as the expected performance of the queries varies significantly from one implementation to another likewise the specific set of indexing options and configuration that are available vary greatly by implementation it is here that the document store varies most from the key value store in theory the values in a key value store are opaque to the store they are essentially black boxes they may offer search systems similar to those of a document store but may have less understanding about the organization of the content document stores use the metadata in the document to classify the content allowing them for instance to understand that one series of digits is a phone number and another is a postal code this allows them to search on those types of data for instance all phone numbers containing 555 which would ignore the zip code 55555 editing document databases typically provide some mechanism for updating or editing the content or other metadata of a document either by allowing for replacement of the entire document or individual structural pieces of the document organization document database implementations offer a variety of ways of organizing documents including notions of collections groups of documents where depending on implementation a document may be enforced to live inside one collection or may be allowed to live in multiple collections tags and non visible metadata additional data outside the document content directory hierarchies groups of documents organized in a tree like structure typically based on path or uri sometimes these organizational notions vary in how much they are logical vs physical e g on disk or in memory representations relationship to other databases relationship to key value stores a document oriented database is a specialized key value database key value store which itself is another nosql database category in a simple key value store the document content is opaque a document oriented database provides apis or a query update language that exposes the ability to query or update based on the internal structure in the document this difference may be moot for users that do not need richer query retrieval or editing apis that are typically provided by document databases modern key value stores often include features for working with metadata blurring the lines between document stores relationship to search engines some search engines aka information retrieval systems like elasticsearch provide enough of the core operations on documents to fit the definition of a document oriented database relationship to relational databases cleanup section reason requires cleanup date july 2016 in a relational database data is first categorized into a number of predefined types and tables are created to hold individual entries or records of each type the tables define the data within each record s fields meaning that every record in the table has the same overall form the administrator also defines the relationships between the tables and selects certain fields that they believe will be most commonly used for searching and defines indexes on them a key concept in the relational design is that any data that may be repeated is normally placed in its own table and if these instances are related to each other a column is selected to group them together the foreign key this design is known as database normalization ref cite web url https support microsoft com en ca kb 283878 title description of the database normalization basics website microsoft ref for example an address book application will generally need to store the contact name an optional image one or more phone numbers one or more mailing addresses and one or more email addresses in a canonical relational database solution tables would be created for each of these rows with predefined fields for each bit of data the contact table might include first name last name and image columns while the phone number table might include country code area code phone number and type home work etc the phone number table also contains a foreign key column contact id which holds the unique id number assigned to the contact when it was created in order to recreate the original contact the database engine uses the foreign keys to look for the related items across the group of tables and reconstruct the original data in contrast in a document oriented database there may be no internal structure that maps directly onto the concept of a table and the fields and relationships generally don t exist as predefined concepts instead all of the data for an object is placed in a single document and stored in the database as a single entry in the address book example the document would contain the contact s name image and any contact info all in a single record that entry is accessed through its key which allows the database to retrieve and return the document to the application no additional work is needed to retrieve the related data all of this is returned in a single object a key difference between the document oriented and relational models is that the data formats are not predefined in the document case in most cases any sort of document can be stored in any database and those documents can change in type and form at any time if one wishes to add a country flag to a contact this field can be added to new documents as they are inserted this will have no effect on the database or the existing documents already stored to aid retrieval of information from the database document oriented systems generally allow the administrator to provide hints to the database to look for certain types of information these work in a similar fashion to indexes in the relational case most also offer the ability to add additional metadata outside of the content of the document itself for instance tagging entries as being part of an address book which allows the programmer to retrieve related types of information like all the address book entries this provides functionality similar to a table but separates the concept categories of data from its physical implementation tables in the classic normalized relational model objects in the database are represented as separate rows of data with no inherent structure beyond that given to them as they are retrieved this leads to problems when trying to translate programming objects to and from their associated database rows a problem known as object relational impedance mismatch ref cite web url http www agiledata org essays impedancemismatch html title the object relational impedance mismatch first scott last wambler website agile data ref document stores more closely or in some cases directly map programming objects into the store this eliminates the impedance mismatch problem and is offered as one of the main advantages of the nosql approach implementations main cat document oriented databases class wikitable sortable name publisher license languages supported notes representational state transfer restful api basex basex team free bsd license java programming language java xquery support for xml json and binary formats client server based architecture concurrent structural and full text searches and updates yes intersystems cach\xc3\xa9 cach\xc3\xa9 intersystems corporation proprietary java programming language java c sharp programming language c node js commonly used in health business and government applications yes cloudant cloudant inc proprietary erlang programming language erlang java programming language java scala programming language scala and c programming language c distributed database service based on bigcouch the company s open source fork of the apache software foundation apache backed couchdb project uses json model yes clusterpoint clusterpoint database clusterpoint ltd proprietary with free download javascript sql php net java programming language java python programming language python node js c programming language c c distributed document oriented xml json database platform with acid compliant transaction processing transactions high availability data replication and sharding built in full text search engine with relevance ranking js sql query language geographic information system gis available as pay per use cloud database database as a service or as an on premise free software download ref http www clusterpoint com document oriented database clusterpoint retrieved on 2015 10 08 ref yes couchbase server couchbase inc free apache license c net java programming language java python programming language python node js php sql golang spring framework linq distributed nosql document database json model and sql based query language yes ref http www couchbase com docs documentation couchbase retrieved on 2013 09 18 ref couchdb apache software foundation free apache license any language that can make http requests json over rest http with multi version concurrency control and limited acid properties uses map higher order function map and fold higher order function reduce for views and queries ref http couchdb apache org docs overview html couchdb overview webarchive url https web archive org web 20111020074113 http couchdb apache org docs overview html date october 20 2011 ref yes ref http wiki apache org couchdb http document api couchdb document api ref crateio crate technology gmbh free apache license java programming language java use familiar sql syntax for real time distributed queries across a cluster based on lucene elasticsearch ecosystem with built in support for binary objects blobs yes ref cite web url https crate io docs stable sql rest html title archived copy accessdate 2015 06 22 deadurl yes archiveurl https web archive org web 20150622174526 https crate io docs stable sql rest html archivedate 2015 06 22 df ref djondb djondb com gnu gpl and commercial c net java python nodejs php document store with support to transactions no documentdb microsoft proprietary net java programming language java python programming language python node js javascript sql platform as a service offering part of the microsoft azure platform yes elasticsearch shay banon free apache license java programming language java json search engine yes exist exist free lgpl xquery java programming language java xml over rest http webdav lucene fulltext search binary data support validation versioning clustering triggers url rewriting collections acls xquery update yes ref http exist db org exist db open source native xml database exist db org retrieved on 2013 09 18 ref hyperdex hyperdex org free bsd license c programming language c c go programming language go node js python programming language python ruby programming language ruby support for json and binary documents no informix ibm proprietary with no cost editions ref http www ibm com developerworks data library techarticle dm 0801doe ref various compatible with mongodb api rdbms with json replication sharding and acid compliance yes apache jackrabbit jackrabbit apache foundation free apache license java programming language java java content repository implementation dunno lotus notes ibm lotus domino ibm proprietary lotusscript java programming language java lotus formula multivalue yes marklogic marklogic corporation free developer license or commercial ref http developer marklogic com licensing ref rest java programming language java javascript node js xquery sparql xslt c distributed document oriented database for json xml and resource description framework rdf triples built in full text search acid transactions high availability and disaster recovery certified security yes mongodb mongodb inc free affero general public license gnu agpl v3 0 for the dbms apache 2 license for the client drivers ref http www mongodb org about licensing mongodb licensing ref c programming language c c c sharp programming language c java programming language java perl php python programming language python node js ruby programming language ruby scala programming language scala ref http docs mongodb org ecosystem drivers community supported drivers additional 30 community mongodb supported drivers ref document database with replication and sharding bson store binary format json yes ref http www mongodb org display docs http interface httpinterface restinterfaces mongodb rest interfaces ref mumps database dunno proprietary software proprietary and affero general public license affero gpl ref http sourceforge net projects fis gtm gtm mumps foss on sourceforge ref mumps commonly used in health applications dunno objectdatabase ekky software proprietary c c sharp programming language c tscript binary native c class structures dunno orientdb orient technologies free apache license java programming language java json over http sql support acid transactions yes postgresql postgresql free http www postgresql org about licence postgresql free license c programming language c hstore json store 9 2 json function 9 3 hstore2 9 4 jsonb 9 4 no qizx qualcomm commercial software commercial rest java programming language java xquery xslt c programming language c c python programming language python distributed document oriented xml database with integrated full text search support for json text and binaries yes rethinkdb dunno free affero general public license gnu agpl for the dbms apache 2 license for the client drivers c python programming language python javascript ruby programming language ruby java programming language java distributed document oriented json database with replication and sharding no rocket u2 rocket software proprietary dunno unidata universe yes beta sedna database sedna sedna org free apache license c xquery xml database no simpledb amazon proprietary online service erlang programming language erlang dunno solr apache free apache license java programming language java search engine yes tokumx tokutek free gnu affero general public license c c sharp programming language c go programming language go mongodb with fractal tree index fractal tree indexing dunno virtuoso universal server openlink virtuoso openlink software gplv2 1 and proprietary c c sharp programming language c java programming language java sparql middleware and database engine hybrid yes xml database implementations further information xml database most xml databases are document oriented databases see also database theory data hierarchy full text search in memory database internet message access protocol imap machine readable documents nosql object database online database real time database relational database notes notelist references reflist further reading assaf arkin 2007 september 20 https web archive org web 20080327222152 http blog labnotes org 80 2007 09 20 read consistency dumb databases smart services read consistency dumb databases smart services refend external links http db engines com en ranking document store db engines ranking of document stores by popularity updated monthly database models databases category document oriented databases category data management category database management systems category types of databases'
b'orphan date january 2016 don t mess with this line write your article below this line the sedona canada principles are a set of authoritative guidelines published by the sedona conference \xc2\xae to aid members of the canadian legal community involved in the identification collection preservation review and production of electronically stored information esi the principles were drafted by a small group of lawyers judges and technologists called the sedona working group 7 or sedona canada sedona canada is an offshoot of the sedona conference \xc2\xae which is an american non profit\xe2\x80\xa6research and educational institute dedicated to the advanced study of law and policy in the areas of antitrust law complex litigation and intellectual property rights ref cite web url https thesedonaconference org title the sedona conference\xc2\xae moving the law forward in a reasoned and just way work thesedonaconference org ref background civil procedure in canada is jurisdictional with each province following its own rules of civil procedure ref cite web url https en wikibooks org wiki canadian civil procedure rules by province title canadian civil procedure rules by province work wikibooks org ref however each province must address the fact that due to the advancement of technology the discovery process enshrined in the rules of civil procedure can be potentially derailed due to the sheer volume of electronically stored information esi ref name mccarthy ca cite web url http www mccarthy ca article detail aspx id 4068 title mccarthy t\xc3\xa9trault taming the beast of electronic discovery with sedona canada principles article detail work mccarthy ca ref when dealing with litigation matters that involve electronically stored information esi the discovery process is commonly called electronic discovery e discovery the problems associated with electronic discovery e discovery in canada led to the creation of the sedona canada principles ref name mccarthy ca rule 29 1 03 4 of the wikibooks ontario rules of civil procedure specifically refers to the sedona canada principles in referencing principles re electronic discovery although it has been reported that this rule has been largely ignored in practice ref name canlii org cite web url http www canlii org en on onsc doc 2010 2010onsc3670 2010onsc3670 html title canlii 2010 onsc 3670 canlii work canlii org ref summary the sedona canada principles largely refer to the processes found in the electronic discovery reference model ref cite web url http www edrm net resources edrm stages explained title edrm stages work edrm net ref the principles urge proportionality due to the potentially enormous volumes of documents that may be discoverable when dealing with esi they also encourage good faith in the document preservation stage and regular meetings between parties to discuss the scope of the litigation parties are urged to be aware of the potential costs involved in producing relevant esi but are advised that only reasonably accessible esi need be produced the principles stipulate that parties should not be required to search for or collect deleted material unless there is an agreement or court order related to those terms the use of electronic tools and processes such as data sampling and web harvesting are acceptable practices parties are encouraged to agree early in the litigation process on production format required for the exchange of relevant documents as part of the discovery process native files pdf tagged image file format tiff metadata requirements etc agreements or direction should be sought if necessary with respect to wikt privilege privilege or other confidential information related to production of electronic documents and data parties should be aware that legal precedents can be formed as a result of e discovery practices and sanctions can be considered for a party s failure to meet their discovery obligations unless it can be demonstrated that the failure was not intentional all parties must bear the reasonable costs associated with e discovery but other arrangements can be agreed upon by the parties or by court order ref cite web url https www canlii org en commentary sedonacanada principles en html title canlii the sedona canada principles addressing electronic discovery jan 2008 work canlii org ref caselaw in warman v national post company proportionality was at issue in a case where the plaintiff was suing the defendant for libel a motion was brought by the defendant to have the plaintiff provide a mirror image of his hard drive in an effort to prove an internet article was indeed authored by the plaintiff issues of proportionality and the work of the sedona conference and sedona canada principles were factored in to the decision to grant the defendant only limited access to the hard drive ref name canlii org in innovative health group inc v calgary health region the plaintiff s legal obligation to produce imaged hard drives is in question justice conrad refers to the advice of sedona canada on proportionality and problems associated with time and expense related to the difficulties associated with electronically stored information ref cite web url http www canlii org en ab abca doc 2008 2008abca219 2008abca219 html title canlii 2008 abca 219 canlii work canlii org ref in york university v michael markicevic justice brown specifically refers to the need for the parties to agree upon a formal e discovery plan to be drafted in consultation with sedona canada principles ref name canlii org1 cite web url http www canlii org en on onsc doc 2013 2013onsc378 2013onsc378 html title canlii 2013 onsc 378 canlii work canlii org ref in friends of lansdowne v ottawa master macleod refers to the need for sedona canada principles and states this is particularly true in the current information age when e mail is ubiquitous and multiple copies or variants of messages may be held on various kinds of data storage devices including individual hard drives e mail and blackberry servers even documents that ultimately exist in paper form normally begin their life on computers and negotiations frequently involve exchanges of electronic drafts to find every scrap of paper and every electronic trace of relevant information has become a nightmarish task that threatens to render any kind of litigation extravagantly expensive ref name canlii org1 criticism critics of the sedona canada principles believe they should address system integrity and that the true history of any file preserved cannot be identified without proof of the integrity of the electronic record systems management it comes from ref cite web url http papers ssrn com sol3 papers cfm abstract id 2530515 title the sedona canada principles are very inadequate on records management and for electronic discovery work ssrn com ref other criticism is more directed to the sedona canada working group and complaints that it is insular and irrelevant ref cite web url http www canadianlawyermag com 5078 sedona canada is alive and well html title sedona canada is alive and well author colin campbell and james swanson work canadianlawyermag com ref external links https www canlii org en commentary sedonacanada principles en html the sedona canada principles http www canadianlawyermag com 5078 sedona canada is alive and well html sedona canada is alive and well https www highbeam com doc 1g1 181488000 html taming the beast of ediscovery with sedona canada principles https www highbeam com doc 1g1 400332555 html 2014 ediscovery year in review includes sedona canada principles http www canadianlawyermag com legalfeeds 469 ontario judge slams dark ages court system html ontario courts discuss sedona canada principles http www canadianlawyermag com 3988 what is predictive coding and can it help me html sedona canada principles and predictive coding http www canadianlawyermag com 5019 alternative routes html document review using sedona canada principles references reflist after listing your sources please cite them using inline citations and place them after the information they cite please see http en wikipedia org wiki wikipedia refb for instructions on how to add citations stop be warned that by using this process instead of articles for creation this article is subject to scrutiny as an article in mainspace it will be deleted if there are problems not just declined if you wish to use afc please return to the wizard and continue from there category data management'
b'orphan date march 2016 data philanthropy describes a form of collaboration in which private sector companies share data for public benefit ref name pawelke pawelke a and tatevossian a 2013 may 8 http www unglobalpulse org data philanthropy where are we now data philanthropy where are we now united nations global pulse ref there are multiple uses of data philanthropy being explored from humanitarian corporate human rights and academic use since the introduction of this term the united nations global pulse has began pushing for a global data philanthropy movement ref name coren coren m 2011 december 9 http www fastcoexist com 1678963 data philanthropy open data for world changing solutions data philanthropy open data for world changing solutions fast company ref definition a large amount of data collected from the internet comes from user generated content this includes blogs posts on social networks and information submitted in forms besides user generated data corporations are also currently data mining data from consumers in order to understand customers identify new markets and make investment decisions kirkpatrick the director at united nations global pulse labels this data massive passive data or data exhaust ref name kirkpatrick kirkpatrick r 2011 september 20 http www forbes com sites oreillymedia 2011 09 20 data philanthropy is good for business data philanthropy is good for business forbes ref data philanthropy is the idea that something positive can come from this overload of data data philanthropy is defined as the private sector sharing this data in ways that the public can benefit ref name pawelke the term philanthropy helps to emphasis that data sharing is a positive act and that the shared data is a public good ref name kirkpatrick challenges a challenge that comes with sharing data is the internet privacy of the user whose data is being used mathematical techniques differential privacy and space time boxes have been introduced in order to make personal data accessible while providing the users providing such data with anonymity but even if these algorithms work there is always the possibility and fear of re identification ref name pawelke the other challenge is convincing corporations to share their data the big data corporations collect provides them with market competitiveness they are able to infer meaning regarding consumer behavior the fear is that by sharing all their information they may lose their competitive edge ref name pawelke sharing strategies the goal of data philanthropy is to create a global data commons where companies governments and individuals can contribute anonymous aggregated datasets ref name coren the united nations global pulse offers four different tactics that companies can use to share their data that preserve consumer anonymity these include ref name pawelke share aggregated and derived data sets for analysis under nondisclosure agreements nda allow researchers to analyze data within the private company s own network under nda real time data commons data pooled and aggregated between multiple companies of the same industry to protect competitiveness public private alerting network companies mine data behind their own firewalls and share indicators by providing these four tactics united nations global pulse hopes to provide initiative and options for companies to share their data with the public digital disease detection data philanthropy has led to advancements in the field of health and wellness by using data gathered from social media cell phones and other communication modes health researchers have been able to track the spread of diseases ref name schmidt schmidt c 2012 http www ncbi nlm nih gov pmc articles pmc3261963 trending now using social media to predict and track disease outbreaks environ health perspect 120 1 a30 a33 a30 a33 ref in the united states healthmap a freely available website and mobile app software is using data philanthropy related tactics to track the outbreak of diseases healthmap analyzes data from publicly available media sources such as news websites government alerts and social media sites like twitter for outbreaks of various illnesses around the world ref name schmidt ref name reddy reddy e 2015 july 14 https blog twitter com 2015 twitter data public health using twitter data to study the world s health twitter ref the creators of healthmap have another website flu near you which allows users to report their own health status on a weekly basis traditional flu surveillance can take up to 2 weeks to confirm outbreaks ref name schmidt doctors must wait for virological test to confirm the outbreak before reporting it to the centers for disease control this form of data philanthropy allows for up to date information regarding various health concerns by using publicly available information gathered from news outlets government alerts and social media sites it is the data gathered on social media sites where users are not aware their data is being mined that leads to healthmap and flue near you being considered data philanthropy ref name schmidt the centers for disease control and prevention collaborated with google and launched google flu trends in 2008 a website that tracks flu related searches and user location to track the spread of the flu users can visit the website to compare the amount of flu related search activity against the reported numbers of flu outbreaks on a graphic map the difficulty with this method of tracking is that google searched are sometimes performed due to curiosity rather than because an individual is suffering from the flu according to ashley fowlkes an epidemiologist in the cdc influenza division the google flu trends system tries to account for that type of media bias by modeling search terms over time to see which ones remain stable ref name schmidt google flu trends is not longer publishing current flu estimates on the public website visitors to the site can still view and download previous estimates current data can be shared with verified researchers ref name o connor o connor f 2015 august 20 http www pcworld com article 2974153 websites google flu trends calls out sick indefinitely html google flu trends calls out sick indefinitely pc world ref a study by harvard school of public health hsph released in the october 12 2012 issues of the journal science discussed how phone data helped curb the spread of malaria in kenya the researchers mapped phone calls and texts made by 14 816 521 kenyan mobile phone subscribers ref name datz datz t 2012 october 11 http www hsph harvard edu news press releases cell phone data malaria using cell phone data to curb the spread of malaria harvard chan ref when individuals left their primary living location the destination and length of journey was calculated this data was then compared to a 2009 malaria prevalence map to estimate the disease s commonness in each location combining all this information the researchers can estimate the probability of an individual carrying malaria and map the movement of the disease this research a result of data philanthropy can be used to track the spread of similar diseases ref name datz application in various fields through data philanthropy big data corporations such as social networking sites telecommunication companies search engines amongst others collect and make user generated information available to a data sharing system this also permits institutions to give back to a beneficial cause with the onset of technological advancements sharing data on a global scale and an in depth analysis of these data structures could alter the reaction towards certain occurrences be it natural disaster s epidemics worldwide economic problems and many other events some analyst have argued ref name forbes http www forbes com sites oreillymedia 2011 09 20 data philanthropy is good for business data philanthropy is good for business by robert kirkpatrick forbes 2011 09 20 ref that this aggregated information is beneficial for the common good and can lead to developments in research and data production in a range of varied fields ref name forbes humanitarian aid calling patterns of mobile phone users can determine the socioeconomic standings of the populace which can be used to deduce its access to housing education healthcare and basic services such as water and electricity ref name forbes researchers from columbia university and karolinska institute utilize information from mobile phone providers in order to assist in the dispersal of resources by deducing the movement of those displaced by natural disasters big data can also provide information on looming disasters and can assist relief organizations in rapid response and locating displaced individuals by analyzing certain patterns within this big data could successively transform the response to destructive occurrences like natural disasters outbreak s of diseases and global economic distress by employing real time information to achieve a comprehension of the welfare of individuals corporations utilize digital services such as human sensor systems to detect and solve impending problems within communities this is a strategy implemented by the private sector in order to protect its citizens by anonymously dispersing customer information to the public sector whilst also ensuring the protection of their privacy ref name forbes impoverished areas poverty still remains a worldwide issue with over 2 5 billion people ref name smart data collective http www smartdatacollective com rick delgado 200566 lifting how big data can help eliminate poverty lifting up how big data can help eliminate poverty by rick delgado smart data collection 2014 05 23 ref currently impoverished accumulating accurate data has been a complex issue but developments in technology and utilising big data ref name smart data collective is one solution for improving this situation statistics indicate the widespread use of mobile phones even within impoverished communities this availability could prove vital in gathering data on populations living in poverty additional data can be collected through internet access social media utility payments and governmental statistics data driven activities can lead to the cumulation of big data which in turn can assist international non governmental organization in documenting and evaluating the needs of underprivileged populations through data philanthropy ngo s can distribute information whilst cooperating with governments and private companies ref name smart data collective corporate data philanthropy incorporates aspects of social philanthropy by permitting corporations to create profound impacts through the act of giving back by dispersing proprietary datasets ref name irevolution http irevolution net 2012 06 04 big data philanthropy for humanitarian response big data philanthropy for humanitarian response by irevolution 2012 07 04 ref the public sector is faced with an unequal and limited access to the frequency of data and they also produce collect and preserve information which has proven to be an essential asset company s track and analyze users online activities so as to gain more insight into their needs in relation to new products and services ref https hbr org 2014 07 sharing data is a form of corporate philanthropy sharing data is a form of corporate philanthropy by matt stempeck harvard business review 2014 07 24 ref these companies view the welfare of the population as a vital key to the expansion and progression of businesses by using their data to place a spotlight on the plight of global citizens ref name forbes expert s in the private sector contend the importance of merging various data streams such as retail mobile phone and social media data to create necessary solutions to handle global issues despite the inevitable risk of sharing private information it works in a beneficial manner and serves the interest of the public ref https hbr org 2013 03 a new type of philanthropy don cm sp article links top 20of 20page 20recirculation a new type of philanthropy donating data by robert kirkpatrick harvard business review 2013 03 21 ref the digital revolution causes an extensive production of big data that is user generated and available on the web corporations accumulate information on customer preferences through the digital services they utilize and products they purchase in order to gain a clear insight on their clientele and future market opportunities ref name forbes however the rights of individuals concerning privacy and ownership of data are a controversial issue as governments and other institutions can use this collective data for other unethical purposes companies monitor and probe consumer online activities in order to better comprehend and develop tailored needs for their clientele and in turn increase their profits ref name jim fruchterman https hbr org 2013 03 big data means more than big p big data means more than big profits by jim fruchterman harvard business review 2013 03 19 ref academia data philanthropy plays an important role in academia researchers encounter countless obstacles whilst attempting to access data this data is available to a limited number of researchers with sole access to restricted resources who are authorized to utilize this information like social media streams enabling them to produce more knowledge and develop new studies for example twitter markets access to its real time apis at exorbitant prices which often surpasses the budgets of most researchers data grants ref name jim fruchterman is a trial program created by twitter that provides a selective number of academics and researchers with access to real time databases in order to garner more knowledge they apply to gain entry into vast data downloads on specific topics ref name jim fruchterman human rights data philanthropy aids the human rights movement by assisting in the dispersal of evidence for truth commissions and war crimes tribunals proponents of human rights accumulate data on abuse occurring within states which is then used for scientific analysis and propels awareness and action for example non profit organizations compile data from human rights monitors in war zones in order to assist the un high commissioner for human rights it uncovers inconsistencies in the number of casualties of war which in turn leads to international attention and exerts influence on discussions relating to global policy ref name jim fruchterman see also big data references references external links http www unglobalpulse org data philanthropy where are we now data philanthropy where are we now in un global pulse blog by adreas pawelke and anoush rima tatevossian 2013 05 08 category big data category data management'
b'infobox software name altitude3 net logo altitude en png screenshot caption altitude3 net dashboard developer nm\xc3\xa9dia solutions status active released start date and age 2000 06 01 operating system platform net framework net genre content management system content management framework alexa website url http altitude3 net altitude3 net is an electronic business development platform that allows to create web and mobile solutions along with interactive communication strategies the platform has the same functionalities ref cite web language fr title pinpoint url https pinpoint microsoft com fr ca companies 4296539037 services publisher pinpoint microsoft com accessdate 2015 06 22 ref ref cite web title altitude3 net url http www cmsmatrix org matrix cms matrix altitude 3 net website cms matrix accessdate 27 july 2015 ref than a content management system cms and communicates with other systems accounting systems manufacturing management software mrp business management software enterprise resource planning erp database excel files xml csv or all other kinds of structural data nm\xc3\xa9dia solutions developed altitude3 net in 2001 using microsoft s net framework technology the platform is currently using the 4 5 version of microsoft s framework history in 2001 nm\xc3\xa9dia solutions created the content management system altitude sup mc sup ref cite web title about url http altitude3 net home website altitude3 net accessdate 27 july 2015 ref as it went on many versions were developed altitude moto and altitude auto 2001 to 2006 altitude 2 2006 altitude3 net 2010 list of main functionalities the altitude3 net platform is structured in many modules ref cite web title altitude advantages url http altitude3 net home website altitude3 net accessdate 27 july 2015 ref content management contact management and mass emailing control of advanced seo parameters microsoft flexibility computability security access management security permission management e commerce solutions centralized product management cpm services this module includes several functionalities interface for mass product modification centralized coupon management custom management by product group inventory by store location shopping cart price currency management catalog management centralized database supplier management product by media product comparison tool based on common characteristics syncing accounting software inventory with altitude3 net a microsoft azure solution cloud computing omnichannel marketing other functionalities on site search engines for meta data and documents text word excel and pdf html5 video player with descending compatibility integrated functions enabling an entire site to be generated in hypertext markup language html or enabling to export all its data data and import it in any other cms awards and recognitions in 2010 altitude3 net is finalist in the it category software application of the m\xc3\xa9rites du fran\xc3\xa7ais during the francof\xc3\xaate in 2011 nm\xc3\xa9dia solutions wins the title of web development partner of the year awarded by the microsoft partner network ref cite news title microsoft honore deux entreprises de la r\xc3\xa9gion url http www lapresse ca la tribune economie et innovation 201112 19 01 4479237 microsoft honore deux entreprises de la region php accessdate 27 july 2015 publisher la presse canadian newspaper date 19 december 2011 language fr ref ref cite news title drummondville triomphe \xc3\xa0 toronto url http www journalexpress ca actualites economie 2011 12 05 article 2825943 drummondville triomphe a toronto 1 accessdate 27 july 2015 publisher journal l express date 5 december 2011 language fr ref in 2012 altitude3 net wins the prix franco awarded by the drummondville young chamber of commerce during its annual gala ref cite web title prix franco 2012 nm\xc3\xa9dia solutions inc url http www jccd ca concours prix franco prix franco 2012 nmedia solutions inc aspx website jeune chambre de commerce de drummondville accessdate 27 july 2015 language fr ref in 2015 the cpm module of altitude3 is finalist in the web solutions category at the octas ref cite web title les laur\xc3\xa9ats du concours des octas 2015 url http www actionti com microsites octas gagnants nos gagnants website r\xc3\xa9seau action ti accessdate 27 july 2015 language fr ref ref cite news title nm\xc3\xa9dia en lice aux octas url http www journalexpress ca actualites 2015 05 12 article 4144312 nmedias en lice aux octas 1 accessdate 27 july 2015 publisher journal l express date 12 may 2015 language fr ref see also list of content management systems references reflist 30em external links http altitude3 net altitude3 net s website category content management systems category website management category content management systems category data management category technical communication'
b'ad date august 2016 orphan date april 2016 use dmy dates date september 2016 infobox company name gb smith logo gb smith png caption type private traded as successor foundation 2007 founder sebastien goiffon and alexandre biegala defunct location city location country location locations area served key people industry software products services revenue operating income net income owner num employees 70 parent divisions subsid homepage url gbandsmith com footnotes intl gb smith is an independent software editor which provides layer independent matrix based console allowing instant visual review on any supported computing platform history gb smith was founded in 2007 by sebastien goiffon and alexandre biegala ref cite web title price entrepreneur of the year 2014 winners north region url http business lesechos fr entrepreneurs success stories prix de l entrepreneur de l annee 2014 les laureats region nord 103724 php publisher les \xc3\xa9chos newspaper les \xc3\xa9chos accessdate 20 march 2016 ref ref cite web title s implanter \xc3\xa0 londres les cl\xc3\xa9s du succ\xc3\xa8s selon gb smith url http www lesechos fr 02 04 2015 lesechos fr 0204265266931 s implanter a londres les cles du succes selon gb smith htm publisher les \xc3\xa9chos newspaper les \xc3\xa9chos accessdate 19 march 2016 ref as of 2016 company is becoming agnostic gradually offering it is security administration solutions for microsoft oracle corporation oracle ibm tableau software tableau ref name les cite web title gb smith secures corporate data url http www lesechos fr 26 12 2013 lesechos 21592 075 ech gb smith securise les donnees d entreprise htm publisher les \xc3\xa9chos newspaper les \xc3\xa9chos accessdate 20 march 2016 ref alexandre biegala and sebastien goiffon invented a method around identity access management iam to easily review and administer security rights of various applications and over multiple technologies through a single user interface ref name les ref cite web title gb smith lille 761 de croissance en cinq ans url http www lavoixdunord fr economie gb smith lille 761 de croissance en cinq ans ia0b0n1806557 publisher la voix du nord daily la voix du nord accessdate 20 march 2016 ref gbandsmith was granted a patent for a solution on security administration of rights and has become the security administration company and known to be a pioneer in administration intelligence and real self service security administration ref cite web title us patent issued to gb smith on feb 10 for matrix security management system for managing user accounts and security settings url https www highbeam com doc 1p3 3585409721 html website highbeam com accessdate 19 march 2016 ref businessobjects co founder denis payre joined gb smith on 1 april 2016 in 1996 denis payre and his partner bernard liautaud were ranked by business week among the best entrepreneurs alongside steve jobs and steven spielberg ref cite web title gb smith announces denis payre co founder of business objects to join its board of directors url http www bizjournals com prnewswire press releases 2016 04 01 ne60812 website the business journals accessdate 1 april 2016 ref solutions 365suite agnostic self service security administration 365suite is a set of agnostic tools solutions focused on security administrations such as access rights management security policy audits and related metadata 365suite enables centralizing security administration into a single console managing multiple applications 365 runs on top of solutions such as microsoft sharepoint microsoft active directory sap sap businessobjects bo sap hana hana oracle oracle business intelligence suite enterprise edition obiee oracle database odb tableau software tableau etc ref cite web title le comparateur assurance remporte le premier prix du fast50 avec une croissance de 1 562 en 4 ans url http www lavoixdunord fr economie le comparateur assurance remporte le premier prix du fast50 ia0b0n3165825 publisher la voix du nord daily la voix du nord accessdate 20 march 2016 ref 365 solutions consists in two solutions 365view single security administration console to operate multiple it solutions simultaneously sharepoint oarcle bi 365eyes centralized metadata repository focused on security administration with ability to operate monitor restore and compare metadata from multiple it solutions 360suite 360suite consists in a suite of eight solutions focused around sap se sap businessobjects 360plus backup incremental backup promotion including ability to restore deleted files ref cite web title le fran\xc3\xa7ais gb smith invente le concept prometteur d administration intelligence url http www channelnews fr le francais gb a smith invente le concept prometteur dadministration intelligence 21842 publisher channelnews accessdate 19 march 2016 ref 360view security administration via a security matrix crossing ressources and users bulk updates unv to unx unbounded documents ref cite web title gb smith un esprit de conqu\xc3\xaate et un esprit libre url http www lopinion fr 2 decembre 2014 gb smith esprit conquete esprit libre 18979 publisher l opinion newspaper l opinion accessdate 19 march 2016 ref 360cast schedule and burst dynamically reports ref cite web title la soci\xc3\xa9t\xc3\xa9 ugitech choisit les solutions 360suite de gb smith pour administrer sa plateforme sap businessobjects bi 4 0 url http www decideo fr la societe ugitech choisit les solutions 360suite de gb smith pour administrer sa plateforme sap businessobjects bi 4 0 a6323 html publisher decideo accessdate 19 march 2016 ref 360eyes explore and analyze bo metadata and perform impact analysis 360eyes compliance compliance to ensure bo compliance 360vers facilitate and monitor bo versioning 360bind automate bo non regression tests with ability to compare results and pixels from webi deski and crystal reports 360init initialize and import your bo security recognition 2013 15 deloitte fast 500 fast 500 emea deloitte emea technology fast 500 ref cite web title technology fast 500 emea 2013 ranking url http www2 deloitte com content dam deloitte global documents technology media telecommunications dttl tmt event fast 500 2013 winners ranking pdf publisher deloitte com accessdate 19 march 2016 ref ref cite web title 2015 technology fast 500tm europe middle east africa emea ranking url https www2 deloitte com content dam deloitte global documents technology media telecommunications gx deloitte tmt emea fast500 2015 rankings pdf publisher deloitte fast 500 accessdate 19 march 2016 ref ref cite web title technology fast 50 url http www2 deloitte com content dam deloitte fr documents technology 20fast 2050 deloitte palmar c3 a8s fast50 2014 pdf publisher deloitte com accessdate 19 march 2016 ref 2014 ernst young emerging company ref cite web title s\xc3\xa9bastien goiffon et alexandre biegala re\xc3\xa7oivent le prix de l entreprise d avenir de l ann\xc3\xa9e 2014 pour le nord de france url http blog gbandsmith com wp content uploads 2014 10 cp resultats ceremonie nord de france 2014 gbs2 0 pdf publisher ey com accessdate 19 march 2016 ref references reflist external links official website http www gbandsmith com defaultsort gb and smith category computer access control category business intelligence companies category identity management category data analysis software category data management'
b'redirect structured storage the microsoft technology also known as structured storage com structured storage a nosql originally referring to non sql non relational or not only sql ref http nosql database org nosql definition next generation databases mostly addressing some of the points being non relational distributed open source and horizontally scalable ref database provides a mechanism for computer data storage storage and data retrieval retrieval of data which is modeled in means other than the tabular relations used in relational database s such databases have existed since the late 1960s but did not obtain the nosql moniker until a surge of popularity in the early twenty first century r leavitt triggered by the needs of web 2 0 companies such as facebook google and amazon com ref cite conference title history repeats itself sensible and nonsensql aspects of the nosql hoopla first c last mohan conference proc 16th int l conf on extending database technology year 2013 url http openproceedings eu 2013 conf edbt mohan13 pdf ref ref http www eventbrite com e nosql meetup tickets 341739151 dynamo clones and bigtables ref ref http www wired com 2012 01 amazon dynamodb amazon helped start the nosql movement ref nosql databases are increasingly used in big data and real time web applications ref cite web url http db engines com en blog post 23 title rdbms dominate the database market but nosql systems are catching up publisher db engines com date 21 nov 2013 accessdate 24 nov 2013 ref nosql systems are also sometimes called not only sql to emphasize that they may support sql like query languages ref cite web url http searchdatamanagement techtarget com definition nosql not only sql title nosql not only sql quote nosql database also called not only sql ref ref cite web url http martinfowler com bliki nosqldefinition html title nosqldefinition first martin last fowler authorlink martin fowler quote many advocates of nosql say that it does not mean a no to sql rather it means not only sql ref motivations for this approach include simplicity of design simpler horizontal scaling horizontal and vertical scaling horizontal scaling to cluster computing clusters of machines which is a problem for relational databases ref name leavitt cite journal first neal last leavitt title will nosql databases live up to their promise journal ieee computer year 2010 url http www leavcom com pdf nosql pdf ref and finer control over availability the data structures used by nosql databases e g key value wide column graph or document are different from those used by default in relational databases making some operations faster in nosql the particular suitability of a given nosql database depends on the problem it must solve sometimes the data structures used by nosql databases are also viewed as more flexible than relational database tables ref http www allthingsdistributed com 2012 01 amazon dynamodb html customers like simpledb s table interface and its flexible data model not having to update their schemas when their systems evolve makes life much easier ref many nosql stores compromise consistency database systems consistency in the sense of the cap theorem in favor of availability partition tolerance and speed barriers to the greater adoption of nosql stores include the use of low level query languages instead of sql for instance the lack of ability to perform ad hoc joins across tables lack of standardized interfaces and huge previous investments in existing relational databases ref cite web url http www journalofcloudcomputing com content pdf 2192 113x 2 22 pdf title data management in cloud environments nosql and newsql data stores first1 k last1 grolinger first2 w a last2 higashino first3 a last3 tiwari first4 m a m last4 capretz date 2013 publisher aira springer accessdate 8 jan 2014 ref most nosql stores lack true acid transactions although a few databases such as marklogic aerospike database aerospike faircom c treeace google spanner database spanner though technically a newsql database symas lightning memory mapped database lmdb and orientdb have made them central to their designs see acid and join support acid and join support instead most nosql databases offer a concept of eventual consistency in which database changes are propagated to all nodes eventually typically within milliseconds so queries for data might not return updated data immediately or might result in reading data that is not accurate a problem known as stale reads ref https aphyr com posts 322 call me maybe mongodb stale reads ref additionally some nosql systems may exhibit lost writes and other forms of data loss ref martin zapletal large volume data analysis on the typesafe reactive platform scaladays 2015 http www slideshare net martinzapletal zapletal martinlargevolumedataanalytics slides ref fortunately some nosql systems provide concepts such as write ahead logging to avoid data loss ref http www dummies com how to content 10 nosql misconceptions html nosql databases lose data section ref for distributed transaction processing across multiple databases data consistency is an even bigger challenge that is difficult for both nosql and relational databases even current relational databases do not allow referential integrity constraints to span databases ref https iggyfernandez wordpress com 2013 07 28 no to sql and no to nosql ref there are few systems that maintain both acid transactions and x open xa standards for distributed transaction processing history the term nosql was used by carlo strozzi in 1998 to name his lightweight strozzi nosql rdbms strozzi nosql open source relational database that did not expose the standard sql structured query language sql interface but was still relational ref name 0 cite web url http publications lib chalmers se records fulltext 123839 pdf title investigating storage solutions for large data a comparison of well performing and scalable data storage solutions for real time extraction and batch insertion of data first adam last lith first2 jakob last2 mattson date 2010 publisher department of computer science and engineering chalmers university of technology location g\xc3\xb6teborg page 70 accessdate 12 may 2011 quote carlo strozzi first used the term nosql in 1998 as a name for his open source relational database that did not offer a sql interface ref his nosql rdbms is distinct from the circa 2009 general concept of nosql databases strozzi suggests that because the current nosql movement departs from the relational model altogether it should therefore have been called more appropriately norel ref cite web url http www strozzi it cgi bin csa tw7 i en us nosql home 20page title nosql relational database management system home page publisher strozzi it date 2 october 2007 accessdate 29 march 2010 ref referring to no relational johan oskarsson of last fm reintroduced the term nosql in early 2009 when he organized an event to discuss open source distributed database distributed non relational databases ref cite web url http blog sym link com 2009 05 12 nosql 2009 html title nosql 2009 publisher blog sym link com date 12 may 2009 accessdate 29 march 2010 ref the name attempted to label the emergence of an increasing number of non relational distributed data stores including open source clones of google s bigtable mapreduce and amazon s dynamo most of the early nosql systems did not attempt to provide acid atomicity consistency isolation and durability guarantees contrary to the prevailing practice among relational database systems ref cite web url http databases about com od specificproducts a acid htm title the acid model first mike last chapple ref based on 2014 revenue the nosql market leaders are marklogic mongodb and datastax ref cite web accessdate 2015 11 17 url http wikibon com hadoop nosql software and services market forecast 2013 2017 title hadoop nosql rankings ref based on 2015 popularity rankings the most popular nosql databases are mongodb apache cassandra and redis ref cite web accessdate 2015 07 31 url http db engines com en ranking title db engines ranking ref types and examples of nosql databases there have been various approaches to classify nosql databases each with different categories and subcategories some of which overlap what follows is a basic classification by data model with examples column data store column accumulo apache cassandra cassandra druid open source data store druid hbase vertica sap hana document oriented database document apache couchdb clusterpoint couchbase documentdb hyperdex lotus notes ibm domino marklogic mongodb orientdb qizx rethinkdb key value store key value aerospike database aerospike couchbase dynamo storage system dynamo faircom c treeace foundationdb hyperdex memcachedb mumps oracle nosql database orientdb redis riak berkeley db graph database graph allegrograph arangodb infinitegraph apache giraph marklogic neo4j orientdb virtuoso universal server virtuoso stardog multi model database multi model alchemy database arangodb cortexdb couchbase foundationdb marklogic orientdb a more detailed classification is the following based on one from stephen yen ref cite web url https dl dropboxusercontent com u 2075876 nosql steve yen pdf format pdf title nosql is a horseless carriage last yen first stephen publisher northscale accessdate 2014 06 26 ref style text align left class wikitable sortable type examples of this type key value cache oracle coherence coherence ibm websphere extreme scale extreme scale gigaspaces gemfire hazelcast infinispan jboss cache memcached repcached terracotta inc terracotta velocity memory cache velocity key value store flare keyspace ramcloud schemafree hyperdex aerospike database aerospike key value store eventually consistent dovetaildb oracle nosql database dynamo storage system dynamo riak dynomite motiondb voldemort distributed data store voldemort subrecord key value store ordered actord foundationdb lightcloud lightning memory mapped database lmdb luxio memcachedb nmdb scalaris tokyotyrant data structures server redis tuple store jini apache river coord gigaspaces object database db4o objectivity db perst shoal zope object database zopedb document store clusterpoint couchbase couchdb documentdb lotus notes ibm domino marklogic mongodb qizx rethinkdb xml database xml databases wide column store wide column store bigtable apache cassandra cassandra druid open source data store druid apache hbase hbase hypertable kai kdi openneptune qbase correlation database s are model independent and instead of row based or column based storage use value based storage key value store main key value database key value kv stores use the associative array also known as a map or dictionary as their fundamental data model in this model data is represented as a collection of key value pairs such that each possible key appears at most once in the collection ref cite web accessdate 1 january 2012 publisher stackexchange location http dba stackexchange com questions 607 what is a key value store database title key value stores and the nosql movement author sandy date 14 january 2011 url http dba stackexchange com a 619 quote key value stores allow the application developer to store schema less data this data usually consists of a string that represents the key and the actual data that is considered the value in the key value relationship the data itself is usually some kind of primitive of the programming language a string an integer or an array or an object that is being marshaled by the programming language s bindings to the key value store this structure replaces the need for a fixed data model and allows proper formatting ref ref cite web accessdate 1 january 2012 publisher marc seeger location http blog marc seeger de 2009 09 21 key value stores a practical overview title key value stores a practical overview first marc last seeger date 21 september 2009 url http blog marc seeger de assets papers ultra large sites ss09 seeger key value stores pdf quote key value stores provide a high performance alternative to relational database systems with respect to storing and accessing data this paper provides a short overview of some of the currently available key value stores and their interface to the ruby programming language ref the key value model is one of the simplest non trivial data models and richer data models are often implemented as an extension of it the key value model can be extended to a discretely ordered model that maintains keys in lexicographical order lexicographic order this extension is computationally powerful in that it can efficiently retrieve selective key ranges ref cite web accessdate 8 may 2014 publisher ilya katsov title nosql data modeling techniques first ilya last katsov date 1 march 2012 url http highlyscalable wordpress com 2012 03 01 nosql data modeling techniques ref key value stores can use consistency model s ranging from eventual consistency to serializability some databases support ordering of keys there are various hardware implementations and some users maintain data in memory ram while others employ solid state drive s or hard disk drive rotating disks examples include oracle nosql database redis and dbm document store main document oriented database xml database the central concept of a document store is the notion of a document while each document oriented database implementation differs on the details of this definition in general they all assume that documents encapsulate and encode data or information in some standard formats or encodings encodings in use include xml yaml and json as well as binary forms like bson documents are addressed in the database via a unique key that represents that document one of the other defining characteristics of a document oriented database is that in addition to the key lookup performed by a key value store the database offers an api or query language that retrieves documents based on their contents different implementations offer different ways of organizing and or grouping documents collections tags non visible metadata directory hierarchies compared to relational databases for example collections could be considered analogous to tables and documents analogous to records but they are different every record in a table has the same sequence of fields while documents in a collection may have fields that are completely different graph main graph database this kind of database is designed for data whose relations are well represented as a graph discrete mathematics graph consisting of elements interconnected with a finite number of relations between them the type of data could be social relations public transport links road maps or network topologies graph databases and their query language style text align left class wikitable sortable name language s notes allegrograph sparql resource description framework rdf triple store dex graph database dex sparksee c java programming language java net framework net python programming language python graph database flockdb scala programming language scala graph database ibm db2 sparql resource description framework rdf triple store added in db2 10 infinitegraph java programming language java graph database marklogic java programming language java javascript sparql xquery multi model document oriented database document database and resource description framework rdf triple store neo4j cypher query language cypher graph database ontotext owlim java programming language java sparql sparql 1 1 resource description framework rdf triple store oracle database oracle sparql sparql 1 1 resource description framework rdf triple store added in 11g orientdb java programming language java sql multi model document oriented database document and graph database sqrrl sqrrl enterprise java programming language java graph database virtuoso universal server openlink virtuoso c c sharp programming language c java programming language java sparql middleware and database engine hybrid stardog java programming language java sparql graph database object database main object database db4o gemstone database gemstone s intersystems cach\xc3\xa9 jade programming language jade objectdatabase objectdb objectivity db objectstore odaba odaba perst virtuoso universal server openlink virtuoso versant object database zodb tabular apache accumulo bigtable hbase apache hbase hypertable mnesia virtuoso universal server openlink virtuoso tuple store apache river gigaspaces tarantool tibco software tibco activespaces virtuoso universal server openlink virtuoso triple quad store rdf database main triplestore named graph allegrograph jena framework apache jena it is a framework not a database marklogic ontotext ontotext owlim oracle nosql database oracle nosql database virtuoso universal server stardog hosted amazon dynamodb amazon simpledb appengine datastore on google appengine clusterpoint clusterpoint database cloudant cloudant data layer couchdb freebase database freebase microsoft azure table service microsoft azure tables ref http azure microsoft com en gb services storage tables ref documentdb microsoft azure documentdb ref http azure microsoft com en gb services documentdb ref virtuoso universal server openlink virtuoso multivalue databases d3 pick database extensible storage engine ese nt infinitydb intersystems cach\xc3\xa9 jbase pick database northgate information solutions reality the original pick mv database openqm revelation software s openinsight rocket u2 multimodel database couchbase foundationdb marklogic orientdb performance ben scofield rated different categories of nosql databases as follows ref cite web url http www slideshare net bscofield nosql codemash 2010 title nosql death to relational databases last scofield first ben date 2010 01 14 accessdate 2014 06 26 ref style text align left class wikitable sortable data model performance scalability flexibility complexity functionality key value store high high high none variable none column oriented store high high moderate low minimal document oriented store high variable high high low variable low graph database variable variable high high graph theory relational database variable variable low moderate relational algebra performance and scalability comparisons are sometimes done with the ycsb benchmark see also comparison of structured storage software handling relational data since most nosql databases lack ability for joins in queries the database schema generally needs to be designed differently there are three main techniques for handling relational data in a nosql database see table join and acid support for nosql databases that support joins multiple queries instead of retrieving all the data with one query it s common to do several queries to get the desired data nosql queries are often faster than traditional sql queries so the cost of having to do additional queries may be acceptable if an excessive number of queries would be necessary one of the other two approaches is more appropriate caching replication non normalized data instead of only storing foreign keys it s common to store actual foreign values along with the model s data for example each blog comment might include the username in addition to a user id thus providing easy access to the username without requiring another lookup when a username changes however this will now need to be changed in many places in the database thus this approach works better when reads are much more common than writes ref name datamodeling couchbase com december 5 2014c cite web url http www couchbase com sites default files uploads all whitepapers couchbase whitepaper transitioning relational to nosql pdf title making the shift from relational to nosql newspaper couchbase com accessdate december 5 2014 ref nesting data with document databases like mongodb it s common to put more data in a smaller number of collections for example in a blogging application one might choose to store comments within the blog post document so that with a single retrieval one gets all the comments thus in this approach a single document contains all the data you need for a specific task acid and join support if a database is marked as supporting acid or join sql joins then the documentation for the database makes that claim the degree to which the capability is fully supported in a manner similar to most sql databases or the degree to which it meets the needs of a specific application is left up to the reader to assess class wikitable database acid joins aerospike company aerospike yes no arangodb yes yes couchdb yes yes c treeace yes yes hyperdex yes refn name hyperdexacid group nb hyperdex currently offers acid support via its warp extension which is a commercial add on yes infinitydb yes no lightning memory mapped database lmdb yes no marklogic yes yes refn name marklogicjoins group nb joins do not necessarily apply to document databases but marklogic can do joins using semantics ref http www gennet com big data cant joins marklogic just matter semantics ref orientdb yes yes reflist group nb see also please do not list specific implementations here cap theorem comparison of object database management systems comparison of structured storage software correlation database distributed cache faceted search multivalue database multi model database triplestore schema agnostic databases references reflist 33em further reading cite book first1 pramod last1 sadalage first2 martin last2 fowler authorlink2 martin fowler date 2012 title nosql distilled a brief guide to the emerging world of polyglot persistence publisher addison wesley isbn 0 321 82662 0 cite book first1 dan last1 mccreary first2 ann last2 kelly date 2013 title making sense of nosql a guide for managers and the rest of us isbn 9781617291074 cite book first1 lena last1 wiese date 2015 title advanced data management for sql nosql cloud and distributed databases publisher degruyter oldenbourg isbn 978 3 11 044140 6 cite web first christof last strauch date 2012 title nosql databases url http www christof strauch de nosqldbs pdf cite journal last1 moniruzzaman first1 a b last2 hossain first2 s a date 2013 title nosql database new era of databases for big data analytics classification characteristics and comparison arxiv 1307 0191 cite journal first kai last orend date 2013 title analysis and classification of nosql databases and evaluation of their ability to replace an object relational persistence layer citeseerx 10 1 1 184 483 cite web first1 ganesh last1 krishnan first2 sarang last2 kulkarni first3 dharmesh kirit last3 dadbhawala title method and system for versioned sharing consolidating and reporting information url https www google com patents us7383272 pg pa1 dq ganesh krishnan hl en sa x external links cite web url http www christof strauch de nosqldbs pdf title nosql whitepaper first christoph last strauch publisher hochschule der medien location stuttgart cite web url http nosql database org title nosql database list first stefan last edlich cite web year 2010 url http www infoq com articles graph nosql neo4j title graph databases nosql and neo4j first peter last neubauer cite web year 2012 url http www networkworld com article 2160905 tech primers a vendor independent comparison of nosql databases cassandra hbase mongodb riak html title a vendor independent comparison of nosql databases cassandra hbase mongodb riak first sergey last bushik publisher networkworld cite web year 2014 url http www odbms org category downloads nosql data stores nosql data stores articles title nosql data stores articles papers presentations first roberto v last zicari website odbms org use dmy dates date february 2012 databases category nosql category data management category distributed data stores category structured storage'
b'primary sources date november 2010 windows communication foundation wcf data services formerly ado net data services ref cite web url http blogs msdn com astoriateam archive 2009 11 17 simplifying our n tier development platform making 3 things 1 thing aspx title simplifying our n tier development platform making 3 things 1 thing date 2009 11 17 accessdate 2009 12 17 publisher ado net data services team blog ref codename astoria ref cite web url http blogs msdn com data archive 2007 12 10 ado net data services ctp released aspx title ado net data services ctp released accessdate 2007 11 12 ref is a platform for what microsoft calls data services it is actually a combination of the runtime and a web service through which the services are exposed in addition it also includes the data services toolkit which lets astoria data services be created from within asp net itself the astoria project was announced at mix microsoft mix 2007 and the first developer preview was made available on april 30 2007 the first software release life cycle beta ctp was made available as a part of the asp net 3 5 extensions preview the final version was released as part of service pack 1 of the net framework 3 5 on august 11 2008 the name change from ado net data services to wcf data services was announced at the 2009 professional developers conference pdc overview wcf data services exposes data represented as ado net entity framework entity data model edm objects via web services accessed over http the data can be addressed using a representational state transfer rest like uri the data service when accessed via the http get method with such a uri will return the data the web service can be configured to return the data in either plain xml json or resource description framework rdf xml in the initial release formats like rss and atom are not supported though they may be in the future in addition using other http methods like put post or delete the data can be updated as well post can be used to create new entities put for updating an entity and delete for deleting an entity description windows communication foundation wcf comes to the rescue when we find ourselves not able to achieve what we want to achieve using web services i e other protocols support and even duplex communication with wcf we can define our service once and then configure it in such a way that it can be used via http tcp ipc and even message queues we can consume web services using server side scripts asp net javascript object notations json and even rest representational state transfer understanding the basics when we say that a wcf service can be used to communicate using different protocols and from different kinds of applications we will need to understand how we can achieve this if we want to use a wcf service from an application then we have three major questions 1 where is the wcf service located from a client s perspective 2 how can a client access the service i e protocols and message formats 3 what is the functionality that a service is providing to the clients once we have the answer to these three questions then creating and consuming the wcf service will be a lot easier for us the wcf service has the concept of endpoints a wcf service provides endpoints which client applications can use to communicate with the wcf service the answer to these above questions is what is known as the abc of wcf services and in fact are the main components of a wcf service so let s tackle each question one by one address like a webservice a wcf service also provides a uri which can be used by clients to get to the wcf service this uri is called as the address of the wcf service this will solve the first problem of where to locate the wcf service for us binding once we are able to locate the wcf service we should think about how to communicate with the service protocol wise the binding is what defines how the wcf service handles the communication it could also define other communication parameters like message encoding etc this will solve the second problem of how to communicate with the wcf service for us contract now the only question we are left up with is about the functionalities that a wcf service provides contract is what defines the public data and interfaces that wcf service provides to the clients the uris representing the data will contain the physical location of the service as well as the service name in addition it will also need to specify an edm entity set or a specific entity instance as in respectively nowiki http dataserver service svc musiccollection nowiki or nowiki http dataserver service svc musiccollection someartist nowiki the former will list all entities in the collection set whereas the latter will list only for the entity which is indexed by someartist in addition the uris can also specify a traversal of a relationship in the entity data model for example nowiki http dataserver service svc musiccollection somesong genre nowiki traverses the relationship genre in sql parlance joins with the genre table and retrieves all instances of genre that are associated with the entity somesong simple predicates can also be specified in the uri like nowiki http dataserver service svc musiccollection someartist releasedate year eq 2006 nowiki will fetch the items that are indexed by someartist and had their release in 2006 filtering and partition information can also be encoded in the url as nowiki http dataserver service svc musiccollection orderby releasedate skip 100 top 50 nowiki it is important to note that although the presence of skip and top keywords indicate paging support in data services version 1 there is no method of determining the number of records available and thus impossible to determine how many pages there may be the open data protocol odata 2 0 spec adds support for the count path segment to return just a count of entities and inlinecount to retrieve a page worth of entities and a total count without a separate round trip ref http msdn microsoft com en us library ee373845 aspx ref references reflist refbegin cite web url http blogs msdn com pablo archive 2007 04 30 codename astoria data services for the web aspx title codename astoria data services for the web accessdate 2007 04 30 http astoria mslivelabs com ado net data services framework formerly project astoria refend external links http msdn microsoft com en us library cc907912 aspx using microsoft ado net data services http www asp net downloads 3 5 extensions asp net 3 5 extensions preview http blogs msdn com astoriateam ado net data services project astoria team blog http entmag com news article asp editorialsid 9105 access cloud data with astoria ent news online net framework category data management category web services category ado net data access technologies category net framework'
b'data discovery is a business intelligence architecture aimed at creating and using interactive reports and explorable data from multiple sources according to the united states information technology research and advisory firm gartner data discovery has become a mainstream architecture in 2012 ref kern j 2013 http www information management com news data discovery saas lead bi market review 10024484 1 html data discovery saas lead bi market review information management ref definition data discovery is a user driven process of searching for patterns or specific items in a data set data discovery applications use visual tools such as geographical maps pivot tables and heat maps to make the process of finding patterns or specific items rapid and intuitive data discovery may leverage statistical and data mining techniques to accomplish these goals data discovery and business intelligence bi data discovery is a type of business intelligence in that they both provide the end user with an application that visualizes data traditional bi covered dashboards static and parameterized reports and pivot tables visualization of data in traditional bi incorporated standard charting kpis and limited graphical representation and interactivity bi is undergoing transformation in capabilities it offers with a focus on end user data analysis and discovery access to larger volumes of data and an ability to create high fidelity presentations of information players data discovery vendors include tableau software tableau qlik tibco software tibco spotfire microsoft power bi microstrategy sap lumira platfora datameer clearstory data answerrocket and datawatch ref the rise of data discovery has set the stage for a major strategic shift in the bi and analytics platform market 15 june 2015 g00277789 analyst s josh parenteau rita l sallam cindi howson ref see also business intelligence business intelligence tools references reflist 30em category business intelligence category data management'
b'use mdy dates date february 2012 file kd sqlia classification 2010 png thumb alt classification of sql injection attack vectors in 2010 a classification of sql injection attacking vector as of 2010 sql injection is a code injection technique used to attack computing attack data driven applications in which nefarious sql statements are inserted into an entry field for execution e g to dump the database contents to the attacker ref cite web url http technet microsoft com en us library ms161953 28v sql 105 29 aspx title sql injection accessdate 2013 08 04 author microsoft quote sql injection is an attack in which malicious code is inserted into strings that are later passed to an instance of sql server for parsing and execution any procedure that constructs sql statements should be reviewed for injection vulnerabilities because sql server will execute all syntactically valid queries that it receives even parameterized data can be manipulated by a skilled and determined attacker ref sql injection must exploit a security vulnerability in an application s software for example when user input is either incorrectly filtered for string literal escape sequence escape characters embedded in sql statements or user input is not strongly typed programming language strongly typed and unexpectedly executed sql injection is mostly known as an attack vector malware vector for websites but can be used to attack any type of sql database sql injection attacks allow attackers to spoof identity tamper with existing data cause repudiation issues such as voiding transactions or changing balances allow the complete disclosure of all data on the system destroy the data or make it otherwise unavailable and become administrators of the database server in a 2012 study it was observed that the average web application received 4 attack campaigns per month and retailers received twice as many attacks as other industries ref cite web url http www imperva com docs hii web application attack report ed4 pdf title imperva web application attack report accessdate 2013 08 04 author imperva date july 2012 format pdf quote retailers suffer 2x as many sql injection attacks as other industries while most web applications receive 4 or more web attack campaigns per month some websites are constantly under attack one observed website was under attack 176 out of 180 days or 98 of the time ref history the first public discussions of sql injection started appearing around 1998 ref cite web title how was sql injection discovered the researcher once known as rain forrest puppy explains how he discovered the first sql injection more than 15 years ago author sean michael kerner date november 25 2013 url http www esecurityplanet com network security how was sql injection discovered html ref for example a 1998 article in phrack magazine ref cite journal title nt web technology vulnerabilities author jeff forristal signing as rain forest puppy journal phrack magazine volume 8 issue 54 article 8 date dec 25 1998 url http www phrack com issues html issue 54 id 8 article ref form sql injection sqli is considered one of the top 10 web application vulnerabilities of 2007 and 2010 by the owasp open web application security project ref cite web url https www owasp org index php category owasp top ten project title category owasp top ten project publisher owasp accessdate 2011 06 03 ref in 2013 sqli was rated the number one attack on the owasp top ten ref cite web url https www owasp org index php top 10 2013 top 10 title category owasp top ten project publisher owasp accessdate 2013 08 13 ref there are four main sub classes of sql injection classic sqli blind or inference sql injection database management system specific sqli compounded sqli sql injection insufficient authentication ref cite web url http www xiom com whid 2007 60 title whid 2007 60 the blog of a cambridge university security team hacked publisher xiom accessdate 2011 06 03 ref sql injection ddos attacks ref cite web url http www xiom com content whid 2009 1 gaza conflict cyber war title whid 2009 1 gaza conflict cyber war publisher xiom accessdate 2011 06 03 ref sql injection dns hijacking ref http www xiom com whid list dns 20hijacking webarchive url https web archive org web 20090618125914 http www xiom com whid list dns 20hijacking date june 18 2009 ref sql injection cross site scripting xss ref cite web url http www darkreading com security management showarticle jhtml articleid 211201482 title third wave of web attacks not the last publisher dark reading accessdate 2012 07 29 ref the storm worm is one representation of compounded sqli ref cite web last danchev first dancho url http ddanchev blogspot com 2007 01 social engineering and malware html title mind streams of information security knowledge social engineering and malware publisher ddanchev blogspot com date 2007 01 23 accessdate 2011 06 03 ref this classification represents the state of sqli respecting its evolution until 2010 further refinement is underway ref cite web last deltchev first krassen title new web 2 0 attacks url http www nds ruhr uni bochum de teaching theses web20 work b sc thesis publisher ruhr university bochum accessdate february 18 2010 ref technical implementations incorrectly filtered escape characters this form of sql injection occurs when user input is not filtered for escape character s and is then passed into an sql statement this results in the potential manipulation of the statements performed on the database by the end user of the application the following line of code illustrates this vulnerability statement source lang sql enclose none select from users where name source username source lang sql enclose none source this sql code is designed to pull up the records of the specified username from its table of users however if the username variable is crafted in a specific way by a malicious user the sql statement may do more than the code author intended for example setting the username variable as pre or 1 1 pre or using comments to even block the rest of the query there are three types of sql comments ref citation title ibm informix guide to sql syntax overview of sql syntax gt how to enter sql comments publisher ibm url http publib boulder ibm com infocenter idshelp v10 index jsp topic com ibm sqls doc sqls36 htm ref all three lines have a space at the end pre or 1 1 or 1 1 or 1 1 pre renders one of the following sql statements by the parent language source lang sql select from users where name or 1 1 source source lang sql select from users where name or 1 1 source if this code were to be used in an authentication procedure then this example could be used to force the selection of every data field from all users rather than from one specific user name as the coder intended because the evaluation of 1 1 is always true short circuit evaluation the following value of username in the statement below would cause the deletion of the users table as well as the selection of all data from the userinfo table in essence revealing the information of every user using an api that allows multiple statements a source lang sql enclose none drop table users select from userinfo where t t source this input renders the final sql statement as follows and specified source lang sql select from users where name a drop table users select from userinfo where t t source while most sql server implementations allow multiple statements to be executed with one call in this way some sql apis such as php s code mysql query code function do not allow this for security reasons this prevents attackers from injecting entirely separate queries but doesn t stop them from modifying queries incorrect type handling this form of sql injection occurs when a user supplied field is not strongly typed or is not checked for data type type constraints this could take place when a numeric field is to be used in a sql statement but the programmer makes no checks to validate that the user supplied input is numeric for example statement source lang sql enclose none select from userinfo where id source a variable it is clear from this statement that the author intended a variable to be a number correlating to the id field however if it is in fact a string computer science string then the end user may manipulate the statement as they choose thereby bypassing the need for escape characters for example setting a variable to pre 1 drop table users pre will drop delete the users table from the database since the sql becomes source lang sql select from userinfo where id 1 drop table users source blind sql injection blind sql injection is used when a web application is vulnerable to an sql injection but the results of the injection are not visible to the attacker the page with the vulnerability may not be one that displays data but will display differently depending on the results of a logical statement injected into the legitimate sql statement called for that page this type of attack has traditionally been considered time intensive because a new statement needed to be crafted for each bit recovered and depending on its structure the attack may consist of many unsuccessful requests recent advancements have allowed each request to recover multiple bits with no unsuccessful requests allowing for more consistent and efficient extraction ref cite web url http howto hackallthethings com 2016 07 extracting multiple bits per request html title extracting multiple bits per request from full blind sql injection vulnerabilities publisher hack all the things accessdate july 8 2016 archiveurl https web archive org web 20160708190141 http howto hackallthethings com 2016 07 extracting multiple bits per request html archivedate july 8 2016 ref there are several tools that can automate these attacks once the location of the vulnerability and the target information has been established ref cite web url http www justinclarke com archives 2006 03 sqlbrute html title using sqlbrute to brute force data from a blind sql injection point publisher justin clarke accessdate october 18 2008 archiveurl http web archive org web 20080614203711 http www justinclarke com archives 2006 03 sqlbrute html bot retrieved archive archivedate june 14 2008 ref conditional responses one type of blind sql injection forces the database to evaluate a logical statement on an ordinary application screen as an example a book review website uses a query string to determine which book review to display so the url code nowiki http books example com showreview php id 5 nowiki code would cause the server to run the query source lang sql select from bookreviews where id value id source from which it would populate the review page with data from the review with identifier id 5 stored in the table database table bookreviews the query happens completely on the server the user does not know the names of the database table or fields nor does the user know the query string the user only sees that the above url returns a book review a hacker computer security hacker can load the urls code source lang sql enclose none http books example com showreview php id 5 or 1 1 source code and code source lang sql enclose none http books example com showreview php id 5 and 1 2 source code which may result in queries source lang sql select from bookreviews where id 5 or 1 1 select from bookreviews where id 5 and 1 2 source respectively if the original review loads with the 1 1 url and a blank or error page is returned from the 1 2 url and the returned page has not been created to alert the user the input is invalid or in other words has been caught by an input test script the site is likely vulnerable to a sql injection attack as the query will likely have passed through successfully in both cases the hacker may proceed with this query string designed to reveal the version number of mysql running on the server code source lang mysql enclose none http books example com showreview php id 5 and substring version 1 instr version 1 4 source code which would show the book review on a server running mysql 4 and a blank or error page otherwise the hacker can continue to use code within query strings to glean more information from the server until another avenue of attack is discovered or his or her goals are achieved ref cite web url http forum intern0t org web hacking war games 818 blind sql injection html title blind sql injection tutorial author macd3v accessdate 6 december 2012 ref ref cite web title tdss botnet full disclosure url http nobunkum ru analytics en tdss botnet accessdate 6 december 2012 author andrey rassokhin author2 dmitry oleksyuk ref second order sql injection second order sql injection occurs when submitted values contain malicious commands that are stored rather than executed immediately in some cases the application may correctly encode an sql statement and store it as valid sql then another part of that application without controls to protect against sql injection might execute that stored sql statement this attack requires more knowledge of how submitted values are later used automated web application security scanners would not easily detect this type of sql injection and may need to be manually instructed where to check for evidence that it is being attempted mitigation an sql injection is a well known attack and easily prevented by simple measures after an apparent sql injection attack on talktalk group talktalk in 2015 the bbc reported that security experts were stunned that such a large company would be vulnerable to it ref cite web title questions for talktalk bbc news url http www bbc com news technology 34636308 website bbc news accessdate 2015 10 26 language en ref parameterized statements main article prepared statement with most development platforms parameterized statements that work with parameters can be used sometimes called placeholders or bind variable s instead of embedding user input in the statement a placeholder can only store a value of the given type and not an arbitrary sql fragment hence the sql injection would simply be treated as a strange and probably invalid parameter value in many cases the sql statement is fixed and each parameter is a scalar computing scalar not a table database table the user input is then assigned bound to a parameter ref cite web title sql injection prevention cheat sheet url https www owasp org index php sql injection prevention cheat sheet publisher open web application security project accessdate 3 march 2012 ref enforcement at the coding level using object relational mapping libraries avoids the need to write sql code the orm library in effect will generate parameterized sql statements from object oriented code escaping a straightforward though error prone way to prevent injections is to escape characters that have a special meaning in sql the manual for an sql dbms explains which characters have a special meaning which allows creating a comprehensive blacklist computing blacklist of characters that need translation for instance every occurrence of a single quote code code in a parameter must be replaced by two single quotes code nowiki nowiki code to form a valid sql string literal for example in php it is usual to escape parameters using the function code mysqli real escape string code before sending the sql query source lang php mysqli new mysqli hostname db username db password db name query sprintf select from users where username s and password s mysqli real escape string username mysqli real escape string password mysqli query query source this function prepends backslashes to the following characters x00 n r and x1a this function is normally used to make data safe before sending a query to mysql ref cite web url http in2 php net manual en mysqli real escape string php title mysqli real escape string php manual publisher php net ref br there are other functions for many database types in php such as pg escape string for postgresql the function code addslashes string str code works for escaping characters and is used especially for querying on databases that do not have escaping functions in php it returns a string with backslashes before characters that need to be quoted in database queries etc these characters are single quote double quote backslash and nul the null byte ref cite web url http pl2 php net manual en function addslashes php title addslashes php manual publisher php net ref br routinely passing escaped strings to sql is error prone because it is easy to forget to escape a given string creating a transparent layer to secure the input can reduce this error proneness if not entirely eliminate it ref cite web url http www xarg org 2010 11 transparent query layer for mysql title transparent query layer for mysql publisher robert eisele date november 8 2010 ref pattern check integer float or boolean string parameters can be checked if their value is valid representation for the given type strings that must follow some strict pattern date uuid alphanumeric only etc can be checked if they match this pattern database permissions limiting the permissions on the database logon used by the web application to only what is needed may help reduce the effectiveness of any sql injection attacks that exploit any bugs in the web application for example on microsoft sql server a database logon could be restricted from selecting on some of the system tables which would limit exploits that try to insert javascript into all the text columns in the database source lang tsql deny select on sys sysobjects to webdatabaselogon deny select on sys objects to webdatabaselogon deny select on sys tables to webdatabaselogon deny select on sys views to webdatabaselogon deny select on sys packages to webdatabaselogon source examples in february 2002 jeremiah jacks discovered that guess com was vulnerable to an sql injection attack permitting anyone able to construct a properly crafted url to pull down 200 000 names credit card numbers and expiration dates in the site s customer database ref cite web url http www securityfocus com news 346 title guesswork plagues web hole reporting publisher securityfocus date march 6 2002 ref on november 1 2005 a teenaged hacker used sql injection to break into the site of a taiwan ese information security magazine from the tech target group and steal customers information ref cite web url http www xiom com whid 2005 46 title whid 2005 46 teen uses sql injection to break to a security magazine web site publisher web application security consortium date november 1 2005 accessdate december 1 2009 ref on january 13 2006 russia n computer criminals broke into a government of rhode island rhode island government website and allegedly stole credit card data from individuals who have done business online with state agencies ref cite web url http www xiom com whid 2006 3 title whid 2006 3 russian hackers broke into a ri gov website publisher web application security consortium date january 13 2006 accessdate may 16 2008 ref on march 29 2006 a hacker discovered an sql injection flaw in an official government of india indian government s tourism in india tourism site ref cite web url http www xiom com whid 2006 27 title whid 2006 27 sql injection in incredibleindia org publisher web application security consortium date march 29 2006 accessdate march 12 2010 ref on june 29 2007 a computer criminal defaced the microsoft uk website using sql injection ref cite web url http www cgisecurity net 2007 06 hacker defaces html title hacker defaces microsoft u k web page publisher cgisecurity net author robert date june 29 2007 accessdate may 16 2008 ref ref cite web url http rcpmag com news article aspx editorialsid 8762 title hacker defaces microsoft uk web page publisher redmond channel partner online author keith ward date june 29 2007 accessdate may 16 2008 ref uk website the register quoted a microsoft spokesperson acknowledging the problem on september 19 2007 and january 26 2009 the turkish hacker group m0sted used sql injection to exploit microsoft s sql server to hack web servers belonging to mcalester army ammunition plant and the united states army corps of engineers us army corps of engineers respectively ref cite web url http www informationweek com architecture anti us hackers infiltrate army servers d d id 1079964 publisher information week title anti u s hackers infiltrate army servers date may 29 2009 accessdate december 17 2016 ref in january 2008 tens of thousands of pcs were infected by an automated sql injection attack that exploited a vulnerability in application code that uses microsoft sql server as the database store ref name chinesefarm in july 2008 kaspersky lab kaspersky s malaysia n site was hacked by the m0sted hacker group using sql injection on april 13 2008 the sex offender registries in the united states sexual and violent offender registry of oklahoma shut down its website for routine maintenance after being informed that 10 597 social security number s belonging to sex offender s had been downloaded via an sql injection attack ref cite web url http thedailywtf com articles oklahoma leaks tens of thousands of social security numbers other sensitive data aspx publisher the daily wtf title oklahoma leaks tens of thousands of social security numbers other sensitive data author alex papadimoulis date april 15 2008 accessdate may 16 2008 ref in may 2008 a server farm inside china used automated queries to google search google s search engine to identify microsoft sql server sql server websites which were vulnerable to the attack of an automated sql injection tool ref name chinesefarm cite web url http www pcworld com businesscenter article 146048 mass sql injection attack targets chinese web sites html title mass sql injection attack targets chinese web sites author sumner lemon idg news service publisher pc world magazine pcworld date may 19 2008 accessdate may 27 2008 ref ref name attackspecifics cite web url http www bloombit com articles 2008 05 ascii encoded binary string automated sql injection aspx title ascii encoded binary string automated sql injection attack author michael zino date may 1 2008 ref in 2008 at least april through august a sweep of attacks began exploiting the sql injection vulnerabilities of microsoft s internet information services iis web server and microsoft sql server sql server database server the attack does not require guessing the name of a table or column and corrupts all text columns in all tables in a single request ref name broad inject specifics cite web url http hackademix net 2008 04 26 mass attack faq title mass attack faq author giorgio maone date april 26 2008 ref a html string that references a malware javascript file is appended to each value when that database value is later displayed to a website visitor the script attempts several approaches at gaining control over a visitor s system the number of exploited web pages is estimated at 500 000 ref name broad inject numbers cite web url http www computerworld com article 2535473 security0 huge web hack attack infects 500 000 pages html title huge web hack attack infects 500 000 pages author gregg keizer date april 25 2008 accessdate october 16 2015 ref on august 17 2009 the united states department of justice charged an american citizen albert gonzalez and two unnamed russians with the theft of 130 million credit card numbers using an sql injection attack in reportedly the biggest case of identity theft in american history the man stole cards from a number of corporate victims after researching their payment processor payment processing system s among the companies hit were credit card processor heartland payment systems convenience store chain 7 eleven 7 8209 eleven and supermarket chain hannaford brothers ref cite news url http news bbc co uk 2 hi americas 8206305 stm title us man stole 130m card numbers publisher bbc date august 17 2009 accessdate august 17 2009 ref in december 2009 an attacker breached a rockyou plaintext database containing the encryption unencrypted usernames and passwords of about 32 nbsp million users using an sql injection attack ref cite news url http www nytimes com external readwriteweb 2009 12 16 16readwriteweb rockyou hacker 30 of sites store plain text 13200 html title rockyou hacker 30 of sites store plain text passwords work new york times first jolie last o dell date december 16 2009 accessdate may 23 2010 ref on july 2010 a south american security researcher who goes by the user computing handle ch nbsp russo obtained sensitive user information from popular bittorrent site the pirate bay he gained access to the site s administrative control panel and exploited a sql injection vulnerability that enabled him to collect user account information including ip address es md5 cryptographic hash function password hashes and records of which torrents individual users have uploaded ref cite news url http krebsonsecurity com 2010 07 pirate bay hack exposes user booty title the pirate bay attack date july 7 2010 ref from july 24 to 26 2010 attackers from japan and china used an sql injection to gain access to customers credit card data from neo beat an osaka based company that runs a large online supermarket site the attack also affected seven business partners including supermarket chains izumiya co maruetsu inc and ryukyu jusco co the theft of data affected a reported 12 191 customers as of august 14 2010 it was reported that there have been more than 300 cases of credit card information being used by third parties to purchase goods and services in china on september 19 during the swedish general election 2010 2010 swedish general election a voter attempted a code injection by hand writing sql commands as part of a write in candidate write 8209 in vote ref cite web url http alicebobandmallory com articles 2010 09 23 did little bobby tables migrate to sweden title did little bobby tables migrate to sweden publisher alicebobandmallory com accessdate 2011 06 03 ref on november 8 2010 the british royal navy website was compromised by a romanian hacker named tinkode using sql injection ref http www bbc co uk news technology 11711478 royal navy website attacked by romanian hacker bbc news 8 11 10 accessed november 2010 ref ref cite web url http news sky com skynews home world news stuxnet worm virus targeted at irans nuclear plant is in hands of bad guys sky news sources say article 201011415827544 title super virus a target for cyber terrorists author sam kiley date november 25 2010 accessdate november 25 2010 ref on february 5 2011 hbgary a technology security firm was broken into by lulzsec using a sql injection in their cms driven website ref cite web url http www par anoia net we are anonymous inside the hacker world of lulzse pdf title we are anonymous inside the hacker world of lulzsec publisher little brown and company ref on march 27 2011 http www mysql com mysql com the official homepage for mysql was compromised by a hacker using sql blind injection ref cite web url http blog sucuri net 2011 03 mysql com compromised html title mysql com compromised publisher sucuri ref on april 11 2011 barracuda networks was compromised using an sql injection flaw email address es and usernames of employees were among the information obtained ref cite web url http www networkworld com news 2011 041211 hacker breaks into barracuda networks html hpg1 bn title hacker breaks into barracuda networks database ref over a period of 4 nbsp hours on april 27 2011 an automated sql injection attack occurred on broadband reports website that was able to extract 8 of the username password pairs 8 000 random accounts of the 9 000 active and 90 000 old or inactive accounts ref name dslreports cite web url http www dslreports com forum r25793356 title site user password intrusion info publisher dslreports com accessdate 2011 06 03 ref ref name cnet news cite news url http news cnet com 8301 27080 3 20058471 245 html title dslreports says member information stolen publisher cnet news date 2011 04 28 accessdate 2011 04 29 ref ref name the tech herald cite news url http www thetechherald com article php 201117 7127 dslreports com breach exposed more than 100 000 accounts title dslreports com breach exposed more than 100 000 accounts publisher the tech herald date 2011 04 29 accessdate 2011 04 29 ref on june 1 2011 hacktivist s of the group lulzsec were accused of using sqli to steal coupon s download keys and passwords that were stored in plaintext on sony s website accessing the personal information of a million users ref citation title lulzsec hacks sony pictures reveals 1m passwords unguarded date june 2 2011 work electronista com url http www electronista com articles 11 06 02 lulz security hits sony again in security message ref ref citation title lulzsec hacker arrested group leaks sony database author ridge shan date june 6 2011 work the epoch times url http www theepochtimes com n2 technology lulzsec member arrested group leaks sony database 57296 html ref in june 2011 pbs was hacked mostly likely through use of sql injection the full process used by hackers to execute sql injections was described in this http blog imperva com 2011 05 pbs breached how hackers probably did it html imperva blog ref name pbs breached how hackers probably did it cite news url http blog imperva com 2011 05 pbs breached how hackers probably did it html title imperva com pbs hacked how hackers probably did it accessdate 2011 07 01 ref in may 2012 the website for wurm online a massively multiplayer online game was shut down from an sql injection while the site was being updated ref cite web url http wurmonline tumblr com post 22835329693 wurm online restructuring title wurm online is restructuring date may 11 2012 ref 2012 yahoo voices hack in july 2012 a hacker group was reported to have stolen 450 000 login credentials from yahoo the logins were stored in plain text and were allegedly taken from a yahoo subdomain yahoo voices the group breached yahoo s security by using a set operations sql union operator union based sql injection technique ref chenda ngak http www cbsnews com 8301 501465 162 57470956 501465 yahoo reportedly hacked is your account safe yahoo reportedly hacked is your account safe cbs news july 12 2012 retrieved july 16 2012 ref ref http www zdnet com 450000 user passwords leaked in yahoo breach 7000000772 ref on october 1 2012 a hacker group called team ghostshell published the personal records of students faculty employees and alumni from 53 universities including harvard princeton university princeton stanford cornell johns hopkins university johns hopkins and the university of zurich on pastebin pastebin com the hackers claimed that they were trying to raise awareness towards the changes made in today s education bemoaning changing education laws in europe and increases in college tuition in the united states tuition in the united states ref cite news last perlroth first nicole title hackers breach 53 universities and dump thousands of personal records online url http bits blogs nytimes com 2012 10 03 hackers breach 53 universities dump thousands of personal records online newspaper new york times date 3 october 2012 ref in february 2013 a group of maldivian hackers hacked the website un maldives using sql injection on june 27 2013 hacker group redhack breached istanbul administration site ref cite news title redhack breaches istanbul administration site hackers claim to have erased debts url http news softpedia com news redhack breaches istanbul administration site hackers claim to have erased debts 364000 shtml ref they claimed that they ve been able to erase people s debts to water gas internet electricity and telephone companies additionally they published admin user name and password for other citizens to log in and clear their debts early morning they announced the news from twitter ref cite news title redhack tweet about their achievement url http twitter com redhack en statuses 350461821456613376 ref on november 4 2013 hacktivist group raptorswag allegedly compromised 71 chinese government databases using an sql injection attack on the chinese chamber of international commerce the leaked data was posted publicly in cooperation with anonymous group anonymous ref http news softpedia com news hackers leak data allegedly stolen from chinese chamber of commerce website 396936 shtml ref on february 2 2014 avs tv had 40 000 accounts leaked by a hacking group called deletesec ref http www maurihackers info 2014 02 40000 avs tv accounts leaked html ref on february 21 2014 united nations internet governance forum had 3 215 account details leaked ref http www batblue com united nations internet governance forum breached ref on february 21 2014 hackers of a group called deletesec hacked spirol international after allegedly threatening to have the hackers arrested for reporting the security vulnerability 70 000 user details were exposed over this conflict ref http news softpedia com news details of 70 000 users leaked by hackers from systems of spirol international 428669 shtml ref on march 7 2014 officials at johns hopkins university publicly announced that their biomedical engineering servers had become victim to an sql injection attack carried out by an anonymous hacker named hooky and aligned with hacktivist group raptorswag the hackers compromised personal details of 878 students and staff posting a http pastebin com ug4fynby press release and the leaked data on the internet ref http articles baltimoresun com 2014 03 07 news bs md hopkins servers hacked 20140306 1 engineering students identity theft server ref in august 2014 milwaukee based computer security company hold security disclosed that it uncovered 2014 russian hacker password theft a theft of confidential information from nearly 420 000 websites through sql injections ref damon poeter http www pcmag com article2 0 2817 2462057 00 asp close knit russian hacker gang hoards 1 2 billion id creds pc magazine august 5 2014 ref the new york times confirmed this finding by hiring a security expert to check the claim ref nicole perlroth http www nytimes com 2014 08 06 technology russian gang said to amass more than a billion stolen internet credentials html r 0 russian gang amasses over a billion internet passwords the new york times august 5 2014 ref in october 2015 an sql injection attack was used to steal the personal details of 156 959 customers from british telecommunications company talktalk group talk talk s servers exploiting a vulnerability in a legacy web portal ref https ico org uk about the ico news and events news and blogs 2016 10 talktalk gets record 400 000 fine for failing to prevent october 2015 attack ref in popular culture unauthorized login to web sites by means of sql injection forms the basis of one of the subplots in j k rowling s novel the casual vacancy published in 2012 an xkcd cartoon involved a character robert drop table students named to carry out a sql injection as a result of this cartoon sql injection is sometimes informally referred to as bobby tables ref cite web last munroe first randall title xkcd exploits of a mom url http xkcd com 327 accessdate 26 february 2013 ref ref cite web title bobby tables a guide to preventing sql injection url http bobby tables com accessdate 6 october 2013 ref in 2014 an individual in poland legally renamed his business to nowiki dariusz jakubowski x drop table users select 1 nowiki in an attempt to disrupt operation of spammers web scraping harvesting bots ref cite web title jego firma ma w nazwie sql injection nie zazdro\xc5\x9bcimy tym kt\xc3\xb3rzy b\xc4\x99d\xc4\x85 go fakturowali website niebezpiecznik language pl date 11 september 2014 url http niebezpiecznik pl post jego firma ma w nazwie sql injection nie zazdroscimy tym ktorzy beda go fakturowali accessdate 26 september 2014 ref the 2015 game hacknet has a hacking program called sql memcorrupt it is described as injecting a table entry that causes a corruption error in a sql database then queries said table causing a sql database crash and core dump see also portal software testing code injection cross site scripting metasploit project owasp open web application security project sgml entity uncontrolled format string w3af web application security references reflist 30em external links http www techyfreaks com 2012 05 manual sql injection tutorial html manual sql injection tutorial by the ajay devgan http www websec ca kb sql injection sql injection knowledge base by websec http www sqlinjectionwiki com sql injection wiki http projects webappsec org sql injection wasc threat classification sql injection entry by the web application security consortium https docs google com leaf id 0byknnutb95yzytrjmjnjmwetodbmns00yzgwltlmmgytnwzmodi2mtnmzwyw sort name layout list num 50 why sql injection won t go away by stuart thomas http www owasp org index php sql injection prevention cheat sheet sql injection prevention cheat sheet by owasp http sqlmap org sqlmap automatic sql injection and database takeover tool http go microsoft com linkid 9707610 sdl quick security references on sql injection by bala neerumalla http arstechnica com information technology 2016 10 how security flaws work sql injection how security flaws work sql injection https www netsparker com blog web security sql injection cheat sheet sql injection cheat sheet by netsparker category data management category injection exploits category sql category articles with example sql code category computer security exploits'
b'npov date may 2016 infobox book name small data the tiny clues that uncover huge trends author martin lindstrom language english country united states publisher st martins isbn 978 1250080684 small data the tiny clues that uncover huge trends is martin lindstrom s seventh book it chronicles his work as a branding expert working with consumers across the world to better understand their behavior the theory behind the book is that businesses can better create products and services based on observing consumer behavior in their homes as opposed to relying solely on big data content the book is based on a several year period of consumer studies for major corporations across the globe ref cite web url https www martinlindstrom com small data title small data martin lindstrom bestselling author website martinlindstrom com martin lindstrom branding expert consultant language en us access date 2016 03 27 ref it features case studies of the author s work interviewing consumers in their homes and using his observations to create hypotheses as to why they use products the way that they do public reception the book was a new york times bestseller ref cite web url http www nytimes com best sellers books 2016 03 13 advice how to and miscellaneous list html title best sellers the new york times website www nytimes com access date 2016 03 27 ref upon release and was positively reviewed on several websites including entrepreneur magazine entrepreneur ref cite web url http www entrepreneur com article 271992 title from an elon musk bio to malcolm gladwell s blink these 9 books are must reads last agius first aaron website entrepreneur access date 2016 03 27 ref and forbes ref cite web url http www forbes com sites davidburkus 2016 01 10 16 must read business books for 2016 4ce073648bae title 16 must read business books for 2016 website forbes access date 2016 03 27 ref references reflist category 2016 books category 2016 non fiction books category data management'
b'secure electronic delivery sed is a service created in 2003 and provided by the british library document supply service british library document supply service bldss its purpose is to enable faster delivery of digital materials as encryption encrypted copyright compliant portable document format pdf document s to a personal e mail address these documents are supplied from the british library via its on demand service ref cite web url http www bl uk sed title secure electronic delivery author publisher british library date accessdate ref ref cite web url http www bl uk reshelp atyourdesk docsupply help receiving deliveryoptions electronic sed sedhelpsheetfinal pdf title secure electronic delivery technical helpsheet author publisher british library date accessdate ref when the british library supplies articles electronically it sends them securely in order to ensure its usage is permitted research purposes and copyright law is observed methods as the publishing publishing industry authors and creators become highly protective of their assets and intellectual property they impose strict rules on delivery methods to prevent copyright infringement nowadays digital rights management drm enabled secure delivery appears to be the most widely used solution to address issues faced by libraries in supplying ebooks and digital materials to their users ref cite news title secure e mail delivery poised to take off url https books google com mx books id pwhbljaq57gc pg pa38 lpg pa38 dq document secure delivery technology source bl ots ygfe16c0yy sig qd0r1j9ltzi6 hji6zqhcgobzfq hl en sa x redir esc y v onepage q document 20secure 20delivery 20technology f false date 23 august 1999 author dominique deckmyn newspaper computerworld ref ref cite web title practical problems for libraries distributing ebooks secure electronic delivery url http www locklizard com libraries secure electronic delivery publisher locklizard limited date accessdate ref sed one of these solutions is using adobe livecycle digital rights management lcdrm as an encryption method to deliver documents ref cite web url http www lancaster ac uk library using the library interlending and document supply secure electronic delivery title british library on demand electronic delivery author publisher lancaster university library date accessdate ref advantages sed offers convenience quality and speed as documents are delivered upon request at any location and on any device requested articles are scanned for high quality reproduction opened anywhere on any machine including mobile devices ref cite web url http www brad ac uk library media library interlibraryloans sed pdf title sed secure electronic delivery author publisher university of bradford date accessdate ref restrictions the following are restrictions hold in a sed service implementation the digital material is accessible only for 14 days via a link sent to a personal message due to copyright reasons ref cite journal last eiblum first paula last2 ardito first2 stephanie date september 1999 title document delivery copyright librarians take the fifth url journal online magazine publisher volume 23 issue 5 pages 74 77 doi access date 7 may 2016 ref the material can be opened only once saved for 14 days and does not allow a copy paste action upon display the material must be printed from the same device and reprinted only once the on demand encryption technology works best on the default safari browser although other browsers may accommodate it see also digital rights management digital asset management references reflist external links http www bl uk sed sed web page defaultsort secure electronic delivery category information technology management category content management systems category document management systems category data management category secure communication'
b'infobox company name bright computing logo logo size logo alt logo caption logo padding image image size image alt image caption type privately held company private founded 2009 if known start date and age yyyy mm dd in city state country founder unbulleted list matthijs van leeuwen alex ninaber hq location unbulleted list amsterdam the netherlands san jose california hq location city hq location country area served global or areas served key people unbulleted list bill wagner chief executive officer ceo martijn de vries chief technology officer cto kristin hansen chief marketing officer cmo bill griffin chief financial officer cfo industry enterprise software products unbulleted list bright cluster manager for hpc bright cluster manager for big data bright openstack brands services former name clustervision spin off website url brightcomputing com or homepage url example com bright computing inc is a developer of software for deploying and managing supercomputer high performance hpc clusters big data clusters and openstack in data center s and using cloud computing ref cite web url http www hpcwire com 2016 02 03 24601 title create mixed hpc big data clusters today says bright computing date 2016 02 03 website hpcwire access date 2016 05 24 ref history bright computing was founded by matthijs van leeuwen in 2009 who spun the company out of clustervision which he had co founded with alex ninaber and arijan sauer alex and matthijs had worked together at uk s compusys which was one of the first companies to commercially build hpc clusters ref cite web url http www bloomberg com research stocks private person asp personid 282144720 privcapid 115561075 previouscapid 115561075 previoustitle bright 2520computing 2520inc title matthijs van leeuwen executive profile biography businessweek website www bloomberg com access date 2016 05 24 ref ref cite web url http www hpcwire com 2009 10 07 clustervision spins off cluster management software company title clustervision spins off cluster management software company date 2009 10 07 website hpcwire access date 2016 05 24 ref they left compusys in 2002 to start clustervision in the netherlands after determining there was a growing market for building and managing supercomputer clusters using off the shelf hardware components and open source software tied together with their own customized scripts ref cite web url http www clustervision com content management team title clustervision europe s leading hpc and cloud specialist website clustervision language en gb access date 2016 05 24 ref clustervision also provided delivery and installation support services for hpc clusters at universities and government entities ref cite web url http clustervision com about the company title about the company clustervision website clustervision language en gb access date 2016 05 24 ref in 2004 martijn de vries joined clustervision and began development of cluster management software the software was made available to customers in 2008 under the name clustervisionos v4 ref cite web url http www beowulf org pipermail beowulf 2008 september 023265 html title roll your own cluster management system with clustervisionos v4 was beowulf what services do you run on your cluster nodes last holway first andrew date 2008 09 23 access date 2016 05 24 ref in 2009 bright computing was spun out of clustervision clustervisionos was renamed bright cluster manager and van leeuwen was named bright computing s ceo ref cite web url http www hpcwire com 2009 10 07 clustervision spins off cluster management software company title clustervision spins off cluster management software company date 2009 10 07 website hpcwire access date 2016 05 24 ref in 2010 ing group ing corporate investments made a 2 5 million investment in bright computing in 2014 draper fisher jurvetson dfj us dfj esprit uk prime ventures nl and ing group ing corporate investments invested 14 5 million in bright computing at that time bright computing and clustervision were completely separated ref cite web url https www crunchbase com organization ing corporate investments entity title ing corporate investments crunchbase website www crunchbase com access date 2016 05 24 ref in february 2016 bright appointed bill wagner as chief executive officer matthijs van leeuwen became chief strategy officer and board member ref cite web url http insidehpc com 2016 02 bright computing names bill wagner as chief executive officer title bright computing names bill wagner as ceo insidehpc date 2016 02 16 website insidehpc language en us access date 2016 05 24 ref customers early customers included boeing ref cite web url http insidehpc com 2011 12 boeing consolidates on brite cluster manager title boeing consolidates on bright cluster manager insidehpc date 2011 12 06 website insidehpc language en us access date 2016 05 24 ref sandia national laboratories ref cite web url http www brightcomputing com news sandia labs adopts bright cluster manager title sandia national laboratories adopts bright cluster manager to manage departmental clusters last staff first bright website www brightcomputing com access date 2016 05 24 ref virginia tech ref cite web url https finance yahoo com news virginia bioinformatics institute selects bright 151500615 html title virginia bioinformatics institute selects bright cluster manager for big data to test new research methods website yahoo finance access date 2016 05 24 ref hewlett packard enterprise services hewlett packard ref cite web url http www8 hp com h20195 v2 getpdf aspx 4aa5 2604enw pdf title hpc accelerates smbs last first date website publisher hewlett packard development company l p access date 2007 05 24 ref national security agency nsa and drexel university many early customers were introduced through resellers including sicorp ref cite web url http insidehpc com 2010 02 bright computing and sicorp sign reseller agreement title bright computing and sicorp sign reseller agreement insidehpc date 2010 02 22 website insidehpc language en us access date 2016 05 24 ref cray ref cite web url http www cray com company collaboration partners title partner relationships cray website www cray com access date 2016 05 24 ref dell ref cite web url http www bizjournals com prnewswire press releases 2015 11 11 mn54237 title bright computing announces integration with dell poweredge servers for hpc environments the business journals website the business journals access date 2016 05 24 ref appro and advanced hpc ref cite web url http www advancedhpc com high performance servers gpu computing bright computing html title advanced hpc gpu computing gpu software bright computing website www advancedhpc com access date 2016 05 24 ref by 2014 the company estimated 400 customers including more than 20 fortune 500 companies ref cite web url http www primeventures com portfolio bright computing title prime ventures bright computing website www primeventures com access date 2016 05 25 ref products and services bright cluster manager for hpc lets customers deploy and manage complete clusters it provides management for the hardware the operating system the hpc software and users ref cite web url http www hpcwire com off the wire bright computing roll new line products title bright computing to roll out new line of products website hpcwire language en us access date 2016 05 24 ref in 2014 bright computing introduced bright cluster manager for apache hadoop clusters ref cite web url http www bizjournals com prnewswire press releases 2014 04 03 mn96527 title bright computing announces full support for apache hadoop at the hadoop summit europe in amsterdam the business journals website the business journals access date 2016 05 24 ref it was later extended to support apache spark and other big data applications and was renamed bright cluster manager for big data it can be used as a complete solution or to manage big data software distributions from leading vendors including cloudera and hortonworks it also has several features that allow the combination of big data and hpc workloads on the same cluster ref cite web url http www deskeng com de bright computing releases bright cluster manager 7 2 title bright computing releases bright cluster manager 7 2 date 2016 01 22 website desktop engineering language en us access date 2016 05 24 ref in 2014 the company announced bright openstack software to deploy provision and manage openstack based private cloud infrastructures ref cite web url http www hpcwire com off the wire bright computing announces bright openstack integration talligent openbook billing software openstack clouds title bright computing announces bright openstack integration with talligent software website hpcwire language en us access date 2016 05 24 ref in january 2016 version 7 2 was released the updates supported containers using docker software docker improved integration with puppet software puppet and job based metrics ref cite web url http www hpcwire com off the wire bright computing highlights version 7 2 bright cluster manager software solution enterprisehpc 2016 title bright computing highlights version 7 2 of bright cluster manager software solutions at enterprisehpc 2016 website hpcwire language en us access date 2016 05 24 ref bright cluster manager software is frequently sold through original equipment manufacturer oem resellers including dell and cray ref cite web url http www brightcomputing com usa title usa last computing first bright website www brightcomputing com access date 2016 05 24 ref technology partners include div col 3 adaptive computing ref cite web url http www adaptivecomputing com news adaptive computing bright computing deepen product integration enhance provisioning workflow optimization technical computing environments title adaptive computing and bright computing deepen product integration to enhance provisioning and workflow optimization in technical computing environments adaptive computing date 2014 06 24 website adaptive computing language en us access date 2016 05 24 ref allinea ref cite web url http insidehpc com 2015 09 bright computing collaborates on openhpec accelerator suite title bright computing collaborates on openhpec accelerator suite date 2015 09 16 website insidehpc language en us access date 2016 05 24 ref altair engineering altair ref cite web url http www prweb com releases 2011 9 prweb8788932 htm title bright computing now resells altair pbs professional workload manager delivering hpc operational efficiencies and cost savings website prweb access date 2016 05 24 ref amazon web services amazon ref cite web url http www hpcwire com 2011 11 08 bright computing bursts into cloud title bright computing bursts into cloud date 2011 11 08 website hpcwire access date 2016 05 24 ref ansys ref cite web url http www brightcomputing com technology partner ansys title technology partner ansys last computing first bright website www brightcomputing com access date 2016 05 24 ref cavium ref cite web url http www cavium com newsevents cavium collaboration with brightcomputing html title cavium announces collaboration with bright computing to support the thunderx processor family website www cavium com access date 2016 05 24 ref cisco systems cisco ref cite web url https gigaom com 2014 07 28 bright computing takes in 14 5m to push its cluster management smarts title bright computing takes in 14 5m to push its cluster management smarts last darrow first barb date 2014 07 28 website gigaom com access date 2016 05 24 ref cloudera ref cite web url http insidebigdata com 2014 04 10 bright computing achieves cloudera certification bright cluster manager title bright computing achieves cloudera certification for bright cluster manager last brueckner first rich date 2014 04 10 website insidebigdata access date 2016 05 24 ref cray ref cite web url https cug org proceedings cug2015 proceedings includes files pap176 file2 pdf title cray user group bright cluster manager presentation last first date website publisher access date ref ddn storage ref cite web url http www brightcomputing com technology partner ddn title technology partner ddn last computing first bright website www brightcomputing com access date 2016 05 24 ref dell ref cite web url http insidehpc com 2015 12 dellbright title bright cluster manager integrates with dell poweredge servers for hpc environments insidehpc date 2015 12 09 website insidehpc language en us access date 2016 05 24 ref fujitsu ref cite web url http www brightcomputing com technology partner fujitsu title technology partner fujitsu last computing first bright website www brightcomputing com access date 2016 05 24 ref hewlett packard enterprise ref cite web url http www datacenterknowledge com archives 2014 07 29 bright cluster management raises dough title bright networks cluster management gets 14 5m round date 2014 07 29 website data center knowledge language en us access date 2016 05 24 ref hortonworks ref cite web url http hortonworks com partner bright computing title bright computing partners with hortonworks for hadoop website hortonworks language en us access date 2016 05 24 ref huawei ref cite web url http www bizjournals com prnewswire press releases 2014 09 16 mn12537 title huawei signs global reseller agreement with bright computing the business journals website the business journals access date 2016 05 24 ref ibm ref cite web url http www brightcomputing com technology partner ibm title technology partner ibm last computing first bright website www brightcomputing com access date 2016 05 24 ref inspur ref cite web url http www brightcomputing com technology partner inspur title technology partner inspur last computing first bright website www brightcomputing com access date 2016 05 24 ref intel ref cite web url https ctovision com 2015 08 bright computing integrated intel enterprise edition lustre title bright computing integrated with intel enterprise edition for lustre ctovision com date 2015 08 13 website ctovision com language en us access date 2016 05 24 ref lenovo ref cite web url http www brightcomputing com technology partner lenovo title technology partner lenovo last computing first bright website www brightcomputing com access date 2016 05 24 ref magma ref cite web url http www brightcomputing com technology partner magma title technology partner magma last computing first bright website www brightcomputing com access date 2016 05 24 ref mellanox technologies ref cite web url http www openstack org news view 214 bright computing announces bright openstack 25e2 2584 25a2 integration with mellanox technologies infiniband switches and virtual extensible lan vxlan offloading title \xc2\xbb openstack open source cloud computing software website www openstack org access date 2016 05 24 ref microsoft ref cite web url http www brightcomputing com technology partner microsoft title technology partner microsoft last computing first bright website www brightcomputing com access date 2016 05 24 ref nice ref cite web url https www nice software com partners bright computing title bright computing nice website www nice software com access date 2016 05 24 ref nvidia nvidia ref cite web url https developer nvidia com bright cluster manager title bright cluster manager date 2012 01 12 website nvidia developer access date 2016 05 24 ref red hat redhat ref cite web url http www brightcomputing com technology partner redhat title technology partner redhat last computing first bright website www brightcomputing com access date 2016 05 24 ref runtime design automation ref cite web url http www brightcomputing com technology partner runtime title technology partner runtime last computing first bright website www brightcomputing com access date 2016 05 24 ref silicon graphics international sgi ref cite web url https www sgi com partners technology partners html title sgi partners technology partners sgi partnerships website www sgi com access date 2016 05 24 ref supermicro ref cite web url http www montana edu rci hyalitecluster html title hyalite cluster research computing group montana state university website www montana edu access date 2016 05 24 ref suse ref citation last suse title bright computing and suse share common values system date 2015 10 29 url https www youtube com watch v 4wwaklobhbo accessdate 2016 05 24 ref taligent ref cite web url https finance yahoo com news bright computing announces bright openstack 073000503 html title bright computing announces bright openstack\xe2\x84\xa2 integration with talligent openbook billing software for openstack clouds website yahoo finance access date 2016 05 24 ref univa ref cite web url http www prweb com releases 2012 10 prweb9961985 htm title bright computing and univa offer combined cluster workload management solution website prweb access date 2016 05 24 ref div col end bright computing was covered by software magazine ref cite web url http www softwaremag com bright computing releases version 7 2 of bright cluster manager for hpc bright cluster manager for big data and bright openstack title bright computing releases version 7 2 of bright cluster manager for hpc bright cluster manager for big data and bright openstack website www softwaremag com access date 2016 05 24 ref and yahoo finance ref cite web url https finance yahoo com news bright computing showcase bright openstack 162500598 html title bright computing to showcase bright openstack\xe2\x84\xa2 and hpc cluster as a service caas at 2016 openstack summit in austin website yahoo finance access date 2016 05 24 ref among other publications awards in 2016 bright computing was awarded a \xe2\x82\xac1 5m horizon 2020 sme instrument grant from the european commission ref cite web url http www hpcwire com off the wire bright computing receives horizon 2020 grant european commission title bright computing receives horizon 2020 grant from european commission website hpcwire language en us access date 2016 05 24 ref bright computing was one of only 33 grant recipients from 960 submitted proposals ref cite web url https ec europa eu easme en horizons 2020 sme instrument title horizon 2020 s sme instrument easme european commission website easme access date 2016 05 24 ref in its category only 5 out of 260 grants were awarded ref cite web url https ec europa eu digital single market en open disruptive innovation 0 title open disruptive innovation digital single market european commission website digital single market access date 2016 05 24 ref 2015 hpcwire editor s choice award for best hpc cluster solution or technology ref cite web url http www hpcwire com 2015 hpcwire readers choice awards title 2015 hpcwire awards readers editors choice hpcwire website hpcwire language en us access date 2016 05 24 ref main software 50 highest growth award winner 2013 ref cite web url http www bizjournals com prnewswire press releases 2013 11 14 mn16669 title bright computing wins main software 50 highest growth award the business journals website the business journals access date 2016 05 24 ref deloitte technology fast50 rising star 2013 award winner ref cite web url http www2 deloitte com be en pages about deloitte articles ayden wins fast50 html title adyen wins deloitte technology fast50 deloitte belgium tmt news press release website deloitte belgium access date 2016 05 24 ref bio it world conference expo 13 boston ma winner of it hardware infrastructure category of the best of show award program ref cite web url http www bio itworld com 2013 4 10 2013 bio it world best show winners named html title 2013 bio it world best of show winners named bio it world access date 2016 05 24 ref red herring magazine red herring top 100 global award 2013 ref cite web url http www redherring com events rhna 2013 rhnawinners title 2013 top 100 north america winners red herring website red herring language en us access date 2016 05 24 ref references reflist 30em further reading morgan timothy prickett june 20 2011 http www theregister co uk 2011 06 20 bright cluster manager 5 2 bright computing revs up cluster manager the register morgan timothy prickett november 8 2011 http www theregister co uk 2011 11 08 bright cluster manager amazon cloud bursting bright computing bursts hpc to ec2 clouds the register category big data companies category cloud computing category cloud infrastructure category cluster computing category data management category supercomputers'
b'infobox company name cambridge semantics logo logo size 200 type private company private genre fate predecessor successor foundation 2007 founder sean martin br lee feigenbaum br simon martin br emmett eldred defunct location city boston ma location country united states locations 2 boston ma san diego ca area served key people chuck pieper ceo br alok prasad president industry computer software products production services revenue operating income net income aum assets equity slogan the smart data company owner num employees parent divisions subsid homepage url cambridgesemantics com footnotes intl cambridge semantics is a privately held company headquartered in boston massachusetts with a west coast office in san diego california the company develops and sells a suite of smart data products for data management data discovery and enterprise analytics history cambridge semantics was founded in 2007 by sean martin lee feigenbaum simon martin rouben meschian and emmett eldred who all previously worked at ibm s advanced technology internet group ref cite web last1 lynch first1 brendan website boston business journal title ex ibmers aim at better search tech url http www bizjournals com boston blog mass high tech 2008 03 ex ibmers aim at better search tech html accessdate 27 april 2016 ref ref cite web last1 resende first1 patricia title with explosion of big data comes big growth for cambridge semantics url http www bizjournals com boston blog techflash 2015 02 with explosion of big data comes big growth for html website boston business journal accessdate 27 april 2016 ref in 2012 cambridge semantics appointed chuck pieper as chief executive officer prior to joining cambridge semantics pieper was vice chairman of alternative investments and managing director of credit suisse within the asset management division ref cite web last1 seiffert first1 don title chuck pieper named ceo at cambridge semantics url http www bizjournals com boston blog techflash 2012 12 chuck pieper named ceo at cambridge html website boston business journal accessdate 27 april 2016 ref in 2015 cambridge semantics formed an alliance with marklogic ref cite web title cambridge semantics and marklogic partner to advance semantic driven data management url http www dbta com editorial news flashes cambridge semantics and marklogic partner to advance semantic driven data management 106569 aspx website dbta com accessdate 27 april 2016 language en us date 24 september 2015 ref ref cite web title marklogic cambridge semantics partner for nosql url http www kmworld com articles news news marklogic cambridge semantics partner for nosql 106568 aspx website kmworld kmworld magazine accessdate 27 april 2016 language en us date 24 september 2015 ref in january 2016 cambridge semantics acquired sparql city and its graph database intellectual property ref cite web last1 leopold first1 george title cambridge semantics buys graph database specialist url http www datanami com 2016 01 14 cambridge semantics buys graph database specialist website datanami accessdate 27 april 2016 date 14 january 2016 ref products anzo smart data platform is a platform for building unified information solutions based on a set of open data standards implemented using semantic web semantic web technologies ref cite web last1 bertolucci first1 jeff title big data semantic web love at first terabyte informationweek url http www informationweek com big data big data analytics big data semantic web love at first terabyte d d id 1107520 website informationweek accessdate 28 april 2016 ref ref cite web last1 shacklett first1 mary title a start to solving the enterprise data usage problem techrepublic url http www techrepublic com article a start to solving the enterprise data usage problem website techrepublic accessdate 28 april 2016 ref it allows it departments and their business users to quickly and flexibly access all of their diverse data for breakthrough insights ref cite web last1 lawson first1 loraine title cambridge semantics offers new integration tool url http www itbusinessedge com blogs integration cambridge semantics offers new integration tool html website it business edge accessdate 27 april 2016 ref ref cite web title cambridge semantics launches anzo smart data integration url http www econtentmag com articles news news item cambridge semantics launches anzo smart data integration 98007 htm website econtent magazine accessdate 27 april 2016 language en us date 3 july 2014 ref ref cite web title the time for smart data has finally arrived cambridge semantics inc url http thesiliconreview com magazines the time for smart data has finally arrived cambridge semantics inc website the silicon review accessdate 27 april 2016 language en us ref ref cite web last1 kutz first1 erin title cambridge semantics looking to put microsoft excel on steroids brings intelligent data sorting to non techies url http www xconomy com boston 2010 07 08 cambridge semantics looking to put microsoft excel on steroids brings intelligent data sorting to non techies website xconomy accessdate 27 april 2016 language en us date 8 july 2010 ref ref cite web last1 mcnamara first1 paul title book of odds opening eyes to new probabilities url http www networkworld com article 2231870 data center book of odds opening eyes to new probabilities html website network world accessdate 28 april 2016 ref anzo smart data manager anzo graph query engine anzo smart data lake awards and recognition cambridge semantics named software and information industry association siia codie award 2016 finalist ref cite web title 2016 finalists url https www siia net codie 2016 finalists website siia net accessdate 27 april 2016 ref cambridge semantics named kmworld s 2016 100 companies that matter in knowledge management ref cite web title kmworld 100 companies that matter in knowledge management url http www kmworld com articles editorial features kmworld 100 companies that matter in knowledge management 109344 aspx website kmworld kmworld magazine accessdate 27 april 2016 language en us date 1 march 2016 ref and kmworld trend setting products of 2015 ref cite web last1 mckellar first1 hugh title kmworld trend setting products of 2015 url http www kmworld com articles editorial features kmworld trend setting products of 2015 105783 aspx website kmworld kmworld magazine accessdate 27 april 2016 language en us date 1 september 2015 ref cambridge semantics named 2016 bio it world best of show people s choice award contenders ref cite web title 2016 bio it world best of show people s choice award contenders url http www bio itworld com 2016 3 29 2016 best of show peoples choice award contenders asp website bio it world accessdate 27 april 2016 ref and 2015 bio it best of show finalist ref cite web title bio it world recognizes 2015 best of show winners url http www bio itworld com 2015 4 27 bio it world recognizes 2015 best of show winners html website bio it world accessdate 27 april 2016 ref cio review recognizes cambridge semantics as 2015 top 20 tech solution provider for pharmaceutical industry pharma and life sciences industry ref cite web title 20 most promising pharma and life sciences tech solution providers 20 15 url http pharma life sciences cioreview com vendors 2015 20special1 website cioreview accessdate 27 april 2016 ref ref cite web title cambridge semantics smart data management and advanced analytics for pharma and life sciences url http pharma life sciences cioreview com vendor 2015 cambridge semantics website cioreview accessdate 27 april 2016 ref anzo insider trading investigation and surveillance named 2015 codie award finalist ref cite web title finalists 2015 siia codie awards url https www siia net archive codies 2015 finalists asp website siia net accessdate 27 april 2016 ref cambridge semantics selected as finalist for 2014 mit sloan cio symposium s innovation showcase ref cite web title lead your digital enterprise mit sloan cio url http www mitcio com wp content uploads 2015 12 mitcio 2014 pdf accessdate 27 april 2016 ref cambridge semantics named software and information industry association siia codie award 2014 finalist ref cite web title finalists 2014 siia codie awards url http archive siia net codies 2014 finalist detail asp id 3 website siia net accessdate 27 april 2016 dead link date november 2016 bot internetarchivebot fix attempted yes ref cambridge semantics win 2013 software and information industry association siia codie award for best business intelligence and analytics solution ref cite web title 2013 codie award winners url http www siia net archive codies 2015 pw 2013 asp website siia net accessdate 27 april 2016 ref cambridge semantics wins kmworld 2012 promise award ref cite web title kmworld promise award winner url http www kmworld com articles editorial features kmworld 2012 promise and reality award winners and finalists 85829 aspx website kmworld magazine accessdate 27 april 2016 language en us date 30 october 2012 ref cambridge semantics wins best of show at 2012 bio it world conference ref cite web title 2012 best of show winners url http www bio itworld com 2012 04 26 2012 best of show winners html website bio it world accessdate 27 april 2016 ref references reflist 2 external links https www cambridgesemantics com official website category software companies based in massachusetts category companies established in 2007 category data management'
b'database normalization or simply normalization is the process of organizing the column database columns attributes and table database tables relations of a relational database to reduce data redundancy and improve data integrity normalization involves arranging attributes in tables based on dependency theory database theory dependencies between attributes ensuring that the dependencies are properly enforced by database integrity constraints normalization is accomplished through applying some formal rules either by a process of synthesis or decomposition synthesis creates a normalized database design based on a known set of dependencies decomposition takes an existing insufficiently normalized database design and improves it based on the known set of dependencies edgar f codd the inventor of the relational model rm introduced the concept of normalization and what we now know as the first normal form 1nf in 1970 ref name codd1970 cite journal first e f last codd authorlink e f codd title a relational model of data for large shared data banks journal communications of the acm volume 13 issue 6 date june 1970 pages 377 387 url http www acm org classics nov95 toc html doi 10 1145 362384 362685 ref codd went on to define the second normal form 2nf and third normal form 3nf in 1971 ref name codd e f 1971 codd e f further normalization of the data base relational model presented at courant computer science symposia series 6 data base systems new york city may 24 25 1971 ibm research report rj909 august 31 1971 republished in randall j rustin ed data base systems courant computer science symposia series 6 prentice hall 1972 ref and codd and raymond f boyce defined the boyce codd normal form boyce codd normal form bcnf in 1974 ref name coddbcnf codd e f recent investigations into relational data base systems ibm research report rj1385 april 23 1974 republished in proc 1974 congress stockholm sweden 1974 n y north holland 1974 ref informally a relational database table is often described as normalized if it meets third normal form ref name dateintrodbsys c j date an introduction to database systems addison wesley 1999 p 290 ref most 3nf tables are free of insertion update and deletion anomalies objectives a basic objective of the first normal form defined by codd in 1970 was to permit data to be queried and manipulated using a universal data sub language grounded in first order logic ref the adoption of a relational model of data permits the development of a universal data sub language based on an applied predicate calculus a first order predicate calculus suffices if the collection of relations is in first normal form such a language would provide a yardstick of linguistic power for all other proposed data languages and would itself be a strong candidate for embedding with appropriate syntactic modification in a variety of host ianguages programming command or problem oriented codd http www acm org classics nov95 toc html a relational model of data for large shared data banks p 381 ref sql is an example of such a data sub language albeit one that codd regarded as seriously flawed ref codd e f chapter 23 serious flaws in sql in the relational model for database management version 2 addison wesley 1990 pp 371 389 ref the objectives of normalization beyond 1nf first normal form were stated as follows by codd quotation to free the collection of relations from undesirable insertion update and deletion dependencies to reduce the need for restructuring the collection of relations as new types of data are introduced and thus increase the life span of application programs to make the relational model more informative to users to make the collection of relations neutral to the query statistics where these statistics are liable to change as time goes by e f codd further normalization of the data base relational model ref codd e f further normalization of the data base relational model p 34 ref the sections below give details of each of these objectives free the database of modification anomalies file update anomaly svg 280px thumb right an update anomaly employee 519 is shown as having different addresses on different records file insertion anomaly svg 280px thumb right an insertion anomaly until the new faculty member dr newsome is assigned to teach at least one course his details cannot be recorded file deletion anomaly svg 280px thumb right a deletion anomaly all information about dr giddens is lost if he temporarily ceases to be assigned to any courses when an attempt is made to modify update insert into or delete from a table undesired side effects may arise in tables that have not been sufficiently normalized an insufficiently normalized table might have one or more of the following characteristics the same information can be expressed on multiple rows therefore updates to the table may result in logical inconsistencies for example each record in an employees skills table might contain an employee id employee address and skill thus a change of address for a particular employee will potentially need to be applied to multiple records one for each skill if the update is not carried through successfully if that is the employee s address is updated on some records but not others then the table is left in an inconsistent state specifically the table provides conflicting answers to the question of what this particular employee s address is this phenomenon is known as an update anomaly there are circumstances in which certain facts cannot be recorded at all for example each record in a faculty and their courses table might contain a faculty id faculty name faculty hire date and course code thus we can record the details of any faculty member who teaches at least one course but we cannot record the details of a newly hired faculty member who has not yet been assigned to teach any courses except by setting the course code to null this phenomenon is known as an insertion anomaly under certain circumstances deletion of data representing certain facts necessitates deletion of data representing completely different facts the faculty and their courses table described in the previous example suffers from this type of anomaly for if a faculty member temporarily ceases to be assigned to any courses we must delete the last of the records on which that faculty member appears effectively also deleting the faculty member unless we set the course code to null in the record itself this phenomenon is known as a deletion anomaly minimize redesign when extending the database structure when a fully normalized database structure is extended to allow it to accommodate new types of data the pre existing aspects of the database structure can remain largely or entirely unchanged as a result applications interacting with the database are minimally affected normalized tables and the relationship between one normalized table and another mirror real world concepts and their interrelationships example querying and manipulating the data within a data structure that is not normalized such as the following non 1nf representation of customers credit card transactions involves more complexity than is really necessary class wikitable customer cust id transactions jones 1 class wikitable tr id date amount 12890 14 oct 2003 minus 87 12904 15 oct 2003 minus 50 wilkins 2 class wikitable tr id date amount 12898 14 oct 2003 minus 21 stevens 3 class wikitable tr id date amount 12907 15 oct 2003 minus 18 14920 20 nov 2003 minus 70 15003 27 nov 2003 minus 60 br to each customer corresponds a repeating group of transactions the automated evaluation of any query relating to customers transactions therefore would broadly involve two stages unpacking one or more customers groups of transactions allowing the individual transactions in a group to be examined and deriving a query result based on the results of the first stage for example in order to find out the monetary sum of all transactions that occurred in october 2003 for all customers the system would have to know that it must first unpack the transactions group of each customer then sum the amounts of all transactions thus obtained where the date of the transaction falls in october 2003 one of codd s important insights was that this structural complexity could always be removed completely leading to much greater power and flexibility in the way queries could be formulated by user computing users and application software applications and evaluated by the database management system dbms the normalized equivalent of the structure above would look like this class wikitable customer cust id jones 1 wilkins 2 stevens 3 class wikitable cust id tr id date amount 1 12890 14 oct 2003 minus 87 1 12904 15 oct 2003 minus 50 2 12898 14 oct 2003 minus 21 3 12907 15 oct 2003 minus 18 3 14920 20 nov 2003 minus 70 3 15003 27 nov 2003 minus 60 in the modified structure the keys are customer and cust id in the first table cust id tr id in the second table now each row represents an individual credit card transaction and the dbms can obtain the answer of interest simply by finding all rows with a date falling in october and summing their amounts the data structure places all of the values on an equal footing exposing each to the dbms directly so each can potentially participate directly in queries whereas in the previous situation some values were embedded in lower level structures that had to be handled specially accordingly the normalized design lends itself to general purpose query processing whereas the unnormalized design does not the normalized version also allows the user to change the customer name in one place and guards against errors that arise if the customer name is misspelled on some records list of normal forms unf denormalization unnormalized form first normal form 1nf first normal form second normal form 2nf second normal form third normal form 3nf third normal form elementary key normal form eknf elementary key normal form boyce codd normal form bcnf boyce codd normal form fourth normal form 4nf fourth normal form http researcher watson ibm com researcher files us fagin icdt12 pdf etnf essential tuple normal form fifth normal form 5nf fifth normal form sixth normal form 6nf sixth normal form domain key normal form dknf domain key normal form see also refactoring notes and references reflist 2 refbegin refend further reading date c j 1999 http www aw bc com catalog academic product 0 1144 0321197844 00 html an introduction to database systems 8th ed addison wesley longman isbn 0 321 19784 4 kent w 1983 http www bkent net doc simple5 htm a simple guide to five normal forms in relational database theory communications of the acm vol 26 pp nbsp 120 125 h j schek p pistor data structures for an integrated data base management and information retrieval system external links http databases about com od specificproducts a normalization htm database normalization basics by mike chapple about com http www databasejournal com sqletc article php 1428511 database normalization intro http www databasejournal com sqletc article php 26861 1474411 1 part 2 http mikehillyer com articles an introduction to database normalization an introduction to database normalization by mike hillyer http phlonx com resources nf3 a tutorial on the first 3 normal forms by fred coulson http www dbnormalization com db normalization examples http support microsoft com kb 283878 description of the database normalization basics by microsoft http www barrywise com 2008 01 database normalization and design techniques database normalization and design techniques by barry wise recommended reading for the harvard mis http www bkent net doc simple5 htm a simple guide to five normal forms in relational database theory http beginnersbook com 2015 05 normalization in dbms normalization in dbms by chaitanya beginnersbook com database normalization database databases defaultsort database normalization category database normalization category database constraints category data management category data modeling category relational algebra'
b'research data archiving is the computer data storage volatility long term storage of scholarly research data including the natural sciences social sciences and life sciences the various academic journals have differing policies regarding how much of their data and methods researchers are required to store in a public archive and what is actually archived varies widely between different disciplines similarly the major grant giving institutions have varying attitudes towards public archival of data in general the tradition of science has been for publications to contain sufficient information to allow fellow researchers to replicate and therefore test the research in recent years this approach has become increasingly strained as research in some areas depends on large datasets which cannot easily be replicated independently data archiving is more important in some fields than others in a few fields all of the data necessary to replicate the work is already available in the journal article in drug development a great deal of data is generated and must be archived so researchers can verify that the reports the drug companies publish accurately reflect the data the requirement of data archiving is a recent development in the history of science it was made possible by advances in information technology allowing large amounts of data to be stored and accessed from central locations for example the american geophysical union agu adopted their first policy on data archiving in 1993 about three years after the beginning of the www ref policy on referencing data in and archiving data for agu publications http www agu org pubs authors policies data policy shtml ref this policy mandates that datasets cited in agu papers must be archived by a recognised data center it permits the creation of data papers and it establishes agu s role in maintaining data archives but it makes no requirements on paper authors to archive their data prior to organized data archiving researchers wanting to evaluate or replicate a paper would have to request data and methods information from the author the academic community expects authors to data sharing science share supplemental data this process was recognized as wasteful of time and energy and obtained mixed results information could become lost or corrupted over the years in some cases authors simply refuse to provide the information the need for data archiving and due diligence is greatly increased when the research deals with health issues or public policy formation ref the case for due diligence when empirical research is used in policy formation by bruce mccullough and ross mckitrick http economics ca 2006 papers 0685 pdf ref ref http gking harvard edu replication shtml data sharing and replication a website by gary king ref selected policies by journals the american naturalist quote the american naturalist requires authors to deposit the data associated with accepted papers in a public archive for gene sequence data and phylogenetic trees deposition in genbank or treebase respectively is required there are many possible archives that may suit a particular data set including the dryad repository dryad repository for ecological and evolutionary biology data all accession numbers for genbank treebase and dryad must be included in accepted manuscripts before they go to production if the data is deposited somewhere else please provide a link if the data is culled from published literature please deposit the collated data in dryad for the convenience of your readers any impediments to data sharing should be brought to the attention of the editors at the time of submission so that appropriate arrangements can be worked out jstor ref http www jstor org page journal amernatu forauthor html data supporting data and material ref journal of heredity quote the primary data underlying the conclusions of an article are critical to the verifiability and transparency of the scientific enterprise and should be preserved in usable form for decades in the future for this reason journal of heredity requires that newly reported nucleotide or amino acid sequences and structural coordinates be submitted to appropriate public databases e g genbank the embl nucleotide sequence database dna database of japan the protein data bank and swiss prot accession numbers must be included in the final version of the manuscript for other forms of data e g microsatellite genotypes linkage maps images the journal endorses the principles of the joint data archiving policy jdap in encouraging all authors to archive primary datasets in an appropriate public archive such as dryad treebase or the knowledge network for biocomplexity authors are encouraged to make data publicly available at time of publication or if the technology of the archive allows opt to embargo access to the data for a period up to a year after publication the american genetic association also recognizes the vast investment of individual researchers in generating and curating large datasets consequently we recommend that this investment be respected in secondary analyses or meta analyses in a gracious collaborative spirit oxfordjournals org ref http www oxfordjournals org our journals jhered for authors msprep submission html 4 20data 20archiving 20policy data archiving policy ref molecular ecology quote molecular ecology expects that data supporting the results in the paper should be archived in an appropriate public archive such as genbank gene expression omnibus treebase dryad the knowledge network for biocomplexity your own institutional or funder repository or as supporting information on the molecular ecology web site data are important products of the scientific enterprise and they should be preserved and usable for decades in the future authors may elect to have the data publicly available at time of publication or if the technology of the archive allows may opt to embargo access to the data for a period up to a year after publication exceptions may be granted at the discretion of the editor especially for sensitive information such as human subject data or the location of endangered species wiley ref http www wiley com bw submit asp ref 0962 1083 site 1 policy on data archiving ref nature quote such material must be hosted on an accredited independent site url and accession numbers to be provided by the author or sent to the nature journal at submission either uploaded via the journal s online submission service or if the files are too large or in an unsuitable format for this purpose on cd dvd five copies such material cannot solely be hosted on an author s personal or institutional web site ref http www nature com authors editorial policies availability html availability of data and materials the policy of nature magazine ref nature requires the reviewer to determine if all of the supplementary data and methods have been archived the policy advises reviewers to consider several questions including should the authors be asked to provide supplementary methods or data to accompany the paper online such data might include source code for modelling studies detailed experimental protocols or mathematical derivations nature journal nature ref cite web title guide to publication policies of the nature journals date march 14 2007 url http www nature com authors gta pdf ref science quote science supports the efforts of databases that aggregate published data for the use of the scientific community therefore before publication large data sets including microarray data protein or dna sequences and atomic coordinates or electron microscopy maps for macromolecular structures must be deposited in an approved database and an accession number provided for inclusion in the published paper ref http www sciencemag org about authors prep gen info dtl datadep general policies of science magazine ref materials and methods science now requests that in general authors place the bulk of their description of materials and methods online as supporting material providing only as much methods description in the print manuscript as is necessary to follow the logic of the text obviously this restriction will not apply if the paper is fundamentally a study of a new method or technique science journal science ref http www sciencemag org about authors prep prep online dtl preparing your supporting online material ref royal society quote to allow others to verify and build on the work published in royal society journals it is a condition of publication that authors make available the data code and research materials supporting the results in the article datasets and code should be deposited in an appropriate recognised publicly available repository where no data specific repository exists authors should deposit their datasets in a general repository such as dryad repository or figshare royal society ref https royalsociety org journals ethics policies data sharing mining data sharing and mining ref policies by funding agencies in the united states the national science foundation nsf has tightened requirements on data archiving researchers seeking funding from nsf are now required to file a data management plan as a two page supplement to the grant application ref http news sciencemag org scienceinsider 2010 05 nsf to ask every grant applicant html nsf to ask every grant applicant for data management plan ref the nsf datanet initiative has resulted in funding of the data observation network for earth dataone project which will provide scientific data archiving for ecological and environmental data produced by scientists worldwide dataone s stated goal is to preserve and provide access to multi scale multi discipline and multi national data the community of users for dataone includes scientists ecosystem managers policy makers students educators and the public data archives natural sciences the following list refers to scientific data archives cisl research data archive dryad repository dryad eso st ecf science archive facility http www ncdc noaa gov paleo treering html international tree ring data bank http www icpsr umich edu inter university consortium for political and social research http knb ecoinformatics org knowledge network for biocomplexity national archive of computerized data on aging national archive of criminal justice data http www icpsr umich edu nacjd national climatic data center national geophysical data center national snow and ice data center national oceanographic data center http daac ornl gov oak ridge national laboratory distributed active archive center pangaea data library pangaea data publisher for earth environmental science world data center dataone social sciences cleanup merge data archives date april 2016 data archives are professional institutions for the acquisition preparation preservation and dissemination of social and behavioral data the term is also sometimes used about natural science institutions e g cisl research data archive see scientific data archiving and borgman 2007 p nbsp 18 ref borgman christine l 2007 scholarship in the digital age information infrastructure and the internet cambridge ma the mit press ref but here seems data centers to be the most used term data archives in the social sciences evolved in the 1950s and has been perceived as an international movement blockquote by 1964 the international social science council issc had sponsored a second conference on social science data archives and had a standing committee on social science data both of which stimulated the data archives movement by the beginning of the twenty first century most developed countries and some developing countries had organized formal and well functioning national data archives in addition college and university campuses often have data libraries that make data available to their faculty staff and students most of these bear minimal archival responsibility relying for that function on a national institution rockwell 2001 p 3227 ref rockwell r c 2001 data archives international in smelser n j baltes p b eds international encyclopedia of the social and behavioral sciences vol 5 pp 3225 3230 amsterdam elsevier ref blockquote registry of research data repositories re3data org is a global registry of research data repository indexing data archives from all disciplines http www re3data org cessda members are data archives and other organisations that archive social science data and provide data for secondary use http www cessda net about members html consortium of european social science data archives http www cessda org the danish data archives http www sa dk content us about us specific page only in danish http www sa dk dda default htm inter university consortium for political and social research http www icpsr umich edu the roper center for public opinion research http www ropercenter uconn edu the social science data archive http dataarchives ss ucla edu the ncar research data archive http rda ucar edu life sciences stub section date april 2016 see also data archive references reflist notes registry of research data repositories re3data org http service re3data org search results term statistical checklist required by nature http www nature com nature authors gta statistical checklist doc policies of proceedings of the national academy of sciences u s http www pnas org misc iforc shtml policies the us national committee for codata http www7 nationalacademies org usnc codata archiving html the role of data and program code archives in the future of economic research http research stlouisfed org wp 2005 2005 014 pdf data sharing and replication gary king website http gking harvard edu replication shtml the case for due diligence when empirical research is used in policy formation by mccullough and mckitrick http economics ca 2006 papers 0685 pdf thoughts on refereed journal publication by chuck doswell http www cimms ou edu doswell pubreviews html how to encourage the right behaviour an opinion piece published in nature march 2002 http www nature com nature journal v416 n6876 full 416001b html nasa astrophysics data system http cdsads u strasbg fr panton principles for open data in science at citizendium http en citizendium org wiki panton principles inter university consortium for political and social research http www icpsr umich edu category computer archives category data management category data publishing category digital preservation category information retrieval techniques category knowledge representation category structured storage'
b'outdated date january 2015 file vcf 2010 domesday tray open jpg thumb 300px a domesday project machine with its modified laserdisc the domesday project was published in 1986 digital obsolescence is a situation where a digital resource is no longer readable because of its archaic format the physical media the reader required to read the media the hardware or the software that runs on it is no longer available ref name national archives cite web last first authorlink title managing digital obsolescence risks work publisher the national archives date april 2009 url http www nationalarchives gov uk documents information management siro guidance on the risk of digital obsolescence pdf format pdf doi accessdate archiveurl http webarchive nationalarchives gov uk http www nationalarchives gov uk documents information management siro guidance on the risk of digital obsolescence pdf archivedate 28 jun 2011 ref a prime example of this is the bbc domesday project from the 1980s although its data was eventually recovered after a significant amount of effort cornell university library s http www icpsr umich edu dpm dpm eng eng index html digital preservation tutorial now hosted by icpsr has a timeline of obsolete media formats called the http www icpsr umich edu dpm dpm eng oldmedia index html chamber of horrors that shows how rapidly new technologies are created and cast aside introduction the rapid evolution and proliferation of different kinds of computer hardware modes of digital encoding operating systems and general or specialized software ensures that digital obsolescence will become a problem in the future ref rothenberg j 1998 http www clir org pubs reports rothenberg introduction html longevity avoiding technological quicksand finding a viable technical foundation for digital preservation ref many versions of word processing programs data storage media standards for encoding images and films are considered standards for some time but in the end are always replaced by new versions of the software or completely new hardware files meant to be read or edited with a certain program for example microsoft word will be unreadable in other programs and as operating systems and hardware move on even old versions of programs developed by the same company become impossible to use on the new platform for instance older versions of microsoft works before works 4 5 cannot be run under windows 2000 or later early attention was brought to the challenges of preserving machine readable data by the work of charles m dollar in the 1970s but it was only during the 1990s that libraries and archives came to appreciate the significance of the problem ref hedstrom m 1995 http www uky edu 7ekiernan dl hedstrom html digital preservation a time bomb for digital libraries ref and has been discussed among professionals in those branches though so far without any obvious solutions other than continual forward migration of files and information to the latest data storage standards file formats should be widespread backward compatible often upgraded and ideally open format in 2002 the national initiative for a networked cultural heritage cited ref national initiative for a networked cultural heritage 2002 http www nyu edu its humanities ninchguide v ninch guide to good practice in the digital representation and management of cultural heritage materials ref the following as de facto formats that are unlikely to be rendered obsolete in the near future uncompressed tiff and ascii and rich text format rtf for text in order to prevent this from happening it is important that an institution regularly evaluate and explore its current technologies and evaluate its long term business model ref name national archives types digital objects are vulnerable to three types of obsolescence ref cite web publisher national archives of australia title obsolescence a key challenge in the digital age url http www naa gov au records management agency preserve e preservation obsolescence aspx accessdate 17 march 2014 ref physical media the physical carrier of the digital file becomes obsolete e g 8 inch floppy disks which are no longer commercially available hardware the hardware needed to access the digital file becomes obsolete e g floppy disk drive which computers are no longer manufactured with software the software needed to access the digital file becomes obsolete e g wordstar a word processor popular in the 1980s which used a open data closed data format and is no longer readily available strategies any organization that has digital records should assess its records to identify any potential risks for file format obsolescence the library of congress maintains http www digitalpreservation gov formats intro intro shtml sustainability of digital formats which includes technical details about many different format types the uk national archives maintains an online registry of file formats called http www nationalarchives gov uk pronom default aspx pronom in its 2014 agenda the national digital stewardship alliance recommended developing file format action plans it is important to shift from more abstract considerations about file format obsolescence to develop actionable strategies for monitoring and mining information about the heterogeneous digital files the organizations are managing ref cite web publisher national digital stewardship alliance title national agenda for digital stewardship 2014 url http www digitalpreservation gov ndsa documents 2014nationalagenda pdf accessdate 17 march 2014 date 2014 ref file format action plans are documents internal to an organization which list the type of digital files in its holdings and assess what actions should be taken to ensure its ongoing accessibility ref cite web last owens first trevor title file format action plans in theory and practice url http blogs loc gov digitalpreservation 2014 01 file format action plans in theory and practice accessdate 17 march 2014 date 6 january 2014 ref examples include the http fclaweb fcla edu node 795 florida digital archive action plan and university of michigan s http deepblue lib umich edu static about deepbluepreservation html deep blue preservation and format support policy copyright issues untangling copyright issues also presented a significant challenge for projects attempting to overcome the obsolescence issues related to the bbc domesday project in addition to copyright surrounding the many contributions made by the estimated 1 million people who took part in the project there are also copyright issues that relate to the technologies employed it is likely that the domesday project will not be completely free of copyright restrictions until at least 2090 unless copyright laws are revised for earlier copyright term expiration of software into public domain software public domain ref cite web url http www2 si umich edu camileon reports iprreport doc title the camileon project legal issues arising from the work aiming to preserve elements of the interactive multimedia work entitled the bbc domesday project first andrew last charlesworth date 5 november 2002 publisher information law and technology unit university of hull location kingston upon hull format microsoft word accessdate 23 march 2011 ref intentional obsolescence in some cases obsolete technologies are used in a deliberate attempt to avoid data intrusion in a strategy known as security through obsolescence ref cite news url http www linux com articles 23313 title security through obsolescence first robin last miller publisher linux com date 2002 06 06 accessdate 2008 07 18 ref see also obsolescence digital preservation digital dark age camileon emulation computing m disc references reflist 30em external links http www digitalpreservation gov formats the library of congress sustainability of digital formats wired magazine http wired vig wired com wired archive 6 09 saved html what death can t destroy and how to digitize it https www icpsr umich edu icpsrweb content datamanagement preservation digital preservation at icpsr digitalpreservation defaultsort digital obsolescence category data management category digital libraries category digital preservation category future problems category obsolescence category records management'
b'other uses knowledge base an information repository is an easy way to deploy a secondary tier of data storage device data storage that can comprise multiple networked data storage technologies running on diverse operating system s where data that no longer needs to be in primary storage is protected classified according to captured metadata processed de duplicated and then purged automatically based on data service level objectives and requirements in information repositories data storage resources are virtualized as composite storage sets and operate as a federation information technology federated environment information repositories were developed to mitigate problems arising from data proliferation and eliminate the need for separately deployed data storage solutions because of the concurrent deployment of diverse storage technologies running diverse operating systems they feature centralized management for all deployed data storage resources they are self contained support heterogeneous storage resources support resource management to add maintain recycle and terminate media track of off line media and operate autonomously automated data management since one of the main reasons for the implementation of an information repository is to reduce the maintenance workload placed on it staff by traditional data storage systems information repositories are automated automation is accomplished via polices that can process data based on time events data age and data content policies manage the following file system space management irrelevant data elimination mp3 games etc secondary storage resource management data is processed according to media type storage virtualization storage pool and data storage device storage technology because information repositories are intended to reduce it staff workload they are designed to be easy to deploy and offer configuration flexibility virtually limitless extensibility redundancy and reliable failover data recovery information repositories feature robust client based data search and recovery capabilities that based on permissions enable end users to search the information repository view information repository contents including data on off line media and recover individual files or multiple files to either their original computer network network computer or another network computer references ngdc conference understand advanced it infrastructures protecting information benefits of a federated information repository as a secondary storage tier http www networkworld com ngdc snia enterprise information world 2007 conference benefits of a federated information repository as a secondary storage tier http www enterpriseinformationworld com abstracts benefits federated info htm defaultsort information repository category information technology management category content management systems category data management category data security category records management'
b'orphan date january 2017 infobox company name imprima irooms logo imprima logo hires 300dpi jpg logo size logo alt imprima logo hires 300dpi jpg logo caption type private company private industry virtual data room technology founded 1910 br 2001 small relaunched small founder hq location london united kingdom area served worldwide website url www imprima com imprima irooms is a private company headquartered in london it provides virtual data room services to organisations worldwide including the likes of morgan stanley hsbc and others ref name printweek cite news last1 francis first1 jo title mbo at imprima print operation printweek url http www printweek com print week news 1148840 mbo imprima print operation accessdate 22 august 2016 publisher printweek ref it also has offices in paris frankfurt amsterdam and new york ref name growthbusiness cite news title the 21st century virtual data room a how to guide url http www growthbusiness co uk growing a business technology for business 2471012 the 21st century virtual data room a howto guide thtml accessdate 22 august 2016 publisher growth business uk ref history imprima was founded over 100 years ago and during this time has served the financial sector in a variety of different capacities ref cite web title imprima de bussy limited private company information url http www bloomberg com research stocks private snapshot asp privcapid 613207 publisher bloomberg businessweek accessdate 22 august 2016 ref by 1990 the company was a leading provider of financial print solutions involved in the publication and delivery of sensitive and business critical communications for their clients ref cite web title companies and products url http www ukauthority com market report news 4614 companies and products publisher uk authority accessdate 25 august 2016 ref in doing so imprima amassed an impressive customer list featuring some of the world s most reputable financial advisors law firms and corporations ref name printweek ref name cloudnewsdaily cite web title virtual data room providers url http cloudnewsdaily com virtual data room publisher cloud news daily accessdate 22 august 2016 ref irooms in 2001 imprima launched their virtual data room platform irooms ref name teletrader irooms is used by organisations worldwide for projects requiring secure online file storage and collaboration key use cases include mergers acquisitions m a activity and real estate transaction real estate transactions ref name francis cite news title rebrand for imprima financial print url http www printweek com print week news 1154511 rebrand for imprima financial print accessdate 22 august 2016 work www printweek com publisher printweek ref in 2012 irooms software was completely revamped and receives regular upgrades ref name cloudnewsdaily ref name francis new ownership in 2014 iroom was acquired by its current owner otm participation at that time imprima operated two product lines financial print and irooms virtual data rooms ref name growthbusiness in november 2014 otm participation took the decision to divest away the financial print division whose directors carried out an mbo ref name francis ref name teletrader cite web title imprima adds multiple language interfaces to new irooms release url http www teletrader com news details 6743031 ts 1471881604044 publisher www teletrader com accessdate 22 august 2016 ref references reflist category data management category 1910 establishments in the united kingdom category companies based in london category technology companies established in 1910 category technology companies of the united kingdom'
b'refimprove date april 2008 file physical data model options jpg thumb 320px physical data model options ref name wh05 http georgewbush whitehouse archives gov omb egov documents crm pdf fea consolidated reference model document whitehouse gov may 2005 p 91 webarchive url https web archive org web 20100705040628 http georgewbush whitehouse archives gov omb egov documents crm pdf date july 5 2010 ref a physical data model or database design is a representation of a data design as implemented or intended to be implemented in a database management system in the project lifecycle lifecycle of a project it typically derives from a logical data model though it may be reverse engineer ed from a given database implementation a complete physical data model will include all the database artifact s required to create relationships between table s or to achieve performance goals such as index database index es constraint definitions linking tables partitioned table s or cluster computing cluster s analysts can usually use a physical data model to calculate storage estimates it may include specific storage allocation details for a given database system as of 2012 seven main databases dominate the commercial marketplace informix dynamic server informix oracle database oracle postgresql postgres microsoft sql server sql server sybase ibm db2 db2 and mysql other rdbms systems tend either to be legacy databases or used within academia such as universities or further education colleges physical data models for each implementation would differ significantly not least due to underlying operating system operating system requirements that may sit underneath them for example sql server runs only on microsoft windows operating systems while oracle and mysql can run on solaris linux and other unix based operating systems as well as on windows this means that the disk requirements security requirements and many other aspects of a physical data model will be influenced by the rdbms that a database administrator or an organization chooses to use physical schema physical schema is a term used in data management to describe how data is to be represented and stored files indices et al in secondary storage using a particular database management system dbms e g oracle rdbms sybase sql server etc in the ansi sparc architecture ansi sparc architecture three schema approach the internal schema is the view of data that involved data management technology this is as opposed to an external schema that reflects an individual s view of the data or the conceptual schema that is the integration of a set of external schemas subsequently citation needed date june 2012 the internal schema was recognized to have two parts the logical schema was the way data were represented to conform to the constraints of a particular approach to database management at that time the choices were hierarchical and network describing the logical schema however still did not describe how physically data would be stored on disk drives that is the domain of the physical schema now logical schemas describe data in terms of relational tables and columns object oriented classes and xml tags a single set of tables for example can be implemented in numerous ways up to and including an architecture where table rows are maintained on computers in different countries see also database schema logical schema references reflist defaultsort physical data model category data modeling category data management http www whitehouse gov sites default files omb assets fea docs fea crm v23 final oct 2007 revised pdf fea consolidated reference model document whitehouse gov oct 2007 ja \xe3\x82\xb9\xe3\x82\xad\xe3\x83\xbc\xe3\x83\x9e \xe3\x83\x87\xe3\x83\xbc\xe3\x82\xbf\xe3\x83\x99\xe3\x83\xbc\xe3\x82\xb9'
b'machine readable documents are document s whose content can be readily processed by computer s such documents are distinguished from machine readable data by virtue of having sufficient structure to provide the necessary context to support the business processes for which they are created data without context language use is meaningless and lacks the four essential http www archives gov records mgmt policy managing web records html 1 0 characteristics of trustworthy business record s specified in iso 15489 information and documentation records management reliability authenticity integrity usability the vast bulk of information is unstructured data and from a business perspective that means it is immature i e level 1 chaotic of the capability maturity model such immaturity fosters inefficiency diminishes quality and limits effectiveness unstructured information is also ill suited for records management functions provides inadequate evidence for legal purposes drives up the cost of discovery law in litigation and makes access and usage needlessly cumbersome in routine ongoing business process es there are at least four aspects to machine readability first words or phrases should be discretely delineated tagged so that computer software and or hardware logic can be applied to them as individual conceptual elements second the semantics of each element should be specified so that computers can help human beings achieve a common understanding of their meanings and potential usages third if the relationships among the individual elements are also specified computers can automatically apply inferences to them thereby further relieving human beings of the burden of trying to understand them particularly for purposes of inquiry discovery and analysis fourth if the structures of the documents in which the elements occur are also specified human understanding is further enhanced and the data becomes more reliable for legal and business quality purposes as early as 1981 the u s government accountability office gao began reporting on the problem of inadequate record keeping practices in the u s federal government ref cite web url http www gao gov products plrd 81 2 title federal records management a history of neglect work gao gov date 1981 02 24 accessdate 2016 09 08 ref such deficiencies are not unique to government and advances in information technology mean that most information is now born digital and thus potentially far more easily managed by automated means ref cite web url http www oclc org content dam research activities hiddencollections borndigital pdf title defining born digital an essay by ricky erway oclc research work oclc org date 2010 11 30 accessdate 2016 09 08 ref however in testimony to congress in 2010 gao highlighted problems with managing electronic records and as recently as 2015 gao has continued to report inadequacies in the performance of executive branch agencies in meeting records management requirements ref cite web url http www gao gov new items d10838t pdf title information management the challenges of managing electronic records statement of valerie c melvin director information management and human capital issues work gao gov date 2010 06 17 accessdate 2016 09 08 ref ref cite web url http www gao gov products gao 15 339 title information management additional actions are needed to meet requirements of the managing government records directive work gao gov date 2015 05 14 accessdate 2016 09 08 ref moreover more than two decades after a major and formerly highly respected auditing firm arthur andersen met its demise due to a records destruction scandal record keeping practices became a central issue in the 2016 presidential election on january 4 2011 president obama signed h r 2142 the government performance and results act gpra modernization act of 2010 gprama into law as p l 111 352 section 10 of gprama requires u s federal agencies to publish their strategic and performance plans and reports in searchable machine readable format ref cite web url http xml fido gov stratml references pl111 532stratml htm sec10 title gprama sec 10 format of performance plans and reports work congress gov date 2011 01 04 accessdate 2016 09 08 ref additionally in 2013 he issued executive order 13642 making open and machine readable the new default for government information in general ref cite web url http xml fido gov stratml carmel eoomrdwstyle xml title executive order 13642 in open standard machine readable strategy markup language format work whitehouse gov date 2013 05 09 accessdate 2016 09 08 ref on july 28 2016 the office of management and budget omb followed up by including in the revised issuance of circular a 130 direction for agencies to use http xml fido gov stratml carmel iso a130wstyle xml 3f7d15f0 5799 11e6 8d37 8523b3fa12e0 open machine readable formats and to publish http xml fido gov stratml carmel iso a130wstyle xml 3f7d449e 5799 11e6 8d37 8523b3fa12e0 public information online in a manner that promotes analysis and reuse for the widest possible range of purposes meaning that the information is both publicly accessible and machine readable in support of such policy direction technological advancement is enabling more efficient and effective management and use of machine readable electronic records document oriented database s have been developed for storing retrieving and managing document oriented information also known as semi structured data extensible markup language xml is a world wide web consortium w3c world wide web consortium w3c recommendation 28rec 29 recommendation setting forth rules for encoding documents in a format that is both human readable and machine readable many xml editor tools have been developed and most if not all major information technology applications support xml to greater or lesser degrees the fact that xml itself is an open standard machine readable format makes it relatively easy for application developers to do so the w3c s accompanying xml schema xsd recommendation specifies how to formally describe the elements in an xml document with respect to the specification of xml schemas the organization for the advancement of structured information standards oasis is a leading standards developing organization json json schema json schema was proposed by the internet engineering task force ietf but was allowed to expire in 2013 and thus is less mature and a riskier alternative to xsd the most recent version of which was approved by the w3c in 2012 the w3c s extensible stylesheet language xsl family of languages provides for the transformation and rendering of xml documents for human readable presentation machine readable documents can be automatically rendered in human readable format but documents formatted primarily for attractiveness of presentation cannot easily be processed by computers to support usability by human beings the portable document format pdf is a file format used to present documents in a manner independent of application software hardware and operating systems each pdf file encapsulates a complete description of the presentation of the document including the text fonts graphics and other information needed to display it pdf a is an iso standardized version of the pdf specialized for use in the archiving and long term preservation of electronic documents pdf a 3 allows embedding of other file formats including xml into pdf a conforming documents thus potentially providing the best of both human and machine readability the w3c s xsl fo xsl formatting objects markup language is commonly used to generate pdf files metadata data about data can be used to organize electronic resources provide digital identification and support the archiving and preservation of resources in well structured machine readable electronic records the content can be repurposing repurposed as both data and metadata in the context of electronic record keeping systems the terms management and metadata are virtually synonymous given proper metadata records management functions can be automated thereby reducing the risk of spoliation of evidence and other fraudulent manipulations of records moreover such records can be used to automate the process of audit ing data maintained in database s thereby reducing the risk of single points of failure associated with the machiavellianism in the workplace machiavellian concept of a single source of truth blockchain database is a new technology for maintaining continuously growing lists of records secured from tampering and revision a key feature is that every node in a decentralized system has a copy of the blockchain so there is no single point of failure subject to manipulation and fraud see also budapest declaration on machine readable travel documents comparison of xml editors integrity and particularly data integrity linked data machine readable passport open data data reliability reliability particularly reliability statistics data reliability reliability computer networking and reliability research methods strategy markup language stratml structured document tag metadata universal business language ubl xbrl extensible business reporting language references reflist external links http xml fido gov stratml carmel m 13 13wstyle xml 78e85ef4 b91c 11e2 bf2b 79d279ad226c omb m 13 13 open data policy managing information as an asset which requires agencies to use open machine readable data format standards http ambur net caponeconsultancymethod pdf driving a stake in the heart of the capone consultancy method of records management best practices for correcting non records non policy nonsense march 9 2015 the u s code which includes http uscode house gov search xhtml searchstring machine readable pagenumber 1 itemsperpage 100 sortfield code order action search q bwfjagluzs1yzwfkywjszq 3d 3d 7c 3a 3a 3a 3a 3a 3a 3a 3afalse 3a 7c 3a 3a 3a 3a 3a 3a 3a 3afalse 3a 7cfalse 7c 5b 3a 3a 3a 3a 3a 3a 3a 3afalse 3a 5d 7c 5b 3a 5d 51 references to the term machine readable as of september 10 2016 category data management category records management'
b'intelligence engines are a type of enterprise information management that combine business rule management system business rule management predictive analytics predictive and prescriptive analytics to form a unified information access platform that provides real time intelligence through web search engine search technologies dashboard management information systems dashboards and or existing business infrastructure intelligence engines are process and or business problem specific resulting in industry and or function specific marketing trademark s associated with them they can be differentiated from enterprise resource planning erp software in that intelligence engines include organization level business rules and proactive decision management functionality history the first intelligence engine application appears to have been introduced in 2001 by sonus networks sonus networks inc in their patent us6961334 b1 ref name sonus cite patent country us number 6961334 status patent title intelligence engine gdate 2005 11 01 invent1 kaczmarczyk casimer m ref applied to the field of telecommunications systems the intelligence engine was composed of a database queried by a data distributor layer received by a telephony management layer and acted upon by a facility management command control layer ref name sonus this combined standalone business intelligence tools like a data warehouse reporting and querying software and a decision support system the concept was reinforced in 2002 in patent application us20030236689 a1 ref name 2002patent cite patent country us number 20030236689 status application title analyzing decision points in business processes pubdate 2003 12 25 invent1 casati fabio invent2 sayal mehmet invent3 guadalupe castellanos maria invent4 gunopulos dimitrios url https worldwide espacenet com publicationdetails biblio cc us nr 2003236689 kc a1 ft e locale en ep ref which applied predictive quantitative models to data and used rules to correlate context data at different stages of the business process with business process outcomes to be presented to end users ref name 2002patent logrhythm logrhythm inc advanced the concept in 2010 by adding event managers to the end of the intelligence engine s process to determine reporting remediation and other outcomes ref name logrhythm cite patent country us number 2012131185 status application title advanced intelligence engine pubdate 2012 05 24 invent1 petersen chris invent2 villella phillip invent3 aisa brad url https worldwide espacenet com publicationdetails biblio cc us nr 2012131185a1 kc a1 ft d ref in 2016 professional service company kpmg continued to advance the concept by commercializing intelligence engines with the introduction of third party intelligence which is differentiated from past intelligence engines in its increased use of embedded intellectual property diversity of global data inputs and focus on predictive analytics to mitigate risk and yield cost savings ref name cioreview cite web url http www cioreview com news kmpg launches third party intelligence intelligence engine to anticipate thirdparty disruptions nid 14407 cid 78 html title kmpg launches third party intelligence intelligence engine to anticipate third party disruptions accessdate 2016 07 22 ref traits as a system that combines human intelligence data inputs automated decision making and unified information access intelligence engines are an advancement in business intelligence tools because they integrate structured data and unstructured content in a single index ref name itbiz cite web url http www itbusinessedge com blogs it unmasked attivio applies predictive analytics to indexed data html title attivio applies predictive analytics to indexed data accessdate 2016 07 22 ref provide advanced workflow automation that can trigger multiple business processes ref name salesforce cite press release url http www salesforce com company news press press releases 2015 03 150309 jsp title salesforce unveils service cloud intelligence engine fueling smarter customer service for the connected world publisher salesforce com accessdate 2016 07 22 ref project future impact of data ref name inboundlog cite web url http resources inboundlogistics com digital issues il digital may2016 pdf title inbound logistics magazine may 2016 accessdate 2016 07 22 ref such as supply chain threats ref name 3pie cite web url https www kpmgspectrum com 3pie index html title kpmg spectrum accessdate 2016 07 22 ref recommend best actions ref name custmatrix cite web url http www customermatrix com news and press releases press releases 145 customermatrix unveils first ever cognitive intelligence engine for crm 2 title customermatrix unveils first ever cognitive intelligence engine for crm accessdate 2016 07 22 ref highlight opportunities for process improvement ref name parasoft cite web url https www parasoft com capability process intelligence engine title process intelligence engine pie accessdate 2016 07 22 ref leverage business intelligence from a variety of experts ref name inboundlog combine human expertise with the power of technology to deliver actionable intelligence ref name cioreview scale data visualization capabilities with the number of users ref name armanta cite web url http www armanta com product technology intelligence engine title a big data user experience accessdate 2016 07 22 ref applications attivio active intelligence engine\xc2\xae ref name attivo pr cite web url http info attivio com rs attivio images attivio customer succes story general electric pdf title active intelligence engine reg aie reg case study general electric accessdate 2016 07 22 ref kpmg spectrum intelligence engine s ref name kpmgie cite web url https www kpmgspectrum com 3pie about html title kpmg spectrum action through intelligence accessdate 2016 07 22 ref salesforce com salesforce service cloud intelligence engine ref name salesforce fireeye threat intelligence engine ref name fireeye cite web url https www fireeye com products dynamic threat intelligence threat intelligence engine html title fireeye threat intelligence engine accessdate 2016 07 22 ref factiva intelligence engine ref name factiva cite web url http solutions dowjones com collateral files dj factivacom brochure f 3465 pdf title factiva reg the intelligence engine accessdate 2016 07 22 ref parasoft process intelligence engine ref name parasoft see also business intelligence bi business intelligence tools business rule management system enterprise information management predictive analytics prescriptive analytics decision management data science data mining references reflist defaultsort intelligence engine category data management category information management category big data category business terms category business intelligence category information systems category supply chain management terms'
b'underlinked date july 2016 don t mess with this line write your article below this line storediq was a company founded for information lifecycle management ilm of unstructured data founded in 2001 as deepfile ref cite news title deepfile comes to the surface url http www networkcomputing com storage deepfile comes surface 865316998 publisher network computing ref in austin texas by jeff erramouspe jeff bone russell turpin rudy rouhana laura arbilla and brett funderburg ref cite news title enterprise file management made easy url http www networkworld com article 2332452 wireless deepfile html publisher network world ref the company changed its name in 2005 to storediq ref cite news title deep file becomes storediq url http www networkcomputing com storage deepfile becomes storediq 1788209585 publisher network computing ref and continued to operate successfully for over a decade until it was acquired in 2012 by ibm ref cite web title ibm extends ilg suite and big data governance with storediq acquisition url http public dhe ibm com software data sw library ecm programs parity research storediq whitepaper pdf website ibm ref it now serves as a platform for ibm s information life cycle governance big data governance and enterprise content management technologies ref cite web title storediq is now an ibm company url https www 01 ibm com software info storediq website ibm ref storediq was awarded five patents by the uspto the first originally filed in 2003 enabled unstructured data in file systems to be manipulated in a similar way to information stored in databases ref cite web title method and apparatus for managing file systems and file based data storage url http patents justia com assignee storediq inc website justia patents ref subsequent patents only added to storediq s market dominance by building upon the patented actionable file system with further enhancements specific to enterprise policy management and expanding the reach of storediq s management capability all the way to individual desktops ref cite web title patents by assignee storediq inc url http patents justia com assignee storediq inc website justia patents ref in 2008 storediq was recognized as best in compliance by network products guide ref cite web title storediq wins network products guide award for best in compliance url http www datastorageconnection com doc storediq network products best in compliance 0001 publisher data storage connection ref at the same time storediq was being recognized as a top 5 provider by the prestigious socha gelbmann ediscovery survey ref cite web title storediq recognized with top 5 provider rating in socha gelbmann ediscovery survey url http www datastorageconnection com doc torediq ediscovery survey storage 0001 publisher data storage connection ref this incredible breath of information governance capability is what originally drew the attention of emc corporation storediq s first potential acquirer initially a strategic investor in storediq many experts who date august 2016 predicted an inevitable acquisition however the company shunned their first suitor leaving emc to acquire a competitor ref cite web title emc acquires kazeon stiffs storediq url http www informationweek com software information management emc acquires kazeon stiffs storediq d d id 1082836 publisher information week ref the company published a whitepaper titled the truth about big data this promotion combined with storediq s patented technology led to ibm selecting storediq as the basis for some products ref cite news last1 butta first1 tom title the truth behind ibm s plans to acquire big data company storediq url http www huffingtonpost com entry ibm storediq b 2377339 publisher huffington post date 2012 12 31 ref references reflist after listing your sources please cite them using inline citations and place them after the information they cite please see http en wikipedia org wiki wikipedia refb for instructions on how to add citations category companies established in 2001 category companies based in austin texas category information technology management category data management category data warehousing'
b'big memory is a term used to describe server workloads which need to run on machines with a large amount of ram random access memory memory some example workloads are databases in memory caches and graph analytics ref cite web url http research cs wisc edu multifacet papers isca13 direct segment pdf title efficient virtual memory for big memory servers accessdate 2016 09 24 ref or more generally data science and big data some database systems are designed to run mostly in memory rarely if ever retrieving data from disk or flash memory see a list of in memory databases the performance of big memory systems depends on how the cpu s or cpu cores access the memory via a conventional memory controller or via numa non uniform memory access performance also depends on the size and design of the cpu cache performance also depends on os design the huge pages feature in linux can improve the efficiency of virtual memory ref cite web url http lwn net articles 374424 title huge pages part 1 introduction accessdate 2016 09 24 ref the new transparent huge pages feature in linux can offer better performance for some big memory workloads ref cite web url http lwn net articles 423584 title transparent huge pages in 2 6 38 accessdate 2016 09 24 ref the large page support in microsoft windows enables server applications to establish large page memory regions which are typically three orders of magnitude larger than the native page size ref cite web url https msdn microsoft com en us library windows desktop aa366720 v vs 85 aspx title large page support accessdate 2016 09 24 ref references reflist database stub category big data category data management category distributed computing problems category technology forecasting category transaction processing'
b'multiple issues orphan date october 2016 refimprove date october 2016 data exhaust refers to the trail of data left by the activities of an internet user during his her online activity an enormous amount of often raw data are created these data which can take the form of cookie computing cookies temporary files log file s etc can help to improve the online experience for example through customized content but they can also compromise privacy as they offer a valuable insight into the user s habits it can be used to improve tracking trends and studying data exhaust also improves the user interface and the layout design ref name techtarget cite web url http whatis techtarget com definition data exhaust title what is data exhaust definition from whatis com publisher ref unlike primary content these data are not purposefully created by the user who is often unaware of their very existence a bank for example would consider as primary data information concerning the sums and parties of a transaction whilst secondary data might include the percentage of transactions carried out at a cash machine instead of a real bank ref cite web url http www pcworld com article 3069507 5 things you need to know about data exhaust html title 5 things you need to know about data exhaust publisher ref references references category data management internet stub'
b'object storage also known as object based storage ref cite journal last mesnier first mike author2 gregory r ganger author3 erik riedel title object based storage journal ieee communications magazine date august 2003 pages 84 90 url http www storagevisions com white 20papers mesnierieee03 pdf accessdate 27 october 2013 doi 10 1109 mcom 2003 1222722 ref is a storage architecture that manages data as objects as opposed to other storage architectures like file systems which manage data as a file hierarchy and block storage which manages data as blocks within sectors and tracks ref cite web last porter de leon first yadin author2 tony piscopo title object storage versus block storage understanding the technology differences url http www druva com blog object storage versus block storage understanding technology differences publisher druva com accessdate 19 january 2015 ref each object typically includes the data itself a variable amount of metadata and a globally unique identifier object storage can be implemented at multiple levels including the device level object storage device the system level and the interface level in each case object storage seeks to enable capabilities not addressed by other storage architectures like interfaces that can be directly programmable by the application a namespace that can span multiple instances of physical hardware and data management functions like data replication and data distribution at object level granularity object storage systems allow relatively inexpensive scalable and self healing retention of massive amounts of unstructured data object storage is used for diverse purposes such as storing photos on facebook songs on spotify or files in online collaboration services such as dropbox service dropbox ref cite web authors chandrasekaran arun dayley alan title critical capabilities for object storage publisher gartner research date 11 february 2014 url http www gartner com technology reprints do id 1 1r78pj9 ct 140226 st sb ref history origins in 1995 new research by garth gibson et al on network attached secure disks first promoted the concept of splitting less common operations like namespace manipulations from common operations like reads and writes to optimize the performance and scale of both ref name nasd cite web title file server scaling with network attached secure disks url http www pdl cmu edu ftp nasd sigmetrics97 pdf publisher proceedings of the acm international conference on measurement and modeling of computer systems sigmetrics 97 accessdate 27 october 2013 author garth a gibson author2 nagle d author3 amiri k author4 chan f author5 feinberg e author6 gobioff h author7 lee c author8 ozceri b author9 riedel e author10 rochberg d author11 zelenka j ref in the same year 1995 a belgium company filepool was established to build the basis for archiving functions by using those and own concepts object storage was proposed at carnegie mellon university carnegie mellon university s parallel data lab as a research project in 1996 ref cite web last1 factor first1 michael last2 meth first2 k last3 naor first3 d last4 rodeh first4 o last5 satran first5 j title object storage the future building block for storage systems url http webhdd ru library files positionosd pdf publisher ibm haifa research labs accessdate 26 september 2013 ref another key concept was abstracting the writes and reads of data to more flexible data containers objects fine grained access control through object storage architecture ref cite web title security for network attached storage devices cmu cs 97 185 url http repository cmu edu cgi viewcontent cgi article 1147 context pdl publisher parallel data laboratory accessdate 7 november 2013 author gobioff howard author2 gibson garth a author3 tygar doug date 1 october 1997 ref was further described by one of the nasd team howard gobioff who later was one of the inventors of the google file system ref cite web title the google file system url http research google com archive gfs sosp2003 pdf publisher google accessdate 7 november 2013 author sanjay ghemawat author2 howard gobioff author3 shun tak leung date october 2003 ref other related work includes the coda file system coda filesystem project at carnegie mellon which started in 1987 and spawned the lustre file system lustre file system ref name lustre cite web last braam first peter title lustre the intergalactic \xef\xac\x81le system url http ols fedoraproject org ols reprints 2002 braam reprint pdf accessdate 17 september 2013 ref there is also the oceanstore project at uc berkeley ref cite web title oceanstore url http oceanstore cs berkeley edu accessdate 18 september 2013 ref which started in 1999 ref cite journal last1 kubiatowicz first1 john last2 bindel first2 d last3 chen first3 y last4 czerwinski first4 s last5 eaton first5 p last6 geels first6 d last7 gummadi first7 r last8 rhea first8 s last9 weatherspoon first9 h last10 weimer first10 w last11 wells first11 c last12 zhao first12 b title oceanstore an architecture for global scale persistent storage journal proceedings of the ninth international conference on architectural support for programming languages and operating systems asplos 2000 date november 2000 url http oceanstore cs berkeley edu publications papers pdf asplos00 pdf accessdate 18 september 2013 ref one of the earliest and best known object storage products emc s centera debuted in 2002 ref cite news title emc unveils low cost data storage product url http articles latimes com 2002 apr 30 business fi techbriefs30 3 accessdate 17 september 2013 newspaper la times date april 30 2002 ref content addressable storage centera s technology has been developed at filepool and the company had been acquired by emc\xc2\xb2 in 2002 development overall industry investment in object storage technology has been sustained for over a decade from 1999 to 2013 there has been at least 300 million of venture financing related to object storage including vendors like amplidata bycast cleversafe cloudian nirvanix and scality ref cite web last leung first leo title after 10 years object storage investment continues and begins to bear significant fruit url http blog oxygencloud com 2013 09 16 after 10 years object storage investment continues and begins to bear significant fruit accessdate 17 september 2013 date 16 september 2013 ref this doesn t include millions of dollars of private engineering from systems vendors like datadirect networks wos http www emc com en us storage ecs index htm collapse tab14 0 dell emc elastic cloud storage centera atmos hds hcp hp hp openstack ibm netapp storagegrid redhat glusterfs and http www keepertech com keeper technology http www keepertech com products keepersafe keepersafe cloud services vendors like amazon aws s3 microsoft microsoft azure and google google cloud storage or the many man years of open source development at lustre file system lustre openstack openstack object storage 28swift 29 swift mogilefs ceph file system ceph skylable sx object storage skylable sx and openio ref name mellor cite web last mellor first chris dec 2 2015 title openio s objective is opening up object storage space url http www theregister co uk 2015 12 02 openio object storage upstart ref ref name nicolas cite web last nicolas first philippe oct 2 2015 title openio ready to take off url http filestorage blogspot fr 2015 10 openio ready to take off html ref ref name raffo cite web last raffo first dave may 20 2016 title openio joins object storage cloud scrum url http searchcloudstorage techtarget com news 450296765 openio joins object storage cloud scrum ref ref name maleval cite web last maleval first jean jacques apr 25 2016 title start up profile openio url http www storagenewsletter com rubriques start ups start up profile openio ref a great article written by philippe nicolas illustrating products timeline was published in july 2016 on the register with all players pioneers mergers and acquisitions and of course genesis with cas included ref cite web last nicolas first philippe july 15 2016 title the history boys object storage from the beginning url http www theregister co uk 2016 07 15 the history boys cas and object storage map ref architecture file high level object storage architecture png thumb abstraction of storage one of the design principles of object storage is to abstract some of the lower layers of storage away from the administrators and applications thus data is exposed and managed as objects instead of files or blocks objects contain additional descriptive properties which can be used for better indexing or management administrators do not have to perform lower level storage functions like constructing and managing logical unit number logical volumes to utilize disk capacity or setting raid levels to deal with disk failure object storage also allows the addressing and identification of individual objects by more than just file name and file path object storage adds a unique identifier within a bucket or across the entire system to support much larger namespaces and eliminate name collisions inclusion of rich custom metadata within the object object storage explicitly separates file metadata from data to support additional capabilities as opposed to fixed metadata in file systems filename creation date type etc object storage provides for full function custom object level metadata in order to capture application specific or user specific information for better indexing purposes support data management policies e g a policy to drive object movement from one storage tier to another centralize management of storage across many individual nodes and clusters optimize metadata storage e g encapsulated database or key value storage and caching indexing when authoritative metadata is encapsulated with the metadata inside the object independently from the data storage e g unstructured binary storage additionally in some object based file system implementations the file system clients only contact metadata servers once when the file is opened and then get content directly via object storage servers vs block based file systems which would require constant metadata access data objects can be configured on a per file basis to allow adaptive stripe width even across multiple object storage servers supporting optimizations in bandwidth and i o object based storage devices osd as well as some software implementations e g caringo swarm manage metadata and data at the storage device level instead of providing a block oriented interface that reads and writes fixed sized blocks of data data is organized into flexible sized data containers called objects each object has both data an uninterpreted sequence of bytes and metadata an extensible set of attributes describing the object physically encapsulating both together benefits recoverability the command interface includes commands to create and delete objects write bytes and read bytes to and from individual objects and to set and get attributes on objects security mechanisms provide per object and per command access control programmatic data management object storage provides programmatic interfaces to allow applications to manipulate data at the base level this includes crud functions for basic read write and delete operations some object storage implementations go further supporting additional functionality like object versioning object replication and movement of objects between different tiers and types of storage most api implementations are representational state transfer rest based allowing the use of many standard http calls implementation object based storage devices object storage at the protocol and device layer was proposed 20 years ago and approved for the scsi command set nearly 10 years ago as object based storage device commands osd ref cite web last riedel first erik title object storage and applications url https www usenix org legacy event lsf07 tech riedel pdf accessdate 3 november 2013 author2 sami iren date february 2007 ref but has not been productized until the development of the seagate kinetic open storage platform ref cite web title the seagate kinetic open storage vision url http www seagate com tech insights kinetic vision how seagate new developer tools meets the needs of cloud storage platforms master ti publisher seagate accessdate 3 november 2013 ref ref cite news last gallagher first sean title seagate introduces a new drive interface ethernet url http arstechnica com information technology 2013 10 seagate introduces a new drive interface ethernet accessdate 3 november 2013 newspaper arstechnica com date 27 october 2013 ref the scsi command set for object storage devices was developed by a working group of the storage networking industry association snia for the t10 committee of the international committee for information technology standards incits ref cite web last corbet first jonathan title linux and object storage devices url https lwn net articles 305740 accessdate 8 november 2013 newspaper lwn net date 4 november 2008 ref t10 is responsible for all scsi standards object based file systems some distributed file systems use an object based architecture where file metadata is stored in metadata servers and file data is stored in object storage servers file system client software interacts with the distinct servers and abstracts them to present a full file system to users and applications ibm general parallel file system ibm spectrum scale also known as gpfs http www emc com en us storage ecs index htm collapse tab14 0 dell emc elastic cloud storage ceph software ceph xtreemfs and lustre file system lustre are examples of this type of object storage archive storage some early incarnations of object storage were used for archiving as implementations were optimized for data services like immutability not performance content addressable storage emc centera and hitachi hcp formerly known as hcap are two commonly cited object storage products for archiving another example is quantum lattus object storage platform cloud storage the vast majority of cloud storage available in the market leverages an object storage architecture two notable examples are aws s3 amazon web services s3 which debuted in 2005 and rackspace files whose code was released as openstack swift openstack swift other major cloud storage services include microsoft azure google cloud storage alibaba cloud oss oracle elastic storage service and dreamhost based on ceph captive object storage some large internet companies developed their own software when object storage products were not commercially available or use cases were very specific facebook famously invented their own object storage software code named haystack to address their particular massive scale photo management needs efficiently ref name haystack cite web last vajgel first peter title needle in a haystack efficient storage of billions of photos url https www facebook com note php note id 76191543919 accessdate 17 september 2013 ref hybrid storage a few object storage systems such as ceph software ceph glusterfs cloudian ref name primesberger cite web last primesberger first chris 27 october 2016 title cloudian raises 41 million vc for hybrid cloud object storage url http www eweek com storage cloudian raises 41 million vc for hybrid cloud object storage html ref and scality support unified file and object ufo storage allowing some clients to store objects on a storage system while simultaneously other clients store files on the same storage system while hybrid storage is not a widely accepted term for this concept interoperable interfaces to the same set of data is becoming available in some object storage products virtual object storage in addition to object storage systems that own the managed files some systems provide an object abstraction on top of one or more traditional filesystem based solutions these solutions do not own the underlaying raw storage but instead actively mirror the filesystem changes and replicate them in their own object catalog alongside any metadata that can be automatically extracted from the files users can then contribute additional metadata through the virtual object storage apis a global namespace and replication capabilities both inside and across filesystems are typically supported notable examples in this category are nirvana software nirvana and its open source cousin irods most products in this category have recently extended their capabilities to support other object store solutions as well object storage systems more general purpose object storage systems came to market around 2008 lured by the incredible growth of captive storage systems within web applications like yahoo mail and the early success of cloud storage object storage systems promised the scale and capabilities of cloud storage with the ability to deploy the system within an enterprise or at an aspiring cloud storage service provider notable examples of object storage systems include emc atmos openstack object storage swift openstack swift scality scality ring caringo swarm ref cite web last nicolas first philippe sept 21 2009 title caringo filefly back to the future url http continuousdataprotection blogspot fr 2015 09 caringo filefly back to future html ref formerly castor cloudian ref name primesberger and openio ref name mellor market adoption file titan supercomputer at the oak ridge national laboratory jpg thumb the titan supercomputer at oak ridge national laboratory one of the first object storage products lustre is used in 70 of the top 100 supercomputers and 50 of the top 500 ref cite web last dilger first andreas title lustre future development url http storageconference org 2012 presentations m04 dilger pdf publisher ieee msst accessdate 27 october 2013 ref as of june 16 2013 this includes 7 of the top 10 including the current fastest system on the list china s tianhe 2 and the second fastest the titan supercomputer titan supercomputer at oak ridge national laboratory pictured on the right ref cite web title datadirect networks to build world s fastest storage system for titan the world s most powerful supercomputer url http www multivu com mnr 60497 datadirect networks titan supercomputer storage system ornl accessdate 27 october 2013 ref object storage systems had good adoption in the early 2000s as an archive platform particularly in the wake of compliance laws like sarbanes oxley after five years in the market emc s centera product claimed over 3 500 customers and 150 petabytes shipped by 2007 ref cite web title emc marks five years of emc centera innovation and market leadership url http www emc com about news press us 2007 04182007 5028 htm publisher emc accessdate 3 november 2013 date 18 april 2007 ref hitachi s hcp product also claims many petabyte scale customers ref cite web title hitachi content platform supports multiple petabytes billions of objects url http www techvalidate com portals hitachi content platform customers with more than 1pb of data stored publisher techvalidate com accessdate 19 september 2013 ref newer object storage systems have also gotten some traction particularly around very large custom applications like ebay s auction site where emc atmos is used to manage over 500 million objects a day ref cite news last robb first drew title emc world continues focus on big data cloud and flash url http www infostor com backup and recovery cloud storage emc world continues focus on big data cloud and flash html accessdate 19 september 2013 newspaper infostor date 11 may 2011 ref as of march 3 2014 emc claims to have sold over 1 5 exabytes of atmos storage ref cite web last hamilton first george title in it for the long run emc s object storage leadership url http www rethinkstorage com in it for the long run emcs object storage leadership uyezj9yllfi accessdate 15 march 2014 ref on july 1 2014 los alamos national lab chose the scality scality ring as the basis for a 500 petabyte storage environment which would be among the largest ever ref cite news last1 mellor first1 chris title los alamos national laboratory likes it puts scality s ring on it url http www theregister co uk 2014 07 01 scalitys ring goes faster accessdate 26 january 2015 publisher the register date 1 july 2014 ref captive object storage systems like facebook s haystack have scaled impressively in april 2009 haystack was managing 60 billion photos and 1 5 petabytes of storage adding 220 million photos and 25 terabytes a week ref name haystack ref cite web last nicolas first philippe sept 13 2009 title haystack chez facebook url http filestorage blogspot com 2009 09 haystack chez facebook html ref facebook more recently stated that they were adding 350 million photos a day and were storing 240 billion photos ref cite news last miller first rich title facebook builds exabyte data centers for cold storage url http www datacenterknowledge com archives 2013 01 18 facebook builds new data centers for cold storage accessdate 6 november 2013 newspaper datacenterknowledge com date 13 january 2013 ref this could equal as much as 357 petabytes ref cite web last leung first leo title how much data does x store url http techexpectations org 2014 05 17 how much data does x store publisher techexpectations org accessdate 23 may 2014 date 17 may 2014 ref cloud storage has become pervasive as many new web and mobile applications choose it as a common way to store binary data ref cite web last leung first leo title object storage already dominates our days we just didn t notice url http blog oxygencloud com 2012 01 11 object storage already dominates accessdate 27 october 2013 date january 11 2012 ref as the storage backend to many popular applications like smugmug and dropbox service dropbox aws s3 has grown to massive scale citing over 2 trillion objects stored in april 2013 ref cite news last harris first derrick title amazon s3 goes exponential now stores 2 trillion objects url http gigaom com 2013 04 18 amazon s3 goes exponential now stores 2 trillion objects accessdate 17 september 2013 newspaper gigaom date 18 april 2013 ref two months later microsoft claimed that they stored even more objects in azure at 8 5 trillion ref cite news last wilhelm first alex title microsoft azure powers 299m skype users 50m office web apps users stores 8 5t objects url http thenextweb com microsoft 2013 06 27 microsoft our cloud powers hundreds of millions accessdate 18 september 2013 newspaper thenextweb com date 27 june 2013 ref by april 2014 azure claimed over 20 trillion objects stored ref cite news last1 nelson first1 fritz title microsoft azure s 44 new enhancements 20 trillion objects url http www tomsitpro com articles microsoft azure paas iaas cloud computing 1 1841 html accessdate 3 september 2014 publisher tom s it pro date 4 april 2014 ref windows azure storage manages blobs user files tables structured storage and queues message delivery and counts them all as objects ref cite web last calder first brad title windows azure storage a highly available cloud storage service with strong consistency url http sigops org sosp sosp11 current 2011 cascais printable 11 calder pdf publisher microsoft accessdate 6 november 2013 location 23rd acm symposium on operating systems principles sosp ref market analysis international data corporation idc has begun to assess the object based storage market annually using its marketscape methodology idc describes the marketscape as a quantitative and qualitative assessment of the characteristics that assess a vendor s current and future success in the said market or market segment and provide a measure of their ascendancy to become a leader or maintain a leadership idc marketscape assessments are particularly helpful in emerging markets that are often fragmented have several players and lack clear leaders ref cite web last1 nadkarni first1 ashish title idc marketscape worldwide object based storage 2013 vendor assessment url http www idc com getdoc jsp containerid 244081 website http www idc com publisher idc accessdate 26 january 2015 ref in 2013 idc rated cleversafe scality datadirect networks amplidata and emc corporation emc as leaders ref cite news last1 mellor first1 chris title idc s explicit snapshot everyone who s anyone in object storage in 3d url http www theregister co uk 2013 11 27 idcs objectscape pretty as a picture accessdate 26 january 2015 publisher the register date 27 november 2013 ref in 2014 it rated scality cleversafe datadirect networks hitachi data systems amplidata emc corporation emc and cloudian ref cite web last nicolas first philippe sept 14 2015 title cloudian shakes the object storage market url http filestorage blogspot fr 2015 09 cloudian shakes object storage market html ref ref cite web last mellor first chris june 21 2016 title cloudian clobbers car drivers with targeted ads url http www theregister co uk 2016 06 21 cloudian could clobber car drives with targeted ads ref ref cite web last nicolas first philippe june 22 2016 title cloudian is the real s3 leader url http filestorage blogspot fr 2016 06 cloudian is real s3 leader html ref as leaders ref cite news last1 mellor first1 chris title idc who s hot and who s not in object storage in 2014 url http www theregister co uk 2015 01 06 idc shows emcs object presence shrinking accessdate 26 january 2015 publisher the register date 6 january 2015 ref ref cite web last mellor first chris nov 24 2015 title we pick storage brains has object storage endgame started url http www channelregister co uk 2015 11 24 object storage endgame ref ref cite web last nicolas first philippe oct 19 2015 title red alert for object storage vendors url http filestorage blogspot com 2015 10 red alert for object storage vendors html ref standards object based storage device standards osd version 1 in the first version of the osd standard ref cite web title incits 400 2004 url http www techstreet com cgi bin detail product id 1204555 publisher international committee for information technology standards accessdate 8 november 2013 ref objects are specified with a 64 bit partition id and a 64 bit object id partitions are created and deleted within an osd and objects are created and deleted within partitions there are no fixed sizes associated with partitions or objects they are allowed to grow subject to physical size limitations of the device or logical quota constraints on a partition an extensible set of attributes describe objects some attributes are implemented directly by the osd such as the number of bytes in an object and the modify time of an object there is a special policy tag attribute that is part of the security mechanism other attributes are uninterpreted by the osd these are set on objects by the higher level storage systems that use the osd for persistent storage for example attributes might be used to classify objects or to capture relationships among different objects stored on different osds a list command returns a list of identifiers for objects within a partition optionally filtered by matches against their attribute values a list command can also return selected attributes of the listed objects read and write commands can be combined or piggy backed with commands to get and set attributes this ability reduces the number of times a high level storage system has to cross the interface to the osd which can improve overall efficiency osd version 2 a second generation of the scsi command set object based storage devices 2 osd 2 added support for snapshots collections of objects and improved error handling ref cite web title incits 458 2011 url http www techstreet com products 1801667 publisher international committee for information technology standards accessdate 8 november 2013 date 15 march 2011 ref a snapshot computer storage snapshot is a point in time copy of all the objects in a partition into a new partition the osd can implement a space efficient copy using copy on write techniques so that the two partitions share objects that are unchanged between the snapshots or the osd might physically copy the data to the new partition the standard defines clones which are writeable and snapshots which are read only a collection is a special kind of object that contains the identifiers of other objects there are operations to add and delete from collections and there are operations to get or set attributes for all the objects in a collection collections are also used for error reporting if an object becomes damaged by the occurrence of a media defect i e a bad spot on the disk or by a software error within the osd implementation its identifier is put into a special error collection the higher level storage system that uses the osd can query this collection and take corrective action as necessary differences between key value and object stores disputed date december 2015 let s first clarify what a key value store and an object store are using the traditional block storage interface one has a series of fixed size blocks which are numbered starting at 0 data must be that exact fixed size and can be stored in a particular block which is identified by its logical block number lbn later one can retrieve that block of data by specifying its unique lbn with a key value store data is identified by a key rather than a lbn a key might be cat or olive or 42 it can be an arbitrary sequence of bytes of arbitrary length data called a value in this parlance does not need to be a fixed size and also can be an arbitrary sequence of bytes of arbitrary length one stores data by presenting the key and data value to the data store and can later retrieve the data by presenting the key you ve seen this concept before in programming languages python calls them dictionaries perl calls them hashes java and c call them maps etc several data stores also implement key value stores such as memcached redis and couchdb object stores are similar to key value stores except that the key must be a positive integer like a lbn however unlike a lbn the key can be any positive integer it does not have to map to an existing logical block number in practice it is usually limited to 64 bits more like a key value store than the traditional block storage interface data is not limited to a fixed size block but may be an arbitrary size object stores also allow one to associate a limited set of attributes with each piece of data the key value and set of attributes is referred to as an object to add more confusion sometimes key value stores are loosely referred to as object stores but technically there is a difference ref http blog gigaspaces com were flash keyvalue and object stores made for each other guest post by johann george sandisk ref see also cloud storage clustered file system object access method references reflist 2 after listing your sources please cite them using inline citations and place them after the information they cite please see http en wikipedia org wiki wikipedia refb for instructions on how to add citations external links http docs aws amazon com amazons3 latest api welcome html aws s3 api documentation https developers google com storage google cloud storage api documentation http docs openstack org developer swift openstack swift api documentation https developers seagate com display kv kinetic open storage documentation wiki seagate kinetic open storage documentation http msdn microsoft com en us library windowsazure dd179355 aspx windows azure storage api documentation https nkolayofis com a saas solution in turkey https quictransfer com a cloud storage category data management category data management software category computer file systems category computer data storage category network file systems category cloud storage'
b'about the broadcasting practice wipe disambiguation multiple issues refimprove date february 2007 original research date september 2009 globalize date september 2009 wiping also known as junking is a colloquial term for action taken by radio and television production and broadcasting companies in which old audiotape s videotape s and telerecording s kinescope s are list of lost television broadcasts erased reused or destroyed although the practice was once very common especially in the 1960s and 1970s wiping is now practiced much less frequently older video and audio formats took up much more storage space than modern digital video or audio files making their retention more costly thus increasing the incentive of discarding existing broadcast material to recover storage space for newer programmes the advent of domestic audiovisual playback technology e g videocassette and dvd has made wiping less beneficial with broadcasters and production houses realizing both the economic and cultural value of keeping archived material for both rebroadcast and potential profits through release on home video australia australian broadcasters did not gain access to videotape recording technology until the early 1960s and as a result nearly all programmes prior to that were broadcast live to air very little programming survives from the earliest years of australian tv 1956 1960 as kinescope recording to film was expensive and most of what was recorded in this way has since been lost or destroyed some early programmes have survived however for example atn 7 a sydney station prerecorded via kinescopes some of their 1950s output such as autumn affair 1958 1959 the pressure pak show 1957 1958 and leave it to the girls australian tv series leave it to the girls 1957 1958 some of these kinescopes have survived and are now held by the national film and sound archive ref cite web url http colsearch nfsa gov au nfsa search summary summary w3p adv group groupequals page 0 parentid query autumn 20affair 20media 3a 22television 22 querytype rescount 200 title nsfa autumn affair ref ref cite web url http colsearch nfsa gov au nfsa search summary summary w3p adv group groupequals page 0 parentid query leave 20it 20to 20the 20girls 20media 3a 22television 22 querytype rescount 10 title nsfa leave it to the girls ref ref cite web url http colsearch nfsa gov au nfsa search summary summary w3p adv yes group groupequals page 0 parentid query pressure 20pak 20show 20years 3a 3e 3d1957 20years 3a 3c 3d1958 20media 3a 22television 22 querytype rescount 20 title nsfa pressure pak show\xc2\xb0 ref with soap opera autumn affair surviving near intact likely one of the earliest australian series for which this is the case abc the australian broadcasting corporation abc erased much of its early output much of the videotaped abc programme material from the 1960s and early 1970s was erased as part of an economy policy instituted in the late 1970s in which old programme tapes were surrendered for bulk erasure and reuse this policy particularly targeted older programmes recorded in black and white leading to the loss of many recordings made before 1975 when australian television converted to colour the abc continued erasing older television output into the early 1980s programmes known to have been lost include most studio segments from the 1960s current affairs shows this day tonight and monday conference hundreds of episodes of the long running rural serial bellbird tv series bellbird all but a handful of episodes of the early 1970s drama series certain women television series certain women an early 1970s miniseries of dramatizations based on norman lindsay s novels and nearly all of the first 18 months of the weekly pop music show countdown australian tv series countdown network ten many episodes of popular australian commercial tv series are also lost in the 1970s network ten had an official policy to reuse tapes hence many tapes of young talent time and number 96 tv series number 96 were wiped to this day network ten still only keeps some of its programming citation needed date november 2008 other notable losses from the ten archive include hundreds of episodes of the melbourne based pop music shows commissioned and broadcast by atv 0 melbourne in the 1960s and early 1970s the go show 1964 1967 kommotion 1964 1967 uptight 1968 70 and the happening 70s series 1970 1972 nine network the nine network discarded copies of some of their programs including the popular gtv 9 series in melbourne tonight hosted by graham kennedy though it ran five nights a week from 1957 to 1970 fewer than 100 episodes are known to survive and many of the surviving episodes are edited prints made for rebroadcast across australia early episodes of hey hey it s saturday do not exist because the programme was broadcast live and did not begin videotape recordings until a number of years later brazil from 1968 1969 rede tupi tv tupi produced new episodes of the soap opera beto rockfeller by recording over previous episodes as a result few episodes survive after the closure of tv tupi in 1980 the 536 tapes at its s\xc3\xa3o paulo studios were simply left to deteriorate until they were recovered in 1985 and subsequently restored by tv cultura in 1989 only two tv tupi o os are known to have any preserved videotapes tv itacolomi s archives are now owned by the unrelated tv alterosa affiliated with sistema brasileiro de televis\xc3\xa3o sbt whereas the few remaining tapes belonging to tv piratini are stored privately in a museum in porto alegre albeit in a deteriorated state rede record also lost much footage from the 1960s due to wiping fires and deterioration most of the m\xc3\xbasica popular brasileira mpb music festivals no longer exist and the sitcom pt fam\xc3\xadlia trapo fam\xc3\xadlia trapo has only one surviving episode featuring pel\xc3\xa9 until 1997 rede record had no policy on archiving videotapes since then at least 600 videotapes that were previously believed to be lost have been recovered rede globo lost the first 35 broadcasts of both fant\xc3\xa1stico and jornal nacional in addition to many segments of their other soap operas as a result of wiping and also due to three fires that occurred in 1969 1971 and 1976 where an estimated 920 to 1500 tapes were destroyed most of rede excelsior s output was damaged in a fire in 1969 however in the late 1990s about 100 tapes of rede excelsior programming were discovered and these tapes were subsequently donated to the cinemateca brasileira in 2001 canada the canadian broadcasting corporation never practiced wiping and maintains a complete archive of all programming that was recorded ref cite web url http archives cbc ca info archives archives en 04 asp idlan 1 title cbc archives date 10 april 2013 publisher ref the ctv television network has admitted to wiping many programmes during the 1970s because of canadian content requirements the need for canadian produced programming led to more preservation of the shows they produced and even very poorly received programmes such as the infamous the trouble with tracy were saved and rerun for several years after their cancellation furthermore canadian rebroadcasts have been a source of some broadcasts that are otherwise lost in the united states and the united kingdom japan some tv stations in japan practiced wiping this example included the doraemon 1973 anime first anime adaption of doraemon philippines episodes from 1979 to 1982 of the longest running noontime show eat bulaga have been lost another example of the wiping of tv archives in the philippines was when martial law was declared soldiers raided the abs cbn abs cbn broadcast center and placed it under military control as a result abs cbn s pre martial law archives dating from 1953 to 1972 were lost mexico due to its multiple studio facilities namely its chapultepec and televisa san angel san angel studios televisa preserved most of its scripted series for broadcast years after the preserved programs had ended their original runs some televisa programs however were lost not due to wiping but due to the 1985 mexico city earthquake that destroyed part of the network s archive however smaller channels such as xeipn tv and xhdf tv did not began to preserve their recorded broadcasts until the early 1980s monterrey s multimedios televisi\xc3\xb3n keeps most of its programming though some special historical programming dealing with xhaw tdt its flagship station s history clearly shows that some footage has been either donated by viewers recorded from its original broadcast or uses footage of its programming recorded by fans and uploaded to youtube united kingdom bbc the bbc the united kingdom s first public service broadcaster had no policy on archiving until 1978 ref cite web url http cuttingsarchive org uk missing mis overv htm title cuttings archive the missing episodes overview publisher cuttings archive accessdate 2008 11 23 deadurl yes archiveurl https web archive org web 20080725012437 http www cuttingsarchive org uk missing mis overv htm archivedate july 25 2008 ref much of the corporation s output between the 1930s and 1980s has been lost rationales behind this policy include technological the bbc s television service dates back to 1936 and was originally a nearly live only medium the hours of transmission were very limited and the bulk of the programming was transmitted either live from the studio or from outside broadcasting outside broadcast ob units film was a minor contributor to the output when the first television broadcasts were made there were two competing systems in use the emi electronic system using 405 lines competed with the baird 240 line mechanical television system baird adopted an intermediate film technique where the live material was filmed using a standard film camera mounted on a large cabinet which contained a rapid processing unit and an early flying spot scanner to produce the video output for transmission the pioneer broadcasts were not however preserved on this intermediate film as the nitrate celluloid stock was scanned while still wet from the fixer bath and never washed to remove the fixer chemicals consequently the film decomposed very soon after transmission nothing is known to have survived no studio or ob programmes from 1936 to 1939 or 1946 to 1947 have survived because there was no means of preserving them historical firsts from this era the world s earliest television crime drama telecrime 1938 39 and 1946 or pinwright s progress 1946 47 the world s first regular situation comedy only remain visually as a handful of still photographs the earliest recording method for television was kinescope telerecording which involved recording the image from a special television monitor onto film with a modified film camera early examples made by this method include the first two episodes of the quatermass experiment 1953 transmitted live while simultaneously telerecorded the visual quality of the second episode s recording was considered so poor a fly entered the gap between the camera and monitor at one point that the remainder of the series was not recorded although quadruplex videotape recording technology was utilised in the uk from 1958 this system was expensive and complex recorded programmes were often erased after broadcast the vast majority of live programmes were never recorded at all videotape was not initially thought to be a permanent archivable medium its high cost and the potential reuse of the tapes led to the transfer of programme material to film via kinescope telerecording whenever sales of overseas screening rights were possible or preservation deemed worthwhile the recycling of videotapes coupled with savings made on the storage of the bulky 2 tapes ref by 1973 about 20 000 hours of recorded material was stored on videotape at the bbc weighing about 400 000 lbs in total see bbc engineering no 95 september 1973 london bbc publications p 3 ref enabled the bbc to keep costs down cultural drama and entertainment output was studio based and followed the tradition of live theatre conventional filmmaking was only gradually introduced from the 1960s the sunday night play a major event in the 1950s was performed live in the studio on thursday because telerecording was of insufficient broadcast quality another live performance followed the artists returning to perform the play again today most programmes are pre recorded and it is relatively inexpensive to preserve programming for posterity even so the bbc charter makes no mention of any obligation to retain all of them rights all television programmes have copyright and other rights issues associated with them for some genres of programmes such as drama and entertainment the actors writers and musicians involved in a production all have underlying rights in the past these rights were defended rigorously permission could even be denied by a contributor for the repeat or re use of a programme talent trade union unions were highly suspicious of the threat to new work if programmes were repeated indeed before 1955 equity trade union equity insisted that any telerecording made of a repeat performance could only be viewed privately on bbc premises and not transmitted colour television the introduction of colour television in the united kingdom from 1967 meant that broadcasters felt there was even less value in retaining monochrome recordings such tapes could not be re used for colour production so they were disposed of to create space for the new colour tapes in the archives which were quickly filling up the increased cost of colour 2 inch quadruplex videotape approximately \xc2\xa31000 per tape at today s prices meant that companies still often re used the tapes for efficiency negative attitudes to a programme s value also persisted for these reasons many programmes survive only as monochrome film recordings if at all some colour productions were telerecorded onto monochrome film for export to countries which did not yet have colour television in some cases early colour programmes only survive in this form significant wiped programmes high profile examples of programme losses include many early episodes of doctor who 97 the wednesday play most of the seminal comedy series not only but also all of the 1950s televised francis durbridge serials further the first two serials were never recorded the vast majority of the bbc s apollo 11 british television apollo 11 coverage moon landing studio coverage all but one of the 39 episodes of the first lady tv series the first lady ref cite web url http www lostshows com default aspx programme ec5863f7 6843 4552 acda 07e19396fdae title lost uk tv shows search engine author simon coward invisible technology ltd publisher ref and all 147 episodes of the soap opera united there are many gaps in many long running bbc series dixon of dock green hancock s half hour sykes out of the unknown and z cars the beatles only live appearance on top of the pops in 1966 performing the single paperback writer is believed to have been wiped clean in a clear out in the 1970s the first acting appearance of musician bob dylan in a 1963 play entitled the madhouse on castle street was erased in 1968 ref cite news url http www offthetelly co uk reviews 2005 arenadylan htm title arena dylan in the madhouse date 2005 09 28 last worthington first tj work off the telly ref there is lost material in all genres mdash as late as 1993 a large number of videotaped children s programmes from the 1970s and 1980s were irretrievably wiped by adam lee of the bbc archives on the assumption that they were of no use without consulting the bbc children s department itself ref cite news url http www offthetelly co uk oldott www offthetelly co uk index8e01 html page id 781 title of finger mice and mr men the story of watch with mother part eleven andy is waving goodbye last worthington first tj date november 2006 work off the telly ref other lost material virtually the entire runs of the corporation s pre 1970s soap operas have been lost in the 1950s and 1960s the bbc soap operas the appleyards the grove family compact soap opera compact the newcomers tv series the newcomers 199 park lane and united produced approximately 1200 episodes altogether there are no episodes of either united or 199 park lane in the archives while only one episode of the appleyards three episodes of the grove family and four episodes each of compact and the newcomers are known to exist also vulnerable to the corporation s wiping policy were programmes that only lasted for one season abigail and roger the airbase as good cooks go the 1960 adaptation of the citadel novel the citadel the 1956 adaptation of david copperfield novel david copperfield the dark island the gnomes of dulwich hurricane for richer for poorer hereward the wake the naked lady night train to surbiton outbreak of murder where do i sit and witch hunt have all been wiped with no footage surviving while four out of seven episodes of the paranormal anthology series dead of night tv series dead of night were wiped an edition of hugh and i chinese crackers starring hugh lloyd terry scott john le mesurier and david jason was located by kaleidoscope publishing in 2010 in the archives of ucla and brought to general public attention in february 2011 early episodes of the pop music chart show top of the pops were wiped or never recorded while they were being transmitted live including the only in studio appearance by the beatles clips of the beatles miming can t buy me love and you can t do that on an episode from 25 march 1964 were found online by missing episode hunter ray langstone in 2015 the last lost edition dates from 8 september 1977 there are only four complete totp episodes surviving from the 1960s while many otherwise missing episodes survive only as fragments only two episodes still exist of the sandie shaw supplement a music variety show hosted by the singer recorded in 1967 finding missing bbc programmes since the establishment of an archival policy for television in 1978 bbc archivists and others over the years have used various contacts in the uk and abroad to try to track down missing programmes for example all bbc worldwide customers broadcasters around the world who had bought programmes from the corporation were contacted to see if they still had copies which could be returned doctor who is a prime example of how this method recovered episodes that the corporation did not hold itself at the turn of the 21st century the bbc established its bbc archives archive treasure hunt archive treasure hunt a public appeal to recover lost productions which has had some successes ref name bbcth cite web url http www bbc co uk cult treasurehunt about listoffinds shtml title bbc online cult treasure hunt list of finds publisher bbc co uk date accessdate 30 july 2010 ref the bbc also has close contacts with the national film and television archive which is part of the british film institute and its missing believed wiped event which was first held in 1993 and is part of a campaign to locate lost items from british television s past there is also a network of collectors who if they find any programmes missing from the bbc archives will contact the corporation with information or sometimes even the actual footage some examples of programmes recovered for the archives are doctor who steptoe and son dad s army letter from america ref http www bbc co uk informationandarchives archivenews 2014 letters from america rediscovered html letter from america rediscovered bbc co uk 28 march 2014 ref the likely lads and play for today for many years the television pilot pilot episode of are you being served survived only in black and white appearing in this form on the 2003 dvd release of the show in 2009 a colour version was colour recovery reconstructed when it was realised that the black and white film reel had actually recorded sufficient colour information as a dot crawl pattern to allow colour recovery itv the bbc was not alone in this practice the commercial companies that formed its main rival itv tv network itv also wiped videotapes and destroyed telerecording s leaving gaps in their archive holdings the state of the archives varies greatly between the different companies granada television holds a large number of its older black and white programmes the company having an unofficial policy of retaining as much of its broadcast material albeit by telerecording as possible despite financial hardship in its early years this includes the entirety of the soap opera coronation street which is now held at the yorkshire television archive which itself possesses largely intact archives although some early colour shows from the late 1960s and the early 1970s such as the entire output of the drama castle haven the first two series of sez les and the children s variety show junior showtime are missing and believed wiped the former itv company thames television also has a significant library these cases tend to be the exception however the former nature of the itv network in which private independent companies were awarded licences to serve geographical areas for a set period of time meant that when companies lost their licences their archives were often sold to third parties and became fragmented and or risked being destroyed as ownership and copyright remained with the production companies rather than with the network the archive of networked programmes made by southern television for example is now owned by the otherwise unconnected australian media company southern star group but southern s regional output is in the hands of itv plc the few surviving tapes of associated rediffusion belong to many different organisations as the majority of associated rediffusion s tapes were recorded in monochrome and therefore deemed of no use upon the arrival of colour broadcasting as such they were disposed of by london successor thames television although in recent years there have been occasional discoveries such as a 1959 episode of double your money and the remaining missing episode of around the world with orson welles found by ray langstone in 2011 many master tapes belonging to associated television atv have since deteriorated due to bad storage and are unsuitable for broadcasting in particular the atv version of the popular soap crossroads soap opera crossroads is missing 2 850 episodes of its original 3 555 also often largely lost are quiz shows few editions exist of the 1970s version of celebrity squares with bob monkhouse or southern s children s quiz runaround game show runaround cn date december 2016 further responsibility for archive preservation was left to individual companies for example itv has no record of its live coverage of apollo 11 the 1969 moon landings after the station responsible for providing the coverage london weekend television wiped the tapes of the 96 united kingdom british inserts to the 1980s franchised united kingdom anglo united states american canada canadian children s show fraggle rock only 12 are known to exist as the library of the british producer television south tvs has been sold and subsequently split up in recent years the trend of preserving material has started to change the archives of westward television and television south west are now held in trust for the public as the south west film and television archive whilst changes in legislation mean that itv companies which lose their franchises must donate archives to the british film institute however the change of itv from a federal structure to one centralised company means that changes of regional companies in the future seems highly unlikely most material from the 1960s also only survive as telerecordings some early episodes are also believed to be damaged or in poor quality whereas much of the output of other broadcasters such as many early episodes of the avengers tv series the avengers which were shot in the electronic studio rather than on film produced by associated british corporation have been destroyed no copies of the adventures of francie and josie francie josie exist as most of scottish television s early shows were destroyed in a fire in late 1969 although some sources state 1973 the adventures of francie josie was made from 1961 to 1965 by stv recovery of missing programmes since the bbc library was first audited in 1978 missing programmes or extracts on either film or tape are often found in unexpected places an appeal to broadcasters in other countries who had shown missing programmes notably australia new zealand canada and africa n nations such as nigeria produced missing episodes from the archives of those television companies episodes have also been returned to broadcasters by private film collectors who had acquired 16mm film copies from various sources cn date december 2016 two series 1 episodes of the avengers tv series the avengers an associated british corporation production which were thought to be missing were recovered from the ucla film television archive in the united states it emerged in september 2010 that more than 60 recordings of bbc and itv drama productions originally sent for broadcast in the united states by the public broadcasting service pbs station wnet which serves new york city and new jersey had been found at the library of congress ref vanessa thorpe https www theguardian com tv and radio 2010 sep 12 lost tapes classic british television lost tapes of classic british television found in the us the observer 15 september 2010 ref the bbc sitcom steptoe and son is completely intact although approximately half of the colour episodes only exist in monochrome this was after copies of episodes thought to be lost were recovered in the late 1990s from early non broadcast standard video recordings made for writers ray galton and alan simpson scriptwriter alan simpson by bbc technicians a few audio recordings of til death us do part have been recovered as well as an extract of the pilot and two episodes from series three copies of several compilations from the british 1960s comedy at last the 1948 show held by many to be a forerunner of monty python s flying circus were discovered in the archives of the swedish broadcaster sveriges television svt to whom the producers associated rediffusion rediffusion london had sold them upon the companies loss of its broadcasting licence the master tapes along with much of rediffusion s programming were wiped or disposed of by london successor thames television their recovery enabled the reconstruction of otherwise missing original editions of the programme meaning most of the series exists in visual form off air home audio recordings of various television programmes have also been recovered at least preserving the soundtracks to otherwise missing shows and some of these particularly from doctor who have been released on cd by the bbc following restoration and the addition of narration to describe purely visual elements tele snaps a commercial service of off screen shots of programmes often purchased by actor s and television director s to keep a record of their work in the days before videocassette recorder s have also been recovered for many lost programmes preservation of the current archive advances in technology have resulted in old programmes being transferred to new digital media where they can be restored or if they are damaged or otherwise cannot be restored kept from decaying further in the united kingdom the archives of both the bbc and those available of itv along with other channels are being switched from cumbersome quadruplex videotape 2 inch quadruplex videotape to digital format this is an extensive and expensive process and one that will take many years to complete live broadcasts in britain are still not necessarily kept and wiping of material has not ceased according to writer and broadcaster matthew sweet writer matthew sweet there are big gaps in the record of children s television of the nineties ref matthew sweet http www telegraph co uk culture tvandradio 10492487 searching for televisions missing gems doctor who woody allen ridley scott and dennis potter html searching for television s missing gems doctor who woody allen ridley scott and dennis potter telegraph co uk 4 december 2013 ref united states in the united states the major broadcast networks also engaged in the practice of wiping recordings until the late 1970s many episodes were erased especially daytime and late night programming such as daytime soap opera s and game show s the daytime shows almost all of them having been taped were erased because it was believed at the time that nobody wanted to see them after their first broadcast the success of cable television networks devoted to reruns of these genres proved that this was not the case as the large number of episodes that were required for a daily program made even a short run game show an ideal candidate for broadcast syndication syndication by this time however the damage had already been done preservation by institutions such as museums some museums and other cultural institution s such as the paley center for media have taken steps to discover and preserve see e g paley center for media archives old recordings previously thought to have been wiped or discarded lost or misfiled hosting sequences hosting sequences on videotape nearly always featuring celebrities were sometimes made for telecasts of family films notably for the first nine telecasts of mgm s the wizard of oz 1939 film the wizard of oz it is not known if those made for oz survived since they have not been seen since 1967 one hosting sequence from that era that does survive is the one eddie albert made for the 1965 cbs telecast of the nutcracker starring edward villella patricia mcbride and melissa hayden dancer melissa hayden it has even been included on the dvd release of the program ref cite web url http www wbshop com product nutcracker the 1965 tv sp 1000179869 do from search title nutcracker the 1965 tv sp mod work www wbshop com ref ernie kovacs many of ernie kovacs s videotaped network programs were also wiped during different times as comedian writer and performer kovacs had programs on all four major television networks american broadcasting company abc cbs dumont television network dumont and nbc after kovacs s death the networks wiped many programs kovacs s widow edie adams obtained as many programs and episodes as she could find donating them to ucla s special collections soap operas though most soap operas transitioned from live broadcast to videotaping their shows during the 1960s it was still common practice to wipe and reuse the tapes this practice was due to the high cost of videotape at the time while soap operas began routinely saving their episodes between 1976 and 1979 several soaps have saved recordings of most or all their episodes days of our lives has recordings of all its episodes its first two episodes exist on their original master tapes and were aired by soapnet in 2005 the young and the restless dark shadows and ryan s hope saved most of their episodes despite the fact that they debuted during the 1960s and 1970s before retaining tapes became common practice episodes of the doctors soap opera the doctors began to be saved no later than december 4 1967 this is where reruns of the series began when picked up by retro television network in september 2014 episodes of other soaps broadcast during the 1950s to 1970s do exist in different forms and have been showcased in various places online procter and gamble started saving their shows around 1979 very few pre 1979 color episodes of the procter and gamble sponsored shows survive with most extant episodes preserved as monochrome kinescopes exceptions include two episodes of the guiding light 1973 and 1977 which have been released on dvd as the world turns and the edge of night aired live until 1975 the year the edge of night moved to american broadcasting company abc and as the world turns expanded from a 30 minute broadcast to one hour both shows began taping episodes in preparation for the move of the edge of night to abc the edge of night nowiki s nowiki abc debut is believed to have survived overall the number of surviving monochrome episodes recorded on kinescope outnumber color episodes for these programs agnes nixon initially produced her series one life to live and all my children through her own production company creative horizons inc and kept a complete archive of monochrome kinescopes until abc bought the shows from her in 1975 when the network wanted to expand all my children from 30 minutes to a full hour in the late 1970s nixon agreed on the condition that the network would begin saving the episodes abc complied and full hour broadcasts began on april 25 1977 however a fire destroyed the vast majority of the early 1970s kinescopes leaving only a few random episodes from each season virtually all episodes of general hospital from its premiere in april 1963 through august 1970 are archived at ucla the ucla film television archive holds a large number of daytime television airings that were spared from the wiping practice also archived there are handfuls of episodes of each soap opera that was on the air from 1971 and 1973 including a world apart tv series a world apart where the heart is 1969 tv series where the heart is and return to peyton place tv series return to peyton place dumont programs it is believed that virtually the entire archive of the dumont television network covering its whole history from 1946 to 1956 was disposed of during the 1970s by a successor broadcaster presumably metromedia the holder of dumont s assets who dumped all of the kinescopes videotapes into the east river to make room for other tapes believed to be abc s at a new york city warehouse ref name loc cite web last adams first edie authorlink edie adams title television video preservation study los angeles public hearing work national film preservation board publisher library of congress date march 1996 url http www loc gov film hrng96la html accessdate 2008 05 09 ref further a large number of dumont s kinescopes were destroyed in about 1958 for their silver content of the over 20 000 shows carried by dumont in its ten year existence list of surviving dumont television network broadcasts approximately 350 or so episodes of dumont programming are known to exist today less than two percent of its total output the remainder were either never recorded e g nfl on dumont or were dumped in the earlier purges the tonight show see also the tonight show starring johnny carson almost all of the tonight show with jack paar and the first ten years hosted by his successor johnny carson were taped over by the network with carson s blessing under the assumption that the broadcasts were of no real value ref http www washingtonpost com entertainment tv carson on tcm shows why johnny was the king 2013 07 03 3c25d1ce e264 11e2 aef3 339619eab080 story html carson on tcm shows why johnny was the king the washington post retrieved july 9 2013 ref this is part of the reason why carson s late 1960s shows had poorer picture quality citation needed date november 2010 compared to his competitor dick cavett on american broadcasting company abc nbc was using the tonight show tapes repeatedly another reason for their poorer quality is that many of the 1960s tonight show episodes only survived in the kinescope format cavett s abc shows were also taped over by his network in favor of other shows produced at abc s studios in new york early sporting events see also list of world series broadcasters list of super bowl broadcasters nfl on cbs nfl on nbc many early sporting events such as the world series and the first two super bowl s were also lost though a nearly intact recording of the first super bowl was found in 2005 national football league super bowl i was aired by both cbs and nbc the only super bowl to be aired by two networks but neither network felt the need to preserve the game long term cbs saved the telecast for a few months and reran it as filler programming at least once before wiping it a color videotape containing the first second and fourth quarters of the telecast from wyou the cbs affiliate for scranton pennsylvania was found in 2005 and is in the process of being restored ref fybush scott 2011 02 07 http www fybush com nerw 2011 110207 nerw html will new york outlaw pirate radio northeast radio watch retrieved 2011 02 07 ref on january 15 2016 the nfl network reaired the first super bowl featuring audio from nbc radio and most of the tv network broadcast and newly discovered nfl films footage of the game super bowl ii was aired exclusively by cbs and was believed to have been erased but it was later found that the entire telecast fully exists and rests in the vaults of nfl films ref name foo cite web url http www marketwatch com story the hunt for tvs lost baseball treasures 2010 10 27 pagenumber 2 title the hunt for tv s lost baseball treasures author david b wilkerson date october 27 2010 work publisher wall street journal marketwatch accessdate november 26 2012 ref though the telecast of super bowl iii exists in full color only half of the super bowl iv broadcast does the rest was preserved by canadian television in black and white the first three quarters of super bowl v broadcast by nbc los angeles affiliate knbc exist but the fourth quarter is missing though the mike curtis american football mike curtis interception and jim o brien american football jim o brien game winning field goal were recovered via news highlights from cbc television cbc super bowl vi also exists in its entirety it was not until super bowl vii that a continuous archive was established ref name foo similarly all of the telecasts of the nfl championship game s prior to the super bowl are believed to have been lost with all surviving footage of those games coming from separately produced film the status of most regular season and playoff games from the early years of television up to the immediate years following the 1970 afl nfl merger are also unknown among the footage that has survived include at least some of nbc s coverage from the 1972 afc divisional playoff game between the 1972 pittsburgh steelers season pittsburgh steelers and 1972 oakland raiders season oakland raiders that featured the immaculate reception as well as the inaugural telecast of monday night football between the 1970 cleveland browns season cleveland browns and the 1970 new york jets season new york jets though several monday night football games in the ensuing seasons were lost a 1974 nfl season 1974 game that featured john lennon being interviewed by howard cosell in the booth only survived due to a home video recording of the game the game itself was wiped by abc cbs kept coverage of a 1978 eagles giants rivalry matchup between the 1978 new york giants season new york giants and 1978 philadelphia eagles season philadelphia eagles that would feature the now infamous miracle at the meadowlands although the existence of many 1978 games on cbs by private collectors shows that the networks by that point started keeping recordings of regular season games there are rare exceptions of cbs games from 1977 nfl season 1977 back but by 1978 nfl season 1978 the library of most teams is almost fully complete nbc is another story the nfl had its own filmmakers nfl films filming the game with its own equipment thus preserving the telecasts on tape was not seen as a priority by the networks when another source was available though the sportscasters play by play comments as a result were lost world series telecasts all telecasts of world series games starting in 1975 world series 1975 1975 cincinnati reds season reds 1975 boston red sox season red sox are known to exist in full ref name surviving world series telecasts cite web url http www dbsforums com vbulletin showthread php t 78232 title www dbsforums com publisher ref follows is the known footage of world series telecasts prior to 1975 1952 world series 1952 1952 new york yankees season yankees 1952 brooklyn dodgers season dodgers games 6 7 are intact 1955 world series 1955 1955 new york yankees season yankees 1955 brooklyn dodgers season dodgers only the first half of game 5 is known to exist 1956 world series 1956 1956 new york yankees season yankees 1956 brooklyn dodgers season dodgers only the last three innings of game 2 are known to exist game 3 is intact minus the second and third inning game 5 don larsen s perfect game is intact minus the first inning and was aired on january 1 2009 during the mlb network s first broadcast day 1957 world series 1957 1957 new york yankees season yankees 1957 milwaukee braves season braves game 1 is intact by way of a print from the united states american forces network armed forces radio and television service ref https www youtube com watch v 72eo0pika4o ref game 3 is intact minus a snip of tony kubek s second home run in the top 7th inning games 6 most of the first six innings and 7 reportedly exist as well 1960 world series 1960 1960 new york yankees season yankees 1960 pittsburgh pirates season pirates game 7 with bill mazeroski s series clinching walk off home run was found intact on kinescope in december 2009 in the wine cellar of pirates part owner bing crosby who had the game recorded at his own expense mlb network aired it in december 2010 ref cite news url http www nytimes com 2010 09 24 sports baseball 24crosby html r 1 src mv title in bing crosby s wine cellar vintage baseball first richard last sandomir authorlink richard sandomir publisher the new york times date 2010 09 23 accessdate 2010 09 25 ref 1961 world series 1961 1961 new york yankees season yankees 1961 cincinnati reds season reds half hour segments of games 3 the first two innings 4 the 4th and 5th innings and 5 open and top of the 1st inning are known to exist 1963 world series 1963 1963 new york yankees season yankees 1963 los angeles dodgers season dodgers game 3 is intact 1965 world series 1965 1965 minnesota twins season twins 1965 los angeles dodgers season dodgers all seven games were preserved by the cbc television cbc on kinescope 1968 world series 1968 1968 detroit tigers season tigers 1968 st louis cardinals season cardinals all seven games were preserved by the cbc television cbc on kinescope it is likely the 1965 and 1968 series were preserved by the cbc due to the twins and tigers proximity to canada the country would not get its own mlb team until the montreal expos began play in 1969 1969 world series 1969 1969 baltimore orioles season orioles 1969 new york mets season mets games 1 2 were preserved by the cbc television cbc on kinescope while games 3 5 exist on their original color videotape from truck feeds 1970 world series 1970 1970 baltimore orioles season orioles 1970 cincinnati reds season reds games 1 4 were preserved by the cbc television cbc on kinescope while game 5 exists on its original color videotape from the truck feed 1971 world series 1971 1971 baltimore orioles season orioles 1971 pittsburgh pirates season pirates games 1 2 and 6 7 are intact while games 3 5 only partially exist and game 4 the first world series night game is near complete 1972 world series 1972 1972 oakland athletics season a s 1972 cincinnati reds season reds game 4 is intact along with nearly all of game 5 and a fair chunk of game 2 fragments exist for games 1 3 and 6 while game 7 is missing 1973 world series 1973 1972 oakland athletics season a s 1972 new york mets season mets game 1 is intact game 2 is missing the last inning and a half including both mike andrews plays game 3 is complete minus the last inning game 4 is intact from the pregame show to the top of the 4th inning and game 5 only has the last two innings about 30 minutes of excerpts from game 6 survive while game 7 cuts off with one out at the top of the 9th inning while the last inning and a half of game 2 is missing from the major league baseball major league baseball on nbc nbc copy the andrews plays totaling about 60 seconds of coverage survived because after the world series nbc put together a 20 minute presentation tape narrated by curt gowdy to submit to the peabody awards in order to get consideration for an award for their coverage by the committee the tape includes the two andrews plays with gowdy and tony kubek s calls and analysis of them the presentation tape is held by the peabody vault creating a case where reconstructing a game in an incomplete format would require going to two different outlets 1974 world series 1974 1974 oakland athletics season a s 1974 los angeles dodgers season dodgers games 1 4 are complete game 5 is near intact but the bottom of the 9th inning is missing and only exists on the original radio broadcast league championship series telecasts for the league championship series telecasts spanning from 1969 to 1975 only game 2 of the 1972 american league championship series 1972 oakland athletics season oakland 1972 detroit tigers season detroit is known to exist ref name surviving world series telecasts however the copy on the trade circuit is missing the bert campaneris lerrin lagrow brawl there are some instances where the only brief glimpse of telecast footage of an early lcs game can be seen in a surviving newscast from that night clips of game 5 of the 1972 national league championship series featuring the then cincinnati reds announcer al michaels calling the two crucial plays of the game the game tying home run by johnny bench and wild pitch bringing home george foster baseball george foster with the series clinching run are available the last out of the 1973 national league championship series as described by jim simpson sportscaster jim simpson was played on that night s nbc nightly news but other than that the entire game is gone on the day the 1969 new york mets season new york mets and 1969 baltimore orioles season baltimore orioles wrapped up their respective league championship series in 1969 a feature story on the cbs evening news showed telecast clips of the 1969 american league championship series alcs game albeit with no original sound this is all that likely remains of anything from that third game of the 1969 baltimore orioles season orioles 1969 minnesota twins season twins series while all telecasts of world series games starting with 1972 world series 1975 are accounted for and exist the lcs is still a spotty situation through the late 1970s 1976 american league championship series 1976 alcs game 5 is intact from the american broadcasting company abc vault 1976 national league championship series 1976 nlcs game 3 is intact albeit an off air recording taped in the katu portland market apparently this copy is the only extant version because the abc vault copy has no sound 1977 national league championship series 1977 nlcs game 3 is intact from the 1977 philadelphia phillies season philadelphia phillies local kyw tv nbc affiliate a copy is held by major league baseball who also appears to have game 4 as well 1977 american league championship series 1977 alcs game 5 is intact with both the wpix tv wpix and major league baseball on nbc nbc versions existing through off air recordings clips of these games may be seen in highlight shows such as yankeeography it is believed that incomplete tapes of the alcs exist it is possible these games are not shown in part because the audio quality is poor a common method of getting around such deficiencies would be to overlay a radio telecast or narration by a player or commentator where gaps exist 1978 american league championship series 1978 alcs all four games major league baseball on abc abc version are intact via off air recordings 1978 national league championship series 1978 nlcs game 4 is intact again from off air recordings nba finals see also list of nba finals broadcasters 1963 nba finals 1963 boston celtics celtics los angeles lakers lakers game 6 is intact 1969 nba finals 1969 celtics lakers only the entire 4th quarter of game 7 exists 1970 nba finals 1970 lakers new york knicks knicks game 7 is intact 1971 nba finals 1971 milwaukee bucks bucks washington bullets bullets only nearly all of the second half of game 4 exists 1972 nba finals 1972 knicks lakers game 5 is intact with the exception of the last 3 4 minutes of the game 1973 nba finals 1973 knicks lakers games 1 4 are missing while the entire game 5 wasn t found until 2013 and some of which was shown in the 30 for 30 documentary when the garden was eden 1974 nba finals 1974 bucks celtics only the 4th quarter and 2 overtime of game 6 and the 4th quarter of game 7 exist 1975 nba finals bullets warriors game 1 2 3 intact 1976 suns celtics games 5 6 are intact wiped programs early live shows many programs in the early days of television were live broadcasts that are lost because they were not recorded most prime time programs that were preserved used the kinescope kinescope recording process which involved filming the live broadcast from a television screen using a motion picture camera videotape for recording programs was not perfected until the late 1950s and was not widely used until the late 1960s this was also a common practice for broadcasting live tv shows to the west coast of the united states west coast as performers often performed a show back to back but never back to back to back daytime programs however were generally not kinescoped for preservation although many were temporarily kinescoped for later broadcast episodes recorded in this way were often junked many local station and network newscasts were prone to wiping news some early news programs such as camel news caravan are largely lost moving images of walter cronkite reading the news in his studio every night for six years are gone with the exception of his coverage of the cuban missile crisis in 1962 and the jfk assassination in 1963 studio shots of peter jennings inside his american broadcasting company abc studio during his first year there 1965 are also gone vanderbilt university has kept all evening national news telecasts since monday august 5 1968 as of 1997 cbs had saved 1 000 000 videotapes of news reports broadcasts stock footage and outtakes according to a report that year from the national film preservation board the same report added television stations still erase and recycle their video cassettes referring to local news programs ref cite web url http www loc gov film tvstudy html title television video preservation study volume 1 report author librarian of congress date october 1997 work publisher library of congress accessdate 26 november 2012 ref many local stations contract with outside companies for archiving news coverage situation comedy little of the first sitcom mary kay and johnny the mary kay and johnny show remains today it was initially live and not recorded but later in its run kinescopes were made for rebroadcasting fragments of episodes and one complete installment are known to exist game shows game show s more than any other genre were prone to wiping many games between 1941 and 1980 had insignificantly short runs some measured in a span of weeks or even days that the networks felt it unnecessary to keep them for posterity whereas recycling the tapes would be more profitable and less of an effort than attempting to sell the series in reruns in an era before cable television mark goodson bill todman productions and to a lesser extent barry enright productions barry enright productions and chuck barris productions and to an even lesser extent heatter quigley productions had the foresight to preserve many of their games for later reruns for years these shows dominated the game show network gsn line up and now make up a major portion of buzzr tv s lineup most other game shows from that era were not so fortunate all of the bob stewart television bob stewart except pyramid game show pyramid heatter quigley productions heatter quigley except for pdq game show pdq which aired in syndication as well as many episodes of hollywood squares stefan hatos monty hall productions hatos hall except for a large portion of let s make a deal and pre 1980 merv griffin productions have been destroyed with the exception of a few rare pilots and cast aside episodes the few remaining episodes have therefore become collectors items and an active trading circuit exists among collectors nbc and abc continued the wiping process well into the 1970s while abc ceased in early 1978 nbc continued to wipe some shows into 1980 leaving much of their daytime game show content lost forever cbs abandoned the wiping process by september 1972 largely as a result of their collaboration with goodson todman as a result even the network s shorter lived games such as spin off game show spin off still exist in their entirety incidentally all three networks ended their wiping practices during the time fred silverman led their respective networks while it remained in business dumont wished to keep its programs as intact as possible however the network ceased to exist in 1956 and its archive was destroyed in the 1970s the corporate successor to dumont fox broadcasting company fox not only has never aired any daytime programming other than its fox kids block from 1990 to 2001 but debuted in 1986 well beyond the wiping era award shows several award shows from the 1950s and 1960s such as the academy awards and the emmy awards only survive in kinescope format from 29th academy awards 1957 to 37th academy awards 1965 the academy awards were taped in black and white but only survive in kinescope format for overseas distribution especially for the european tv audiences which used another system 576i 625 lines as opposed to 480i 525 lines as the tapes used for late broadcasting were reused all of the taped broadcasts of the oscars from 38th academy awards 1966 the first to be broadcast in color remain intact see also portal television doctor who missing episodes doctor who missing episodes british television apollo 11 coverage missing believed wiped kinescope lost film list of lost television broadcasts film preservation list of surviving dumont television network broadcasts footnotes reflist 2 references cite book last fiddy first dick authorlink coauthors title missing believed wiped searching for the lost treasures of british television year 2002 publisher british film institute location london isbn 978 0 85170 866 9 external links http www bbc co uk cult treasurehunt full details of the bbc s treasure hunt http www lostshows com default aspx lost shows uk search engine kaleidoscope website http www missing episodes com british tv missing episodes index http www wipednews com wiped news com a news and features website devoted to missing tv film radio http www marketwatch com story story print guid e880d4c8 e078 11df b7d4 002128049ad6 the hunt for tv s lost baseball treasures http www tvobscurities com lost lostormissing television obscurities television lost or missing major league baseball on national television national basketball association on television national football league on television and radio category television terminology category data management category television preservation'
b'other uses atomicity disambiguation in database system s atomicity or atomicness citation needed date february 2016 from greek language greek atomos undividable is one of the acid database transaction transaction properties an atomic transaction is an indivisible and irreducible series of database operations such that either all occur or nothing occurs ref cite web accessdate 2011 03 23 location http www webopedia com publisher webopedia title atomic operation quote an operation during which a processor can simultaneously read a location and write it in the same bus operation this prevents any other processor or i o device from writing or reading memory until the operation is complete url http www webopedia com term a atomic operation html ref a guarantee of atomicity prevents updates to the database occurring only partially which can cause greater problems than rejecting the whole series outright as a consequence the transaction cannot be observed to be in progress by another database client at one moment in time it has not yet happened and at the next it has already occurred in whole or nothing happened if the transaction was cancelled in progress an example of an atomic transaction is a monetary transfer from bank account a to account b it consists of two operations withdrawing the money from account a and saving it to account b performing these operations in an atomic transaction ensures that the database remains in a data consistency consistent state that is money is not lost nor created if either of those two operations fail ref cite web url http archive oreilly com pub a onjava 2001 11 07 atomic html title atomic file transactions part 1 o reilly media last amsterdam first jonathan website archive oreilly com access date 2016 02 28 ref orthogonality atomicity does not behave completely orthogonal computing orthogonally with regard to the other acid properties of the transactions for example isolation database systems isolation relies on atomicity to roll back changes in the event of isolation failures such as deadlock consistency database systems consistency also relies on rollback in the event of a consistency violation by an illegal transaction finally atomicity itself relies on durability database systems durability to ensure the atomicity of transactions even in the face of external failures as a result of this failure to detect errors and roll back the enclosing transaction may cause failures of isolation and consistency implementation typically systems implement atomicity by providing some mechanism to indicate which transactions have started and which finished or by keeping a copy of the data before any changes occurred read copy update several filesystems have developed methods for avoiding the need to keep multiple copies of data using journaling see journaling file system databases usually implement this using some form of logging journaling to track changes the system synchronizes the logs often the metadata as necessary once the actual changes have successfully taken place afterwards crash recovery simply ignores incomplete entries although implementations vary depending on factors such as concurrency issues the principle of atomicity i e complete success or complete failure remain ultimately any application level implementation relies on operating system operating system functionality at the file system level posix compliant systems provide system call s such as code open 2 code and code flock 2 code that allow applications to atomically open or lock a file at the process level posix threads provide adequate synchronization primitives the hardware level requires linearizability atomic operations such as test and set fetch and add compare and swap or load link store conditional together with memory barrier s portable operating systems cannot simply block interrupts to implement synchronization since hardware that lacks actual concurrent execution such as hyper threading or multi processing is now extremely rare citation needed date december 2016 in nosql concept nosql data store s with eventual consistency the atomicity is also weaker specified than in relational database systems and exists only in row s i e column family column families ref cite web accessdate 2011 03 23 author olivier mallassi date 2010 06 09 location http blog octo com en publisher octo talks title let s play with cassandra\xe2\x80\xa6 part 1 3 quote atomicity is also weaker than what we are used to in the relational world cassandra guarantees atomicity within a code columnfamily code so for all the columns of a row url http blog octo com en nosql lets play with cassandra part 13 ref see also atomic operation transaction processing long running transaction read copy update references reflist defaultsort atomicity database systems category data management category transaction processing'
b'about data in computer science data computing other uses pp move indef file data types en svg thumb right 200px some of the different types of data data ipac en \xcb\x88 d e\xc9\xaa t \xc9\x99 respell day t\xc9\x99 ipac en \xcb\x88 d \xc3\xa6 t \xc9\x99 respell da t\xc9\x99 or ipac en \xcb\x88 d \xc9\x91\xcb\x90 t \xc9\x99 respell dah t\xc9\x99 ref the pronunciation ipac en \xcb\x88 d e\xc9\xaa t \xc9\x99 respell day t\xc9\x99 is widespread throughout most varieties of english the pronunciation ipac en \xcb\x88 d \xc3\xa6 t \xc9\x99 respell da t\xc9\x99 is chiefly hiberno english irish and american english north american the pronunciation ipac en \xcb\x88 d \xc9\x91\xcb\x90 t \xc9\x99 respell dah t\xc9\x99 is chiefly australian english australian new zealand english new zealand and south african english south african each pronunciation may be realized differently depending on the dialect language of the speaker ref is a set mathematics set of values of qualitative data qualitative or quantitative data quantitative variables an example of qualitative data would be an anthropologist s handwritten notes about her interviews with people of an indigenous tribe pieces of data are individual pieces of information while the concept of data is commonly associated with scientific research data is collected by a huge range of organizations and institutions including businesses e g sales data revenue profits stock price governments e g crime rate s unemployment rate s literacy rates and non governmental organizations e g censuses of the number of homelessness homeless people by non profit organizations data is measurement measured data reporting collected and reported and data analysis analyzed whereupon it can be data visualization visualized using graphs images or other analysis tools data as a general concept refers to the fact that some existing information or knowledge is knowledge representation and reasoning represented or code d in some form suitable for better usage or data processing processing raw data unprocessed data is a collection of number s or character computing characters before it has been cleaned and corrected by researchers raw data needs to be corrected to remove outlier s or obvious instrument or data entry errors e g a thermometer reading from an outdoor arctic location recording a tropical temperature data processing commonly occurs by stages and the processed data from one stage may be considered the raw data of the next stage field work field data is raw data that is collected in an uncontrolled in situ environment experimental data is data that is generated within the context of a scientific investigation by observation and recording data has been described as the new petroleum oil of the digital economy ref https www wired com insights 2014 07 data new oil digital economy data is the new oil of the digital economy ref ref https spotlessdata com blog data new oil data is the new oil ref etymology and terminology the first english use of the word data is from the 1640s using the word data to mean transmittable and storable computer information was first done in 1946 the expression data processing was first used in 1954 ref name eol http www etymonline com index php term data ref the data word latin word data is the plural of datum thing given neuter past participle of dare to give ref name eol data may be used as a plural noun in this sense with some writers in the 2010s using datum in the singular and data for plural in the 2010s though in non specialist everyday writing data is most commonly used in the singular as a mass noun like information sand or rain ref cite web last hickey first walt url http fivethirtyeight com datalab elitist superfluous or popular we polled americans on the oxford comma title elitist superfluous or popular we polled americans on the oxford comma publisher fivethirtyeight date 2014 06 17 accessdate 2015 05 04 ref meaning data information knowledge and wisdom are closely related concepts but each has its own role in relation to the other and each term has its own meaning data is collected and analyzed data only becomes information suitable for making decisions once it has been analyzed in some fashion ref cite web title joint publication 2 0 joint intelligence url http www dtic mil doctrine new pubs jp2 0 pdf work defense technical information center dtic publisher department of defense accessdate february 22 2013 pages gl 11 date 22 june 2007 ref knowledge is derived from extensive amounts of experience dealing with information on a subject for example the height of mount everest is generally considered data the height can be recorded precisely with an altimeter and entered into a database this data may be included in a book along with other data on mount everest to describe the mountain in a manner useful for those who wish to make a decision about the best method to climb it using an understanding based on experience climbing mountains to advise persons on the way to reach mount everest s peak may be seen as knowledge some complement the series data information and knowledge with wisdom which would mean the status of a person in possession of a certain knowledge who also knows under which circumstances is good to use it data is the least abstract concept information the next least and knowledge the most abstract ref cite web author akash mitra year 2011 title classifying data for successful modeling url http www dwbiconcepts com data warehousing 12 data modelling 101 classifying data for successful modeling html ref data becomes information by interpretation e g the height of mount everest is generally considered data a book on mount everest geological characteristics may be considered information and a climber s guidebook containing practical information on the best way to reach mount everest s peak may be considered knowledge information bears a diversity of meanings that ranges from everyday usage to technical use generally speaking the concept of information is closely related to notions of constraint communication control data form instruction knowledge meaning mental stimulus pattern perception and representation given by nupur seth beynon davies uses the concept of a sign to differentiate between data and information data is a series of symbols while information occurs when the symbols are used to refer to something ref cite book author p beynon davies year 2002 title information systems an introduction to informatics in organisations publisher palgrave macmillan location basingstoke uk isbn 0 333 96390 3 ref ref cite book author p beynon davies year 2009 title business information systems publisher palgrave location basingstoke uk isbn 978 0 230 20368 6 ref before the development of computing devices and machines only people could collect data and impose patterns on it since the development of computing devices and machines these devices can also collect data in the 2010s computers are widely used in many fields to collect data and sort or process it in disciplines ranging from marketing analysis of social services usage by citizens to scientific research these patterns in data are seen as information which can be used to enhance knowledge these patterns may be interpreted as truth though truth can be a subjective concept and may be authorized as aesthetic and ethical criteria in some disciplines or cultures events that leave behind perceivable physical or virtual remains can be traced back through data marks are no longer considered data once the link between the mark and observation is broken ref cite book author sharon daniel title the database an aesthetics of dignity ref mechanical computing devices are classified according to the means by which they represent data an analog computer represents a datum as a voltage distance position or other physical quantity a computer digital computer represents a piece of data as a sequence of symbols drawn from a fixed alphabet the most common digital computers use a binary alphabet that is an alphabet of two characters typically denoted 0 and 1 more familiar representations such as numbers or letters are then constructed from the binary alphabet some special forms of data are distinguished a computer program is a collection of data which can be interpreted as instructions most computer languages make a distinction between programs and the other data on which programs operate but in some languages notably lisp programming language lisp and similar languages programs are essentially indistinguishable from other data it is also useful to distinguish metadata that is a description of other data a similar yet earlier term for metadata is ancillary data the prototypical example of metadata is the library catalog which is a description of the contents of books in other fields though data is also increasingly used in other fields it has been suggested that the highly interpretive nature of them might be at odds with the ethos of data as given peter checkland introduced the term capta from the latin capered to take to distinguish between an immense number of possible data and a sub set of them to which attention is oriented ref cite book author p checkland and s holwell title information systems and information systems making sense of the field year 1998 publisher john wiley sons location chichester west sussex isbn 0 471 95820 4 pages 86 89 ref johanna drucker has argued that since the humanities affirm knowledge production as situated partial and constitutive using data may introduce assumptions that are counterproductive for example that phenomena are discrete or are observer independent ref cite web author johanna drucker year 2011 title humanities approaches to graphical display url http www digitalhumanities org dhq vol 5 1 000091 000091 html ref the term capta which emphasizes the act of observation as constitutive is offered as an alternative to data for visual representations in the humanities see also div col 5 biological data data acquisition data analysis data cable dark data data domain data element data farming data governance data integrity data maintenance data management data mining data modeling data visualization computer data processing data publication information privacy data protection data remanence data set data warehouse database datasheet environmental data rescue fieldwork metadata open data scientific data archiving statistics computer memory data structure raw data secondary data div col end references foldoc reflist external links wiktionary http purl org nxg note singular data data is a singular noun a detailed assessment statistics category computer data category data category data management'
b'technical date december 2012 in computing a data definition specification dds is a guideline to ensure comprehensive and consistent data definition it represents the attributes required to quantify data definition a comprehensive data definition specification encompasses enterprise data the hierarchy of data management prescribed guidance enforcement and criteria to determine compliance overview a data definition specification may be developed for any organization or specialized field improving the quality of its products through consistency and transparency it eliminates redundancy since all contributing areas are referencing the same specification and provides standardization making it easier and more efficient to create modify verify analyze and share information across the enterprise ref gouin deborah corcoran charmane k 2008 developing the msu enterprise data definition standard michigan state university web site http eis msu edu uploads university 20eis 20working 20committee 20meetings 05 20august 202008 enterprise 20data 20definition 20standard 20presentation082708 pdf ref to understand how a data definition specification works in an enterprise we must look at the elements of a dds writing data definitions defining business terms or rules in the context of a particular environment provides structure for an organization s data architecture in developing these definitions the words used must be traceable to clearly defined data a data definition specification may be used in the following activities to provide consistency and clarity between departments supporting the activity ref name datagovernance thomas gwen 2008 writing enterprise quality data definitions tips for creating terms and definitions data governance institute web site http www datagovernance com dgi wp writing enterprise quality data definitions pdf ref business intelligence business process modeling business rules management data analysis and data modeling modeling information architecture metadata modeling report generation criteria a data definition specification requires data definitions to be atomic singular describing only one concept commonly used and ambiguous terms should be defined ref name datagovernance while a term refers to one concept several words may be used in a term file a concept identifiable with one word file extension a concept identifiable with more than one word traceable mapped to a specific data element in business a term may be traced to an entity for example a customer or an attribute such as a customer s name a term may be a value in a data set such as gender or designate the data set itself traceability indicates relationships in the data hierarchy consistent used in a standard syntax programming languages syntax if used in a specific context the context is noted accurate precise correct and unambiguous stating what the term is and is not ref international organization for standardization jtc1 sc32 committee 2004 iso 11179 4 http standards iso org ittf publiclyavailablestandards index html ref clear readily understood by the reader complete with the term its description and contextual references concise to avoid circular references applications enterprise data a data definition specification was produced by the open mobile alliance to document charging data ref cite web url http www openmobilealliance org technical release program docs charging data v1 0 20110201 a oma dds charging data v1 0 20110201 a pdf date 1 february 2011 website open mobile alliance pages 6 35 format pdf title charging data archiveurl https web archive org web 20131006172727 http technical openmobilealliance org technical release program docs charging data v1 0 20110201 a oma dds charging data v1 0 20110201 a pdf archivedate 6 october 2013 accessdate 12 march 2014 ref the document the centralized catalog of data elements defined for interfaces specifies the mapping of these data elements to protocol fields in the interfaces created for the exchange of financial data market data definition language mddl is an xml specification designed quote to enable the interchange of information necessary to account to analyze and to trade financial instruments of the world s markets it defines an xml based interchange format and common data dictionary on the fields needed to describe 1 financial instruments 2 corporate events affecting value and tradability and 3 market related economic and industrial indicators the principal function of mddl is to allow entities to exchange market data by standardizing formats and definitions mddl provides a common format for market data so that it can be efficiently passed from one processing system to another and provides a common understanding of market data content by standardizing terminology and by normalizing the relationships of various data elements to one another nbsp from the user perspective the goal of mddl is to enable users to integrate data from multiple sources by standardizing both the input feeds used for data warehousing i e define what s being provided by vendors and the output methods by which client applications request the data i e ensure compatibility on how to get data in and out of applications ref cite web title market data definition language mddl date december 26 2002 website cover pages url http xml coverpages org mddl html archiveurl https web archive org web 20131214075132 http xml coverpages org mddl html archivedate december 14 2013 accessdate march 12 2014 ref clinical submissions the clinical data interchange standards consortium a global multidisciplinary non profit organization has established standards to support the acquisition exchange submission and archiving of clinical research data and metadata cdisc standards are vendor neutral platform independent and freely available from the cdisc website the case report tabulation data definition specification define xml draft version 2 0 the oldest data definition specification is part of the evolution from the 1999 fda electronic submission esub guidance and electronic common technical document ectd documents specifying that a document describing the content and structure of included data be included in a submission define xml was developed to automate the review process by generating a machine readable data definition document define xml has standardized submissions to the food and drug administration reducing review times from over two years to several months ref cite web title define xml year 2012 website clinical data interchange standards consortium url http www cdisc org define xml archiveurl https web archive org web 20131004232219 http www cdisc org define xml archivedate october 4 2013 accessdate march 12 2014 ref archival data a data definition specification is the foundation of metadata for scientific data archiving the metadata encoding and transmission standard mets uses one principle of a dds consistent use of key terms to catalog digital objects for global use the mets schema is a flexible mechanism for encoding descriptive administrative and structural metadata for a digital library object and expressing complex links between metadata and can provide a useful standard for the exchange of digital library objects between repositories ref metadata encoding transmission standard mets web site from the library of congress standards http www loc gov standards mets ref a similar effort is underway to preserve complex data associated with video game archiving preserving virtual worlds attempted to address archival format deficiencies citing the lack of suitable documentation for interactive fiction and games at the bit level specifically the absence of representation information needed to map raw bits into higher level data constructs ref meta data schema development 2008 http pvw illinois edu pvw page id 25 preserving virtual worlds website ref preserving virtual worlds 2 is a research project expanding on initial efforts in this field ref preserving virtual worlds 2 researching best practices for videogame preservation 2012 http pvw illinois edu pvw2 ref see also clinical data interchange standards consortium cdisc data governance iso iec 11179 metadata encoding and transmission standard mets oasis organization oasis references reflist category data management'
b'file diagram of lambda architecture generic png thumb flow of data through the processing and serving layers of a generic lambda architecture lambda architecture is a data processing data processing architecture designed to handle massive quantities of data by taking advantage of both batch processing batch and stream processing stream processing methods this approach to architecture attempts to balance latency engineering latency throughput and fault tolerance by using batch processing to provide comprehensive and accurate views of batch data while simultaneously using real time stream processing to provide views of online data the two view outputs may be joined before presentation the rise of lambda architecture is correlated with the growth of big data real time analytics and the drive to mitigate the latencies of map reduce ref cite web last1 schuster first1 werner title nathan marz on storm immutability in the lambda architecture clojure url http www infoq com interviews marz lambda architecture website www infoq com interview with nathan marz 6 april 2014 ref lambda architecture depends on a data model with an append only immutable data source that serves as a system of record ref name bijnens slide bijnens nathan http lambda architecture net architecture 2013 12 11 a real time architecture using hadoop and storm devoxx a real time architecture using hadoop and storm 11 december 2013 ref rp 32 it is intended for ingesting and processing timestamped events that are appended to existing events rather than overwriting them state is determined from the natural time based ordering of the data overview lambda architecture describes a system consisting of three layers batch processing speed or real time processing and a serving layer for responding to queries ref name big data marz nathan warren james big data principles and best practices of scalable realtime data systems manning publications 2013 ref rp 13 the processing layers ingest from an immutable master copy of the entire data set batch layer the batch layer precomputes results using a distributed processing system that can handle very large quantities of data the batch layer aims at perfect accuracy by being able to process all available data when generating views this means it can fix any errors by recomputing based on the complete data set then updating existing views output is typically stored in a read only database with updates completely replacing existing precomputed views ref name big data rp 18 hadoop apache hadoop is the de facto standard batch processing system used in most high throughput architectures ref kar saroj http cloudtimes org 2014 05 28 hadoop sector will have annual growth of 58 for 2013 2020 hadoop sector will have annual growth of 58 for 2013 2020 28 may 2014 cloud times ref speed layer file diagram of lambda architecture named components png thumb diagram showing the flow of data through the processing and serving layers of lambda architecture example named components are shown the speed layer processes data streams in real time and without the requirements of fix ups or completeness this layer sacrifices throughput as it aims to minimize latency by providing real time views into the most recent data essentially the speed layer is responsible for filling the gap caused by the batch layer s lag in providing views based on the most recent data this layer s views may not be as accurate or complete as the ones eventually produced by the batch layer but they are available almost immediately after data is received and can be replaced when the batch layer s views for the same data become available ref name big data rp 203 stream processing technologies typically used in this layer include storm event processor apache storm sqlstream sqlstream and apache spark output is typically stored on fast nosql databases ref name kinley kinley james http jameskinley tumblr com post 37398560534 the lambda architecture principles for architecting the lambda architecture principles for architecting realtime big data systems retrieved 26 august 2014 ref ref ferrera bertran pere http www datasalt com 2014 01 lambda architecture a state of the art lambda architecture a state of the art 17 january 2014 datasalt ref serving layer file diagram of lambda architecture druid data store png thumb diagram showing a lambda architecture with a druid data store output from the batch and speed layers are stored in the serving layer which responds to ad hoc queries by returning precomputed views or building views from the processed data examples of technologies used in the serving layer include druid open source data store druid which provides a single cluster to handle output from both layers ref name metamarkets lambda yang fangjin and merlino gian https speakerdeck com druidio real time analytics with open source technologies 1 real time analytics with open source technologies 30 july 2014 ref dedicated stores used in the serving layer include apache cassandra or apache hbase for speed layer output and https github com nathanmarz elephantdb elephant db or cloudera impala for batch layer output ref name bijnens slide rp 45 ref name kinley optimizations to optimize the data set and improve query efficiency various rollup and aggregation techniques are executed on raw data ref name metamarkets lambda rp 23 while estimation techniques are employed to further reduce computation costs ref ray nelson https metamarkets com 2013 histograms the art of approximating distributions histograms and quantiles at scale 12 september 2013 metamarkets ref and while expensive full recomputation is required for fault tolerance incremental computation algorithms may be selectively added to increase efficiency and techniques such as partial computation and resource usage optimizations can effectively help lower latency ref name big data rp 93 287 293 lambda architecture in use metamarkets which provides analytics for companies in the programmatic advertising space employs a version of the lambda architecture that uses druid open source data store druid for storing and serving both the streamed and batch processed data ref name metamarkets lambda rp 42 for running analytics on its advertising data warehouse yahoo has taken a similar approach also using storm event processor apache storm hadoop apache hadoop and druid open source data store druid ref name yahoo lambda rao supreeth gupta sunil http www slideshare net hadoop summit interactive analytics in human time next slideshow 1 interactive analytics in human time 17 june 2014 ref rp 9 16 the netflix suro project has separate processing paths for data but does not strictly follow lambda architecture since the paths may be intended to serve different purposes and not necessarily to provide the same type of views ref name netflix bae jae hyeon yuan danny tonse sudhir http techblog netflix com 2013 12 announcing suro backbone of netflixs html announcing suro backbone of netflix s data pipeline netflix 9 december 2013 ref nevertheless the overall idea is to make selected real time event data available to queries with very low latency while the entire data set is also processed via a batch pipeline the latter is intended for applications that are less sensitive to latency and require a map reduce type of processing criticism criticism of lambda architecture has focused on its inherent complexity and its limiting influence the batch and streaming sides each require a different code base that must be maintained and kept in sync so that processed data produces the same result from both paths yet attempting to abstract the code bases into a single framework puts many of the specialized tools in the batch and real time ecosystems out of reach ref cite web last1 kreps first1 jay title questioning the lambda architecture url http radar oreilly com 2014 07 questioning the lambda architecture html website radar oreilly com publisher oreilly accessdate 15 august 2014 ref kreps ref in a technical discussion over the merits of employing a pure streaming approach it was noted that using a flexible streaming framework such as apache samza could provide some of the same benefits as batch processing without the latency ref https news ycombinator com item id 7976785 hacker news retrieved 20 august 2014 ref such a streaming framework could allow for collecting and processing arbitrarily large windows of data accommodate blocking and handle state references see http en wikipedia org wiki wikipedia footnotes on how to create references using ref ref tags these references will then appear here automatically reflist external links http lambda architecture net repository of information on lambda of architecture categories category articles created via the article wizard category data processing category big data category data management category free software projects category software architecture'
b'infobox software name rasdaman logo image rasdaman logo png frame center x250px alt rasdaman logo used with permission of copyright holder rasdaman logo used with permission of copyright holder developer rasdaman gmbh latest release version rasdaman v9 2 1 latest release date release date 2016 02 17 status active operating system most unix like operating systems programming language c ref cite web url https www openhub net p rasdaman title the rasdaman open source project on open hub work open hub publisher black duck software accessdate 2016 08 01 ref genre array dbms license gnu general public license gpl v3 gnu lesser general public license lgpl v3 or proprietary software proprietary ref cite web url http rasdaman org wiki license title rasdaman license publisher rasdaman org date accessdate 2016 08 01 ref website url http rasdaman org url http rasdaman com rasdaman raster data manager is an array dbms that is a database management system which adds capabilities for storage and retrieval of massive multi dimensional array data structure arrays such as sensor image and statistics data a frequently used synonym to arrays is raster data such as in 2 d raster graphics this actually has motivated the name rasdaman however rasdaman has no limitation in the number of dimensions it can serve for example 1 d measurement data 2 d satellite imagery 3 d x y t image time series and x y z exploration data 4 d ocean and climate data and even beyond spatio temporal dimensions history in 1989 peter baumann computer scientist peter baumann started a research on database support for images then at fraunhofer society fraunhofer computer graphics institute following an in depth investigation on raster data formalizations in imaging in particular the afatl image algebra he established a database model for multi dimensional arrays including a data model and declarative query language ref baumann p http www informatik uni trier de ley db journals vldb vldb3 html baumann94 on the management of multidimensional discrete data vldb journal 4 3 1994 special issue on spatial database systems pp 401 444 ref at technical university munich tu munich in the eu funded basic research project rasdaman a first prototype was established on top of the o2 object oriented database object oriented dbms and tested in earth and life science applications ref name cordis europa eu http cordis europa eu result rcn 20754 en html ref over further eu funded projects this system was completed and extended to support relational dbmss a dedicated research spin off rasdaman gmbh ref name rasdaman com http www rasdaman com ref was established to give commercial support in addition to the research which subsequently has been continued at jacobs university bremen jacobs university ref name rasdaman com archive http www rasdaman com news archive php ref since then both entities collaborate on the further development and use of the rasdaman technology concepts data model based on an array algebra ref baumann p http www informatik uni trier de ley db conf ngits ngits99 html baumann99 a database array algebra for spatio temporal data and beyond proc ngits 99 lncs 1649 springer 1999 pp 76 93 ref specifically developed for database purposes rasdaman adds a new attribute type array to the relational model as this array definition is parametrized it constitutes a second order logic second order construct or template c template this fact is reflected by the second order functionals in the algebra and query language for historical reasons table database tables are called collections as initial design emphasized an embedding into the object oriented database standard odmg anticipating a full integration with sql rasdaman collections represent a binary relation with the first attribute being an object identifier and the second being the array this allows to establish foreign key foreign key references between arrays and regular tuple relational tuples raster query language the rasdaman query language rasql embeds itself into standard sql and its set oriented processing on the new attribute type multi dimensional arrays a set of extra operations is provided which all are based on a minimal set of algebraically defined core operators an array constructor which establishes a new array and fills it with values and an array condenser which similarly to sql aggregates derives scalar summary information from an array the query language is declarative and hence optimizable and safe in evaluation that is every query is guaranteed to return after a finite number of processing steps the rasql query guide ref n n http rasdaman org browser manuals and examples manuals doc guides ql guide pdf rasdaman query language guide ref provides details here some examples may illustrate its use from all 4 d x y z t climate simulation data cubes a cutout which contains all in x a y extract between 100 and 200 all available along z and a slice at position 42 effectively resulting in a 3 d x y z cube source lang sql select c 100 200 42 from climatesimulations as c source in all landsat satellite images suppress all non green areas source lang sql select img img green 130 from landsatarchive as img source note this is a very naive phrasing of vegetation search in practice one would use the ndvi formula use null values for cloud masking and several more techniques all mri images where in some region defined by the bit masks intensity exceeds a threshold of 250 source lang sql select img from mri as img masks as m where some cells img 250 and m source a 2 d x y slice from all 4 d climate simulation data cubes each one encoded in png format source lang sql select png c 100 42 from climatesimulations as c source architecture storage management image sample tiling of an array for storage in rasdaman png frame x110px alt sample rasdaman tiling sample array tiling in rasdaman raster objects are maintained in a standard relational database based on the partitioning of an raster object into tiles ref furtado p baumann p http www informatik uni trier de ley db conf icde icde99 html furtadob99 storage of multidimensional arrays based on arbitrary tiling proc icde 99 march 23 26 1999 sydney australia pp 328 336 ref aside from a regular subdivision any user or system generated partitioning is possible as tiles form the unit of disk access it is of critical importance that the tiling pattern is adjusted to the query access patterns several tiling strategies assist in establishing a well performing tiling a geo index is employed to quickly determine the tiles affected by a query optionally tiles are compressed using one of various choices including lossless and lossy wavelet algorithms independently from that query results can be comressed for transfer to the client both tiling strategy and compression comprise database tuning parameters tiles and tile index are stored as binary large object blobs in a relational database which also holds the data dictionary needed by rasdaman s dynamic type system adaptors are available for several relational systems among them open source postgresql postgresql for arrays larger than disk space hierarchical storage management hsm support has been developed query processing queries are parsed optimised and executed in the rasdaman server the parser receives the query string and generates the operation tree further it applies algebraic optimisation rules to the query tree where applicable of the 150 algebraic rewriting rules 110 are actually optimising while the other 40 serve to transform the query into canonical form parsing and optimization together take less than a millisecond on a laptop execution follows a tile streaming paradigm whenever possible array tiles addressed by a query are fetched sequentially and each tile is discarded after processing this leads to an architecture scalable to data volumes exceeding server main memory by orders of magnitude query execution is parallelised first rasdaman offers inter query parallelism a dispatcher schedules requests into a pool of server processes on a per transaction basis intra query parallelism transparently distributes query subtrees across available cores gpus or cloud nodes client apis the primary interface to rasdaman is the query language embeddings into c and java apis allow invocation of queries as well as client side convenience functions for array handling arrays per se are delivered in the main memory format of the client language and processor architecture ready for further processing data format codecs allow to retrieve arrays in common raster formats such as comma separated values csv portable network graphics png and netcdf netcdf a web design toolkit raswct is provided which allows to establish web query frontends easily including graphical widgets for parametrized query handling such as sliders for thresholds in queries geo web services a java programming language java servlet petascope running as a rasdaman client offers web service interfaces specifically for geo data access processing and filtering the following open geospatial consortium ogc standards are supported web map service wms web coverage service wcs web coverage processing service wcps and web processing service wps for web coverage service wcs and web coverage processing service wcps rasdaman is the reference implementation status and license model today rasdaman is a fully fledged implementation offering select insert update delete array query functionality it is being used in both research and commercial installations in a collaboration of the original code owner rasdaman gmbh ref name rasdaman com and jacobs university a code split was performed in 2008 2009 resulting in rasdaman community ref http www rasdaman org ref an open source branch and rasdaman enterprise the commercial branch since then rasdaman community is being maintained by jacobs university whereas rasdaman enterprise remains proprietary to rasdaman gmbh the difference between both variants mainly consists of performance boosters such as specific optimization techniques intended to support particularly large databases user numbers and complex queries details are available on the rasdaman community website ref http rasdaman eecs jacobs university de trac rasdaman wiki license rasdaman license model ref the rasdaman community license releases the server in gpl and all client parts in lgpl thereby allowing to use the system in any kind of license environment impact and use being the first array dbms shipped first prototype available in 1996 rasdaman has shaped this recent database research domain concepts of the data and query model declarativeness sometimes choice of operators find themselves in more recent approaches in 2008 the open geospatial consortium released the web coverage processing service standard which defines a raster query language based on the concept of a coverage data coverage operator semantics ref baumann p http www springerlink com openurl asp genre article id doi 10 1007 s10707 009 0087 2 the ogc web coverage processing service wcps standard geoinformatica 14 4 2010 pp 447 479 ref is influenced by the rasdaman array algebra earthlook ref http standards rasdaman org ref is a showcase for open geospatial consortium ogc coverage data coverage standards in action offering 1 d through 4 d use cases of raster data access and ad hoc processing earthlook is built on rasdaman a sample large project in which rasdaman is being used for large scale services in all earth sciences is earthserver ref http www earthserver eu ref six services with a volume of at least 100 terabyte each are being set up for integrated data metadata retrieval and distributed query processing references reflist defaultsort rasdaman category free database management systems category proprietary database management systems category nosql category data management category query languages'
b'pp move indef small yes use dmy dates date october 2016 file schlagwortkatalog jpg thumb 200px in the 2010s metadata typically refers to digital forms however even traditional card catalogues from the 1960s and 1970s are an example of metadata as the cards contain information about the books in the library author title subject etc metadata is data information that provides information about other data ref http www merriam webster com dictionary metadata ref three distinct types of metadata exist descriptive metadata structural metadata and administrative metadata ref name metadata basics outline cite web url http marciazeng slis kent edu metadatabasics types htm title metadata types and functions publisher niso date 2004 accessdate 5 october 2016 author zeng marcia ref descriptive metadata describes a resource for purposes such as discovery and identification it can include elements such as title abstract author and keywords structural metadata is metadata about containers of metadata and indicates how compound objects are put together for example how pages are ordered to form chapters administrative metadata provides information to help manage a resource such as when and how it was created file type and other technical information and who can access it ref name understanding metadata 2 cite book url http www niso org publications press understandingmetadata pdf title understanding metadata publisher niso press author national information standards organization niso year 2001 isbn 1 880124 62 9 page 1 ref history metadata was traditionally used in the library catalog card catalogs of library libraries until the 1980s when libraries converted their catalog data to digital databases in the 2000s as digital formats are becoming the prevalent way of storing data and information metadata is also used to describe digital data using metadata standards there are different metadata standards for each different discipline e g museum collections digital audio file s website s etc describing the content media contents and context computing context of data or computer file data files increases its usefulness for example a web page may include metadata specifying what software language the page is written in e g html what tools were used to create it what subjects the page is about and where to find more information about the subject this metadata can automatically improve the reader s experience and make it easier for users to find the web page online ref name practices in using metadata cite web url http www library illinois edu dcc bestpractices chapter 11 structuralmetadata html title best practices for structural metadata publisher university of illinois date 15 december 2010 accessdate 17 june 2016 ref a cd may include metadata providing information about the musicians singers and songwriters whose work appears on the disc a principal purpose of metadata is to help users find relevant information and discover resources metadata also helps to organize electronic resources provide digital identification and support the archiving and preservation of resources metadata assists users in resource discovery by allowing resources to be found by relevant criteria identifying resources bringing similar resources together distinguishing dissimilar resources and giving location information ref name understanding metadata metadata of telecommunication activities including internet traffic is very widely collected by various national governmental organizations this data is used for the purposes of traffic analysis and can be used for mass surveillance ref https www schneier com essays archives 2014 03 metadata surveillanc html ref in many countries the metadata relating to emails telephone calls web pages video traffic ip connections and cell phone locations are routinely stored by government organizations ref name nsa watching http www washingtonsblog com 2014 03 nsa recorded every single call one country country america html ref definition metadata means data about data although the meta prefix from the greek language greek preposition and prefix \xce\xbc\xce\xb5\xcf\x84\xce\xac means after or beyond it is used to mean about in epistemology metadata is defined as the data providing information about one or more aspects of the data it is used to summarize basic information about data which can make tracking and working with specific data easier ref cite web title a guardian guide to your metadata website theguardian com publisher guardian news and media limited date 12 june 2013 url https www theguardian com technology interactive 2013 jun 12 what is metadata nsa surveillance meta 0000000 ref some examples include means of creation of the data purpose of the data time and date of creation creator or author of the data location on a computer network where the data was created technical standard standards used file size for example a digital image may include metadata that describes how large the picture is the color depth the image resolution when the image was created the shutter speed and other data ref cite web url http www adeoimaging com title adeo imaging tiff metadata accessdate 2013 05 20 ref a text document s metadata may contain information about how long the document is who the author is when the document was written and a short summary of the document metadata within web pages can also contain descriptions of page content as well as key words linked to the content ref name rouse m 2014 cite web last rouse first margaret title metadata work whatis publisher techtarget date july 2014 url http whatis techtarget com definition metadata ref these links are often called metatags which were used as the primary factor in determining order for a web search until the late 1990s ref name rouse m 2014 the reliance of metatags in web searches was decreased in the late 1990s because of keyword stuffing ref name rouse m 2014 metatags were being largely misused to trick search engines into thinking some websites had more relevance in the search than they really did ref name rouse m 2014 metadata can be stored and managed in a database often called a metadata registry or metadata repository ref h\xc3\xbcner k otto b \xc3\xb6sterle h collaborative management of business metadata in international journal of information management 2011 ref however without context and a point of reference it might be impossible to identify metadata just by looking at it ref cite web url http www bls gov ore pdf st000010 pdf title metadata standards and metadata registries an overview format pdf accessdate 2011 12 23 ref for example by itself a database containing several numbers all 13 digits long could be the results of calculations or a list of numbers to plug into an equation without any other context the numbers themselves can be perceived as the data but if given the context that this database is a log of a book collection those 13 digit numbers may now be identified as isbn s information that refers to the book but is not itself the information within the book the term metadata was coined in 1968 by philip bagley in his book extension of programming language concepts where it is clear that he uses the term in the iso 11179 traditional sense which is structural metadata i e data about the containers of data rather than the alternate sense content about individual instances of data content or metacontent the type of data usually found in library catalogues ref name bagley cite journal author philip bagley title extension of programming language concepts date november 1968 url http www dtic mil dtic tr fulltext u2 680815 pdf publisher university city science center location philadelphia ref ref the notion of metadata introduced by bagley cite journal last solntseff first n 1 last2 yezerski first2 a year 1974 title a survey of extensible programming languages series annual review in automatic programming publisher elsevier science ltd volume 7 pages 267 307 doi 10 1016 0066 4138 74 90001 9 ref since then the fields of information management information science information technology librarianship and gis have widely adopted the term in these fields the word metadata is defined as data about data ref name niso cite book last niso authorlink niso title understanding metadata publisher niso press url http www niso org publications press understandingmetadata pdf isbn 1 880124 62 9 accessdate 5 january 2010 ref page needed date november 2016 while this is the generally accepted definition various disciplines have adopted their own more specific explanation and uses of the term types while the metadata application is manifold covering a large variety of fields there are specialized and well accepted models to specify types of metadata francis bretherton bretherton singley 1994 distinguish between two distinct classes structural control metadata and guide metadata ref cite conference first1 f p last1 bretherton author1 link francis bretherton first2 p t last2 singley title metadata a user s view proceedings of the international conference on very large data bases vldb pages 1091 1094 publisher year 1994 ref structural metadata describes the structure of database objects such as tables columns keys and indexes guide metadata helps humans find specific items and are usually expressed as a set of keywords in a natural language according to ralph kimball metadata can be divided into 2 similar categories technical metadata and business metadata technical metadata corresponds to internal metadata and business metadata corresponds to external metadata kimball adds a third category process metadata on the other hand niso distinguishes among three types of metadata descriptive structural and administrative ref name niso descriptive metadata is typically used for discovery and identification as information to search and locate an object such as title author subjects keywords publisher structural metadata describes how the components of an object are organized an example of structural metadata would be how pages are ordered to form chapters of a book finally administrative metadata gives information to help manage the source administrative metadata refers to the technical information including file type or when and how the file was created two sub types of administrative metadata are rights management metadata and preservation metadata rights management metadata explains intellectual property rights while preservation metadata contains information to preserve and save a resource ref name understanding metadata cite book last national information standards organization title understanding metadata year 2004 publisher niso press location bethesda md isbn 1 880124 62 9 url http www niso org publications press understandingmetadata pdf author2 rebecca guenther author3 jaqueline radebaugh accessdate 2 april 2014 ref page needed date november 2016 structures metadata metacontent or more correctly the vocabularies used to assemble metadata metacontent statements is typically structured according to a standardized concept using a well defined metadata scheme including metadata standards and metadata modeling metadata models tools such as controlled vocabulary controlled vocabularies taxonomy general taxonomies thesaurus information retrieval thesauri data dictionary data dictionaries and metadata registry metadata registries can be used to apply further standardization to the metadata structural metadata commonality is also of paramount importance in data model development and in database design syntax metadata metacontent syntax refers to the rules created to structure the fields or elements of metadata metacontent ref cite web last cathro first warwick authorlink title metadata an overview year 1997 url http www nla gov au nla staffpaper cathro3 html accessdate 6 january 2010 ref a single metadata scheme may be expressed in a number of different markup or programming languages each of which requires a different syntax for example dublin core may be expressed in plain text html xml and resource description framework rdf ref cite web last dcmi authorlink dublin core metadata initiative title semantic recommendations date 5 october 2009 url http dublincore org specifications accessdate 6 january 2010 ref a common example of guide metacontent is the bibliographic classification the subject the list of dewey decimal classes dewey decimal class number there is always an implied statement in any classification of some object to classify an object as for example dewey class number 514 topology i e books having the number 514 on their spine the implied statement is nowiki book subject heading 514 nowiki this is a subject predicate object triple or more importantly a class attribute value triple the first two elements of the triple class attribute are pieces of some structural metadata having a defined semantic the third element is a value preferably from some controlled vocabulary some reference master data the combination of the metadata and master data elements results in a statement which is a metacontent statement i e metacontent metadata master data all of these elements can be thought of as vocabulary both metadata and master data are vocabularies which can be assembled into metacontent statements there are many sources of these vocabularies both meta and master data uml edifact xsd dewey udc loc skos iso 25964 pantone linnaean binomial nomenclature etc using controlled vocabularies for the components of metacontent statements whether for indexing or finding is endorsed by iso 25964 if both the indexer and the searcher are guided to choose the same term for the same concept then relevant documents will be retrieved ref https www iso org obp ui iso std iso 25964 1 ed 1 v1 en ref this is particularly relevant when considering search engines of the internet such as google the process indexes pages then matches text strings using its complex algorithm there is no intelligence or inferencing occurring just the illusion thereof hierarchical linear and planar schemata metadata schemata can be hierarchical in nature where relationships exist between metadata elements and elements are nested so that parent child relationships exist between the elements an example of a hierarchical metadata schema is the learning object metadata ieee lom schema in which metadata elements may belong to a parent metadata element metadata schemata can also be one dimensional or linear where each element is completely discrete from other elements and classified according to one dimension only an example of a linear metadata schema is the dublin core metadata initiative dublin core schema which is one dimensional metadata schemata are often two dimensional or planar where each element is completely discrete from other elements but classified according to two orthogonal dimensions ref cite web title types of metadata publisher university of melbourne date 15 august 2006 url http www infodiv unimelb edu au metadata add info html accessdate 6 january 2010 archiveurl https web archive org web 20091024112353 http www infodiv unimelb edu au metadata add info html archivedate 2009 10 24 ref hypermapping in all cases where the metadata schemata exceed the planar depiction some type of hypermap ping is required to enable display and view of metadata according to chosen aspect and to serve special views hypermapping frequently applies to layering of geographical and geological information overlays ref cite web url http www isprs org proceedings xxxii part4 kuebler51 pdf title the design and development of a geologic hypermap prototype first1 stefanie last1 k\xc3\xbcbler first2 wolfdietrich last2 skala first3 agn\xc3\xa8s last3 voisard ref granularity the degree to which the data or metadata is structured is referred to as its data granularity granularity granularity refers to how much detail is provided metadata with a high granularity allows for deeper more detailed and more structured information and enables greater levels of technical manipulation a lower level of granularity means that metadata can be created for considerably lower costs but will not provide as detailed information the major impact of granularity is not only on creation and capture but moreover on maintenance costs as soon as the metadata structures become outdated so too is the access to the referred data hence granularity must take into account the effort to create the metadata as well as the effort to maintain it standards international standards apply to metadata much work is being accomplished in the national and international standards communities especially ansi american national standards institute and international organization for standardization iso international organization for standardization to reach consensus on standardizing metadata and registries the core metadata registry standard is international organization for standardization iso international electrotechnical commission iec 11179 metadata registries mdr the framework for the standard is described in iso iec 11179 1 2004 ref cite web url http www iso org iso iso catalogue catalogue tc catalogue detail htm csnumber 39438 title iso iec 11179 1 2004 information technology metadata registries mdr part 1 framework publisher iso org date 2009 03 18 accessdate 2011 12 23 ref a new edition of part 1 is in its final stage for publication in 2015 or early 2016 it has been revised to align with the current edition of part 3 iso iec 11179 3 2013 ref cite web url http standards iso org ittf publiclyavailablestandards c050340 iso iec 11179 3 2013 zip title iso iec 11179 3 2013 information technology metadata registries part 3 registry metamodel and basic attributes publisher iso org date 2014 ref which extends the mdr to support registration of concept systems see iso iec 11179 this standard specifies a schema for recording both the meaning and technical structure of the data for unambiguous usage by humans and computers iso iec 11179 standard refers to metadata as information objects about data or data about data in iso iec 11179 part 3 the information objects are data about data elements value domains and other reusable semantic and representational information objects that describe the meaning and technical details of a data item this standard also prescribes the details for a metadata registry and for registering and administering the information objects within a metadata registry iso iec 11179 part 3 also has provisions for describing compound structures that are derivations of other data elements for example through calculations collections of one or more data elements or other forms of derived data while this standard describes itself originally as a data element registry its purpose is to support describing and registering metadata content independently of any particular application lending the descriptions to being discovered and reused by humans or computers in developing new applications databases or for analysis of data collected in accordance with the registered metadata content this standard has become the general basis for other kinds of metadata registries reusing and extending the registration and administration portion of the standard the geospatial community has a tradition of specialized geospatial metadata standards particularly building on traditions of map and image libraries and catalogues formal metadata is usually essential for geospatial data as common text processing approaches are not applicable the dublin core metadata terms are a set of vocabulary terms which can be used to describe resources for the purposes of discovery the original set of 15 classic ref cite web url http dublincore org specifications title dcmi specifications publisher dublincore org date 2009 12 14 accessdate 2013 08 17 ref metadata terms known as the dublin core metadata element set ref cite web url http dublincore org documents dces title dublin core metadata element set version 1 1 publisher dublincore org accessdate 2013 08 17 ref are endorsed in the following standards documents ietf rfc 5013 ref cite web url http www ietf org rfc rfc5013 txt title the dublin core metadata element set author j kunze t baker work ietf org year 2007 accessdate 17 august 2013 ref iso standard 15836 2009 ref cite web url http www iso org iso iso catalogue catalogue ics catalogue detail ics htm csnumber 52142 title iso 15836 2009 information and documentation the dublin core metadata element set publisher iso org date 2009 02 18 accessdate 2013 08 17 ref niso standard z39 85 ref cite web url http www niso org kst reports standards step 2 gid none project key 9b7bffcd2daeca6198b4ee5a848f9beec2f600e5 title niso standards national information standards organization publisher niso org date 2007 05 22 accessdate 2013 08 17 ref although not a standard microformat also mentioned in the section metadata metadata on the internet metadata on the internet below is a web based approach to semantic markup which seeks to re use existing html xhtml tags to convey metadata microformat follows xhtml and html standards but is not a standard in itself one advocate of microformats tantek \xc3\xa7elik characterized a problem with alternative approaches cquote here s a new language we want you to learn and now you need to output these additional files on your server it s a hassle microformats lower the barrier to entry ref name wharton000 cite web title what s the next big thing on the web it may be a small simple thing microformats work knowledge wharton publisher wharton school of the university of pennsylvania date 2005 07 27 url http knowledge wharton upenn edu index cfm fa printarticle id 1247 ref use photographs metadata may be written into a digital photo file that will identify who owns it copyright and contact information what brand or model of camera created the file along with exposure information shutter speed f stop etc and descriptive information such as keywords about the photo making the file or image searchable on a computer and or the internet some metadata is created by the camera and some is input by the photographer and or software after downloading to a computer most digital cameras write metadata about model number shutter speed etc and some enable you to edit it ref cite web publisher gurucamera com title how to copyright your photos with metadata url https gurucamera com copyright photos metadata work guru camera ref this functionality has been available on most nikon dslrs since the nikon d3 on most new canon cameras since the canon eos 7d and on most pentax dslrs since the pentax k 3 metadata can be used to make organizing in post production easier with the use of key wording filters can be used to analyze a specific set of photographs and create selections on criteria like rating or capture time photographic metadata standards are governed by organizations that develop the following standards they include but are not limited to iptc information interchange model iim international press telecommunications council international press telecommunications council iptc core schema for xmp extensible metadata platform xmp extensible metadata platform an iso standard exchangeable image file format exif exchangeable image file format maintained by cipa camera imaging products association and published by jeita japan electronics and information technology industries association dublin core dublin core metadata initiative dcmi plus picture licensing universal system http www loc gov standards vracore schemas html vra core visual resource association ref cite web title vra core support pages url http core vraweb org website visual resource association foundation publisher visual resource association foundation accessdate 27 february 2016 ref telecommunications information on the times origins and destinations of phone calls electronic messages instant messages and other modes of telecommunication as opposed to message content is another form of metadata bulk collection of this call detail record metadata by intelligence agencies has proven controversial after disclosures by edward snowden intelligence agencies such as the nsa are keeping online metadata of millions of internet user for up to a year regardless of whether or not they are persons of interest to the agency video metadata is particularly useful in video where information about its contents such as transcripts of conversations and text descriptions of its scenes is not directly understandable by a computer but where efficient search of the content is desirable there are two sources in which video metadata is derived 1 operational gathered metadata that is information about the content produced such as the type of equipment software date and location 2 human authored metadata to improve search engine visibility discoverability audience engagement and providing advertising opportunities to video publishers ref cite web last webcase first weblog authorlink title examining video file metadata year 2011 url http veresoftware com blog p 364 accessdate 25 november 2015 ref in today s society most professional video editing software has access to metadata avid s metasync and adobe s bridge are two prime examples of this ref cite web last oak tree press authorlink title metadata for video year 2011 url http veresoftware com blog p 364 accessdate 25 november 2015 ref web pages web page s often include metadata in the form of meta element meta tags description and keywords in meta tags are commonly used to describe the web page s content meta elements also specify page description key words authors of the document and when the document was last modified ref name rouse m 2014 web page metadata helps search engines and users to find the types of web pages they are looking for creation metadata can be created either by automated information processing or by manual work elementary metadata captured by computers can include information about when an object was created who created it when it was last updated file size and file extension in this context an object refers to any of the following a physical item such as a book cd dvd a paper map chair table flower pot etc an electronic file such as a digital image digital photo electronic document program file database table etc data virtualization main article data virtualization data virtualization has emerged in the 2000s as the new software technology to complete the virtualization stack in the enterprise metadata is used in data virtualization servers which are enterprise infrastructure components alongside database and application servers metadata in these servers is saved as persistent repository and describe business object s in various enterprise systems and applications structural metadata commonality is also important to support data virtualization statistics and census services standardization work has had a large impact on efforts to build metadata systems in the statistical community citation needed date may 2013 several metadata standards which date may 2013 are described and their importance to statistical agencies is discussed applications of the standards which date may 2013 at the census bureau environmental protection agency bureau of labor statistics statistics canada and many others are described citation needed date may 2013 emphasis is on the impact a metadata registry can have in a statistical agency library and information science metadata has been used in various ways as a means of cataloging items in libraries in both digital and analog format such data helps classify aggregate identify and locate a particular book dvd magazine or any object a library might hold in its collection until the 1980s many library catalogues used 3x5 inch cards in file drawers to display a book s title author subject matter and an abbreviated alphanumeric alpha numeric string call number which indicated the physical location of the book within the library s shelves the dewey decimal classification dewey decimal system employed by libraries for the classification of library materials by subject is an early example of metadata usage beginning in the 1980s and 1990s many libraries replaced these paper file cards with computer databases these computer databases make it much easier and faster for users to do keyword searches another form of older metadata collection is the use by us census bureau of what is known as the long form the long form asks questions that are used to create demographic data to find patterns of distribution ref cite web title agls metadata element set part 2 usage guide a non technical guide to using agls metadata for describing resources author national archives of australia year 2002 url http www naa gov au records management publications agls element aspx accessdate 17 march 2010 ref library libraries employ metadata in library catalog ues most commonly as part of an library management system integrated library management system metadata is obtained by library cataloguing cataloging rules cataloguing resources such as books periodicals dvds web pages or digital images this data is stored in the integrated library management system library management system ilms using the marc standards marc metadata standard the purpose is to direct patrons to the physical or electronic location of items or areas they seek as well as to provide a description of the item s in question more recent and specialized instances of library metadata include the establishment of digital library digital libraries including eprint e print repositories and digital image libraries while often based on library principles the focus on non librarian use especially in providing metadata means they do not follow traditional or common cataloging approaches given the custom nature of included materials metadata fields are often specially created e g taxonomic classification fields location fields keywords or copyright statement standard file information such as file size and format are usually automatically included ref name solodovnik cite journal last1 solodovnik first1 iryna year 2011 title metadata issues in digital libraries key concepts and perspectives journal jlis it jlis it italian journal of library archives and information science volume 2 issue 2 publisher university of florence doi 10 4403 jlis it 4663 url http leo cilea it index php jlis article view 4663 accessdate 29 june 2013 ref library operation has for decades been a key topic in efforts toward international standardization standards for metadata in digital libraries include dublin core metadata encoding and transmission standard mets metadata object description schema mods data documentation initiative ddi digital object identifier doi uniform resource name urn preservation metadata implementation strategies premis schema ecological metadata language eml and protocol for metadata harvesting oai pmh leading libraries in the world give hints on their metadata standards strategies ref cite web author library of congress network development and marc standards office url http www loc gov standards metadata html title library of congress washington dc on metadata publisher loc gov date 2005 09 08 accessdate 2011 12 23 ref ref cite web url http www dnb de de netzpublikationen ablieferung metadatenkernset metadatenkernset node html title deutsche nationalbibliothek frankfurt on metadata ref in museums metadata in a museum context is the information that trained cultural documentation specialists such as archivist s librarian s museum registrar museum registrar s and curator s create to index structure describe identify or otherwise specify works of art architecture cultural objects and their images ref name 0 cite journal last zange first charles s date 31 january 2015 title community makers major museums and the keet s aaxw learning about the role of museums in interpreting cultural objects url http mw2015 museumsandtheweb com paper community makers major museums and the keet saaxw learning about the role of museums in interpreting cultural objects publisher museums and the web ref ref name 1 cite book title cataloging cultural objects a guide to describing cultural works and their images visual resources association last baca first murtha publisher visual resources association year 2006 isbn location pages ref page needed date november 2016 ref name 2 cite book title introduction to metadata second edition los angeles getty information institute last baca first murtha publisher getty information institute year 2008 isbn location los angeles pages ref page needed date november 2016 descriptive metadata is most commonly used in museum contexts for object identification and resource recovery purposes ref name 1 usage metadata is developed and applied within collecting institutions and museums in order to facilitate resource discovery and execute search queries ref name 2 create digital archives that store information relating to various aspects of museum collections and cultural objects and serves for archival and managerial purposes ref name 2 provide public audiences access to cultural objects through publishing digital content online ref name 1 ref name 2 standards many museums and cultural heritage centers recognize that given the diversity of art works and cultural objects no single model or standard suffices to describe and catalogue cultural works ref name 0 ref name 1 ref name 2 for example a sculpted indigenous artifact could be classified as an artwork an archaeological artifact or an indigenous heritage item the early stages of standardization in archiving description and cataloging within the museum community began in the late 1990s with the development of standards such as categories for the description of works of art cdwa spectrum the conceptual reference model cidoc cataloging cultural objects cco and the cdwa lite xml schema ref name 1 these standards use html and xml markup languages for machine processing publication and implementation ref name 1 the anglo american cataloguing rules aacr originally developed for characterizing books have also been applied to cultural objects works of art and architecture ref name 2 standards such as the cco are integrated within a museum s collection management system cms a database through which museums are able to manage their collections acquisitions loans and conservation ref name 2 scholars and professionals in the field note that the quickly evolving landscape of standards and technologies create challenges for cultural documentarians specifically non technically trained professionals ref name 3 cite book title linked data for libraries archives and museums how to clean link and publish your metadata last hooland first seth van last2 verborgh first2 ruben publisher facet year 2014 isbn location london pages ref page needed date november 2016 most collecting institutions and museums use a relational database to categorize cultural works and their images ref name 2 relational databases and metadata work to document and describe the complex relationships amongst cultural objects and multi faceted works of art as well as between objects and places people and artistic movements ref name 1 ref name 2 relational database structures are also beneficial within collecting institutions and museums because they allow for archivists to make a clear distinction between cultural objects and their images an unclear distinction could lead to confusing and inaccurate searches ref name 2 cultural objects and art works an object s materiality function and purpose as well as the size e g measurements such as height width weight storage requirements e g climate controlled environment and focus of the museum and collection influence the descriptive depth of the data attributed to the object by cultural documentarians ref name 2 the established institutional cataloging practices goals and expertise of cultural documentarians and database structure also influence the information ascribed to cultural objects and the ways in which cultural objects are categorized ref name 0 ref name 2 additionally museums often employ standardized commercial collection management software that prescribes and limits the ways in which archivists can describe artworks and cultural objects ref name 3 as well collecting institutions and museums use controlled vocabulary controlled vocabularies to describe cultural objects and artworks in their collections ref name 1 ref name 2 getty vocabularies and the library of congress controlled vocabularies library of congress controlled vocabularies are reputable within the museum community and are recommended by cco standards ref name 2 museums are encouraged to use controlled vocabularies that are contextual and relevant to their collections and enhance the functionality of their digital information systems ref name 1 ref name 2 controlled vocabularies are beneficial within databases because they provide a high level of consistency improving resource retrieval ref name 1 ref name 2 metadata structures including controlled vocabularies reflect the ontology information science ontologies of the systems from which they were created often the processes through which cultural objects are described and categorized through metadata in museums do not reflect the perspectives of the maker communities ref name 0 ref cite journal last srinivasan first ramesh date december 2006 title indigenous ethnic and cultural articulations of new media url http ics sagepub com content 9 4 497 abstract journal international journal of cultural studies volume 9 issue 4 doi 10 1177 1367877906069899 ref museums and the internet metadata has been instrumental in the creation of digital information systems and archives within museums and has made it easier for museums to publish digital content online this has enabled audiences who might not have had access to cultural objects due to geographic or economic barriers to have access to them ref name 1 in the 2000s as more museums have adopted archival standards and created intricate databases discussions about linked data linked data between museum databases have come up in the museum archival and library science communities ref name 3 collection management systems cms and digital asset management digital asset management tools can be local or shared systems ref name 2 digital humanities digital humanities scholars note many benefits of interoperability between museum databases and collections while also acknowledging the difficulties achieving such interoperability ref name 3 law united states of america globalize date march 2015 problems involving metadata in litigation in the united states are becoming widespread when date february 2011 courts have looked at various questions involving metadata including the discovery law discoverability of metadata by parties although the federal rules of civil procedure have only specified rules about electronic documents subsequent case law has elaborated on the requirement of parties to reveal metadata ref cite journal last gelzer first reed d title metadata law and the real world slowly the three are merging journal journal of ahima volume 79 issue 2 pages 56 57 64 publisher american health information management association date february 2008 url http library ahima org xpedio groups public documents ahima bok1 036537 hcsp ddocname bok1 036537 accessdate 8 january 2010 ref in october 2009 the arizona supreme court has ruled that metadata records are public record ref cite news last walsh first jim title ariz supreme court rules electronic data is public record newspaper the arizona republic location phoenix arizona date 30 october 2009 url http www azcentral com arizonarepublic local articles 2009 10 30 20091030metadata1030 html accessdate 8 january 2010 ref document metadata have proven particularly important in legal environments in which litigation has requested metadata which can include sensitive information detrimental to a certain party in court using metadata removal tool s to clean or redact documents can mitigate the risks of unwittingly sending sensitive data this process partially see data remanence protects law firms from potentially damaging leaking of sensitive data through electronic discovery australia in australia the need to strengthen national security has resulted in the introduction of a new metadata storage law ref senate passes controversial metadata laws ref this new law means that both security and policing agencies will be allowed to access up to two years of an individual s metadata supposedly to make it easier to stop any terrorist attacks and serious crimes from happening in the 2000s the law does not allow access to content of people s messages phone calls or email and web browsing history but these provisions could be changed by the government in healthcare australian medical research pioneered the definition of metadata for applications in health care that approach offers the first recognized attempt to adhere to international standards in medical sciences instead of defining a proprietary standard under the world health organization who umbrella the medical community yet did not approve the need to follow metadata standards despite research that supported these standards ref m l\xc3\xb6be m knuth r m\xc3\xbccke http ceur ws org vol 559 paper1 pdf tim a semantic web application for the specification of metadata items in clinical research ceur ws org urn nbn de 0074 559 9 ref data warehousing data warehouse dw is a repository of an organization s electronically stored data data warehouses are designed to manage and store the data data warehouses differ from business intelligence bi systems because bi systems are designed to use data to create reports and analyze the information to provide strategic guidance to management ref inmon w h tech topic what is a data warehouse prism solutions volume 1 1995 ref metadata is an important tool in how data is stored in data warehouses the purpose of a data warehouse is to house standardized structured consistent integrated correct cleaned and timely data extracted from various operational systems in an organization the extracted data are integrated in the data warehouse environment to provide an enterprise wide perspective data are structured in a way to serve the reporting and analytic requirements the design of structural metadata commonality using a data modeling method such as entity relationship model diagramming is important in any data warehouse development effort they detail metadata on each piece of data in the data warehouse an essential component of a data warehouse business intelligence system is the metadata and tools to manage and retrieve the metadata ralph kimball ref cite book last kimball first ralph authorlink ralph kimball title the data warehouse lifecycle toolkit edition second location new york publisher wiley year 2008 isbn 978 0 470 14977 5 ref harv pages 10 115 117 131 132 140 154 155 ref page needed date november 2016 describes metadata as the dna of the data warehouse as metadata defines the elements of the data warehouse and how they work together ralph kimball kimball et al ref harvnb kimball 2008 pages 116 117 ref refers to three main categories of metadata technical metadata business metadata and process metadata technical metadata is primarily definitional while business metadata and process metadata is primarily descriptive the categories sometimes overlap technical metadata defines the objects and processes in a dw bi system as seen from a technical point of view the technical metadata includes the system metadata which defines the data structures such as tables fields data types indexes and partitions in the relational engine as well as databases dimensions measures and data mining models technical metadata defines the data model and the way it is displayed for the users with the reports schedules distribution lists and user security rights business metadata is content from the data warehouse described in more user friendly terms the business metadata tells you what data you have where they come from what they mean and what their relationship is to other data in the data warehouse business metadata may also serve as a documentation for the dw bi system users who browse the data warehouse are primarily viewing the business metadata process metadata is used to describe the results of various operations in the data warehouse within the extract transform load etl process all key data from tasks is logged on execution this includes start time end time cpu seconds used disk reads disk writes and rows processed when troubleshooting the etl or information retrieval query process this sort of data becomes valuable process metadata is the fact measurement when building and using a dw bi system some organizations make a living out of collecting and selling this sort of data to companies in that case the process metadata becomes the business metadata for the fact and dimension tables collecting process metadata is in the interest of business people who can use the data to identify the users of their products which products they are using and what level of service they are receiving on the internet the html format used to define web pages allows for the inclusion of a variety of types of metadata from basic descriptive text dates and keywords to further advanced metadata schemes such as the dublin core e gms and agls ref national archives of australia agls metadata standard accessed 7 january 2010 http www naa gov au records management create capture describe describe agls index aspx ref standards pages can also be geotagging geotagged with geographic coordinate system coordinates metadata may be included in the page s header or in a separate file microformat s allow metadata to be added to on page data in a way that regular web users do not see but computers web crawler s and search engine s can readily access many search engines are cautious about using metadata in their ranking algorithms due to exploitation of metadata and the practice of search engine optimization search engine optimization seo to improve rankings see meta element article for further discussion this cautious attitude may be justified as people according to doctorow ref metacrap putting the torch to seven straw men of the meta utopia http www well com doctorow metacrap htm ref are not executing care and diligence when creating their own metadata and that metadata is part of a competitive environment where the metadata is used to promote the metadata creators own purposes studies show that search engines respond to web pages with metadata implementations ref the impact of webpage content characteristics on webpage visibility in search engine results http web simmons edu braun 467 part 1 pdf ref and google has an announcement on its site showing the meta tags that its search engine understands ref cite web url https support google com webmasters answer 79812 hl en title meta tags that google understands publisher google com accessdate 2014 05 22 ref enterprise search startup swiftype recognizes metadata as a relevance signal that webmasters can implement for their website specific search engine even releasing their own extension known as meta tags 2 ref cite web url https swiftype com documentation meta tags2 title swiftype specific meta tags work swiftype documentation publisher swiftype date 3 october 2014 ref in broadcast industry in broadcast industry metadata is linked to audio and video broadcast media to identify the media media clip clip or playlist names duration timecode etc describe the content notes regarding the quality of video content rating description for example during a sport event index term keywords like goal red card will be associated to some clips classify media metadata allows to sort the media or to easily and quickly find a video content a tv news could urgently need some archiving archive content for a subject for example the bbc have a large subject classification system lonclass a customized version of the more general purpose universal decimal classification this metadata can be linked to the video media thanks to the video server broadcast automation video servers most major broadcast sport events like fifa world cup or the olympic games use this metadata to distribute their video content to tv station s through index term keywords it is often the host broadcaster ref cite web url http www hbs tv hostbroadcasting title hbs is the fifa host broadcaster publisher hbs tv date 2011 08 06 accessdate 2011 12 23 ref who is in charge of organizing metadata through its international broadcast centre and its video servers this metadata is recorded with the images and are entered by metadata operators loggers who associate in live metadata available in metadata grids through software such as multicam lsm or ipdirector used during the fifa world cup or olympic games ref cite web url http www evs global com 01 mydocuments cs bob evscontributon 0808 eng pdf title host broadcast media server and related applications format pdf accessdate 2013 08 17 deadurl yes archiveurl https web archive org web 20111102235256 http www evs global com 01 mydocuments cs bob evscontributon 0808 eng pdf archivedate 2 november 2011 ref ref cite web url http broadcastengineering com worldcup fifa world cup techonlogy 0610 title logs during sport events publisher broadcastengineering com accessdate 2011 12 23 ref geospatial metadata that describes geographic objects in electronic storage or format such as datasets maps features or documents with a geospatial component has a history dating back to at least 1994 refer http libraries mit edu guides subjects metadata standards fgdc html mit library page on fgdc metadata this class of metadata is described more fully on the geospatial metadata article ecological and environmental ecological and environmental metadata is intended to document the who what when where why and how of data collection for a particular study this typically means which organization or institution collected the data what type of data which date s the data was collected the rationale for the data collection and the methodology used for the data collection metadata should be generated in a format commonly used by the most relevant science community such as darwin core ecological metadata language ref http knb ecoinformatics org software eml eml 2 0 1 index html webarchive url https web archive org web 20110423161141 http knb ecoinformatics org software eml eml 2 0 1 index html date 23 april 2011 ref or dublin core metadata editing tools exist to facilitate metadata generation e g metavist ref cite web url http metavist djames net title metavist 2 publisher metavist djames net accessdate 2011 12 23 ref mercury metadata search system morpho ref cite web url http knb ecoinformatics org morphoportal jsp title knb data morpho publisher knb ecoinformatics org date 2009 05 20 accessdate 2011 12 23 ref metadata should describe data provenance provenance of the data where they originated as well as any transformations the data underwent and how to give credit for cite the data products digital music when first released in 1982 compact discs only contained a table of contents toc with the number of tracks on the disc and their length in samples http s3 amazonaws com academia edu documents 32801641 morris 2012 making music behave pdf awsaccesskeyid akiaj56tqjrtwsmtnpea expires 1477195681 signature 2tlmhapcr0m5eysfmq8fgg2tza0 3d response content disposition inline 3b 20filename 3dmaking music behave metadata and the dig pdf https books google com books id gkiagz0hwcmc pg pa48 source gbs toc r cad 4 v onepage q f false fourteen years later in 1996 a revision of the compact disc digital audio cd red book standard added cd text to carry additional metadata http web ncf ca aa571 cdtext htm but cd text was not widely adopted shortly thereafter it became common for personal computers to retrieve metadata from external sources e g cddb gracenote based on the toc digital sound recording and reproduction audio formats such as digital audio file s superseded music formats such as cassette tape s and cds in the 2000s digital audio files could be labelled with more information than could be contained in just the file name that descriptive information is called the audio tag or audio metadata in general computer programs specializing in adding or modifying this information are called tag editor s metadata can be used to name describe catalogue and indicate ownership or copyright for a digital audio file and its presence makes it much easier to locate a specific audio file within a group typically through use of a search engine that accesses the metadata as different digital audio formats were developed attempts were made to standardize a specific location within the digital files where this information could be stored as a result almost all digital audio formats including mp3 broadcast wav and aiff files have similar standardized locations that can be populated with metadata the metadata for compressed and uncompressed digital music is often encoded in the id3 tag common editors such as taglib support mp3 ogg vorbis flac mpc speex wavpack trueaudio wav aiff mp4 and asf file formats cloud applications with the availability of cloud computing cloud applications which include those to add metadata to content metadata is increasingly available over the internet administration and management storage metadata can be stored either internally ref name id3 cite web first dan last o neill url http id3 org title id3 org ref in the same file or structure as the data this is also called embedded metadata or externally in a separate file or field from the described data a data repository typically stores the metadata detached from the data but can be designed to support embedded metadata approaches each option has advantages and disadvantages internal storage means metadata always travels as part of the data they describe thus metadata is always available with the data and can be manipulated locally this method creates redundancy precluding normalization and does not allow managing all of a system s metadata in one place it arguably increases consistency since the metadata is readily changed whenever the data is changed external storage allows collocating metadata for all the contents for example in a database for more efficient searching and management redundancy can be avoided by normalizing the metadata s organization in this approach metadata can be united with the content when information is transferred for example in streaming media or can be referenced for example as a web link from the transferred content on the down side the division of the metadata from the data content especially in standalone files that refer to their source metadata elsewhere increases the opportunities for misalignments between the two as changes to either may not be reflected in the other metadata can be stored in either human readable or binary form storing metadata in a human readable format such as xml can be useful because users can understand and edit it without specialized tools ref name sutter cite book first1 robbie last1 de sutter first2 stijn last2 notebaert first3 rik last3 van de walle chapter evaluation of metadata standards in the context of digital audio visual libraries title research and advanced technology for digital libraries 10th european conference edcl 2006 editor1 last gonzalo editor1 first julio editor2 last thanos editor2 first constantino editor3 last verdejo editor3 first m felisa editor4 last carrasco editor4 first rafael date september 2006 url https books google com books id ku7lqqowp54c pg pa226 cad 4 v onepage isbn 978 3540446361 publisher springer page 226 ref however text based formats are rarely optimized for storage capacity communication time or processing speed a binary metadata format enables efficiency in all these respects but requires special software to convert the binary information into human readable content database management each relational database system has its own mechanisms for storing metadata examples of relational database metadata include tables of all tables in a database their names sizes and number of rows in each table tables of columns in each database what tables they are used in and the type of data stored in each column in database terminology this set of metadata is referred to as the database catalog catalog the sql standard specifies a uniform means to access the catalog called the information schema but not all databases implement it even if they implement other aspects of the sql standard for an example of database specific metadata access methods see oracle metadata programmatic access to metadata is possible using apis such as jdbc or schemacrawler ref name schemacrawler cite web author sualeh fatehi url http schemacrawler sourceforge net title schemacrawler work sourceforge ref see also div col 25em agris international information system for the agricultural sciences and technology classification scheme crosswalk metadata dataone data dictionary aka metadata repository dublin core folksonomy geoms generic earth observation metadata standard geospatial metadata ipdirector iso iec 11179 knowledge tag mercury metadata search system meta element if map metadata access point interface metadata discovery metadata facility for java v 4 b metadata metadata from wikiversity metadata publishing metadata registry metamathematics metafor common metadata for climate modelling digital repositories microcontent microformat multicam lsm observations and measurements ontology computer science official statistics paratext preservation metadata sdmx semantic web sgml the metadata company universal data element framework vocabulary onesource xsd div col end references reflist colwidth 30em external links wiktionary metadata http www niso org apps group public download php 17446 understanding 20metadata understanding metadata what is metadata and what is it for niso 2017 http web archive org web 20140522165110 http www theguardian com technology interactive 2013 jun 12 what is metadata nsa surveillance meta 1111111 a guardian guide to your metadata the guardian wednesday 12 june 2013 http www well com doctorow metacrap htm metacrap putting the torch to seven straw men of the meta utopia cory doctorow s opinion on the limitations of metadata on the internet 2001 http www dataone org dataone investigator toolkit http www informaworld com openurl genre journal issn 1938 6389 journal of library metadata routledge taylor francis group issn 1937 5034 http www inderscience com ijmso international journal of metadata semantics and ontologies ijmso inderscience publishers issn 1744 263x webarchive url https web archive org web 20130126101115 http www metalounge org literature 52579 stephen machin e2 80 93 on metadata and metacontent date 26 january 2013 title metadata and metacontent pdf archived version software engineering data warehouse authority control category data management category records management category knowledge representation category library cataloging and classification category metadata category technical communication category business intelligence'
b'multiple issues essay like date december 2016 notability date december 2016 original research date december 2016 orphan date december 2016 data can be viewed as a measurement of numbers and characters that are set in a way to understand a certain subject however there are many different ways to view data such as conceptualizing data these are five ways of conceptualizing data they all have positive and negative points to each technique although they are different they all bring up questions and concerns with data collection and what happens with the information afterward another concern is what is the goal with the data that has been collected depending on the category the five ways of conceptualizing data are technically ethically politically and economically spatially temporal and philosophically typically viewed by critical data scholars they have all of these ways of viewing data because it is important to see the different ways that data can be viewed and to see if there may be any bias not only is it important to see if there is any bias however it is also important to understand what all the data will mean in the bigger picture the way that this is normally done is by understanding raw data then placing them into categories that will help with the better understanding and creating new knowledge technically technically viewing data concerns the knowledge about the quality of data if it is reliable if it is authentic if it is valid it is also about knowing how the data is structured shared processed and analyzed ref kitchin p 12 ref there are views about the concerns around data such as the representativeness how it is uncertain the reliability of it the chances of any errors the likelihood of any bias and around the measuring of the research design and the execution of it ref kitchin p 13 ref there are also questions around if this form of scientific technique is going to bring the data that is wanted and needed ref kitchin p 13 ref other reliability concerns go with this technical view about data such as quixotic reliability diachronic reliability and synchronic reliability quixotic reliability concern is where there is one observation method which produces unvarying measurements ref kitchin p 13 14 ref diachronic reliability is the stability of an observation through time lastly synchronic reliability is the similarity of observations within the same time period ref kitchin p 14 ref with it being technology there are many different ways that errors could arise such as missing data mistakes misunderstaning s bias and uncertainty ref kitchin p 14 ref ethically the ethics ethical view of data is more about the idea of why the data is generated and what use the data is going to be placed in there are concerns around how the data will be shared protected traded and to how they are employed ref kitchin p 15 ref this also deals with the issue of sensitivity some data is low when it comes to sensitivity such as the traffic however some are a lot higher such as speaking to survivors of crime ref kitchin p 15 ref with the sensitivity scale there comes privacy issues how someone may be treated and the issue of human rights ref kitchin p 15 ref it is helpful to know that some companies have a data protection act and have privacy laws ref kitchin p 15 ref other components that add to the category of ethics are the question of equality fairness justice honesty respect entitlements rights and care of the information that is provided and towards those that give the information ref kitchin p 14 ref the honesty respect and the care of the information can also be misinformed to the subject that is giving the data willingly causing ethical concerns for how long the information will be kept or what the information will be used for this is an ethical concern in the exchange of the subject and the researcher ref jacob ref politically and economically politics politically and economic ally viewed data is seen to how the data could be viewed or theorized as public goods intellectual property political capital and how they are traded and how they are regulated ref kitchin p 16 ref economically there are many decisions when funding data researching as well as investing in data researching data could be used to manage goals and raise the profits and values to those that invest in it ref kitchin p 15 ref such as the multi billion dollar data marketplace where many companies are trading and using that data to help themselves make a profit it is positively effecting due to the production of knowledge ref kitchin p 15 ref the more that the company knows about what the people want and how to market to them the more that they may profit and gain off of the data due to them giving what the people want however there still is the political side to this although the data can make a profit and is economically great there is also the competition which want to influence opinions and make the data terrain greater ref kitchin p 16 ref it is also political because the difference between publicly good data which is shared with anyone that can have access to it is much different than business data this is because business data is wanting to keep the data that they have found and use it to their advantage such as the production of knowledge ref kitchin p 16 ref the publicly good data is free to anyone that wants to view it which would not be helpful in any way to any business strategies or marketing ref kitchin p 16 ref spatially temporal spatial and temporal views data around technical ethical political and economic regime with the production of the data ref kitchin p 17 ref the way that the terms spatial and temporal can be viewed is around how the data is developed and changed across time and space although depending on the time and where this data is being collected the process the analysis the storage of some information yet not of others will be different just due to a time frame and area will be different than others because of the different history that has happened and the different geographical locations as noted the process of taking in data changes over time however they are never sudden changes these changes happen slowly over time due to different laws that come in place around how data is handled or protected the different forms of organizing the improvements around administration if any new technology has formed when the methods of data sorting have changed along with the methods of sorting the data the geographical statistics that vary and the new techniques of statistics ref kitchin p 17 ref not only does the geographical location change how the assemblage of data is taken is but it can also be different depending on the person due to how they manage the data or how they produce it ref kitchin p 17 ref looking over data temporally can bring forth either questions or patterns depending on what the data is about an example of this is looking at graphs that have time in them they present inclines and declines in a pattern about the data over time ref whitney ref spatial data on the other hand looks more towards the geographical sense in the data the information that is gathered could be about the location the size or the shape of a particular object a system that uses spatial data is gis geographical information system ref rouse ref philosophical philosophy brings forth views around the areas of epistemology and ontology in this view of data there is no interpretations opinions importance or relevance of the data that has been found and processed the data is simply measured for what it is which brings forth to how it is viewed the data that is viewed philosophically is also viewed in an objective way which means that the data is fixed in some way to prove a specific point although the data may be truthful how the data was provided and how it is placed makes the difference the data is also viewed in a realist view such as how things truly are no information is changed everything is the way that it is and is seen for that ref kitchin p 17 19 ref this view also brings up issues around property rights ref liu p 61 ref who would own what and who can have the right to take things references reflist 20em works cited refbegin cite book last kitchin first rob year 2014 title the data revolution big data open data data infrastructures their consequences place london publisher sage chapter conceptualising data pp 1 26 chapter url http www uk sagepub com upm data 63923 kitchin ch1 pdf format pdf metcalf jacob emily f keller and danah boyd 2016 perspectives on big data ethics and society council for big data ethics and society rouse margaret 2013 what is spatial data definition from whatis com searchsqlserver techtarget liu hong 2016 philosophical reflections on data philosophical reflections on data science direct whitney hunter 2014 it s about time it s about time visualizing temporal data to reveal patterns and stories ux magazine ux magazine refend category data management'
b'notability date december 2016 orphan date december 2016 critical data studies is the systematic study of data and its criticisms ref dalton craig and jim thatcher 2014 ref the field was named by scholars craig dalton and jim thatcher in their 2015 article titled what does a critical data studies look like and why do we care interest has developed in this domain as a response to the emergence and reliance on big data in contemporary society ref ibid ref some of the other key scholars in this discipline include rob kitchen and tracey p lauriault ref kitchin rob and tracey p lauriault 2014 ref ref kitchin rob 2014 ref scholars have attempted to make sense of data through different theoretical frameworks some of which include analyzing data technically ethically politically economically temporally spatially and philosophically ref kitchin rob 2014 ref some of the key academic journals related to critical data studies include the journal of big data and big data and society why is a critical approach to data needed in their article in which they coin the term critical data studies dalton and thatcher also provide several justifications as to why data studies is a discipline worthy of a critical approach ref dalton craig and jim thatcher 2014 ref firstly big data is an important aspect of twenty first century society and the analysis of big data allows for a deeper understanding of what is happening and for what reasons ref dalton craig and jim thatcher 2014 ref furthermore big data as a technological tool and the information that it yields are not neutral according to dalton and thatcher ref ibid ref making it worthy of critical analysis in order to identify and address its biases building off this idea another justification for a critical approach is that the relationship between big data and society is an important one and therefore worthy of study ref ibid ref dalton and thatcher stress how the relationship is not an example of technological determinism but rather how big data can shape the lives of individuals big data technology can cause significant changes in society s structure and in the everyday lives of people ref ibid ref and being a product of society big data technology is worthy of sociological investigation ref ibid ref moreover data sets are almost never completely raw that is to say without any influences dalton and thatcher describe how data are shaped by the vision or goals of a research team and during the data collection process certain things are quantified stored sorted and even discarded by the research team ref ibid ref a critical approach is thus necessary in order to understand and reveal the intent behind the information being presented dalton and thatcher also argue how data alone cannot speak for itself in order to possess any concrete meaning data must be accompanied by theoretical insight or be accompanied by alternative quantitative or qualitative research measures ref ibid ref dalton and thatcher argue that if one were to only think of data in terms of its exploitative power there is no possibility of using data for revolutionary liberatory purposes ref ibid ref finally dalton and thatcher propose that a critical approach in studying data allows for big data to be combined with older small data and thus create more thorough research opening up more opportunities questions and topics to be explored ref ibid ref issues and concerns for critical data scholars the use of data in modern society brings about new ways of understanding and measuring the world but also brings with it certain concerns or issues ref kitchin rob 2014 ref data scholars attempt to bring some of these issues to light in their quest to be critical of data rob kitchin identifies both technical and organizational issues of data as well as some normative and ethical questions ref ibid ref technical and organization issues concerning data range from the scope of datasets access to the data the quality of the data the integration of the data the application of analytics and ecological fallacies as well as the skills and organizational capabilities of the research team ref ibid ref some of the normative and ethical concerns addressed by kitchin include surveillance through one s data dataveillance the privacy of one s data the ownership of one s data the security of one s data anticipatory or corporate governance and finally profiling individuals by their data ref ibid ref all of these concerns must be taken into account by scholars of data in their objective to be critical references reflist 24em sources dalton craig and jim thatcher what does a critical data studies look like and why do we care seven points for a critical approach to big data society and space open site 2014 retrieved october 23 2016 elkins james r the critical thinking movement alternating currents in one teacher s thinking myweb wvnet edu 1999 retrieved 29 november 2016 kitchin rob the data revolution big data open data data infrastructures and their consequences sage 2014 retrieved october 23 2016 kitchin rob and tracey p lauriault towards critical data studies charting and unpacking data assemblages and their work 2014 retrieved october 23 2016 category data management'
b'use dmy dates date march 2012 business administration business intelligence bi are the set of strategies processes application software applications data products technologies and technical architectures which are used to support the collection analysis presentation and dissemination of business information ref dedi\xc4\x87 n stanier c 2016 measuring the success of changes to existing business intelligence solutions to improve business intelligence reporting lecture notes in business information processing springer international publishing volume 268 pp 225 236 ref bi technologies provide historical current and predictive views of business operations common functions of business intelligence technologies are business reporting reporting online analytical processing analytics data mining process mining complex event processing business performance management benchmarking text mining predictive analysis predictive analytics and prescriptive analytics prescriptive analytics and are capable of handling large amounts of structured and sometimes unstructured data to help identify develop and otherwise create new strategic business opportunities the goal is to allow for the easy interpretation of these big data identifying new opportunities and implementing an effective strategy based on insights can provide businesses with a competitive market advantage and long term stability ref cite book title business intelligence success factors tools for aligning your business in the global economy last rud first olivia year 2009 publisher wiley sons location hoboken n j isbn 978 0 470 39240 9 page pages url accessdate ref business intelligence can be used to support a wide range of business decisions ranging from operational to strategic basic operating decisions include product positioning or pricing strategic business decisions include priorities goals and directions at the broadest level in all cases bi is most effective when it combines data derived from the market in which a company operates external data with data from company sources internal to the business such as financial and operations data internal data when combined external and internal data can provide a more complete picture which in effect creates an intelligence that cannot be derived by any singular set of data ref cite book last1 coker first1 frank title pulse understanding the vital signs of your business publisher ambient light publishing publication date 2014 pages 41 42 isbn 978 0 9893086 0 1 ref amongst myriad uses business intelligence tools empower organisations to gain insight into new markets assess demand and suitability of products and services for different market segments and gauge the impact of marketing efforts ref name 0 chugh r grandhi s 2013 why business intelligence significance of business intelligence tools and integrating bi governance with corporate governance international journal of e entrepreneurship and innovation vol 4 no 2 pp 1 14 https www researchgate net publication 273861123 why business intelligence significance of business intelligence tools and integrating bi governance with corporate governance ref components business intelligence is made up of an increasing number of components including multidimensional aggregation and allocation denormalization tagging and standardization realtime reporting with analytical alert a method of interfacing with unstructured data sources group consolidation budgeting and rolling forecast s statistical inference and probabilistic simulation key performance indicator s optimization version control and process management open item management history the earliest known use of the term business intelligence is in richard millar devens in the cyclop\xc3\xa6dia of commercial and business anecdotes from 1865 devens used the term to describe how the banker sir henry furnese gained profit by receiving and acting upon information about his environment prior to his competitors throughout holland flanders france and germany he maintained a complete and perfect train of business intelligence the news of the many battles fought was thus received first by him and the siege of namur 1695 fall of namur added to his profits owing to his early receipt of the news devens 1865 p nbsp 210 the ability to collect and react accordingly based on the information retrieved an ability that furnese excelled in is today still at the very heart of bi ref name miller devens cite book last miller devens first richard title cyclopaedia of commercial and business anecdotes comprising interesting reminiscences and facts remarkable traits and humors of merchants traders bankers etc in all ages and countries url https books google dk books id 9mspaaaayaaj pg pa210 dq 22business intelligence 22 hl en ei a5eptdarisownaevyyhqdg sa x oi book result ct result redir esc y v onepage q 22business 20intelligence 22 f false publisher d appleton and company accessdate 15 february 2014 page 210 ref in a 1958 article ibm researcher hans peter luhn used the term business intelligence he employed the webster s dictionary definition of intelligence the ability to apprehend the interrelationships of presented facts in such a way as to guide action towards a desired goal ref cite journal url http www research ibm com journal rd 024 ibmrd0204h pdf doi 10 1147 rd 24 0314 title a business intelligence system author h p luhn authorlink hans peter luhn year 1958 journal ibm journal volume 2 issue 4 pages 314 ref business intelligence as it is understood today is said to have evolved from the decision support system s dss that began in the 1960s and developed throughout the mid 1980s dss originated in the computer aided models created to assist with decision making and planning from dss data warehouse s executive information system s online analytical processing olap and business intelligence came into focus beginning in the late 80s in 1989 howard dresner later a gartner analyst proposed business intelligence as an umbrella term to describe concepts and methods to improve business decision making by using fact based support systems ref name power cite web url http dssresources com history dsshistory html title a brief history of decision support systems version 4 0 accessdate 10 july 2008 author d j power date 10 march 2007 publisher dssresources com ref it was not until the late 1990s that this usage was widespread ref cite web url http dssresources com history dsshistory html title a brief history of decision support systems last power first d j accessdate 1 november 2010 ref critics see bi as evolved from mere business reporting together with the advent of increasingly powerful and easy to use data analysis tools in this respect it has also been criticized as a marketing buzzword in the context of the big data surge ref cite web title decoding big data buzzwords year 2015 quote bi refers to the approaches tools mechanisms that organizations can use to keep a finger on the pulse of their businesses also referred by unsexy versions dashboarding mis or reporting publisher cio com url http www cio com article 2919082 big data what are they talking about decoding big data buzzwords html ref data warehousing often bi applications use data gathered from a data warehouse dw or from a data mart and the concepts of bi and dw sometimes combine as bi dw ref cite book last1 golden first1 bernard title amazon web services for dummies url https books google com books id xsvwaaaaqbaj series for dummies publisher john wiley sons publication date 2013 page 234 isbn 9781118652268 accessdate 2014 07 06 quote traditional business intelligence or data warehousing tools the terms are used so interchangeably that they re often referred to as bi dw are extremely expensive ref or as bidw a data warehouse contains a copy of analytical data that facilitates decision support however not all data warehouses serve for business intelligence nor do all business intelligence applications require a data warehouse to distinguish between the concepts of business intelligence and data warehouses forrester research defines business intelligence in one of two ways using a broad definition business intelligence is a set of methodologies processes architectures and technologies that transform raw data into meaningful and useful information used to enable more effective strategic tactical and operational insights and decision making ref cite web url http www forrester com rb research topic overview business intelligence q id 39218 t 2 title topic overview business intelligence last evelson first boris date 21 november 2008 ref under this definition business intelligence also includes technologies such as data integration data quality data warehousing master data management text and content analytics and many others that the market sometimes lumps into the information management segment therefore forrester refers to data preparation and data usage as two separate but closely linked segments of the business intelligence architectural stack forrester defines the narrower business intelligence market as referring to just the top layers of the bi architectural stack such as reporting analytics and dashboards management information systems dashboards ref cite web url http blogs forrester com boris evelson 10 04 29 want know what forresters lead data analysts are thinking about bi and data domain title want to know what forrester s lead data analysts are thinking about bi and the data domain last evelson first boris date 29 april 2010 ref comparison with competitive intelligence though the term business intelligence is sometimes a synonym for competitive intelligence because they both support decision making bi uses technologies processes and applications to analyze mostly internal structured data and business processes while competitive intelligence gathers analyzes and disseminates information with a topical focus on company competitors if understood broadly business intelligence can include the subset of competitive intelligence ref cite web url http blogs forrester com james kobielus 10 04 30 what e2 80 99s not bi oh don e2 80 99t get me startedoops too latehere goes title what s not bi oh don t get me started oops too late here goes last kobielus first james date 30 april 2010 quote business intelligence is a non domain specific catchall for all the types of analytic data that can be delivered to users in reports dashboards and the like when you specify the subject domain for this intelligence then you can refer to competitive intelligence market intelligence social intelligence financial intelligence hr intelligence supply chain intelligence and the like ref comparison with business analytics business intelligence and business analytics are sometimes used interchangeably but there are alternate definitions ref cite web url http timoelliott com blog 2011 03 business analytics vs business intelligence html title business analytics vs business intelligence publisher timoelliott com date 2011 03 09 accessdate 2014 06 15 ref one definition contrasts the two stating that the term business intelligence refers to collecting business data to find information primarily through asking questions reporting and online analytical processes business analytics on the other hand uses statistical and quantitative tools for explanatory and predictive modelling ref cite web url http www businessanalytics com difference between business analytics and business intelligence title difference between business analytics and business intelligence publisher businessanalytics com date 2013 03 15 accessdate 2014 06 15 ref in an alternate definition thomas h davenport thomas davenport professor of information technology and management at babson college argues that business intelligence should be divided into information retrieval querying business reporting reporting online analytical processing olap an alerts tool and business analytics in this definition business analytics is the subset of bi focusing on statistics prediction and optimization rather than the reporting functionality ref cite interview url http www informationweek com news software bi 222200096 title analytics at work q a with tom davenport last henschen first doug date 4 january 2010 ref applications in an enterprise business intelligence can be applied to the following business purposes in order to drive business value citation needed date october 2010 measurement nbsp program that creates a hierarchy of performance metrics see also metrics reference model and benchmarking that informs business leaders about progress towards business goals business process management analytics nbsp program that builds quantitative processes for a business to arrive at optimal decisions and to perform business knowledge discovery frequently involves data mining process mining statistical analysis predictive analytics predictive modeling business process modeling data lineage complex event processing and prescriptive analytics prescriptive analytics business reporting reporting enterprise reporting nbsp program that builds infrastructure for strategic reporting to serve the strategic management of a business not operational reporting frequently involves data visualization executive information system and olap collaboration collaboration platform nbsp program that gets different areas both inside and outside the business to work together through data sharing and electronic data interchange knowledge management nbsp program to make the company data driven through strategies and practices to identify create represent distribute and enable adoption of insights and experiences that are true business knowledge knowledge management leads to learning management and regulatory compliance in addition to the above business intelligence can provide a pro active approach such as alert functionality that immediately notifies the end user if certain conditions are met for example if some business metric exceeds a pre defined threshold the metric will be highlighted in standard reports and the business analyst may be alerted via e mail or another monitoring service this end to end process requires data governance which should be handled by the expert citation needed date january 2012 prioritization of projects it can be difficult to provide a positive business case for business intelligence initiatives and often the projects must be prioritized through strategic initiatives bi projects can attain higher prioritization within the organization if managers consider the following as described by kimball ref kimball et al 2008 29 ref the bi manager must determine the tangible benefits such as eliminated cost of producing legacy reports data access for the entire organization must be enforced ref cite web url http content dell com us en enterprise d large business ready business intelligence aspx title are you ready for the new business intelligence publisher dell com accessdate 19 june 2012 ref in this way even a small benefit such as a few minutes saved makes a difference when multiplied by the number of employees in the entire organization as described by ross weil roberson for enterprise architecture ref jeanne w ross peter weill david c robertson 2006 enterprise architecture as strategy p 117 isbn 1 59139 839 8 ref managers should also consider letting the bi project be driven by other business initiatives with excellent business cases to support this approach the organization must have enterprise architects who can identify suitable business projects using a structured and quantitative methodology to create defensible prioritization in line with the actual needs of the organization such as a weighted decision matrix ref cite web last krapohl first donald title a structured methodology for group decision making url http www augmentedintel com wordpress index php a structured methodology for group decision making publisher augmentedintel accessdate 22 april 2013 ref success factors of implementation according to kimball et al there are three critical areas that organizations should assess before getting ready to do a bi project ref kimball et al 2008 p 298 ref the level of commitment and sponsorship of the project from senior management the level of business need for creating a bi implementation the amount and quality of business data available business sponsorship the commitment and wikt sponsor sponsor ship of senior management is according to kimball et al the most important criteria for assessment ref kimball et al 2008 16 ref this is because having strong management backing helps overcome shortcomings elsewhere in the project however as kimball et al state even the most elegantly designed dw bi system cannot overcome a lack of business management sponsorship ref kimball et al 2008 18 ref it is important that personnel who participate in the project have a vision and an idea of the benefits and drawbacks of implementing a bi system the best business sponsor should have organizational clout and should be well connected within the organization it is ideal that the business sponsor is demanding but also able to be realistic and supportive if the implementation runs into delays or drawbacks the management sponsor also needs to be able to assume accountability and to take responsibility for failures and setbacks on the project support from multiple members of the management ensures the project does not fail if one person leaves the steering group however having many managers work together on the project can also mean that there are several different interests that attempt to pull the project in different directions such as if different departments want to put more emphasis on their usage this issue can be countered by an early and specific analysis of the business areas that benefit the most from the implementation all stakeholders in the project should participate in this analysis in order for them to feel invested in the project and to find common ground another management problem that may be encountered before the start of an implementation is an overly aggressive business sponsor problems of scope creep occur when the sponsor requests data sets that were not specified in the original planning phase business needs because of the close relationship with senior management another critical thing that must be assessed before the project begins is whether or not there is a business need and whether there is a clear business benefit by doing the implementation ref name kimball et al 2008 17 kimball et al 2008 17 ref the needs and benefits of the implementation are sometimes driven by competition and the need to gain an advantage in the market another reason for a business driven approach to implementation of bi is the acquisition of other organizations that enlarge the original organization it can sometimes be beneficial to implement dw or bi in order to create more oversight companies that implement bi are often large multinational organizations with diverse subsidiaries ref cite web title how companies are implementing business intelligence competency centers url http www computerworld com pdfs sas intel bicc pdf publisher computer world deadurl yes accessdate 1 april 2014 archiveurl https web archive org web 20130528054421 http www computerworld com pdfs sas intel bicc pdf archivedate 28 may 2013 ref a well designed bi solution provides a consolidated view of key business data not available anywhere else in the organization giving management visibility and control over measures that otherwise would not exist amount and quality of available data without proper data or with too little quality data any bi implementation fails it does not matter how good the management sponsorship or business driven motivation is before implementation it is a good idea to do data profiling this analysis identifies the content consistency and structure ref name kimball et al 2008 17 of the data this should be done as early as possible in the process and if the analysis shows that data is lacking put the project on hold temporarily while the it department figures out how to properly collect data when planning for business data and business intelligence requirements it is always advisable to consider specific scenarios that apply to a particular organization and then select the business intelligence features best suited for the scenario often scenarios revolve around distinct business processes each built on one or more data sources these sources are used by features that present that data as information to knowledge workers who subsequently act on that information the business needs of the organization for each business process adopted correspond to the essential steps of business intelligence these essential steps of business intelligence include but are not limited to go through business data sources in order to collect needed data convert business data to information and present appropriately query and analyze data act on the collected data the quality aspect in business intelligence should cover all the process from the source data to the final reporting at each step the quality gates are different source data data standardization make data comparable same unit same pattern master data management master data management unique referential operational data store operational data store ods data cleansing data cleansing detect correct inaccurate data data profiling check inappropriate value null empty data warehouse completeness check that all expected data are loaded referential integrity unique and existing referential over all sources consistency between sources check consolidated data vs sources reporting uniqueness of indicators only one share dictionary of indicators formula accuracy local reporting formula should be avoided or checked user aspect some considerations must be made in order to successfully integrate the usage of business intelligence systems in a company ultimately the bi system must be accepted and utilized by the users in order for it to add value to the organization ref name kimball kimball ref ref name swain swain scheps business intelligence for dummies 2008 isbn 978 0 470 12723 0 ref if the usability of the system is poor the users may become frustrated and spend a considerable amount of time figuring out how to use the system or may not be able to really use the system if the system does not add value to the users\xc2\xb4 mission they simply don t use it ref name swain to increase user acceptance of a bi system it can be advisable to consult business users at an early stage of the dw bi lifecycle for example at the requirements gathering phase ref name kimball this can provide an insight into the business process and what the users need from the bi system there are several methods for gathering this information such as questionnaires and interview sessions when gathering the requirements from the business users the local it department should also be consulted in order to determine to which degree it is possible to fulfill the business s needs based on the available data ref name kimball taking a user centered approach throughout the design and development stage may further increase the chance of rapid user adoption of the bi system ref name swain besides focusing on the user experience offered by the bi applications it may also possibly motivate the users to utilize the system by adding an element of competition kimball ref name kimball suggests implementing a function on the business intelligence portal website where reports on system usage can be found by doing so managers can see how well their departments are doing and compare themselves to others and this may spur them to encourage their staff to utilize the bi system even more in a 2007 article h j watson gives an example of how the competitive element can act as an incentive ref name watson cite journal title the current state of business intelligence year 2007 doi 10 1109 mc 2007 331 last1 watson first1 hugh j last2 wixom first2 barbara h journal computer volume 40 issue 9 pages 96 ref watson describes how a large call centre implemented performance dashboards for all call agents with monthly incentive bonuses tied to performance metrics also agents could compare their performance to other team members the implementation of this type of performance measurement and competition significantly improved agent performance bi chances of success can be improved by involving senior management to help make bi a part of the organizational culture and by providing the users with necessary tools training and support ref name watson training encourages more people to use the bi application ref name kimball providing user support is necessary to maintain the bi system and resolve user problems ref name swain user support can be incorporated in many ways for example by creating a website the website should contain great content and tools for finding the necessary information furthermore helpdesk support can be used the help desk can be manned by power users or the dw bi project team ref name kimball bi portals a business intelligence portal bi portal is the primary access interface for data warehouse data warehouse dw and business intelligence bi applications the bi portal is the user s first impression of the dw bi system it is typically a browser application from which the user has access to all the individual services of the dw bi system reports and other analytical functionality the bi portal must be implemented in such a way that it is easy for the users of the dw bi application to call on the functionality of the application ref name ralph the data warehouse lifecycle toolkit 2nd ed ralph kimball 2008 ref the bi portal s main functionality is to provide a navigation system of the dw bi application this means that the portal has to be implemented in a way that the user has access to all the functions of the dw bi application the most common way to design the portal is to custom fit it to the business processes of the organization for which the dw bi application is designed in that way the portal can best fit the needs and requirements of its users ref name wiley microsoft data warehouse toolkit wiley publishing 2006 ref the bi portal needs to be easy to use and understand and if possible have a look and feel similar to other applications or web content of the organization the dw bi application is designed for consistency the following is a list of desirable features for web portal s in general and bi portals in particular usable user should easily find what they need in the bi tool content rich the portal is not just a report printing tool it should contain more functionality such as advice help support information and documentation clean the portal should be designed so it is easily understandable and not over complex as to confuse the users current the portal should be updated regularly interactive the portal should be implemented in a way that makes it easy for the user to use its functionality and encourage them to use the portal scalability and customization give the user the means to fit the portal to each user value oriented it is important that the user has the feeling that the dw bi application is a valuable resource that is worth working on marketplace there are a number of business intelligence vendors often categorized into the remaining independent pure play vendors and consolidated megavendors that have entered the market through a recent trend ref cite news url http www zdnet com gartner releases 2013 bi magic quadrant 7000011264 title gartner releases 2013 bi magic quadrant publisher zdnet author andrew brust date 2013 02 14 accessdate 21 august 2013 ref of acquisitions in the bi industry ref cite web url http www bi verdict com fileadmin freeanalyses consolidations htm title consolidations in the bi industry date 7 march 2008 last pendse first nigel work the olap report ref the business intelligence market is gradually growing in 2012 business intelligence services brought in 13 1 billion in revenue ref cite web title why business intelligence is key for competitive advantage url https cisonline bu edu news resources why business intelligence is key for competitive advantage website boston university accessdate 23 october 2014 ref some companies adopting bi software decide to pick and choose from different product offerings best of breed rather than purchase one comprehensive integrated solution full service ref cite web url http www b eye network com view 2608 title three trends in business intelligence technology last imhoff first claudia date 4 april 2006 ref industry specific specific considerations for business intelligence systems have to be taken in some sectors such as bank regulation governmental banking regulations or healthcare ref cite journal vauthors mettler t vimarlund v title understanding business intelligence in the context of healthcare journal health informatics journal volume 15 issue 3 pages 254 264 year 2009 doi 10 1177 1460458209337446 ref the information collected by banking institutions and analyzed with bi software must be protected from some groups or individuals while being fully available to other groups or individuals therefore bi solutions must be sensitive to those needs and be flexible enough to adapt to new regulations and changes to existing law citation needed date may 2016 semi structured or unstructured data businesses create a huge amount of valuable information in the form of e mails memos notes from call centers news user groups chats reports web pages presentations image files video files and marketing material and news according to merrill lynch more than 85 of all business information exists in these forms these information types are called either semi structured data semi structured or unstructured data unstructured data however organizations often only use these documents once ref name rao cite journal doi 10 1109 mitp 2003 1254966 url http www ramanarao com papers rao itpro 2003 11 pdf title from unstructured data to actionable intelligence year 2003 last1 rao first1 r journal it professional volume 5 issue 6 pages 29 ref the managements of semi structured data is recognized as a major unsolved problem in the information technology industry ref name blumberg cite journal url http soquelgroup com articles dmreview 0203 problem pdf author1 blumberg r author2 s atre lastauthoramp yes title the problem with unstructured data journal dm review year 2003 pages 42 46 ref according to projections from gartner 2003 white collar workers spend anywhere from 30 to 40 percent of their time searching finding and assessing unstructured data bi uses both structured and unstructured data but the former is easy to search and the latter contains a large quantity of the information needed for analysis and decision making ref name blumberg ref name negash cite journal url http site xavier edu sena info600 businessintelligence pdf author negash s title business intelligence journal communications of the association of information systems volume 13 year 2004 pages 177 195 ref because of the difficulty of properly searching finding and assessing unstructured or semi structured data organizations may not draw upon these vast reservoirs of information which could influence a particular decision task or project this can ultimately lead to poorly informed decision making ref name rao therefore when designing a business intelligence dw solution the specific problems associated with semi structured and unstructured data must be accommodated for as well as those for the structured data ref name negash unstructured data vs semi structured data unstructured and semi structured data have different meanings depending on their context in the context of relational database systems unstructured data cannot be stored in predictably ordered columns and row database rows one type of unstructured data is typically stored in a blob binary large object a catch all data type available in most relational database management systems unstructured data may also refer to irregularly or randomly repeated column patterns that vary from row to row within each file or document citation needed date may 2016 many of these data types however like e mails word processing text files ppts image files and video files conform to a standard that offers the possibility of metadata metadata can include information such as author and time of creation and this can be stored in a relational database therefore it may be more accurate to talk about this as semi structured documents or data ref name blumberg but no specific consensus seems to have been reached unstructured data can also simply be the knowledge that business users have about future business trends business forecasting naturally aligns with the bi system because business users think of their business in aggregate terms capturing the business knowledge that may only exist in the minds of business users provides some of the most important data points for a complete bi solution problems with semi structured or unstructured data there are several challenges to developing bi with semi structured data according to inmon nesavich ref name inmon inmon b a nesavich unstructured textual data in the organization from managing unstructured data in the organization prentice hall 2008 pp 1 13 ref some of those are physically accessing unstructured textual data nbsp unstructured data is stored in a huge variety of formats terminology nbsp among researchers and analysts there is a need to develop a standardized terminology volume of data nbsp as stated earlier up to 85 of all data exists as semi structured data couple that with the need for word to word and semantic analysis searchability of unstructured textual data nbsp a simple search on some data e g apple results in links where there is a reference to that precise search term inmon nesavich 2008 ref name inmon gives an example a search is made on the term felony in a simple search the term felony is used and everywhere there is a reference to felony a hit to an unstructured document is made but a simple search is crude it does not find references to crime arson murder embezzlement vehicular homicide and such even though these crimes are types of felonies the use of metadata to solve problems with searchability and assessment of data it is necessary to know something about the content this can be done by adding context through the use of metadata ref name rao many systems already capture some metadata e g filename author size etc but more useful would be metadata about the actual content nbsp e g summaries topics people or companies mentioned two technologies designed for generating metadata about content are automatic categorization and information extraction 2009 predictions a 2009 paper predicted ref http www gartner com it page jsp id 856714 gartner reveals five business intelligence predictions for 2009 and beyond gartner com 15 january 2009 ref these developments in the business intelligence market because of lack of information processes and tools through 2012 more than 35 percent of the top 5 000 global companies regularly fail to make insightful decisions about significant changes in their business and markets by 2012 business units will control at least 40 percent of the total budget for business intelligence by 2012 one third of analytic applications applied to business processes will be delivered through granularity coarse grained application mashup web application hybrid mashups bi has a huge scope in entrepreneurship however majority of new entrepreneurs ignore its potential ref http brighterkashmir com role of business intelligence in entrepreneurship huge scope in entrepreneurship ref a 2009 information management special report predicted the top bi trends green computing social networking service s data visualization mobile business intelligence mobile bi predictive analytics composite application s cloud computing and multi touch multitouch ref cite web url http www information management com specialreports 2009 148 business intelligence data vizualization social networking analytics 10015628 1 html title 10 red hot bi trends last campbell first don date 23 june 2009 work information management ref research undertaken in 2014 indicated that employees are more likely to have access to and more likely to engage with cloud based bi tools than traditional tools ref cite web url http www aberdeen com aberdeen library 8906 rr analytics cloud saas bi aspx title cloud analytics in 2014 infusing the workforce with insight last lock first michael date 27 march 2014 ref other business intelligence trends include the following third party soa bi products increasingly address extract transform load etl issues of volume and throughput companies embrace in memory processing 64 bit processing and pre packaged analytic bi applications operational applications have callable bi components with improvements in response time scaling and concurrency near or real time bi analytics is a baseline expectation open source bi software replaces vendor offerings other lines of research include the combined study of business intelligence and uncertain data ref cite journal last rodriguez first carlos last2 daniel first2 florian last3 casati first3 fabio last4 cappiello first4 cinzia year 2010 title toward uncertain business intelligence the case of key indicators doi 10 1109 mic 2010 59 journal ieee internet computing volume 14 issue 4 pages 32 ref ref citation author rodriguez c author2 daniel f author3 casati f author4 cappiello c last author amp yes url http mitiq mit edu iciq documents iq 20conference 202009 papers 3 c pdf title computing uncertain key indicators from uncertain data pages 106 120 conference iciq 09 year 2009 ref in this context the data used is not assumed to be precise accurate and complete instead data is considered uncertain and therefore this uncertainty is propagated to the results produced by bi according to a study by the aberdeen group there has been increasing interest in software as a service saas business intelligence over the past years with twice as many organizations using this deployment approach as one year ago nbsp 15 in 2009 compared to 7 in 2008 ref cite web last1 julian first1 taylor title business intelligence implementation according to customer s needs url http apro software com services software development business intelligence publisher apro software accessdate 16 may 2016 date 10 january 2010 ref an article by infoworld s chris kanaracus points out similar growth data from research firm idc which predicts the saas bi market will grow 22 percent each year through 2013 thanks to increased product sophistication strained it budgets and other factors ref http infoworld com d cloud computing saas bi growth will soar in 2010 511 saas bi growth will soar in 2010 cloud computing infoworld 2010 02 01 retrieved 17 january 2012 ref an analysis of top 100 business intelligence and analytics scores and ranks the firms based on several open variables ref cite web url http www appsbi com top 100 analytics companies ranked and scored by mattermark title top 100 analytics companies ranked and scored by mattermark business intelligence dashboards big data publisher ref see also colbegin 3 accounting intelligence analytic applications artificial intelligence marketing business intelligence 2 0 business process discovery business process management business activity monitoring business service management comparison of olap servers customer dynamics data presentation architecture data visualization decision engineering enterprise planning systems infonomics intelligent document document intelligence integrated business planning location intelligence media intelligence meteorological intelligence mobile business intelligence multiway data analysis operational intelligence business information systems business intelligence tools process mining real time business intelligence runtime intelligence sales intelligence test and learn colend references reflist 30em bibliography ralph kimball et al the data warehouse lifecycle toolkit 2nd ed wiley isbn 0 470 47957 4 peter rausch alaa sheta aladdin ayesh business intelligence and performance management theory systems and industrial applications springer verlag u k 2013 isbn 978 1 4471 4865 4 external links http online sju edu resource engineering technology key role hadoop plays in business intelligence the key role hadoop plays in business intelligence and data warehousing st joseph s university cite journal url http cacm acm org magazines 2011 8 114953 an overview of business intelligence technology fulltext title an overview of business intelligence technology date august 2011 accessdate 26 october 2011 first1 surajit last1 chaudhuri first2 umeshwar last2 dayal first3 vivek last3 narasayya journal communications of the acm volume 54 issue 8 pages 88 98 doi 10 1145 1978542 1978562 data warehouse defaultsort business intelligence category business intelligence category financial data analysis category data management category financial technology category information management'
