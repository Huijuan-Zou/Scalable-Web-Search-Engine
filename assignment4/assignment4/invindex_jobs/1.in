<?xml version='1.0' encoding='utf8'?>
<mediawiki><siteinfo>
    <sitename>Wikipedia</sitename>
    <dbname>enwiki</dbname>
    <base>https://en.wikipedia.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.29.0-wmf.9</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace case="first-letter" key="-2">Media</namespace>
      <namespace case="first-letter" key="-1">Special</namespace>
      <namespace case="first-letter" key="0" />
      <namespace case="first-letter" key="1">Talk</namespace>
      <namespace case="first-letter" key="2">User</namespace>
      <namespace case="first-letter" key="3">User talk</namespace>
      <namespace case="first-letter" key="4">Wikipedia</namespace>
      <namespace case="first-letter" key="5">Wikipedia talk</namespace>
      <namespace case="first-letter" key="6">File</namespace>
      <namespace case="first-letter" key="7">File talk</namespace>
      <namespace case="first-letter" key="8">MediaWiki</namespace>
      <namespace case="first-letter" key="9">MediaWiki talk</namespace>
      <namespace case="first-letter" key="10">Template</namespace>
      <namespace case="first-letter" key="11">Template talk</namespace>
      <namespace case="first-letter" key="12">Help</namespace>
      <namespace case="first-letter" key="13">Help talk</namespace>
      <namespace case="first-letter" key="14">Category</namespace>
      <namespace case="first-letter" key="15">Category talk</namespace>
      <namespace case="first-letter" key="100">Portal</namespace>
      <namespace case="first-letter" key="101">Portal talk</namespace>
      <namespace case="first-letter" key="108">Book</namespace>
      <namespace case="first-letter" key="109">Book talk</namespace>
      <namespace case="first-letter" key="118">Draft</namespace>
      <namespace case="first-letter" key="119">Draft talk</namespace>
      <namespace case="first-letter" key="446">Education Program</namespace>
      <namespace case="first-letter" key="447">Education Program talk</namespace>
      <namespace case="first-letter" key="710">TimedText</namespace>
      <namespace case="first-letter" key="711">TimedText talk</namespace>
      <namespace case="first-letter" key="828">Module</namespace>
      <namespace case="first-letter" key="829">Module talk</namespace>
      <namespace case="first-letter" key="2300">Gadget</namespace>
      <namespace case="first-letter" key="2301">Gadget talk</namespace>
      <namespace case="case-sensitive" key="2302">Gadget definition</namespace>
      <namespace case="case-sensitive" key="2303">Gadget definition talk</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>Information retrieval</title>
    <ns>0</ns>
    <id>15271</id>
    <revision>
      <id>744833846</id>
      <parentid>744800746</parentid>
      <timestamp>2016-10-17T18:44:14Z</timestamp>
      <contributor>
        <username>CAPTAIN RAJU</username>
        <id>25523690</id>
      </contributor>
      <minor />
      <comment>Reverted 1 edit by [[Special:Contributions/61.3.77.128|61.3.77.128]] identified as test/vandalism using [[WP:STiki|STiki]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="37974" xml:space="preserve">{{Information science}}

'''Information retrieval''' ('''IR''') is the activity of obtaining [[information]] resources relevant to an information need from a collection of information resources.  Searches can be based on [[Full text search|full-text]] or other content-based indexing.

Automated information retrieval systems are used to reduce what has been called "[[information overload]]". Many [[University|universities]] and [[public library|public libraries]] use IR systems to provide access to books, journals and other documents. [[Web search engine]]s are the most visible [[Information retrieval applications|IR applications]].

== Overview ==

An information retrieval process begins when a user enters a [[query string|query]] into the system. Queries are formal statements of [[information need]]s, for example search strings in web search engines. In information retrieval a query does not uniquely identify a single object in the collection. Instead, several objects may match the query, perhaps with different degrees of [[Relevance (information retrieval)|relevancy]].

An object is an entity that is represented by information in a content collection or [[database]]. User queries are matched against the database information. However, as opposed to classical SQL queries of a database, in information retrieval the results returned may or may not match the query, so results are typically ranked. This ranking of results is a key difference of information retrieval searching compared to database searching.&lt;ref&gt;Jansen, B. J. and Rieh, S. (2010) [https://faculty.ist.psu.edu/jjansen/academic/jansen_theoretical_constructs.pdf The Seventeen Theoretical Constructs of Information Searching and Information Retrieval]. Journal of the American Society for Information Sciences and Technology. 61(8), 1517-1534.&lt;/ref&gt;

Depending on the [[Information retrieval applications|application]] the data objects may be, for example, text documents, images,&lt;ref name=goodron2000&gt;{{cite journal |first=Abby A. |last=Goodrum |title=Image Information Retrieval: An Overview of Current Research |journal=Informing Science |volume=3 |number=2 |year=2000 }}&lt;/ref&gt; audio,&lt;ref name=Foote99&gt;{{cite journal |first=Jonathan |last=Foote |title=An overview of audio information retrieval |journal=Multimedia Systems |year=1999 |publisher=Springer }}&lt;/ref&gt; [[mind maps]]&lt;ref name=Beel2009&gt;{{cite conference|first=Jöran |last=Beel |first2=Bela |last2=Gipp |first3=Jan-Olaf |last3=Stiller |title=Information Retrieval On Mind Maps - What Could It Be Good For? |url=http://www.sciplore.org/publications_en.php |conference=Proceedings of the 5th International Conference on Collaborative Computing: Networking, Applications and Worksharing (CollaborateCom'09) |year=2009 |publisher=IEEE |place=Washington, DC }}&lt;/ref&gt; or videos. Often the documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by document surrogates or [[metadata]].

Most IR systems compute a numeric score on how well each object in the database matches the query, and rank the objects according to this value. The top ranking objects are then shown to the user. The process may then be iterated if the user wishes to refine the query.&lt;ref name="Frakes1992"&gt;{{cite book |last=Frakes |first=William B. |title=Information Retrieval Data Structures &amp; Algorithms |publisher=Prentice-Hall, Inc. |year=1992 |isbn=0-13-463837-9 |url=http://www.scribd.com/doc/13742235/Information-Retrieval-Data-Structures-Algorithms-William-B-Frakes }}&lt;/ref&gt;

== History ==
{{Rquote|right|there is ... a machine called the Univac ... whereby letters and figures are coded as a pattern of magnetic spots on a long steel tape. By this means the text of a document, preceded by its subject code symbol, ca be recorded ... the machine ... automatically selects and types out those references which have been coded in any desired way at a rate of 120 words a minute| J. E. Holmstrom, 1948}}
The idea of using computers to search for relevant pieces of information was popularized in the article ''[[As We May Think]]'' by [[Vannevar Bush]] in 1945.&lt;ref name="Singhal2001"&gt;{{cite journal |last=Singhal |first=Amit |title=Modern Information Retrieval: A Brief Overview |journal=Bulletin of the IEEE Computer Society Technical Committee on Data Engineering|volume=24 |issue=4 |pages=35–43 |year =2001 |url=http://singhal.info/ieee2001.pdf }}&lt;/ref&gt; It would appear that Bush was inspired by patents for a 'statistical machine' - filed by [[Emanuel Goldberg]] in the 1920s and '30s - that searched for documents stored on film.&lt;ref name="Sanderson2012"&gt;{{cite journal |author=Mark Sanderson &amp; W. Bruce Croft |title=The History of Information Retrieval Research |journal=Proceedings of the IEEE |volume=100 |pages=1444–1451 |year =2012 |url=http://dx.doi.org/10.1109/JPROC.2012.2189916 |doi=10.1109/jproc.2012.2189916}}&lt;/ref&gt; The first description of a computer searching for information was described by Holmstrom in 1948,&lt;ref name="Holmstrom1948"&gt;{{cite journal |author=JE Holmstrom |title=‘Section III. Opening Plenary Session |journal=The Royal Society Scientific Information Conference, 21 June-2 July 1948: report and papers submitted |pages=85|year =1948|url=https://books.google.com.au/books?ei=44VxVZrkGYqU8QX4wYPoBA&amp;id=M34lAAAAMAAJ&amp;dq=%E2%80%98Section+III.+Opening+Plenary+Session%22.+The+Royal+Society+Scientific+Information+Conference&amp;focus=searchwithinvolume&amp;q=univac}}&lt;/ref&gt; detailing an early mention of the [[Univac]] computer. Automated information retrieval systems were introduced in the 1950s: one even featured in the 1957 romantic comedy, [[Desk Set]]. In the 1960s, the first large information retrieval research group was formed by [[Gerard Salton]] at Cornell. By the 1970s several different retrieval techniques had been shown to perform well on small [[text corpora]] such as the Cranfield collection (several thousand documents).&lt;ref name="Singhal2001" /&gt; Large-scale retrieval systems, such as the Lockheed Dialog system, came into use early in the 1970s.

In 1992, the US Department of Defense along with the [[National Institute of Standards and Technology]] (NIST), cosponsored the [[Text Retrieval Conference]] (TREC) as part of the TIPSTER text program. The aim of this was to look into the information retrieval community by supplying the infrastructure that was needed for evaluation of text retrieval methodologies on a very large text collection. This catalyzed research on methods that [[scalability|scale]] to huge corpora. The introduction of [[web search engine]]s has boosted the need for very large scale retrieval systems even further.

== Model types ==
[[File:Information-Retrieval-Models.png|thumb|500px|Categorization of IR-models (translated from [[:de:Informationsrückgewinnung#Klassifikation von Modellen zur Repräsentation natürlichsprachlicher Dokumente|German entry]], original source [http://www.logos-verlag.de/cgi-bin/engbuchmid?isbn=0514&amp;lng=eng&amp;id= Dominik Kuropka]).]]
For effectively retrieving relevant documents by IR strategies, the documents are typically transformed into a suitable representation. Each retrieval strategy incorporates a specific model for its document representation purposes. The picture on the right illustrates the relationship of some common models. In the picture, the models are categorized according to two dimensions: the mathematical basis and the properties of the model.

=== First dimension: mathematical basis ===
* ''Set-theoretic'' models represent documents as sets of words or phrases. Similarities are usually derived from set-theoretic operations on those sets. Common models are:
** [[Standard Boolean model]]
** [[Extended Boolean model]]
** [[Fuzzy retrieval]]
* ''Algebraic models'' represent documents and queries usually as vectors, matrices, or tuples. The similarity of the query vector and document vector is represented as a scalar value.
** [[Vector space model]]
** [[Generalized vector space model]]
** [[Topic-based vector space model|(Enhanced) Topic-based Vector Space Model]]
** [[Extended Boolean model]]
** [[Latent semantic indexing]] a.k.a. [[latent semantic analysis]]
* ''Probabilistic models'' treat the process of document retrieval as a probabilistic inference. Similarities are computed as probabilities that a document is relevant for a given query. Probabilistic theorems like the [[Bayes' theorem]] are often used in these models.
** [[Binary Independence Model]]
** [[Probabilistic relevance model]] on which is based the [[Probabilistic relevance model (BM25)|okapi (BM25)]] relevance function
** [[Uncertain inference]]
** [[Language model]]s
** [[Divergence-from-randomness model]]
** [[Latent Dirichlet allocation]]
* ''Feature-based retrieval models'' view documents as vectors of values of ''feature functions'' (or just ''features'') and seek the best way to combine these features into a single relevance score, typically by [[learning to rank]] methods. Feature functions are arbitrary functions of document and query, and as such can easily incorporate almost any other retrieval model as just another feature.

=== Second dimension: properties of the model ===
* ''Models without term-interdependencies'' treat different terms/words as independent. This fact is usually represented in vector space models by the [[orthogonality]] assumption of term vectors or in probabilistic models by an [[Independence (mathematical logic)|independency]] assumption for term variables.
* ''Models with immanent term interdependencies'' allow a representation of interdependencies between terms. However the degree of the interdependency between two terms is defined by the model itself. It is usually directly or indirectly derived (e.g. by [[dimension reduction|dimensional reduction]]) from the [[co-occurrence]] of those terms in the whole set of documents.
* ''Models with transcendent term interdependencies'' allow a representation of interdependencies between terms, but they do not allege how the interdependency between two terms is defined. They rely an external source for the degree of interdependency between two terms. (For example, a human or sophisticated algorithms.)

== Performance and correctness measures ==
{{further|Evaluation measures (information retrieval)}}

The '''evaluation of an information retrieval system''' is the process of assessing how well a system meets the information needs of its users. Traditional evaluation metrics, designed for [[Standard Boolean model|Boolean retrieval]] or top-k retrieval, include [[precision and recall]].  Many more measures for evaluating the performance of information retrieval systems have also been proposed. In general, measurement considers a collection of documents to be searched and a search query. All common measures described here assume a [[ground truth]] notion of relevancy: every document is known to be either relevant or non-relevant to a particular query. In practice, queries may be [[ill-posed]] and there may be different shades of relevancy.

Virtually all modern evaluation metrics (e.g., [[Information retrieval#Mean average precision|mean average precision]], [[Information retrieval#Discounted cumulative gain|discounted cumulative gain]]) are designed for '''ranked retrieval''' without any explicit rank cutoff, taking into account the relative order of the documents retrieved by the search engines and giving more weight to documents returned at higher ranks.{{citation needed|date=June 2015}}

The mathematical symbols used in the formulas below mean:
* &lt;math&gt;X \cap Y&lt;/math&gt; - [[Intersection (set theory)|Intersection]] - in this case, specifying the documents in ''both'' sets X and Y
* &lt;math&gt;| X |&lt;/math&gt; - [[Cardinality]] - in this case, the number of documents in set X
* &lt;math&gt;\int&lt;/math&gt; - [[Integral]]
* &lt;math&gt;\sum&lt;/math&gt; - [[Summation]]
* &lt;math&gt;\Delta&lt;/math&gt; - [[Symmetric difference]]

=== Precision ===
{{main|Precision and recall}}

Precision is the fraction of the documents retrieved that are [[Relevance (information retrieval)|relevant]] to the user's information need.

:&lt;math&gt; \mbox{precision}=\frac{|\{\mbox{relevant documents}\}\cap\{\mbox{retrieved documents}\}|}{|\{\mbox{retrieved documents}\}|} &lt;/math&gt;

In [[binary classification]], precision is analogous to [[positive predictive value]]. Precision takes all retrieved documents into account. It can also be evaluated at a given cut-off rank, considering only the topmost results returned by the system. This measure is called ''precision at n'' or ''P@n''.

Note that the meaning and usage of "precision" in the field of information retrieval differs from the definition of [[accuracy and precision]] within other branches of science and [[statistics]].

=== Recall ===
{{main|Precision and recall}}

Recall is the fraction of the documents that are relevant to the query that are successfully retrieved.

:&lt;math&gt;\mbox{recall}=\frac{|\{\mbox{relevant documents}\}\cap\{\mbox{retrieved documents}\}|}{|\{\mbox{relevant documents}\}|} &lt;/math&gt;

In binary classification, recall is often called [[sensitivity and specificity|sensitivity]]. So it can be looked at as ''the probability that a relevant document is retrieved by the query''.

It is trivial to achieve recall of 100% by returning all documents in response to any query. Therefore, recall alone is not enough but one needs to measure the number of non-relevant documents also, for example by computing the precision.

=== Fall-out ===
The proportion of non-relevant documents that are retrieved, out of all non-relevant documents available:

:&lt;math&gt; \mbox{fall-out}=\frac{|\{\mbox{non-relevant documents}\}\cap\{\mbox{retrieved documents}\}|}{|\{\mbox{non-relevant documents}\}|} &lt;/math&gt;

In binary classification, fall-out is closely related to [[sensitivity and specificity|specificity]] and is equal to &lt;math&gt;(1-\mbox{specificity})&lt;/math&gt;. It can be looked at as ''the probability that a non-relevant document is retrieved by the query''.

It is trivial to achieve fall-out of 0% by returning zero documents in response to any query.

=== F-score / F-measure ===
{{main|F-score}}
The weighted [[harmonic mean]] of precision and recall, the traditional F-measure or balanced F-score is:

:&lt;math&gt;F = \frac{2 \cdot \mathrm{precision} \cdot \mathrm{recall}}{(\mathrm{precision} + \mathrm{recall})}&lt;/math&gt;

This is also known as the &lt;math&gt;F_1&lt;/math&gt; measure, because recall and precision are evenly weighted.

The general formula for non-negative real &lt;math&gt;\beta&lt;/math&gt; is:
:&lt;math&gt;F_\beta = \frac{(1 + \beta^2) \cdot (\mathrm{precision} \cdot \mathrm{recall})}{(\beta^2 \cdot \mathrm{precision} + \mathrm{recall})}\,&lt;/math&gt;

Two other commonly used F measures are the &lt;math&gt;F_{2}&lt;/math&gt; measure, which weights recall twice as much as precision, and the &lt;math&gt;F_{0.5}&lt;/math&gt; measure, which weights precision twice as much as recall.

The F-measure was derived by van Rijsbergen (1979) so that &lt;math&gt;F_\beta&lt;/math&gt; "measures the effectiveness of retrieval with respect to a user who attaches &lt;math&gt;\beta&lt;/math&gt; times as much importance to recall as precision".  It is based on van Rijsbergen's effectiveness measure &lt;math&gt;E = 1 - \frac{1}{\frac{\alpha}{P} + \frac{1-\alpha}{R}}&lt;/math&gt;.  Their relationship is:
:&lt;math&gt;F_\beta = 1 - E&lt;/math&gt; where &lt;math&gt;\alpha=\frac{1}{1 + \beta^2}&lt;/math&gt;

F-measure can be a better single metric when compared to precision and recall; both precision and recall give different information that can complement each other when combined. If one of them excels more than the other, F-measure will reflect it.{{citation needed|date=June 2015}}

=== Average precision ===
&lt;!-- [[Average precision]] redirects here --&gt;
Precision and recall are single-value metrics based on the whole list of documents returned by the system. For systems that return a ranked sequence of documents, it is desirable to also consider the order in which the returned documents are presented. By computing a precision and recall at every position in the ranked sequence of documents, one can plot a precision-recall curve, plotting precision &lt;math&gt;p(r)&lt;/math&gt; as a function of recall &lt;math&gt;r&lt;/math&gt;. Average precision computes the average value of &lt;math&gt;p(r)&lt;/math&gt; over the interval from &lt;math&gt;r=0&lt;/math&gt; to &lt;math&gt;r=1&lt;/math&gt;:&lt;ref name="zhu2004"&gt;{{cite journal |first=Mu |last=Zhu |title=Recall, Precision and Average Precision |url=http://sas.uwaterloo.ca/stats_navigation/techreports/04WorkingPapers/2004-09.pdf |year=2004 }}&lt;/ref&gt;
:&lt;math&gt;\operatorname{AveP} = \int_0^1 p(r)dr&lt;/math&gt;
That is the area under the precision-recall curve.
This integral is in practice replaced with a finite sum over every position in the ranked sequence of documents:
:&lt;math&gt;\operatorname{AveP} = \sum_{k=1}^n P(k) \Delta r(k)&lt;/math&gt;
where &lt;math&gt;k&lt;/math&gt; is the rank in the sequence of retrieved documents, &lt;math&gt;n&lt;/math&gt; is the number of retrieved documents, &lt;math&gt;P(k)&lt;/math&gt; is the precision at cut-off &lt;math&gt;k&lt;/math&gt; in the list, and &lt;math&gt;\Delta r(k)&lt;/math&gt; is the change in recall from items &lt;math&gt;k-1&lt;/math&gt; to &lt;math&gt;k&lt;/math&gt;.&lt;ref name="zhu2004" /&gt;

This finite sum is equivalent to:
:&lt;math&gt; \operatorname{AveP} = \frac{\sum_{k=1}^n (P(k) \times \operatorname{rel}(k))}{\mbox{number of relevant documents}} \!&lt;/math&gt;
where &lt;math&gt;\operatorname{rel}(k)&lt;/math&gt; is an indicator function equaling 1 if the item at rank &lt;math&gt;k&lt;/math&gt; is a relevant document, zero otherwise.&lt;ref name="Turpin2006"&gt;{{cite journal |last=Turpin |first=Andrew |last2=Scholer |first2=Falk |title=User performance versus precision measures for simple search tasks |journal=Proceedings of the 29th Annual international ACM SIGIR Conference on Research and Development in information Retrieval (Seattle, WA, August 06–11, 2006) |publisher=ACM |location=New York, NY |pages=11–18 |doi=10.1145/1148170.1148176 |year=2006 |isbn=1-59593-369-7 }}&lt;/ref&gt; Note that the average is over all relevant documents and the relevant documents not retrieved get a precision score of zero.

Some authors choose to interpolate the &lt;math&gt;p(r)&lt;/math&gt; function to reduce the impact of "wiggles" in the curve.&lt;ref name=voc2010&gt;{{cite journal |last=Everingham |first=Mark |last2=Van Gool |first2=Luc |last3=Williams |first3=Christopher K. I. |last4=Winn |first4=John |last5=Zisserman |first5=Andrew |title=The PASCAL Visual Object Classes (VOC) Challenge |journal=International Journal of Computer Vision |volume=88 |issue=2 |pages=303–338 |publisher=Springer |date=June 2010 |url=http://pascallin.ecs.soton.ac.uk/challenges/VOC/pubs/everingham10.pdf |accessdate=2011-08-29 |doi=10.1007/s11263-009-0275-4 }}&lt;/ref&gt;&lt;ref name="nlpbook"&gt;{{cite book |last=Manning |first=Christopher D. |last2=Raghavan |first2=Prabhakar |last3=Schütze |first3=Hinrich |title=Introduction to Information Retrieval |publisher=Cambridge University Press |year=2008 |url=http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html }}&lt;/ref&gt; For example, the PASCAL Visual Object Classes challenge (a benchmark for computer vision object detection) computes average precision by averaging the precision over a set of evenly spaced recall levels {0, 0.1, 0.2, ... 1.0}:&lt;ref name="voc2010" /&gt;&lt;ref name="nlpbook" /&gt;
:&lt;math&gt;\operatorname{AveP} = \frac{1}{11} \sum_{r \in \{0, 0.1, \ldots, 1.0\}} p_{\operatorname{interp}}(r)&lt;/math&gt;
where &lt;math&gt;p_{\operatorname{interp}}(r)&lt;/math&gt; is an interpolated precision that takes the maximum precision over all recalls greater than &lt;math&gt;r&lt;/math&gt;:
:&lt;math&gt;p_{\operatorname{interp}}(r) = \operatorname{max}_{\tilde{r}:\tilde{r} \geq r} p(\tilde{r})&lt;/math&gt;.

An alternative is to derive an analytical &lt;math&gt;p(r)&lt;/math&gt; function by assuming a particular parametric distribution for the underlying decision values. For example, a ''binormal precision-recall curve'' can be obtained by assuming decision values in both classes to follow a Gaussian distribution.&lt;ref&gt;K.H. Brodersen, C.S. Ong, K.E. Stephan, J.M. Buhmann (2010). [http://icpr2010.org/pdfs/icpr2010_ThBCT8.28.pdf The binormal assumption on precision-recall curves]. ''Proceedings of the 20th International Conference on Pattern Recognition'', 4263-4266.&lt;/ref&gt;

=== Precision at K ===

For modern (Web-scale) information retrieval, recall is no longer a meaningful metric, as many queries have thousands of relevant documents, and few users will be interested in reading all of them. [[Precision and recall#Precision|Precision]] at k documents (P@k) is still a useful metric (e.g., P@10 or "Precision at 10" corresponds to the number of relevant results on the first search results page), but fails to take into account the positions of the relevant documents among the top k.{{citation needed|date=June 2015}}  Another shortcoming is that on a query with fewer relevant results than k, even a perfect system will have a score less than 1.&lt;ref name="stanford" /&gt;  It is easier to score manually since only the top k results need to be examined to determine if they are relevant or not.

=== R-Precision ===

R-precision requires knowing all documents that are relevant to a query.  The number of relevant documents, &lt;math&gt;R&lt;/math&gt;, is used as the cutoff for calculation, and this varies from query to query.  For example, if there are 15 documents relevant to "red" in a corpus (R=15), R-precision for "red" looks at the top 15 documents returned, counts the number that are relevant &lt;math&gt;r&lt;/math&gt; turns that into a relevancy fraction: &lt;math&gt;r/R = r/15&lt;/math&gt;.&lt;ref name="trec15"/&gt;

Precision is equal to recall at the '''R'''-th position.&lt;ref name="stanford"&gt;{{cite web|url=http://nlp.stanford.edu/IR-book/pdf/08eval.pdf|title=Chapter 8: Evaluation in information retrieval|accessdate=2015-06-14|date=2009|authors=Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze}}  Part of ''Introduction to Information Retrieval'' [http://nlp.stanford.edu/IR-book/]&lt;/ref&gt;

Empirically, this measure is often highly correlated to mean average precision.&lt;ref name="stanford" /&gt;

=== Mean average precision ===
&lt;!-- [[Mean average precision]] redirects here --&gt;
Mean average precision for a set of queries is the mean of the average precision scores for each query.
:&lt;math&gt; \operatorname{MAP} = \frac{\sum_{q=1}^Q \operatorname{AveP(q)}}{Q} \!&lt;/math&gt;
where ''Q'' is the number of queries.

=== Discounted cumulative gain ===
{{main|Discounted cumulative gain}}
DCG uses a graded relevance scale of documents from the result set to evaluate the usefulness, or gain, of a document based on its position in the result list. The premise of DCG is that highly relevant documents appearing lower in a search result list should be penalized as the graded relevance value is reduced logarithmically proportional to the position of the result.

The DCG accumulated at a particular rank position &lt;math&gt;p&lt;/math&gt; is defined as:

:&lt;math&gt; \mathrm{DCG_{p}} = rel_{1} + \sum_{i=2}^{p} \frac{rel_{i}}{\log_{2}i}. &lt;/math&gt;

Since result set may vary in size among different queries or systems, to compare performances the normalised version of DCG uses an ideal DCG. To this end, it sorts documents of a result list by relevance, producing an ideal DCG at position p (&lt;math&gt;IDCG_p&lt;/math&gt;), which normalizes the score:

:&lt;math&gt; \mathrm{nDCG_{p}} = \frac{DCG_{p}}{IDCG{p}}. &lt;/math&gt;

The nDCG values for all queries can be averaged to obtain a measure of the average performance of a ranking algorithm. Note that in a perfect ranking algorithm, the &lt;math&gt;DCG_p&lt;/math&gt; will be the same as the &lt;math&gt;IDCG_p&lt;/math&gt; producing an nDCG of 1.0. All nDCG calculations are then relative values on the interval 0.0 to 1.0 and so are cross-query comparable.

=== Other measures ===
{{Confusion matrix terms}}
* [[Mean reciprocal rank]]
* [[Spearman's rank correlation coefficient]]
* bpref - a summation-based measure of how many relevant documents are ranked before irrelevant documents&lt;ref name="trec15"&gt;http://trec.nist.gov/pubs/trec15/appendices/CE.MEASURES06.pdf&lt;/ref&gt;
* GMAP - geometric mean of (per-topic) average precision&lt;ref name="trec15" /&gt;
* Measures based on marginal relevance and document diversity - see {{section link|Relevance (information retrieval)|Problems and alternatives}}

===Visualization===

Visualizations of information retrieval performance include:
* Graphs which chart precision on one axis and recall on the other&lt;ref name="trec15" /&gt;
* Histograms of average precision over various topics&lt;ref name="trec15" /&gt;
* [[Receiver operating characteristic]] (ROC curve)
* [[Confusion matrix]]

== Timeline ==

* Before the '''1900s'''
*: '''1801''': [[Joseph Marie Jacquard]] invents the [[Jacquard loom]], the first machine to use punched cards to control a sequence of operations.
*: '''1880s''': [[Herman Hollerith]] invents an electro-mechanical data tabulator using punch cards as a machine readable medium.
*: '''1890''' Hollerith [[Punched cards|cards]], [[keypunch]]es and [[Tabulating machine|tabulators]] used to process the [[1890 US Census]] data.
* '''1920s-1930s'''
*: [[Emanuel Goldberg]] submits patents for his "Statistical Machine” a document search engine that used photoelectric cells and pattern recognition to search the metadata on rolls of microfilmed documents.
* '''1940s–1950s'''
*: '''late 1940s''': The US military confronted problems of indexing and retrieval of wartime scientific research documents captured from Germans.
*:: '''1945''': [[Vannevar Bush]]'s ''[[As We May Think]]'' appeared in ''[[Atlantic Monthly]]''.
*:: '''1947''': [[Hans Peter Luhn]] (research engineer at IBM since 1941) began work on a mechanized punch card-based system for searching chemical compounds.
*: '''1950s''': Growing concern in the US for a "science gap" with the USSR motivated, encouraged funding and provided a backdrop for mechanized literature searching systems ([[Allen Kent]] ''et al.'') and the invention of citation indexing ([[Eugene Garfield]]).
*: '''1950''': The term "information retrieval" was coined by [[Calvin Mooers]].&lt;ref&gt;Mooers, Calvin N.; ''[https://babel.hathitrust.org/cgi/pt?id=mdp.39015034570591;view=1up;seq=3 The Theory of Digital Handling of Non-numerical Information and its Implications to Machine Economics]'' (Zator Technical Bulletin No. 48), cited in {{cite journal|last1=Fairthorne|first1=R. A.|title=Automatic Retrieval of Recorded Information|journal=The Computer Journal|date=1958|volume=1|issue=1|page=37|doi=10.1093/comjnl/1.1.36|url=http://comjnl.oxfordjournals.org/content/1/1/36.short}}&lt;/ref&gt;
*: '''1951''': Philip Bagley conducted the earliest experiment in computerized document retrieval in a master thesis at [[MIT]].&lt;ref name="Doyle1975"&gt;{{cite book |last=Doyle |first=Lauren |last2=Becker |first2=Joseph |title=Information Retrieval and Processing |publisher=Melville |year=1975 |pages=410 pp. |isbn=0-471-22151-1 }}&lt;/ref&gt;
*: '''1955''': Allen Kent joined [[Case Western Reserve University]], and eventually became associate director of the Center for Documentation and Communications Research. That same year, Kent and colleagues published a paper in American Documentation describing the precision and recall measures as well as detailing a proposed "framework" for evaluating an IR system which included statistical sampling methods for determining the number of relevant documents not retrieved.&lt;ref&gt;{{cite journal |title=Machine literature searching X. Machine language; factors underlying its design and development |doi=10.1002/asi.5090060411}}&lt;/ref&gt;
*: '''1958''': International Conference on Scientific Information Washington DC included consideration of IR systems as a solution to problems identified. See: ''Proceedings of the International Conference on Scientific Information, 1958'' (National Academy of Sciences, Washington, DC, 1959)
*: '''1959''': [[Hans Peter Luhn]] published "Auto-encoding of documents for information retrieval."
* '''1960s''':
*: '''early 1960s''': [[Gerard Salton]] began work on IR at Harvard, later moved to Cornell.
*: '''1960''': [[Melvin Earl Maron]] and John Lary&lt;!-- sic --&gt; Kuhns&lt;ref name="Maron2008"&gt;{{cite journal |title=An Historical Note on the Origins of Probabilistic Indexing |last=Maron | first=Melvin E. |journal=Information Processing and Management |volume=44 |year=2008 |pages=971–972 |url=http://yunus.hacettepe.edu.tr/~tonta/courses/spring2008/bby703/maron-on-probabilistic%20indexing-2008.pdf |doi=10.1016/j.ipm.2007.02.012 |issue=2 }}&lt;/ref&gt; published "On relevance, probabilistic indexing, and information retrieval" in the Journal of the ACM 7(3):216–244, July 1960.
*: '''1962''':
*:* [[Cyril W. Cleverdon]] published early findings of the Cranfield studies, developing a model for IR system evaluation. See: Cyril W. Cleverdon, "Report on the Testing and Analysis of an Investigation into the Comparative Efficiency of Indexing Systems". Cranfield Collection of Aeronautics, Cranfield, England, 1962.
*:* Kent published ''Information Analysis and Retrieval''.
*: '''1963''':
*:* Weinberg report "Science, Government and Information" gave a full articulation of the idea of a "crisis of scientific information."  The report was named after Dr. [[Alvin Weinberg]].
*:* Joseph Becker and [[Robert M. Hayes]] published text on information retrieval. Becker, Joseph; Hayes, Robert Mayo. ''Information storage and retrieval: tools, elements, theories''. New York, Wiley (1963).
*: '''1964''':
*:* [[Karen Spärck Jones]] finished her thesis at Cambridge, ''Synonymy and Semantic Classification'', and continued work on [[computational linguistics]] as it applies to IR.
*:* The [[National Bureau of Standards]] sponsored a symposium titled "Statistical Association Methods for Mechanized Documentation." Several highly significant papers, including G. Salton's first published reference (we believe) to the [[SMART Information Retrieval System|SMART]] system.
*:'''mid-1960s''':
*::* National Library of Medicine developed [[MEDLARS]] Medical Literature Analysis and Retrieval System, the first major machine-readable database and batch-retrieval system.
*::* Project Intrex at MIT.
*:: '''1965''': [[J. C. R. Licklider]] published ''Libraries of the Future''.
*:: '''1966''': [[Don Swanson]] was involved in studies at University of Chicago on Requirements for Future Catalogs.
*: '''late 1960s''': [[F. Wilfrid Lancaster]] completed evaluation studies of the MEDLARS system and published the first edition of his text on information retrieval.
*:: '''1968''':
*:* Gerard Salton published ''Automatic Information Organization and Retrieval''.
*:* John W. Sammon, Jr.'s RADC Tech report "Some Mathematics of Information Storage and Retrieval..." outlined the vector model.
*:: '''1969''': Sammon's "A nonlinear mapping for data structure analysis" (IEEE Transactions on Computers) was the first proposal for visualization interface to an IR system.
* '''1970s'''
*: '''early 1970s''':
*::* First online systems—NLM's AIM-TWX, MEDLINE; Lockheed's Dialog; SDC's ORBIT.
*::* [[Theodor Nelson]] promoting concept of [[hypertext]], published ''Computer Lib/Dream Machines''.
*: '''1971''': [[Nicholas Jardine]] and [[Cornelis J. van Rijsbergen]] published "The use of hierarchic clustering in information retrieval", which articulated the "cluster hypothesis."&lt;ref&gt;{{cite journal|author=N. Jardine, C.J. van Rijsbergen|title=The use of hierarchic clustering in information retrieval|journal=Information Storage and Retrieval|volume=7|issue=5|pages=217–240|date=December 1971|doi=10.1016/0020-0271(71)90051-9}}&lt;/ref&gt;
*: '''1975''': Three highly influential publications by Salton fully articulated his vector processing framework and term discrimination model:
*::* ''A Theory of Indexing'' (Society for Industrial and Applied Mathematics)
*::* ''A Theory of Term Importance in Automatic Text Analysis'' ([[JASIS]] v. 26)
*::* ''A Vector Space Model for Automatic Indexing'' ([[Communications of the ACM|CACM]] 18:11)
*: '''1978''': The First [[Association for Computing Machinery|ACM]] [[Special Interest Group on Information Retrieval|SIGIR]] conference.
*: '''1979''': C. J. van Rijsbergen published ''Information Retrieval'' (Butterworths). Heavy emphasis on probabilistic models.
*: '''1979''': Tamas Doszkocs implemented the CITE natural language user interface for MEDLINE at the National Library of Medicine. The CITE system supported free form query input, ranked output and relevance feedback.&lt;ref&gt;Doszkocs, T.E. &amp; Rapp, B.A. (1979). "Searching MEDLINE in English: a Prototype User Inter-face with Natural Language Query, Ranked Output, and relevance feedback," In: Proceedings of the ASIS Annual Meeting, 16: 131-139.&lt;/ref&gt;
* '''1980s'''
*: '''1980''': First international ACM SIGIR conference, joint with British Computer Society IR group in Cambridge.
*: '''1982''': [[Nicholas J. Belkin]], Robert N. Oddy, and Helen M. Brooks proposed the ASK (Anomalous State of Knowledge) viewpoint for information retrieval. This was an important concept, though their automated analysis tool proved ultimately disappointing.
*: '''1983''': Salton (and Michael J. McGill) published ''Introduction to Modern Information Retrieval'' (McGraw-Hill), with heavy emphasis on vector models.
*: '''1985''': David Blair and [[Bill Maron]] publish: An Evaluation of Retrieval Effectiveness for a Full-Text Document-Retrieval System
*: '''mid-1980s''': Efforts to develop end-user versions of commercial IR systems.
*:: '''1985–1993''': Key papers on and experimental systems for visualization interfaces.
*:: Work by [[Donald B. Crouch]], [[Robert R. Korfhage]], Matthew Chalmers, Anselm Spoerri and others.
*: '''1989''': First [[World Wide Web]] proposals by [[Tim Berners-Lee]] at [[CERN]].
* '''1990s'''
*: '''1992''': First [[Text Retrieval Conference|TREC]] conference.
*: '''1997''': Publication of [[Robert R. Korfhage|Korfhage]]'s ''Information Storage and Retrieval''&lt;ref name="Korfhage1997"&gt;{{cite book |last=Korfhage |first=Robert R. |title=Information Storage and Retrieval |publisher=Wiley |year=1997 |pages=368 pp. |isbn=978-0-471-14338-3 |url=http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471143383,descCd-authorInfo.html }}&lt;/ref&gt; with emphasis on visualization and multi-reference point systems.
*: '''late 1990s''': [[Web search engine]]s implementation of many features formerly found only in experimental IR systems. Search engines become the most common and maybe best instantiation of IR models.

== Awards in the field ==

* [[Tony Kent Strix award]]
* [[Gerard Salton Award]]

== Leading IR Research Groups ==
* [[Center for Intelligent Information Retrieval]] (CIIR) at the University of Massachusetts Amherst &lt;ref&gt;{{Cite web|url=http://ciir.cs.umass.edu|title=Center for Intelligent Information Retrieval {{!}} UMass Amherst|website=ciir.cs.umass.edu|access-date=2016-07-29}}&lt;/ref&gt;
* Information Retrieval Group at the University of Glasgow &lt;ref&gt;{{Cite web|url=http://www.gla.ac.uk/schools/computing/research/researchoverview/informationretrieval/|title=University of Glasgow - Schools - School of Computing Science - Research - Research overview - Information Retrieval|website=www.gla.ac.uk|access-date=2016-07-29}}&lt;/ref&gt;
* Information and Language Processing Systems (ILPS) at the University of Amsterdam &lt;ref&gt;{{Cite web|url=http://ilps.science.uva.nl/|title=ILPS - information and language processing systems|website=ILPS|language=en-US|access-date=2016-07-29}}&lt;/ref&gt;
* Language Technologies Institutes (LTI) at the Carnegie Mellon University
* Text Information Management and Analysis Group (TIMAN) at  the University of Illinois at Urbana-Champaign

==See also==

{{div col}}

* [[Adversarial information retrieval]]
* [[Collaborative information seeking]]
* [[Controlled vocabulary]]
* [[Cross-language information retrieval]]
* [[Data mining]]
* [[European Summer School in Information Retrieval]]
* [[Human–computer information retrieval]] (HCIR)
* [[Information extraction]]
* [[Information Retrieval Facility]]
* [[Knowledge visualization]]
* [[Multimedia information retrieval]]
* [[Personal information management]]
* [[Relevance (Information Retrieval)]]
* [[Relevance feedback]]
* [[Rocchio Classification]]
* [[Index (search engine)|Search index]]
* [[Social information seeking]]
* [[Special Interest Group on Information Retrieval]]
* [[Subject indexing]]
* [[Temporal information retrieval]]
* [[tf-idf]]
* [[XML-Retrieval]]

{{div col end}}

== References ==
{{reflist}}

==Further reading==
* Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch&amp;uuml;tze. [http://www-csli.stanford.edu/~hinrich/information-retrieval-book.html Introduction to Information Retrieval]. Cambridge University Press, 2008.
*Stefan B&amp;uuml;ttcher, Charles L. A. Clarke, and Gordon V. Cormack. [http://www.ir.uwaterloo.ca/book/ Information Retrieval: Implementing and Evaluating Search Engines]. MIT Press, Cambridge, Mass., 2010.

==External links==
{{wikiquote}}
* [http://www.acm.org/sigir/ ACM SIGIR: Information Retrieval Special Interest Group]
* [http://irsg.bcs.org/ BCS IRSG: British Computer Society - Information Retrieval Specialist Group]
* [http://trec.nist.gov Text Retrieval Conference (TREC)]
* [http://www.isical.ac.in/~fire Forum for Information Retrieval Evaluation (FIRE)]
* [http://www.dcs.gla.ac.uk/Keith/Preface.html Information Retrieval] (online book) by [[C. J. van Rijsbergen]]
* [http://ir.dcs.gla.ac.uk/wiki/ Information Retrieval Wiki]
* [http://ir-facility.org/ Information Retrieval Facility]
* [http://www.nonrelevant.net Information Retrieval @ DUTH]
* [http://trec.nist.gov/pubs/trec15/appendices/CE.MEASURES06.pdf TREC report on information retrieval evaluation techniques]
* [http://www.ebaytechblog.com/2010/11/10/measuring-search-relevance/ How eBay measures search relevance]
* [http://retrieval.ceti.gr Information retrieval performance evaluation tool @ Athena Research Centre]

{{Authority control}}

{{DEFAULTSORT:Information Retrieval}}
[[Category:Articles with inconsistent citation formats]]
[[Category:Information retrieval| ]]
[[Category:Natural language processing]]</text>
      <sha1>6kxixhrxiaty7w8zd4znvgjcio79sds</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Information retrieval evaluation</title>
    <ns>14</ns>
    <id>46965336</id>
    <revision>
      <id>666933492</id>
      <parentid>666713992</parentid>
      <timestamp>2015-06-14T17:58:19Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>The main overview for this category is at {{section link|Information retrieval|Performance and correctness_measures}}.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="154" xml:space="preserve">The main overview for this category is at {{section link|Information retrieval|Performance and correctness_measures}}.

[[Category:Information retrieval]]</text>
      <sha1>m98afgn44mtneoi1kmkjgfk5zton892</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Information retrieval techniques</title>
    <ns>14</ns>
    <id>46965346</id>
    <revision>
      <id>666714185</id>
      <timestamp>2015-06-13T03:44:13Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>Category:Information retrieval</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="34" xml:space="preserve">[[Category:Information retrieval]]</text>
      <sha1>dobw7kl3saam8hrak0oyyn1jbi7dqlz</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Information retrieval genres</title>
    <ns>14</ns>
    <id>46965446</id>
    <revision>
      <id>666716254</id>
      <timestamp>2015-06-13T04:14:38Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>Category:Information retrieval</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="34" xml:space="preserve">[[Category:Information retrieval]]</text>
      <sha1>dobw7kl3saam8hrak0oyyn1jbi7dqlz</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Information retrieval researchers</title>
    <ns>14</ns>
    <id>46967136</id>
    <revision>
      <id>666733941</id>
      <timestamp>2015-06-13T08:06:20Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <comment>[[WP:AES|←]]Created page with '[[Category:Computer scientists by field of research|Information retrieval]] [[Category:Information retrieval]]'</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="110" xml:space="preserve">[[Category:Computer scientists by field of research|Information retrieval]]
[[Category:Information retrieval]]</text>
      <sha1>3gw4qibpvqdsspfc9rlr58azftr4b7x</sha1>
    </revision>
  </page>
  <page>
    <title>Dwell time (information retrieval)</title>
    <ns>0</ns>
    <id>48317971</id>
    <revision>
      <id>726341772</id>
      <parentid>694217514</parentid>
      <timestamp>2016-06-21T15:24:15Z</timestamp>
      <contributor>
        <ip>92.27.3.65</ip>
      </contributor>
      <comment>Added a new reference to better explain the topic of dwell time</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1197" xml:space="preserve">{{one source|date=October 2015}}

In [[information retrieval]], '''dwell time''' denotes the time which a user spends viewing a document after clicking a link on a [[Search engine results page|search engine results page (SERP)]]. Dwell time is the duration between when a user clicks on a [[search engine]] result, and when the user returns from that result, or the user is otherwise seen to have left the result. Dwell time is a [[Relevance (information retrieval)|relevance]] indicator of the search result correctly satisfying the [[Information needs|intent]] of the user. Short dwell times indicate the user's query intent was not satisfied by viewing the result. Long dwell times indicate the user's query intent was satisfied.&lt;ref&gt;{{Cite web|url=https://blogs.bing.com/webmaster/2011/08/02/how-to-build-quality-content/|title=How To Build Quality Content|publisher=Bing blogs}}&lt;/ref&gt; 

==References==
&lt;references /&gt;2. [https://www.impression.co.uk/blog/4004/dwell-time/ "Understanding dwell time and its impact on search rankings"] Impression Digital
[[Category:Information retrieval]]
[[Category:Information retrieval evaluation]]
[[Category:Internet search engines]]

{{web-software-stub}}</text>
      <sha1>01hndryu1aknulzl4p5fxjodbu3lz9f</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Knowledge representation</title>
    <ns>14</ns>
    <id>796635</id>
    <revision>
      <id>742239637</id>
      <parentid>735894183</parentid>
      <timestamp>2016-10-02T14:39:38Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <comment>removed [[Category:Information, knowledge, and uncertainty]], this is a subfield of microeconomics</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="506" xml:space="preserve">&lt;!--''Main article : [[Knowledge representation and reasoning]]''--&gt;
{{Cat main|Knowledge representation}}

Significant articles:
* [[Library classification]]
* [[Ontology (computer science)]]
* [[Semantic network]]
{{Category TOC}}

{{Commons cat|Knowledge representation}}

[[Category:Artificial intelligence]]
[[Category:Information science]]
[[Category:Knowledge engineering]]
[[Category:Programming paradigms]]
[[Category:Reasoning]]
[[Category:Scientific modeling]]
[[Category:Information retrieval]]</text>
      <sha1>ixypgtjat1ssnen0u68ykbdl5w08eb1</sha1>
    </revision>
  </page>
  <page>
    <title>Wiener connector</title>
    <ns>0</ns>
    <id>45655492</id>
    <revision>
      <id>759706479</id>
      <parentid>725381093</parentid>
      <timestamp>2017-01-12T18:57:49Z</timestamp>
      <contributor>
        <ip>68.181.207.217</ip>
      </contributor>
      <comment>added link to references and added categories</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8842" xml:space="preserve">{{Orphan|date=February 2016}}

In mathematics applied to the study of networks, the '''Wiener connector''', named in honor of chemist [[Harry Wiener]] who first introduced the [[Wiener Index]], is a means of maximizing efficiency in connecting specified "query vertices" in a network. Given a [[connected graph|connected]], [[undirected graph]] and a set of query vertices in a graph, the '''minimum Wiener connector''' is an [[induced subgraph]] that connects the query vertices and minimizes the sum of [[shortest path]] distances among all pairs of vertices in the subgraph. In [[combinatorial optimization]], the '''minimum Wiener connector problem''' is the problem of finding the minimum Wiener connector. It can be thought of as a version of the classic [[Steiner tree problem]] (one of [[Karp's 21 NP-complete problems]]), where instead of minimizing the size of the tree, the objective is to minimize the distances in the subgraph.&lt;ref name="steiner"&gt;{{cite journal|last1=Hwang|first1=Frank|last2=Richards|first2=Dana|last3=Winter|first3=Dana|last4=Winter|first4=Pawel|title=The Steiner Tree Problem|journal=Annals of Discrete Mathematics|date=1992|url=http://www.sciencedirect.com/science/bookseries/01675060/53}}&lt;/ref&gt;&lt;ref name="dimacs"&gt;[http://dimacs11.cs.princeton.edu/ DIMACS Steiner Tree Challenge]&lt;/ref&gt;

The minimum Wiener connector was first presented by Ruchansky, et al. in 2015.&lt;ref name="sigmod"&gt;{{cite journal|last2=Bonchi|first2=Francesco|last3=Garcia-Soriano|first3=David|last4=Gullo|first4=Francesco|last5=Kourtellis|first5=Nicolas|date=2015|year=|title=The Minimum Wiener Connector|url=https://arxiv.org/abs/1504.00513|journal=SIGMOD|volume=|pages=|via=|last1=Ruchansky|first1=Natali}}&lt;/ref&gt;

The minimum Wiener connector has applications in many domains where there is a graph structure and an interest in learning about connections between sets of individuals. For example, given a set of patients infected with a viral disease, which other patients should be checked to find the culprit? Or given a set of proteins of interest, which other proteins participate in pathways with them?

==Problem definition==
The [[Wiener index]] is the sum of shortest path distances in a (sub)graph. Using &lt;math&gt;d(u,v)&lt;/math&gt; to denote the shortest path between &lt;math&gt;u&lt;/math&gt; and &lt;math&gt;v&lt;/math&gt;, the Wiener index of a (sub)graph &lt;math&gt;S&lt;/math&gt;, denoted &lt;math&gt;W(S)&lt;/math&gt;, is defined as
: &lt;math&gt;W(S) = \sum_{(u, v) \in S} d(u,v)&lt;/math&gt;.

The minimum Wiener connector problem is defined as follows. Given an undirected and unweighted graph with vertex set &lt;math&gt;V&lt;/math&gt; and edge set &lt;math&gt;E&lt;/math&gt; and a set of query vertices &lt;math&gt;Q\subseteq V&lt;/math&gt;, find a connector &lt;math&gt;H\subseteq V&lt;/math&gt; of minimum Wiener index. More formally, the problem is to compute
: &lt;math&gt;\operatorname*{arg\,min}_H W(H\cup Q)&lt;/math&gt;,
that is, find a connector &lt;math&gt;H&lt;/math&gt; that minimizes the sum of shortest paths in &lt;math&gt;H&lt;/math&gt;.

==Relationship to Steiner tree==
[[File:SteinerExample nicer.pdf|thumb|upright=2.0|The optimal solutions to the Steiner tree problem and the minimum Wiener connector can differ. Define the set of query vertices ''Q'' by ''Q'' = {''v''&lt;sub&gt;1&lt;/sub&gt;, &amp;hellip;, ''v''&lt;sub&gt;10&lt;/sub&gt;}. The unique optimal solution to the Steiner tree problem is ''Q'' itself, which has Wiener index 165, whereas the optimal solution for the minimum Wiener connector problem is ''Q'' ∪ {''r''&lt;sub&gt;1&lt;/sub&gt;, ''r''&lt;sub&gt;2&lt;/sub&gt;}, which has Wiener index 142.]]
The minimum Wiener connector problem is related to the [[Steiner tree problem]]. In the former, the [[objective function]] in the minimization is the Wiener index of the connector, whereas in the latter, the objective function is the sum of the weights of the edges in the connector. The optimum solutions to these problems may differ, given the same graph and set of query vertices. In fact, a solution for the Steiner tree problem may be arbitrarily bad for the minimum Wiener connector problem; the graph on the right provides an example.

== Computational complexity ==

===Hardness===
The problem is [[NP-hard]], and does not admit a [[polynomial-time approximation scheme]] unless [[P = NP|'''P''' = '''NP''']].&lt;ref name="sigmod"/&gt; This can be proven using the [[inapproximability]] of [[vertex cover]] in bounded degree graphs.&lt;ref name="dinursafra"&gt;{{cite journal|last1=Dinur|first1=Irit|last2=Safra|first2=Samuel|title=On the hardness of approximating minimum vertex cover|journal=Annals of Mathematics|date=2005}}&lt;/ref&gt; Although there is no polynomial-time approximation scheme, there is a polynomial-time constant-factor approximation—an algorithm that finds a connector whose Wiener index is within a constant multiplicative factor of the Wiener index of the optimum connector. In terms of [[complexity class]]es, the minimum Wiener connector problem is in '''[[APX]]''' but is not in '''PTAS''' unless '''P''' = '''NP'''.

=== Exact algorithms ===
An exhaustive search over all possible subsets of vertices to find the one that induces the connector of minimum Wiener index yields an algorithm that finds the optimum solution in &lt;math&gt;2^{O(n)}&lt;/math&gt; time (that is, [[exponential time]]) on graphs with ''n'' vertices. In the special case that there are exactly two query vertices, the optimum solution is the [[shortest path]] joining the two vertices, so the problem can be solved in [[polynomial time]] by computing the shortest path. In fact, for any fixed constant number of query vertices, an optimum solution can be found in polynomial time.

=== Approximation algorithms ===
There is a constant-factor approximation algorithm for the minimum Wiener connector problem that runs in time &lt;math&gt;O(q (m \log n + n \log^2 n))&lt;/math&gt; on a graph with ''n'' vertices, ''m'' edges, and ''q'' query vertices, roughly the same time it takes to compute shortest-path distances from the query vertices to every other vertex in the graph.&lt;ref name="sigmod"/&gt; The central approach of this algorithm is to reduce the problem to the vertex-weighted Steiner tree problem, which admits a constant-factor approximation in particular instances related to the minimum Wiener connector problem.

==Behavior==

The minimum Wiener connector behaves like [[Centrality#Betweenness centrality|betweenness centrality]].

When the query vertices belong to the same community, the non-query vertices that form the minimum Wiener connector tend to belong to the same community and have high centrality within the community.  Such vertices are likely to be [[influential]] vertices playing leadership roles in the community. In a [[social network]], these influential vertices might be good users for spreading information or to target in a viral marketing campaign.&lt;ref name="viral"&gt;{{cite journal | first1=Oliver | last1=Hinz | first2=Bernd | last2=Skiera | first3=Christian | last3=Barrot | first4=Jan U. | last4=Becker | title=Seeding Strategies for Viral Marketing: An Empirical Comparison | journal=Journal of Marketing | volume=75 | number=6 | pages=55–71 | year = 2011 | doi=10.1509/jm.10.0088}}&lt;/ref&gt;

When the query vertices belong to different communities, the non-query vertices that form the minimum Wiener connector contain vertices adjacent to edges that bridge the different communities. These vertices span a [[Social Network#Structural holes|structural hole]] in the graph and are important.&lt;ref name="structhole"&gt;{{cite conference|last1=Lou|first1=Tiancheng|last2=Tang|first2=Jie|title=Mining Structural Hole Spanners Through Information Diffusion in Social Networks|booktitle=Proceedings of the 22nd International Conference on World Wide Web|date=2013|isbn=9781450320351|location=Rio de Janeiro, Brazil|pages=825–836|url=http://dl.acm.org/citation.cfm?id=2488388.2488461|publisher=International World Wide Web Conferences Steering Committee}}&lt;/ref&gt;

==Applications==
The minimum Wiener connector is useful in applications in which one wishes to learn about the relationship between a set of vertices in a graph. For example,
* in [[biology]], it provides insight into how a set of proteins in a [[protein–protein interaction]] network are related,
* in [[social network]]s (like [[Twitter]]), it demonstrates the communities to which a set of users belong and how these communities are related,
* in [[computer network]]s, it may be useful in identifying an efficient way to route a [[multicast]] message to a set of destinations.

==References==
{{reflist}}

[[Category:NP-complete problems]]
[[Category:Trees (graph theory)]]
[[Category:Computational problems in graph theory]]
[[Category:Geometric algorithms]]
[[Category:Geometric graphs]]
[[Category:Graph algorithms]]
[[Category:Data mining]]
[[Category:Social networks]]
[[Category:Computational biology]]
[[Category:Computer science]]
[[Category:Algorithms]]
[[Category:Information retrieval]]
__INDEX__</text>
      <sha1>bvbcr40ojals22bj58wcu20m3tvdne4</sha1>
    </revision>
  </page>
  <page>
    <title>Navigational database</title>
    <ns>0</ns>
    <id>622805</id>
    <revision>
      <id>744575422</id>
      <parentid>696272052</parentid>
      <timestamp>2016-10-16T03:52:47Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* Description */http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6615" xml:space="preserve">{{Refimprove|date=July 2007}}
A '''navigational database''' is a type of [[database]] in which [[Record (computer science)|records]] or [[Object (computer science)|objects]] are found primarily by following references from other objects. They were a common type of database in the era when data was stored on [[magnetic tape]]; the navigational references told the computer where the next record on the tape was stored, allowing fast-forwarding (and in some cases, reversing) through the records without having to read every record along the way to see if it matched a given criterion.

The introduction of low-cost [[hard drive]]s that provided semi-random access to data led to new models of database storage better suited to these devices. Among these, the [[relational database]] and especially [[SQL]] became the canonical solution from the 1980s through to about 2010. At that time a reappraisal of the entire database market began, the various [[NoSQL]] concepts, which has led to the navigational model being reexamined. Offshoots of the concept, especially the [[graph database]], are finding new uses in modern [[transaction processing]] workloads.

==Description==
Navigational interfaces are usually procedural, though some modern systems like [[XPath]] can be considered to be simultaneously navigational and declarative. 

Navigational access is traditionally associated with the [[network model]] and [[hierarchical model]] of [[database]] interfaces, and some have even acquired set-oriented features.&lt;ref&gt;{{cite book | author = Błażewicz, Jacek |author2=Królikowski, Zbyszko |author3=Morzy, Tadeusz | title = Handbook on Data Management in Information Systems  | publisher = Springer  | year = 2003  | location =  | page = 18  | url = https://books.google.com/books?id=AvLziHKyuLcC&amp;pg=PA18&amp;dq=%22Navigational+database%22+-wikipedia+network+model+and+hierarchical+model&amp;ie=ISO-8859-1| doi =  | isbn = 3-540-43893-9 }}&lt;/ref&gt; Navigational techniques use "pointers" and "paths" to navigate among data records (also known as "nodes"). This is in contrast to the [[relational model]] (implemented in [[relational database]]s), which strives to use "declarative" or [[logic programming]] techniques that ask the system for ''what'' to fetch instead of ''how'' to navigate to it.  

For example, to give directions to a house, the navigational approach would resemble something like "Get on highway 25 for 8 miles, turn onto Horse Road, left at the red barn, then stop at the 3rd house down the road", whereas the declarative approach would resemble "Visit the green house(s) within the following coordinates...."

Hierarchical models are also considered navigational because one "goes" up (to parent), down (to leaves), and there are "paths", such as the familiar file/folder paths in hierarchical file systems. In general, navigational systems will use combinations of paths and prepositions such as "next", "previous", "first", "last", "up", "down", "owner", etc.

"Paths" are often formed by concatenation of [[Node (computer science)|node]] names or node addresses. Example:

[[File:6n-graf.svg|thumb|250px|Sample database nodes: A labeled graph on 6 vertices and 7 edges. (Numbers are used for illustration purposes only. In practice more meaningful names are often used. Other potential attributes are not shown.)]]

  Node6.Node4.Node5.Node1

Or

  Node6/Node4/Node5/Node1

If there is no link between given nodes, then an error condition is usually triggered with a message such as "Invalid Path".  The path "Node6.Node2.Node1" would be invalid in most systems because there is no direct link between Node 6 and Node 2.

The usage of the term "navigational" allegedly is derived from a statement by [[Charles Bachman]] in which he describes the "programmer as navigator" while accessing his favored type of database.&lt;ref&gt;{{cite web|url=http://portal.acm.org/citation.cfm?id=362534&amp;coll=portal&amp;dl=ACM |title=The programmer as navigator |doi=10.1145/355611.362534 |publisher=Portal.acm.org |accessdate=2012-10-01}}&lt;/ref&gt;

Except for hierarchical file systems (which some consider a form of database), navigational techniques fell out of favor by the 1980s. However, [[object oriented programming]] and [[XML]] have kindled a renewed, but controversial interest in navigational techniques.

Critics of navigational techniques view them as "unstructured spaghetti messes", and liken them to the "[[Goto (command)|goto]]" of pre-[[structured programming]]. In other words, they are allegedly to data organization what goto's were to behavior flow. In this view, relational techniques provide improved discipline and consistency to data organization and usage because of its roots in [[set theory]] and [[predicate calculus]]. 

Some also suggest that navigational database engines are easier to build and take up less memory (RAM) than relational equivalents. However, the existence of relational or relational-based products of the late 1980s that possessed small engines (by today's standards) because they didn't use SQL suggest this is not necessarily the case. Whatever the reason, navigational techniques are still the preferred way to handle smaller-scale structures.

A current example of navigational structuring can be found in the [[Document Object Model]] (DOM) often used in web browsers and closely associated with [[JavaScript]]. The DOM "engine" is essentially a light-weight navigational database. The [[World Wide Web]] itself and Wikipedia could potentially be considered forms of navigational databases, though they focus on human-readable text rather than data (on a large scale, the Web is a network model and on smaller or local scales, such as domain and URL partitioning, it uses hierarchies).  In contrast, the [[Linked Data]] facet of the [[Semantic Web]] is specifically concerned with network-scale [[machine-readable data]], and follows precisely the 'follow your nose' paradigm implied by the navigational idea.

A new kind of navigational databases{{fact|date=August 2015}} has recently{{when|date=August 2015}} emerged, the [[graph databases]]. This category of databases is often included as one of the four family of the [[NoSQL]] databases.

==Examples==
* [[IBM Information Management System]]
* [[IDMS]]

==See also==
* [[CODASYL]]
* [[Graph database]]
* [[Network database]]
* [[Object database]]
* [[Relational database]]

==References==
{{Reflist}}

==External links==
* [http://db-engines.com/en/ranking/navigational+dbms DB-Engines Ranking of Navigational DBMS] by popularity, updated by month


[[Category:Data management]]
[[Category:Types of databases]]</text>
      <sha1>dwbtu66puzi6m18phdehjdncaioni0a</sha1>
    </revision>
  </page>
  <page>
    <title>Rollback (data management)</title>
    <ns>0</ns>
    <id>1015240</id>
    <revision>
      <id>761417861</id>
      <parentid>757600594</parentid>
      <timestamp>2017-01-22T21:59:24Z</timestamp>
      <contributor>
        <username>ImperfectlyInformed</username>
        <id>5106682</id>
      </contributor>
      <comment>/* See also */ add [[schema migration]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2696" xml:space="preserve">{{Other uses|Rollback (disambiguation)}}
{{Selfref|For the Wikipedia tool, see [[Wikipedia:Rollback feature]].}}
{{no footnotes|date=June 2009}}

In [[database]] technologies, a '''rollback''' is an operation which returns the database to some previous state. Rollbacks are important for database [[data integrity|integrity]], because they mean that the database can be restored to a clean copy even after erroneous operations are performed. They are crucial for recovering from database server crashes; by rolling back any [[Database transaction|transaction]] which was active at the time of the crash, the database is restored to a consistent state.

The rollback feature is usually implemented with a [[Database log|transaction log]], but can also be implemented via [[multiversion concurrency control]].

==Cascading rollback==
A ''cascading rollback'' occurs in database systems when a transaction (T1) causes a failure and a rollback must be performed. Other transactions dependent on T1's actions must also be rollbacked due to T1's failure, thus causing a cascading effect. That is, one transaction's failure causes many to fail.

Practical database recovery techniques guarantee cascadeless rollback, therefore a cascading rollback is not a desirable result.

==SQL==
In [[SQL]], &lt;code&gt;ROLLBACK&lt;/code&gt; is a command that causes all data changes since the last &lt;code&gt;[[Begin work (SQL)|BEGIN WORK]]&lt;/code&gt;, or &lt;code&gt;[[Start transaction (SQL)|START TRANSACTION]]&lt;/code&gt; to be discarded by the [[relational database management systems]] (RDBMS), so that the state of the data is "rolled back" to the way it was before those changes were made.

A &lt;code&gt;ROLLBACK&lt;/code&gt; statement will also release any existing [[savepoint]]s that may be in use.

In most SQL dialects, &lt;code&gt;ROLLBACK&lt;/code&gt;s are connection specific.  This means that if two connections are made to the same database, a &lt;code&gt;ROLLBACK&lt;/code&gt; made in one connection will not affect any other connections.  This is vital for proper [[Concurrent programming|concurrency]].

==See also==
*[[Savepoint]]
*[[Commit (data management)|Commit]]
*[[Undo]]
*[[Schema migration]]

==References==
*{{cite book |author = [[Ramez Elmasri]] |title= Fundamentals of Database Systems |publisher= [[Pearson Addison Wesley]] |year= 2007|isbn= 0-321-36957-2 }}
*[http://msdn2.microsoft.com/en-us/library/ms181299.aspx "ROLLBACK Transaction"], Microsoft SQL Server.
*[http://www.pantz.org/software/mysql/mysqlcommands.html "Sql Commands"], MySQL.

{{Databases}}
{{Web syndication}}

[[Category:Data management]]
[[Category:Database theory]]
[[Category:SQL]]
[[Category:Transaction processing]]
[[Category:Reversible computing]]


{{compu-prog-stub}}</text>
      <sha1>m6x3r76p4ngwpki24vjfexqc0wltt0f</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Computer file systems</title>
    <ns>14</ns>
    <id>754856</id>
    <revision>
      <id>732172789</id>
      <parentid>546483788</parentid>
      <timestamp>2016-07-30T03:39:47Z</timestamp>
      <contributor>
        <username>Ushkin N</username>
        <id>28390915</id>
      </contributor>
      <comment>see [[Category_talk:Computer_storage#Category:Computer_file_systems_and_subcategories_here]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="982" xml:space="preserve">{{catdiffuse}}
{{Cat main|File system}}
{{Commonscat|File systems}}
A '''[[file system]]''' in computing, is a method for storing and organizing [[computer file]]s and the data they contain to make it easy to find and access them. File systems may use a [[data storage device]] such as a [[hard disk]] or [[CD-ROM]] and involve maintaining the physical location of the files, or they may be virtual and exist only as an access method for virtual data or for data over a network (e.g. [[Network File System (protocol)|NFS]]).

More formally, a file system is a set of [[abstract data type]]s that are implemented for the storage, hierarchical organization, manipulation, navigation, access, and retrieval of [[data]].

== See also ==
* [[:Category:Computer storage]]

[[Category:Data management]]
[[Category:Operating system technology|File systems]]
&lt;!--[[Category:Computer systems|File systems]] deleted  "Computer systems" not an index of systems --&gt;
[[Category:Storage software]]</text>
      <sha1>7c7e34eyuo5deqb86lpzi9bvuxeh0lh</sha1>
    </revision>
  </page>
  <page>
    <title>Schedule (computer science)</title>
    <ns>0</ns>
    <id>400457</id>
    <revision>
      <id>753088132</id>
      <parentid>731512263</parentid>
      <timestamp>2016-12-05T03:18:02Z</timestamp>
      <contributor>
        <ip>68.68.88.19</ip>
      </contributor>
      <comment>/* Conflict equivalence */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="14441" xml:space="preserve">{{refimprove|date=November 2012}}

In the fields of [[database]]s and [[transaction processing]] (transaction management), a '''schedule''' (or '''history''') of a system is an abstract model to describe execution of transactions running in the system. Often it is a ''list'' of operations (actions) ordered by time, performed by a set of [[Database transaction|transactions]] that are executed together in the system. If order in time between certain operations is not determined by the system, then a ''[[partial order]]'' is used. Examples of such operations are requesting a read operation, reading, writing, aborting, committing, requesting lock, locking, etc. Not all transaction operation types should be included in a schedule, and typically only selected operation types (e.g., data access operations) are included, as needed to reason about and describe certain phenomena. Schedules and schedule properties are fundamental concepts in database [[concurrency control]] theory.

==Formal description==

The following is an example of a schedule:

:&lt;math&gt;D = \begin{bmatrix}
T1 &amp; T2 &amp; T3 \\
R(X) &amp;  &amp;  \\
W(X) &amp;  &amp;  \\
Com. &amp;  &amp;  \\
 &amp; R(Y) &amp; \\
 &amp; W(Y) &amp; \\
 &amp; Com. &amp; \\
 &amp;&amp; R(Z) \\
 &amp;&amp; W(Z) \\
 &amp;&amp; Com. \end{bmatrix}&lt;/math&gt;

In this example, the horizontal axis represents the different transactions in the schedule D. The vertical axis represents time order of operations. Schedule D consists of three transactions T1, T2, T3.  The schedule describes the actions of the transactions as seen by the [[DBMS]].
First T1 Reads and Writes to object X, and then Commits. Then T2 Reads and Writes to object Y and Commits, and finally T3 Reads and Writes to object Z and Commits.  This is an example of a ''serial'' schedule, i.e., sequential with no overlap in time, because the actions of in all three transactions are sequential, and the transactions are not interleaved in time.

Representing the schedule D above by a table (rather than a list) is just for the convenience of identifying each transaction's operations in a glance. This notation is used throughout the article below. A more common way in the technical literature for representing such schedule is by a list:

:::D = R1(X) W1(X) Com1 R2(Y) W2(Y) Com2 R3(Z) W3(Z) Com3

Usually, for the purpose of reasoning about concurrency control in databases, an operation is modeled as ''[[Atomic operation|atomic]]'', occurring at a point in time, without duration. When this is not satisfactory start and end time-points and possibly other point events are specified (rarely). Real executed operations always have some duration and specified respective times of occurrence of events within them (e.g., "exact" times of beginning and completion), but for concurrency control reasoning usually only the precedence in time of the whole operations (without looking into the quite complex details of each operation) matters, i.e., which operation is before, or after another operation. Furthermore, in many cases the before/after relationships between two specific operations do not matter and should not be specified, while being specified for other pairs of operations.

In general operations of transactions in a schedule can interleave (i.e., transactions can be executed concurrently), while time orders between operations in each transaction remain unchanged as implied by the transaction's program. Since not always time orders between all operations of all transactions matter and need to be specified, a schedule is, in general, a ''[[partial order]]'' between operations rather than a ''[[total order]]'' (where order for each pair is determined, as in a list of operations). Also in the general case each transaction may consist of several processes, and itself be properly represented by a partial order of operations, rather than a total order. Thus in general a schedule is a partial order of operations, containing ([[embedding]]) the partial orders of all its transactions.

Time-order between two operations can be represented by an ''[[ordered pair]]'' of these operations (e.g., the existence of a pair (OP1,OP2) means that OP1 is always before OP2), and a schedule in the general case is a [[set (mathematics)|set]] of such ordered pairs. Such a set, a schedule, is a [[partial order]] which can be represented by an ''[[acyclic directed graph]]'' (or ''directed acyclic graph'', DAG) with operations as nodes and time-order as a [[directed edge]] (no cycles are allowed since a cycle means that a first (any) operation on a cycle can be both before and after (any) another second operation on the cycle, which contradicts our perception of [[Time]]). In many cases a graphical representation of such graph is used to demonstrate a schedule.

'''Comment:''' Since a list of operations (and the table notation used in this article) always represents a total order between operations, schedules that are not a total order cannot be represented by a list (but always can be represented by a DAG).

==Types of schedule==

===Serial===

The transactions are executed non-interleaved (see example above)
i.e., a serial schedule is one in which no transaction starts until a running transaction has ended.

===Serializable===&lt;!-- This section is linked from [[Concurrency control]] --&gt;

A schedule that is equivalent (in its outcome) to a serial schedule has the [[serializability]] property.

In schedule E, the order in which the actions of the transactions are executed is not the same as in D, but in the end, E gives the same result as D.
:&lt;math&gt;E = \begin{bmatrix}
T1 &amp; T2 &amp; T3 \\
R(X) &amp;  &amp;  \\
   &amp; R(Y) &amp; \\
 &amp;&amp; R(Z) \\

W(X) &amp;  &amp;  \\
 &amp; W(Y) &amp; \\
 &amp;&amp; W(Z) \\
Com. &amp; Com. &amp; Com. \end{bmatrix}&lt;/math&gt;

====Conflicting actions====

Two actions are said to be in conflict (conflicting pair) if: 

# The actions belong to different transactions.
# At least one of the actions is a write operation.
# The actions access the same object (read or write).

The following set of actions is conflicting: 
* R1(X), W2(X), W3(X) (3 conflicting pairs)

While the following sets of actions are not: 
* R1(X), R2(X), R3(X)
* R1(X), W2(Y), R3(X)

====Conflict equivalence====

The schedules S1 and S2 are said to be conflict-equivalent if the following two conditions are satisfied: 

# Both schedules S1 and S2 involve the same set of transactions (including ordering of actions within each transaction).
# Both schedules have same set of conflicting operations.

====Conflict-serializable====

A schedule is said to be conflict-serializable when the schedule is conflict-equivalent to one or more serial schedules. 

Another definition for conflict-serializability is that a schedule is conflict-serializable if and only if its [[precedence graph]]/serializability graph, when only committed transactions are considered, is acyclic (if the graph is defined to include also uncommitted transactions, then cycles involving uncommitted transactions may occur without conflict serializability violation).

:&lt;math&gt;G = \begin{bmatrix}
T1 &amp; T2 \\
R(A) &amp;   \\
 &amp; R(A) \\
W(B) &amp; \\
Com. &amp; \\
 &amp; W(A) \\
 &amp; Com. \\
 &amp;\end{bmatrix}&lt;/math&gt;

Which is conflict-equivalent to the serial schedule &lt;T1,T2&gt;, but not &lt;T2,T1&gt;.

====Commitment-ordered====
{{POV-section|Commitment ordering|date=November 2011}}
A schedule is said to be commitment-ordered (commit-ordered), or commitment-order-serializable, if it obeys the [[Commitment ordering]] (CO; also commit-ordering or commit-order-serializability) schedule property. This means that the order in time of transactions' commitment events is compatible with the precedence (partial) order of the respective transactions, as induced by their schedule's acyclic precedence graph (serializability graph, conflict graph). This implies that it is also conflict-serializable. The CO property is especially effective for achieving [[Global serializability]] in distributed systems.

'''Comment:''' [[Commitment ordering]], which was discovered in 1990, is obviously not mentioned in ([[#Bern1987|Bernstein et al. 1987]]). Its correct definition appears in ([[#Weikum2001|Weikum and Vossen 2001]]), however the description there of its related techniques and theory is partial, inaccurate, and misleading.{{Says who|date=December 2011}} For an extensive coverage of commitment ordering and its sources see ''[[Commitment ordering]]'' and ''[[The History of Commitment Ordering]]''.

====View equivalence====

Two schedules S1 and S2 are said to be view-equivalent when the following conditions are satisfied:

# If the transaction &lt;math&gt;T_i&lt;/math&gt; in S1 reads an initial value for object X, so does the transaction &lt;math&gt;T_i&lt;/math&gt; in S2. 
# If the transaction &lt;math&gt;T_i&lt;/math&gt; in S1 reads the value written by transaction &lt;math&gt;T_j&lt;/math&gt; in S1 for object X, so does the transaction &lt;math&gt;T_i&lt;/math&gt; in S2.
# If the transaction &lt;math&gt;T_i&lt;/math&gt; in S1 is the final transaction to write the value for an object X, so is the transaction &lt;math&gt;T_i&lt;/math&gt; in S2.

====View-serializable====

A schedule is said to be view-serializable if it is view-equivalent to some serial schedule. 
Note that by definition, all conflict-serializable schedules are view-serializable. 

:&lt;math&gt;G = \begin{bmatrix}
T1 &amp; T2 \\
R(A) &amp;   \\
 &amp; R(A) \\
W(B) &amp; \\
 \end{bmatrix}&lt;/math&gt;

Notice that the above example (which is the same as the example in the discussion of conflict-serializable) is both view-serializable and conflict-serializable at the same time.) There are however view-serializable schedules that are not conflict-serializable: those schedules with a transaction performing a [[blind write]]:
 
:&lt;math&gt;H = \begin{bmatrix}
T1 &amp; T2 &amp; T3 \\
R(A) &amp; &amp; \\
 &amp; W(A) &amp; \\
 &amp; Com. &amp; \\
W(A) &amp; &amp; \\
Com. &amp; &amp; \\
 &amp; &amp; W(A) \\
 &amp; &amp; Com. \\
 &amp; &amp; \end{bmatrix}&lt;/math&gt;

The above example is not conflict-serializable, but it is view-serializable since it has a view-equivalent serial schedule &lt;T1,&amp;nbsp;T2,&amp;nbsp;T3&gt;. 

Since determining whether a schedule is view-serializable is [[NP-complete]], view-serializability has little practical interest.{{citation needed|date=April 2015}}

===Recoverable===&lt;!-- This section is linked from [[Concurrency control]] --&gt;

Transactions commit only after all transactions whose changes they read, commit.

:&lt;math&gt;F = \begin{bmatrix}
T1 &amp; T2 \\
R(A) &amp;   \\
W(A) &amp;   \\
 &amp; R(A) \\
 &amp; W(A) \\
Com. &amp; \\
 &amp; Com.\\
 &amp;\end{bmatrix} 
F2 = \begin{bmatrix}
T1 &amp; T2 \\
R(A) &amp;   \\
W(A) &amp;   \\
 &amp; R(A) \\
 &amp; W(A) \\
Abort &amp;  \\
&amp; Abort \\
 &amp;\end{bmatrix}&lt;/math&gt;

These schedules are recoverable.  F is recoverable because T1 commits before T2, that makes the value read by T2 correct.  Then T2 can commit itself.  In F2, if T1 aborted, T2 has to abort because the value of A it read is incorrect.  In both cases, the database is left in a consistent state.

====Unrecoverable====

If a transaction T1 aborts, and a transaction T2 commits, but T2 relied on T1, we have an unrecoverable schedule.

:&lt;math&gt;G = \begin{bmatrix}
T1 &amp; T2 \\
R(A) &amp;   \\
W(A) &amp;   \\
 &amp; R(A) \\
 &amp; W(A) \\
 &amp; Com. \\
Abort &amp; \\
 &amp;\end{bmatrix}&lt;/math&gt;

In this example, G is unrecoverable, because T2 read the value of A written by T1, and committed.  T1 later aborted, therefore the value read by T2 is wrong, but since T2 committed, this schedule is unrecoverable.

====Avoids cascading aborts / rollbacks (ACA)====

Also named cascadeless. Avoids that a single transaction abort leads to a series of transaction rollbacks. A strategy to prevent cascading aborts is to disallow a transaction from reading uncommitted changes from another transaction in the same schedule. 

The following examples are the same as the ones in the discussion on recoverable: 

:&lt;math&gt;F = \begin{bmatrix}
T1 &amp; T2 \\
R(A) &amp;   \\
W(A) &amp;   \\
 &amp; R(A) \\
 &amp; W(A) \\
Com. &amp; \\
 &amp; Com.\\
 &amp;\end{bmatrix} 
F2 = \begin{bmatrix}
T1 &amp; T2 \\
R(A) &amp;   \\
W(A) &amp;   \\
 &amp; R(A) \\
 &amp; W(A) \\
Abort &amp;  \\
&amp; Abort \\
 &amp;\end{bmatrix}&lt;/math&gt;

In this example, although F2 is recoverable, it does not avoid 
cascading aborts. It can be seen that if T1 aborts, T2 will have to 
be aborted too in order to maintain the correctness of the schedule 
as T2 has already read the uncommitted value written by T1. 

The following is a recoverable schedule which avoids cascading abort. Note, however, that the update of A by T1 is always lost (since T1 is aborted).

:&lt;math&gt;F3 = \begin{bmatrix}
T1 &amp; T2 \\
 &amp; R(A) \\
R(A) &amp;   \\
W(A) &amp;   \\
 &amp; W(A) \\
Abort &amp;  \\
&amp; Commit \\
 &amp;\end{bmatrix}&lt;/math&gt;
Note that this Schedule would not be serializable if T1 would be committed.
Cascading aborts avoidance is sufficient but not necessary for a schedule to be recoverable.

====Strict====

A schedule is strict - has the strictness property - if for any two transactions T1, T2, if a write operation of T1 precedes a ''conflicting'' operation of T2 (either read or write), then the commit or abort event of T1 also precedes that conflicting operation of T2.

Any strict schedule is cascadeless, but not the converse. Strictness allows efficient recovery of databases from failure.

==Hierarchical relationship between serializability classes==

The following expressions illustrate the hierarachical (containment) relationships between [[serializability]] and [[Serializability#Correctness - recoverability|recoverability]] classes: 

* Serial &amp;sub; commitment-ordered &amp;sub; conflict-serializable &amp;sub; view-serializable &amp;sub; all schedules
* Serial &amp;sub; strict &amp;sub; avoids cascading aborts &amp;sub; recoverable &amp;sub; all schedules

The [[Venn diagram]] (below) illustrates the above clauses graphically. 

[[File:Schedule-serializability.png|frame|none|Venn diagram for serializability and recoverability classes]]

==Practical implementations==

In practice, most general purpose database systems employ conflict-serializable and recoverable (primarily strict) schedules.

==See also==
* [[schedule (project management)]]

==References==

*&lt;cite id=Bern1987&gt;[[Phil Bernstein|Philip A. Bernstein]], Vassos Hadzilacos, Nathan Goodman: [http://research.microsoft.com/en-us/people/philbe/ccontrol.aspx  ''Concurrency Control and Recovery in Database Systems''], Addison Wesley Publishing Company, 1987, ISBN 0-201-10715-5&lt;/cite&gt;
*&lt;cite id=Weikum2001&gt;[[Gerhard Weikum]], Gottfried Vossen: [http://www.elsevier.com/wps/find/bookdescription.cws_home/677937/description#description  ''Transactional Information Systems''], Elsevier, 2001, ISBN 1-55860-508-8&lt;/cite&gt;

[[Category:Data management]]
[[Category:Transaction processing]]</text>
      <sha1>qzcrywnup7vhz0b5njlz99q07rcjgw8</sha1>
    </revision>
  </page>
  <page>
    <title>Data bank</title>
    <ns>0</ns>
    <id>40990</id>
    <revision>
      <id>663186027</id>
      <parentid>641573880</parentid>
      <timestamp>2015-05-20T02:08:42Z</timestamp>
      <contributor>
        <username>Hugo999</username>
        <id>3006008</id>
      </contributor>
      <comment>/* See also */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1261" xml:space="preserve">In [[telecommunication]]s, a '''data bank''' is a repository of information on one or more subjects that is organized in a way that facilitates local or remote information retrieval. A data bank may be either centralized or decentralized.
In computers the data bank is the same as in telecommunication (i.e. it is the repository of data. The data in the data bank can be things such as credit card transactions or it can be any data base of a company where large quantities of queries are being processed on daily bases). 
 
'''Data bank''' may also refer to an organization primarily concerned with the construction and maintenance of a [[database]].

== See also ==

* [[Star Wars Databank]]
* [[Protein Data Bank]]
* [[National Trauma Data Bank]]
* [[memory bank]]
* [[International Tree-Ring Data Bank]]
* [[Hazardous Substances Data Bank]]
* [[electron microscopy data bank]]
* [[Dortmund Data Bank]]
* [[Casio Databank]]
* [[conformational dynamics data bank]]
* [[Databank Systems Limited]] a former New Zealand banking agency

==Sources==
*{{FS1037C MS188}}
*''[[The American Heritage Dictionary of the English Language]], Fourth Edition''. [[Houghton Mifflin]], 2000.

==External links==
{{wiktionary}}

[[Category:Data management]]


{{telecomm-stub}}</text>
      <sha1>idq0yjp8xs0sof0bjtqdn61cjfgdx7r</sha1>
    </revision>
  </page>
  <page>
    <title>QuickPar</title>
    <ns>0</ns>
    <id>1756767</id>
    <revision>
      <id>762794371</id>
      <parentid>747441420</parentid>
      <timestamp>2017-01-30T19:55:39Z</timestamp>
      <contributor>
        <username>Bulat Ziganshin</username>
        <id>11345125</id>
      </contributor>
      <comment>/* External links */ Removed link to malicious multipar.eu site!</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4288" xml:space="preserve">{{Merge to |Parchive |date=March 2014}}

{{Infobox software
| logo                   = 
| screenshot             = [[File:QuickPar Screenshot.png|250px]]
| caption                = QuickPar 0.9 checking a series of [[RAR (file format)|RAR]] files for integrity.
| collapsible            = 
| author                 = 
| developer              = Peter Clements
| released               = 0.1, (February 5, 2003)&lt;ref&gt;{{cite web|url=http://www.quickpar.org.uk/ReleaseNotes2.htm|title=QuickPar - Old Release Notes|accessdate=2010-11-19}}&lt;/ref&gt;
| latest release version = 0.9.1
| latest release date    = {{Start date and age|2004|07|04}}&lt;ref&gt;{{cite web |url=http://www.quickpar.org.uk/ |title=QuickPar for Windows |accessdate=2009-09-27}}&lt;/ref&gt;
| latest preview version = 
| latest preview date    = &lt;!-- {{Start date and age|YYYY|MM|DD}} --&gt;
| frequently updated     = 
| programming language   = 
| operating system       = [[Microsoft Windows]]
| platform               = [[x86]]
| size                   = 
| language               = 
| status                 = 
| genre                  = [[Data recovery]]
| license                = [[Proprietary software|Proprietary]], [[Freeware]]
| website                = {{URL|www.quickpar.org.uk}}
}}

'''QuickPar''' is a computer program that creates [[parchive]]s used as verification and recovery information for a file or group of files, and uses the recovery information, if available, to attempt to reconstruct the originals from the damaged files and the PAR volumes.

Designed for the [[Microsoft Windows]] [[operating system]], it is often used to recover damaged or missing files that have been downloaded through [[Usenet]].&lt;ref&gt;{{cite book
| last        = Wang
| first       = Wallace
| authorlink  = 
| title       = Steal this File Sharing Book
| url         = https://books.google.com/books?id=FGfMS5kymmcC&amp;pg=PT183
| accessdate  = 2009-09-24
| edition     = 1st
| date        = 2004-10-25
| publisher   = [[No Starch Press]]
| location    = [[San Francisco, California]]
| isbn        = 1-59327-050-X
| pages       = 164 – 167
| chapter     = Finding movies (or TV shows): Recovering missing RAR files with PAR and PAR2 files
}}&lt;/ref&gt; QuickPar may also be used under [[Linux]] via [[Wine (software)|Wine]].&lt;ref&gt;{{cite book
| last        = Petersen
| first       = Richard
| title       = Ubuntu 9.04 Desktop Handbook
| url         = https://books.google.com/books?id=-XLrpiHDYoQC&amp;pg=PT224
| accessdate  = 2009-09-27
| date        = 2009-05-01
| publisher   = Surfing Turtle Press
| location    = [[Los Angeles, California]]
| isbn        = 0-9820998-4-3
| page        = 224
| chapter     = Internet Applications
}}&lt;/ref&gt;
[[Image:QuickPar Protect Screenshot.png|thumb|right|Par2 file creation screen]]

There are two main versions of [[Parchive|PAR files]]: PAR and PAR2. The PAR2 file format lifts many of its previous restrictions.&lt;ref&gt;{{cite web
| url         = http://www.quickpar.org.uk/AboutPAR2.htm
| title       = QuickPar - About PAR2
| accessdate  = 2009-09-27
}}&lt;/ref&gt; QuickPar is [[freeware]] but not [[open source]]. It uses the [[Reed-Solomon error correction]] algorithm internally to create the error correcting information.&lt;ref name="newsgroups"&gt;{{Cite journal | last1 = Fellows | first1 = G. | title = Newsgroups reborn – the binary posting renaissance | doi = 10.1016/j.diin.2006.04.006 | journal = Digital Investigation | volume = 3 | issue = 2 | pages = 73–78 | year = 2006 | pmid =  | pmc = }}&lt;/ref&gt;

==Abandonware==
Though QuickPar works well, it is currently considered [[abandonware]], since there have been no updates for it in {{age|2004|07|04}} years.  The software, [[Parchive#Windows| MultiPar]], is actively being developed by another author named Yutaka Sawada, who is adding support for the new PAR3 file format.

==See also==
* [[:nl:Data Archiving and Networked Services|DANS; has some similar software]]

==References==
{{Reflist}}

==External links==
* {{Official website|http://www.quickpar.org.uk/}}
* [http://www.quickpar.org.uk/Tutorials.htm QuickPar tutorial referencing Usenet downloads]
* [https://www.livebusinesschat.com/smf/index.php?board=396.0 MultiPar], successor to QuickPar, supports PAR2, PAR3 and multicore cpu's

[[Category:Data management]]


{{storage-software-stub}}</text>
      <sha1>sky0fcczl0xx96h4lqpnymj56i8ewu9</sha1>
    </revision>
  </page>
  <page>
    <title>Match report</title>
    <ns>0</ns>
    <id>2575602</id>
    <revision>
      <id>722501202</id>
      <parentid>720628645</parentid>
      <timestamp>2016-05-28T15:10:35Z</timestamp>
      <contributor>
        <username>Pegship</username>
        <id>355698</id>
      </contributor>
      <minor />
      <comment>stub sort</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1020" xml:space="preserve">{{Unreferenced|date=May 2009}}
In [[metadata]], a '''match report''' is a report that compares two distinct [[data dictionary|data dictionaries]] and creates a list of the [[data element]]s that have been identified as [[Semantic equivalence|semantically equivalent]].

== Use of match reports ==

Match reports are critical for systems that wish to automatically exchange data such as intelligent software agents.  If one computer system is requesting a report from a remote system that uses a distinct data dictionary and all of the data elements on the report manifest are included in the '''match report''' the report request can be executed.

Match reports are useful if data dictionaries use a metadata tagging system such as the [[UDEF]].

==See also==
*[[Data dictionary]]
*[[Data warehouse]]
*[[Metadata]]
*[[Semantic equivalence]]
*[[Universal Data Element Framework]]

[[Category:Knowledge representation]]
[[Category:Data management]]
[[Category:Technical communication]]
[[Category:Metadata]]
{{compu-stub}}</text>
      <sha1>tn7u4sg8ao1oc3xqzn4htei7sg6eh2p</sha1>
    </revision>
  </page>
  <page>
    <title>Data archaeology</title>
    <ns>0</ns>
    <id>2588620</id>
    <revision>
      <id>757142692</id>
      <parentid>757142652</parentid>
      <timestamp>2016-12-29T01:23:00Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <minor />
      <comment>/* See also */ alpha</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5447" xml:space="preserve">{{for|the computer-based analysis of archaeological data|Computational archaeology}}

'''Data archaeology''' refers to the art and science of recovering [[computer]] [[data]] encoded and/or encrypted in now obsolete [[Computer media|media]] or [[content format|formats]]. Data archaeology can also refer to recovering information from damaged electronic formats after natural or man made disasters.

The term originally appeared in 1993 as part of the [[Global Oceanographic Data Archaeology and Rescue Project]] (GODAR). The original impetus for data archaeology came from the need to recover computerized records of climatic conditions stored on old computer tape, which can provide valuable evidence for testing theories of [[climate change]]. These approaches allowed the reconstruction of an image of the Arctic that had been captured by the [[Nimbus program|Nimbus 2]] satellite on September 23, 1966, in higher resolution than ever seen before from this type of data.&lt;ref&gt;[http://nsidc.org/monthlyhighlights/january2010.html Techno-archaeology rescues climate data from early satellites] U.S. National Snow and Ice Data Center (NSIDC), January 2010 [http://www.webcitation.org/5xN1sNyDp Archived]&lt;/ref&gt;

[[NASA]] also utilizes the services of data archaeologists to recover information stored on 1960s era vintage computer tape, as exemplified by the [[Lunar Orbiter Image Recovery Project]] (LOIRP).&lt;ref&gt;[http://www.nasa.gov/topics/moonmars/features/LOIRP/ LOIRP Overview] NASA website November 14, 2008 [http://www.webcitation.org/5xN1DjLG4 Archived]&lt;/ref&gt;

==Recovery==
It is also important to make the distinction in data archaeology between data recovery, and data intelligibility. You may be able to recover the data, but not understand it. For data archaeology to be effective the data must be intelligible.&lt;ref name="www.ukoln.ac.uk"&gt; [http://www.ukoln.ac.uk/services/elib/papers/supporting/pdf/p2.pdf] Study on website October 23, 2011 &lt;/ref&gt;

===Disaster recovery===
Data archaeologists can also use [[data recovery]] after natural disasters such as fires, floods, earthquakes, or even hurricanes. For example, in 1995 during [[Hurricane Marilyn]] the National Media Lab assisted the [[National Archives and Records Administration]] in recovering data at risk due to damaged equipment. The hardware was damaged from rain, salt water, and sand, yet it was possible to clean some of the disks and refit them with new cases thus saving the data within.&lt;ref name="www.ukoln.ac.uk"/&gt;

===Recovery techniques===
When deciding whether or not to try and recover data, the cost must be taken into account. If there is enough time and money, most data will be able to be recovered. In the case of [[magnetic media]], which are the most common type used for data storage, there are various techniques that can be used to recover the data depending on the type of damage.&lt;ref name="www.ukoln.ac.uk"/&gt;{{rp|17}}

Humidity can cause tapes to become unusable as they begin to deteriorate and become sticky.  In this case, a heat treatment can be applied to fix this problem, by causing the oils and residues to either be reabsorbed into the tape or evaporate off the surface of the tape.  However, this should only be done in order to provide access to the data so it can be extracted and copied to a medium that is more stable.&lt;ref name="www.ukoln.ac.uk"/&gt;{{rp|17–18}}

Lubrication loss is another source of damage to tapes. This is most commonly caused by heavy use, but can also be a result of improper storage or natural evaporation.  As a result of heavy use, some of the lubricant can remain on the read-write heads which then collect dust and particles.  This can cause damage to the tape.  Loss of lubrication can be addressed by re-lubricating the tapes.  This should be done cautiously, as excessive re-lubrication can cause tape slippage, which in turn can lead to media being misread and the loss of data.&lt;ref name="www.ukoln.ac.uk"/&gt;{{rp|18}}

Water exposure will damage tapes over time.  This often occurs in a disaster situation.  If the media is in salty or dirty water, it should be rinsed in fresh water.  The process of cleaning, rinsing, and drying wet tapes should be done at room temperature in order to prevent heat damage.  Older tapes should be recovered prior to newer tapes, as they are more susceptible to water damage.&lt;ref name="www.ukoln.ac.uk"/&gt;{{rp|18}}

==Prevention==
To prevent the need of data archaeology, creators and holders of digital documents should take care to employ [[digital preservation]].

==See also==
* [[Data degradation|Bit rot]]
* [[Digital dark age]]
* [[Knowledge discovery]]

==References==
&lt;references /&gt;
*[http://www.worldwidewords.org/turnsofphrase/tp-dat1.htm World Wide Words: Data Archaeology]
*O'Donnell, James Joseph.  ''Avatars of the Word:  From Papyrus to Cyperspace''  Harvard University Press, 1998.
* {{cite book | last1  = Ross | first1 = Seamus | last2  = Gow | first2 = Ann| lastauthoramp = yes| title = Digital archaeology : rescuing neglected and damaged data resources| publisher = British Library and Joint Information Systems Committee| place     = London &amp; Bristol| series = Electronic libraries programme studies
| year    = 1999| | language = EN| url = http://www.ukoln.ac.uk/services/elib/papers/supporting/pdf/p2.pdf| isbn = 1-90050-851-6}}

[[Category:Data management|Archaeology]]
[[Category:Digital preservation]]
[[Category:Archaeological sub-disciplines]]</text>
      <sha1>cwjcbbyq13ianhr8fweg3vygl6y5pbh</sha1>
    </revision>
  </page>
  <page>
    <title>Data steward</title>
    <ns>0</ns>
    <id>6212365</id>
    <revision>
      <id>762071258</id>
      <parentid>762071101</parentid>
      <timestamp>2017-01-26T13:27:13Z</timestamp>
      <contributor>
        <username>RichardWeiss</username>
        <id>193093</id>
      </contributor>
      <comment>rm all languages, the spanish is incorrect, we have wikidata now and should exclusively use that</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6738" xml:space="preserve">{{merge|Data custodian|date=February 2016}}

A '''data steward''' is a person responsible for the management and fitness of [[data element]]s  - both the content and [[metadata]]. Data stewards have a specialist role that incorporates processes, policies, guidelines and responsibilities for administering organizations' entire data in compliance with policy and/or regulatory obligations. A data steward may share some responsibilities with a [[data custodian]].

The overall objective of a data steward is [[data quality]], in regard to the key/critical data elements existing within a specific enterprise operating structure, of the elements in their respective domains. This includes capturing/documenting (meta)information for their elements (such as: definitions, related rules/governance, physical manifestation, related data models, etc. With most of these properties being specific to an attribute/concept relationship), identifying owners/custodians/various  responsibilities, relations insight pertaining to attribute quality, aiding with project requirement data facilitation and documentation of capture rules. 

Data stewards begin the [[stewardship|stewarding]] process with the identification of the elements which they will steward, with the ultimate result being standards, [[Control (disambiguation)|control]]s and [[Data entry clerk|data entry]].{{citation needed|date=October 2014}}  The steward works closely with business glossary standards analysts (for standards), with [[data architect]]/[[Data modeling|modeler]]s (for standards), with  [[Data quality|DQ]] analysts (for controls) and with [[Computer operator|operations team member]]s (good-quality data going in per business rules) while entering data.

Data stewardship roles are common when organizations attempt to exchange data precisely and consistently between computer systems and to reuse data-related resources.{{citation needed|date=October 2014}}  [[Master data management]] often{{quantify|date=October 2014}} makes references to the need for data stewardship for its implementation to succeed. Data stewardship must have precise purpose, fit for purpose or fitness.

==Data Steward Responsibilities==
A data steward ensures that each assigned data element:
# Has clear and unambiguous [[data element definition]].
# Does not conflict with other data elements in the metadata registry (removes duplicates, overlap etc.)
# Has clear enumerated value definitions if it is of type [[Code (metadata)|Code]].
# Is still being used (remove unused data elements)
# Is being used consistently in various computer systems
# Is being used, fit for purpose = Data Fitness.
# Has adequate documentation on appropriate usage and notes
# Documents the origin and sources of authority on each metadata element
# Is protected against unauthorised access or change

==Benefits of data stewardship==

Systematic data stewardship can foster fitness through:

# consistent use of data management resources
# easy mapping of data between computer systems and exchange documents
# lower costs associated with migration to (for example) [[Service Oriented Architecture]] (SOA)

Assignment of each data element to a person sometimes seems like an unimportant process. But many groups{{Which|date=July 2010}} have found that users have greater trust and usage rates in systems where they can contact a person with questions on each data element.

== Examples ==
{{Expand section|date=July 2010}}

The [http://www.epa.gov/edr [[United States Environmental Protection Agency|EPA]] metadata registry] furnishes an example of data stewardship.  Note that each data element therein has a "POC"  (point of contact).

== Data Stewardship Applications ==
A new market for data governance applications is emerging, one in which both technical and business staff — stewards — manage policies. These new applications, like previous generations, deliver a strong business glossary capability, but they don't stop there. Vendors are introducing additional features addressing the roles of business in addition to technical stewards' concerns.&lt;ref&gt;{{Cite web|url=https://www.forrester.com/report/The+Forrester+Wave+Data+Governance+Stewardship+Applications+Q1+2016/-/E-RES117915|title=The Forrester Wave™: Data Governance Stewardship Applications, Q1 2016|website=www.forrester.com|access-date=2016-12-20}}&lt;/ref&gt;

Information stewardship applications are business solutions used by business users acting in the role of information steward (interpreting and enforcing information governance policy, for example). These developing solutions represent, for the most part, an amalgam of a number of disparate, previously IT-centric tools already on the market, but are organized and presented in such a way that information stewards (a business role) can support the work of information policy enforcement as part of their normal, business-centric, day-to-day work in a range of use cases.

The initial push for the formation of this new category of packaged software came from operational use cases — that is, use of business data in and between transactional and operational business applications. This is where most of the master data management (MDM) efforts are undertaken in organizations. However, there is also now a faster-growing interest in the new data lake arena for more analytical use cases.&lt;ref&gt;{{Cite web|url=https://www.gartner.com/document/3284717?ref=TypeAheadSearch&amp;qid=744b6ad6c678d064cc2d6eb831a4c959|title=Market Guide for Information Stewardship Applications|last=De Simoni|first=Guido|date=15 April 2016|website=www.gartner.com|publisher=Gartner|access-date=}}&lt;/ref&gt;

==See also==
* [[Metadata]]
* [[Metadata registry]]
* [[Data curation]]
* [[Data element]]
* [[Data element definition]]
* [[Representation term]]
* [[ISO/IEC 11179]]

==References==
* ''Universal Meta Data Models'', by David Marco and Michael Jennings, Wiley, 2004, page 93-94 ISBN 0-471-08177-9
* ''Metadata Solution'' by Adrinne Tannenbaum, Addison Wesley, 2002, page 412
* ''Building and Managing the Meta Data Repository'', by David Marco, Wiley, 2000, pages 61–62
* ''The Data Warehouse Lifecycle Toolkit'', by [[Ralph Kimball]] et. el., Wiley, 1998, also briefly mentions the role of data steward in the context of data warehouse project management on page 70.
* ''Developing Geospatial Intelligence Stewardship for Multinational Operations'', by Jeff Thomas, US Army Command General Staff College, 2010, www.dtic.mil/dtic/tr/fulltext/u2/a524227.pdf.

==Notes==
{{reflist}}

[[Category:Data management]]
[[Category:Information technology governance]]
[[Category:Knowledge representation]]
[[Category:Library occupations]]
[[Category:Metadata]]
[[Category:Technical communication]]</text>
      <sha1>t8189qv50hzjn1o7qp7ii6exelwgvpz</sha1>
    </revision>
  </page>
  <page>
    <title>Workflow engine</title>
    <ns>0</ns>
    <id>7711975</id>
    <revision>
      <id>742608756</id>
      <parentid>742608754</parentid>
      <timestamp>2016-10-04T18:17:06Z</timestamp>
      <contributor>
        <username>ClueBot NG</username>
        <id>13286072</id>
      </contributor>
      <minor />
      <comment>Reverting possible vandalism by [[Special:Contribs/144.160.5.102|144.160.5.102]] to version by 69.255.124.243. [[WP:CBFP|Report False Positive?]] Thanks, [[WP:CBNG|ClueBot NG]]. (2784062) (Bot)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3015" xml:space="preserve">A '''workflow engine''' is a [[software application]] that manages business processes. It is a key component in [[workflow technology]] and typically makes use of a [[database server]].

A workflow engine manages and monitors the state of activities in a [[workflow]], such as the processing and approval of a loan application form, and determines which new activity to transition to according to defined processes (workflows).&lt;ref&gt;http://docs.oracle.com/cd/B13789_01/workflow.101/b10286/wfapi.htm&lt;/ref&gt; The actions may be anything from saving an application form in a [[document management system]] to sending a reminder e-mail to users or escalating overdue items to management. A workflow engine facilitates the flow of information, tasks, and events. Workflow engines may also be referred to as Workflow Orchestration Engines.&lt;ref&gt;http://pic.dhe.ibm.com/infocenter/tivihelp/v48r1/index.jsp?topic=%2Fcom.ibm.sco.doc_2.2%2Fenablement%2Fworkfloworchestration.html&lt;/ref&gt;

Workflow engines mainly have three functions:
*	Verification of the current status: Check whether the command is valid in executing a task.
*	Determine the authority of users: Check if the current user is permitted to execute the task.
*	Executing condition script: After passing the previous two steps, the workflow engine begins to evaluate the condition script in which the two processes are carried out, if the condition is true, workflow engine execute the task, and if the execution successfully completes, it returns the success, if not, it reports the error to trigger and roll back the change.&lt;ref&gt;The Workflow Engine Model. [http://msdn.microsoft.com/en-us/library/aa188337%28office.10%29.aspx  The Workflow Engine Model] Accessed 1 Dec. 2010.&lt;/ref&gt;

A workflow engine is a core technique for task allocation software, such as [[business process management]], in which the workflow engine allocates tasks to different executors while communicating data among participants. A workflow engine can execute any arbitrary sequence of steps, for example, a healthcare data analysis.&lt;ref name=hf2010&gt;{{Cite journal | last1 = Huser | first1 = V. | last2 = Rasmussen | first2 = L. V. | last3 = Oberg | first3 = R. | last4 = Starren | first4 = J. B. | title = Implementation of workflow engine technology to deliver basic clinical decision support functionality | doi = 10.1186/1471-2288-11-43 | journal = BMC Medical Research Methodology | volume = 11 | pages = 43 | year = 2011 | pmid = 21477364 | pmc = 3079703}}&lt;/ref&gt;

== See also ==
*[[Business rules engine]]
*[[Business rule management system]]
*[[Comparison of BPEL engines]]
*[[Inference engine]]
*[[Java Rules Engine API]]
*[[Rete algorithm]]
*[[Ripple down rules]]
*[[Semantic reasoner]]
*[[BPEL|Business Process Execution Language]]
*[[Production system (computer science)|Production system]]
*[[Workflow management system]]

== References ==
{{Reflist}}

[[Category:Data management]]
[[Category:Servers (computing)]]
[[Category:Workflow technology]]
[[Category:Workflow software]]</text>
      <sha1>sy0rnkx6d7ot9whrbfyjik7xq5soa03</sha1>
    </revision>
  </page>
  <page>
    <title>Database administration and automation</title>
    <ns>0</ns>
    <id>8078610</id>
    <revision>
      <id>711470821</id>
      <parentid>706780344</parentid>
      <timestamp>2016-03-23T02:06:54Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor />
      <comment>/* See also */Remove blank line(s) between list items per [[WP:LISTGAP]] to fix an accessibility issue for users of [[screen reader]]s. Do [[WP:GENFIXES]] and cleanup if needed. Discuss this at [[Wikipedia talk:WikiProject Accessibility#LISTGAP]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9462" xml:space="preserve">{{More footnotes|date=March 2011}}
'''Database administration''' is the function of managing and maintaining [[database management system]]s (DBMS) software. Mainstream DBMS software such as [[Oracle database|Oracle]], [[IBM DB2]] and [[Microsoft SQL Server]] need ongoing management. As such, corporations that use DBMS software often hire specialized IT ([[Information technology|Information Technology]]) personnel called [[Database administrator|Database Administrators]] or DBAs.

==DBA Responsibilities==

* Installation, configuration and upgrading of Database server software and related products. 
* Evaluate Database features and Database related products.
* Establish and maintain sound backup and recovery policies and procedures. 
* Take care of the [[Database design]] and implementation. 
* Implement and maintain database security (create and maintain users and roles, assign privileges). 
* [[Database tuning]] and performance monitoring. 
* Application tuning and performance monitoring. 
* Setup and maintain documentation and standards. 
* Plan growth and changes (capacity planning). 
* Work as part of a team and provide 24x7 support when required. 
* Do general technical troubleshooting and give cons.
* Database recovery.

== Types of database administration ==
There are three types of DBAs:

#Systems DBAs (also referred to as Physical DBAs, Operations DBAs or Production Support DBAs): focus on the physical aspects of database administration such as DBMS installation, configuration, patching, upgrades, backups, restores, refreshes, performance optimization, maintenance and disaster recovery.
#Development DBAs: focus on the logical and development aspects of database administration such as [[data model]] design and maintenance, DDL ([[Data Definition Language|data definition language]]) generation, SQL writing and tuning, coding [[stored procedure]]s, collaborating with developers to help choose the most appropriate DBMS feature/functionality and other pre-production activities. 
#Application DBAs: usually found in organizations that have purchased [[Third-party developer|3rd party]] [[application software]] such as ERP ([[enterprise resource planning]]) and CRM ([[customer relationship management]]) systems. Examples of such application software includes [[Oracle Applications]], Siebel and [[PeopleSoft]] (both now part of Oracle Corp.) and SAP. Application DBAs straddle the fence between the DBMS and the application software and are responsible for ensuring that the application is fully optimized for the database and vice versa. They usually manage all the [[Software componentry|application components]] that interact with the database and carry out activities such as application installation and patching, application upgrades, database cloning, building and running data cleanup routines, data load [[process management]], etc.

While individuals usually specialize in one type of database administration, in smaller organizations, it is not uncommon to find a single individual or group performing more than one type of database administration.

== Nature of database administration ==
The degree to which the administration of a database is automated dictates the skills and personnel required to manage databases.  On one end of the spectrum, a system with minimal automation will require significant experienced resources to manage; perhaps 5-10 databases per DBA.  Alternatively an organization might choose to automate a significant amount of the work that could be done manually therefore reducing the skills required to perform tasks.  As automation increases, the personnel needs of the organization splits into highly [[skilled worker]]s to create and manage the automation and a group of lower skilled "line" DBAs who simply execute the automation.

Database administration work is complex, repetitive, time-consuming and requires significant training. Since databases hold valuable and mission-critical data, companies usually look for candidates with multiple years of experience. Database administration often requires DBAs to put in work during off-hours (for example, for planned after hours downtime, in the event of a database-related outage or if performance has been severely degraded). DBAs are commonly well compensated for the long hours

One key skill required and often overlooked when selecting a DBA is database recovery (under disaster recovery).  It is not a case of “if” but a case of “when” a database suffers a failure, ranging from a simple failure to a full catastrophic failure.  The failure may be data corruption, media failure, or user induced errors.  In either situation the DBA must have the skills to recover the database to a given point in time to prevent a loss of data.  A highly skilled DBA can spend a few minutes or exceedingly long hours to get the database back to the operational point.

== Database administration tools ==
Often, the DBMS software comes with certain tools to help DBAs manage the DBMS. Such tools are called native tools. For example, Microsoft SQL Server comes with SQL Server Management Studio and Oracle has tools such as [[SQL*Plus]] and Oracle Enterprise Manager/Grid Control. In addition, 3rd parties such as BMC, [[Quest Software]], [[Embarcadero Technologies]], [[EMS Database Management Solutions]] and SQL Maestro Group offer GUI tools to monitor the DBMS and help DBAs carry out certain functions inside the database more easily.

Another kind of database software exists to manage the provisioning of new databases and the management of existing databases and their related resources.  The process of creating a new database can consist of hundreds or thousands of unique steps from satisfying prerequisites to configuring backups where each step must be successful before the next can start.  A human cannot be expected to complete this procedure in the same exact way time after time - exactly the goal when multiple databases exist.  As the number of DBAs grows, without automation the number of unique configurations frequently grows to be costly/difficult to support.  All of these complicated procedures can be modeled by the best DBAs into database automation software and executed by the standard DBAs.  Software has been created specifically to improve the reliability and repeatability of these procedures such as [[Stratavia]]'s [[Data Palette]] and [[GridApp Systems]] Clarity.

== The impact of IT automation on database administration ==
Recently, automation has begun to impact this area significantly. Newer technologies such as [[Stratavia]]'s [[Data Palette]] suite and [[GridApp Systems]] Clarity have begun to increase the automation of databases causing the reduction of database related tasks. However at best this only reduces the amount of mundane, repetitive activities and does not eliminate the need for DBAs. The intention of DBA automation is to enable DBAs to focus on more proactive activities around database architecture, deployment, performance and service level management.

''Every database requires a database owner account that can perform all schema management operations. This account is specific to the database and cannot log in to Data Director. You can add database owner accounts after database creation. Data Director users must log in with their database-specific credentials to view the database, its entities, and its data or to perform database management tasks.
Database administrators and application developers can manage databases only if they have appropriate permissions and roles granted to them by the organization administrator. The permissions and roles must be granted on the database group or on the database, and they only apply within the organization in which they are granted.''

== Learning database administration ==
There are several education institutes that offer professional courses, including late-night programs, to allow candidates to learn database administration. Also, DBMS vendors such as Oracle, Microsoft and IBM offer certification programs to help companies to hire qualified DBA practitioners.  College degree in Computer Science or related field is helpful but not necessarily a prerequisite.

==See also==

*[[Column-oriented DBMS]]
*[[Data warehouse]]
*[[Directory service]]
*[[Distributed database management system]]
*[[Hierarchical model]]
*[[Navigational database]]
*[[Network model]]
*[[Object model]]
*[[Object database]] (OODBMS)
*[[Object-relational database]] (ORDBMS)
*[[Run Book Automation]] (RBA)
*[[Relational model]] (RDBMS)
*[[Comparison of relational database management systems]]
*[[Comparison of database tools]]
*[[SQL]] is a language for database management

== External links ==
* {{cite journal | publisher = ACM [[Special Interest Group on Information Retrieval]] | work = SIGIR Forum | volume = 7 | issue = 4 | date = Winter 1972 | pages = 45–55 | url = http://portal.acm.org/citation.cfm?id=1095495.1095500 | title = A set theoretic data structure and retrieval language }}
* {{cite journal | publisher = [[SIGMOD|ACM Special Interest Group on Management of Data]] | work = SIGMOD Record | volume = 35 | issue = 2 | date = June 2006 | url = http://www.tomandmaria.com/tom/Writing/VeritableBucketOfFactsSIGMOD.pdf | title = Origins of the Data Base Management System | format=PDF | author = Thomas Haigh }}

{{Databases}}
{{FOLDOC}}

[[Category:Database management systems]]
[[Category:Data management]]</text>
      <sha1>h3avt8pd3zctkoezzbdeaovbputl71r</sha1>
    </revision>
  </page>
  <page>
    <title>Content Engineering</title>
    <ns>0</ns>
    <id>7829016</id>
    <revision>
      <id>697741270</id>
      <parentid>695600181</parentid>
      <timestamp>2016-01-01T16:16:10Z</timestamp>
      <contributor>
        <ip>123.201.65.218</ip>
      </contributor>
      <comment>editing</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1570" xml:space="preserve">{{Unreferenced|date=May 2009}}
'''Content Engineering''' is a term applied to an engineering speciality dealing with the issues around the use of [[Content (media and publishing)|content]] in computer-facilitated environments.  Content production, [[content management]], content modelling, content conversion, and content use and repurposing are all areas involving this speciality.  It is not a speciality with wide industry recognition and is often performed on an ad hoc basis by members of software development or content production staff, but is beginning to be recognized as a necessary function in any complex content-centric project involving both content production as well as software system development.

Content engineering tends to bridge the gap between groups involved in the production of content ([[Publishing]] and [[Editing|Editorial staff]], [[Marketing]], [[Sales]], [[Human resources|HR]]) and more technologically oriented departments such as [[Software Development]], or [[Information technology|IT]] that put this content to use in web or other software-based environments, and requires an understanding of the issues and processes of both sides.

Typically, Content Engineering involves extensive use of Embedded,  [[XML]] technologies, XML being the most widespread language for representing structured content. [[Content_management_system|Content Management Systems]] are often key technology used in this practice though frequently Content Engineering fills the gap where no formal CMS has been put into place.

[[Category:Data management]]</text>
      <sha1>jbn8ech15k7jv7e2my19994mnfrhs94</sha1>
    </revision>
  </page>
  <page>
    <title>Versomatic</title>
    <ns>0</ns>
    <id>9372544</id>
    <revision>
      <id>655076536</id>
      <parentid>532264024</parentid>
      <timestamp>2015-04-05T18:48:11Z</timestamp>
      <contributor>
        <username>RadicalRedRaccoon</username>
        <id>15526134</id>
      </contributor>
      <minor />
      <comment>Removed excess spaces.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2556" xml:space="preserve">'''Versomatic''' installs as a file system service where it tracks file changes and preemptively archives a copy of a file before it is modified. Archiving copies pre-emptively obviates the need to archive a reference copy of the files beforehand, as would be the case if the files were archived after being edited.

Starting from the moment Versomatic is installed, the last version of a file remains where the user expects it to be, and prior versions, if any, reside in a separate archive. Without this capability, we would have to scan your entire hard drive beforehand and make a duplicate copy of every file in order to create a baseline for subsequent revisions.

Files can be moved, renamed and copied without losing connection to their revision histories. Depending on user preferences, files can be monitored on local, removable and network drives. File revision histories are stored in a central database. Thus, for example, a user may insert a USB drive into his computer and edit a file on the USB drive. The edited file remains on the USB drive but a copy of the original unedited version is copied to the archive.

When a file is deleted, the deleted file is added to the repository together with any previous versions of the deleted file. This provides excellent level of protection against inadvertent file deletion.

Access to prior versions of a file is easy. A user merely selects a file and clicks the right mouse button to display a contextual pop-up menu listing the X most recent previous versions of the file, if any.

Upon user selection of one of the entries the appropriate previous version is opened with read-only privileges using the same application that created the file. Versomatic can also retrieve a copy of a previous version and move it into the same directory where the current version resides. The previous version will have a date &amp; time stamp added to its file name in order to distinguish it from the current version. If a previous version is exported from Versomatic’s archive and changes are made thereto, a new revision history is created for the new file.

==History==

Versomatic is the first product from the long time team of [[Joaquin de Soto]], Jorge Miranda and Manny Menendez released under their new company [[Acertant]]. The team has a long list of hit products to their credit: MacLightning, [[ACD Canvas|Canvas]], Spelling Coach, BigThesaurus, Comment, UltraPaint, Artworks, [[ACDSee]], DenebaCAD, etc.

==External links==
*[http://www.acertant.com Company Web Page]

[[Category:Data management]]</text>
      <sha1>cdes6py2m55sbww1bmd7t5310c8mjzo</sha1>
    </revision>
  </page>
  <page>
    <title>Comparison of ADO and ADO.NET</title>
    <ns>0</ns>
    <id>10701295</id>
    <revision>
      <id>635858892</id>
      <parentid>509011794</parentid>
      <timestamp>2014-11-29T04:58:37Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor />
      <comment>Robot - Speedily moving category .NET framework to [[:Category:.NET Framework]] per [[WP:CFDS|CFDS]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3131" xml:space="preserve">''Note: The following content requires a knowledge of [[database]] technologies.''

The following is a comparison of two different database access technologies from [[Microsoft]], namely, [[ActiveX Data Objects|ActiveX Data Objects (ADO)]] and [[ADO.NET]]. Before comparing the two technologies, it is essential to get an overview of [[Microsoft Data Access Components]] (MDAC) and the [[.NET Framework]]. [[Microsoft Data Access Components]] provide a uniform and comprehensive way of developing applications for accessing almost any data store entirely from [[Managed code#Managed and unmanaged|unmanaged code]]. The [[.NET Framework]] is an [[Virtual machine#Application virtual machine|application virtual machine]]-based software environment that provides security mechanisms, [[memory management]], and [[exception handling]] and is designed so that developers need not consider the capabilities of the specific CPU that will execute the .NET application. The .NET [[Virtual machine#Application virtual machine|application virtual machine]] turns [[intermediate language]] (IL) into machine code. High-level language compilers for [[C Sharp (programming language)|C#]], [[Visual Basic .NET|VB.NET]] and [[C++]] are provided to turn source code into IL. [[ADO.NET]] is shipped with the Microsoft NET Framework.

[[ActiveX Data Objects|ADO]] relies on [[Component Object Model|COM]] whereas [[ADO.NET]] relies on managed-providers defined by the .NET [[Common Language Runtime|CLR]]. ADO.NET does not replace ADO for the COM programmer; rather, it provides the .NET programmer with access to relational data sources, XML, and application data.

{| class="wikitable"
|-
! 
! ADO
! ADO.NET
|-
| Business Model
| Connection-oriented Models used mostly
| Disconnected models are used:Message-like Models.
|-
| Disconnected Access
| Provided by Record set
| Provided by Data Adapter and Data set
|-
| [[XML]] Support
| Limited
| Robust Support
|-
|Connection Model
|Client application needs to be connected always to data-server while working on the data, unless using client-side cursors or a disconnected Record set
|Client disconnected as soon as the data is processed. DataSet is disconnected at all times.
|-
|Data Passing
|ADO objects communicate in binary mode.
|ADO.NET uses XML for passing the data.
|-
|Control of data access behaviors
|Includes implicit behaviors that may not always be required in an application and that may therefore limit performance.
|Provides well-defined, factored components with predictable behavior, performance, and semantics.
|-
|Design-time support
|Derives information about data implicitly at run time, based on metadata that is often expensive to obtain.
|Leverages known metadata at design time in order to provide better run-time performance and more consistent run-time behavior.
|}

== References ==
* [http://msdn2.microsoft.com/en-us/library/ms973217.aspx ADO.NET for the ADO programmer]

[[Category:Data management]]
[[Category:.NET Framework]]
[[Category:Microsoft application programming interfaces]]
[[Category:SQL data access]]
[[Category:Software comparisons|ADO and ADO.NET]]</text>
      <sha1>tf10r6yvdr2z92z3s5huh62ofpa9dbr</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Transaction processing</title>
    <ns>14</ns>
    <id>11300280</id>
    <revision>
      <id>586756500</id>
      <parentid>544805793</parentid>
      <timestamp>2013-12-19T07:35:25Z</timestamp>
      <contributor>
        <username>Codename Lisa</username>
        <id>16847332</id>
      </contributor>
      <comment>removed [[Category:System software]]; added [[Category:Utility software by type]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="159" xml:space="preserve">{{Cat main|Transaction processing}}

[[Category:Concurrency control]]
[[Category:Data management]]
[[Category:Utility software by type]]
[[Category:Databases]]</text>
      <sha1>kbyik55mn8agrequ2q5cr877v09b9p5</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Recording</title>
    <ns>14</ns>
    <id>1479108</id>
    <revision>
      <id>543943325</id>
      <parentid>537137306</parentid>
      <timestamp>2013-03-13T22:34:50Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor />
      <comment>[[User:Addbot|Bot:]] Migrating 9 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q6580221]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="116" xml:space="preserve">{{Commons category|Recording}}
{{Cat main|Recording}}

[[Category:Data management]]
[[Category:Information storage]]</text>
      <sha1>44kcubtvwcjez2llpf0aeqk35v2rjgp</sha1>
    </revision>
  </page>
  <page>
    <title>CA Gen</title>
    <ns>0</ns>
    <id>965842</id>
    <revision>
      <id>725386288</id>
      <parentid>725386121</parentid>
      <timestamp>2016-06-15T10:13:11Z</timestamp>
      <contributor>
        <ip>212.167.5.6</ip>
      </contributor>
      <comment>/* External links */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5963" xml:space="preserve">'''CA Gen''' is a [[Computer Aided Software Engineering]] (CASE) application development environment marketed by [[CA Technologies]]. Gen was previously known as '''IEF''' ('''Information Engineering Facility'''), '''Composer by IEF''', '''Composer''', '''COOL:Gen''', '''Advantage:Gen''' and '''AllFusion Gen'''.

The toolset originally supported the [[information engineering]] methodology developed by [[Clive Finkelstein]], [[James Martin (author)|James Martin]] and others in the early 1980s. Early versions supported IBM's [[IBM DB2|DB2]] database, [[IBM 3270|3270]] 'block mode' screens and generated [[COBOL]] code.

In the intervening years the toolset has been expanded to support additional development techniques such as [[Component-based software engineering|component-based development]]; creation of [[Client–server model|client/server]] and [[web application]]s and generation of [[C (programming language)|C]], [[Java (programming language)|Java]] and [[C Sharp (programming language)|C#]]. In addition, other platforms are now supported such as many variants of *ix-like Operating Systems (AIX, HP-UX, Solaris, Linux) as well as Windows.

Its range of supported database technologies have widened to include [[Oracle Database|ORACLE]], [[Microsoft SQL Server]], [[ODBC]], [[Java Database Connectivity|JDBC]] as well as the original DB2.

The toolset is fully integrated - objects identified during analysis carry forward into design without redefinition. All information is stored in a repository (central encyclopedia). The encyclopedia allows for large team development - controlling access so that multiple developers may not change the same object simultaneously.&lt;ref&gt;https://communities.ca.com/web/ca-gen-edge-global-user-community/wiki/-/wiki/EDGE+User+Group+CA+Gen+Wiki/What+is+CA+Gen?&amp;#p_36&lt;/ref&gt;

==Overview==
It was initially produced by [[Texas Instruments]], with input from [[James Martin (author)|James Martin]] and his consultancy firm James Martin Associates, and was based on the Information Engineering Methodology (IEM). The first version was launched in 1987.

IEF became popular among large government departments and public utilities. It initially supported a [[CICS]]/COBOL/DB2 target environment.  However, it now supports a wider range of relational databases and operating systems. IEF was intended to shield the developer from the complexities of building complete multi-tier cross-platform applications.

In 1995, Texas Instruments decided to change their marketing focus for the product. Part of this change included a new name - "Composer".

By 1996, IEF had become a popular tool. However, it was criticized by some IT professionals for being too restrictive, as well as for having a high per-workstation cost ($15K USD). But it is claimed that IEF reduces development time and costs by removing complexity and allowing rapid development of large scale enterprise transaction processing systems.

In 1997, Composer had another change of branding, Texas Instruments sold the [[Texas Instruments Software]] division, including the Composer rights, to [[Sterling Software]]. Sterling software changed the well known name "Information Engineering Facility" to "COOL:Gen". COOL was an acronym for "Common Object Oriented Language" - despite the fact that there was little [[Object-oriented programming|object orientation]] in the product.

In 2000, Sterling Software was acquired by [[Computer Associates]] (now CA). CA has rebranded the product three times to date and the product is still used widely today. Under CA, recent releases of the tool added support for the CA-[[DATACOM/DB|Datacom]] DBMS, the Linux operating system, C# code generation and [[ASP.NET]] web clients. The current version is known as CA Gen - version 8 being released in May 2010, with support for customised web services, and more of the toolset being based around the [[Eclipse (software)|Eclipse framework]].

There are a variety of "add-on" tools available for CA Gen, including Project Phoenix from Jumar - a collection of software tools and services focused on the modernisation and re-platforming of existing/legacy CA Gen applications to new environments,&lt;ref&gt;[http://www.jumar-solutions.com/ Jumar]&lt;/ref&gt; GuardIEn - a [[Configuration Management]] and Developer Productivity Suite,&lt;ref&gt;[http://www.iet.co.uk IET Ltd]&lt;/ref&gt; QAT Wizard,&lt;ref&gt;[http://www.qat.com/qat_wizard.asp QAT Wizard]&lt;/ref&gt; an interview style wizard that takes advantage of the meta model in Gen, products for multi-platform application reporting and XML/SOAP enabling of Gen applications.,&lt;ref&gt;[http://www.canamsoftware.com/ Canam Software Labs]&lt;/ref&gt; and developer productivity tools such as Access Gen, APMConnect, QA Console and Upgrade Console from Response Systems &lt;ref&gt;[http://www.response-systems.com Response Systems]&lt;/ref&gt;
Recently CA GEN has released its latest version 8.5.

==References==
{{Reflist}}

==External links==
* [http://www.uk.capgemini.com/public-sector/tax-welfare/regenerate Capgemini REGENERATE offering] - Support, Update, Migrate
* [http://www.edgeusergroup.org EDGE User Group] - the user group for CA Gen
* [http://www.edgeusergroup.org/wiki CA Gen Wiki] - sponsored by the EDGE User Group
* [http://www.gentalk.biz gentalk.biz] - CA Gen Blog - inactive
* [http://www.qat.com QAT Global] - CA Gen Services and Training Provider (USA)
* [http://www.iet.co.uk IET] - CA Gen Product and Services Provider (UK)
* [http://www.jumar-solutions.com/ Jumar Solutions] - CA Gen Product and Services Provider (UK)
* [http://www.response-systems.com/ Response Systems] - CA Gen Product and Services Provider (UK)
* [http://www.facet.com.au/ Facet Consulting] - CA Gen Services Provider (Australia)
* [http://www.canamsoftware.com/ Canam Software Labs, Inc.] - CA Gen Product and Service Provider (Canada)

[[Category:Computer-aided software engineering tools]]
[[Category:Data management]]
[[Category:CA Technologies]]


Edited by: Sambit Mishra</text>
      <sha1>7rbv8tdyrhzaxki5503od13or719924</sha1>
    </revision>
  </page>
  <page>
    <title>Data Reference Model</title>
    <ns>0</ns>
    <id>3576033</id>
    <revision>
      <id>662285168</id>
      <parentid>662275258</parentid>
      <timestamp>2015-05-14T11:14:09Z</timestamp>
      <contributor>
        <username>Melcous</username>
        <id>20472590</id>
      </contributor>
      <minor />
      <comment>Reverted 1 edit by [[Special:Contributions/62.254.171.194|62.254.171.194]] ([[User talk:62.254.171.194|talk]]) to last revision by Omnipaedista. ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3134" xml:space="preserve">[[Image:DRM Collaboration Process.jpg|thumb|320px|The DRM Collaboration Process.]]
The '''Data Reference Model''' ('''DRM''') is one of the five reference models of the [[Federal Enterprise Architecture]] (FEA). 

== Overview ==
The DRM is a framework whose primary purpose is to enable information sharing and reuse across the [[United States federal government]] via the standard description and discovery of common data and the promotion of uniform data management practices. The DRM describes artifacts which can be generated from the data architectures of federal government agencies. The DRM provides a flexible and standards-based approach to accomplish its purpose. The scope of the DRM is broad, as it may be applied within a single agency, within a [[Community of interest (computer security)|Community of Interest]] (COI)1, or cross-COI.

== Data Reference Model topics ==
=== DRM structure ===
The DRM provides a standard means by which [[data]] may be described, categorized, and shared. These are reflected within each of the DRM’s three standardization areas:

* ''Data Description'': Provides a means to uniformly describe data, thereby supporting its discovery and sharing.
* ''Data Context'': Facilitates discovery of data through an approach to the categorization of data according to taxonomies. Additionally, enables the definition of authoritative data assets within a COI.
* ''Data Sharing'': Supports the access and exchange of data where access consists of ''ad hoc'' requests (such as a query of a data asset), and exchange consists of fixed, re-occurring transactions between parties. Enabled by capabilities provided by both the Data Context and Data Description standardization areas.

===DRM Version 2 ===
The Data Reference Model version 2 released in November 2005 is a 114 page document with detailed architectural diagrams and an extensive glossary of terms.

The DRM also make many references to ISO standards specifically the [[ISO/IEC 11179]] metadata registry standard.

=== DRM usage ===
The DRM is not technically a published technical interoperability standard such as web services, it is an excellent starting point for data architects within federal and state agencies.  Any federal or state agencies that are involved with exchanging information with other agencies or that are involved in [[Data warehouse]]ing efforts should use this document as a guide.

==See also==
* [[Enterprise architecture framework]]
* [[Enterprise application integration]]
* [[Enterprise service bus]]
* [[Federal Enterprise Architecture]]
* [[ISO/IEC 11179]]
* [[Metadata publishing]]
* [[Semantic spectrum]]
* [[Semantic web]]
* [[Synonym ring]]
&lt;!--
== References ==
{{reflist}}--&gt;

==External links==
* [https://web.archive.org/web/20070617034325/http://www.defenselink.mil/cio-nii/docs/DoD_DRM_V04_5aug.pdf US Department of Defense Data Reference Model]
* [http://www.whitehouse.gov/sites/default/files/omb/assets/egov_docs/DRM_2_0_Final.pdf US Federal Enterprise Architecture Program Data Reference Model Version 2.0]
[[Category:Computer data]]
[[Category:Data management]]
[[Category:Reference models]]</text>
      <sha1>sv73w9cg9yj9ypm0fj2b6otbz2io53o</sha1>
    </revision>
  </page>
  <page>
    <title>Tagsistant</title>
    <ns>0</ns>
    <id>12989031</id>
    <revision>
      <id>748259515</id>
      <parentid>677002446</parentid>
      <timestamp>2016-11-07T07:39:54Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor />
      <comment>/* Main criticisms */[[WP:CHECKWIKI]] error fixes using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9171" xml:space="preserve">{{POV|date=October 2012}}

{{Infobox software
| name                   = Tagsistant
| logo                   = [[file:Tagsistant logo.png|300px]]
| developer              = Tx0 &lt;tx0@strumentiresistenti.org&gt;
| latest release version = 0.6
| frequently_updated     = yes&lt;!-- Release version update? Don't edit this page, just click on the version number! --&gt;
| programming language   = [[C (programming language)|C]]
| operating system       = Linux kernel
| language               = English
| genre                  = [[Semantic file system]]
| license                = [[GNU General Public License|GNU GPL]]
| website                = http://www.tagsistant.net/
}}
{{Infobox filesystem
| name = Tagsistant
| developer = Tx0
| full_name =
| introduction_date =
| introduction_os =
| partition_id =
| directory_struct =
| file_struct =
| bad_blocks_struct =
| max_file_size =
| max_files_no =
| max_filename_size =
| max_volume_size =
| dates_recorded =
| date_range =
| date_resolution =
| forks_streams =
| attributes =
| file_system_permissions =
| compression =
| encryption =
| OS =
}}
'''Tagsistant''' is a [[semantic file system]] for the [[Linux kernel]], written in [[C (programming language)|C]] and based on [[Filesystem in Userspace|FUSE]]. Unlike traditional [[file systems]] that use hierarchies of directories to locate objects, Tagsistant introduces the concept of [[Tag (metadata)|tags]].

==Design and differences with hierarchical file systems==

In computing, a [[file system]] is a type of data store which could be used to store, retrieve and update [[Computer file|files]]. Each file can be uniquely located by its [[Path (computing)|path]]. The user must know the path in advance to access a file and the path does not necessarily include any information about the content of the file.

Tagsistant uses a complementary approach based on [[Tag (metadata)|tags]]. The user can create a set of tags and apply those tags to files, [[File directory|directories]] and other objects ([[Device file|devices]], [[Named pipe|pipes]], ...). The user can then search all the objects that match a subset of tags, called a query. This kind of approach is well suited for managing user contents like pictures, audio recordings, movies and text documents but is incompatible with system files (like libraries, commands and configurations) where the univocity of the path is a [[Computer security|security]] requirement to prevent the access to a wrong content.

==The tags/ directory==

A Tagsistant file system features four main directories:

:archive/
:relations/
:stats/
:tags/

Tags are created as sub directories of the &lt;code&gt;tags/&lt;/code&gt; directory and can be used in queries complying to this syntax:

:&lt;code&gt;tags/subquery/[+/subquery/[+/subquery/]]/@/&lt;/code&gt;&lt;ref&gt;{{cite web|title=tags/ and relations/ directories|url=http://www.tagsistant.net/documents-about-tagsistant/0-6-howto?showall=&amp;start=3}}&lt;/ref&gt;

where a subquery is an arbitrarily long list of tags, concatenated as directories:

:&lt;code&gt;tag1/tag2/tag3/.../tagN/&lt;/code&gt;

The portion of a path delimited by &lt;code&gt;tags/&lt;/code&gt; and &lt;code&gt;@/&lt;/code&gt; is the actual query. The &lt;code&gt;+/&lt;/code&gt; operator joins the results of different sub-queries in one single list. The &lt;code&gt;@/&lt;/code&gt; operator ends the query.

To be returned as a result of the following query:

:&lt;code&gt;tags/t1/t2/+/t1/t4/@/&lt;/code&gt;

an object must be tagged as both &lt;code&gt;t1/&lt;/code&gt; and &lt;code&gt;t2/&lt;/code&gt; or as both &lt;code&gt;t1/&lt;/code&gt; and &lt;code&gt;t4/&lt;/code&gt;. Any object tagged as &lt;code&gt;t2/&lt;/code&gt; or &lt;code&gt;t4/&lt;/code&gt;, but not as &lt;code&gt;t1/&lt;/code&gt; will not be retrieved.

The query syntax deliberately violates the [[POSIX]] file system semantics by allowing a path token to be a descendant of itself, like in &lt;code&gt;tags/t1/t2/+/t1/t4/@&lt;/code&gt; where &lt;code&gt;t1/&lt;/code&gt; appears twice. As a consequence a recursive scan of a Tagsistant file system  will exit with an error or endlessly loop, as done by [[UNIX]] &lt;code&gt;[[Find|find]]&lt;/code&gt;:

&lt;syntaxhighlight lang="bash"&gt;
~/tagsistant_mountpoint$ find tags/
tags/
tags/document
tags/document/+
tags/document/+/document
tags/document/+/document/+
tags/document/+/document/+/document
tags/document/+/document/+/document/+
[...]
&lt;/syntaxhighlight&gt;

This drawback is balanced by the possibility to list the tags inside a query in any order. The query &lt;code&gt;tags/t1/t2/@/&lt;/code&gt; is completely equivalent to &lt;code&gt;tags/t2/t1/@/&lt;/code&gt; and &lt;code&gt;tags/t1/+/t2/t3/@/&lt;/code&gt; is equivalent to &lt;code&gt;tags/t2/t3/+/t1/@/&lt;/code&gt;.

The &lt;code&gt;@/&lt;/code&gt; element has the precise purpose of restoring the POSIX semantics: the path &lt;code&gt;tags/t1/@/directory/&lt;/code&gt; refers to a traditional directory and a recursive scan of this path will properly perform.

==The reasoner and the relations/ directory==

Tagsistant features a simple [[Semantic Reasoner|reasoner]] which expands the results of a query by including objects tagged with related tags. A relation between two tags can be established inside the &lt;code&gt;relations/&lt;/code&gt; directory following a three level pattern:

:&lt;code&gt;relations/tag1/rel/tag2/&lt;/code&gt;

The &lt;code&gt;rel&lt;/code&gt; element can be ''includes'' or ''is_equivalent''. To include the ''rock'' tag in the ''music'' tag, the UNIX command &lt;code&gt;mkdir&lt;/code&gt; can be used:

:&lt;code&gt;mkdir -p relations/music/includes/rock&lt;/code&gt;

The reasoner can recursively resolve relations, allowing the creation of complex structures:

:&lt;code&gt;mkdir -p relations/music/includes/rock&lt;/code&gt;
:&lt;code&gt;mkdir -p relations/rock/includes/hard_rock&lt;/code&gt;
:&lt;code&gt;mkdir -p relations/rock/includes/grunge&lt;/code&gt;
:&lt;code&gt;mkdir -p relations/rock/includes/heavy_metal&lt;/code&gt;
:&lt;code&gt;mkdir -p relations/heavy_metal/includes/speed_metal&lt;/code&gt;

The web of relations created inside the &lt;code&gt;relations/&lt;/code&gt; directory constitutes a basic form of [[Ontology (information science)|ontology]].

==Autotagging plugins==

Tagsistant features an autotagging plugin stack which gets called when a file or a symlink is written.&lt;ref&gt;{{cite web|title=How to write a plugin for Tagsistant?|url=http://www.tagsistant.net/documents-about-tagsistant/coding-and-debugging/7-how-to-write-a-plugin-for-tagsistant}}&lt;/ref&gt; Each plugin is called if its declared [[Mime type|MIME type]] matches

The list of working plugins released with Tagsistant 0.6 is limited to:

* text/html: tags the file with each word in &lt;code&gt;&lt;title&gt;&lt;/code&gt; and &lt;code&gt;&lt;keywords&gt;&lt;/code&gt; elements and with ''document'', ''webpage'' and ''html'' too
* image/jpeg: tags the file with each [[Exchangeable image file format|Exif]] tag

==The repository==

Each Tagsistant file system has a corresponding repository containing an &lt;code&gt;archive/&lt;/code&gt; directory where the objects are actually saved and a &lt;code&gt;tags.sql&lt;/code&gt; file holding tagging information as an [[SQLite]] database. If the [[MySQL]] database engine was specified with the &lt;code&gt;--db&lt;/code&gt; argument, the &lt;code&gt;tags.sql&lt;/code&gt; file will be empty. Another file named &lt;code&gt;repository.ini&lt;/code&gt; is a [[GLib]] ini store with the repository configuration.&lt;ref&gt;{{cite web|title=Key-value file parser|url=https://developer.gnome.org/glib/2.32/glib-Key-value-file-parser.html}}&lt;/ref&gt;

Tagsistant 0.6 is compatible with the MySQL and Sqlite dialects of SQL for tag reasoning and tagging resolution. While porting its logic to other SQL dialects is possible, differences in basic constructs (especially the INTERSECT SQL keyword) must be considered.

==The archive/ and stats/ directories==

The &lt;code&gt;archive/&lt;/code&gt; directory has been introduced to provide a quick way to access objects without using tags. Objects are listed with their inode number prefixed.&lt;ref&gt;{{cite web|title=Tagsistant 0.6 howto - Inodes|url=http://www.tagsistant.net/documents-about-tagsistant/0-6-howto?showall=&amp;start=6}}&lt;/ref&gt;

The &lt;code&gt;stats/&lt;/code&gt; directory features some read-only files containing usage statistics. A file &lt;code&gt;configuration&lt;/code&gt; holds both compile time information and current repository configuration.

==Main criticisms==

It has been highlighted that relying on an external database to store tags and tagging information could cause the complete loss of metadata if the database gets corrupted.&lt;ref&gt;{{cite web|title=Extended attributes and tag file systems|url=http://www.lesbonscomptes.com/pages/tagfs.html}}&lt;/ref&gt;

It has been highlighted that using a flat namespace tends to overcrowd the &lt;code&gt;tags/&lt;/code&gt; directory.&lt;ref&gt;{{cite web|title=The major problem with this approach is scalability|publisher=https://news.ycombinator.com/item?id=2573318}}&lt;/ref&gt; This could be mitigated introducing [[Tag (metadata)#Triple tags|triple tags]].

==See also==
{{Portal|Free software}}
[[Semantic file system]]

==References==
{{Reflist}}

==External links==
* {{Official website|http://www.tagsistant.net/}}
* [https://aur.archlinux.org/packages.php?ID=54644 Arch Linux package]
* [https://news.ycombinator.com/item?id=2573318 Discussion on Hacker News]
* [http://www.lesbonscomptes.com/pages/tagfs.html Extended attributes and tag file systems]
* [http://lakm.us/logit/2010/03/tagsistant-on-production-2/ Tagsistant On Production]

[[Category:Computer file systems]]
[[Category:Data management]]
[[Category:Semantic file systems]]</text>
      <sha1>gkyvib83wpoyik8o5eco8mh75yuvnxh</sha1>
    </revision>
  </page>
  <page>
    <title>Client-side persistent data</title>
    <ns>0</ns>
    <id>13150801</id>
    <revision>
      <id>621409809</id>
      <parentid>603840031</parentid>
      <timestamp>2014-08-15T21:48:54Z</timestamp>
      <contributor>
        <username>Jmabel</username>
        <id>28107</id>
      </contributor>
      <comment>/* See also */ + [[Web storage]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1429" xml:space="preserve">'''Client-side persistent data''' or CSPD is a term used in [[computing]] for storing data required by [[web application |web applications]] to complete internet tasks on the [[client-side]] as needed rather than exclusively on the [[Server (computing) |server]]. As a framework it is one solution to the needs of [[Occasionally connected computing]] or OCC.

A major challenge for [[HTTP]] as a [[Stateless server |stateless]] [[Protocol (computing)|protocol]] has been asynchronous tasks. The [[Ajax (programming)|AJAX]] pattern using [[XMLHttpRequest]] was first introduced by [[Microsoft]] in the context of the [[Outlook Web App|Outlook]] e-mail product.

The first CSPD were the [[HTTP cookie |'cookies']] introduced by the [[Netscape]] [[Netscape (web browser)|Navigator]].  [[ActiveX]] components which have entries in the [[Windows registry]] can also be viewed as a form of [[client-side]] [[Persistence (computer science)|persistence]].

==See also==
* [[Occasionally connected computing]]
* [[Curl (programming_language)]]
* [[Ajax (programming)|AJAX]]
* [[HTTP]]
* [[Web storage]]

==External links==
* [http://www.curl.com/developer/faq/cspd/ CSPD]
* [http://safari.ciscopress.com/0596101996/jscript5-CHP-19-SECT-6 Safari] preview
* [http://wp.netscape.com/newsref/std/cookie_spec.html Netscape] on persistent client state

[[Category:Clients (computing)]]
[[Category:Data management]]
[[Category:Web applications]]</text>
      <sha1>0qaq04jf9hzew8j13w65xf8prc2fr73</sha1>
    </revision>
  </page>
  <page>
    <title>Microsoft SQL Server Master Data Services</title>
    <ns>0</ns>
    <id>13430116</id>
    <revision>
      <id>736956402</id>
      <parentid>736955850</parentid>
      <timestamp>2016-08-30T22:33:44Z</timestamp>
      <contributor>
        <username>Vpnicholls</username>
        <id>29068868</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7876" xml:space="preserve">{{multiple issues|
{{Advert|date=March 2011}}
{{Update|inaccurate=yes|date=April 2010}}
}}

'''Microsoft SQL Server Master Data Services''' is a [[Master Data Management]] (MDM) product from [[Microsoft]] that ships as a part of the [[Microsoft SQL Server]] relational database management system.&lt;ref&gt;https://msdn.microsoft.com/en-us/library/ms130214.aspx&lt;/ref&gt;  Master Data Services (MDS) is the SQL Server solution for master data management. Master data management (MDM) enables your organization to discover and define non-transactional lists of data, and compile maintainable, reliable master lists. Master Data Services first shipped with Microsoft SQL Server 2008 R2.  Microsoft SQL Server 2016 includes many enhancements to Master Data Services, such as improved performance and security, and the ability to clear transaction logs, create custom indexes, share entity data between different models, and support for many-to-many relationships. For more information, see [https://msdn.microsoft.com/en-us/library/ff929136.aspx What's New in Master Data Services (MDS)]

==Overview==
In Master Data Services, the model is the highest level container in the structure of your master data. You create a model to manage groups of similar data. A model contains one or more entities, and entities contain members that are the data records. An entity is similar to a table.

Like other MDM products, Master Data Services aims to create a centralized data source and keep it synchronized, and thus reduce redundancies, across the applications which process the data.{{cn|date=January 2015}}

Sharing the architectural core with Stratature +EDM, Master Data Services uses a [[Microsoft SQL Server]] database as the physical data store. It is a part of the ''Master Data Hub'', which uses the database to store and manage data [[Entity Data Model|entities]].{{cn|date=January 2015}} It is a database with the software to validate and manage the data, and keep it synchronized with the systems that use the data.&lt;ref name="arch"&gt;{{cite web | url = http://msdn2.microsoft.com/en-us/library/bb410798.aspx | title = Master Data Management (MDM) Hub Architecture | author = Roger Walter | publisher = MSDN TechNet | accessdate = 2007-09-25}}&lt;/ref&gt; The master data hub has to extract the data from the source system, validate, sanitize and shape the data, remove duplicates, and update the hub repositories, as well as synchronize the external sources.&lt;ref name="arch"/&gt; The entity schemas, attributes, data hierarchies, validation rules and access control information are specified as [[metadata]] to the Master Data Services runtime. Master Data Services does not impose any limitation on the data model. Master Data Services also allows custom ''Business rules'', used for validating and sanitizing the data entering the data hub, to be defined, which is then run against the data matching the specified criteria. All changes made to the data are validated against the rules, and a log of the transaction is stored persistently. Violations are logged separately, and optionally the owner is notified, automatically. All the data entities can be [[Revision control system|versioned]].{{cn|date=January 2015}}

Master Data Services allows the master data to be categorized by hierarchical relationships, such as employee data are a subtype of organization data. Hierarchies are generated by relating data attributes. Data can be automatically categorized using rules, and the categories are introspected programmatically. Master Data Services can also expose the data as [[Microsoft SQL Server]] [[view (database)|views]], which can be pulled by any [[SQL]]-compatible client. It uses a role-based access control system to restrict access to the data. The views are generated dynamically, so they contain the latest data entities in the master hub. It can also push out the data by writing to some external journals. Master Data Services also includes a web-based UI for viewing and managing the data. It uses [[AJAX]] in the front-end and [[ASP.NET]] in the back-end.{{cn|date=January 2015}}

Master Data Services also includes certain features not available in the Stratature +EDM product. It gains a [[Web service]] interface to expose the data, as well as an [[API]], which internally uses the exposed web services, exposing the feature set, programmatically, to access and manipulate the data. It also integrates with [[Active Directory]] for authentication purposes. Unlike +EDM, Master Data Services supports [[Unicode]] characters, as well as support multilingual user interfaces.{{cn|date=January 2015}}

There has been a significant [http://www.faceofit.com/why-is-sql-server-2016-is-faster-than-ever performance increase] in Master Data Services in SQL Server 2016 as well as the Excel Add-In.&lt;ref&gt;http://www.faceofit.com/why-is-sql-server-2016-is-faster-than-ever&lt;/ref&gt;

== Terminology ==

* ''Model'' is the highest level of an MDS instance. It is the primary container for specific groupings of master data. In many ways it is very similar to the idea of a database. 
* ''Entities'' are containers created within a model. Entities provide a home for members, and are in many ways analogous to database tables. (e.g. Customer)
* ''Members'' are analogous to the records in a database table (Entity) e.g. Will Smith. Members are contained within entities. Each member is made up of two or more attributes. 
* ''Attributes'' are analogous to the columns within a database table (Entity) e.g. Surname. Attributes exist within entities and help describe members (the records within the table). Name and Code attributes are created by default for each entity and serve to describe and uniquely identify leaf members. Attributes can be related to other attributes from other entities which are called 'domain-based' attributes. This is similar to the concept of a foreign key.
Other attributes however, will be of type 'free-form' (most common) or 'file'.
* ''Attribute Groups'' are explicitly defined collections of particular attributes. Say you have an entity "customer" that has 50 attributes &amp;mdash; too much information for many of your users. Attribute groups enable the creation of custom sets of hand-picked attributes that are relevant for specific audiences. (e.g. "customer - delivery details" that would include just their name and last known delivery address). This is very similar to a database view.
*  ''Hierarchies'' organize members into either Derived or Explicit hierarchical structures. Derived hierarchies, as the name suggests, are derived by the MDS engine based on the relationships that exist between attributes. Explicit hierarchies are created by hand using both leaf and consolidated members.
*  ''Business Rules'' can be created and applied against model data to ensure that custom business logic is adhered to. In order to be committed into the system data must pass all business rule validations applied to them. e.g. Within the Customer Entity you may want to create a business rule that ensures all members of the 'Country' Attribute contain either the text "USA" or "Canada". The Business Rule once created and ran will then verify all the data is correct before it accepts it into the approved model.
*  ''Versions'' provide system owners / administrators with the ability to Open, Lock or Commit a particular version of a model and the data contained within it at a particular point in time. As the content within a model varies, grows or shrinks over time versions provide a way of managing metadata so that subscribing systems can access to the correct content.

== References ==
{{Reflist}}

==External links==
*[https://msdn.microsoft.com/en-us/library/ee633763.aspx Microsoft SQL Server 2016 Master Data Services]

[[Category:Data management]]
[[Category:Microsoft software|SQL Server Master Data Services]]
[[Category:2010 software]]</text>
      <sha1>a21wc7c4dtjsyc9bgq9kt6dimnd3htv</sha1>
    </revision>
  </page>
  <page>
    <title>Distributed concurrency control</title>
    <ns>0</ns>
    <id>13329119</id>
    <revision>
      <id>737010418</id>
      <parentid>737010416</parentid>
      <timestamp>2016-08-31T06:30:15Z</timestamp>
      <contributor>
        <username>ClueBot NG</username>
        <id>13286072</id>
      </contributor>
      <minor />
      <comment>Reverting possible vandalism by [[Special:Contribs/114.143.229.206|114.143.229.206]] to older version. [[WP:CBFP|Report False Positive?]] Thanks, [[WP:CBNG|ClueBot NG]]. (2745209) (Bot)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4829" xml:space="preserve">{{POV|Commitment ordering|date=November 2011}}
'''Distributed concurrency control''' is the [[concurrency control]] of a system [[Distributed computing|distributed]] over a [[computer network]] ([[#Bern87|Bernstein et al. 1987]], [[#Weikum01|Weikum and Vossen 2001]]). 

In ''[[database systems]]'' and ''[[transaction processing]]'' (''transaction management'') distributed concurrency control refers primarily to the concurrency control of a [[distributed database]]. It also refers to the concurrency control in a multidatabase (and other multi-[[transactional object]]) environment (e.g., [[federated database]], [[grid computing]], and [[cloud computing]] environments. A major goal for distributed concurrency control is distributed [[serializability]] (or [[global serializability]] for multidatabase systems). Distributed concurrency control poses special challenges beyond centralized one, primarily due to communication and computer [[latency (engineering)|latency]]. It often requires special techniques, like [[distributed lock manager]] over fast [[computer network]]s with low latency, like [[switched fabric]] (e.g., [[InfiniBand]]). [[commitment ordering]] (or commit ordering) is a general serializability technique that achieves distributed serializability (and global serializability in particular) effectively on a large scale, without concurrency control information distribution (e.g., local precedence relations, locks, timestamps, or tickets), and thus without performance penalties that are typical to other serializability techniques ([[#Raz92|Raz 1992]]).

The most common distributed concurrency control technique is ''strong strict two-phase locking'' ([[two phase locking#Strong strict two-phase locking|SS2PL]], also named ''rigorousness''), which is also a common centralized concurrency control technique. SS2PL provides both the ''serializability'', ''[[schedule (computer science)#Strict|strictness]]'', and ''commitment ordering'' properties. Strictness, a special case of recoverability, is utilized for effective recovery from failure, and commitment ordering allows participating in a general solution for global serializability. For large-scale distribution and complex transactions, distributed locking's typical heavy performance penalty (due to delays, latency) can be saved by using the [[atomic commitment]] protocol, which is needed in a distributed database for (distributed) transactions' [[Atomicity (database systems)|atomicity]] (e.g., [[Two-phase commit protocol|two-phase commit]], or a simpler one in a reliable system), together with some local commitment ordering variant (e.g., local [[two phase locking#Strong strict two-phase locking|SS2PL]]) instead of distributed locking, to achieve global serializability in the entire system. All the commitment ordering theoretical results are applicable whenever atomic commitment is utilized over partitioned, distributed recoverable (transactional) data, including automatic ''distributed deadlock'' resolution. Such technique can be utilized also for a large-scale [[parallel database]], where a single large database, residing on many nodes and using a distributed lock manager, is replaced with a (homogeneous) multidatabase, comprising many relatively small databases (loosely defined; any process that supports transactions over partitioned data and participates in atomic commitment complies), fitting each into a single node, and using commitment ordering (e.g., SS2PL, strict CO) together with some appropriate atomic commitment protocol (without using a distributed lock manager).

==See also==
*[[Global concurrency control]]

==References==
*&lt;cite id=Bern87&gt;[[Phil Bernstein|Philip A. Bernstein]], Vassos Hadzilacos, Nathan Goodman (1987): [http://research.microsoft.com/en-us/people/philbe/ccontrol.aspx  ''Concurrency Control and Recovery in Database Systems''], Addison Wesley Publishing Company, 1987, ISBN 0-201-10715-5 &lt;/cite&gt;
*&lt;cite id=Weikum01&gt;[[Gerhard Weikum]], Gottfried Vossen (2001): [http://www.elsevier.com/wps/find/bookdescription.cws_home/677937/description#description  ''Transactional Information Systems''], Elsevier, ISBN 1-55860-508-8 &lt;/cite&gt;
*&lt;cite id=Raz92&gt;[[Yoav Raz]] (1992): [http://www.informatik.uni-trier.de/~ley/db/conf/vldb/Raz92.html  "The Principle of Commitment Ordering, or Guaranteeing Serializability in a Heterogeneous Environment of Multiple Autonomous Resource Managers Using Atomic Commitment."]  ''Proceedings of the Eighteenth International Conference on Very Large Data Bases'' (VLDB), pp. 292-312, Vancouver, Canada, August 1992. (also DEC-TR 841, [[Digital Equipment Corporation]], November 1990) &lt;/cite&gt;

[[Category:Data management]]
[[Category:Distributed computing problems]]
[[Category:Databases]]
[[Category:Concurrency control]]
[[Category:Transaction processing]]</text>
      <sha1>1pdwwmspfghsdal7bx94kflpubh9x1p</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Data modeling</title>
    <ns>14</ns>
    <id>1116481</id>
    <revision>
      <id>761378463</id>
      <parentid>728513127</parentid>
      <timestamp>2017-01-22T17:37:49Z</timestamp>
      <contributor>
        <username>JustBerry</username>
        <id>19075131</id>
      </contributor>
      <minor />
      <comment>/* top */Cleaning up... using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="417" xml:space="preserve">{{Commons cat|Data modeling}}
In information system design, '''[[data modeling]]''' is the analysis and design of the information in the system, concentrating on the logical entities and the logical dependencies between these entities

{{catdiffuse}}

&lt;!--  --&gt;

[[Category:Computer-aided software engineering tools]]
[[Category:Data management|Modeling]]
[[Category:Scientific modeling]]
[[Category:Software design]]</text>
      <sha1>3xkpenlemii03c7usxmw2pg9zq68uht</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Data synchronization</title>
    <ns>14</ns>
    <id>7645825</id>
    <revision>
      <id>547448093</id>
      <parentid>498449680</parentid>
      <timestamp>2013-03-28T14:19:54Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor />
      <comment>[[User:Addbot|Bot:]] Migrating 2 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q8363890]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="188" xml:space="preserve">[[Category:Synchronization]]
[[Category:Data management|Synchronization]]
[[Category:Distributed computing problems]]
[[Category:Fault-tolerant computer systems]]
[[Category:Data quality]]</text>
      <sha1>0o9p199s3p26nv88de1wvpq4e4hb25e</sha1>
    </revision>
  </page>
  <page>
    <title>Core data integration</title>
    <ns>0</ns>
    <id>14124151</id>
    <revision>
      <id>607596684</id>
      <parentid>607596617</parentid>
      <timestamp>2014-05-08T07:49:17Z</timestamp>
      <contributor>
        <username>Doozler</username>
        <id>20649805</id>
      </contributor>
      <minor />
      <comment>finished</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1440" xml:space="preserve">{{Citations missing|date=December 2007}}
'''Core data integration''' is the use of [[data integration]] technology for a significant, centrally planned and managed IT initiative within a company. Examples of core data integration initiatives could include:

* ETL ([[Extract, transform, load]]) implementations
* EAI ([[Enterprise Application Integration]]) implementations
* SOA ([[Service-Oriented Architecture]]) implementations
* ESB ([[Enterprise Service Bus]]) implementations

Core data integrations are often designed to be enterprise-wide integration solutions. They may be designed to provide a data abstraction layer, which in turn will be used by individual core data integration implementations, such as ETL servers or applications integrated through EAI.

Because it is difficult to promptly roll out a centrally managed data integration solution that anticipates and meets all data integration requirements across an organization, IT engineers and even business users create [[edge data integration]], using technology that may be incompatible with that used at the core. In contrast to a core data integration, an edge data integration is not centrally planned and is generally completed with a smaller budget and a tighter deadline. 

==See also==
* [[data integration]]
* [[edge data integration]]

== References ==
* http://searchsoa.techtarget.com/tip/0,289483,sid26_gci1171085,00.html* 
* 

[[Category:Data management]]</text>
      <sha1>r07klvtdlkqsp5wmx035bsg1rczcwbg</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Computer-aided software engineering tools</title>
    <ns>14</ns>
    <id>15189720</id>
    <revision>
      <id>547616000</id>
      <parentid>391541149</parentid>
      <timestamp>2013-03-29T13:32:59Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor />
      <comment>[[User:Addbot|Bot:]] Migrating 2 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q8407536]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="118" xml:space="preserve">{{Cat main|Computer-aided software engineering}}

[[Category:Data management]]
[[Category:Computer programming tools]]</text>
      <sha1>g9quzl4o7qq2ksqp2h5xn5hsrztp0dw</sha1>
    </revision>
  </page>
  <page>
    <title>Paper data storage</title>
    <ns>0</ns>
    <id>13756939</id>
    <revision>
      <id>752737302</id>
      <parentid>752736972</parentid>
      <timestamp>2016-12-03T00:06:27Z</timestamp>
      <contributor>
        <username>Riceissa</username>
        <id>20698937</id>
      </contributor>
      <comment>/* Limits */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5247" xml:space="preserve">{{refimprove|date=August 2012}}
'''Paper data storage''' refers to the use of [[paper]] as a [[data storage device]]. This includes [[writing]], [[illustrating]], and the use of data that can be interpreted by a machine or is the result of the functioning of a machine.  A defining feature of paper data storage is the ability of humans to produce it with only simple tools and interpret it visually.

Though this is now mostly obsolete, paper was once also an important form of [[computer data storage]].

==History==
Before paper was used for storing data, it had been used in several applications for storing instructions to specify a machine's operation.  The earliest use of paper to store instructions for a machine was the work of [[Basile Bouchon]] who, in 1725, used punched paper rolls to control textile looms.  This technology was later developed into the wildly successful [[Jacquard loom]].  The 19th century saw several other uses of paper for controlling machines.  In 1846, telegrams could be prerecorded on [[punched tape]] and rapidly transmitted using [[Alexander Bain (inventor)|Alexander Bain]]'s automatic telegraph.  Several inventors took the concept of a mechanical organ and used paper to represent the music.  

In the late 1880s [[Herman Hollerith]] invented the recording of data on a medium that could then be read by a machine.  Prior uses of machine readable media, above, had been for control ([[automaton]]s, [[piano roll]]s, [[Jacquard loom|looms]], ...), not data.  "After some initial trials with paper tape, he settled on [[punched card]]s..."&lt;ref&gt;[http://www.columbia.edu/acis/history/hollerith.html Columbia University Computing History - Herman Hollerith]&lt;/ref&gt;  Hollerith's method was used in the 1890 census.&lt;!-- The Census Bureau is not "an independent 3rd party" source - as required by Wikipedia - for Census Bureau performance claims. FOLLOWING CLAIM DELETED. --- and the completed results were "... finished months ahead of schedule and far under budget".&lt;ref&gt;[http://www.census.gov/history/www/technology/010873.html U.S. Census Bureau: Tabulation and Processing]&lt;/ref&gt;--&gt;  Hollerith's company eventually became the core of [[International Business Machines|IBM]]. 

Other technologies were also developed that allowed  machines to work with marks on paper instead of punched holes.  This technology was widely used for [[optical scan voting system|tabulating votes]] and grading [[scantron|standardized tests]].  [[Barcode]]s made it possible for any object that was to be sold or transported to have some computer readable information securely attached to it. Banks used magnetic ink on checks, supporting MICR scanning.

In an early electronic computing device, the [[Atanasoff-Berry Computer]], electric sparks were used to singe small holes in paper cards to represent binary data.  The altered [[dielectric constant]] of the paper at the location of the holes could then be used to read the binary data back into the machine by means of electric sparks of lower voltage than the sparks used to create the holes.  This form of paper data storage was never made reliable and was not used in any subsequent machine.

As of 2014, [[Universal Product Code]] barcodes, first used in 1974, are ubiquitous.

Some people recommend a width of at least 3 pixels for each minimum-width gap and each minimum-width bar for 1D barcodes;
and a width of at least 4 pixels—e.g., a 4&amp;nbsp;×&amp;nbsp;4 pixel = 16 pixel module for [[2D barcode]]s.&lt;ref&gt;
Accusoft.
[http://www.accusoft.com/whitepapers/barcodes/BarcodesinDocuments-BestPractices.pdf "Using Barcodes in Documents – Best Practices"].
2007.
Retrieved 2014-04-25.
&lt;/ref&gt;
For a typical black-and-white barcode scanned by a typical 300 dpi [[image scanner]],
and assuming roughly half the space is occupied by finder patterns, fiducial alignment patterns, and error detection and correction codes, that recommendation gives a maximum data density of roughly 50 bits per linear inch (about 2 bit/mm) for 1D barcodes, and roughly 2 800 bits per square inch (about 4.4 bit/mm&lt;sup&gt;2&lt;/sup&gt;).

==Limits==
The limits of data storage depend on the technology to write and read such data.  For example, an 8″&amp;nbsp;×&amp;nbsp;10″ (roughly A4 without margins) 300dpi 8-bit greyscale image map contains 7.2 megabytes of data—assuming a scanner can accurately reproduce the printed image to that resolution and [[color depth]], and a program can accurately interpret such an image.  A similarly sized image in 2400dpi 24-bit true color theoretically contains 1.38 gigabytes of information.

==See also==
{{div col|3}}
*[[Banknote]] read by a [[vending machine]]
*[[Book music]]
*[[Edge-notched card]]
*[[Index card]]
*[[Kimball tag]]
*[[Machine-readable medium]]
*[[Magnetic ink character recognition]]
*[[Mark sense]]
*[[Music roll]]
*[[Optical mark recognition]]
*[[Paper disc]]
*[[Perfin]]
*[[Perforation]]
*[[Punched tape]]
*[[Spindle (stationery)]]
*[[Stenotype]]
*[[Ticker tape]]
{{div col end}}

==References==
{{reflist}}

==External links==
* [http://www.microglyphs.com/english/html/dataglyphs.shtml DataGlyphs]
* [http://ollydbg.de/Paperbak/ PaperBack data storage]
{{Paper data storage media}}

[[Category:Data management]]
[[Category:Storage media]]</text>
      <sha1>add7p6ytvn1seizpp643nggtywawq39</sha1>
    </revision>
  </page>
  <page>
    <title>Data proliferation</title>
    <ns>0</ns>
    <id>13651081</id>
    <revision>
      <id>753459300</id>
      <parentid>738669492</parentid>
      <timestamp>2016-12-07T08:07:51Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 1 sources and tagging 0 as dead. #IABot (v1.2.7.1)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6361" xml:space="preserve">'''Data proliferation''' refers to the prodigious amount of [[data]], [[structured data|structured]] and unstructured, that businesses and governments continue to generate at an unprecedented rate and the [[usability]] problems that result from attempting to store and manage that data. While originally pertaining to problems associated with paper [[documentation]], data proliferation has become a major problem in primary and secondary [[data storage device|data storage]] on computers.

While digital storage has become cheaper, the associated costs, from raw power to maintenance and from metadata to search engines, have not kept up with the proliferation of data. Although the power required to maintain a unit of data has fallen, the cost of facilities which house the digital storage has tended to rise.&lt;ref&gt;{{cite web |url =http://www.deloitte.co.uk/TMTPredictions/technology/Downsizing-digital-attic-data-storage.cfm
|title=Downsizing the digital attic |work=Deloitte Technology Predictions |archiveurl=https://web.archive.org/web/20110722194032/http://www.deloitte.co.uk/TMTPredictions/technology/Downsizing-digital-attic-data-storage.cfm |archivedate=July 22, 2011}}&lt;/ref&gt;

{{rquote|right| At the simplest level, company [[e-mail]] systems spawn large amounts of data. Business e-mail – some of it important to the enterprise, some much less so – is estimated to be growing at a rate of 25-30% annually. And whether it’s relevant or not, the load on the system is being magnified by practices such as multiple addressing and the attaching of large text, audio and even [[video file formats|video file]]s.|IBM Global Technology Services&lt;ref name=IBM&gt;[https://web.archive.org/web/20090206010415/http://www-03.ibm.com/systems/resources/systems_storage_solutions_pdf_toxic_tb.pdf “The Toxic [[Terabyte]]”, IBM Global Technology Services, July 2006]&lt;/ref&gt;}}

Data proliferation has been documented as a problem for the [[U.S. military]] since August 1971, in particular regarding the excessive documentation submitted during the acquisition of major weapon systems.&lt;ref name=DODPP&gt;[http://stinet.dtic.mil/oai/oai?&amp;verb=getRecord&amp;metadataPrefix=html&amp;identifier=AD0892652 Evolution of the Data Proliferation Problem within Major Air Force Acquisition Programs.]&lt;/ref&gt; Efforts to mitigate data proliferation and the problems associated with it are ongoing.&lt;ref&gt;[http://www.thic.org/pdf/Jun02/dod.rroderique.020612.pdf Data Proliferation: Stop That]&lt;/ref&gt;

==Problems caused==
The problem of data proliferation is affecting all areas of commerce as the result of the availability of relatively inexpensive data storage devices. This has made it very easy to dump data into secondary storage immediately after its window of usability has passed. This masks problems that could gravely affect the profitability of businesses and the efficient functioning of health services, police and security forces, local and national governments, and many other types of organizations.&lt;ref name=IBM /&gt; Data proliferation is problematic for several reasons:
*Difficulty when trying to find and retrieve information. At [[Xerox]], on average it takes employees more than one hour per week to [[document retrieval|find]] hard-copy documents, costing $2,152 a year to manage and store them. For businesses with more than 10 employees, this increases to almost two hours per week at $5,760 per year.&lt;ref&gt;[http://www.itbusiness.ca/it/client/en/home/News.asp?id=40615&amp;cid=13 “Dealing with data proliferation”; Vawn Himmelsbach. it business.ca: Canadian Technology News, September 19, 2006]&lt;/ref&gt; In large [[storage network|networks]] of primary and secondary data storage, problems finding electronic data are analogous to problems finding hard copy data.
*[[Data loss]] and legal liability when data is disorganized, not properly replicated, or cannot be found in a timely manner. In April 2005, the [[TD Ameritrade|Ameritrade Holding Corporation]] told 200,000 current and past customers that a [[Magnetic tape data storage|tape]] containing confidential information had been lost or destroyed in transit. In May of the same year, [[Time Warner Incorporated]] reported that 40 tapes containing personal data on 600,000 current and former employees had been lost en route to a storage facility. In March 2005, a Florida judge hearing a $2.7 billion lawsuit against Morgan Stanley issued an "[[adverse inference]] order" against the company for "willful and gross abuse of its discovery obligations." The judge cited Morgan Stanley for repeatedly finding misplaced tapes of e-mail messages long after the company had claimed that it had turned over all such tapes to the court.&lt;ref&gt;[http://www.computerworld.com/printthis/2005/0,4814,103541,00.html “Data: Lost, Stolen or Strayed”, Computer World, Security]&lt;/ref&gt;
*Increased manpower requirements to manage increasingly chaotic data storage resources.
*Slower networks and application performance due to excess traffic as users search and search again for the material they need.&lt;ref name=IBM /&gt;
*High cost in terms of the energy resources required to operate storage hardware. A 100 terabyte system will cost up to $35,040 a year to run—not counting cooling costs.&lt;ref&gt;[http://findarticles.com/p/articles/mi_m0BRZ/is_10_23/ai_111062988 "Power and storage: the hidden cost of ownership”, Computer Technology Review, October 2003]&lt;/ref&gt;

==Proposed solutions==
*Applications that better utilize modern technology
*Reductions in duplicate data (especially as caused by data movement)
*Improvement of [[metadata]] structures
*Improvement of file and storage transfer structures
*User education and discipline&lt;ref name=DODPP /&gt;
*The implementation of [[Information Lifecycle Management]] solutions to eliminate low-value information as early as possible before putting the rest into actively managed long-term storage in which it can be quickly and cheaply accessed.&lt;ref name=IBM /&gt;

==See also==
*[[Backup]]
* [[Digital Asset Management]]
*[[Disk storage]]
*[[Document management system]]
*[[Hierarchical storage management]]
*[[Information Lifecycle Management]]
*[[Information repository]]
*[[Magnetic tape data storage]]
*[[Retention period|Retention schedule]]

==References==
{{reflist}}

[[Category:Information technology management]]
[[Category:Content management systems]]
[[Category:Data management]]</text>
      <sha1>qdcq6awrz8huuimiplm5beyd3igxri9</sha1>
    </revision>
  </page>
  <page>
    <title>Vector-field consistency</title>
    <ns>0</ns>
    <id>18477184</id>
    <revision>
      <id>724046458</id>
      <parentid>623353624</parentid>
      <timestamp>2016-06-06T20:26:53Z</timestamp>
      <contributor>
        <username>Dcirovic</username>
        <id>11795905</id>
      </contributor>
      <minor />
      <comment>refs using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2897" xml:space="preserve">'''Vector-Field Consistency'''&lt;ref group="nb"&gt;&lt;sub&gt;Designation coined by L. Veiga.&lt;/sub&gt;&lt;/ref&gt; is a [[consistency model]] for replicated data (for example, objects), initially described in a paper&lt;ref&gt;{{cite conference |author1=Nuno Santos |author2=Luís Veiga |author3=Paulo Ferreira | year=2007 | title=Vector-Field Consistency for Adhoc Gaming| booktitle = ACM/IFIP/Usenix Middleware Conference 2007 | url=http://www.gsd.inesc-id.pt/~pjpf/middleware07vector.pdf | format=PDF}}&lt;/ref&gt; which was awarded the best-paper prize in the ACM/IFIP/Usenix Middleware Conference 2007. It has since been enhanced for increased scalability and fault-tolerance in a recent paper.&lt;ref&gt;{{cite conference |author1=Luís Veiga |author2=André Negrão |author3=Nuno Santos |author4=Paulo Ferreira | year=2010 | title=Unifying Divergence Bounding and Locality Awareness in Replicated Systems with Vector-Field Consistency 
| booktitle = JISA, Journal of Internet Services and Applications, Volume 1, Number 2, 95-115, Springer, 2010 | url=http://www.gsd.inesc-id.pt/~lveiga/vfc-JISA-2010.pdf | format=PDF}}&lt;/ref&gt;

== Description ==
This consistency model was initially designed for replicated [[data management]] in adhoc gaming in order to minimize bandwidth usage without sacrificing playability. Intuitively, it captures the notion that although players require, wish, and take advantage of information regarding the whole of the game world (as opposed to a restricted view to rooms, arenas, etc. of limited size employed in many [[multiplayer game]]s), they need to know information with greater freshness, frequency, and accuracy as other game entities are located closer and closer to the player's position.

It prescribes a multidimensional divergence bounding scheme, based on a [[vector field]] that employs consistency vectors k=(θ,σ,ν), standing for maximum allowed '''t'''ime - or replica staleness, '''s'''equence - or missing updates, and '''v'''alue&lt;ref group="nb"&gt;&lt;sub&gt;Since in the [[Greek alphabet]] there was no letter for the ''vee'' sound, the ''nu'' letter was preferred for its resemblance with the roman V, for ''v''alue, instead of β (''beta'') for the ''vee'' sound in contemporary Greek speaking.&lt;/sub&gt;&lt;/ref&gt; - or user-defined measured replica divergence, applied to all space coordinates in game scenario or world.

The consistency vector-fields emanate from field-generators designated as pivots (for example, players) and [[Field strength|field intensity]] attenuates as distance grows from these pivots in concentric or square-like regions. This consistency model unifies locality-awareness techniques employed in message routing and consistency enforcement for multiplayer games, with divergence bounding techniques traditionally employed in replicated database and web scenarios.

== Notes ==
&lt;references group="nb"/&gt;

== References ==
&lt;references/&gt;

[[Category:Data management]]</text>
      <sha1>ts2a7eg5n1qp06glx07kez2aws2a4za</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Document-oriented databases</title>
    <ns>14</ns>
    <id>19642057</id>
    <revision>
      <id>670394070</id>
      <parentid>641828402</parentid>
      <timestamp>2015-07-07T17:54:58Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>{{see also cat|Key-value databases}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="387" xml:space="preserve">A '''document-oriented database''' is a [[database management system]] designed for document-oriented applications
{{Cat main|Document-oriented database}}
{{see also|Document-oriented database#Implementations}}
{{see also cat|Full text databases}}
{{see also cat|Key-value databases}}

[[Category:Data management]]
[[Category:Database management systems]]
[[Category:Types of databases]]</text>
      <sha1>2yilxram2ry5s8ymwzdm4mto1wj1zkx</sha1>
    </revision>
  </page>
  <page>
    <title>Conference on Innovative Data Systems Research</title>
    <ns>0</ns>
    <id>21047573</id>
    <revision>
      <id>607088523</id>
      <parentid>580102254</parentid>
      <timestamp>2014-05-04T23:01:53Z</timestamp>
      <contributor>
        <username>BattyBot</username>
        <id>15996738</id>
      </contributor>
      <minor />
      <comment>changed {{Notability}} to {{Notability|Events}} &amp; [[WP:AWB/GF|general fixes]] using [[Project:AWB|AWB]] (10095)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1244" xml:space="preserve">{{multiple issues|
{{notability|Events|date=September 2011}}
{{primary sources|date=September 2011}}
}}

{{Infobox Academic Conference
 | history = 2002–
 | discipline = [[Database]]
 | abbreviation = CIDR
 | publisher = CIDR Conference
 | country= [[United States]]
 | frequency = biennial
}}
The '''Conference on Innovative Data Systems Research''' ('''CIDR''') is a biennial [[computer science]] conference focused on research into new techniques for [[data management]]. It was started in 2002 by [[Michael Stonebraker]], [[Jim Gray (computer scientist)|Jim Gray]], and [[David DeWitt]], and is held at the [[Asilomar Conference Grounds]] in [[Pacific Grove, California]].

CIDR focuses on presenting work that is more speculative, radical, or provocative than what is typically accepted by the traditional database research conferences (such as the [[International Conference on Very Large Data Bases]] (VLDB) and the [[ACM SIGMOD Conference]]).

==See also==
* [[International Conference on Very Large Data Bases]] (VLDB)
* [[ACM SIGMOD Conference]]

==External links==
* [http://www-db.cs.wisc.edu/cidr/ CIDR website]

[[Category:Data management]]
[[Category:Computer science conferences]]


{{database-stub}}
{{compu-conference-stub}}</text>
      <sha1>p45qi95t5l2xu1jl8o6gnsy9n14zd1o</sha1>
    </revision>
  </page>
  <page>
    <title>Metadata controller</title>
    <ns>0</ns>
    <id>21423528</id>
    <revision>
      <id>732166611</id>
      <parentid>623254040</parentid>
      <timestamp>2016-07-30T02:33:43Z</timestamp>
      <contributor>
        <username>Ushkin N</username>
        <id>28390915</id>
      </contributor>
      <comment>about SAN</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="585" xml:space="preserve">'''Metadata controller''' (or MDC) is a [[storage area network]] (SAN) technology for managing [[file locking]], space allocation and data access authorization.
This is needed when several clients are given block level access to the same disk volume, [[Computer data storage|data storage]] sharing.

The abstract for the patent describing this technology can be read [http://www.freepatentsonline.com/7448077.html here]

[[Category:Data management]]
[[Category:Telecommunications engineering]]
[[Category:Storage area networks]]
[[Category:Local area networks]]

{{compu-storage-stub}}</text>
      <sha1>mte5e3v0huy7ajvpu96kg4rhb2q6p3b</sha1>
    </revision>
  </page>
  <page>
    <title>Parchive</title>
    <ns>0</ns>
    <id>526495</id>
    <revision>
      <id>757771215</id>
      <parentid>749546109</parentid>
      <timestamp>2017-01-01T17:14:55Z</timestamp>
      <contributor>
        <ip>70.36.223.208</ip>
      </contributor>
      <comment>Multipar dev now considers multipar.eu malicious, unauthorized changes https://www.livebusinesschat.com/smf/index.php?topic=6108.0</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="18906" xml:space="preserve">{{Merge from|QuickPAR|date=March 2014}}

{{Infobox file format
| name = Parchive
| extension = .par, .par2, .p??, (.par3 future)
| mime =
| owner =
| creatorcode =
| genre = [[Erasure code]]
| containerfor =
| containedby =
| extendedfrom =
| extendedto =
}}

'''Parchive''' (a [[portmanteau]] of '''parity archive''', and formally known as '''Parity Volume Set Specification'''&lt;ref&gt;[https://www.livebusinesschat.com/smf/index.php?topic=5736.msg38234#msg38234 Re: Correction to Parchive on Wikipedia], reply #3, by Yutaka Sawada: "Their formal title are "Parity Volume Set Specification 1.0" and "Parity Volume Set Specification 2.0."&lt;/ref&gt;) is an [[erasure code]] system that produces '''par''' files for [[checksum]] verification of [[data integrity]], with the capability to perform [[data recovery]] operations that can repair or regenerate corrupted or missing data. 

Parchive was originally written to solve the problem of reliable file sharing on [[Usenet]],&lt;ref&gt;{{cite web
| url         = http://parchive.sourceforge.net/#desc
| title       = Parchive: Parity Archive Volume Set
| accessdate  = 2009-10-29
| quote       = The original idea behind this project was to provide a tool to apply the data-recovery capability concepts of RAID-like systems to the posting and recovery of multi-part archives on Usenet.
}}&lt;/ref&gt; but it is now commonly used for protecting any kind of data from [[data corruption]], [[disc rot]], [[data degradation|bit rot]], and accidental or malicious damage. Despite the name, Parchive uses more advanced techniques that do not utilize simplistic [[Parity bit|parity]] methods of [[error detection and correction]].

As of 2014, '''PAR1''' is obsolete, '''PAR2''' is mature for widespread use, and '''PAR3''' is an experimental version being developed by MultiPar author Yutaka Sawada.&lt;ref&gt;[http://www.livebusinesschat.com/smf/index.php?topic=5098.0 possibility of new PAR3 file]&lt;/ref&gt;&lt;ref&gt;[http://www.livebusinesschat.com/smf/index.php?topic=3339.0 Question about your usage of PAR3]&lt;/ref&gt;&lt;ref&gt;[http://www.livebusinesschat.com/smf/index.php?topic=5025.msg29912;topicseen#msg29912 Risk of undetectable intended modification]&lt;/ref&gt;&lt;ref&gt;[http://www.livebusinesschat.com/smf/index.php?topic=3527.msg8850;topicseen#msg8850 PAR3 specification proposal not finished as of April 2011]&lt;/ref&gt;  The original SourceForge Parchive project has been inactive since November 9, 2010.&lt;ref&gt;{{cite web |url = http://sourceforge.net/projects/parchive/ |title = Parchive: Parity Archive Tool |accessdate = 2012-09-02}}&lt;/ref&gt; 

== History ==
Parchive was intended to increase the reliability of transferring files via Usenet [[newsgroup]]s. Usenet was originally designed for informal conversations, and the underlying protocol, [[NNTP]] was not designed to transmit arbitrary binary data. Another limitation, which was acceptable for conversations but not for files, was that messages were normally fairly short in length and limited to 7-bit [[ASCII]] text.&lt;ref&gt;{{cite IETF
| title       = Network News Transfer Protocol
| rfc         = 977
| sectionname = Character Codes
| section     = 2.2
| page        = 5
| last1       = Kantor
| first1      = Brian
| authorlink1 =
| last2       = Lapsley
| first2      = Phil
| authorlink2 = Phil Lapsley
| year        = 1986
| month       = February
| publisher   = [[Internet Engineering Task Force|IETF]]
| accessdate  = 2009-10-29
}}&lt;/ref&gt;

Various techniques were devised to send files over Usenet, such as [[uuencode|uuencoding]] and [[Base64]]. Later Usenet software allowed  8 bit [[Extended ASCII]], which permitted new techniques like [[yEnc]]. Large files were broken up to reduce the effect of a corrupted download, but the unreliable nature of Usenet remained.

With the introduction of Parchive, parity files could be created that were then uploaded along with the original data files. If any of the data files were damaged or lost while being propagated between Usenet servers, users could download parity files and use them to reconstruct the damaged or missing files. Parchive included the construction of small index files (*.par in version 1 and *.par2 in version 2) that do not contain any recovery data. These indexes contain [[hash function|file hash]]es that can be used to quickly identify the target files and verify their integrity.

Because the index files were so small, they minimized the amount of extra data that had to be downloaded from Usenet to verify that the data files were all present and undamaged, or to determine how many parity volumes were required to repair any damage or reconstruct any missing files. They were most useful in version 1 where the parity volumes were much larger than the short index files. These larger parity volumes contain the actual recovery data along with a duplicate copy of the information in the index files (which allows them to be used on their own to verify the integrity of the data files if there is no small index file available).

In July 2001, Tobias Rieper and Stefan Wehlus proposed the Parity Volume Set specification, and with the assistance of other project members, version 1.0 of the specification was published in October 2001.&lt;ref&gt;{{cite web|url=http://sourceforge.net/docman/display_doc.php?docid=7273&amp;group_id=30568 |title=Parchive: Parity Volume Set specification 1.0 |accessdate=2009-04-07 |last=Nahas |first=Michael |date=2001-10-14 |deadurl=yes |archiveurl=https://web.archive.org/web/20081220184024/http://sourceforge.net/docman/display_doc.php?docid=7273&amp;group_id=30568 |archivedate=December 20, 2008 }}&lt;/ref&gt; Par1 used [[Reed–Solomon error correction]] to create new recovery files. Any of the recovery files can be used to rebuild a missing file from an incomplete [[download]].

Version 1 became widely used on Usenet, but it did suffer some limitations:
* It was restricted to handle at most 255 files.
* The recovery files had to be the size of the largest input file, so it did not work well when the input files were of various sizes. (This limited its usefulness when not paired with the proprietary RAR compression tool.)
* The recovery algorithm had a bug, due to a flaw&lt;ref&gt;{{cite web
| url         = http://web.eecs.utk.edu/~plank/plank/papers/CS-03-504.html
| title       = Note: Correction to the 1997 Tutorial on Reed-Solomon Coding
| accessdate  = 2009-10-29
| last        = Plank
| first       = James S.
|author2=Ding, Ying
|date=April 2003
}}&lt;/ref&gt; in the academic paper&lt;ref&gt;{{cite web
| url         = http://web.eecs.utk.edu/~plank/plank/papers/SPE-9-97.html
| title       = A Tutorial on Reed-Solomon Coding for Fault-Tolerance in RAID-like Systems
| accessdate  = 2009-10-29
| last        = Plank
| first       = James S.
|date=September 1997
}}&lt;/ref&gt; on which it was based.
* It was strongly tied to Usenet and it was felt that a more general tool might have a wider audience.

In January 2002, Howard Fukada proposed that a new Par2 specification should be devised with the significant changes that data verification and repair should work on blocks of data rather than whole files, and that the algorithm should switch to using 16 bit numbers rather than the 8 bit numbers that PAR 1 used. Michael Nahas and Peter Clements took up these ideas in July 2002, with additional input from Paul Nettle and Ryan Gallagher (who both wrote Par1 clients). Version 2.0 of the Parchive specification was published by Michael Nahas in September 2002.&lt;ref&gt;{{cite web
| url         = http://parchive.sourceforge.net/docs/specifications/parity-volume-spec/article-spec.html
| title       = Parity Volume Set Specification 2.0
| accessdate  = 2009-10-29
| last        = Nahas
| first       = Michael |author2=Clements, Peter |author3=Nettle, Paul |author4=Gallagher, Ryan
| date        = 2003-05-11
}}&lt;/ref&gt;

Peter Clements then went on to write the first two Par2 implementations, [[QuickPar]] and par2cmdline. Abandoned since 2004, Paul Houle created phpar2 to supersede par2cmdline. Yutaka Sawada created MultiPar to supersede QuickPar. Sawada maintains par2cmdline to use as MultiPar's PAR engine backend.

On May 10, 2014, Sawada reported a hash collision security problem in par2cmdline (the backend for MultiPar):&lt;ref name="livebusinesschat.com"&gt;[https://www.livebusinesschat.com/smf/index.php?topic=5579.0 v1.2.5.3 is public]&lt;/ref&gt;

&lt;blockquote&gt;I'm not sure this problem can be used for DoS attack against automated Par2 usage. If someone has a skill to forge CRC-32, it is possible to make a set of source file and Par2 file, which freeze a Par2 client for several hours.&lt;/blockquote&gt;

== Versions ==
Versions 1 and 2 of the [[file format]] are incompatible. (However, many clients support both.)

=== Parity Volume Set Specification 1.0 ===
For Par1, the files ''f1'', ''f2'', ..., ''fn'', the Parchive consists of an index file (''f.par''), which is CRC type file with no recovery blocks, and a number of "parity volumes" (''f.p01'', ''f.p02'', etc.). Given all of the original files except for one (for example, ''f2''), it is possible to create the missing ''f2'' given all of the other original files and any one of the parity volumes. Alternatively, it is possible to recreate two missing files from any two of the parity volumes and so forth.&lt;ref&gt;{{cite book
| last        = Wang
| first       = Wallace
| authorlink  =
| title       = Steal this File Sharing Book
| url         = https://books.google.com/books?id=FGfMS5kymmcC&amp;pg=PT183
| accessdate  = 2009-09-24
| edition     = 1st
| date        = 2004-10-25
| publisher   = [[No Starch Press]]
| location    = [[San Francisco, California]]
| isbn        = 1-59327-050-X
| pages       = 164 – 167
| chapter     = Finding movies (or TV shows): Recovering missing RAR files with PAR and PAR2 files
}}&lt;/ref&gt;

Par1 supports up to 256 recovery files. Each recovery file must be the size of the largest input file.

=== Parity Volume Set Specification 2.0 ===
Par2 files generally use this naming/extension system: ''filename.vol000+01.PAR2'', ''filename.vol001+02.PAR2'', ''filename.vol003+04.PAR2'', ''filename.vol007+06.PAR2'', etc. The +01, +02, etc. in the filename indicates how many blocks it contains, and the vol000, vol001, vol003 etc. indicates the number of the first recovery block within the PAR2 file. If an index file of a download states that 4 blocks are missing, the easiest way to repair the files would be by downloading ''filename.vol003+04.PAR2''. However, due to the redundancy, ''filename.vol007+06.PAR2'' is also acceptable. There is also an index file ''filename.PAR2'', it is identical in function to the small index file used in PAR1.

Par2 supports up to 65536 (2&lt;sup&gt;16&lt;/sup&gt;) recovery blocks (however, par2cmdline, the official PAR2 implementation, it limited to 32767 blocks at once). Input files are split into multiple equal-sized blocks so that recovery files do not need to be the size of the largest input file.

Although [[Unicode]] is mentioned in the PAR2 specification as an option, most PAR2 implementations do not support unicode.&lt;ref&gt;[http://www.quickpar.co.uk/forum/viewtopic.php?id=1065 QuickPar forum posting] {{webarchive |url=https://web.archive.org/web/20120302104523/http://www.quickpar.co.uk/forum/viewtopic.php?id=1065 |date=March 2, 2012 }}&lt;/ref&gt;

Directory support is included in the PAR2 specification, but most or all implementations do not support it.

=== Parity Volume Set Specification 3.0 ===
Par3 is a planned improvement over Par2.&lt;ref&gt;{{cite web|url=http://hp.vector.co.jp/authors/VA021385/|title=MultiPar announcement|publisher=}}&lt;/ref&gt;&lt;ref&gt;[http://www.quickpar.org.uk/forum/viewtopic.php?id=1264 QuickPar forum posting&amp;nbsp;– status PAR3] {{webarchive |url=https://web.archive.org/web/20101127125317/http://www.quickpar.org.uk/forum/viewtopic.php?id=1264 |date=November 27, 2010 }}&lt;/ref&gt;&lt;ref&gt;[http://www.quickpar.co.uk/forum/viewtopic.php?id=1047 QuickPar forum posting&amp;nbsp;– PAR3 specifications] {{webarchive |url=https://web.archive.org/web/20120316104813/http://www.quickpar.co.uk/forum/viewtopic.php?id=1047 |date=March 16, 2012 }}&lt;/ref&gt;&lt;ref&gt;[http://hp.vector.co.jp/authors/VA021385/par3_spec_prop.htm PAR3 proposal] {{webarchive |url=https://web.archive.org/web/20100911002706/http://hp.vector.co.jp/authors/VA021385/par3_spec_prop.htm |date=September 11, 2010 }}&lt;/ref&gt; The authors intend to fix problems related to creating or repairing when the block count or block size is very high. Par3 also adds support for including directories (file folders) in a parchive and Unicode characters in file names. In addition, the authors plan to enable the Par3 algorithm to identify files that have been moved or renamed.&lt;ref&gt;http://www.livebusinesschat.com/smf/index.php?topic=4751.0 PAR3 move/rename brainstorming&lt;/ref&gt;

== Software ==

=== Windows ===
* MultiPar (freeware) &amp;nbsp;— Builds upon QuickPar's features and [[GUI]], and Yutaka Sawada's fork of par2cmdline as the PAR2 backend.&lt;ref name="livebusinesschat.com"/&gt; It has support for Par3, [[multithreading (software)|multithreading]], [[Symmetric multiprocessor system|multiple processors]], and the ability to recurse subfolders. MultiPar is able to add recovery data to [[Zip (file format)|ZIP]] and [[7-Zip]]&lt;ref&gt;{{cite web|url=https://sourceforge.net/p/sevenzip/feature-requests/1006/|title=7-Zip|publisher=}}&lt;/ref&gt; files, with a few minor caveats.&lt;ref&gt;[http://www.livebusinesschat.com/smf/index.php?topic=4922.0 How to add recovery record to ZIP or 7-Zip archive]&lt;/ref&gt; MultiPar is also verified to work with [[Wine (software)|Wine]] under [[TrueOS]], and may work with other operating systems too.&lt;ref&gt;[http://www.livebusinesschat.com/smf/index.php?topic=4902.0 MultiPar works with PCBSD 9.0]&lt;/ref&gt; Although the Par2 and Par3 components are (or will be) open source, the MultiPar GUI on top of them is currently not open source.&lt;ref&gt;[https://www.livebusinesschat.com/smf/index.php?topic=5402.0 contacted you, asking about sourcecode]&lt;/ref&gt;  Download from [https://www.livebusinesschat.com/smf/index.php?board=396.0 MultiPar forum]. 
* [[QuickPar]] (freeware)&amp;nbsp;— unmaintained since 2004, superseded by MultiPar.
* [https://web.archive.org/web/20110311041855/http://chuchusoft.com/par2_tbb/ par2+tbb] ([[GNU General Public License|GPLv2]])&amp;nbsp;— a concurrent (multithreaded) version of par2cmdline 0.4 using [[Threading Building Blocks|TBB]].
* Par-N-Rar ([[GNU General Public License|GPL]])
* [http://paulhoule.com/phpar2/index.php phpar2] &amp;nbsp;— advanced par2cmdline with multithreading and highly optimized assemblercode (about 66% faster than QuickPar 0.9.1)
* Rarslave ([[GNU General Public License|GPLv2]])
* [[SmartPAR]] (freeware) &amp;nbsp;— Unmaintained since 2002 and obsolete as this application written for Microsoft Windows only works with the original Par1 (PAR) Parchive format parity files. Superseded by QuickPar. It uses Reed–Solomon error correction to create new recovery files. SmartPAR is able to correct errors and recover missing parts of distributed files from PAR files.&lt;ref&gt;{{cite book
| last        = Wang
| first       = Wallace
| authorlink  = 
| title       = Steal this File Sharing Book
| url         = https://books.google.com/books?id=FGfMS5kymmcC&amp;pg=PT183
| accessdate  = 2009-09-24
| edition     = 1st
| date        = 2004-10-25
| publisher   = [[No Starch Press]]
| location    = [[San Francisco, California]]
| isbn        = 1-59327-050-X
| pages       = 164 – 167 
| chapter     = Finding movies (or TV shows): Recovering missing RAR files with PAR and PAR2 files
}}&lt;/ref&gt; Last stable release 0.13d1 dated {{Start date and age|2002|01|22}}&lt;ref&gt;{{cite web |url=http://parchive.sourceforge.net/ |title=Parchive: Parity archive tool |accessdate=2009-09-26}}&lt;/ref&gt;
* [http://www.wehlus.de/mirror/index.html Mirror]&amp;nbsp;— First PAR implementation, unmaintained since 2001.
* [http://parchive.sourceforge.net/ Original par2cmdline]&amp;nbsp;— (obsolete).
* [https://github.com/Parchive/par2cmdline par2cmdline] by BlackIkeEagle.

=== Mac OS X ===
* [https://gp.home.xs4all.nl/Site/MacPAR_deLuxe.html MacPAR deLuxe 4.2]
* [http://www.unrarx.com/ UnRarX]
* [https://web.archive.org/web/20110311041855/http://chuchusoft.com/par2_tbb/ par2+tbb] is a concurrent (multithreaded) version of par2cmdline 0.4 using [[Threading Building Blocks|TBB]], [[GNU General Public License|GPLv2]], or later.

=== [[Linux]] ===
* The [https://github.com/Parchive/par2cmdline par2] utility, which is a maintained fork of par2cmdline. 
* [http://pypar2.silent-blade.org/index.php?n=Main.HomePage PyPar2 1.4], a frontend for par2.
* [http://sourceforge.net/projects/parchive/ GPar2 2.03]
* [https://web.archive.org/web/20110311041855/http://chuchusoft.com/par2_tbb/ par2+tbb] is a concurrent (multithreaded) version of par2cmdline 0.4 using [[Threading Building Blocks|TBB]], [[GNU General Public License|GPLv2]], or later.
* [https://github.com/jkansanen/par2cmdline-mt par2cmdline-mt] is another multithreaded version of par2cmdline using [[OpenMP]], [[GNU General Public License|GPLv2]], or later.

=== [[FreeBSD]] ===
* [https://web.archive.org/web/20110311041855/http://chuchusoft.com/par2_tbb/ par2+tbb] is a concurrent (multithreaded) version of par2cmdline 0.4 using [[Threading Building Blocks|TBB]], [[GNU General Public License|GPLv2]], or later. It is available in the [[FreeBSD Ports]] system as [https://www.freshports.org/archivers/par2cmdline-tbb/ par2cmdline-tbb].
* [http://parchive.sourceforge.net/ par2cmdline] is available in the [[FreeBSD Ports]] system as [https://www.freshports.org/archivers/par2cmdline/ par2cmdline].

=== [[POSIX]] ===
Software for POSIX conforming operating systems:
* [http://sourceforge.net/projects/ekpar2/ Par2 for KDE 4]

== See also ==
* [[Data degradation|Bit rot]]
* [[Disc rot]]
* [[Data corruption]]
* [[Checksum]]
* [[Comparison of file archivers]] – Some [[file archivers]] are capable of integrating parity data into their formats for error detection and correction:
* [[RAID]]&amp;nbsp;– RAID levels at and above RAID 5 make use of parity data to detect and repair errors.

== References ==
{{Reflist|30em}}

== External links ==
* [http://parchive.sourceforge.net/ Parchive project - full specifications and math behind it]
* [http://www.ydecode.com/page_articles003.htm Introduction to PAR and PAR2]
* [http://www.slyck.com/Newsgroups_Guide_PAR_PAR2_Files Slyck's Guide To The Usenet Newsgroups: PAR &amp; PAR2 Files]
* [http://www.warezfaq.com/allaboutpar.htm Another introduction to PAR and PAR2] and [http://www.warezfaq.com/more_info.htm more information from the same site]
* [http://www.binaries4all.com/quickpar/repair.php Guide to repair files using PAR2]
* [https://web.archive.org/web/20100912073937/http://chuchusoft.com/par2_tbb/ par2+tbb]
* [http://www.milow.net/public/projects/parnrar/parnrar.html Par-N-Rar]
* [http://www.irasnyder.com/devel/#rarslave Rarslave]

[[Category:Archive formats]]
[[Category:Data management]]
[[Category:Usenet]]</text>
      <sha1>nh1llz1q1kvr113izyhxpvsb2i1qmry</sha1>
    </revision>
  </page>
  <page>
    <title>Control flow diagram</title>
    <ns>0</ns>
    <id>21084005</id>
    <revision>
      <id>759302694</id>
      <parentid>658301452</parentid>
      <timestamp>2017-01-10T11:04:13Z</timestamp>
      <contributor>
        <username>Kku</username>
        <id>5846</id>
      </contributor>
      <minor />
      <comment>link [[Process control]] using [[:en:User:Edward/Find link|Find link]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5520" xml:space="preserve">{{this|flow diagrams in business process modeling{{clarify|reason=From the lead, I'm unable to give a more informative characterization of 'Control flow diagram'.|date=January 2014}}|directed graphs representing the control flow of imperative computer programs|control flow graph}}
[[File:Performance seeking control flow diagram.jpg|thumb|240px|Example of a "performance seeking" control flow diagram.&lt;ref name="GO92"&gt; Glenn B. Gilyard and John S. Orme (1992) [http://www.nasa.gov/centers/dryden/pdf/88262main_H-1808.pdf ''Subsonic Flight Test Evaluationof a Performance Seeking ControlAlgorithm on an F-15 Airplane''] NASA Technical Memorandum 4400.&lt;/ref&gt;]]
A '''control flow diagram''' ('''CFD''') is a [[diagram]] to describe the [[control flow]] of a [[business process]], [[process (engineering)|process]] or review

Control flow diagrams were developed in the 1950s, and are widely used in multiple [[engineering]] disciplines. They are one of the classic [[business process modeling]] methodologies, along with [[flow chart]]s, [[data flow diagram]]s, [[functional flow block diagram]], [[Gantt chart]]s, [[PERT]] diagrams, and [[IDEF]].&lt;ref name="TD03"&gt; Thomas Dufresne &amp; James Martin (2003). [http://mason.gmu.edu/~tdufresn/paper.doc "Process Modeling for E-Business"]. INFS 770 Methods for Information Systems Engineering:  Knowledge Management and E-Business. Spring 2003&lt;/ref&gt;

== Overview ==
A control flow diagram can consist of a subdivision to show sequential steps, with if-then-else conditions, repetition, and/or case conditions. Suitably annotated geometrical figures are used to represent operations, data, or equipment, and arrows are used to indicate the sequential flow from one to another.&lt;ref&gt;[http://www.fda.gov/ora/Inspect_ref/igs/gloss.html FDA glossary of terminology applicable to software development and computerized systems]. Accessed 14 Jan 2008.&lt;/ref&gt;

There are several types of control flow diagrams, for example:
* Change control flow diagram, used in [[project management]]
* Configuration decision control flow diagram, used in [[configuration management]]
* [[Process control]] flow diagram, used in [[process management]]  
* Quality control flow diagram, used in [[quality control]].

In software and systems development control flow diagrams can be used in [[control flow analysis]], [[data flow analysis]], [[algorithm analysis]], and [[simulation]]. Control and data are most applicable for real time and data driven systems. These flow analyses transform logic and data requirements text into graphic flows which are easier to analyze than the text. PERT, state transition, and transaction diagrams are examples of control flow diagrams.&lt;ref&gt;Dolores R. Wallace et al. (1996). [http://hissa.nist.gov/HHRFdata/Artifacts/ITLdoc/234/val-proc.html ''Reference Information for the Software Verification and Validation Process''], NIST Special Publication 500-234.&lt;/ref&gt;

== Types of Control Flow Diagrams ==
=== Process Control Flow Diagram ===
A flow diagram can be developed for the process [[control system]] for each critical activity. Process control is normally a closed cycle in which a [[sensor]] provides information to a process control [[software application]] through a [[communications system]]. The application determines if the sensor information is within the predetermined (or calculated) data parameters and constraints. The results of this comparison are fed to an actuator, which controls the critical component. This [[feedback]] may control the component electronically or may indicate the need for a manual action.&lt;ref name="NIoJ02"&gt; National Institute of Justice (2002). [http://www.ncjrs.gov/txtfiles1/nij/195171.txt '' A Method to Assess the Vulnerability of U.S. Chemical Facilities]''. Series: Special Report.&lt;/ref&gt; 

This closed-cycle process has many checks and balances to ensure that it stays safe. The investigation of how the process control can be subverted is likely to be extensive because all or part of the process control may be
oral instructions to an individual monitoring the process. It may be fully computer controlled and automated, or it may be a hybrid in which only the sensor is automated and the action requires manual intervention. Further, some process control systems may use prior generations of hardware and software, while others are state of the art.&lt;ref name="NIoJ02"/&gt;

=== Performance seeking control flow diagram ===
The figure presents an example of a performance seeking control [[flow diagram]] of the algorithm. The control law consists of estimation, modeling, and optimization processes. In the [[Kalman filter]] estimator, the inputs, outputs, and residuals were recorded. At the compact propulsion system modeling stage, all the estimated inlet and engine parameters were recorded.&lt;ref name="GO92"/&gt;  

In addition to temperatures, pressures, and control positions, such estimated parameters as stall margins, thrust, and drag components were recorded. In the optimization phase, the operating condition constraints, optimal solution, and linear programming health status condition codes were recorded. Finally, the actual commands that were sent to the engine through the DEEC were recorded.&lt;ref name="GO92"/&gt;

== See also ==
* [[Data flow diagram]]
* [[Control flow graph]]
* [[DRAKON]]
* [[Flow process chart]]

== References ==
{{NIST-PD}}
{{reflist}}

[[Category:Information systems]]
[[Category:Data management]]
[[Category:Diagrams]]
[[Category:Systems analysis]]
{{DEFAULTSORT:Control Flow Diagram}}</text>
      <sha1>aolqmm6ooor49a6htq2suku0tqu9j8j</sha1>
    </revision>
  </page>
  <page>
    <title>Data aggregation</title>
    <ns>0</ns>
    <id>10186403</id>
    <revision>
      <id>754578731</id>
      <parentid>753457884</parentid>
      <timestamp>2016-12-13T11:58:26Z</timestamp>
      <contributor>
        <username>RichardWeiss</username>
        <id>193093</id>
      </contributor>
      <comment>/* References */ new cat [[Category:Data laws]], I've added it here because of the legal implications section</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7000" xml:space="preserve">'''Data aggregation''' is the compiling of [[information]] from [[databases]] with intent to prepare combined datasets for [[data processing]].&lt;ref&gt;{{cite journal|author1=Stanley, Jay  |author2=Steinhardt, Barry|title=Bigger Monster, Weaker Chains: The Growth of an American Surveillance Society|publisher=American Civil Liberties Union|date=January 2003}}&lt;/ref&gt;

==Description==
The source information for data aggregation may originate from public records and [[criminal]] databases. The information is packaged into aggregate reports and then sold to [[business]]es, as well as to [[Local government|local]], [[State government|state]], and government agencies. This information can also be useful for [[marketing]] purposes. In the United States, many data brokers' activities fall under the [[Fair Credit Reporting Act]] (FCRA) which regulates [[Credit bureau|consumer reporting agencies]]. The agencies then gather and package personal information into [[consumer]] reports that are sold to [[creditor]]s, [[employer]]s, [[insurer]]s, and other businesses.

Various reports of information are provided by database aggregators. Individuals may request their own consumer reports which contain basic [[biographical]] information such as name, date of birth, current address, and phone number. Employee [[background check]] reports, which contain highly detailed information such as past addresses and length of residence, [[professional]] [[Licensure|licenses]], and criminal history, may be requested by eligible and qualified third parties. Not only can this data be used in employee background checks, but it may also be used to make decisions about insurance coverage, pricing, and law enforcement. [[Privacy]] activists argue that database aggregators can provide erroneous information.&lt;ref&gt;{{cite web|url=http://www.privacyactivism.org/docs/DataAggregatorsStudy.html |title=Data Aggregators: A Study of Data Quality and Responsiveness |author1=Pierce, Deborah |author2=Ackerman, Linda |publisher=Privacyactivism.org |date=2005-05-19 |accessdate=2007-04-02 |archiveurl=https://web.archive.org/web/20070319220412/http://www.privacyactivism.org/docs/DataAggregatorsStudy.html |archivedate=2007-03-19 |deadurl=yes |df= }}&lt;/ref&gt;

==Role of the Internet==
The potential of the [[Internet]] to consolidate and manipulate information has a new application in data aggregation, also known as ''screen scraping''. The Internet gives users the opportunity to consolidate their [[username]]s and [[password]]s, or PINs. Such consolidation enables consumers to access a wide variety of PIN-protected [[website]]s containing personal information by using one master PIN on a single website. Online account providers include [[financial institution]]s, [[stockbroker]]s, [[airline]] and frequent flyer and other reward programs, and [[e-mail]] accounts. Data aggregators can gather account or other information from designated websites by using account holders' PINs, and then making the users' account information available to them at a single website operated by the aggregator at an account holder's request. Aggregation services may be offered on a standalone basis or in conjunction with other financial services, such as [[portfolio (finance)|portfolio]] tracking and [[Bill (payment)|bill]] payment provided by a specialized website, or as an additional service to augment the online presence of an enterprise established beyond the virtual world. Many established companies with an Internet presence appear to recognize the value of offering an aggregation service to enhance other web-based services and attract visitors. Offering a data aggregation service to a website may be attractive because of the potential that it will frequently draw users of the service to the hosting website.

==Local business data aggregation==
When it comes to compiling location information on local businesses, there are several major data aggregators that collect information such as the business name, address, phone number, website, description and hours of operation. They then validate this information using various validation methods. Once the business information has been verified to be accurate, the data aggregators make it available to publishers like [[Google]] and [[Yelp]].

When Yelp, for example, goes to update their Yelp listings, they will pull data from these local data aggregators. Publishers take local business data from different sources and compare it to what they currently have in their database. They then update their database it with what information they deem accurate.

==Legal implications==
Financial institutions are concerned about the possibility of [[legal liability|liability]] arising from data aggregation activities, potential [[security]] problems, infringement on [[intellectual property]] rights and the possibility of diminishing traffic to the institution's website. The aggregator and financial institution may agree on a data feed arrangement activated on the customer's request, using an Open Financial Exchange (OFX) standard to request and deliver information to the site selected by the customer as the place from which they will view their account data. Agreements provide an opportunity for institutions to negotiate to protect their customers' interests and offer aggregators the opportunity to provide a robust service. Aggregators who agree with information providers to extract data without using an OFX standard may reach a lower level of consensual relationship; therefore, "screen scraping" may be used to obtain account data, but for business or other reasons, the aggregator may decide to obtain prior consent and negotiate the terms on which customer data is made available. "Screen scraping" without consent by the content provider has the advantage of allowing subscribers to view almost any and all accounts they happen to have opened anywhere on the Internet through one website.

==Outlook==
Over time, the transfer of large amounts of account data from the account provider to the aggregator's server could develop into a comprehensive profile of a user, detailing their banking and [[credit card]] transactions, balances, securities transactions and portfolios, and [[travel]] history and preferences. As the sensitivity to data protection considerations grows, it is likely there will be a considerable focus on the extent to which data aggregators may seek to use this data either for their own purposes or to share it on some basis with the operator of a website on which the service is offered or with other third parties.&lt;ref&gt;{{cite web|url=http://www.ffhsj.com/bancmail/bmarts/aba_art.htm|title=Scrape It, Scrub It and Show It: The Battle Over Data Aggregation|author1=Ledig, Robert H.  |author2=Vartanian, Thomas P.|publisher=Fried Frank|date=2002-09-11|accessdate=2007-04-02}}&lt;/ref&gt;

==References==
&lt;references /&gt;

{{DEFAULTSORT:Data Aggregator}}
[[Category:Data management]]
[[Category:Information privacy]]
[[Category:Data laws]]</text>
      <sha1>bft0ee8gkri8cqxvhjmvkix7vnydgy2</sha1>
    </revision>
  </page>
  <page>
    <title>Two-phase commit protocol</title>
    <ns>0</ns>
    <id>787850</id>
    <revision>
      <id>761695313</id>
      <parentid>761695308</parentid>
      <timestamp>2017-01-24T08:18:05Z</timestamp>
      <contributor>
        <username>ClueBot NG</username>
        <id>13286072</id>
      </contributor>
      <minor />
      <comment>Reverting possible vandalism by [[Special:Contribs/49.248.74.34|49.248.74.34]] to version by 37.228.230.157. [[WP:CBFP|Report False Positive?]] Thanks, [[WP:CBNG|ClueBot NG]]. (2908071) (Bot)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="14270" xml:space="preserve">{{Redirect|2PC|the play in American and Canadian football|Two-point conversion|the cryptographic protocol|Commitment scheme}}

In [[transaction processing]], [[database]]s, and [[computer networking]], the '''two-phase commit protocol''' ('''2PC''') is a type of [[Atomic commit|atomic commitment protocol]] (ACP). It is a [[distributed algorithm]] that coordinates all the processes that participate in a [[Distributed transaction|distributed atomic transaction]] on whether to ''[[Commit (data management)|commit]]'' or ''abort'' (''roll back'') the transaction (it is a specialized type of [[Consensus (computer science)|consensus]] protocol). The protocol achieves its goal even in many cases of temporary system failure (involving either process, network node, communication, etc. failures), and is thus widely used.&lt;ref name="bernstein1987"&gt;[[Phil Bernstein|Philip A. Bernstein]], Vassos Hadzilacos, Nathan Goodman (1987): [http://research.microsoft.com/en-us/people/philbe/ccontrol.aspx  ''Concurrency Control and Recovery in Database Systems''], Chapter 7, Addison Wesley Publishing Company, ISBN 0-201-10715-5&lt;/ref&gt;&lt;ref name="weikum2001"&gt;[[Gerhard Weikum]], Gottfried Vossen (2001): [http://www.elsevier.com/wps/find/bookdescription.cws_home/677937/description#description  ''Transactional Information Systems''], Chapter 19, Elsevier, ISBN 1-55860-508-8&lt;/ref&gt;&lt;ref name=Bern2009&gt;Philip A. Bernstein, Eric Newcomer (2009): [http://www.elsevierdirect.com/product.jsp?isbn=9781558606234 ''Principles of Transaction Processing'', 2nd Edition], Chapter 8, Morgan Kaufmann (Elsevier), ISBN 978-1-55860-623-4&lt;/ref&gt;
However, it is not resilient to all possible failure configurations, and in rare cases, user (e.g., a system's administrator) intervention is needed to remedy an outcome. To accommodate recovery from failure (automatic in most cases) the protocol's participants use [[Server log|logging]] of the protocol's states. Log records, which are typically slow to generate but survive failures, are used by the protocol's [[recovery procedure]]s. Many protocol variants exist that primarily differ in logging strategies and recovery mechanisms. Though usually intended to be used infrequently, recovery procedures compose a substantial portion of the protocol, due to many possible failure scenarios to be considered and supported by the protocol.

In a "normal execution" of any single [[distributed transaction]] ( i.e., when no failure occurs, which is typically the most frequent situation), the protocol consists of two phases:
#The ''commit-request phase'' (or ''voting phase''), in which a ''coordinator'' process attempts to prepare all the transaction's participating processes (named ''participants'', ''cohorts'', or ''workers'') to take the necessary steps for either committing or aborting the transaction and to ''vote'', either "Yes": commit (if the transaction participant's local portion execution has ended properly), or "No": abort (if a problem has been detected with the local portion), and
#The ''commit phase'', in which, based on ''voting'' of the cohorts, the coordinator decides whether to commit (only if ''all'' have voted "Yes") or abort the transaction (otherwise), and notifies the result to all the cohorts. The cohorts then follow with the needed actions (commit or abort) with their local transactional resources (also called ''recoverable resources''; e.g., database data) and their respective portions in the transaction's other output (if applicable).

Note that the two-phase commit (2PC) protocol should not be confused with the [[two-phase locking]] (2PL) protocol, a [[concurrency control]] protocol.

==Assumptions==
The protocol works in the following manner: one node is a designated '''coordinator''', which is the master site, and the rest of the nodes in the network are designated the '''cohorts'''. The protocol assumes that there is [[stable storage]] at each node with a [[Write ahead logging|write-ahead log]], that no node crashes forever, that the data in the write-ahead log is never lost or corrupted in a crash, and that any two nodes can communicate with each other. The last assumption is not too restrictive, as network communication can typically be rerouted. The first two assumptions are much stronger; if a node is totally destroyed then data can be lost.

The protocol is initiated by the coordinator after the last step of the transaction has been reached. The cohorts then respond with an '''agreement''' message or an '''abort''' message depending on whether the transaction has been processed successfully at the cohort.

==Basic algorithm==

===Commit request phase===
or '''voting phase'''

#The coordinator sends a '''query to commit''' message to all cohorts and waits until it has received a reply from all cohorts.
#The cohorts execute the transaction up to the point where they will be asked to commit.  They each write an entry to their ''undo log'' and an entry to their ''[[redo log]]''.
#Each cohort replies with an '''agreement''' message (cohort votes '''Yes''' to commit), if the cohort's actions succeeded, or an '''abort''' message (cohort votes '''No''', not to commit), if the cohort experiences a failure that will make it impossible to commit.

===Commit phase===
or '''Completion phase'''

====Success====
If the coordinator received an '''agreement''' message from ''all'' cohorts during the commit-request phase:
#The coordinator sends a '''commit''' message to all the cohorts.
#Each cohort completes the operation, and releases all the locks and resources held during the transaction.
#Each cohort sends an '''acknowledgment''' to the coordinator.
#The coordinator completes the transaction when all acknowledgments have been received.

====Failure====
If ''any'' cohort votes '''No''' during the commit-request phase (or the coordinator's timeout '''expires'''):
#The coordinator sends a '''rollback''' message to all the cohorts.
#Each cohort undoes the transaction using the undo log, and releases the resources and locks held during the transaction.
#Each cohort sends an '''acknowledgement''' to the coordinator.
#The coordinator undoes the transaction when all acknowledgements have been received.

====Message flow====
&lt;pre&gt;
Coordinator                                         Cohort
                              QUERY TO COMMIT
                --------------------------------&gt;
                              VOTE YES/NO           prepare*/abort*
                &lt;-------------------------------
commit*/abort*                COMMIT/ROLLBACK
                --------------------------------&gt;
                              ACKNOWLEDGMENT        commit*/abort*
                &lt;--------------------------------  
end
&lt;/pre&gt;
An * next to the record type means that the record is forced to stable storage.&lt;ref name="mohan1986"&gt;[[C. Mohan]], Bruce Lindsay and R. Obermarck (1986): [http://dl.acm.org/citation.cfm?id=7266  "Transaction management in the R* distributed database management system"],''ACM Transactions on Database Systems (TODS)'', Volume 11 Issue 4, Dec. 1986, Pages 378 - 396&lt;/ref&gt;

==Disadvantages==
The greatest disadvantage of the two-phase commit protocol is that it is a blocking protocol. If the coordinator fails permanently, some cohorts will never resolve their transactions: After a cohort has sent an '''agreement''' message to the coordinator, it will block until a '''commit''' or '''rollback''' is received.

==Implementing the two-phase commit protocol==

===Common architecture===
In many cases the 2PC protocol is distributed in a computer network. It is easily distributed by implementing multiple dedicated 2PC components similar to each other, typically named ''[[Transaction manager]]s'' (TMs; also referred to as ''2PC agents'' or Transaction Processing Monitors), that carry out the protocol's execution for each transaction (e.g., [[The Open Group]]'s [[X/Open XA]]). The databases involved with a distributed transaction, the ''participants'', both the coordinator and cohorts, ''register'' to close TMs (typically residing on respective same network nodes as the participants) for terminating that transaction using 2PC. Each distributed transaction has an ad hoc set of TMs, the TMs to which the transaction participants register. A leader, the coordinator TM, exists for each transaction to coordinate 2PC for it, typically the TM of the coordinator database. However, the coordinator role can be transferred to another TM for performance or reliability reasons. Rather than exchanging 2PC messages among themselves, the participants exchange the messages with their respective TMs. The relevant TMs communicate among themselves to execute the 2PC protocol schema above, "representing" the respective participants, for terminating that transaction. With this architecture the protocol is fully distributed (does not need any central processing component or data structure), and scales up with number of network nodes (network size) effectively.

This common architecture is also effective for the distribution of other [[atomic commitment protocol]]s besides 2PC, since all such protocols use the same voting mechanism and outcome propagation to protocol participants.&lt;ref name="bernstein1987" /&gt;&lt;ref name="weikum2001" /&gt;

===Protocol optimizations===
[[Database]] research has been done on ways to get most of the benefits of the two-phase commit protocol while reducing costs by ''protocol optimizations''&lt;ref name="bernstein1987" /&gt;&lt;ref name="weikum2001" /&gt;&lt;ref name="Bern2009" /&gt; and protocol operations saving under certain system's behavior assumptions.

====Presumed Abort and Presumed Commit====
''Presumed abort'' or ''Presumed commit'' are common such optimizations.&lt;ref name="weikum2001" /&gt;&lt;ref name=Bern2009/&gt;&lt;ref name="mohan1983"&gt;[[C. Mohan]], Bruce Lindsay (1985): [http://portal.acm.org/citation.cfm?id=850772  "Efficient commit protocols for the tree of processes model of distributed transactions"],''ACM SIGOPS Operating Systems Review'',
19(2),pp. 40-52 (April 1985)&lt;/ref&gt; An assumption about the outcome of transactions, either commit, or abort, can save both messages and logging operations by the participants during the 2PC protocol's execution. For example, when presumed abort, if during system recovery from failure no logged evidence for commit of some transaction is found by the recovery procedure, then it assumes that the transaction has been aborted, and acts accordingly. This means that it does not matter if aborts are logged at all, and such logging can be saved under this assumption. Typically a penalty of additional operations is paid during recovery from failure, depending on optimization type. Thus the best variant of optimization, if any, is chosen according to failure and transaction outcome statistics.

====Tree two-phase commit protocol====
The '''[[Tree (data structure)|Tree]] 2PC protocol'''&lt;ref name="weikum2001" /&gt; (also called ''Nested 2PC'', or ''Recursive 2PC'') is a common variant of 2PC in a [[computer network]], which better utilizes the underlying communication infrastructure. The participants in a distributed transaction are typically invoked in an order which defines a tree structure, the ''invocation tree'', where the participants are the nodes and the edges are the invocations (communication links). The same tree is commonly utilized to complete the transaction by a 2PC protocol, but also another communication tree can be utilized for this, in principle. In a tree 2PC the coordinator is considered the root ("top") of a communication tree (inverted tree), while the cohorts are the other nodes. The coordinator can be the node that originated the transaction (invoked recursively (transitively) the other participants), but also another node in the same tree can take the coordinator role instead. 2PC messages from the coordinator are propagated "down" the tree, while messages to the coordinator are "collected" by a cohort from all the cohorts below it, before it sends the appropriate message "up" the tree (except an '''abort''' message, which is propagated "up" immediately upon receiving it or if the current cohort initiates the abort).

The '''Dynamic two-phase commit''' (Dynamic two-phase commitment, D2PC) '''protocol'''&lt;ref name="weikum2001" /&gt;&lt;ref name="raz1995"&gt;[[Yoav Raz]] (1995): [http://www.springerlink.com/content/pv12p828kk616258/  "The Dynamic Two Phase Commitment (D2PC) protocol "],''Database Theory — ICDT '95'', ''Lecture Notes in Computer Science'', Volume 893/1995, pp. 162-176, Springer, ISBN 978-3-540-58907-5&lt;/ref&gt; is a variant of Tree 2PC with no predetermined coordinator. It subsumes several optimizations that have been proposed earlier. '''Agreement''' messages ('''Yes''' votes) start to propagate from all the leaves, each leaf when completing its tasks on behalf of the transaction (becoming ''ready''). An intermediate (non leaf) node sends when ''ready'' an '''agreement''' message to the last (single) neighboring node from which '''agreement''' message has not yet been received. The coordinator is determined dynamically by racing '''agreement''' messages over the transaction tree, at the place where they collide. They collide either at a transaction tree node, to be the coordinator, or on a tree edge. In the latter case one of the two edge's nodes is elected as a coordinator (any node). D2PC is time optimal (among all the instances of a specific transaction tree, and any specific Tree 2PC protocol implementation; all instances have the same tree; each instance has a different node as coordinator): By choosing an optimal coordinator D2PC commits both the coordinator and each cohort in minimum possible time, allowing the earliest possible release of locked resources in each transaction participant (tree node).

==See also==
*[[Atomic commit]]
*[[Commit (data management)]]
*[[Three-phase commit protocol]]
*[[X/Open XA|XA]]
*[[Paxos algorithm]]
*[[Two Generals' Problem]]

==References==
{{Reflist}}

==External links==
*[http://exploredatabase.blogspot.in/2014/07/two-phase-commit-protocol-in-pictures.html Two Phase Commit protocol explained in Pictures] by exploreDatabase

{{DEFAULTSORT:Two-Phase Commit Protocol}}
[[Category:Data management]]
[[Category:Transaction processing]]</text>
      <sha1>qet372qxkezgu2hevgtfi77b3fynucr</sha1>
    </revision>
  </page>
  <page>
    <title>Semantic translation</title>
    <ns>0</ns>
    <id>2994894</id>
    <revision>
      <id>720900670</id>
      <parentid>627434220</parentid>
      <timestamp>2016-05-18T16:55:32Z</timestamp>
      <contributor>
        <username>Swpb</username>
        <id>1921264</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2871" xml:space="preserve">{{Unreferenced|date=December 2009}}
'''Semantic translation''' is the process of using [[semantic]] information to aid in the translation of data in one representation or [[data model]] to another representation or data model.  Semantic translation takes advantage of semantics that associate meaning with individual [[data element]]s in one [[data dictionary|dictionary]] to create an equivalent meaning in a second system.

An example of semantic translation is the conversion of [[XML]] data from one data model to a second data model using formal [[ontologies]] for each system such as the [[Web Ontology Language]] (OWL).  This is frequently required by [[intelligent agents]] that wish to perform searches on remote computer systems that use different data models to store their data elements.  The process of allowing a single user to search multiple systems with a single search request is also known as [[federated search]].

Semantic translation should be differentiated from [[data mapping]] tools that do simple one-to-one translation of data from one system to another without actually associating meaning with each data element.

Semantic translation requires that data elements in the source and destination systems have "semantic mappings" to a central registry or registries of data elements. The simplest mapping is of course where there is equivalence.
There are three types of [[Semantic equivalence]]:

* '''[[Class (computer science)|Class]] Equivalence'''{{Anchor|Class equivalence}} - indicating that class or "concepts" are equivalent.  For example: "Person" is the same as "Individual"
* '''[[Relation (mathematics)|Property]] Equivalence'''{{Anchor|Property equivalence}} - indicating that two properties are equivalent.  For example: "PersonGivenName" is the same as "FirstName"
* '''[[Instance (computer science)|Instance]] Equivalence'''{{Anchor|Instance equivalence}} - indicating that two individual instances of objects are equivalent.  For example: "Dan Smith" is the same person as "Daniel Smith"

Semantic translation is very difficult if the terms in a particular data model do not have direct one-to-one mappings to data elements in a foreign data model. In that situation an alternative approach must be used to find mappings from the original data to the foreign data elements.  This problem can be alleviated by centralized metadata registries that use the ISO-11179 standards such as the [[National Information Exchange Model]] (NIEM).

==See also==
* [[Data mapping]]
* [[Semantic heterogeneity]]
* [[Semantic mapper]]
* [[Federated search]]
* [[Intelligent agents]]
* [[ISO/IEC 11179]]
* [[National Information Exchange Model]]
* [[Semantic Web]]
* [[Vocabulary-based transformation]]
* [[Web Ontology Language]]

{{DEFAULTSORT:Semantic Translation}}
[[Category:Data management]]
[[Category:Enterprise application integration]]</text>
      <sha1>jnchv6yatg3xim2zff3kp8tt9atnwpb</sha1>
    </revision>
  </page>
  <page>
    <title>Data dictionary</title>
    <ns>0</ns>
    <id>645139</id>
    <revision>
      <id>761240570</id>
      <parentid>761234121</parentid>
      <timestamp>2017-01-21T21:00:52Z</timestamp>
      <contributor>
        <username>Mindmatrix</username>
        <id>160367</id>
      </contributor>
      <comment>rm promotional link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8421" xml:space="preserve">{{Distinguish|Dictionary (data structure)}}
{{Use dmy dates|date=July 2013}}
A '''data dictionary''', or [[metadata repository]], as defined in the ''IBM Dictionary of Computing'', is a "centralized repository of information about data such as meaning, relationships to other data, origin, usage, and format."&lt;ref&gt;ACM, [http://portal.acm.org/citation.cfm?id=541721 IBM Dictionary of Computing], 10th edition, 1993&lt;/ref&gt; The term can have one of several closely related meanings pertaining to [[database]]s and [[database management system]]s (DBMS):

* A [[document]] describing a database or collection of databases
* An integral [[software component|component]] of a [[Database management system|DBMS]] that is required to determine its structure
* A piece of [[middleware]] that extends or supplants the native data dictionary of a DBMS

==Documentation==
The terms ''data dictionary'' and ''data repository'' indicate a more general software utility than a catalogue. A ''catalogue'' is closely coupled with the DBMS software. It provides the information stored in it to the user and the DBA, but it is mainly accessed by the various software modules of the DBMS itself, such as [[Data definition language|DDL]] and [[Data manipulation language|DML]] compilers, the query optimiser, the transaction processor, report generators, and the constraint enforcer. On the other hand, a ''data dictionary'' is a data structure that stores [[metadata]], i.e., (structured) data about information. The software package for a stand-alone data dictionary or data repository may interact with the software modules of the DBMS, but it is mainly used by the designers, users and administrators of a computer system for information resource management. These systems maintain information on system hardware and software configuration, documentation, application and users as well as other information relevant to system administration.&lt;ref&gt;Ramez Elmasri, Shamkant B. Navathe: ''Fundamentals of Database Systems'', 3rd. ed. sect. 17.5, p. 582&lt;/ref&gt;

If a data dictionary system is used only by the designers, users, and administrators and not by the DBMS Software, it is called a ''passive data dictionary.'' Otherwise, it is called an ''active data dictionary'' or ''data dictionary.''  When a passive data dictionary is updated, it is done so manually and independently from any changes to a DBMS (database) structure. With an active data dictionary, the dictionary is updated first and changes occur in the DBMS automatically as a result.

Database [[User (computing)|users]] and [[Application software|application]] developers can benefit from an authoritative data dictionary document that catalogs the organization, contents, and conventions of one or more databases.&lt;ref&gt;TechTarget, ''SearchSOA'', [http://searchsoa.techtarget.com/sDefinition/0,,sid26_gci211896,00.html What is a data dictionary?]&lt;/ref&gt; This typically includes the names and descriptions of various [[Table (database)|tables]] (records or Entities) and their contents ([[Column (database)|fields]]) plus additional details, like the [[Data type|type]] and length of each [[data element]].  Another important piece of information that a data dictionary can provide is the relationship between Tables.  This is sometimes referred to in Entity-Relationship diagrams, or if using Set descriptors, identifying which Sets database Tables participate in.

In an active data dictionary constraints may be placed upon the underlying data.  For instance, a Range may be imposed on the value of numeric data in a data element (field), or a Record in a Table may be FORCED to participate in a set relationship with another Record-Type.  Additionally, a distributed DBMS may have certain location specifics described within its active data dictionary (e.g. where Tables are physically located).

The data dictionary consists of record types (tables) created in the database by systems generated command files, tailored for each supported back-end DBMS. Command files contain SQL Statements for CREATE TABLE, CREATE UNIQUE INDEX, ALTER TABLE (for referential integrity), etc., using the specific statement required by that type of database.

There is no universal standard as to the level of detail in such a document.

==Middleware==
In the construction of database applications, it can be useful to introduce an additional layer of data dictionary software, i.e. [[middleware]], which communicates with the underlying DBMS data dictionary. Such a "high-level" data dictionary may offer additional features and a degree of flexibility that goes beyond the limitations of the native "low-level" data dictionary, whose primary purpose is to support the basic functions of the DBMS, not the requirements of a typical application. For example, a high-level data dictionary can provide alternative [[entity-relationship model]]s tailored to suit different applications that share a common database.&lt;ref&gt;U.S. Patent 4774661, [http://www.freepatentsonline.com/4774661.html Database management system with active data dictionary], 19 November 1985, AT&amp;T&lt;/ref&gt; Extensions to the data dictionary also can assist in [[query optimization]] against [[distributed database]]s.&lt;ref&gt;U.S. Patent 4769772, [http://www.freepatentsonline.com/4769772.html Automated query optimization method using both global and parallel local optimizations for materialization access planning for distributed databases], 28 February 1985, Honeywell Bull&lt;/ref&gt;  Additionally, DBA functions are often automated using restructuring tools that are tightly coupled to an active data dictionary.

[[Software framework]]s aimed at [[rapid application development]] sometimes include high-level data dictionary facilities, which can substantially reduce the amount of programming required to build [[Menu (computing)|menus]], [[Form (programming)|forms]], reports, and other components of a database application, including the database itself. For example, PHPLens includes a [[PHP]] [[class library]] to automate the creation of tables, indexes, and [[foreign key]] constraints [[Portability (software)|portably]] for multiple databases.&lt;ref&gt;PHPLens, [http://phplens.com/lens/adodb/docs-datadict.htm ADOdb Data Dictionary Library for PHP]&lt;/ref&gt; Another PHP-based data dictionary, part of the RADICORE toolkit, automatically generates program [[Object (computer science)|objects]], [[Scripting language|scripts]], and SQL code for menus and forms with [[data validation]] and complex [[join (SQL)|joins]].&lt;ref&gt;RADICORE, [http://www.radicore.org/viewarticle.php?article_id=5 What is a Data Dictionary?]&lt;/ref&gt; For the [[ASP.NET]] environment, [[Base One International|Base One's]] data dictionary provides cross-DBMS facilities for automated database creation, data validation, performance enhancement ([[Cache (computing)|caching]] and index utilization), [[application security]], and extended [[data type]]s.&lt;ref&gt;Base One International Corp., [http://www.boic.com/b1ddic.htm Base One Data Dictionary]&lt;/ref&gt;  [[Visual DataFlex]] features&lt;ref&gt;VISUAL DATAFLEX,[http://www.visualdataflex.com/features.asp?pageid=1030 features]&lt;/ref&gt; provides the ability to use DataDictionaries as class files to form  middle layer between the user interface and the underlying database.   The intent is to create standardized rules to maintain data integrity and enforce business rules throughout one or more related applications.

==Platform-specific examples==
Developers use a ''data description specification'' (''DDS'') to describe data attributes in file descriptions that are external to the application program that processes the data, in the context of an [[IBM System i]].&lt;ref&gt;{{cite web |url=http://publib.boulder.ibm.com/infocenter/iseries/v5r3/topic/dds/rbafpddsmain.htm |title=DDS documentation for IBM System i V5R3}}&lt;/ref&gt;

==See also==
*[[Data hierarchy]]
*[[Data modeling]]
*[[Database schema]]
*[[ISO/IEC 11179]]
*[[Metadata registry]]
*[[Semantic spectrum]]
*[[Vocabulary OneSource]]
*[[Metadata repository]]

==References==
{{Reflist|30em}}

==External links==
{{Commons category|Data dictionary}}
*Yourdon, ''Structured Analysis Wiki'', [http://yourdon.com/strucanalysis/wiki/index.php?title=Chapter_10 Data Dictionaries]

{{Data warehouse}}

{{Authority control}}

{{DEFAULTSORT:Data Dictionary}}
[[Category:Data management]]
[[Category:Data modeling]]
[[Category:Knowledge representation]]
[[Category:Metadata]]</text>
      <sha1>92h9onkaqwf4ywyd2dggc90kjiema96</sha1>
    </revision>
  </page>
  <page>
    <title>Flat file database</title>
    <ns>0</ns>
    <id>573973</id>
    <revision>
      <id>753671249</id>
      <parentid>749961521</parentid>
      <timestamp>2016-12-08T15:47:12Z</timestamp>
      <contributor>
        <ip>165.225.80.120</ip>
      </contributor>
      <comment>/* Example database */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11669" xml:space="preserve">{{distinguish|Flat file system}}
{{refimprove|date=March 2015}}
{{originalresearch|date=March 2015}}

[[Image:Flat File Model.svg|thumb|280px|Example of a flat file model&lt;ref name="USDT01"&gt;[http://knowledge.fhwa.dot.gov/tam/aashto.nsf/All+Documents/4825476B2B5C687285256B1F00544258/$FILE/DIGloss.pdf Data Integration Glossary] {{webarchive |url=https://web.archive.org/web/20090320001015/http://knowledge.fhwa.dot.gov/tam/aashto.nsf/All+Documents/4825476B2B5C687285256B1F00544258/$FILE/DIGloss.pdf |date=March 20, 2009 }}, U.S. Department of Transportation, August 2001.&lt;/ref&gt;]]

A '''flat file database''' is a [[database]] which is stored on its host computer system as an ordinary unstructured file called a "flat file". To access the structure of the data and manipulate it, the file must be read in its entirety into the computer's memory. Upon completion of the database operations, the file is again written out in its entirety to the host's file system. In this stored mode the database is said to be "flat", meaning that it has no structure for indexing and there are usually no structural relationships between the records. A flat file can be a [[plain text]] file or a [[binary file]].

The term has generally implied a small, simple database. As computer memory has become cheaper, more sophisticated databases can now be entirely held in memory for faster access. These newer databases would not generally be referred to as flat-file databases.

==Overview==

Plain text files usually contain one [[Record (computer science)|record]] per line,&lt;ref&gt;{{Citation
 | last = Fowler
 | first = Glenn
 | year = 1994
 | title = cql: Flat file database query language
 | periodical = WTEC'94: Proceedings of the USENIX Winter 1994 Technical Conference on USENIX Winter 1994 Technical Conference
 | url = http://www.research.att.com/~astopen/publications/cql-1994.pdf
}}&lt;/ref&gt; There are different conventions for depicting data. In [[comma-separated values]] and [[delimiter-separated values]] files, [[field (computer science)|field]]s can be separated by [[delimiters]] such as [[Comma-separated values|comma]] or [[Tab separated values|tab]] characters. In other cases, each field may have a fixed length; short values may be padded with [[space character]]s. Extra formatting may be needed to avoid [[delimiter collision]]. More complex solutions are [[markup language]]s and [[programming language]]s.

Using delimiters incurs some [[Computational overhead|overhead]] in locating them every time they are processed (unlike fixed-width formatting), which may have [[Computer performance|performance]] implications. However, use of character delimiters (especially commas) is also a crude form of [[data compression]] which may assist overall performance by reducing data volumes&amp;nbsp;— especially for [[data transmission]] purposes. Use of character delimiters which include a length component ([[String literal#Declarative notation|Declarative notation]]) is comparatively rare but vastly reduces the overhead associated with locating the extent of each field.

Typical examples of flat files are &lt;code&gt;[[/etc/passwd]]&lt;/code&gt; and &lt;code&gt;[[/etc/group]]&lt;/code&gt; on [[Unix-like]] operating systems. Another example of a flat file is a name-and-address list with the fields ''Name'', ''Address'', and ''Phone Number''.

A list of names, addresses, and phone numbers written by hand on a sheet of paper is a flat file database. This can also be done with any [[typewriter]] or [[word processor]]. A [[spreadsheet]] or [[text editor]] program may be used to implement a flat file database, which may then be printed or used [[online]] for improved search capabilities.

==History==
[[Herman Hollerith]] conceived the idea that data could be represented by holes punched in paper cards then tabulated by machine. He implemented this concept for the [[United States Census Bureau|US Census Bureau]]; thus the [[1890 United States Census]] processing created the first database—consisting of thousands of boxes full of [[punched card]]s.

Hollerith's enterprise grew into the computer giant [[IBM]], which dominated the data processing market for most of the 20th century. IBM's fixed-length field, 80-column punch cards became the ubiquitous means of inputting  electronic data until the 1970s.

In the 1980s, configurable flat-file database [[computer application]]s were popular on [[DOS]] and the [[Apple Macintosh|Macintosh]]. These programs were designed to make it easy for individuals to design and use their own databases, and were almost on par with [[word processors]] and [[spreadsheet]]s in popularity.{{citation needed|date=September 2011}} Examples of flat-file database products were early versions of [[FileMaker]] and the [[shareware]] [[PC-File]].  Some of these, like [[dBase II]], offered limited [[relational database|relational]] capabilities, allowing some data to be shared between files.

In the 2010s flat file databases were used in [[content management system]]s. Instead of using a database, web developers were able to change the content directly in the file system or at the command line.

===Contemporary implementations===
FairCom's [[c-tree]] is an example of a modern enterprise-level solution, and [[spreadsheet]] software and [[text editor]]s can be used for this purpose.  [[WebDNA]] is a scripting language designed for the World Wide Web, with a hybrid flat file in-memory database system making it easy to build resilient database-driven websites. With the in-memory concept, WebDNA searches and database updates are almost realtime while the data is stored as text files within the website itself. Otherwise, flat file database is implemented in [[Microsoft Works]] and [[Apple Works]]. Over time, products like [[Borland]]'s Paradox, and [[Microsoft]]'s [[Microsoft Access|Access]] started offering some relational capabilities, as well as built-in programming languages.  Database Management Systems ([[DBMS]]) like [[MySQL]] or [[Oracle database|Oracle]] generally require programmers to build applications.

Faceless flat file database engines are used internally by [[Mac OS X]], [[Firefox]], and other computer software to store configuration data. Programs to manage collections of books or appointments and [[address book]] are essentially single-purpose flat file database applications, allowing users to store and retrieve information from flat files using a predefined set of fields.

==Data transfer operations==
Flat files are used not only as data storage tools in DB and [[Content_management_system|CMS]] systems, but also as data transfer tools to remote servers (in which case they become known as information streams).

In recent years, this latter implementation has been replaced with [[XML]] files, which not only contain but also describe the data.  Those still using flat files to transfer information are mainframes employing specific procedures which are too expensive to modify.

One criticism often raised against the XML format as a way to perform mass data transfer operations is that file size is significantly larger than that of flat files, which is generally reduced to the bare minimum.  The solution to this problem consists in XML file compression (a solution that applies equally well to flat files), which has nowadays gained [[Efficient XML Interchange|EXI]] standards (i.e., Efficient XML Interchange, which is often used by mobile devices).

It is advisable that transfer data be performed via EXI rather than flat files because defining the compression method is not required, because libraries reading the file contents are readily available, and because there is no need for the two communicating systems to preliminarily establish a protocol describing data properties such as position, alignment, type, and format. However, in those circumstances where the sheer mass of data and/or the inadequacy of legacy systems becomes a problem, the only viable solution remains the use of flat files.  In order to successfully handle those problems connected with data communication, format, validation, control and much else (be it a flat file or an XML file data source), it is advisable to adopt a [[Data Quality Firewall]].

==Terminology==
"Flat file database" may be defined very narrowly, or more broadly. The narrower interpretation is correct in [[Database|database theory]]; the broader covers the term as generally used.

Strictly, a flat file database should consist of nothing but data and, if records vary in length, delimiters. More broadly, the term refers to any database which exists in a single file in the form of rows and columns, with no relationships or links between records and fields except the table structure.

Terms used to describe different aspects of a database and its tools differ from one implementation to the next, but the concepts remain the same. FileMaker uses the term "Find", while MySQL uses the term "Query"; but the concept is the same. FileMaker "files", in version 7 and above, are equivalent to MySQL "databases", and so forth. To avoid confusing the reader, one consistent set of terms is used throughout this article.

However, the basic terms "record" and "field" are used in nearly every flat file database implementation.

==Example database==
The following example illustrates the basic elements of a flat-file database. The [[data]] arrangement consists of a series of columns and rows organized into a [[table (information)|tabular format]]. This specific example uses only one table.

The columns include: ''name'' (a person's name, second column); ''team'' (the name of an athletic team supported by the person, third column); and a numeric ''unique ID'', (used to uniquely identify records, first column).

Here is an example textual representation of the described data:

 id    name    team
 1     Amy     Blues
 2     Bob     Reds
 3     Chuck   Blues
 4     Richard Blues
 5     Ethel   Reds
 6     Fred    Blues
 7     Gilly   Blues
 8     Hank    Reds
 9     Hank    Blues

This type of data representation is quite standard for a flat-file database, although there are some additional considerations that are not readily apparent from the text:
* '''Data types:''' each column in a database table such as the one above is ordinarily restricted to a specific [[data type]]. Such restrictions are usually established by convention, but not formally indicated unless the data is transferred to a [[relational database]] system.
* '''Separated columns:''' In the above example, individual columns are separated using [[Whitespace (computer science)|whitespace]] characters. This is also called indentation or "fixed-width" data formatting. Another common convention is to separate columns using one or more [[delimiter]] characters. More complex solutions are markup and programming languages.
* '''Relational algebra:''' Each row or record in the above table meets the standard definition of a [[tuple]] under [[relational algebra]] (the above example depicts a series of 3-tuples). Additionally, the first row specifies the [[Tuple#Field_names|field names]] that are associated with the values of each row.
* '''Database management system:''' Since the formal operations possible with a text file are usually more limited than desired, the text in the above example would ordinarily represent an intermediary state of the data prior to being transferred into a [[database management system]].

==References==
{{Commons category|Flat file models}}
{{reflist}}

{{Database models}}

{{DEFAULTSORT:Flat File Database}}
[[Category:Data management]]
[[Category:Computer file formats]]
[[Category:Database models]]

[[it:Flat file]]</text>
      <sha1>nawn915xvjwnc5p7ebcbi2483pe1eu1</sha1>
    </revision>
  </page>
  <page>
    <title>Modular serializability</title>
    <ns>0</ns>
    <id>24906307</id>
    <redirect title="Global serializability" />
    <revision>
      <id>608565392</id>
      <parentid>323027401</parentid>
      <timestamp>2014-05-14T16:38:06Z</timestamp>
      <contributor>
        <username>Christian75</username>
        <id>1306352</id>
      </contributor>
      <comment>remove text from redirect: "Modular serializability"</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="158" xml:space="preserve">#REDIRECT [[Global serializability]]

[[Category:Data management]]
[[Category:Databases]]
[[Category:Transaction processing]]
[[Category:Concurrency control]]</text>
      <sha1>er9lfkfzmrmxx4xmj3airzil7pjnala</sha1>
    </revision>
  </page>
  <page>
    <title>Data extraction</title>
    <ns>0</ns>
    <id>12097860</id>
    <revision>
      <id>758734714</id>
      <parentid>758615608</parentid>
      <timestamp>2017-01-07T06:46:30Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor />
      <comment>[[WP:CHECKWIKI]] error fix for #03.  Missing Reflist.  Do [[Wikipedia:GENFIXES|general fixes]] if a problem exists. -</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2747" xml:space="preserve">'''Data extraction''' is the act or process of retrieving [[data]] out of (usually [[unstructured data|unstructured]] or poorly structured) data sources for further [[data processing]] or [[data storage device|data storage]] ([[data migration]]). The [[data import|import]] into the intermediate extracting system is thus usually followed by [[data transformation]] and possibly the addition of [[metadata]] prior to [[data export|export]] to another stage in the data [[workflow]].&lt;ref&gt;[http://www.extractingdata.com Definition of data extraction.]&lt;/ref&gt;

Usually, the term data extraction is applied when ([[experiment]]al) data is first imported into a computer from primary sources, like [[measuring device|measuring]] or [[recording device]]s. Today's [[electronic device]]s will usually present an [[electrical connector]] (e.g. [[USB]]) through which '[[raw data]]' can be [[data stream|streamed]] into a [[personal computer]].

Typical unstructured data sources include web pages, emails, documents, PDFs, scanned text, mainframe reports, spool files, classifieds, etc. Which is further used for sales / marketing leads.&lt;ref&gt;[http://www.suntecdata.com/data-extraction-services.html Data Extraction Services] Retrieved, April 4, 2016&lt;/ref&gt;  Extracting data from these unstructured sources has grown into a considerable technical challenge where as historically data extraction has had to deal with changes in physical hardware formats, the majority of current data extraction deals with extracting data from these unstructured data sources, and from different software formats.  This growing process of data extraction&lt;ref&gt;[http://www.loginworks.com/blogs/web-scraping-blogs/209-web-data-extraction/ data extraction.]&lt;/ref&gt; from the web is referred to as [[Web scraping]].

The act of adding structure to unstructured data takes a number of forms
* Using text pattern matching such as [[regular expression]]s to identify small or large-scale structure e.g. records in a report and their associated data from headers and footers; 
* Using a table-based approach to identify common sections within a limited domain e.g. in emailed resumes, identifying skills, previous work experience, qualifications etc. using a standard set of commonly used headings (these would differ from language to language), e.g. Education might be found under Education/Qualification/Courses;
* Using text analytics to attempt to understand the text and link it to other information

==References==
{{Reflist}}

==External links==
* [http://www.etltools.org/extraction.html Data Extraction] as a part of the ETL process in a Data Warehousing environment

{{Data warehouse}}

{{DEFAULTSORT:Data Extraction}}
[[Category:Data management]]
[[Category:Data warehousing]]</text>
      <sha1>pbmvidmkf5dxmx9asixlo2jn3orykam</sha1>
    </revision>
  </page>
  <page>
    <title>Data integration</title>
    <ns>0</ns>
    <id>4780372</id>
    <revision>
      <id>760747007</id>
      <parentid>760746562</parentid>
      <timestamp>2017-01-18T21:10:22Z</timestamp>
      <contributor>
        <username>Sir mba</username>
        <id>11947451</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="26644" xml:space="preserve">'''Data integration''' involves combining [[data]] residing in different sources and providing users with a unified view of these data.&lt;ref name="refone"&gt;
{{cite conference | author=Maurizio Lenzerini | title=Data Integration:  A Theoretical Perspective | booktitle=PODS 2002 | year=2002 | pages=233–246 | url=http://www.dis.uniroma1.it/~lenzerin/homepagine/talks/TutorialPODS02.pdf}}&lt;/ref&gt; This process becomes significant in a variety of situations, which include both commercial (when two similar companies need to merge their [[database]]s) and scientific (combining research results from different [[bioinformatics]] repositories, for example) domains.  Data integration appears with increasing frequency as the volume and the need to share existing data [[Information explosion|explodes]].&lt;ref name="DataExplode"&gt;{{cite news | author=Frederick Lane | title=IDC: World Created 161 Billion Gigs of Data in 2006 | year=2006 | url=http://www.toptechnews.com/story.xhtml?story_id=01300000E3D0&amp;full_skip=1 }}&lt;/ref&gt;  It has become the focus of extensive theoretical work, and numerous open problems remain unsolved.

==History==
[[File:datawarehouse.png|thumb|right|Figure 1: Simple schematic for a data warehouse.  The [[Extract, transform, load|ETL]] process extracts information from the source databases, transforms it and then loads it into the data warehouse.]]

[[File:dataintegration.png|thumb|right|Figure 2: Simple schematic for a data-integration solution.  A system designer constructs a mediated schema against which users can run queries.  The [[virtual database]] interfaces with the source databases via [[Wrapper pattern|wrapper]] code if required.]]

Issues with combining [[heterogeneous]] data sources, often referred to as [[information silo]]s, under a single query interface have existed for some time. In the early 1980s, computer scientists began designing systems for interoperability of heterogeneous databases.&lt;ref&gt;{{cite news | author= John Miles Smith | title= Multibase: integrating heterogeneous distributed database systems | year=1982 | journal=AFIPS '81 Proceedings of the May 4–7, 1981, national computer conference  | pages= 487–499 |url=http://dl.acm.org/citation.cfm?id=1500483|display-authors=etal}}&lt;/ref&gt; The first data integration system driven by structured metadata was designed at the [[University of Minnesota]] in 1991, for the [[IPUMS|Integrated Public Use Microdata Series (IPUMS)]]. IPUMS used a [[data warehousing]] approach, which [[Extract, transform, load|extracts, transforms, and loads]] data from heterogeneous sources into a single view [[logical schema|schema]] so data from different sources become compatible.&lt;ref&gt;{{cite news | author= [[Steven Ruggles]], J. David Hacker, and Matthew Sobek | title= Order out of Chaos: The Integrated Public Use Microdata Series | year=1995 | journal=Historical Methods |volume=28 | pages= 33–39}}&lt;/ref&gt; By making thousands of population databases interoperable, IPUMS demonstrated the feasibility of large-scale data integration. The data warehouse approach  offers a [[Coupling (computer science)|tightly coupled]] architecture because the data are already physically reconciled in a single queryable repository, so it usually takes little time to resolve queries.&lt;ref&gt;{{cite news | author= Jennifer Widom | title= Research problems in data warehousing | year=1995 | journal=CIKM '95 Proceedings of the fourth international conference on information and knowledge management | pages= 25–30 | url=http://dl.acm.org/citation.cfm?id=221319}}&lt;/ref&gt;

The data warehouse approach is less feasible for datasets that are frequently updated, requiring the [[Extract, transform, load|ETL]] process to be continuously re-executed for synchronization.  Difficulties also arise in constructing data warehouses when one has only a query interface to summary data sources and no access to the full data.  This problem frequently emerges when integrating several commercial query services like travel or classified advertisement web applications.

{{As of | 2009}} the trend in data integration favored loosening the coupling between data{{Citation needed|date=June 2009}} and providing a unified query-interface to access real time data over a [[data mediation|mediated]] schema (see figure 2), which allows information to be retrieved directly from original databases. This is consistent with the [[Service-oriented architecture|SOA]] approach popular in that era. This approach relies on mappings between the mediated schema and the schema of original sources, and transform a query into specialized queries to match the schema of the original databases.  Such mappings can be specified in 2 ways : as a mapping from entities in the mediated schema to entities in the original sources (the "[[Global As View]]" (GAV) approach), or as a mapping from entities in the original sources to the mediated schema (the "[[Local As View]]" (LAV) approach).  The latter approach requires more sophisticated inferences to resolve a query on the mediated schema, but makes it easier to add new data sources to a (stable) mediated schema.

{{As of | 2010}} some of the work in data integration research concerns the [[semantic integration]] problem.  This problem addresses not the structuring of the architecture of the integration, but how to resolve [[semantic]] conflicts between heterogeneous data sources.  For example, if two companies merge their databases, certain concepts and definitions in their respective schemas like "earnings" inevitably have different meanings.  In one database it may mean profits in dollars (a floating-point number), while in the other it might represent the number of sales (an integer).  A common strategy for the resolution of such problems involves the use of [[ontology (computer science)|ontologies]] which explicitly define schema terms and thus help to resolve semantic conflicts. This approach represents [[ontology-based data integration]]. On the other hand, the problem of combining research results from different bioinformatics repositories requires bench-marking of the similarities, computed from different data sources, on a single criterion such as positive predictive value. This enables the data sources to be directly comparable and can be integrated even when the natures of experiments are distinct.&lt;ref&gt;{{cite journal| url=http://shubhrasankar.tripod.com/cgi-bin/combiningMultisourceIEEE.pdf  | journal=IEEE Transactions on Biomedical Engineering | title=Combining Multi-Source Information through Functional Annotation based Weighting: Gene Function Prediction in Yeast| author=Shubhra S. Ray| volume = 56 | pages=229–236 | pmid=19272921 | year=2009| issue=2 | doi=10.1109/TBME.2008.2005955|display-authors=etal}}&lt;/ref&gt;

{{As of | 2011}} it was determined that current [[data modeling]] methods were imparting data isolation into every [[data architecture]] in the form of islands of disparate data and information silos. This data isolation is an unintended artifact of the data modeling methodology that results in the development of disparate data models. Disparate data models, when instantiated as databases, form disparate databases. Enhanced data model methodologies have been developed to eliminate the data isolation artifact and to promote the development of integrated data models.&lt;ref&gt;{{cite news | author= Michael Mireku Kwakye | title= A Practical Approach To Merging Multidimensional Data Models | year=2011 | url=http://hdl.handle.net/10393/20457 }}&lt;/ref&gt;&lt;ref&gt;{{cite web | url=http://www.iri.com/pdf/RapidAce-Brochure.pdf  | title=Rapid Architectural Consolidation Engine&amp;nbsp;– The enterprise solution for disparate data models. | year=2011 }}&lt;/ref&gt; One enhanced data modeling method recasts data models by augmenting them with structural [[metadata]] in the form of standardized data entities.  As a result of recasting multiple data models, the set of recast data models will now share one or more commonality relationships that relate the structural metadata now common to these data models.  Commonality relationships are a peer-to-peer type of entity relationships that relate the standardized data entities of multiple data models.  Multiple data models that contain the same standard data entity may participate in the same commonality relationship.  When integrated data models are instantiated as databases and are properly populated from a common set of master data, then these databases are integrated.

Since 2011, [[data hub]] approaches have been of greater interest than fully structured (typically relational) Enterprise Data Warehouses. Since 2013, [[data lake]] approaches have risen to the level of Data Hubs. (See all three search terms popularity on Google Trends.&lt;ref&gt;{{cite web |title=Hub Lake and Warehouse search trends|url=https://www.google.com/trends/explore#q=enterprise%20data%20warehouse%2C%20%22data%20hub%22%2C%20%22data%20lake%22&amp;cmpt=q&amp;tz=Etc%2FGMT%2B5}}&lt;/ref&gt;) These approaches combine unstructured or varied data into one location, but do not necessarily require an (often complex) master relational schema to structure and define all data in the Hub.

==Example==
Consider a [[web application]] where a user can query a variety of information about cities (such as crime statistics, weather, hotels, demographics, etc.).  Traditionally, the information must be stored in a single database with a single schema.  But any single enterprise would find information of this breadth somewhat difficult and expensive to collect.  Even if the resources exist to gather the data, it would likely duplicate data in existing crime databases, weather websites, and census data.

A data-integration solution may address this problem by considering these external resources as [[materialized view]]s over a [[Virtual database|virtual mediated schema]], resulting in "virtual data integration".  This means application-developers construct a virtual schema—the ''mediated schema''—to best model the kinds of answers their users want.  Next, they design "wrappers" or adapters for each data source, such as the crime database and weather website.  These adapters simply transform the local query results (those returned by the respective websites or databases) into an easily processed form for the data integration solution (see figure 2).  When an application-user queries the mediated schema, the data-integration solution transforms this query into appropriate queries over the respective data sources.  Finally, the virtual database combines the results of these queries into the answer to the user's query.

This solution offers the convenience of adding new sources by simply constructing an adapter or an application software blade for them.  It contrasts with [[Extract, transform, load|ETL]] systems or with a single database solution, which require manual integration of entire new dataset into the system. The virtual ETL solutions leverage [[Virtual database|virtual mediated schema]] to implement data harmonization; whereby the data are copied from the designated "master" source to the defined targets, field by field. Advanced [[data virtualization]] is also built on the concept of object-oriented modeling in order to construct virtual mediated schema or virtual metadata repository, using [[hub and spoke]] architecture.

Each data source is disparate and as such is not designed to support reliable joins between data sources.  Therefore, data virtualization as well as data federation depends upon accidental data commonality to support combining data and information from disparate data sets.  Because of this lack of data value commonality across data sources, the return set may be inaccurate, incomplete, and impossible to validate.

One solution is to recast disparate databases to integrate these databases without the need for [[Extract, transform, load|ETL]]. The recast databases support commonality constraints where referential integrity may be enforced between databases.  The recast databases provide designed data access paths with data value commonality across databases.

==Theory==
The theory of data integration&lt;ref name="refone" /&gt; forms a subset of database theory and formalizes the underlying concepts of the problem in [[first-order logic]].  Applying the theories gives indications as to the feasibility and difficulty of data integration.  While its definitions may appear abstract, they have sufficient generality to accommodate all manner of integration systems,&lt;ref&gt;{{cite web|url=http://link.springer.com/chapter/10.1007/3-540-46093-4_14 |title=A Model Theory for Generic Schema Management}}&lt;/ref&gt; including those that include nested relational / XML databases &lt;ref&gt;{{cite web|url=http://www.vldb.org/conf/2006/p67-fuxman.pdf |title=Nested Mappings: Schema Mapping Reloaded }}&lt;/ref&gt; and those that treat databases as programs.&lt;ref&gt;{{cite web|url=http://homepages.inf.ed.ac.uk/dts/pub/psi.pdf |title=The Common Framework Initiative for algebraic specification and development of software}}&lt;/ref&gt;  Connections to particular databases systems such as Oracle or DB2 are provided by implementation-level technologies such as [[JDBC]] and are not studied at the theoretical level.

===Definitions===
Data integration systems are formally defined as a [[Triple (mathematics)|triple]] &lt;math&gt;\left \langle G,S,M\right \rangle&lt;/math&gt; where &lt;math&gt;G&lt;/math&gt; is the global (or mediated) schema, &lt;math&gt;S&lt;/math&gt; is the heterogeneous set of source schemas, and &lt;math&gt;M&lt;/math&gt; is the mapping that maps queries between the source and the global schemas.  Both &lt;math&gt;G&lt;/math&gt; and &lt;math&gt;S&lt;/math&gt; are expressed in [[formal language|languages]] over [[alphabet (computer science)|alphabets]] composed of symbols for each of their respective [[Relational database|relations]].  The [[Functional predicate|mapping]] &lt;math&gt;M&lt;/math&gt; consists of assertions between queries over &lt;math&gt;G&lt;/math&gt; and queries over &lt;math&gt;S&lt;/math&gt;.  When users pose queries over the data integration system, they pose queries over &lt;math&gt;G&lt;/math&gt; and the mapping then asserts connections between the elements in the global schema and the source schemas.

A database over a schema is defined as a set of sets, one for each relation (in a relational database).  The database corresponding to the source schema &lt;math&gt;S&lt;/math&gt; would comprise the set of sets of tuples for each of the heterogeneous data sources and is called the ''source database''.  Note that this single source database may actually represent a collection of disconnected databases.  The database corresponding to the virtual mediated schema &lt;math&gt;G&lt;/math&gt; is called the ''global database''.  The global database must satisfy the mapping &lt;math&gt;M&lt;/math&gt; with respect to the source database.  The legality of this mapping depends on the nature of the correspondence between &lt;math&gt;G&lt;/math&gt; and &lt;math&gt;S&lt;/math&gt;.  Two popular ways to model this correspondence exist: ''Global as View'' or GAV and ''Local as View'' or LAV.

[[File:GAVLAV.png|thumb|right|Figure 3: Illustration of tuple space of the GAV and LAV mappings.&lt;ref name="refseven"&gt;{{cite journal|author=Christoph Koch |title=Data Integration against Multiple Evolving Autonomous Schemata |year=2001 |url=http://www.csd.uoc.gr/~hy562/Papers/thesis_final.pdf |deadurl=yes |archiveurl=https://web.archive.org/web/20070926211342/http://www.csd.uoc.gr/~hy562/Papers/thesis_final.pdf |archivedate=2007-09-26 |df= }}&lt;/ref&gt; In GAV, the system is constrained to the set of tuples mapped by the mediators while the set of tuples expressible over the sources may be much larger and richer. In LAV, the system is constrained to the set of tuples in the sources while the set of tuples expressible over the global schema can be much larger. Therefore, LAV systems must often deal with incomplete answers.]]

GAV systems model the global database as a set of [[view (database)|views]] over &lt;math&gt;S&lt;/math&gt;.  In this case &lt;math&gt;M&lt;/math&gt; associates to each element of &lt;math&gt;G&lt;/math&gt; a query over &lt;math&gt;S&lt;/math&gt;.  [[Query optimizer|Query processing]] becomes a straightforward operation due to the well-defined associations between &lt;math&gt;G&lt;/math&gt; and &lt;math&gt;S&lt;/math&gt;.  The burden of complexity falls on implementing mediator code instructing the data integration system exactly how to retrieve elements from the source databases.  If any new sources join the system, considerable effort may be necessary to update the mediator, thus the GAV approach appears preferable when the sources seem unlikely to change.

In a GAV approach to the example data integration system above, the system designer would first develop mediators for each of the city information sources and then design the global schema around these mediators.  For example, consider if one of the sources served a weather website.  The designer would likely then add a corresponding element for weather to the global schema.  Then the bulk of effort concentrates on writing the proper mediator code that will transform predicates on weather into a query over the weather website.  This effort can become complex if some other source also relates to weather, because the designer may need to write code to properly combine the results from the two sources.

On the other hand, in LAV, the source database is modeled as a set of [[view (database)|views]] over &lt;math&gt;G&lt;/math&gt;.  In this case &lt;math&gt;M&lt;/math&gt; associates to each element of &lt;math&gt;S&lt;/math&gt; a query over &lt;math&gt;G&lt;/math&gt;.  Here the exact associations between &lt;math&gt;G&lt;/math&gt; and &lt;math&gt;S&lt;/math&gt; are no longer well-defined.  As is illustrated in the next section, the burden of determining how to retrieve elements from the sources is placed on the query processor.  The benefit of an LAV modeling is that new sources can be added with far less work than in a GAV system, thus the LAV approach should be favored in cases where the mediated schema is less stable or likely to change.&lt;ref name="refone" /&gt;

In an LAV approach to the example data integration system above, the system designer designs the global schema first and then simply inputs the schemas of the respective city information sources.  Consider again if one of the sources serves a weather website.  The designer would add corresponding elements for weather to the global schema only if none existed already.  Then programmers write an adapter or wrapper for the website and add a schema description of the website's results to the source schemas.  The complexity of adding the new source moves from the designer to the query processor.

===Query processing===
The theory of query processing in data integration systems is commonly expressed using conjunctive [[Database query language|queries]] and [[Datalog]], a purely declarative [[logic programming]] language.&lt;ref name="reffive"&gt;{{cite conference | author=[[Jeffrey D. Ullman]] | title=Information Integration Using Logical Views | booktitle=ICDT 1997 | year=1997 | pages=19–40 | url=http://www-db.stanford.edu/pub/papers/integration-using-views.ps}}&lt;/ref&gt;  One can loosely think of a [[conjunctive query]] as a logical function applied to the relations of a database such as "&lt;math&gt;f(A,B)&lt;/math&gt; where &lt;math&gt;A &lt; B&lt;/math&gt;".  If a tuple or set of tuples is substituted into the rule and satisfies it (makes it true), then we consider that tuple as part of the set of answers in the query.  While formal languages like Datalog express these queries concisely and without ambiguity, common [[SQL]] queries count as conjunctive queries as well.

In terms of data integration, "query containment" represents an important property of conjunctive queries.  A query &lt;math&gt;A&lt;/math&gt; contains another query &lt;math&gt;B&lt;/math&gt; (denoted &lt;math&gt;A \supset B&lt;/math&gt;) if the results of applying &lt;math&gt;B&lt;/math&gt; are a subset of the results of applying &lt;math&gt;A&lt;/math&gt; for any database.  The two queries are said to be equivalent if the resulting sets are equal for any database.  This is important because in both GAV and LAV systems, a user poses conjunctive queries over a ''virtual'' schema represented by a set of [[view (database)|views]], or "materialized" conjunctive queries.  Integration seeks to rewrite the queries represented by the views to make their results equivalent or maximally contained by our user's query.  This corresponds to the problem of answering queries using views ([[AQUV]]).&lt;ref name="refsix"&gt;{{cite conference | author=[[Alon Y. Halevy]] | title=Answering queries using views: A survey | booktitle=The VLDB Journal | year=2001 | pages=270–294 | url=http://www.cs.uwaterloo.ca/~david/cs740/answering-queries-using-views.pdf}}
&lt;/ref&gt;

In GAV systems, a system designer writes mediator code to define the query-rewriting.  Each element in the user's query corresponds to a substitution rule just as each element in the global schema corresponds to a query over the source.  Query processing simply expands the subgoals of the user's query according to the rule specified in the mediator and thus the resulting query is likely to be equivalent.  While the designer does the majority of the work beforehand, some GAV systems such as [http://www-db.stanford.edu/tsimmis/ Tsimmis] involve simplifying the mediator description process.

In LAV systems, queries undergo a more radical process of rewriting because no mediator exists to align the user's query with a simple expansion strategy.  The integration system must execute a search over the space of possible queries in order to find the best rewrite.  The resulting rewrite may not be an equivalent query but maximally contained, and the resulting tuples may be incomplete.  {{As of | 2009}} the MiniCon algorithm&lt;ref name="refsix" /&gt; is the leading query rewriting algorithm for LAV data integration systems.

In general, the complexity of query rewriting is [[NP-complete]].&lt;ref name="refsix" /&gt;  If the space of rewrites is relatively small this does not pose a problem—even for integration systems with hundreds of sources.

==Tools==
* Alteryx
* Analytics Canvas
* [[Capsenta]]'s Ultrawrap Platform
*[[Cloud Elements]] API Integration
* DataWatch
* [[Denodo|Denodo Platform]]
* [https://github.com/mhaghighat/dcaFuse Discriminant Correlation Analysis (DCA)]&lt;ref name="dca"&gt;M. Haghighat, M. Abdel-Mottaleb, &amp;  W. Alhalabi (2016). [http://ieeexplore.ieee.org/document/7470527/ Discriminant Correlation Analysis: Real-Time Feature Level Fusion for Multimodal Biometric Recognition]. IEEE Transactions on Information Forensics and Security, 11(9), 1984-1996.&lt;/ref&gt;
* [[elastic.io]] Integration Platform
* [http://www.hiperfabric.com HiperFabric]
* [[Lavastorm Analytics|Lavastorm]]
* [[Informatica]] Platform
* Oracle Data Integration Services
* ParseKit (enigma.io)
* Paxata
* [[RapidMiner]] Studio
* [[Red Hat]] JBoss Data Virtualization. Community project: teiid.
* [[Microsoft Azure|Azure]] Data Factory (ADF) 
* [[SQL Server Integration Services|SQL Server Integration Services (SSIS)]]
* [http://www.tmmdata.com TMMData]
* [http://www.dataladder.com Data Ladder]

==In the life sciences==
Large-scale questions in science, such as [[global warming]], [[invasive species]] spread, and [[resource depletion]], are increasingly requiring the collection of disparate data sets for [[meta-analysis]]. This type of data integration is especially challenging for ecological and environmental data because [[metadata standards]] are not agreed upon and there are many different data types produced in these fields. [[National Science Foundation]] initiatives such as [[Datanet]] are intended to make data integration easier for scientists by providing [[cyberinfrastructure]] and setting standards. The five funded [[Datanet]] initiatives are [[DataONE]],&lt;ref&gt;{{cite web|author=William Michener |url=https://www.dataone.org |title=DataONE: Observation Network for Earth |publisher=www.dataone.org | accessdate=2013-01-19|display-authors=etal}}&lt;/ref&gt; led by William Michener at the [[University of New Mexico]]; The Data Conservancy,&lt;ref&gt;{{cite web|author=Sayeed Choudhury |url=https://dataconservancy.org |title=Data Conservancy |publisher=dataconservancy.org | accessdate=2013-01-19|display-authors=etal}}&lt;/ref&gt; led by Sayeed Choudhury of [[Johns Hopkins University]]; SEAD: Sustainable Environment through Actionable Data,&lt;ref&gt;{{cite web|author=[[Margaret Hedstrom]] |url=http://sead-data.net/ |title=SEAD Sustainable Environment - Actionable Data |publisher=sead-data.net | accessdate=2013-01-19|display-authors=etal}}&lt;/ref&gt; led by [[Margaret Hedstrom]] of the [[University of Michigan]]; the DataNet Federation Consortium,&lt;ref&gt;{{cite web|author=[[Reagan Moore]] |url=http://datafed.org/ |title=DataNet Federation Consortium |publisher=datafed.org | accessdate=2013-01-19|display-authors=etal}}&lt;/ref&gt; led by Reagan Moore of the [[University of North Carolina]]; and ''Terra Populus'',&lt;ref&gt;{{cite web|author=[[Steven Ruggles]] |url=http://www.terrapop.org/ |title=Terra Populus: Integrated Data on Population and the Environment |publisher=terrapop.org | accessdate=2013-01-19|display-authors=etal}}&lt;/ref&gt; led by [[Steven Ruggles]] of the [[University of Minnesota]].  The [[Research Data Alliance]],&lt;ref&gt;{{cite web|author=[[Bill Nichols]] |url=http://rd-alliance.org/ |title=Research Data Alliance |publisher=rd-alliance.org | accessdate=2014-10-01}}&lt;/ref&gt; has more recently explored creating global data integration frameworks. The [[OpenPHACTS]] project, funded through the [[European Union]] [[Innovative Medicines Initiative]], built a drug discovery platform by linking datasets from providers such as [[European Bioinformatics Institute]], [[Royal Society of Chemistry]], [[UniProt]], [[WikiPathways]] and [[DrugBank]].

==See also==
{{div col||20em}}
* [[Business semantics management]]
* [[Core data integration]]
* [[Customer data integration]]
* [[Data curation]]
* [[Data fusion]]
* [[Data mapping]]
* [[Data wrangling]]
* [[Database model]]
* [[Dataspaces]]
* [[Edge data integration]]
* [[Enterprise application integration]]
* [[Enterprise architecture framework]]
* [[Enterprise information integration]] (EII)
* [[Enterprise integration]]
* [[Geodi]]: Geoscientific Data Integration
* [[Information integration]]
* [[Information server]]
* [[Information silo]]
* [[Integration Competency Center]]
* [[Integration Consortium]]
* [[JXTA]]
* [[Master data management]]
* [[Object-relational mapping]]
* [[Open Text]]
* [[Schema matching]]
* [[Three schema approach]]
* [[UDEF]]
* [[Web service]]
{{div col end}}

==External links==
* [https://github.com/mhaghighat/dcaFuse Discriminant Correlation Analysis (DCA)]&lt;ref name="dca"&gt;&lt;/ref&gt;

==References==
{{reflist|30em}}

{{data}}

{{DEFAULTSORT:Data Integration}}
[[Category:Data management]]</text>
      <sha1>s2vh7r8mqm7qwlbg4vjjb3hb5spw7te</sha1>
    </revision>
  </page>
  <page>
    <title>Data center</title>
    <ns>0</ns>
    <id>579730</id>
    <revision>
      <id>762095565</id>
      <parentid>759809129</parentid>
      <timestamp>2017-01-26T16:40:40Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor />
      <comment>Reformat 1 archive link. [[User:Green Cardamom/WaybackMedic_2.1|Wayback Medic 2.1]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="70494" xml:space="preserve">{{Refimprove|date=July 2015}}
[[File:NetworkOperations.jpg|thumb|right|An operation engineer overseeing a network operations control room of a data center]]

A '''data center''' is a facility used to house computer systems and associated components, such as [[telecommunication]]s and [[computer data storage|storage systems]]. It generally includes redundant or backup [[power supply|power supplies]], redundant data communications connections, environmental controls (e.g., air conditioning, fire suppression) and various security devices. Large data centers are industrial scale operations using as much electricity as a small town.&lt;ref name=NYT92212&gt;{{cite news|title=Power, Pollution and the Internet|url=http://www.nytimes.com/2012/09/23/technology/data-centers-waste-vast-amounts-of-energy-belying-industry-image.html|accessdate=2012-09-25|newspaper=The New York Times|date=September 22, 2012|author=James Glanz}}&lt;/ref&gt;&lt;ref name="ReferenceDC2"&gt;"[http://www.academia.edu/6982393/Power_Management_Techniques_for_Data_Centers_A_Survey Power Management Techniques for Data Centers: A Survey]", 2014.&lt;/ref&gt;

==History==
[[File:Indiana University Data Center - P1100134.JPG|thumb|[[Indiana University]] Data Center. [[Bloomington, Indiana]]]]
{{Copypaste|section|url=http://www.rackspace.com/blog/datacenter-evolution-1960-to-2000/|date=August 2014}}
{{Unreferenced section|date=August 2014}}
Data centers have their roots in the huge computer rooms of the early ages{{when|date=September 2015}} of the computing industry. Early computer systems, complex to operate and maintain, required a special environment in which to operate. Many cables were necessary to connect all the components, and methods to accommodate and organize these were devised such as standard [[19-inch rack|racks]] to mount equipment, [[raised floor]]s, and [[cable tray]]s (installed overhead or under the elevated floor). A single mainframe required a great deal of power, and had to be cooled to avoid overheating. Security became important&amp;nbsp;– computers were expensive, and were often used for [[military]] purposes. Basic design-guidelines for controlling access to the computer room were therefore devised.

During the boom of the microcomputer industry, and especially during the 1980s, users started to deploy computers everywhere, in many cases with little or no care about operating requirements. However, as information technology (IT) operations started to grow in complexity, organizations grew aware of the need to control IT resources. The advent of [[Unix]] from the early 1970s led to the subsequent proliferation of freely available [[Linux]]-compatible [[personal computer|PC]] operating-systems during the 1990s. These were called "[[Server (computing)|servers]]", as [[timesharing]] operating systems like Unix rely heavily on the [[client-server model]] to facilitate sharing unique resources between multiple users. The availability of inexpensive [[Networking hardware|networking]] equipment, coupled with new standards for network [[structured cabling]], made it possible to use a hierarchical design that put the servers in a specific room inside the company. The use of the term "data center", as applied to specially designed computer rooms, started to gain popular recognition about this time.{{citation needed|date=September 2015}}

The boom of data centers came during the [[dot-com bubble]] of 1997–2000. [[Company|Companies]] needed fast Internet connectivity and non-stop operation to deploy systems and to establish a presence on the Internet. Installing such equipment was not viable for many smaller companies. Many companies started building very large facilities, called '''Internet data centers''' (IDCs), which provide [[customer|commercial client]]s with a range of solutions for systems deployment and operation. New technologies and practices were designed to handle the scale and the operational requirements of such large-scale operations. These practices eventually migrated toward the private data centers, and were adopted largely because of their practical results. Data centers for cloud computing are called '''cloud data centers''' (CDCs). But nowadays, the division of these terms has almost disappeared and they are being integrated into a term "data center".

With an increase in the uptake of [[cloud computing]], business and government organizations scrutinize data centers to a higher degree in areas such as security, availability, environmental impact and adherence to standards. Standards documents from accredited [[professional]] groups, such as the [[Telecommunications Industry Association]], specify the requirements for data-center design. Well-known operational metrics for [[data availability|data-center availability]] can serve to evaluate the [[Business Impact Analysis|commercial impact]] of a disruption. Development continues in operational practice, and also in environmentally-friendly data-center design. Data centers typically cost a lot to build and to maintain.{{citation needed|date=September 2015}}

==Requirements for modern data centers==
[[File:Datacenter-telecom.jpg|thumb|left|Racks of telecommunications equipment in part of a data center]]
{{Copypaste|section|url=https://global.ihs.com/doc_detail.cfm?&amp;rid=TIA&amp;input_doc_number=TIA-942&amp;item_s_key=00414811&amp;item_key_date=860905&amp;input_doc_number=TIA-942&amp;input_doc_title=#abstract|date=August 2014}}
IT operations are a crucial aspect of most organizational operations around the world. One of the main concerns is '''business continuity'''; companies rely on their information systems to run their operations. If a system becomes unavailable, company operations may be impaired or stopped completely. It is necessary to provide a reliable infrastructure for IT operations, in order to minimize any chance of disruption. Information security is also a concern, and for this reason a data center has to offer a secure environment which minimizes the chances of a security breach. A data center must therefore keep high standards for assuring the integrity and functionality of its hosted computer environment. This is accomplished through redundancy of mechanical cooling and power systems (including emergency backup power generators)serving the data center along with fiber optic cables.

The [[Telecommunications Industry Association]]'s Telecommunications Infrastructure Standard for Data Centers&lt;ref&gt;[http://www.tia-942.org TIA-942 Telecommunications Infrastructure Standard for Data Centers]&lt;/ref&gt; specifies the minimum requirements for telecommunications infrastructure of data centers and computer rooms including single tenant enterprise data centers and multi-tenant Internet hosting data centers. The topology proposed in this document is intended to be applicable to any size data center.&lt;ref&gt;{{cite web|url=http://www.tiaonline.org/standards/ |title=Archived copy |accessdate=2011-11-07 |deadurl=yes |archiveurl=https://web.archive.org/web/20111106042758/http://www.tiaonline.org/standards/ |archivedate=2011-11-06 |df= }}&lt;/ref&gt;

Telcordia GR-3160, ''NEBS Requirements for Telecommunications Data Center Equipment and Spaces'',&lt;ref&gt;[http://telecom-info.telcordia.com/site-cgi/ido/docs.cgi?ID=SEARCH&amp;DOCUMENT=GR-3160&amp; GR-3160, NEBS Requirements for Telecommunications Data Center Equipment and Spaces]&lt;/ref&gt; provides guidelines for data center spaces within telecommunications networks, and environmental requirements for the equipment intended for installation in those spaces. These criteria were developed jointly by Telcordia and industry representatives. They may be applied to data center spaces housing data processing or Information Technology (IT) equipment. The equipment may be used to:
* Operate and manage a carrier's telecommunication network
* Provide data center based applications directly to the carrier's customers
* Provide hosted applications for a third party to provide services to their customers
* Provide a combination of these and similar data center applications

Effective data center operation requires a balanced investment in both the facility and the housed equipment. The first step is to establish a baseline facility environment suitable for equipment installation. Standardization and modularity can yield savings and efficiencies in the design and construction of telecommunications data centers.

Standardization means integrated building and equipment engineering. Modularity has the benefits of scalability and easier growth, even when planning forecasts are less than optimal. For these reasons, telecommunications data centers should be planned in repetitive building blocks of equipment, and associated power and support (conditioning) equipment when practical. The use of dedicated centralized systems requires more accurate forecasts of future needs to prevent expensive over construction, or perhaps worse&amp;nbsp;— under construction that fails to meet future needs.

The "lights-out" data center, also known as a darkened or a [[dark data]] center, is a data center that, ideally, has all but eliminated the need for direct access by personnel, except under extraordinary circumstances. Because of the lack of need for staff to enter the data center, it can be operated without lighting. All of the devices are accessed and managed by remote systems, with automation programs used to perform unattended operations. In addition to the energy savings, reduction in staffing costs and the ability to locate the site further from population centers, implementing a lights-out data center reduces the threat of malicious attacks upon the infrastructure.&lt;ref&gt;{{cite book | first=Victor | last=Kasacavage | year=2002 | page=227 | title=Complete book of remote access: connectivity and security | series=The Auerbach Best Practices Series | publisher=CRC Press | isbn=0-8493-1253-1
}}&lt;/ref&gt;&lt;ref&gt;{{cite book |author1=Burkey, Roxanne E. |author2=Breakfield, Charles V. | year=2000 | title=Designing a total data solution: technology, implementation and deployment | page=24 | series=Auerbach Best Practices | publisher=CRC Press | isbn=0-8493-0893-3 }}&lt;/ref&gt;

There is a trend to modernize data centers in order to take advantage of the performance and energy efficiency increases of newer IT equipment and capabilities, such as [[cloud computing]]. This process is also known as data center transformation.&lt;ref name="mspmentor.net"&gt;Mukhar, Nicholas. "HP Updates Data Center Transformation Solutions," August 17, 2011 [http://www.mspmentor.net/2011/08/17/hp-updates-data-transformation-solutions/]&lt;/ref&gt;

Organizations are experiencing rapid IT growth but their data centers are aging. Industry research company [[International Data Corporation]] (IDC) puts the average age of a data center at nine years old.&lt;ref name="mspmentor.net"/&gt; [[Gartner]], another research company says data centers older than seven years are obsolete.&lt;ref&gt;{{cite web|url=http://www.forbes.com/2010/03/12/cloud-computing-ibm-technology-cio-network-data-centers.html |title=Sperling, Ed. "Next-Generation Data Centers," Forbes, March 15. 2010 |publisher=Forbes.com |date= |accessdate=2013-08-30}}&lt;/ref&gt;

In May 2011, data center research organization [[Uptime Institute]] reported that 36 percent of the large companies it surveyed expect to exhaust IT capacity within the next 18 months.&lt;ref&gt;Niccolai, James. "Data Centers Turn to Outsourcing to Meet Capacity Needs," CIO.com, May 10, 2011 [http://www.cio.com/article/681897/Data_Centers_Turn_to_Outsourcing_to_Meet_Capacity_Needs]&lt;/ref&gt;

Data center transformation takes a step-by-step approach through integrated projects carried out over time. This differs from a traditional method of data center upgrades that takes a serial and siloed approach.&lt;ref&gt;Tang, Helen. "Three Signs it's time to transform your data center," August 3, 2010, Data Center Knowledge [http://www.datacenterknowledge.com/archives/2010/08/03/three-signs-it%E2%80%99s-time-to-transform-your-data-center/]&lt;/ref&gt; The typical projects within a data center transformation initiative include standardization/consolidation, virtualization, [[automation]] and security.
* Standardization/consolidation: The purpose of this project is to reduce the number of data centers a large organization may have. This project also helps to reduce the number of hardware, software platforms, tools and processes within a data center. Organizations replace aging data center equipment with newer ones that provide increased capacity and performance. Computing, networking and management platforms are standardized so they are easier to manage.&lt;ref name="datacenterknowledge.com"&gt;Miller, Rich. "Complexity: Growing Data Center Challenge," Data Center Knowledge, May 16, 2007
[http://www.datacenterknowledge.com/archives/2007/05/16/complexity-growing-data-center-challenge/]&lt;/ref&gt;
* Virtualize: There is a trend to use IT virtualization technologies to replace or consolidate multiple data center equipment, such as servers. Virtualization helps to lower capital and operational expenses,&lt;ref&gt;Sims, David. "Carousel's Expert Walks Through Major Benefits of Virtualization," TMC Net, July 6, 2010
[http://virtualization.tmcnet.com/topics/virtualization/articles/193652-carousels-expert-walks-through-major-benefits-virtualization.htm]&lt;/ref&gt; and reduce energy consumption.&lt;ref&gt;Delahunty, Stephen. "The New urgency for Server Virtualization," InformationWeek, August 15, 2011. [http://www.informationweek.com/news/government/enterprise-architecture/231300585]&lt;/ref&gt; Virtualization technologies are also used to create virtual desktops, which can then be hosted in data centers and rented out on a subscription basis.&lt;ref&gt;{{cite web|title=HVD: the cloud's silver lining|url=http://www.intrinsictechnology.co.uk/FileUploads/HVD_Whitepaper.pdf|publisher=Intrinsic Technology|accessdate=2012-08-30}}&lt;/ref&gt;  Data released by investment bank Lazard Capital Markets reports that 48 percent of enterprise operations will be virtualized by 2012. Gartner views virtualization as a catalyst for modernization.&lt;ref&gt;Miller, Rich. "Gartner: Virtualization Disrupts Server Vendors," Data Center Knowledge, December 2, 2008 [http://www.datacenterknowledge.com/archives/2008/12/02/gartner-virtualization-disrupts-server-vendors/]&lt;/ref&gt;
* Automating: Data center automation involves automating tasks such as [[provisioning]], configuration, [[Patch (computing)|patching]], release management and compliance. As enterprises suffer from few skilled IT workers,&lt;ref name="datacenterknowledge.com"/&gt; automating tasks make data centers run more efficiently.
* Securing: In modern data centers, the security of data on virtual systems is integrated with existing security of physical infrastructures.&lt;ref&gt;Ritter, Ted. Nemertes Research, "Securing the Data-Center Transformation Aligning Security and Data-Center Dynamics," [http://lippisreport.com/2011/05/securing-the-data-center-transformation-aligning-security-and-data-center-dynamics/]&lt;/ref&gt; The security of a modern data center must take into account physical security, network security, and data and user security.

==Carrier neutrality==
Today many data centers are run by [[Internet service provider]]s solely for the purpose of hosting their own and third party [[Server (computing)|servers]].

However traditionally data centers were either built for the sole use of one large company, or as [[carrier hotel]]s or [[Network-neutral data center]]s.

These facilities enable interconnection of carriers and act as regional fiber hubs serving local business in addition to hosting content [[Server (computing)|servers]].

==Data center tiers==
&lt;!-- linked from [[data availability]] --&gt;
The [[Telecommunications Industry Association]] is a trade association accredited by ANSI (American National Standards Institute). In 2005 it published [http://global.ihs.com/doc_detail.cfm?currency_code=USD&amp;customer_id=2125452B2C0A&amp;oshid=2125452B2C0A&amp;shopping_cart_id=292558332C4A2020495A4D3B200A&amp;country_code=US&amp;lang_code=ENGL&amp;item_s_key=00414811&amp;item_key_date=940819&amp;input_doc_number=TIA-942&amp;input_doc_title= ANSI/TIA-942], Telecommunications Infrastructure Standard for Data Centers, which defined four levels (called tiers) of data centers in a thorough, quantifiable manner. TIA-942 was amended in 2008 and again in 2010. ''TIA-942:Data Center Standards Overview'' describes the requirements for the data center infrastructure. The simplest is a Tier 1 data center, which is basically a [[server room]], following basic guidelines for the installation of computer systems. The most stringent level is a Tier 4 data center, which is designed to host mission critical computer systems, with fully redundant subsystems and compartmentalized security zones controlled by [[biometric]] access controls methods. Another consideration is the placement of the data center in a subterranean context, for data security as well as environmental considerations such as cooling requirements.&lt;ref&gt;A ConnectKentucky article mentioning Stone Mountain Data Center Complex {{cite web|title=Global Data Corp. to Use Old Mine for Ultra-Secure Data Storage Facility|url=http://connectkentucky.org/_documents/connected_fall_FINAL.pdf|format=PDF|publisher=ConnectKentucky|accessdate=2007-11-01|date=2007-11-01}}&lt;/ref&gt;

The German Datacenter star audit program uses an auditing process to certify 5 levels of "gratification" that affect Data Center criticality.

Independent from the ANSI/TIA-942 standard, the [[Uptime Institute]], a think tank and professional-services organization based in [[Santa Fe, New Mexico|Santa Fe]], [[New Mexico]], has defined its own four levels. The levels describe the availability of data from the hardware at a location. The higher the tier, the greater the availability. The levels are:
&lt;ref&gt;A document from the Uptime Institute describing the different tiers (click through the download page) {{cite web
 |title=Data Center Site Infrastructure Tier Standard: Topology 
 |url=http://uptimeinstitute.org/index.php?option=com_docman&amp;task=doc_download&amp;gid=82 
 |format=PDF 
 |publisher=Uptime Institute 
 |accessdate=2010-02-13 
 |date=2010-02-13 
 |deadurl=yes 
 |archiveurl=https://web.archive.org/web/20100613072610/http://uptimeinstitute.org/index.php?option=com_docman&amp;task=doc_download&amp;gid=82 
 |archivedate=2010-06-13 
 |df= 
}}&lt;/ref&gt;
&lt;ref&gt;The rating guidelines from the Uptime Institute {{cite web
 |title=Data Center Site Infrastructure Tier Standard: Topology 
 |url=http://professionalservices.uptimeinstitute.com/UIPS_PDF/TierStandard.pdf 
 |format=PDF 
 |publisher=Uptime Institute 
 |accessdate=2010-02-13 
 |date=2010-02-13 
 |deadurl=yes 
 |archiveurl=https://web.archive.org/web/20091007121511/http://professionalservices.uptimeinstitute.com:80/UIPS_PDF/TierStandard.pdf 
 |archivedate=2009-10-07 
 |df= 
}}&lt;/ref&gt;

{| class="wikitable"
|-
! Tier Level
! Requirements
|-
! 1
|
* Single non-redundant distribution path serving the IT equipment
* Non-redundant capacity components
* Basic site infrastructure with expected availability of 99.671%
|-
! 2
|
* Meets or exceeds all Tier 1 requirements
* Redundant site infrastructure capacity components with expected availability of 99.741%
|-
! 3
|
* Meets or exceeds all Tier 2 requirements
* Multiple independent distribution paths serving the IT equipment
* All IT equipment must be dual-powered and fully compatible with the topology of a site's architecture
* Concurrently maintainable site infrastructure with expected availability of 99.982%
|-
! 4
|
* Meets or exceeds all Tier 3 requirements
* All cooling equipment is independently dual-powered, including chillers and heating, ventilating and air-conditioning (HVAC) systems
* Fault-tolerant site infrastructure with electrical power storage and distribution facilities with expected availability of 99.995%
|}

The difference between 99.671%, 99.741%, 99.982%, and 99.995%, while seemingly nominal, could be significant depending on the application.

Whilst no down-time is ideal, the tier system allows for unavailability of services as listed below over a period of one year (525,600 minutes):
* Tier 1 (99.671%) status would allow 1729.224 minutes or 28.817 hours
* Tier 2 (99.741%) status would allow 1361.304 minutes or 22.688 hours
* Tier 3 (99.982%) status would allow 94.608 minutes or 1.5768 hours
* Tier 4 (99.995%) status would allow 26.28 minutes or 0.438 hours

The Uptime Institute also classifies the tiers in different categories: design documents, constructed facility, operational sustainability&lt;ref name="uptimeinstitute"&gt;{{cite web|url=http://uptimeinstitute.com/TierCertification/|title=Uptime Institute - Tier Certification|publisher=uptimeinstitute.com|accessdate=2014-08-27}}&lt;/ref&gt;

==Design considerations==
[[File:Rack001.jpg|thumb|right|A typical server rack, commonly seen in [[colocation center|colocation]]]]
A data center can occupy one room of a building, one or more floors, or an entire building. Most of the equipment is often in the form of servers mounted in [[19 inch rack]] cabinets, which are usually placed in single rows forming corridors (so-called aisles) between them. This allows people access to the front and rear of each cabinet. Servers differ greatly in size from [[Rack unit|1U servers]] to large freestanding storage silos which occupy many square feet of floor space. Some equipment such as [[mainframe computer]]s and [[computer storage|storage]] devices are often as big as the racks themselves, and are placed alongside them. Very large data centers may use [[intermodal container|shipping containers]] packed with 1,000 or more servers each;&lt;ref&gt;{{cite web|url=https://www.youtube.com/watch?v=zRwPSFpLX8I|title=Google Container Datacenter Tour (video)}}&lt;/ref&gt; when repairs or upgrades are needed, whole containers are replaced (rather than repairing individual servers).&lt;ref&gt;{{cite web| title=Walking the talk: Microsoft builds first major container-based data center| url=http://www.computerworld.com/action/article.do?command=viewArticleBasic&amp;articleId=9075519| archiveurl=https://web.archive.org/web/20080612193106/http://www.computerworld.com/action/article.do?command=viewArticleBasic&amp;articleId=9075519| archivedate=2008-06-12| accessdate=2008-09-22}}&lt;/ref&gt;

Local building codes may govern the minimum ceiling heights.

===Design programming===
Design programming, also known as architectural programming, is the process of researching and making decisions to identify the scope of a design project.&lt;ref&gt;Cherry, Edith. "Architectural Programming: Introduction", Whole Building Design Guide, Sept. 2, 2009&lt;/ref&gt; Other than the architecture of the building itself there are three elements to design programming for data centers: facility topology design (space planning), engineering infrastructure design (mechanical systems such as cooling and electrical systems including power) and technology infrastructure design (cable plant). Each will be influenced by performance assessments and modelling to identify gaps pertaining to the owner's performance wishes of the facility over time.

Various vendors who provide data center design services define the steps of data center design slightly differently, but all address the same basic aspects as given below.

===Modeling criteria===
Modeling criteria are used to develop future-state scenarios for space, power, cooling, and costs in the data center.&lt;ref&gt;Mullins, Robert. "Romonet Offers Predictive Modelling Tool For Data Center Planning", Network Computing, June 29, 2011 [http://www.networkcomputing.com/data-center/231000669]&lt;/ref&gt; The aim is to create a master plan with parameters such as number, size, location, topology, IT floor system layouts, and power and cooling technology and configurations. The purpose of this is to allow for efficient use of the existing mechanical and electrical systems and also growth in the existing data center without the need for developing new buildings and further upgrading of incoming power supply.

===Design recommendations===
Design recommendations/plans generally follow the modelling criteria phase. The optimal technology infrastructure is identified and planning criteria are developed, such as critical power capacities, overall data center power requirements using an agreed upon PUE (power utilization efficiency), mechanical cooling capacities, kilowatts per cabinet, raised floor space, and the resiliency level for the facility.

===Conceptual design===
Conceptual designs embody the design recommendations or plans and should take into account "what-if" scenarios to ensure all operational outcomes are met in order to future-proof the facility. Conceptual floor layouts should be driven by IT performance requirements as well as lifecycle costs associated with IT demand, energy efficiency, cost efficiency and availability. Future-proofing will also include expansion capabilities, often provided in modern data centers through modular designs.  These allow for more raised floor space to be fitted out in the data center whilst utilising the existing major electrical plant of the facility.

===Detailed design===
Detailed design is undertaken once the appropriate conceptual design is determined, typically including a proof of concept. The detailed design phase should include the detailed architectural, structural, mechanical and electrical information and specification of the facility.  At this stage development of facility schematics and construction documents as well as schematics and performance specification and specific detailing of all technology infrastructure, detailed IT infrastructure design and IT infrastructure documentation are produced.

===Mechanical engineering infrastructure designs===
[[File:CRAC Cabinets 2.jpg|thumb|CRAC Air Handler]]
Mechanical engineering infrastructure design addresses mechanical systems involved in maintaining the interior environment of a data center, such as heating, ventilation and air conditioning (HVAC); humidification and dehumidification equipment; pressurization; and so on.&lt;ref name="nxtbook.com"&gt;Jew, Jonathan. "BICSI Data Center Standard: A Resource for Today's Data Center Operators and Designers," BICSI News Magazine, May/June 2010, page 28. [http://www.nxtbook.com/nxtbooks/bicsi/news_20100506/#/26]&lt;/ref&gt;
This stage of the design process should be aimed at saving space and costs, while ensuring business and reliability objectives are met as well as achieving PUE and green requirements.&lt;ref&gt;Data Center Energy Management: Best Practices Checklist: Mechanical, Lawrence Berkeley National Laboratory [http://hightech.lbl.gov/dctraining/strategies/mam.html]&lt;/ref&gt; Modern designs include modularizing and scaling IT loads, and making sure capital spending on the building construction is optimized.

===Electrical engineering infrastructure design===
Electrical Engineering infrastructure design is focused on designing electrical configurations that accommodate various reliability requirements and data center sizes. Aspects may include utility service planning; distribution, switching and bypass from power sources; uninterruptable power source (UPS) systems; and more.&lt;ref name="nxtbook.com"/&gt;

These designs should dovetail to energy standards and best practices while also meeting business objectives. Electrical configurations should be optimized and operationally compatible with the data center user's capabilities. Modern electrical design is modular and scalable,&lt;ref&gt;Clark, Jeff. "Hedging Your Data Center Power", The Data Center Journal, Oct. 5, 2011. [http://www.datacenterjournal.com/design/hedging-your-data-center-power/]&lt;/ref&gt; and is available for low and medium voltage requirements as well as DC (direct current).

===Technology infrastructure design===
[[File:Under Floor Cable Runs Tee.jpg|thumb|Under Floor Cable Runs]]
Technology infrastructure design addresses the telecommunications cabling systems that run throughout data centers. There are cabling systems for all data center environments, including horizontal cabling, voice, modem, and facsimile telecommunications services, premises switching equipment, computer and telecommunications management connections, keyboard/video/mouse connections and data communications.&lt;ref&gt;Jew, Jonathan. "BICSI Data Center Standard: A Resource for Today's Data Center Operators and Designers," BICSI News Magazine, May/June 2010, page 30. [http://www.nxtbook.com/nxtbooks/bicsi/news_20100506/#/26]&lt;/ref&gt; Wide area, local area, and storage area networks should link with other building signaling systems (e.g. fire, security, power, HVAC, EMS).

===Availability expectations===
The higher the availability needs of a data center, the higher the capital and operational costs of building and managing it. Business needs should dictate the level of availability required and should be evaluated based on characterization of the criticality of IT systems estimated cost analyses from modeled scenarios. In other words, how can an appropriate level of availability best be met by design criteria to avoid financial and operational risks as a result of downtime?
If the estimated cost of downtime within a specified time unit exceeds the amortized capital costs and operational expenses, a higher level of availability should be factored into the data center design. If the cost of avoiding downtime greatly exceeds the cost of downtime itself, a lower level of availability should be factored into the design.&lt;ref&gt;Clark, Jeffrey. "The Price of Data Center Availability—How much availability do you need?", Oct. 12, 2011, The Data Center Journal [http://www.datacenterjournal.com/home/news/languages/item/2792-the-price-of-data-center-availability]&lt;/ref&gt;

===Site selection===
Aspects such as proximity to available power grids, telecommunications infrastructure, networking services, transportation lines and emergency services can affect costs, risk, security and other factors to be taken into consideration for data center design.   Whilst a wide array of location factors are taken into account (e.g. flight paths, neighbouring uses, geological risks) access to suitable available power is often the longest lead time item. Location affects data center design also because the climatic conditions dictate what cooling technologies should be deployed. In turn this impacts uptime and the costs associated with cooling.&lt;ref&gt;Tucci, Linda. "Five tips on selecting a data center location", May 7, 2008, SearchCIO.com [http://searchcio.techtarget.com/news/1312614/Five-tips-on-selecting-a-data-center-location]&lt;/ref&gt; For example, the topology and the cost of managing a data center in a warm, humid climate will vary greatly from managing one in a cool, dry climate.

===Modularity and flexibility===
[[File:Cabinet Asile.jpg|thumb|Cabinet aisle in a data center]]
{{main article|Modular data center}}

Modularity and flexibility are key elements in allowing for a data center to grow and change over time. Data center modules are pre-engineered, standardized building blocks that can be easily configured and moved as needed.&lt;ref&gt;Niles, Susan. "Standardization and Modularity in Data Center Physical Infrastructure," 2011, Schneider Electric, page 4. [http://www.apcmedia.com/salestools/VAVR-626VPD_R1_EN.pdf]&lt;/ref&gt;

A modular data center may consist of data center equipment contained within shipping containers or similar portable containers.&lt;ref&gt;Pitchaikani, Bala. "Strategies for the Containerized Data Center," DataCenterKnowledge.com, Sept. 8, 2011. [http://www.datacenterknowledge.com/archives/2011/09/08/strategies-for-the-containerized-data-center/]&lt;/ref&gt; But it can also be described as a design style in which components of the data center are prefabricated and standardized so that they can be constructed, moved or added to quickly as needs change.&lt;ref&gt;Niccolai, James. "HP says prefab data center cuts costs in half," InfoWorld, July 27, 2010. [http://www.infoworld.com/d/green-it/hp-says-prefab-data-center-cuts-costs-in-half-837?page=0,0]&lt;/ref&gt;

===Environmental control===
{{main article|Data center environmental control}}
The physical environment of a data center is rigorously controlled.
[[Air conditioning]] is used to control the temperature and humidity in the data center. [[ASHRAE]]'s "Thermal Guidelines for Data Processing Environments"&lt;ref&gt;{{cite book|title=Thermal Guidelines for Data Processing Environments|year=2012|publisher=American Society of Heating, Refrigerating and Air-Conditioning Engineers|isbn=978-1936504-33-6|author=ASHRAE Technical Committee 9.9, Mission Critical Facilities, Technology Spaces and Electronic Equipment|edition=3}}&lt;/ref&gt; recommends a temperature range of {{convert|18|–|27|C|F}}, a dew point range of {{convert|5|–|15|C|F}}, and a relative humidity between 40% to 60% for data center environments.&lt;ref name=ServersCheck&gt;{{Cite web| title = Best Practices for data center monitoring and server room monitoring  | url=https://serverscheck.com/sensors/temperature_best_practices.asp | author = ServersCheck | accessdate = 2016-10-07}}&lt;/ref&gt;  The temperature in a data center will naturally rise because the electrical power used heats the air. Unless the heat is removed, the ambient temperature will rise, resulting in electronic equipment malfunction. By controlling the air temperature, the server components at the board level are kept within the manufacturer's specified temperature/humidity range. Air conditioning systems help control [[humidity]] by cooling the return space air below the [[dew point]]. Too much humidity, and water may begin to [[condensation|condense]] on internal components. In case of a dry atmosphere, ancillary humidification systems may add water vapor if the humidity is too low, which can result in [[electrostatics|static electricity]] discharge problems which may damage components. Subterranean data centers may keep computer equipment cool while expending less energy than conventional designs.

Modern data centers try to use economizer cooling, where they use outside air to keep the data center cool. At least one data center (located in [[Upstate New York]]) will cool servers using outside air during the winter. They do not use chillers/air conditioners, which creates potential energy savings in the millions.&lt;ref&gt;{{cite news| url=http://www.reuters.com/article/pressRelease/idUS141369+14-Sep-2009+PRN20090914 | work=Reuters | title=tw telecom and NYSERDA Announce Co-location Expansion | date=2009-09-14}}&lt;/ref&gt;  Increasingly [http://www.datacenterdynamics.com/focus/archive/2013/09/air-air-combat-indirect-air-cooling-wars-0 indirect air cooling] is being deployed in data centers globally which has the advantage of more efficient cooling which lowers power consumption costs in the data center.

Telcordia [http://telecom-info.telcordia.com/site-cgi/ido/docs.cgi?ID=SEARCH&amp;DOCUMENT=GR-2930&amp; GR-2930, ''NEBS: Raised Floor Generic Requirements for Network and Data Centers''], presents generic engineering requirements for raised floors that fall within the strict NEBS guidelines.

There are many types of commercially available floors that offer a wide range of structural strength and loading capabilities, depending on component construction and the materials used. The general types of [[raised floor]]s include stringer, stringerless,  and structural platforms, all of which are discussed in detail in GR-2930 and summarized below.
* '''''Stringered raised floors''''' - This type of raised floor generally consists of a vertical array of steel pedestal assemblies (each assembly is made up of a steel base plate, tubular upright, and a head) uniformly spaced on two-foot centers and mechanically fastened to the concrete floor. The steel pedestal head has a stud that is inserted into the pedestal upright and the overall height is adjustable with a leveling nut on the welded stud of the pedestal head.
* '''''Stringerless raised floors''''' - One non-earthquake type of raised floor generally consists of an array of pedestals that provide the necessary height for routing cables and also serve to support each corner of the floor panels. With this type of floor, there may or may not be provisioning to mechanically fasten the floor panels to the pedestals. This stringerless type of system (having no mechanical attachments between the pedestal heads) provides maximum accessibility to the space under the floor. However, stringerless floors are significantly weaker than stringered raised floors in supporting lateral loads and are not recommended.
* '''''Structural platforms''''' - One type of structural platform consists of members constructed of steel angles or channels that are welded or bolted together to form an integrated platform for supporting equipment. This design permits equipment to be fastened directly to the platform without the need for toggle bars or supplemental bracing. Structural platforms may or may not contain panels or stringers.

Data centers typically have [[raised floor]]ing made up of {{convert|60|cm|ft|abbr=on|0}} removable square tiles. The trend is towards {{convert|80|-|100|cm|in|abbr=on}} void to cater for better and uniform air distribution. These provide a [[plenum space|plenum]] for air to circulate below the floor, as part of the air conditioning system, as well as providing space for power cabling.

====Metal whiskers====
Raised floors and other metal structures such as cable trays and ventilation ducts have caused many problems with [[zinc whiskers]] in the past, and likely are still present in many data centers. This happens when microscopic metallic filaments form on metals such as zinc or tin that protect many metal structures and electronic components from corrosion. Maintenance on a raised floor or installing of cable etc. can dislodge the whiskers, which enter the airflow and may short circuit server components or power supplies, sometimes through a high current metal vapor [[plasma arc]]. This phenomenon is not unique to data centers, and has also caused catastrophic failures of satellites and military hardware.&lt;ref&gt;{{cite web|title=NASA - metal whiskers research|url=http://nepp.nasa.gov/whisker/other_whisker/index.htm|publisher=NASA|accessdate=2011-08-01}}&lt;/ref&gt;

===Electrical power===

[[File:Datacenter Backup Batteries.jpg|thumb|right|A bank of batteries in a large data center, used to provide power until diesel generators can start]]

Backup power consists of one or more [[uninterruptible power supply|uninterruptible power supplies]], battery banks, and/or [[Diesel generator|diesel]] / [[gas turbine]] generators.&lt;ref&gt;Detailed explanation of UPS topologies {{cite web|url=http://www.emersonnetworkpower.com/en-US/Brands/Liebert/Documents/White%20Papers/Evaluating%20the%20Economic%20Impact%20of%20UPS%20Technology.pdf |format=PDF |title=EVALUATING THE ECONOMIC IMPACT OF UPS TECHNOLOGY |deadurl=yes |archiveurl=https://web.archive.org/web/20101122074817/http://emersonnetworkpower.com/en-US/Brands/Liebert/Documents/White%20Papers/Evaluating%20the%20Economic%20Impact%20of%20UPS%20Technology.pdf |archivedate=2010-11-22 |df= }}&lt;/ref&gt;

To prevent [[single point of failure|single points of failure]], all elements of the electrical systems, including backup systems, are typically fully duplicated, and critical servers are connected to both the "A-side" and "B-side" power feeds. This arrangement is often made to achieve [[N+1 redundancy]] in the systems. [[Transfer switch#Static transfer switch|Static transfer switches]] are sometimes used to ensure instantaneous switchover from one supply to the other in the event of a power failure.

===Low-voltage cable routing===
Data cabling is typically routed through overhead [[cable tray]]s in modern data centers. But some{{Who|date=May 2012}} are still recommending under raised floor cabling for security reasons and to consider the addition of cooling systems above the racks in case this enhancement is necessary. Smaller/less expensive data centers without raised flooring may use anti-static tiles for a flooring surface. Computer cabinets are often organized into a [[Data center environmental control#Aisle containment|hot aisle]] arrangement to maximize airflow efficiency.

===Fire protection===
[[File:FM200 Three.jpg|thumb|[[FM200]] Fire Suppression Tanks]]
Data centers feature [[fire protection]] systems, including [[passive fire protection|passive]] and [[Active Design]] elements, as well as implementation of [[fire prevention]] programs in operations. [[Smoke detectors]] are usually installed to provide early warning of a fire at its incipient stage. This allows investigation, interruption of power, and manual fire suppression using hand held fire extinguishers before the fire grows to a large size. An [[active fire protection]] system, such as a [[fire sprinkler system]] or a [[clean agent]] fire suppression gaseous system, is often provided to control a full scale fire if it develops. High sensitivity smoke detectors, such as [[aspirating smoke detector]]s, activating [[clean agent]] fire suppression gaseous systems activate earlier than fire sprinklers.

* Sprinklers = structure protection and building life safety.
* Clean agents = business continuity and asset protection.
* No water = no collateral damage or clean up.

Passive fire protection elements include the installation of [[Firewall (construction)|fire walls]] around the data center, so a fire can be restricted to a portion of the facility for a limited time in the event of the failure of the active fire protection systems. Fire wall penetrations into the server room, such as cable penetrations, coolant line penetrations and air ducts, must be provided with fire rated penetration assemblies, such as [[fire stop]]ping.

===Security===
Physical security also plays a large role with data centers. Physical access to the site is usually restricted to selected personnel, with controls including a layered security system often starting with fencing, [[bollard]]s and [[mantrap (access control)|mantraps]].&lt;ref&gt;{{cite web|author=Sarah D. Scalet |url=http://www.csoonline.com/article/220665 |title=19 Ways to Build Physical Security Into a Data Center |publisher=Csoonline.com |date=2005-11-01 |accessdate=2013-08-30}}&lt;/ref&gt; [[Video camera]] surveillance and permanent [[security guard]]s are almost always present if the data center is large or contains sensitive information on any of the systems within. The use of finger print recognition [[mantrap (snare)|mantrap]]s is starting to be commonplace.

==Energy use==
[[File:Google Data Center, The Dalles.jpg|thumb|[[Google Data Centers|Google Data Center]], [[The Dalles, Oregon]]]]
{{main article|IT energy management}}

Energy use is a central issue for data centers. Power draw for data centers ranges from a few kW for a rack of servers in a closet to several tens of MW for large facilities. Some facilities have power densities more than 100 times that of a typical office building.&lt;ref&gt;{{cite web|url=http://www1.eere.energy.gov/femp/program/dc_energy_consumption.html|title=Data Center Energy Consumption Trends|publisher=U.S. Department of Energy|accessdate=2010-06-10}}&lt;/ref&gt; For higher power density facilities, electricity costs are a dominant [[operating expense]] and account for over 10% of the [[total cost of ownership]] (TCO) of a data center.&lt;ref&gt;J Koomey, C. Belady, M. Patterson, A. Santos, K.D. Lange. [http://www.intel.com/assets/pdf/general/servertrendsreleasecomplete-v25.pdf Assessing Trends Over Time in Performance, Costs, and Energy Use for Servers] Released on the web August 17th, 2009.&lt;/ref&gt; By 2012 the cost of power for the data center is expected to exceed the cost of the original capital investment.&lt;ref&gt;{{cite web|url=http://www1.eere.energy.gov/femp/pdfs/data_center_qsguide.pdf |title=Quick Start Guide to Increase Data Center Energy Efficiency |publisher=U.S. Department of Energy |accessdate=2010-06-10 |deadurl=yes |archiveurl=https://web.archive.org/web/20101122035456/http://www1.eere.energy.gov:80/femp/pdfs/data_center_qsguide.pdf |archivedate=2010-11-22 |df= }}&lt;/ref&gt;

===Greenhouse gas emissions===
In 2007 the entire [[information and communication technologies]] or ICT sector was estimated to be responsible for roughly 2% of global [[Greenhouse gas|carbon emissions]] with data centers accounting for 14% of the ICT footprint.&lt;ref name="smart1"&gt;{{cite web|url=http://www.smart2020.org/_assets/files/03_Smart2020Report_lo_res.pdf |title=Smart 2020: Enabling the low carbon economy in the information age |publisher=The Climate Group for the Global e-Sustainability Initiative |accessdate=2008-05-11 |deadurl=yes |archiveurl=https://web.archive.org/web/20110728032834/http://www.smart2020.org/_assets/files/03_Smart2020Report_lo_res.pdf |archivedate=2011-07-28 |df= }}&lt;/ref&gt; The US EPA estimates that servers and data centers are responsible for up to 1.5% of the total US electricity consumption,&lt;ref name="energystar1"&gt;{{cite web|url=http://www.energystar.gov/ia/partners/prod_development/downloads/EPA_Datacenter_Report_Congress_Final1.pdf|title=Report to Congress on Server and Data Center Energy Efficiency|publisher=U.S. Environmental Protection Agency ENERGY STAR Program}}&lt;/ref&gt; or roughly .5% of US GHG emissions,&lt;ref&gt;A calculation of data center electricity burden cited in the [http://www.energystar.gov/ia/partners/prod_development/downloads/EPA_Datacenter_Report_Congress_Final1.pdf Report to Congress on Server and Data Center Energy Efficiency] and electricity generation contributions to green house gas emissions published by the EPA in the [http://epa.gov/climatechange/emissions/downloads10/US-GHG-Inventory-2010_ExecutiveSummary.pdf Greenhouse Gas Emissions Inventory Report]. Retrieved 2010-06-08.&lt;/ref&gt;  for 2007. Given a business as usual scenario greenhouse gas emissions from data centers is projected to more than double from 2007 levels by 2020.&lt;ref name="smart1"/&gt;

Siting is one of the factors that affect the energy consumption and environmental effects of a datacenter. In areas where climate favors cooling and lots of renewable electricity is available the environmental effects will be more moderate. Thus countries with favorable conditions, such as: Canada,&lt;ref&gt;[http://www.theglobeandmail.com/report-on-business/canada-called-prime-real-estate-for-massive-data-computers/article2071677/ Canada Called Prime Real Estate for Massive Data Computers - Globe &amp; Mail] Retrieved June 29, 2011.&lt;/ref&gt; Finland,&lt;ref&gt;[http://datacenter-siting.weebly.com/ Finland - First Choice for Siting Your Cloud Computing Data Center.]. Retrieved 4 August 2010.&lt;/ref&gt; Sweden,&lt;ref&gt;{{cite web|url=http://www.stockholmbusinessregion.se/templates/page____41724.aspx?epslanguage=EN|title=Stockholm sets sights on data center customers|accessdate=4 August 2010|archiveurl=https://web.archive.org/web/20100819190918/http://www.stockholmbusinessregion.se/templates/page____41724.aspx?epslanguage=EN|archivedate=19 August 2010}}&lt;/ref&gt; Norway &lt;ref&gt;[http://www.innovasjonnorge.no/en/start-page/invest-in-norway/industries/datacenters/ In a world of rapidly increasing carbon emissions from the ICT industry, Norway offers a sustainable solution] Retrieved 1 March 2016.&lt;/ref&gt; and Switzerland,&lt;ref&gt;[http://www.greenbiz.com/news/2010/06/30/swiss-carbon-neutral-servers-hit-cloud Swiss Carbon-Neutral Servers Hit the Cloud.]. Retrieved 4 August 2010.&lt;/ref&gt; are trying to attract cloud computing data centers.

In an 18-month investigation by scholars at Rice University's Baker Institute for Public Policy in Houston and the Institute for Sustainable and Applied Infodynamics in Singapore, data center-related emissions will more than triple by 2020.
&lt;ref&gt;{{Cite news
 |author=Katrice R. Jalbuena 
 |title=Green business news. 
 |quote= 
 |publisher=EcoSeed 
 |date=October 15, 2010 
 |pages= 
 |url=http://ecoseed.org/en/business-article-list/article/1-business/8219-i-t-industry-risks-output-cut-in-low-carbon-economy 
 |accessdate=2010-11-11 
 |deadurl=yes 
 |archiveurl=https://web.archive.org/web/20160618081417/http://ecoseed.org/en/business-article-list/article/1-business/8219-i-t-industry-risks-output-cut-in-low-carbon-economy 
 |archivedate=2016-06-18 
 |df= 
}}&lt;/ref&gt;

===Energy efficiency===
The most commonly used metric to determine the energy efficiency of a data center is [[power usage effectiveness]], or PUE. This simple ratio is the total power entering the data center divided by the power used by the IT equipment.

:&lt;math&gt; \mathrm{PUE}  =  {\mbox{Total Facility Power} \over \mbox{IT Equipment Power}} &lt;/math&gt;

Total facility power consists of power used by IT equipment plus any overhead power consumed by anything that is not considered a computing or data communication device (i.e. cooling, lighting, etc.). An ideal PUE is 1.0 for the hypothetical situation of zero overhead power. The average data center in the US has a PUE of 2.0,&lt;ref name="energystar1"/&gt; meaning that the facility uses two watts of total power (overhead + IT equipment) for every watt delivered to IT equipment. State-of-the-art data center energy efficiency is estimated to be roughly 1.2.&lt;ref&gt;{{cite web|url=https://microsite.accenture.com/svlgreport/Documents/pdf/SVLG_Report.pdf|title=Data Center Energy Forecast|publisher=Silicon Valley Leadership Group}}&lt;/ref&gt; Some large data center operators like [[Microsoft]] and [[Yahoo!]] have published projections of PUE for facilities in development; [[Google]] publishes quarterly actual efficiency performance from data centers in operation.&lt;ref&gt;{{cite web|url=https://www.google.com/about/datacenters/efficiency/internal/|title=Efficiency: How we do it – Data centers|publisher=Google|accessdate=2015-01-19}}&lt;/ref&gt;

The [[U.S. Environmental Protection Agency]] has an [[Energy Star]] rating for standalone or large data centers. To qualify for the ecolabel, a data center must be within the top quartile of energy efficiency of all reported facilities.&lt;ref&gt;Commentary on introduction of Energy Star for Data Centers {{cite web|title=Introducing EPA ENERGY STAR for Data Centers |url=http://www.emerson.com/edc/post/2010/06/15/Introducing-EPA-ENERGY-STARc2ae-for-Data-Centers.aspx |format=Web site |publisher=Jack Pouchet |accessdate=2010-09-27 |date=2010-09-27 |deadurl=yes |archiveurl=https://web.archive.org/web/20100925210539/http://emerson.com/edc/post/2010/06/15/Introducing-EPA-ENERGY-STARc2ae-for-Data-Centers.aspx |archivedate=2010-09-25 |df= }}&lt;/ref&gt;

European Union also has a similar initiative: EU Code of Conduct for Data Centres&lt;ref&gt;{{cite web|url=http://iet.jrc.ec.europa.eu/energyefficiency/ict-codes-conduct/data-centres-energy-efficiency |title=EU Code of Conduct for Data Centres |publisher=iet.jrc.ec.europa.eu |date= |accessdate=2013-08-30 }}&lt;/ref&gt;

===Energy use analysis===
Often, the first step toward curbing energy use in a data center is to understand how energy is being used in the data center. Multiple types of analysis exist to measure data center energy use. Aspects measured include not just energy used by IT equipment itself, but also by the data center facility equipment, such as chillers and fans.&lt;ref&gt;Sweeney, Jim. "Reducing Data Center Power and Energy Consumption: Saving Money and 'Going Green,' " GTSI Solutions, pages 2–3. [http://www.gtsi.com/cms/documents/white-papers/green-it.pdf]&lt;/ref&gt;

===Power and cooling analysis===
Power is the largest recurring cost to the user of a data center.&lt;ref name=DRJ_Choosing&gt;{{Citation
 | title = Choosing a Data Center
 | url = http://www.atlantic.net/images/pdf/choosing_a_data_center.pdf
 | publisher = Disaster Recovery Journal
 | year = 2009
 | author = Cosmano, Joe
 | accessdate = 2012-07-21
}}&lt;/ref&gt;  A power and cooling analysis, also referred to as a thermal assessment, measures the relative temperatures in specific areas as well as the capacity of the cooling systems to handle specific ambient temperatures.&lt;ref&gt;Needle, David. "HP's Green Data Center Portfolio Keeps Growing," InternetNews, July 25, 2007. [http://www.internetnews.com/xSP/article.php/3690651/HPs+Green+Data+Center+Portfolio+Keeps+Growing.htm]&lt;/ref&gt; A power and cooling analysis can help to identify hot spots, over-cooled areas that can handle greater power use density, the breakpoint of equipment loading, the effectiveness of a raised-floor strategy, and optimal equipment positioning (such as AC units) to balance temperatures across the data center. Power cooling density is a measure of how much square footage the center can cool at maximum capacity.&lt;ref name=Inc_Howtochoose&gt;{{Citation
 | title = How to Choose a Data Center
 | url = http://www.inc.com/guides/2010/11/how-to-choose-a-data-center_pagen_2.html
 | year = 2010
 | author = Inc. staff
 | accessdate = 2012-07-21
}}&lt;/ref&gt;

===Energy efficiency analysis===
An energy efficiency analysis measures the energy use of data center IT and facilities equipment. A typical energy efficiency analysis measures factors such as a data center's power use effectiveness (PUE) against industry standards, identifies mechanical and electrical sources of inefficiency, and identifies air-management metrics.&lt;ref&gt;Siranosian, Kathryn. "HP Shows Companies How to Integrate Energy Management and Carbon Reduction," TriplePundit, April 5, 2011. [http://www.triplepundit.com/2011/04/hp-launches-program-companies-integrate-manage-energy-carbon-reduction-strategies/]&lt;/ref&gt;

===Computational fluid dynamics (CFD) analysis===
{{main article|Computational fluid dynamics}}

This type of analysis uses sophisticated tools and techniques to understand the unique thermal conditions present in each data center—predicting the temperature, airflow, and pressure behavior of a data center to assess performance and energy consumption, using numerical modeling.&lt;ref&gt;Bullock, Michael. "Computation Fluid Dynamics - Hot topic at Data Center World," Transitional Data Services, March 18, 2010. [http://blog.transitionaldata.com/aggregate/bid/37840/Seeing-the-Invisible-Data-Center-with-CFD-Modeling-Software] {{webarchive |url=https://web.archive.org/web/20120103183406/http://blog.transitionaldata.com/aggregate/bid/37840/Seeing-the-Invisible-Data-Center-with-CFD-Modeling-Software |date=January 3, 2012 }}&lt;/ref&gt; By predicting the effects of these environmental conditions, CFD analysis in the data center can be used to predict the impact of high-density racks mixed with low-density racks&lt;ref&gt;Bouley, Dennis (editor). "Impact of Virtualization on Data Center Physical Infrastructure," The Green grid, 2010. [http://www.thegreengrid.org/~/media/WhitePapers/White_Paper_27_Impact_of_Virtualization_Data_On_Center_Physical_Infrastructure_020210.pdf?lang=en]&lt;/ref&gt; and the onward impact on cooling resources, poor infrastructure management practices and AC failure of AC shutdown for scheduled maintenance.

===Thermal zone mapping===
Thermal zone mapping uses sensors and computer modeling to create a three-dimensional image of the hot and cool zones in a data center.&lt;ref&gt;Fontecchio, Mark. "HP Thermal Zone Mapping plots data center hot spots," SearchDataCenter, July 25, 2007. [http://searchdatacenter.techtarget.com/news/1265634/HP-Thermal-Zone-Mapping-plots-data-center-hot-spots]&lt;/ref&gt;

This information can help to identify optimal positioning of data center equipment. For example, critical servers might be placed in a cool zone that is serviced by redundant AC units.

===Green data centers===
[[File:Magazin Vauban E.jpg|thumb| This water-cooled data center in the [[Independent Port of Strasbourg|Port of Strasbourg]], France claims the attribute ''green''.]]
Data centers use a lot of power, consumed by two main usages: the power required to run the actual equipment and then the power required to cool the equipment. The first category is addressed by designing computers and storage systems that are increasingly power-efficient.&lt;ref name="ReferenceDC2"/&gt; To bring down cooling costs data center designers try to use natural ways to cool the equipment. Many data centers are located near good fiber connectivity, power grid connections and also people-concentrations to manage the equipment, but there are also circumstances where the data center can be miles away from the users and don't need a lot of local management. Examples of this are the 'mass' data centers like Google or Facebook: these DC's are built around many standardized servers and storage-arrays and the actual users of the systems are located all around the world. After the initial build of a data center staff numbers required to keep it running are often relatively low: especially data centers that provide mass-storage or computing power which don't need to be near population centers.Data centers in arctic locations where outside air provides all cooling are getting more popular as cooling and electricity are the two main variable cost components.&lt;ref&gt;{{cite web|url=http://www.gizmag.com/fjord-cooled-data-center/20938/|title=Fjord-cooled DC in Norway claims to be greenest|access-date=23 December 2011}}&lt;/ref&gt;

==Network infrastructure==
[[File:Paris servers DSC00190.jpg|thumb|left|An example of "rack mounted" servers]]
Communications in data centers today are most often based on [[computer network|networks]] running the [[Internet protocol|IP]] [[protocol (computing)|protocol]] suite. Data centers contain a set of [[Router (computing)|routers]] and [[Network switch|switches]] that transport traffic between the servers and to the outside world. [[Redundancy (engineering)|Redundancy]] of the Internet connection is often provided by using two or more upstream service providers (see [[Multihoming]]).

Some of the servers at the data center are used for running the basic Internet and [[intranet]] services needed by internal users in the organization, e.g., e-mail servers, [[proxy server]]s, and [[Domain Name System|DNS]] servers.

Network security elements are also usually deployed: [[firewall (networking)|firewalls]], [[VPN]] [[Gateway (computer networking)|gateways]], [[intrusion detection system]]s, etc. Also common are monitoring systems for the network and some of the applications. Additional off site monitoring systems are also typical, in case of a failure of communications inside the data center.

==Data center infrastructure management==
[[Data center infrastructure management]] (DCIM) is the integration of information technology (IT) and facility management disciplines to centralize monitoring, management and intelligent capacity planning of a data center's critical systems. Achieved through the implementation of specialized software, hardware and sensors, DCIM enables common, real-time monitoring and management platform for all interdependent systems across IT and facility infrastructures.

Depending on the type of implementation, DCIM products can help data center managers identify and eliminate sources of risk to increase availability of critical IT systems. DCIM products also can be used to identify interdependencies between facility and IT infrastructures to alert the facility manager to gaps in system redundancy, and provide dynamic, holistic benchmarks on power consumption and efficiency to measure the effectiveness of "green IT" initiatives.

It's important to measure and understand data center efficiency metrics.  A lot of the discussion in this area has focused on energy issues, but other metrics beyond the PUE can give a more detailed picture of the data center operations. Server, storage, and staff utilization metrics can contribute to a more complete view of an enterprise data center. In many cases, disc capacity goes unused and in many instances the organizations run their servers at 20% utilization or less.&lt;ref&gt;{{cite web|url=http://content.dell.com/us/en/enterprise/d/large-business/measure-data-center-efficiency.aspx |title=Measuring Data Center Efficiency: Easier Said Than Done |publisher=Dell.com |accessdate=2012-06-25 |deadurl=yes |archiveurl=https://web.archive.org/web/20101027083349/http://content.dell.com:80/us/en/enterprise/d/large-business/measure-data-center-efficiency.aspx |archivedate=2010-10-27 |df= }}&lt;/ref&gt; More effective automation tools can also improve the number of servers or virtual machines that a single admin can handle.

DCIM providers are increasingly linking with [[computational fluid dynamics]] providers to predict complex airflow patterns in the data center. The CFD component is necessary to quantify the impact of planned future changes on cooling resilience, capacity and efficiency.&lt;ref name="gartner"&gt;{{cite web|url=http://www.gartner.com/it-glossary/computational-fluid-dynamic-cfd-analysis|title=Computational-Fluid-Dynamic (CFD) Analysis &amp;#124; Gartner IT Glossary|publisher=gartner.com|accessdate=2014-08-27}}&lt;/ref&gt;

==Managing the capacity of a data center==
{{unreferenced section|date=August 2016}}
[[File:Capacity of a datacenter - Life Cycle.jpg|thumbnail|left|Capacity of a datacenter - Life Cycle]]
Several parameters may limit the capacity of a data center. For long term usage, the main limitations will be available area, then available power. In the first stage of its life cycle, a data center will see its occupied space growing more rapidly than consumed energy. With constant densification of new IT technologies, the need in energy is going to become dominant, equaling then overcoming the need in area (second then third phase of cycle). The development and multiplication of connected objects, the needs in storage and data treatment lead to the necessity of data centers to grow more and more rapidly. It is therefore important to define a data center strategy before being cornered. The decision, conception and building cycle lasts several years. Therefore, it is imperative to initiate this strategic consideration when the data center reaches about 50% of its power capacity. Maximum occupation of a data center needs to be stabilized around 85%, be it in power or occupied area. Resources thus managed will allow a rotation zone for managing hardware replacement and will allow temporary cohabitation of old and new generations. In the case where this limit would be overcrossed durably, it would not be possible to proceed to material replacements, which would invariably lead to smothering the information system. The data center is a resource in its own right of the information system, with its own constraints of time and management (life span of 25 years), it therefore needs to be taken into consideration in the framework of the SI midterm planning (between 3 and 5 years).

==Applications==
[[File:IBMPortableModularDataCenter.jpg|thumb|right|A 40-foot [[Portable Modular Data Center]]]]

The main purpose of a data center is running the IT systems applications that handle the core business and operational data of the organization. Such systems may be proprietary and developed internally by the organization, or bought from [[enterprise software]] vendors. Such common applications are [[Enterprise resource planning|ERP]] and [[Customer relationship management|CRM]] systems.

A data center may be concerned with just [[operations architecture]] or it may provide other services as well.

Often these applications will be composed of multiple hosts, each running a single component. Common components of such applications are [[database]]s, [[file server]]s, [[application server]]s, [[middleware]], and various others.

Data centers are also used for off site backups. Companies may subscribe to backup services provided by a data center. This is often used in conjunction with [[Tape drive|backup tapes]]. Backups can be taken off servers locally on to tapes. However, tapes stored on site pose a security threat and are also susceptible to fire and flooding. Larger companies may also send their backups off site for added security. This can be done by backing up to a data center. Encrypted backups can be sent over the Internet to another data center where they can be stored securely.

For quick deployment or [[disaster recovery]], several large hardware vendors have developed mobile/modular solutions that can be installed and made operational in very short time. Companies such as
[[File:Edge Night 02.jpg|thumb|A modular data center connected to the power grid at a utility substation]]
* [[Cisco Systems]],&lt;ref&gt;{{cite web|title=Info and video about Cisco's solution |url=http://www.datacenterknowledge.com/archives/2008/May/15/ciscos_mobile_emergency_data_center.html |publisher=Datacentreknowledge |accessdate=2008-05-11 |date=May 15, 2007 |deadurl=yes |archiveurl=https://web.archive.org/web/20080519213241/http://www.datacenterknowledge.com:80/archives/2008/May/15/ciscos_mobile_emergency_data_center.html |archivedate=2008-05-19 |df= }}&lt;/ref&gt;
* [[Sun Microsystems]] ([[Sun Modular Datacenter]]),&lt;ref&gt;{{cite web|url=http://www.sun.com/products/sunmd/s20/specifications.jsp|archiveurl=https://web.archive.org/web/20080513090300/http://www.sun.com/products/sunmd/s20/specifications.jsp|archivedate=2008-05-13|title=Technical specs of Sun's Blackbox|accessdate=2008-05-11}}&lt;/ref&gt;&lt;ref&gt;And English Wiki article on [[Sun Modular Datacenter|Sun's modular datacentre]]&lt;/ref&gt;
* [[Groupe Bull|Bull]] (mobull),&lt;ref&gt;{{cite web|title=Mobull Plug and Boot Datacenter|url=http://www.bull.com/extreme-computing/mobull.html|publisher=Bull|first=Daniel|last=Kidger|accessdate=2011-05-24}}&lt;/ref&gt;
* [[IBM]] ([[Portable Modular Data Center]]),
* [[Schneider-Electric]] ([[Portable Modular Data Center]]),
* [[Hewlett-Packard|HP]] ([[HP Performance Optimized Datacenter|Performance Optimized Datacenter]]),&lt;ref&gt;{{cite web|url=http://h18004.www1.hp.com/products/servers/solutions/datacentersolutions/pod/index.html |title=HP Performance Optimized Datacenter (POD) 20c and 40c - Product Overview |publisher=H18004.www1.hp.com |date= |accessdate=2013-08-30}}&lt;/ref&gt;
* [[Huawei]] (Container Data Center Solution),&lt;ref&gt;{{cite web|title=Huawei's Container Data Center Solution|url=http://www.huawei.com/ilink/enenterprise/download/HW_143893|publisher=Huawei|accessdate=2014-05-17}}
&lt;/ref&gt; and
* [[Google]] ([[Google Modular Data Center]]) have developed systems that could be used for this purpose.&lt;ref&gt;{{cite web|url=http://www.crn.com/hardware/208403225 |publisher=ChannelWeb |accessdate=2008-05-11 |title=IBM's Project Big Green Takes Second Step |first=Brian |last=Kraemer |date=June 11, 2008 |deadurl=yes |archiveurl=https://web.archive.org/web/20080611114732/http://www.crn.com:80/hardware/208403225 |archivedate=2008-06-11 |df= }}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://hightech.lbl.gov/documents/data_centers/modular-dc-procurement-guide.pdf |title=Modular/Container Data Centers Procurement Guide: Optimizing for Energy Efficiency and Quick Deployment |format=PDF |date= |accessdate=2013-08-30 |deadurl=yes |archiveurl=https://web.archive.org/web/20130531191212/http://hightech.lbl.gov/documents/data_centers/modular-dc-procurement-guide.pdf |archivedate=2013-05-31 |df= }}&lt;/ref&gt;
* BASELAYER has a patent on the software defined modular data center.&lt;ref&gt;{{Citation|title = System and method of providing computer resources|url = http://www.google.com/patents/US8434804|date = May 7, 2013|accessdate = 2016-02-24|first = George|last = Slessman}}&lt;/ref&gt;&lt;ref&gt;{{Cite web|title = Modular Data Center Firm IO to Split Into Two Companies|url = http://www.datacenterknowledge.com/archives/2014/12/02/modular-data-center-firm-io-to-split-into-two-companies/|website = Data Center Knowledge|access-date = 2016-02-24|language = en-US}}&lt;/ref&gt;

==US wholesale and retail colocation providers==
According to Synergy Research Group, "the scale of the wholesale colocation market in the United States is very significant relative to the retail market, with Q3 wholesale revenues reaching almost $700 million. [[Digital Realty]] Trust is the wholesale market leader, followed at a distance by [[DuPont Fabros]]." Synergy Research also describes the US colocation market as the most mature and well-developed in the world," based on revenue and the continued adoption of cloud infrastructure services.
* Contains estimates from Synergy Research Group.&lt;ref name="srgresearch"&gt;{{cite web|url=https://www.srgresearch.com/articles/mature-us-colocation-market-led-equinix-and-centurylink-savvis|title=Mature US Colocation Market Led by Equinix and CenturyLink-Savvis &amp;#124; Synergy Research Group|author=Synergy Research Group, Reno, NV|publisher=srgresearch.com|accessdate=2014-08-27}}&lt;/ref&gt;

{| class="wikitable sortable"
|-
!Rank !! Company Name !! US Market Share
|-
!1
| Various Providers || 34%
|-
!2
| [[Equinix]] || 18%
|-
!3
| [[CenturyLink-Savvis]] || 8%
|-
!4
| [[SunGard]] || 5%
|-
!5
| [[AT&amp;T]] || 5%
|-
!6
| [[Verizon]] || 5%
|-
!7
| Telx || 4%
|-
!8
| CyrusOne || 4%
|-
!9
| [[Level 3 Communications]] || 3%
|-
!10
| [[Internap]] || 2%
|}

==See also==
{{columns-list|colwidth=20em|
* [[Central apparatus room]]
* [[Colocation center]]
* [[Data center infrastructure management]]
* [[Disaster recovery]]
* [[Dynamic Infrastructure]]
* [[Electrical network]]
* [[HVAC]]
* [[Internet exchange point]]
* [[Internet hosting service]]
* [[Modular data center]]
* [[Neher–McGrath]]
* [[Network operations center]]
* [[Open Compute Project]], by [[Facebook]]
* [[Peering]]
* [[Server farm]]
* [[Server room]]
* [[Server Room Environment Monitoring System]]
* [[Server sprawl]]
* [[Sun Modular Datacenter]]
* [[Telecommunications network]]
* [[Utah Data Center]]
* [[Web hosting service]]
* [[Anderson Powerpole]] connector
}}

==References==
{{reflist|colwidth=30em}}

==External links==
{{Commons category|Data centers}}
{{wikibooks|The Design and Organization of Data Centers}}
{{wiktionary}}
* [http://hightech.lbl.gov/datacenters.html Lawrence Berkeley Lab] - Research, development, demonstration, and deployment of energy-efficient technologies and practices for data centers
* [http://hightech.lbl.gov/dc-powering/faq.html DC Power For Data Centers Of The Future] - FAQ: 380VDC testing and demonstration at a Sun data center.
* [http://www.dccompendium.com/ DC Compendium] - Repository and compendium of data centers globally.
* [http://media.wix.com/ugd/fb8983_e929404b24874e4fa7a8279f1cda58f8.pdf White Paper] - Property Taxes: The New Challenge for Data Centers

{{Authority control}}
{{Cloud computing}}

{{DEFAULTSORT:Data Center}}
[[Category:Computer networking]]
[[Category:Applications of distributed computing]]
[[Category:Cloud storage]]
[[Category:Data management]]
[[Category:Distributed data storage]]
[[Category:Distributed data storage systems]]
[[Category:Servers (computing)]]
[[Category:Data centers| ]]</text>
      <sha1>moov7q8axkagak4ued03d9agsv1wsjm</sha1>
    </revision>
  </page>
  <page>
    <title>Data conditioning</title>
    <ns>0</ns>
    <id>24540689</id>
    <revision>
      <id>677395227</id>
      <parentid>677395170</parentid>
      <timestamp>2015-08-23T00:05:46Z</timestamp>
      <contributor>
        <username>Twimoki</username>
        <id>7226010</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3078" xml:space="preserve">{{Multiple issues|
{{POV|date=August 2015}}
{{One source|date=August 2015}}
{{Notability|date=August 2015}}
}}

'''Data conditioning''' is the use of data management and optimization techniques which result in the intelligent routing, optimization and protection of data for storage or [[data movement]] in a computer system.  Data conditioning features enable enterprise and cloud [[data center]]s to dramatically improve system utilization and increase application performance lowering both [[capital expenditures]] and [[operating cost]]s.

Data conditioning technologies delivered through a Data Conditioning Platform optimize data as it moves through a computer’s I/O ([[Input/Output]]) path or I/O bus—the data path between the main processor complex and storage subsystems.  The functions of a Data Conditioning Platform typically reside on a storage controller add-in card inserted into the [[PCI-e]] slots of a server.  This enables easy integration of new features in a server or a whole data center.

Data conditioning features delivered via a Data Conditioning Platform are designed to simplify system integration, and minimize implementation risks associated with deploying new technologies by ensuring seamless compatibility with all leading server and  storage hardware, operating systems and applications, and meeting all current commercial/off-the-shelf (COTS) standards.  By delivering optimization features via a Data Conditioning Platform, data center managers can improve system efficiency and reduce cost with minimal disruption and avoid the need to modify existing applications or operating systems, and leverage existing hardware systems.

== Summary ==

Data conditioning builds on existing data storage functionality delivered in the I/O path including [[RAID]] (Redundant Arrays of Inexpensive Disks), intelligent I/O-based [http://www.adaptec.com/en-us/_common/greenpower?refURL=/greenpower/ power management], and [[Solid-state drive|SSD]] (Solid-State Drive) performance caching techniques.  Data conditioning is enabled both by advanced [[ASIC]] controller technology and intelligent software.  New data conditioning capabilities can be designed into and delivered via storage controllers in the I/O path  or to achieve the data center’s technical and business goals.

Data Conditioning strategies can also be applied to improving server and storage utilization and for better managing a wide range of hardware and system-level capabilities.

== Background and Purpose ==

Data conditioning principles can be applied to any demanding computing environment to create significant cost, performance and system utilization efficiencies, and are typically deployed by data center managers, system integrators, and storage and server OEMs seeking to optimize hardware and software utilization, simplified, non-intrusive technology integration, and minimal risks and performance hits traditionally associated with incorporating new data center technologies.

== References ==

[http://www.adaptec/maxIQ Adaptec MaxIQ]

[[Category:Data management]]</text>
      <sha1>gl31v81z2hv63njc5k63mm5bmosx4l1</sha1>
    </revision>
  </page>
  <page>
    <title>Database-centric architecture</title>
    <ns>0</ns>
    <id>13783336</id>
    <revision>
      <id>759154366</id>
      <parentid>712430992</parentid>
      <timestamp>2017-01-09T15:06:33Z</timestamp>
      <contributor>
        <username>Mkumba</username>
        <id>12450569</id>
      </contributor>
      <comment>added references to overall enterprise data centric architectures</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4232" xml:space="preserve">'''Database-centric Architecture''' or '''data-centric architecture''' has several distinct meanings, generally relating to [[software architecture]]s in which [[database]]s play a crucial role. Often this description is meant to contrast the design to an alternative approach. For example, the characterization of an architecture as "database-centric" may mean any combination of the following:

* using a standard, general-purpose [[relational database management system]], as opposed to customized in-[[Memory (computers)|memory]] or [[Computer file|file]]-based [[data structures]] and [[access method]]s. With the evolution of sophisticated [[Database management system|DBMS]] software, much of which is either free or included with the [[operating system]], application developers have become increasingly reliant on standard database tools, especially for the sake of [[rapid application development]].
* using dynamic, [[Table (database)|table]]-driven logic, as opposed to logic embodied in previously [[compiled]] [[Computer program|program]]s. The use of table-driven logic, i.e. behavior that is heavily dictated by the contents of a database, allows programs to be simpler and more flexible. This capability is a central feature of [[dynamic programming language]]s. See also [[control table]]s for tables that are normally coded and embedded within programs as [[data structures]] (i.e. not compiled statements) but could equally be read in from a [[flat file]], [[database]] or even retrieved from a [[spreadsheet]].
* using [[stored procedure]]s that run on [[database server]]s, as opposed to greater reliance on logic running in middle-tier [[application server]]s in a [[multi-tier architecture]]. The extent to which [[business logic]] should be placed at the back-end versus another tier is a subject of ongoing debate. For example, Toon Koppelaars presents a detailed analysis of alternative [[Oracle Database|Oracle-based]] architectures that vary in the placement of business logic, concluding that a database-centric approach has practical advantages from the standpoint of ease of development and maintainability.&lt;ref&gt;[https://web.archive.org/web/20060525094651/http://www.oracle.com/technology/pub/articles/odtug_award.pdf] A Database-centric approach to J2EE Application Development&lt;/ref&gt;
* using a shared database as the basis for communicating between [[Parallel computing|parallel processes]] in [[distributed computing]] applications, as opposed to direct [[inter-process communication]] via [[message passing]] functions and [[message-oriented middleware]]. A potential benefit of database-centric architecture in [[distributed application]]s is that it simplifies the design by utilizing DBMS-provided [[transaction processing]] and [[Index (database)|indexing]] to achieve a high degree of reliability, performance, and capacity.&lt;ref&gt;{{Citation |author=Lind P, Alm M |title=A database-centric virtual chemistry system |journal=J Chem Inf Model |volume=46 |issue=3 |pages=1034–9 |year=2006 |pmid=16711722 |doi=10.1021/ci050360b |postscript=. }}&lt;/ref&gt; For example, [[Base One]] describes a database-centric distributed computing architecture for [[Grid computing|grid]] and [[Computer cluster|cluster]] computing, and explains how this design provides enhanced security, fault-tolerance, and [[scalability]].&lt;ref&gt;[http://www.boic.com/dbgrid.htm Database-Centric Grid and Cluster Computing]&lt;/ref&gt;
* an overall enterprise architecture that favors shared data models&lt;ref&gt;{{Cite news|url=http://tdan.com/the-data-centric-revolution/18780|title=The Data Centric Revolution|newspaper=TDAN.com|access-date=2017-01-09}}&lt;/ref&gt; over allowing each application to have its own, idiosyncratic data model. 

==See also==
*[[Control table]]s
*[[:Category:Data-centric programming languages|Data-centric programming languages]]
*The [[data-driven programming]] paradigm, which makes the information used in a system the primary design driver.
*See the [http://datacentricmanifesto.org/ datacentricmanifesto.org] 

==References==
{{Reflist}}

{{Database}}

{{DEFAULTSORT:Database-Centric Architecture}}
[[Category:Software architecture]]
[[Category:Data management]]
[[Category:Distributed computing architecture]]</text>
      <sha1>25uigivesogjv2r9yzzjy6lv7l4gchi</sha1>
    </revision>
  </page>
  <page>
    <title>Query language</title>
    <ns>0</ns>
    <id>494528</id>
    <revision>
      <id>755820936</id>
      <parentid>755667762</parentid>
      <timestamp>2016-12-20T11:21:31Z</timestamp>
      <contributor>
        <username>Feminist</username>
        <id>25530780</id>
      </contributor>
      <minor />
      <comment>/* Examples */[[Talk:Bing (search engine)#Requested move 13 December 2016]], replaced: [[Bing]] → [[Bing (search engine)|Bing]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6603" xml:space="preserve">{{redirect|Database language|other types of database languages|Database#Languages}}
{{Multiple issues|
{{prose|date=October 2010}}
{{refimprove|date=October 2010}}
}}

'''Query languages''' are [[computer language]]s used to make queries in [[database]]s and [[information system]]s.

==Types==
Broadly, query languages can be classified according to whether they are database query languages or [[information retrieval query language]]s. The difference is that a database query language attempts to give factual answers to factual questions, while an information retrieval query language attempts to find documents containing information that is relevant to an area of inquiry.

==Examples==
Examples include:
* [[.QL]] is a proprietary object-oriented query language for querying [[relational database]]s; successor of Datalog;
* [[Contextual Query Language]] (CQL) a formal language for representing queries to [[information retrieval]] systems such as web indexes or bibliographic catalogues.
* CQLF (CODYASYL Query Language, Flat) is a query language for [[CODASYL]]-type databases;
* [[Concept-Oriented Query Language]] (COQL) is used in the concept-oriented model (COM). It is based on a novel [[data modeling]] construct, concept, and uses such operations as projection and de-projection for multi-dimensional analysis, analytical operations and inference;
* [[Cypher Query Language|Cypher]] is a query language for the [[Neo4j]] graph database;
* [[Data Mining Extensions|DMX]] is a query language for [[Data Mining]] models;
* [[Datalog]] is a query language for [[deductive database]]s;
* [[F-logic]] is a declarative object-oriented language for [[deductive database]]s and [[knowledge representation]].
* [[Facebook Query Language|FQL]] enables you to use a [[SQL]]-style interface to query the data exposed by the [[Graph API]]. It provides advanced features not available in the [[Graph API]].&lt;ref&gt;{{cite web|url=https://developers.facebook.com/docs/technical-guides/fql/|title=FQL Overview|work=Facebook Developers}}&lt;/ref&gt;
* [[Gellish English]] is a language that can be used for queries in Gellish English Databases, for dialogues (requests and responses) as well as for information modeling and [[knowledge modeling]];&lt;ref&gt;http://gellish.wiki.sourceforge.net/Querying+a+Gellish+English+database{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }}&lt;/ref&gt;
* [[Gremlin (programming language)|Gremlin]] is an [[Apache Software Foundation]] graph traversal language for OLTP and OLAP graph systems.
* [[HTSQL]] is a query language that translates [[HTTP]] queries to [[SQL]];
* [[ISBL]] is a query language for [[PRTV]], one of the earliest relational database management systems;
* [[LINQ]] query-expressions is a way to query various data sources from [[.NET Framework|.NET]] languages
* [[LDAP]] is an [[application protocol]] for querying and modifying [[directory services]] running over [[TCP/IP]];
* LogiQL is a variant of Datalog and is the query language for the LogicBlox system.
* [[Molecular Query Language|MQL]] is a [[cheminformatics]] query language for a [[substructure search]] allowing beside nominal properties also numerical properties;
* [[MultiDimensional eXpressions|MDX]] is a query language for [[OLAP]] databases;
* [[N1QL]] is a [[Couchbase, Inc.|Couchbase]]'s query language finding data in [[Couchbase Server]]s;
* [[Object Query Language|OQL]] is Object Query Language;
* [[Object Constraint Language|OCL]] (Object Constraint Language). Despite its name, OCL is also an object query language and an [[Object Management Group|OMG]] standard;
* [[OPath]], intended for use in querying [[WinFS]] ''Stores'';
* [[OttoQL]], intended for querying tables, [[XML]], and databases;
* [[Poliqarp Query Language]] is a special query language designed to analyze annotated text. Used in the [[Poliqarp]] search engine;
* [[PQL]] is a [[special-purpose programming language]] for managing [[process model]]s based on information about [[wiktionary:Scenario|scenarios]] that these models describe;
* [[QUEL query languages|QUEL]] is a [[relational database]] access language, similar in most ways to [[SQL]];
* [[RDQL]] is a [[Resource Description Framework|RDF]] query language;
* [[ReQL]] is a query language used in [http://rethinkdb.com/docs/introduction-to-reql/ RethinkDB];
* [[Smiles arbitrary target specification|SMARTS]] is the [[cheminformatics]] standard for a [[substructure search]];
* [[SPARQL]] is a query language for [[Resource Description Framework|RDF]] [[Graph (discrete mathematics)|graphs]];
* [[SPL (Search Processing Language)|SPL]] is a search language for machine-generated [[big data]], based upon Unix Piping and SQL.
* SCL is the Software Control Language to query and manipulate [[Endevor]] objects
* [[SQL]] is a well known query language and [[Data Manipulation Language]] for [[relational database]]s;
* [[SuprTool]] is a proprietary query language for SuprTool, a database access program used for accessing data in ''Image/SQL'' (formerly [[TurboIMAGE]]) and Oracle databases;
* [[TMQL]] Topic Map Query Language is a query language for [[Topic Maps]];
* TQL is a language used to [http://cmshelpcenter.saas.hp.com/CMS/10.21/ucmdb-docs/docs/eng/doc_lib/Content/modeling/Tql_c_Overview.htm query topology for HP products] 
* [[D (data language specification)|Tutorial D]] is a query language for [[Relational database management system|truly relational database management systems]] (TRDBMS);
* [[XQuery]] is a query language for [[XML database|XML data sources]];
* [[XPath]] is a declarative language for navigating XML documents;
* [[XSPARQL]] is an integrated query language combining XQuery with SPARQL to query both XML and RDF data sources at once;
* [[Yahoo! query language|YQL]] is an [[SQL]]-like query language created by [[Yahoo!]]
* Search engine query languages, e.g., as used by [[Google Search|Google]]&lt;ref&gt;
{{cite web
| title = Search operators
| url = https://support.google.com/websearch/answer/2466433?hl=en
| accessdate = August 22, 2015
| publisher = Google
}}&lt;/ref&gt; or [[Bing (search engine)|Bing]]&lt;ref&gt;
{{cite web
| title = Bing Query Language
| url = https://msdn.microsoft.com/en-us/library/ff795667.aspx
| accessdate = August 22, 2015
| publisher = Microsoft
}}&lt;/ref&gt;

== See also ==
* [[Data manipulation language]]

== References ==
{{Reflist}}

{{Database}}
{{Databases}}
{{Computer language}}
{{Query languages}}

{{Authority control}}

{{DEFAULTSORT:Query Language}}
[[Category:Computer languages]]
[[Category:Data management]]
[[Category:Query languages|*]]

[[no:Database#Spørrespråk]]</text>
      <sha1>8w19fmrpyiy8mhdih5bhd7ci3460h3e</sha1>
    </revision>
  </page>
  <page>
    <title>Enterprise Data Planning</title>
    <ns>0</ns>
    <id>25209669</id>
    <revision>
      <id>653212811</id>
      <parentid>653204355</parentid>
      <timestamp>2015-03-23T21:10:15Z</timestamp>
      <contributor>
        <username>Mdd</username>
        <id>113850</id>
      </contributor>
      <minor />
      <comment>+ Link(s)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4912" xml:space="preserve">{{Orphan|date=October 2010}}
{| style="width: 80%; margin: 0 0 0 10%; border-collapse: collapse; background: #FBFBFB; border: 1px solid #aaa; border-left: 10px solid #f28500;"
|-
| style="width: 52px; padding: 2px 0px 2px 0.5em; text-align: center;" | [[Image:Newspaper nicu buculei 01.svg|50px]]
| style="padding: 0.25em 0.5em;" | '''This article or section reads like an [[Wikipedia:What Wikipedia is not#Wikipedia is not a soapbox|advertisement]] for EDMworks.'''&lt;br/&gt;To meet Wikipedia's [[:Category:Wikipedia style guidelines|quality standards]] and comply with Wikipedia's [[Wikipedia:Neutral point of view|neutral point of view]] policy, it may require [[Wikipedia:Cleanup|cleanup]].
|}
'''Enterprise Data Planning''' is the starting point for enterprise wide change. It states the destination and describes how you will get there. It defines benefits, costs and potential risks.  It provides measures to be used along the way to judge progress and adjust the journey according to changing circumstances.

[[Data]] is fundamental to investment enterprises. Effective, economic management of data underpins operations and enables transformations needed to satisfy customer demands, competition and regulation. Data warehouse(s) and other aspects of the overall [[data architecture]] are critical to the enterprise.

EDMworks has created a strategic data planning approach for the Investment Sector.  It consists of a planning process, planning intranets, templates and training materials.

EDMworks planning process is based on the belief that extensive domain knowledge significantly shortens planning iterations and enables progressively higher quality plans to be produced and implemented.&lt;ref name=hull&gt;Introduction to Futures and Options Markets (John Hull) 1995&lt;/ref&gt;&lt;ref name=taylor&gt;Mastering Derivatives Markets (Francesca Taylor) 2007&lt;/ref&gt;  This approach drives the development of an effective and economic Enterprise Data Architecture.

Enterprise Data Planning is based on proven business disciplines.&lt;ref name=stutely&gt;The Definitive Business Plan (Richard Stutely) 2002&lt;/ref&gt; Key architectural layers for data and applications are then added in order to provide an enterprise wide understanding of the uses and interdependencies of data.&lt;ref name=tozer&gt;Planning for Effective Business Information Systems ([[Edwin E. Tozer]]) 1998&lt;/ref&gt; This enables the definition of the core components of the [[Enterprise data management|EDM]] plan:

* Industry structure and business objectives
* Assessment of systems and services
* Target architecture for applications, data and infrastructure
* Target organization structures
* Systems, database, infrastructure and organizational plans
* Business case, costs, benefits, results and risks.

EDMworks uses several components from the Open Systems Group [[TOGAF]] enterprise systems planning process. [[TOGAF]] acts as an extension to good business planning methods to provide a framework for the development of the systems and data architectural components.

==History==

[[James Martin (author)|James Martin]] was one of the pathfinders in data planning methodologies.  He was one of the first to identify data as being an enterprise wide asset that required management.  He developed a series of tools and methods to support that process.&lt;ref name=Martin&gt;Martin 1982&lt;/ref&gt;
 
Most of the large consulting firms developed their own methods to address the same basic issue.  Frequently, their approaches were incorporated into their own branded system development methodologies that encompassed the complete systems development life-cycle. 
 
Others, such as [[Edwin E. Tozer|Ed Tozer]], developed more focused offerings that dealt with the complexities of extracting key business needs from senior management and then defining relevant architectural visions for the specific enterprise.&lt;ref name=tozer/&gt;
 
From these various sources, the concepts of Business, Data, Applications and Technology Architectures emerged. 
 
The Open Group Architectural Framework (TOGAF) has taken this work forward and has established a sound method in TOGAF version 9.
 
EDMworks approach is to adopt these planning and architectural practices as a basis and then add two additional dimensions to the planning and implementation focus:
* Domain knowledge of the Investments sector.  Investments is a complex global industry with a common set of characteristics about clients, information vendors, competition and regulation.  Domain knowledge significantly improves the quality of the planning and implementation processes
* Development of people and teams.  Change is a major feature of in any Enterprise Data Management program and people and teams both need development in order to make EDM effective throughout an organization.

== References ==
{{reflist}}

== External links ==
* [http://www.edmworks.com Enterprise Data Management works]

[[Category:Data management]]</text>
      <sha1>6h9g13zdkfw2kxq11xxsx1zucydblvz</sha1>
    </revision>
  </page>
  <page>
    <title>Synthetic data</title>
    <ns>0</ns>
    <id>25270778</id>
    <revision>
      <id>744133740</id>
      <parentid>713098332</parentid>
      <timestamp>2016-10-13T09:34:02Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* Applications */http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11898" xml:space="preserve">{{Citation style|date=May 2014}}'''Synthetic data''' are "any production data applicable to a given situation that are not obtained by direct measurement" according to the McGraw-Hill Dictionary of Scientific and Technical Terms;&lt;ref name="McGraw"&gt;Synthetic data. (n.d.). ''McGraw-Hill Dictionary of Scientific and Technical Terms''. Retrieved November 29, 2009, from Answers.com Web site: [http://www.answers.com/topic/synthetic-data]&lt;/ref&gt; where Craig S. Mullins, an expert in data management, defines production data as "information that is persistently stored and used by professionals to conduct business processes.".&lt;ref name="Mullins"&gt;Mullins, Craig S. (2009, February 5). ''What is Production Data?'' Message posted to http://www.neon.com/blog/blogs/cmullins/archive/2009/02/05/What-is-Production-Data_3F00_.aspx&lt;/ref&gt;

The creation of synthetic data is an involved process of data [[Anonymity|anonymization]]; that is to say that synthetic data is a [[subset]] of anonymized data.&lt;ref name="MachanavajjhalaEtAl"&gt;{{Cite journal
  | title = Privacy: Theory meets Practice on the Map
  | journal = 2008 IEEE 24th International Conference on Data Engineering
  | doi = 10.1109/ICDE.2008.4497436
  | pages = 277–286
  | year = 2008
  | last1 = MacHanavajjhala
  | first1 = Ashwin
  | last2 = Kifer
  | first2 = Daniel
  | last3 = Abowd
  | first3 = John
  | last4 = Gehrke
  | first4 = Johannes
  | last5 = Vilhuber
  | first5 = Lars}}&lt;/ref&gt; Synthetic data is used in a variety of fields as a filter for information that would otherwise compromise the [[confidentiality]] of particular aspects of the data. Many times the particular aspects come about in the form of human information (i.e. name, home address, [[IP address]], telephone number, social security number, credit card number, etc.).

== Usefulness ==

Synthetic data are generated to meet specific needs or certain conditions that may not be found in the original, real data.  This can be useful when designing any type of system because the synthetic data are used as a simulation or as a theoretical value, situation, etc.  This allows us to take into account unexpected results and have a basic solution or remedy, if the results prove to be unsatisfactory. Synthetic data are often generated to represent the authentic data and allows a baseline to be set.&lt;ref name="Barse"&gt;Barse, E.L., Kvarnström, H., &amp; Jonsson, E. (2003). ''Synthesizing test data for fraud detection systems.'' Manuscript submitted for publication, Department of Computer Engineering, Chalmbers University of Technology, Göteborg, Sweden. Retrieved from http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=1254343&amp;isnumber=28060&lt;/ref&gt; Another use of synthetic data is to protect privacy and confidentiality of authentic data. As stated previously, synthetic data is used in testing and creating many different types of systems; below is a quote from the abstract of an article that describes a software that generates synthetic data for testing fraud detection systems that further explains its use and importance.
"This enables us to create realistic behavior profiles for users and attackers. The data is used to train the [[fraud]] detection system itself, thus creating the necessary adaptation of the system to a specific environment."&lt;ref name="Barse"/&gt;

==History==
The history of the generation of synthetic data dates back to 1993. In 1993, the idea of original fully synthetic data was created by [[Donald Rubin|Rubin]].&lt;ref name="Rubin1993"&gt;{{Cite journal
  | authorlink = Rubin, Donald B.
  | title = Discussion: Statistical Disclosure Limitation
  | journal = Journal of Official Statistics
  | volume = 9
  | pages = 461–468
  | year = 1993}}
&lt;/ref&gt; Rubin originally designed this to synthesize the Decennial Census long form responses for the short form households. He then released samples that did not include any actual long form records - in this he preserved anonymity of the household.&lt;ref name="Abowd"&gt;
{{Cite web
  | last = Abowd
  | first = John M.
  | title = Confidentiality Protection of Social Science Micro Data: Synthetic Data and Related Methods. [Powerpoint slides]
  | url=http://www.idre.ucla.edu/events/PPT/2006_01_30_abowd_UCLA_synthetic_data_presentation.ppt
  | accessdate = 17 February 2011 }} 
&lt;/ref&gt; Later that year, the idea of original partially synthetic data was created by Little. Little used this idea to synthesize the sensitive values on the public use file.&lt;ref name="Little"&gt;{{Cite journal
  | authorlink = Little, Rod
  | title = Statistical Analysis of Masked Data
  | journal = Journal of Official Statistics
  | volume = 9
  | pages = 407–426
  | year = 1993}}
&lt;/ref&gt;

In 1994, [[Stephen Fienberg|Fienberg]] came up with the idea of critical refinement, in which he used a parametric posterior predictive distribution (instead of a Bayes bootstrap) to do the sampling.&lt;ref name="Abowd"/&gt; Later, other important contributors to the development of synthetic data generation are [[Trivellore Raghunathan|Raghunathan]], [[Jerry Reiter|Reiter]], [[Donald Rubin|Rubin]], [[John M. Abowd|Abowd]], [[Jim Woodcock|Woodcock]]. Collectively they came up with a solution for how to treat partially synthetic data with missing data. Similarly they came up with the technique of Sequential Regression Multivariate [[Imputation (statistics)|Imputation]].&lt;ref name="Abowd"/&gt;

==Applications==
Synthetic data are used in the process of [[data mining]].  Testing and training [[fraud]] detection systems, confidentiality systems and any type of system is devised using synthetic data. As described previously, synthetic data may seem as just a compilation of “made up” data, but there are specific algorithms and generators that are designed to create realistic data.&lt;ref name="Deng"&gt;Deng, R. (2002). ''Information and Communications Security''. Proceedings of the 4th International Conference, ICICS 2002 Singapore, December 2002. Retrieved from https://books.google.com/books?id=6mod7enQa8cC&amp;pg=PA265&amp;dq=%22synthetic+data%22#v=onepage&amp;q=%22synthetic%20data%22&amp;f=false&lt;/ref&gt; This synthetic data assists in teaching a system how to react to certain situations or criteria. Researcher doing [[clinical trials]] or any other research may generate synthetic data to aid in creating a baseline for future studies and testing.  For example, intrusion detection software is tested using synthetic data. This data is a representation of the authentic data and may include intrusion instances that are not found in the authentic data. The synthetic data allows the software to recognize these situations and react accordingly. If synthetic data was not used, the software would only be trained to react to the situations provided by the authentic data and it may not recognize another type of intrusion.&lt;ref name="Barse"/&gt;

Synthetic data is also used to protect the [[privacy]] and [[confidentiality]] of a set of data. Real data contains personal/private/confidential information that a programmer, software creator or research project may not want to be disclosed.&lt;ref name="Abowd2"&gt;Abowd, J.M., &amp; Lane, J. (2004). ''New Approaches to Confidentiality Protection: Synthetic Data, Remote Access and Research Data Centers''. Manuscript submitted for publication, Cornell Institute for Social and Economic Research (CISER), Cornell University, Ithica, New York. Retrieved from http://www.springerlink.com/content/27nud7qx09qurg3p/fulltext.pdf&lt;/ref&gt; Synthetic data holds no personal information and cannot be traced back to any individual; therefore, the use of synthetic data reduces confidentiality and privacy issues.

==Calculations==
Researchers test the framework on synthetic data, which is "the only source of ground truth on which they can objectively assess the performance of their [[algorithm]]s".&lt;sup&gt;10&lt;/sup&gt;

"Synthetic data can be generated with random orientations and positions."&lt;sup&gt;8&lt;/sup&gt;  Datasets can be get fairly complicated. A more complicated dataset can be generated by using a synthesizer build.  To create a synthesizer build, first use the original data to create a model or equation that fits the data the best. This model or equation will be called a synthesizer build. This build can be used to generate more data.&lt;sup&gt;9&lt;/sup&gt;

Constructing a synthesizer build involves constructing a [[statistical model]].  In a [[linear regression]] line example, the original data can be plotted, and a best fit [[linear regression|linear line]] can be created from the data.  This [[linear regression|line]] is a synthesizer created from the original data.  The next step will be generating more synthetic data from the synthesizer build or from this linear line equation.  In this way, the new data can be used for studies and research, and it protects the [[confidentiality]] of the original data.&lt;sup&gt;9&lt;/sup&gt;

David Jensen from the Knowledge Discovery Laboratory mentioned how to generate synthetic data in his "Proximity 4.3 Tutorial" chapter 6: "Researchers frequently need to explore the effects of certain data characteristics on their [[data model]]." To help construct [[data set|datasets]] exhibiting specific properties, such as [[autocorrelation|auto-correlation]] or degree disparity, proximity can generate synthetic data having one of several types of graph structure&lt;sup&gt;10&lt;/sup&gt;:[[random graph]]s that is generated by some [[random process]];[[lattice graph]]s having a ring structure;[[lattice graph]]s having a grid structure, etc.
In all cases, the data generation process follows the same process:
1.	Generate the empty [[Graph (data structure)|graph structure]].
2.	Generate [[Attribute-value system|attribute values]] based on user-supplied prior probabilities.

Since the [[Attribute-value system|attribute values]] of one object may depend on the [[Attribute-value system|attribute values]] of related objects, the attribute generation process assigns values collectively.&lt;sup&gt;10&lt;/sup&gt;

==References==
{{Reflist}}
* Wang, A, Qiu, T, &amp; Shao, L. (2009). ''A Simple Method of Radial Distortion Correction with Centre of Distortion Estimation''. 35. Retrieved from http://www.springerlink.com/content/8180144q56t30314/fulltext.pdf
* Duncan, G. (2006). ''Statistical confidentiality: Is Synthetic Data the Answer?'' Retrieved from http://www.idre.ucla.edu/events/PPT/2006_02_13_duncan_Synthetic_Data.ppt
* Jensen, D. (2004). ''Proximity 4.3 Tutorial Chapter 6.'' Retrieved from http://kdl.cs.umass.edu/proximity/documentation/tutorial/ch06s09.html
* Jackson, C, Murphy, R, &amp; Kovaˇcevic´, J. (2009). ''Intelligent Acquisition and Learning of Fluorescence Microscope Data Models.'' 18(9), Retrieved from http://www.andrew.cmu.edu/user/jelenak/Repository/08_JacksonMK.pdf
* {{cite book| author=Adam Coates and Blake Carpenter and Carl Case and Sanjeev Satheesh and Bipin Suresh and Tao Wang and David J. Wu and Andrew Y. Ng| chapter=Text Detection and Character Recognition in Scene Images with Unsupervised Feature Learning| title=ICDAR| year=2011| pages=440–445| accessdate=13 May 2014}}

==External links==
* The "DataGenerator" a model-based synthetic data generator: http://finraos.github.io/DataGenerator/
* The ''datgen'' synthetic data generator: http://www.datasetgenerator.com
* Fienberg, S. E. (1994). "Conflicts between the needs for access to statistical information and demands for confidentiality," Journal of Official Statistics 10, 115–132.
* Little, R (1993). "Statistical Analysis of Masked Data," Journal of Official Statistics, 9, 407-426.
* Raghunathan, T.E., Reiter, J.P., and Rubin, D.B. (2003). "Multiple Imputation for Statistical Disclosure Limitation," Journal of Official Statistics, 19, 1-16.
* Reiter, J.P. (2004). "Simultaneous Use of Multiple Imputation for Missing Data and Disclosure Limitation," Survey Methodology, 30, 235-242.

 	
{{FOLDOC}}
{{Statistics}}

[[Category:Data]]
[[Category:Computer data]]
[[Category:Data management]]</text>
      <sha1>rs61b9y7dwuo3c1rxzo8r1x1o9w0qal</sha1>
    </revision>
  </page>
  <page>
    <title>Reference table</title>
    <ns>0</ns>
    <id>1785206</id>
    <revision>
      <id>731204525</id>
      <parentid>643262171</parentid>
      <timestamp>2016-07-23T19:28:42Z</timestamp>
      <contributor>
        <ip>31.19.67.84</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1628" xml:space="preserve">{{Unreferenced|auto=yes|date=December 2009}}

A '''reference table''' (or table of reference) may mean a set of references that an author may have cited or gained inspiration from whilst writing an article, similar to a [[bibliography]].

It can also mean an [[Table (information)|information table]] that is used as a quick and easy reference for things that are difficult to remember such as comparing [[Imperial unit|imperial]] with [[SI|metric]] measurements. This kind of data is known as [[reference data]].

In the context of [[database design]] a reference table is a table into which an [[enumeration|enumerated]] set of possible values of a certain field data type is divested. For example, in a [[relational model|relational database model]] of a warehouse the entity 'Item' may have a field called 'status' with a predefined set of values such as 'sold', 'reserved', 'out of stock'. In a purely designed database these values would be divested into an extra entity or Reference Table called 'status' in order to achieve [[database normalisation]]. The entity 'status' in this case has no true representative in the real world but rather would an exceptional case where the attribute of a certain database entity is divested into its own table. The advantage of doing this is that internal functionality and optional conditions within the database and the software which utilizes it are easier to modify and extend on that particular aspect.  Establishing an enterprise-wide view of reference tables is called [[master data management]].

{{DEFAULTSORT:Reference Table}}
[[Category:Data management]]

{{Publish-stub}}</text>
      <sha1>63awsp0wpwvpn87vrjwcajwult32iul</sha1>
    </revision>
  </page>
  <page>
    <title>Uniform information representation</title>
    <ns>0</ns>
    <id>1610862</id>
    <revision>
      <id>645177889</id>
      <parentid>645176641</parentid>
      <timestamp>2015-02-01T16:37:38Z</timestamp>
      <contributor>
        <username>Bhny</username>
        <id>285109</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1090" xml:space="preserve">{{unreferenced|date=December 2009}}

'''Uniform information representation'''  allows information from several realms or disciplines to be displayed and worked with as if it came from the same realm or discipline.  It takes information from a number of sources, which may have used different methodologies and metrics in their data collection, and builds a single large collection of information, where some records may be more complete than others across all fields of data

Uniform information representation is particularly important in the fields of [[Enterprise Information Integration]] (EII) and [[Electronic Data Interchange]] (EDI), where different departments of a large organization may have collected information for different purposes, with different labels and units, until one department realized that data already collected by those other departments could be re-purposed for their own needs—saving the enterprise the effort and cost of re-collecting the same information.

{{DEFAULTSORT:Uniform Information Representation}}
[[Category:Data management]]

{{Comp-sci-stub}}</text>
      <sha1>f1lgsks1385u4n97tu1jj3qnap3r1h8</sha1>
    </revision>
  </page>
  <page>
    <title>Nonlinear medium</title>
    <ns>0</ns>
    <id>676568</id>
    <revision>
      <id>715774947</id>
      <parentid>681940813</parentid>
      <timestamp>2016-04-17T22:59:07Z</timestamp>
      <contributor>
        <username>DiscantX</username>
        <id>23278095</id>
      </contributor>
      <comment>/* top */ {{Distinguish|Non-linear media}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="455" xml:space="preserve">{{Distinguish|Non-linear media}}
{{Unreferenced|date=December 2009}}
A '''nonlinear medium''' is one which is intended to be accessed in a nonlinear fashion. It is the opposite of a [[Linear_medium|Linear Medium]]. 

Examples include:
* a [[hard drive]]
* a [[newspaper]]
* a [[phone book]]
* a [[dictionary]]

==See also==
*[[nonlinear]]
*[[linear medium]]
*[[Random access]]

{{DEFAULTSORT:Nonlinear Medium}}
[[Category:Data management]]


{{Tech-stub}}</text>
      <sha1>3kbgt07t2eovn9j22zfrgczdb1u1gck</sha1>
    </revision>
  </page>
  <page>
    <title>Point-in-time recovery</title>
    <ns>0</ns>
    <id>4576703</id>
    <revision>
      <id>733104708</id>
      <parentid>733104655</parentid>
      <timestamp>2016-08-05T11:49:10Z</timestamp>
      <contributor>
        <ip>77.56.53.183</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1105" xml:space="preserve">{{Redirect|PITR|other uses|Pitr (disambiguation){{!}}Pitr}}
{{Unreferenced stub|auto=yes|date=December 2009}}

'''Point-in-time recovery''' ('''PITR''') in the context of [[computer]]s involves systems whereby an administrator can restore or recover a set of data or a particular setting from a time in the past. Note for example [[Windows XP]]'s capability to restore operating-system settings from a past date (before data corruption occurred, for example).  [[Time Machine (OS X)|Time Machine]] for Mac OS X provides another example of point-in-time recovery.

Once PITR logging starts for a PITR-capable [[database]], a [[database administrator]] can restore that database from [[backup]]s to the state that it had at any time since.

==External links==
* [http://blog.ganneff.de/blog/2008/02/15/postgresql-continuous-archivin.html PostgreSQL Continuous Archiving and Point-In-Time Recovery (PITR) blog/article]
* [http://dev.mysql.com/doc/refman/5.5/en/point-in-time-recovery.html MySQL 5.5 Point in Time Recovery]

{{DEFAULTSORT:Point-In-Time Recovery}}
[[Category:Data management]]


{{Compu-stub}}</text>
      <sha1>ikx4o7a3k3p3hdjnlbualuxqaaro0s1</sha1>
    </revision>
  </page>
  <page>
    <title>Comparison of OLAP Servers</title>
    <ns>0</ns>
    <id>24523966</id>
    <revision>
      <id>760530402</id>
      <parentid>760529770</parentid>
      <timestamp>2017-01-17T15:56:31Z</timestamp>
      <contributor>
        <ip>62.84.129.10</ip>
      </contributor>
      <comment>/* OLAP distinctive features */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="23549" xml:space="preserve">The following tables compare general and technical information for a number of [[online analytical processing]] (OLAP) servers supporting MDX language. Please see the individual products articles for further information.

==General information==
{| class="wikitable sortable"  style="font-size: 100%; text-align: center; width: auto;"
|-
! OLAP Server
! Company
! Website
! Latest stable version
! [[Software license]]
! License Pricing
|-
! [[TM1|IBM Cognos TM1]]
| [[IBM]]
|&lt;ref&gt;{{cite web|url=http://www-01.ibm.com/software/data/cognos/index.html|title=Cognos Business Intelligence and Financial Performance Management}}&lt;/ref&gt;
| 10.2.2 FP4
| [[Proprietary software|Proprietary]]
| -
|-
! [[Essbase]]
| [[Oracle Corporation|Oracle]]
|&lt;ref&gt;{{cite web|url=http://www.oracle.com/us/solutions/ent-performance-bi/business-intelligence/essbase/index.html|title=Oracle Essbase}}&lt;/ref&gt;
| 11.1.2.4
| [[Proprietary software|Proprietary]]
| [http://www.oracle.com/us/corporate/pricing/index.htm]
|-
! [[icCube]]
| [[icCube]]
|&lt;ref&gt;{{cite web|url=http://www.icCube.com|title=icCube OLAP Server}}&lt;/ref&gt;
| 6.0
| [[Proprietary software|Proprietary]]
| community/[http://www.iccube.com//prices]
|-
! [[Jedox|Jedox OLAP Server]]
| [[Jedox]]
|&lt;ref&gt;{{cite web|url=http://www.jedox.com/en/home/overview.html |title=Jedox AG Business Intelligence |deadurl=yes |archiveurl=https://web.archive.org/web/20100514124342/http://www.jedox.com:80/en/home/overview.html |archivedate=2010-05-14 |df= }}&lt;/ref&gt;
| 7.0
| [[GNU General Public License|GPL]] v2 or [[EULA]], [[Proprietary software|Proprietary]]
| -
|-
 ! Infor BI OLAP Server
| [[Infor]]
|&lt;ref&gt;{{cite web|url=http://www.infor.com|title=Infor}}&lt;/ref&gt;
| 10.6.0
| [[Proprietary software|Proprietary]]
| -
|-
! [[Microsoft Analysis Services]]
| [[Microsoft]]
|&lt;ref&gt;{{cite web|url=http://www.microsoft.com/Sqlserver/2008/en/us/analysis-services.aspx|title=Microsoft SQL Server 2008 Analysis Services}}&lt;/ref&gt;
| 2016
| [[Proprietary software|Proprietary]]
| [http://www.microsoft.com/sqlserver/2008/en/us/pricing.aspx]
|-
! MicroStrategy Intelligence Server
| [[MicroStrategy]]
|&lt;ref&gt;{{cite web|url=http://www.microstrategy.com/Software/Products/Intelligence_Server/|title=MicroStrategy Intelligence Server}}&lt;/ref&gt;
| 9
| [[Proprietary software|Proprietary]]
| -
|-
! [[Mondrian OLAP server]]
| [[Pentaho]]
|&lt;ref&gt;{{cite web|url=http://mondrian.pentaho.org|title=Pentaho Analysis Services: Mondrian Project}}&lt;/ref&gt;
| 3.7
| [[Eclipse Public License|EPL]]
| free
|-
! [[Oracle OLAP|Oracle Database OLAP Option]]
| [[Oracle Corporation|Oracle]]
|&lt;ref&gt;{{cite web|url=http://www.oracle.com/technology/documentation/olap.html|title=Oracle OLAP Documentation}}&lt;/ref&gt;
| 11g R2
| [[Proprietary software|Proprietary]]
| [http://www.oracle.com/us/corporate/pricing/index.htm]
|-
! [[SAS System|SAS OLAP Server]]
| [[SAS Institute]]
|&lt;ref&gt;{{cite web|url=http://www.sas.com/technologies/dw/storage/mddb/index.html|title=SAS OLAP Server}}&lt;/ref&gt;
| 9.4
| [[Proprietary software|Proprietary]]
| -
|-
! [[SAP NetWeaver Business Intelligence|SAP NetWeaver BW]]
| [[SAP AG|SAP]]
|&lt;ref&gt;{{cite web|url=http://www.sap.com/usa/platform/netweaver/components/businesswarehouse/index.epx |title=Components &amp; Tools}}&lt;/ref&gt;
| 7.30
| [[Proprietary software|Proprietary]]
| -
|-
! [[Cubes (OLAP server)|Cubes]]
| [[Open source|Open source community]]
|&lt;ref&gt;{{cite web|url=http://cubes.databrewery.org|title=Cubes – Lightweight OLAP Python Toolkit}}&lt;/ref&gt;
| 1.0.1
| [[MIT License|MIT]]
| -
|}

==Data storage modes==
{| class="wikitable sortable"  style="font-size: 100%; text-align: center; width: auto;"
|-
! OLAP Server
! [[MOLAP]]
! [[ROLAP]]
! [[HOLAP]]
! In-Memory
! Offline
|-
! [[TM1|IBM Cognos TM1]]
| {{Yes}}
| {{No}}
| {{No}}
| {{No}}
| {{Yes| [http://www-01.ibm.com/support/knowledgecenter/SSVJ22_10.2.2/com.ibm.swg.ba.cognos.dsk_ug.10.2.2.doc/t_dsk_maintain_offline.html%23t_dsk_maintain_offline Cognos Insight Distributed mode]}}
|-
! [[Essbase]]
| {{Yes}}
| {{No}}
| {{No}}
| {{No}}
|
|-
! [[icCube]]
| {{Yes}}
| {{No}}
| {{No}}
| 
| {{Yes | [http://www.iccube.com/support/documentation/user_guide/using/offline_cubes.html Offline Cubes]}}
|-
! Infor BI OLAP Server
| {{Yes}}
| {{No}}
| {{No}}
| {{Yes}}
|Local cubes
|-
! [[Jedox|Jedox OLAP Server]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{No}}
| {{No}}
|-
! [[Microsoft Analysis Services]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes|Local cubes,&lt;br /&gt; [[PowerPivot|PowerPivot for Excel]],&lt;br /&gt;[[Power BI|Power BI Desktop]]}}
|-
! [[MicroStrategy|MicroStrategy Intelligence Server]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{No}}
| {{Yes|[http://www.microstrategy.com/Software/Products/User_Interfaces/Office/ MicroStrategy Office],&lt;br /&gt; [http://www.microstrategy.com/Software/Products/Service_Modules/Report_Services/ Dynamic Dashboards]}}
|-
! [[Mondrian OLAP server]]
| {{No}}
| {{Yes}}
| {{No}}
| {{No}}
|
|-
! [[Oracle OLAP|Oracle Database OLAP Option]]
| {{No}}
| {{Yes}}
| {{No}}
| {{No}}
|
|-
! [[SAS System|SAS OLAP Server]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{No}}
|
|-
! [[IBM Cognos BI]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{No}}
|
|-
! [[SAP NetWeaver Business Intelligence|SAP NetWeaver BW]]
| {{Yes}}
| {{Yes}}
| {{No}}
| {{No}}
|
|-
! [[Cubes (OLAP server)]]
| {{No}}
| {{Yes}}
| {{No}}
|
|
|-
|}

==APIs and query languages==
APIs and query languages OLAP servers support.
{| class="wikitable sortable"  style="font-size: 100%; text-align: center; width: auto;"
|-
! OLAP Server
! [[XML for Analysis]]
! [[OLE DB for OLAP]]
! [[Multidimensional Expressions|MDX]]
! [[Stored procedures]]
! Custom functions
! [[SQL]]
! [[LINQ]]&lt;ref name="linq"&gt;{{cite web|url=http://agiledesignllc.com/products|title=SSAS Entity Framework Provider}}&lt;/ref&gt;
! Visualization
! [[JSON]]
! [[REST API]]
|-
! [[Essbase]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{No}}
| {{Yes}}
| SmartView (Excel-AddIn), WebAnalysis, Financial Reports
| {{dunno}}
| {{dunno}}
|-
! [[icCube]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes|[[Java (programming language)|Java]],&lt;ref&gt;{{cite web|url=http://www.iccube.com/support/documentation/mdx_integration/java_integration.html|title=icCube Java integration documentation|publisher=[[icCube]]}}&lt;/ref&gt; [[R (programming language)|R]]&lt;ref&gt;{{cite web|url=http://www.iccube.com/support/documentation/mdx_integration/r_integration.html|title=icCube R language integration documentation|publisher=[[icCube]]}}&lt;/ref&gt;}}
| {{Yes}}
| {{No}}
| {{Yes}}
| {{Yes|[[Java (programming language)|Java]], [[Javascript]]}}
| {{dunno}}
| {{dunno}}
|-
! Infor BI OLAP Server
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes|OLAP Rules, Push Rules, Application Engine}}
| {{Yes}}
| {{Yes}}
| {{No}}
| {{Yes|Application Studio}}
| {{dunno}}
| {{dunno}}
|-
! [[Jedox|Jedox OLAP Server]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes|Cube Rules, SVS Triggers}}
| {{Yes}}
| {{No}}
| {{Yes}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|-
! [[Microsoft Analysis Services]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes|[[.NET framework|.NET]]}}&lt;ref&gt;{{cite web|url=http://msdn.microsoft.com/en-us/library/ms176113.aspx|title=SQL Server 2008 Books Online (October 2009)Defining Stored Procedures|publisher=[[MSDN]]}}&lt;/ref&gt;
| {{Yes}}&lt;ref&gt;{{cite web|url=http://msdn.microsoft.com/en-us/library/ms145486.aspx|title=SQL Server 2008 Books Online (October 2009)Using Stored Procedures|publisher=[[MSDN]]}}&lt;/ref&gt;
| {{Yes}}&lt;ref&gt;{{cite web|url=http://support.microsoft.com/kb/218592/en-gb|title=How to perform a SQL Server distributed query with OLAP Server|publisher=[[MSDN]]}}&lt;/ref&gt;
| {{Yes}}
| {{Yes|Microsoft Excel, SharePoint, Microsoft Power BI, and 70+ other visualization tools}}&lt;ref&gt;{{cite web|url=http://www.ssas-info.com/analysis-services-client-tools-frontend|title=A collection of SSAS frontend tools|publisher=[[SSAS-info.com]]}}&lt;/ref&gt;
| {{dunno}}
| {{dunno}}
|-
! [[MicroStrategy|MicroStrategy Intelligence Server]]
| {{Yes}}
| {{No}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|-
! [[Mondrian OLAP server]]
| {{Yes}}
| {{Yes}}&lt;ref&gt;{{cite web|url=http://www.simba.com/news/Pentaho-Simba-Partner-for-Excel-Connectivity.htm|title=Pentaho and Simba Technologies Partner to Bring World's Most Popular Open Source OLAP Project to Microsoft Excel Users}}&lt;/ref&gt;
| {{Yes}}
| {{Yes}}
| {{Yes}}&lt;ref&gt;{{cite web|url=http://mondrian.pentaho.org/documentation/schema.php#User-defined_function|title=How to Define a Mondrian Schema|publisher=Pentaho}}&lt;/ref&gt;
| {{Yes}}
| {{Yes}}
| {{No}}
| {{dunno}}
| {{dunno}}
|-
! [[Oracle OLAP|Oracle Database OLAP Option]]
| {{No}}
| {{Yes}}&lt;ref name="oraclemdx"&gt;{{cite web|url=http://www.oracle.com/us/corporate/press/036550|title=Oracle and Simba Technologies Introduce MDX Provider for Oracle OLAP}}&lt;/ref&gt;
| {{Yes}}&lt;ref name="oraclemdx"/&gt;
| {{Yes|[[Java (programming language)|Java]], PL/SQL, [[OLAP DML]]}}
| {{Yes}}
| {{Yes}}&lt;ref&gt;{{cite web|url=http://www.oracle.com/technology/products/bi/olap/11g/demos/olap_sql_demo.html|title=Querying Oracle OLAP Cubes: Fast Answers to Tough Questions Using Simple SQL}}&lt;/ref&gt;
| {{No}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|-
! [[SAS System|SAS OLAP Server]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{No}}
| {{No}}
| {{No}}
| {{Yes}}
| {{Yes|Web Report Studio}}
| {{dunno}}
| {{dunno}}
|-
! [[SAP NetWeaver Business Intelligence|SAP NetWeaver BW]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{No}}
| {{Yes}}
| {{No}}
| {{Yes}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|-
! [[TM1|Cognos TM1]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{No}}
| {{Yes}}
| TM1 Web/TM1 Contributor, IBM Cognos Insight, IBM Performance Modeler, IBM Cognos Cafe for Excel, Cognos BI, TM1 Perspectives for Excel
| {{dunno}}
| {{Yes}}
|-
! [[Cubes (OLAP server)|Cubes]]
| {{No}}
| {{No}}
| {{No}}
| {{No}}
| {{Yes}}
| {{No}}
| {{No}}
| Cubes Viewer&lt;ref&gt;{{cite web|url=https://github.com/jjmontesl/cubesviewer|title=Cubes Viewer|publisher=jjmontes}}&lt;/ref&gt;
| {{Yes}}
| {{dunno}}
|}

==OLAP distinctive features==

A list of OLAP features that are not supported by all vendors. All vendors support features such as parent-child, multilevel hierarchy, drilldown.

Data processing, management and performance related features:

{| class="wikitable sortable"  style="font-size: 100%; text-align: center; width: auto;"
|-
!OLAP Server
!Real Time
!Write-back
!Partitioning
!Usage Based Optimizations
!Load Balancing and Clustering
|-
! [[Essbase]]
| {{No}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
|-
! [[icCube]]
| {{Yes}}&lt;ref&gt;{{cite web|url=http://www.iccube.com/support/documentation/user_guide/walkthrough/walkthrough_rt.html|title=icCube Real Time walkthrough}}&lt;/ref&gt;
| {{Yes}}&lt;ref&gt;{{cite web|url=http://www.iccube.com/support/documentation/mdx/Update%20Cube.html|title=icCube Writeback/Update Cube}}&lt;/ref&gt;
| {{Yes}}&lt;ref&gt;{{cite web|url=http://www.iccube.com/support/documentation/user_guide/reference/partitioning_edition.html|title=icCube Partitioning}}&lt;/ref&gt;
| {{dunno}}
| {{dunno}}
|-
! Infor BI OLAP Server
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{dunno}}
| {{dunno}}
|-
! [[Jedox|Jedox OLAP Server]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{dunno}}
| {{dunno}}
|-
! [[Microsoft Analysis Services]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
|-
! [[MicroStrategy|MicroStrategy Intelligence Server]]
| {{dunno}}
| {{Yes}}&lt;ref&gt;{{cite web|url=http://www.microstrategy.com/Software/Products/Dev_Tools/SDK/extensions.asp|title=Common Extensions of the MicroStrategy Platform}}&lt;/ref&gt;
| {{Yes}}
| {{dunno}}
| {{dunno}}
|-
! [[Mondrian OLAP server]]
| {{Yes}}
| {{Yes2 | Planned}}
| {{Yes}}
| {{dunno}}
| {{dunno}}
|-
! [[Oracle OLAP|Oracle Database OLAP Option]]
| {{dunno}}
| {{Yes}}
| {{Yes}}
| {{No}}
| {{dunno}}
|-
! [[IBM Cognos TM1]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{dunno}}
| {{dunno}}
|-
! [[IBM Cognos BI]]
| {{Yes}}
| {{No}}
| {{Yes}}
| {{Yes}}
| {{dunno}}
|-
! [[SAS OLAP Server]]
| ?
| {{Yes}}
| {{Yes}}
| {{dunno}}
| {{dunno}}
|-
! [[SAP NetWeaver Business Intelligence|SAP NetWeaver BW]]
| ?
| {{Yes}}
| {{Yes}}
| {{dunno}}
| {{dunno}}
|-
! [[Cubes (OLAP server)|Cubes]]
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|}

Data modeling features:
{| class="wikitable sortable"  style="font-size: 100%; text-align: center; width: auto;"
|-
!OLAP Server
!Semi-additive measures
!Many-to-Many 
!Multi-Cube Model
!Perspectives
!KPI
!Translations
!Named Sets
!Multi-attribute Hierarchies
!Actions
|-
! [[Essbase]]
| {{Yes}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{dunno}}
|-
! [[icCube]]
| {{Yes}}&lt;ref&gt;{{cite web|url=http://www.iccube.com/support/documentation/user_guide/schemas_cubes/facts_aggregation.html|title=icCube Aggregatin types}}&lt;/ref&gt;
| {{Yes}}&lt;ref&gt;{{cite web|url=http://www.iccube.com/support/documentation/user_guide/schemas_cubes/facts_many2many.html|title=icCube Many-to-Many}}&lt;/ref&gt;
| {{Yes}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|-
! Infor BI OLAP Server
| {{Yes}}
| {{dunno}}
| {{Yes}}
| {{dunno}}
| {{Yes}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|-
! [[Jedox|Jedox OLAP Server]]
| {{Yes}}
| {{Yes}}
| {{dunno}}
| {{Yes}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|-
! [[Microsoft Analysis Services]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
|-
! [[MicroStrategy|MicroStrategy Intelligence Server]]
| {{Yes}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|-
! [[Mondrian OLAP server]]
| {{Yes}}&lt;ref&gt;{{cite web|url=http://jira.pentaho.com/browse/MONDRIAN-962|title=Support for Non-Additive and Semi-Additive Measures}}&lt;/ref&gt;
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|-
! [[Oracle OLAP|Oracle Database OLAP Option]]
| {{Yes}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|-
! [[IBM Cognos TM1]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|-
! [[IBM Cognos BI]]
| {{Yes}}
| {{Yes}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{Yes}}
| {{Yes}}
| {{dunno}}
|-
! [[SAS OLAP Server]]
| {{Yes}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|-
! [[SAP NetWeaver Business Intelligence|SAP NetWeaver BW]]
| {{Yes}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|-
! [[Cubes (OLAP server)|Cubes]]
| {{dunno}}
| {{Yes}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|}

==System limits==
{| class="wikitable sortable"  style="font-size: 100%; text-align: center; width: auto;"
|-
!OLAP Server
!# cubes
!# measures
!# dimensions
!# dimensions in cube
!# hierarchies in dimension
!# levels in hierarchy
!# dimension members
|-
! [[Essbase]]&lt;ref&gt;{{cite web|url=http://docs.oracle.com/cd/E57185_01/epm.1112/essbase_db/frameset.htm?limits.html|title=Essbase Server Limits|publisher=Oracle}}&lt;/ref&gt;
| ?
| ?
| ?
| 255
| 255
| ?
| 20,000,000 (ASO), 1,000,000 (BSO)
|-
! [[icCube]]&lt;!-- Java Integer, 32 bits --&gt;
| 2,147,483,647
| 2,147,483,647
| 2,147,483,647
| ?
| 2,147,483,647
| 2,147,483,647
| 2,147,483,647
|-
! Infor BI OLAP Server
| ?
| 10,000,000
| ?
| 30
| ?
| ?
| 10,000,000
|-
! [[Jedox|Jedox OLAP Server]]
| 2^32 (32 bits) / 2^64 (64 bits)
| 2^32
| 2^32 (32 bits) / 2^64 (64 bits)
| 250
| 2^32
| 2^32
| 2^32
|-
! [[Microsoft Analysis Services]]&lt;ref&gt;{{cite web|url=http://technet.microsoft.com/en-us/library/ms365363.aspx|title=SQL Server 2008 Books Online (October 2009)Maximum Capacity Specifications (Analysis Services - Multidimensional Data)|publisher=Microsoft}}&lt;/ref&gt;
| 2,147,483,647
| 2,147,483,647
| 2,147,483,647
| 2,147,483,647 (max. number of dimensions in a database)
| 2,147,483,647
| 2,147,483,647
| 2,147,483,647 (xOLAP)
Unrestricted (In-memory)
|-
! [[MicroStrategy|MicroStrategy Intelligence Server]]&lt;!-- Unrestricted by server - based on hardware limits, infinite it's not possible ;-) --&gt;
| Unrestricted{{efn|name=fn0}}
| Unrestricted{{efn|name=fn0}}
| Unrestricted{{efn|name=fn0}}
| ?
| Unrestricted{{efn|name=fn0}}
| Unrestricted{{efn|name=fn0}}
| Unrestricted{{efn|name=fn0}}
|-
! [[SAS System|SAS OLAP Server]]&lt;ref&gt;{{cite web|url=http://support.sas.com/documentation/cdl/en/olapug/63148/HTML/default/viewer.htm#p0m66bhcbgqwjen1jyfhf6woysu3.htm|title=SAS OLAP Cube Size Specifications}}&lt;/ref&gt;
| Unrestricted{{efn|name=fn0}}
| 1024
| 128
| ?
| 128
| 19
| 4,294,967,296
|-
! [[IBM Cognos TM1]]
| Unrestricted{{efn|name=fn0}}
| Unrestricted
| Unrestricted{{efn|name=fn0}}
| 256
| Unrestricted{{efn|name=fn0}}
| Unrestricted
| Unrestricted
|}
{{notelist|notes=
{{efn|name=fn0|Please update as 'unrestricted', is just not possible}}
}}

==Security==
{| class="wikitable sortable"  style="font-size: 100%; text-align: center; width: auto;"
|-
!rowspan="2"| OLAP Server
!rowspan="2"| Authentication
!rowspan="2"| Network encryption
!rowspan="2"| On-the-Fly{{efn|name=fn1}}
!colspan="3"| Data access
|-
!Cell security
!Dimension security
!Visual totals
|-
! [[Essbase]]
| {{Yes|Essbase authentication, [[LDAP]] authentication, [[Microsoft Active Directory]]}}
| {{Yes|[[Transport Layer Security|SSL]]}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{No}}
|-
! [[icCube]]
| {{Yes|HTTP Basic/Form Authentication, Windows SSO (NTLM,Kerberos)}}
| {{Yes|[[Transport Layer Security|SSL]]}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
|-
! Infor BI OLAP Server
| {{Yes|OLAP authentication, Infor Federation Services, [[LDAP]], [[Microsoft Active Directory]]}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{dunno}}
|-
! [[Jedox|Jedox OLAP Server]]
| {{Yes|Jedox authentication, [[LDAP]], [[Microsoft Active Directory]]}}
| {{Yes|[[Transport Layer Security|SSL]]}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{dunno}}
|-
! [[Microsoft Analysis Services]]
| {{Yes|[[NTLM]], [[Kerberos (protocol)|Kerberos]]}}
| {{Yes|[[Transport Layer Security|SSL]] and [[SSPI]]}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
|-
! [[MicroStrategy|MicroStrategy Intelligence Server]]
| {{Yes|Host authentication, database authentication, [[LDAP]], &lt;br /&gt;[[Microsoft Active Directory]], [[NTLM]], SiteMinder, Tivoli, SAP, [[Kerberos (protocol)|Kerberos]]}}
| {{Yes|[[Transport Layer Security|SSL]], AES&lt;ref&gt;[http://latam.microstrategy.com/Software/Products/Intelligence_Server/features.asp MicroStrategy Intelligence Server Features]&lt;/ref&gt;}}
| ?
| {{Yes}}
| {{Yes}}
| {{Yes}}
|-
! [[Oracle OLAP|Oracle Database OLAP Option]]
| {{Yes|Oracle Database authentication}}
| {{Yes|[[Transport Layer Security|SSL]]}}
| ?
| {{Yes}}
| {{Yes}}
| {{dunno}}
|-
! [[SAS System|SAS OLAP Server]]&lt;ref&gt;{{cite web|url=http://support.sas.com/documentation/cdl/en/mdxag/59575/HTML/default/a003230130.htm|title=SAS OLAP Security Totals and Permission Conditions}}&lt;/ref&gt;
| {{Yes|Host authentication,SAS token authentication, [[LDAP]], [[Microsoft Active Directory]]}}
| {{Yes}}&lt;ref&gt;{{cite web|url=http://support.sas.com/documentation/cdl/en/bisecag/61133/HTML/default/a003275910.htm|title=How to Change Over-the-Wire Encryption Settings for SAS Servers}}&lt;/ref&gt;
| ?
| {{Yes}}
| {{Yes}}
| {{Yes}}
|-
! [[IBM Cognos TM1]]
| {{Yes|Builtin, [[LDAP]], [[Microsoft Active Directory]], [[NTLM]], IBM Cognos BI authentication}}
| {{Yes|[[Transport Layer Security|SSL]]}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
|}
{{notelist|notes=
{{efn|name=fn1|On-the-Fly : The ability to define authentication dynamically via programmatic interfaces. New users do not require restarting the server or redefining the security.}}
}}

==Operating systems==
The OLAP servers can run on the following [[operating system]]s:
{| class="wikitable sortable"  style="font-size: 100%; text-align: center; width: auto;"
|-
! OLAP Server
! Windows
! Linux
! UNIX
! z/OS
! AIX
|-
! [[Essbase]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{No}}
|-
! [[icCube]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
|-
! Infor BI OLAP Server
| {{Yes}}
| {{No}}
| {{No}}
| {{No}}
|-
! [[Jedox|Jedox OLAP Server]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{No}}
|-
! [[Microsoft Analysis Services]]
| {{Yes}}
| {{No}}
| {{No}}
| {{No}}
|-
! [[MicroStrategy|MicroStrategy Intelligence Server]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{No}}
|-
! [[Mondrian OLAP server]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
|-
! [[Oracle OLAP|Oracle Database OLAP Option]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
|-
! [[SAS System|SAS OLAP Server]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
|-
! [[SAP NetWeaver Business Intelligence|SAP NetWeaver BW]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
|-
! [[IBM Cognos TM1]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{No}}
| {{Yes}}
|-
! [[Cubes (OLAP server)|Cubes]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{No}}
|}
&lt;cite id="os_java"&gt;Note (1):&lt;/cite&gt;The server availability depends on [[JVM|Java Virtual Machine]] not on the [[operating system]]&lt;/cite&gt;

==Support information==
{| class="wikitable sortable"  style="font-size: 100%; text-align: center; width: auto;"
|-
! OLAP Server
! Issue Tracking System
! Forum/Blog
! Roadmap
! Source code
|-
! [[Essbase]]
| {{Yes|[http://support.oracle.com myOracle Support]}}
| [http://forums.oracle.com/forums/main.jspa?categoryID=84]
| [http://communities.ioug.org/Portals/2/Oracle_Essbase_Roadmap_Sep_09.pdf]
| Closed
|-
! [[icCube]]
| {{Yes | [http://issues.iccube.com/ YouTrack]}}
| |[http://www.iccube.com/forum]
| 
| Open
|-
! Infor BI OLAP Server
| {{Yes|Infor Xtreme}}
| 
| Available upon request
| Closed
|-
! [[Jedox|Jedox OLAP Server]]
| {{Yes|[http://bugs.palo.net/mantis/main_page.php Mantis]}}
| [http://www.jedox.com/community/palo-forum/board.php?boardid=9]
|
| Open
|-
! [[Microsoft Analysis Services]]
| {{Yes|[https://connect.microsoft.com/SQLServer Connect]}}
| [http://social.msdn.microsoft.com/Forums/en-US/sqlanalysisservices/threads]
| -
| Closed
|-
! [[MicroStrategy|MicroStrategy Intelligence Server]]
| {{Yes | [https://resource.microstrategy.com/Support/MainSearch.aspx MicroStrategy Resource Center]}}
| [https://resource.microstrategy.com/Forum/]
| -
| Closed
|-
! [[Mondrian OLAP server]]
| {{Yes|[http://jira.pentaho.com/browse/MONDRIAN Jira]}}
| [http://forums.pentaho.org/forumdisplay.php?f=79]
| [http://mondrian.pentaho.org/documentation/roadmap.php]
| Open
|-
! [[Oracle OLAP|Oracle Database OLAP Option]]
| {{Yes|[http://support.oracle.com myOracle Support]}}
| [http://forums.oracle.com/forums/main.jspa?categoryID=84]
|
| Closed
|-
! [[SAS System|SAS OLAP Server]]
| {{Yes|[http://support.sas.com/forums/index.jspa Support]}}
| [http://blogs.sas.com/]
|
| Closed
|-
! [[SAP NetWeaver Business Intelligence|SAP NetWeaver BW]]
| {{Yes | [http://service.sap.com/ OSS]}}
| [http://forums.sdn.sap.com/index.jspa]
| [http://esworkplace.sap.com/socoview(bD1lbiZjPTAwMSZkPW1pbg==)/render.asp?id=2270EAD629814D05A7ECECECECC8D002&amp;fragID=&amp;packageid=DEE98D07DF9FA9F1B3C7001A64D3F462]
| Closed
|-
! [[IBM Cognos TM1]]
| {{Yes | [http://ibm.com/support/servicerequest/ IBM Service Request]}}
| [http://www.tm1forum.com/viewforum.php?f=3]
|
| Closed
|-
! [[Cubes (OLAP server)|Cubes]]
| {{Yes|[https://github.com/databrewery/cubes/issues Cubes – Github Issues]}}
| [https://groups.google.com/forum/#!forum/cubes-discuss]
| [https://github.com/DataBrewery/cubes/wiki/Roadmap]
| [https://github.com/DataBrewery/cubes Open]
|}

==See also==
* [[Cubes (OLAP server)|Cubes]] (light-weight open-source OLAP server)
* [[icCube]]
* [[Palo (OLAP database)]]

==References==
{{reflist}}

{{Data warehouse}}

{{DEFAULTSORT:Comparison Of Olap Servers}}
[[Category:Online analytical processing| ]]
[[Category:Software comparisons|OLAP Servers]]
[[Category:Data management]]
[[Category:Data warehousing products]]</text>
      <sha1>j25e3psitk0shtuvr6x15qlmex7gc2f</sha1>
    </revision>
  </page>
  <page>
    <title>Content migration</title>
    <ns>0</ns>
    <id>26350658</id>
    <revision>
      <id>732797153</id>
      <parentid>732794863</parentid>
      <timestamp>2016-08-03T09:07:53Z</timestamp>
      <contributor>
        <username>Erik Kennedy</username>
        <id>1115816</id>
      </contributor>
      <comment>Fixed typo</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5537" xml:space="preserve">{{Multiple issues|
{{primary sources|date=March 2011}}
{{cleanup|date=March 2011}}
}}

'''Content Migration''' is the process of moving information stored on a [[Web content management system]]  (CMS), [[Digital asset management]] (DAM), [[Document management system]] (DMS), or flat HTML based system to a new system. Flat HTML content can entail HTML files, [[Active Server Pages]] (ASP), [[JavaServer Pages]] (JSP), [[PHP]], or content stored in some type of [[HTML]]/[[JavaScript]] based system and can be either static or dynamic content.   

Content Migrations can solve a number of issues ranging from:
* Consolidation from one or more CMS systems into one system to allow for more centralized control, governance of content, and better   Knowledge    management and sharing.
* Reorganizing content due to mergers and acquisitions to assimilate as much content from the source systems for a unified look and feel.
* Converting content that has grown organically either in a CMS or Flat HTML and standardizing the formatting so standards can be applied for a unified branding of the content.

There are many ways to access the content stored in a CMS.  Depending on the CMS vendor they offer either an  [[Application programming interface]] (API), [[Web services]], rebuilding a record by writing [[SQL]] queries, [[XML]] exports, or through the web interface.

# The API&lt;ref name="refname1"/&gt; requires a developer to read and understand how to interact with the source CMS’s API layer then develop an application that extracts the content and stores it in a database, XML file, or Excel. Once the content is extracted the developer must read and understand the target CMS API and develop code to push the content into the new System.  The same can be said for Web Services.
# Most CMSs use a database to store and associate content so if no API exists the SQL programmer must reverse engineer the table structure.  Once the structure is reverse engineered, very complex SQL queries are written to pull all the content from multiple tables into an intermediate table or into some type of [[Comma-separated values]] (CSV) or XML file.   Once the developer has the files or database the developer must read and understand the target CMS API and develop code to push the content into the new System.  The same can be said for Web Services.
# XML export creates XML files of the content stored in a CMS but after the files are exported they need to be altered to fit the new scheme of the target CMS system.  This is typically done by a developer by writing some code to do the transformation.
# HTML files, JSP, ASP, PHP, or other application server file formats are the most difficult.  The  structure for Flat HTML files are based on a culmination of  folder structure, HTML file structure, and image locations.  In the early days of content migration, the developer had to use programming languages to parse the html files and save it as structured database, XML or CSV. Typically PERL, JAVA, C++, or C# were used because of the regular expression handling capability.  JSP, ASP, PHP, ColdFusion, and other Application Server technologies usually rely on server side includes to help simplify development but makes it very difficult to migrate content because the content is not assembled until the user looks at it in their web browser.  This makes is very difficult to look at the files and extract the content from the file structure.
# Web Scraping allows users to access most of the content directly from the Web User Interface.  Since a web interface is visual (this is the point of a CMS) some Web Scrapers leverage the UI to extract content and place it into a structure like a Database, XML, or CSV formats.  All CMSs, DAMs, and DMSs use  web interfaces so extracting the content for one or many source sites is basically the same process.  In some cases it is possible to push the content into the new CMS using the web interface but some CMSs use JAVA applets, or Active X Control which are not supported by most web scrapers.  In that case the developer must read and understand the target CMS API and develop code to push the content into the new System.  The same can be said for Web Services.
'''The basic content migration flow'''

1. Obtain an inventory of the content.&lt;br /&gt;
2. Obtain an inventory of Binary content like Images, PDFs, CSS files, Office Docs, Flash, and any binary objects.&lt;br /&gt;
3. Find any broken links in the content or content resources.&lt;br /&gt;
4. Determine the Menu Structure of the Content.&lt;br /&gt;
5. Find the parent/sibling connection to the content so the links to other content and resources are not broken when moving them.&lt;br /&gt;
6. Extract the Resources from the pages and store them into a Database or File structure.  Store the reference in a database or a File.&lt;br /&gt;
7. Extract the HTML content from the site and store locally.&lt;br /&gt;
8. Upload the resources to the new CMS either by using the API or the web interface and store the new location in a Database or XML.&lt;br /&gt;
9. Transform the HTML to meet the new CMSs standards and reconnect any resources.&lt;br /&gt;
10. Upload the transformed content into the new system.

== References ==
&lt;references&gt;
&lt;ref name="refname1"&gt;[http://msdn.microsoft.com/en-us/library/ms453426.aspx What the Content Migration APIs Are Not]&lt;/ref&gt;
&lt;/references&gt;

==External links==
* [http://www.cmswire.com/cms/web-publishing/no-small-task-migrating-content-to-a-new-cms-002437.php No Small Task: Migrating Content to a New CMS]

[[Category:Data management]]</text>
      <sha1>2ng6roje0im161ltyhe1e757gide2yi</sha1>
    </revision>
  </page>
  <page>
    <title>Data migration</title>
    <ns>0</ns>
    <id>1135408</id>
    <revision>
      <id>756533123</id>
      <parentid>755721938</parentid>
      <timestamp>2016-12-24T23:31:57Z</timestamp>
      <contributor>
        <username>Grayfell</username>
        <id>6603956</id>
      </contributor>
      <comment>/* External links */  Removing spam.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8689" xml:space="preserve">{{More footnotes|date=February 2013}}
'''Data migration''' is the process of [[data transfer|transferring]] [[data]] between [[computer data storage|computer storage]] types or [[file format]]s. It is a key consideration for any system implementation, upgrade, or consolidation. Data migration is usually performed programmatically to achieve an ''automated migration'', freeing up human resources from tedious tasks. Data migration occurs for a variety of reasons, including server or storage equipment replacements, maintenance or upgrades, [[Software modernization|application migration]], website consolidation and [[data center]] relocation.&lt;ref&gt;Janssen C, Data migration, http://www.techopedia.com/definition/1180/data-migration (retrieved 12 August 2013)&lt;/ref&gt;

To achieve an effective data migration procedure, data on the old system is [[data mapping|mapped]] to the new system utilising a design for [[data extraction]] and [[data loading]]. The design relates old [[data format]]s to the new system's formats and requirements. Programmatic data migration may involve many phases but it minimally includes ''data extraction'' where data is read from the old system and ''data loading'' where data is written to the new system.

After loading into the new system, results are subjected to [[data verification]] to determine whether data was accurately translated, is complete, and supports processes in the new system. During verification, there may be a need for a parallel run of both systems to identify areas of disparity and forestall erroneous [[data loss]].

Automated and manual data cleaning is commonly performed in migration to improve [[data quality]], eliminate [[data duplication|redundant]] or obsolete information, and match the requirements of the new system.

Data migration phases (design, [[extract, transform, load|extraction]], [[data cleansing|cleansing]], load, verification) for applications of moderate to high complexity are commonly repeated several times before the new system is deployed.

==Categories==

Data is stored on various media in [[Computer file|files]] or [[databases]], and is generated and consumed by [[software applications]] which in turn support [[business processes]]. The need to transfer and convert data can be driven by multiple business requirements and the approach taken to the migration depends on those requirements. Four major migration categories are proposed on this basis.

===Storage migration===
A business may choose to rationalize the physical media to take advantage of more efficient storage technologies. This will result in having to move physical blocks of data from one tape or disk to another, often using [[Storage virtualization|virtualization]] techniques. The data format and content itself will not usually be changed in the process and can normally be achieved with minimal or no impact to the layers above.

===Database migration===
{{main article|Schema migration}}
Similarly, it may be necessary to move from one [[database]] vendor to another, or to upgrade the version of database software being used. The latter case is less likely to require a physical data migration, but this can happen with major upgrades. In these cases a physical transformation process may be required since the underlying data format can change significantly. This may or may not affect behavior in the applications layer, depending largely on whether the data manipulation language or protocol has changed – but modern applications are written to be agnostic to the database technology so that a change from [[Sybase]], [[MySQL]], [[IBM DB2|DB2]] or [[Microsoft SQL Server|SQL Server]] to [[Oracle Database|Oracle]] should only require a testing cycle to be confident that both functional and non-functional performance has not been adversely affected.

===Application migration===
Changing application vendor – for instance a new [[Customer relationship management|CRM]] or [[Enterprise resource planning|ERP]] platform – will inevitably involve substantial transformation as almost every application or suite operates on its own specific data model and also interacts with other applications and systems within the [[enterprise application integration]] environment. Furthermore, to allow the application to be sold to the widest possible market, commercial off-the-shelf packages are generally configured for each customer using [[metadata]]. [[Application programming interfaces]] (APIs) may be supplied by vendors to protect the [[data integrity|integrity of the data]] they have to handle.

===Business process migration===
[[Business processes]] operate through a combination of human and application systems actions, often orchestrated by [[business process management]] tools. When these change they can require the movement of data from one store, database or application to another to reflect the changes to the organization and information about customers, products and operations. Examples of such migration drivers are mergers and acquisitions, business optimization and reorganization to attack new markets or respond to competitive threat.

The first two categories of migration are usually routine operational activities that the IT department takes care of without the involvement of the rest of the business. The last two categories directly affect the operational users of processes and applications, are necessarily complex, and delivering them without significant business downtime can be challenging. A highly adaptive approach, concurrent synchronization, a business-oriented audit capability and clear visibility of the migration for stakeholders are likely to be key requirements in such migrations.

===Project versus process===
There is a difference between data migration and [[data integration]] activities. Data migration is a project by means of which data will be moved or copied from one environment to another, and removed or decommissioned in the source. During the migration (which can take place over months or even years), data can flow in multiple directions, and there may be multiple migrations taking place simultaneously. The [[Extract, Transform, Load]] actions will be necessary, although the means of achieving these may not be those traditionally associated with the [[Extract, Transform, Load|ETL]] acronym.

Data integration, by contrast, is a permanent part of the IT architecture, and is responsible for the way data flows between the various applications and data stores - and is a process rather than a project activity. Standard ETL technologies designed to supply data from operational systems to data warehouses would fit within the latter category.

== Migration as a form of digital preservation ==
Migration, which focuses on the digital object itself, is the act of transferring, or rewriting data from an out-of-date medium to a current medium and has for many years been considered the only viable approach to long-term preservation of digital objects.&lt;ref&gt;{{cite journal|author1=van der Hoeven, Jeffrey|author2= Bram Lohman|author3=Remco Verdegem|title=Emulation for Digital Preservation in Practice: The Results|journal=The International Journal of Digital Curation|volume=2|issue=2|year=2007|pages=123-132|url=http://www.ijdc.net/index.php/ijdc/article/view/50|doi=10.2218/ijdc.v2i2.35}}&lt;/ref&gt; Reproducing brittle newspapers onto [[microform|microfilm]] is an example of such migration.

=== Disadvantages ===
* Migration addresses the possible obsolescence of the data carrier, but does not address the fact that certain technologies which run the data may be abandoned altogether, leaving migration useless.
* Time-consuming – migration is a continual process, which must be repeated every time a medium reaches obsolescence, for all data objects stored on a certain media.
* Costly - an institution must purchase additional data storage media at each migration.&lt;ref&gt;{{cite journal|author=Muira, Gregory|title=Pushing the Boundaries of Traditional Heritage Policy: maintaining long-term access to multimedia content|journal=IFLA Journal|volume=33|year=2007|pages=323-326|url=http://www.ifla.org/files/assets/hq/publications/ifla-journal/ifla-journal-4-2007.pdf}}&lt;/ref&gt;

As a result of the disadvantages listed above, technology professionals have begun to develop alternatives to migration, such as [[emulator|emulation]].

==See also==
* [[Data conversion]]
* [[Data transformation]]
* [[Extract, transform, load]]
* [[System migration]]

==References==
{{reflist}}

== External links ==
* {{dmoz|Computers/Software/Databases/Data_Warehousing/Extraction_and_Transformation|Data Migration}}

{{Authority control}}

[[Category:Data management]]</text>
      <sha1>8znodmubmrbgke8t7rxbscu5hfpv29n</sha1>
    </revision>
  </page>
  <page>
    <title>Content inventory</title>
    <ns>0</ns>
    <id>27255666</id>
    <revision>
      <id>735166545</id>
      <parentid>720256958</parentid>
      <timestamp>2016-08-19T00:18:49Z</timestamp>
      <contributor>
        <username>GermanJoe</username>
        <id>12935443</id>
      </contributor>
      <comment>rmv - spam</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9326" xml:space="preserve">A '''content inventory''' is the process and the result of cataloging the entire contents of a [[website]]. An allied practice—a [[content audit]]—is the process of ''evaluating'' that content.&lt;ref name="Halverson"&gt;{{cite web |url= http://www.peachpit.com/articles/article.aspx?p=1388961 |title= Content Strategy for the Web: Why You Must Do a Content Audit |first=Kristina |last=Halvorson |date=August 2009 |accessdate=6 May 2010}}&lt;/ref&gt;&lt;ref name="Baldwin"&gt;{{cite web |url= http://nform.ca/blog/2010/01/doing-a-content-audit-or-inven |title= Doing a content audit or inventory |first=Scott |last=Baldwin |date=January 2010 |accessdate=29 April 2010}}&lt;/ref&gt;&lt;ref name="Marsh"&gt;{{cite web |url=http://www.hilarymarsh.com/2012/03/12/how-to-do-a-content-audit/ |title=How to do a content audit |first=Hilary |last=Marsh |date=March 2012 |accessdate=2 May 2013}}&lt;/ref&gt; A content inventory and a [[content audit]] are closely related concepts, and they are often conducted in tandem.

==Description==

A content inventory typically includes all information assets on a website, such as [[web page]]s (html), [[meta element]]s (e.g., keywords, description, page title), images, audio and video files, and document files (e.g., .pdf, .doc, .ppt).&lt;ref name="Spencer2006"&gt;{{cite web |url=http://maadmob.net/donna/blog/2006/taking-a-content-inventory |title=Taking a content inventory |first=Donna |last=Spencer |date=January 2006 |accessdate=27 April 2010}}&lt;/ref&gt;&lt;ref name="Doss2007"&gt;{{cite web |url=http://www.fatpurple.com/2010/02/26/content-inventory/ |title=Content Inventory: Sometimes referred to as Web Content Inventory or Web Audit |first=Glen |last=Doss |date=January 2007 |accessdate=27 April 2010}}&lt;/ref&gt;&lt;ref name="Jones2009"&gt;{{cite web |url=http://www.uxmatters.com/mt/archives/2009/08/content-analysis-a-practical-approach.php |title=Content Analysis: A Practical Approach |first=Colleen |last=Jones |date=August 2009 |accessdate=27 April 2010}}&lt;/ref&gt;&lt;ref name="Leise2007"&gt;{{cite web |url=http://boxesandarrows.com/view/content-analysis |title=Content Analysis Heuristics |first=Fred |last=Leise |date=March 2007 |accessdate=27 April 2010}}&lt;/ref&gt;&lt;ref name="Baldwin2010"&gt;{{cite web |url=http://nform.ca/blog/2010/01/doing-a-content-audit-or-inven |title=Doing a content audit or inventory |first=Scott |last=Baldwin |date=January 2010 |accessdate=27 April 2010}}&lt;/ref&gt;&lt;ref name="Krozser"&gt;{{cite web |url=http://www.alttags.org/content-management/the-content-inventory-roadmap-to-a-succesful-cms-implementation/ |title= The Content Inventory: Roadmap to a Successful CMS Implementation |first=Kassia |last=Krozser |date=April 2005 |accessdate=27 April 2010}}&lt;/ref&gt; A content inventory is a [[Quantitative research|quantitative analysis]] of a website. It simply logs what is on a website. The content inventory will answer the question: “What is there?” and can be the start of a website review.&lt;ref name="GOSS Interactive"&gt;{{cite web |url=http://www.gossinteractive.com/community/whitepapers/conducting-a-website-review-and-implementing-results-for-increased-customer-engagement-and-conversions |title= Conducting a website review and implementing results for increased customer engagement and conversions()|first=GOSS Interactive|date=October 2011 |accessdate=8 October 2011}}&lt;/ref&gt; A related (and sometimes confused term) is a [[content audit]], a [[Qualitative research|qualitative analysis]] of information assets on a website. It is the assessment of that content and its place in relationship to surrounding Web pages and information assets. The content audit will answer the question: “Is it any good?”&lt;ref name="Baldwin"/&gt;&lt;ref name="Marsh"/&gt;

Over the years, techniques for creating and managing a content inventory have been developed and refined in the field of website [[content management]].&lt;ref name="Halverson"/&gt;&lt;ref name="Veen2002"&gt;{{cite web |url=http://www.adaptivepath.com/ideas/essays/archives/000040.php |title=Doing a Content Inventory (Or, A Mind-Numbingly Detailed Odyssey Through Your Web Site) |first=Jeffrey |last=Veen |date=June 2002 |accessdate=27 April 2010}}&lt;/ref&gt;&lt;ref name="Bruns"&gt;{{cite web |url=http://donbruns.net/index.php/how-to-automatically-index-a-content-inventory/ |title= Automatically Index a Content Inventory with GetUXIndex() |first=Don |last=Bruns |date=March 2010 |accessdate=6 May 2010}}&lt;/ref&gt;

A [[spreadsheet]] application (e.g., [[Microsoft Excel]] or [[LibreOffice Calc]]) is the preferred tool for keeping a content inventory; the data can be easily configured and manipulated. Typical categories in a content inventory include the following:

* Link — The [[URL]] for the page
* Format — For example, .[[html]], [[.pdf]], [[Microsoft Word|.doc]], [[Microsoft PowerPoint|.ppt]]
* Meta page title — Page title as it appears in the meta &lt;title&gt; tag
* Meta keywords — Keywords as they appear in the [[Meta tag#The keywords attribute|meta name="keywords" tag element]]
* Meta description — Text as it appears in the [[Meta tag#The description attribute|meta name="description" tag element]]
* Content owner — Person responsible for maintaining page content
* Date page last updated — Date of last page update
* Audit Comments (or Notes) — Audit findings and notes

There are other descriptors that may need to be captured on the inventory sheet. Content management experts advise capturing information that might be useful for both short- and long-term purposes. Other information could include:

* the overall topic or area to which the page belongs
* a short description of the information on the page
* when the page was created, date of last revision, and when the next page review is due
* pages this page links to
* pages that link to this page
* page status – keep, delete, revise, in revision process, planned, being written, being edited, in review, ready for posting, or posted
* rank of page on the website – is it a top 50 page? a bottom 50 page? Initial efforts might be more focused on those pages that visitors use the most and least.

Other tabs in the inventory workbook can be created to track related information, such as meta keywords, new Web pages to develop, website tools and resources, or content inventories for sub-areas of the main website. Creating a single, shared location for information related to a website can be helpful for all website content managers, writers, editors, and publishers.

Populating the spreadsheet is a painstaking task, but some up-front work can be automated with software, and other tools and resources can assist the audit work.

==Value==

A content inventory and a content audit are performed to understand what is on a website and why it is there. The inventory sheet, once completed and revised as the site is updated with new content and information assets, can also become a resource for help in maintaining [[website governance]].

For an existing website, the information cataloged in a content inventory and content audit will be a resource to help manage all of the information assets on the website.&lt;ref name="Usability"&gt;{{cite web |url=http://www.usability.gov/methods/design_site/inventory.html |title=Content Inventory |date=26 May 2009 |publisher=U.S. Department of Health &amp; Human Services |accessdate=4 May 2010}}&lt;/ref&gt; The information gathered in the inventory can also be used to plan a website re-design or site migration to a [[web content management system]].&lt;ref name="Krozser"/&gt; When planning a new website, a content inventory can be a useful [[project management]] tool: as a guide to map [[information architecture]] and to track new pages, page revision dates, content owners, and so on.

==See also==

* [[Content audit]]
* [[Web content management system]]
* [[Design methods]]
* [[Information architecture]]
* [[Web design]]
* [[Website governance]]

==References==
{{Reflist}}

==Further reading==

* In his article [http://www.boxesandarrows.com/view/a-map-based-approach A Map-Based Approach to a Content Inventory], Patrick Walsh describes how to use [[Microsoft Access]] and Microsoft Excel to link a data attribute with a structural attribute to create “a tool that can be used throughout the lifetime of a website.”
* In the article [http://www.louisrosenfeld.com/home/bloug_archive/000448.html The Rolling Content Inventory], author Louis Rosenfeld argues that “ongoing, partial content inventories” are more cost-effective and realistic to implement.
* Colleen Jones writes from a UX design perspective in [http://www.uxmatters.com/mt/archives/2009/08/content-analysis-a-practical-approach.php Content Analysis: A Practical Approach].
* [http://xmlpress.net/content-strategy/audits-and-inventories/ Content Audits and Inventories: A Handbook] is a practical guide to conducting content inventories and audits.

==External links==
* [http://home.snafu.de/tilman/xenulink.html Xenu's Link Sleuth]
* [http://siteorbiter.com/ SiteOrbiter]
* [http://www.webconfs.com/similar-page-checker.php Similar Page Checker]
* [http://www.cryer.co.uk/resources/link_checkers.htm Link Checker Tools]
* [http://www.kevinpnichols.com/downloads/kpn_content_audit.xls Kevin P Nichols' Content Inventory and Audit Template]

{{DEFAULTSORT:Content Inventory}}
[[Category:Data management]]
[[Category:Website management]]
[[Category:Content management systems]]</text>
      <sha1>abffprnh67gnnl90uh9avs7aqm66ufp</sha1>
    </revision>
  </page>
  <page>
    <title>Data stream management system</title>
    <ns>0</ns>
    <id>26760516</id>
    <revision>
      <id>753459639</id>
      <parentid>749432124</parentid>
      <timestamp>2016-12-07T08:12:49Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 1 sources and tagging 0 as dead. #IABot (v1.2.7.1)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="12955" xml:space="preserve">{{Use dmy dates|date=July 2013}}

A '''Data stream management system''' (DSMS) is a computer program to manage continuous data streams. It is similar to a [[database management system]] (DBMS), which is, however, designed for static data in conventional databases. A DSMS also offers a flexible query processing so that the information need can be expressed using queries. However, in contrast to a DBMS, a DSMS executes a ''continuous query'' that is not only performed once, but is permanently installed. Therefore, the query is continuously executed until it is explicitly uninstalled. Since most DSMS are data-driven, a continuous query produces new results as long as new data arrive at the system. This basic concept is similar to [[Complex event processing]] so that both technologies are partially coalescing.

== Functional principle ==

One of the most important features of a DSMS is the possibility to handle potentially infinite and rapidly changing data streams by offering flexible processing at the same time, although there are only limited resources such as main memory. The following table provides various principles of DSMS and compares them to traditional DBMS.

{| border="1"
! Database management system (DBMS)
! Data stream management system (DSMS)
|-
|Persistent data (relations)
|volatile data streams
|-
|Random access
|Sequential access
|-
|One-time queries 
|Continuous queries
|-
|(theoretically) unlimited secondary storage
|limited main memory
|-
|Only the current state is relevant
|Consideration of the order of the input
|-
|relatively low update rate
|potentially extremely high update rate
|-
|Little or no time requirements
|Real-time requirements
|-
|Assumes exact data 
|Assumes outdated/inaccurate data
|-
|Plannable query processing
|Variable data arrival and data characteristics
|}

== Processing and streaming models ==
One of the biggest challenges for a DSMS is to handle potentially infinite data streams using a fixed amount of memory and no random access to the data. There are different approaches to limit the amount of data in one pass, which can be divided into two classes. For the one hand, there are compression techniques that try to summarize the data and for the other hand there are window techniques that try to portion the data into (finite) parts.

=== Synopses ===
The idea behind compression techniques is to maintain only a synopsis of the data, but not all (raw) data points of the data stream. The algorithms range from selecting random data points called sampling to summarization using histograms, wavelets or sketching. One simple example of a compression is the continuous calculation of an average. Instead of memorizing each data point, the synopsis only holds the sum and the number of items. The average can be calculated by dividing the sum by the number. However, it should be mentioned that synopses cannot reflect the data accurately. Thus, a processing that is based on synopses may produce inaccurate results.

=== Windows ===
Instead of using synopses to compress the characteristics of the whole data streams, window techniques only look on a portion of the data. This approach is motivated by the idea that only the most recent data are relevant. Therefore, a window continuously cuts out a part of the data stream, e.g. the last ten data stream elements, and only considers these elements during the processing. There are different kinds of such windows like sliding windows that are similar to [[FIFO (computing and electronics)|FIFO]] lists or tumbling windows that cut out disjoint parts. Furthermore, the windows can also be differentiated into element-based windows, e.g., to consider the last ten elements, or time-based windows, e.g., to consider the last ten seconds of data. There are also different approaches to implementing windows. There are, for example, approaches that use timestamps or time intervals for system-wide windows or buffer-based windows for each single processing step. Sliding-window query processing is also suitable to being implemented in parallel processors by exploiting parallelism between different windows and/or within each window extent.&lt;ref&gt;{{cite journal|last1=De Matteis|first1=Tiziano|last2=Mencagli|first2=Gabriele|title=Parallel Patterns for Window-Based Stateful Operators on Data Streams: An Algorithmic Skeleton Approach|journal=International Journal of Parallel Programming|date=25 March 2016|doi=10.1007/s10766-016-0413-x}}&lt;/ref&gt;

== Query Processing ==
Since there are a lot of prototypes, there is no standardized architecture. However, most DSMS are based on the [[Information retrieval|query]] processing in DBMS by using declarative languages to express queries, which are translated into a plan of operators. These plans can be optimized and executed. A query processing often consists of the following steps.

=== Formulation of continuous queries ===
The formulation of queries is mostly done using declarative languages like [[SQL]] in DBMS. Since there are no standardized query languages to express continuous queries, there are a lot of languages and variations. However, most of them are based on [[SQL]], such as the [[Continuous Query Language]] (CQL), [[StreamSQL]] and [[Event stream processing|EPL]]. There are also graphical approaches where each processing step is a box and the processing flow is expressed by arrows between the boxes.

The language strongly depends on the processing model. For example, if windows are used for the processing, the definition of a window has to be expressed. In [[StreamSQL]], a query with a sliding window for the last 10 elements looks like follows:
&lt;source lang="sql"&gt;
SELECT AVG(price) FROM examplestream [SIZE 10 ADVANCE 1 TUPLES] WHERE value &gt; 100.0
&lt;/source&gt;
This stream continuously calculates the average value of "price" of the last 10 tuples, but only considers those tuples whose prices are greater than 100.0.

In the next step, the declarative query is translated into a logical query plan. A query plan is a directed graph where the nodes are operators and the edges describe the processing flow. Each operator in the query plan encapsulates the semantic of a specific operation, such as filtering or aggregation. In DSMSs that process relational data streams, the operators are equal or similar to the operators of the [[Relational algebra]], so that there are operators for selection, projection, join, and set operations. This operator concept allows the very flexible and versatile processing of a DSMS.

=== Optimization of queries ===
The logical query plan can be optimized, which strongly depends on the streaming model. The basic concepts for optimizing continuous queries are equal to those from [[Query optimizer|database systems]]. If there are relational data streams and the logical query plan is based on relational operators from the [[Relational algebra]], a query optimizer can use the algebraic equivalences to optimize the plan. These may be, for example, to push selection operators down to the sources, because they are not so computationally intensive like join operators.

Furthermore, there are also cost-based optimization techniques like in DBMS, where a query plan with the lowest costs is chosen from different equivalent query plans. One example is to choose the order of two successive join operators. In DBMS this decision is mostly done by certain statistics of the involved databases. But, since the data of a data streams is unknown in advance, there are no such statistics in a DSMS. However, it is possible to observe a data stream for a certain time to obtain some statistics. Using these statistics, the query can also be optimized later. So, in contrast to a DBMS, some DSMS allows to optimize the query even during runtime. Therefore, a DSMS needs some plan migration strategies to replace a running query plan with a new one.

=== Transformation of queries ===
Since a logical operator is only responsible for the semantics of an operation but does not consist of any algorithms, the logical query plan must be transformed into an executable counterpart. This is called a physical query plan. The distinction between a logical and a physical operator plan allows more than one implementation for the same logical operator. The join, for example, is logically the same, although it can be implemented by different algorithms like a [[Nested loop join]] or a [[Sort-merge join]]. Notice, these algorithms also strongly depend on the used stream and processing model.
Finally, the query is available as a physical query plan.

=== Execution of queries ===
Since the physical query plan consists of executable algorithms, it can be directly executed. For this, the physical query plan is installed into the system. The bottom of the graph (of the query plan) is connected to the incoming sources, which can be everything like connectors to sensors. The top of the graph is connected to the outgoing sinks, which may be for example a visualization. Since most DSMSs are data-driven, a query is executed by pushing the incoming data elements from the source through the query plan to the sink. Each time when a data element passes an operator, the operator performs its specific operation on the data element and forwards the result to all successive operators.

== Data Stream Management Systems ==
* [http://www.sqlstream.com/stream-processing/ SQLstream]
* [http://www-db.stanford.edu/stream STREAM] &lt;ref name="StandfordStream"&gt;[http://ilpubs.stanford.edu:8090/641/ Arasu, A., et. al. ''STREAM: The Stanford Data Stream Management System.''  Technical Report. 2004, Stanford InfoLab.]&lt;/ref&gt;
* [http://www.cs.brown.edu/research/aurora/ AURORA],&lt;ref name="aurora"&gt;{{cite conference | author = Abadi | title = Aurora: A Data Stream Management System | conference = SIGMOD 2003 | citeseerx = 10.1.1.67.8671 |display-authors=etal}}&lt;/ref&gt; [http://www.streambase.com/ StreamBase Systems, Inc.]
* [http://telegraph.cs.berkeley.edu/telegraphcq/ TelegraphCQ] &lt;ref name="telegraphcq"&gt;[http://www.cs.berkeley.edu/~franklin/Papers/TCQcidr03.pdf Chandrasekaran, S. et al, "TelegraphCQ: Continuous Dataflow Processing for an Uncertain World." CIDR 2003.]&lt;/ref&gt;
* [http://research.cs.wisc.edu/niagara/ NiagaraCQ],&lt;ref name="niagaracq"&gt;[http://www.cs.wisc.edu/niagara/papers/NiagaraCQ.pdf Chen, J. et al, "NiagaraCQ: A Scalable Continuous Query System for Internet Databases." SIGMOD 2000.]&lt;/ref&gt; 
* [http://wwwdb.inf.tu-dresden.de/research-projects/closed-projects/qstream/ QStream]
* [http://dbs.mathematik.uni-marburg.de/Home/Research/Projects/PIPES PIPES], [http://www.softwareag.com/de/products/wm/events/overview/default.asp webMethods Business Events]
* [http://www-db.in.tum.de/research/projects/StreamGlobe/index.shtml StreamGlobe]
* [http://odysseus.informatik.uni-oldenburg.de/ Odysseus]
* [http://www.microsoft.com/sqlserver/en/us/solutions-technologies/business-intelligence/streaming-data.aspx StreamInsight]
* [http://www-01.ibm.com/software/data/infosphere/streams/ InfoSphere Streams]
* [http://www.sas.com/en_us/software/data-management/event-stream-processing.html SAS Event Stream Processing Engine]
* [http://go.sap.com/uk/product/data-mgmt/complex-event-processing.html  SAP Event Stream Processor]
* [https://www.pipelinedb.com/ Pipeline DB]

== See also ==
* [[Complex Event Processing]]
* [[Event stream processing]]
* [[Relational data stream management system]]

== References ==
{{Reflist}}
* {{Cite book
|last=Aggarwal
|first=Charu C.
|authorlink=
|year=2007
|title=Data Streams: Models and Algorithms
|publisher=Springer
|location=New York
|id=
|isbn=978-0-387-47534-9
}}
* {{Cite book
|first1=Lukasz
|last1=Golab
|first2=M. Tamer
|last2=Özsu
|authorlink=
|year=2010
|title=Data Stream Management
|publisher=Morgan and Claypool
|location=Waterloo, USA
|id=
|isbn=978-1-608-45272-9
}}

==External links==
*[http://www.pam2004.org/papers/113.pdf Using Data Stream Management Systems for Traffic Analysis: A Case Study, last visited 2013-01-10]
*[http://infolab.stanford.edu/stream/ STREAM: Stanford Stream Data Manager, last visited 2013-01-10]
*[http://datalab.cs.pdx.edu/niagara/ NiagaraST: A Research Data Stream Management System at Portland State University, last visited 2013-01-10]
*[http://odysseus.informatik.uni-oldenburg.de Odysseus: An open source Java based framework for Data Stream Management Systems, last visited 2013-01-10]
*[http://home.dei.polimi.it/margara/papers/survey.pdf Processing Flows of Information: From Data Stream to Complex Event Processing] - Survey article on Data Stream and Complex Event Processing Systems, last visited 2013-01-10
*[http://www.streambase.com/developers/docs/latest/streamsql/index.html StreamSQL reference, last visited 2013-01-10]
*[https://web.archive.org/web/20140706215458/http://www.sqlstream.com/stream-processing-with-sql/ Stream processing with SQL] - Introduction to streaming data management with SQL

[[Category:Data management]]</text>
      <sha1>7pr3owkjmzp72bvaem7t5mlwlns9ra7</sha1>
    </revision>
  </page>
  <page>
    <title>Lean integration</title>
    <ns>0</ns>
    <id>28252181</id>
    <revision>
      <id>751022926</id>
      <parentid>582242741</parentid>
      <timestamp>2016-11-22T22:11:48Z</timestamp>
      <contributor>
        <ip>142.239.254.20</ip>
      </contributor>
      <comment>/* Lean integration principles */ Fixed a couple of typos, fixed a grammar error and capitalized the name of the metric 'First Time Through'</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9105" xml:space="preserve">'''Lean integration''' is a [[management system]] that emphasizes creating value for customers, continuous improvement, and eliminating waste as a sustainable [[data integration]] and [[system integration]] practice.  Lean integration has parallels with other lean disciplines such as [[lean manufacturing]], [[lean IT]], and [[lean software development]].  It is a specialized collection of tools and techniques that address the unique challenges associated with seamlessly combining information and processes from systems that were independently developed, are based on incompatible data models, and remain independently managed, to achieve a cohesive holistic operation.

==History==

Lean integration was first introduced by John Schmidt in a series of blog articles starting in January 2009 entitled 10 Weeks To Lean Integration.&lt;ref&gt;[http://blogs.informatica.com/perspectives/index.php/2009/01/14/10-weeks-to-lean-integration/ Original Lean Integration Blog Series]&lt;/ref&gt;  This was followed by a white paper&lt;ref&gt;[http://www.cloudyintegration.com/uploads/LEAN_INTEGRATION_AFE_-_John_Schmidt.pdf Lean Integration White Paper]&lt;/ref&gt; on the topic in April 2009 and the book ''Lean Integration, An Integration Factory Approach to Business Agility'' &lt;ref name="Schmidt"&gt;John G.Schmidt, David Lyle (2010) ''Lean Integration: An Integration Factory Approach to Business Agility'', Addison-Wesley Pearson Education, ISBN 0-321-71231-5&lt;/ref&gt; in May 2010.

==Overview==

Lean integration builds on the same set of principles that were developed for [[lean manufacturing]] and [[lean software development]] which is based on the [[Toyota Production System]]. Integration solutions can be broadly categorized as either Process Integration or Data Integration.  

The book&lt;ref name="Schmidt"/&gt; is based on the premise that Integration is an ongoing activity and not a one-time activity;  therefore integration should be viewed as a long term strategy for an organization.  John Schmidt and David Lyle initially articulated in their book the reasons for maintaining an efficient and sustainable integration team.  Lean integration as an integration approach must be ''sustainable'' and ''holistic'' unlike other integration approaches that either tackle only a part of the problem or tackle the problem for a short period of time.  Lean integration drives elimination of waste by adopting reusable elements, high automation and quality improvements.  Lean is a data-driven, fact-based methodology that relies on metrics to ensure that the quality and performance are maintained at a high level. 

An organizational focus is required for the implementation of lean integration principles. The predominant organizational model is the [[Integration Competency Center]] which may be structured as a central group or a more loosely coupled federated team.

==Lean integration principles==

The principles of Lean Integration may at first glance appear similar to that of [[Six Sigma]] but there are some very clear differences between them.  Six-Sigma is an ''analytical technique'' that focuses on quality and reduction of defects while Lean is a ''management system'' that focuses on delivering value to the end customer by continuously improving value delivery processes.  Lean provides a robust framework that facilitates improving efficiency and effectiveness by focusing on critical customer requirements.

As mentioned in lean integration there are seven core ''lean integration principles'' vital for deriving significant and sustainable business benefits. They are as below: 

# Focus on the customer and eliminate waste: Waste elimination should be viewed from the customer perspective and all activities that do not add value to the customer needs to be looked at closely and eliminated or reduced. In an integration context, the customer is often an internal sponsor or group within an organization that uses, benefits from, or pays for, the integrated capabilities.
# Continuously improve: A data driven cycle of hypothesis-validation-implementation should be used to drive innovation and continuously improve the end-to-end process.  Adopting and institutionalizing lessons learned and sustaining integration knowledge are related concepts that assist in the establishment of this principle.
# Empower the team: Creating cross-functional teams and sharing commitments across individuals empower the teams and individuals who have a clear understanding of their roles and the needs of their customers.  The team is also provided the support by senior management to innovate and try new ideas without fear of failure.
# Optimize the whole: Adopt a big-picture perspective of the end-to-end process and optimize the whole to maximize the customer value.  This may at times require performing individual steps and activities that appear to be sub-optimal when viewed in isolation, but aid in streamlining the end-to-end process.
# Plan for change: Application of mass customization techniques like leveraging automated tools, structured processes, and reusable and parameterized integration elements leads to reduction in cost and time in both the build and run stages of the integration life-cycle. Another key technique is a middleware services layer that presents applications with enduring abstractions of data through standardized interfaces, allowing the underlying data structures to change without necessarily impacting the dependent applications.
# Automate processes: Automation of tasks increases the ability to respond to large integration projects as effectively as small changes. In its ultimate form, automation eliminates integration dependencies from the critical implementation path of projects.
# Build quality in : Process excellence is emphasized and quality is built in rather than inspected in. A key metric for this principle is First Time Through (FTT) percentage which is a measure of the number of times an end-to-end process is executed without having to do any rework or repeat any of the steps.

==Benefits of lean integration==

The Lean integration practices transforms integration from an ''art'' into a ''science'', a repeatable and teachable methodology that shifts the focus from integration as a point-in-time activity to integration as a sustainable activity that enables organizational agility.  Once an organization adopts the integration as a science it enhances the organization’s ability to change rapidly without comprising on the IT risk or quality thereby transforming the organization into an agile data driven enterprise.  The following are the advantages derived by adopting the lean integration practices:

# Efficiency: typical improvements are in the scale of 50% labor productivity improvements and 90% lead-time reduction through continuous efforts to eliminate waste.
# Agility: Reusable components, highly automated processes and self-service delivery models improve the agility of the organization.
# Data quality: quality and reliability of data is enhanced and data becomes a real asset.
# Governance: metrics are established that drive continuous improvement.
# Innovation: innovation is facilitated by using fact-based approach.
# Staff Morale: IT staff is kept engaged with high morale driving bottom-up improvements.

==See also==

* [[Integration Competency Center]]
* [[Lean software development]]
* [[Lean IT]]
* [[Data Integration]]
* [[Toyota Production System]]

==References==

&lt;references/&gt;

==External links==
* [http://www.integrationfactory.com Lean Integration book microsite]
* [http://blogs.informatica.com/perspectives/index.php/2010/04/06/health-care-is-ready-for-lean-integration/ Application of Lean Integration to Health Care]
* [http://www.informatica.com/news_events/press_releases/Pages/02082010_lean.aspx  Press Release about Lean Integration Book]
* [http://www.linkedin.com/in/johnschmidt John Schmidt profile]
* [http://www.linkedin.com/in/davelyle David Lyle profile]
* [http://my.safaribooksonline.com/9780321712363 Lean Integration book publisher website]
* [http://www.baselinemag.com/c/a/IT-Management/How-IT-Runs-Lean-419352/ Slide show overview of Lean Integration]
* [http://www.linkedin.com/groups?gid=2302506 LinkedIn Group for Lean Integration Community]
* [http://www.itbusinessedge.com/cm/blogs/vizard/making-the-case-for-lean-integration/?cs=42547 Book review by Mike Vizard of ITBusinessEdge]
* [http://www.bcs.org/server.php?show=conBlogPost.1685 Book review by John Morris]
* [http://www.itbusinessedge.com/cm/blogs/lawson/lean-principles-can-make-it-better-at-integration/?cs=42041&amp;utm_source=itbe&amp;utm_medium=email&amp;utm_campaign=EEB&amp;nr=EEB John Schmidt and David Lyle Interview by Loraine Lawson]
* [http://www.insurancenetworking.com/blogs/insurance_technology_Lean_IT_manufacturing-25138-1.html Book review by Joe McKendrick]
* [http://www.information-management.com/dmradio/-10017194-1.html David Lyle Interview on DM Radio]

[[Category:Data management]]
[[Category:Software development philosophies]]
[[Category:Agile software development]]
[[Category:Information technology]]
[[Category:Quality]]</text>
      <sha1>ocgted89c4fvt2ajnpqnxilaus67cik</sha1>
    </revision>
  </page>
  <page>
    <title>Novell Storage Manager</title>
    <ns>0</ns>
    <id>28205544</id>
    <revision>
      <id>721088934</id>
      <parentid>721088745</parentid>
      <timestamp>2016-05-19T18:27:04Z</timestamp>
      <contributor>
        <ip>134.216.26.215</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4645" xml:space="preserve">{{Infobox software
|name                       = Novell Storage Manager
|logo                       =
|screenshot                 =
|caption                    =
|collapsible                =
|author                     =
|developer                  = [[Novell]]
|released                   = 2004 &lt;!-- {{Start date|YYYY|MM|DD}} --&gt;
|discontinued               =
|latest release version     = 4.1
|latest release date        = {{Start date|2014|10|07}}
|latest preview version     =
|latest preview date        = &lt;!-- {{Start date and age|YYYY|MM|DD}} --&gt;
|frequently updated         =
|programming language       =
|operating system           =
|platform                   =
|size                       =
|language                   =
|status                     =
|genre                      = [[System Software]]
|license                    =
|website                    = [http://www.novell.com/products/storage-manager/ Novell Storage Manager]
}}

'''Novell Storage Manager''' is a [[system software]] package released by [[Novell]] in 2004 &lt;ref&gt;{{Citation | last=Greyzdorf| first=Noemi| title=Novell Delivers a New Way of Intelligently Managing Organizations' File-Based Information| journal=IDC #216013 | volume=1| issue=Storage Software: Technology Assessment| year=2009| pages=1–3| url=http://www.novell.com/docrep/2009/01/Novell%20Delivers%20a%20New%20Way%20of%20Intelligently%20Managing%20Organizations_%20File-Based%20Information_en.pdf}}&lt;/ref&gt; that uses identity, policy and [[Novell eDirectory|directory]] events to automate full lifecycle management of file storage for individual users and organizational groups. By tying storage management to an organization's existing identity infrastructure, it has been pointed out,&lt;ref&gt;{{Citation | last=Greyzdorf| first=Noemi| title=Efficiently Delivering Enterprise-Class File-Based Storage| journal=IDC Spotlight | year=2010| pages=1–5}}&lt;/ref&gt; Novell Storage Manager enables the administration of users across all file servers "as a single pool rather than [in] separate independently managed domains." Novell Storage Manager is a component of the [[Novell File Management Suite]].

==How It Works==

Novell Storage Manager dynamically manages and provisions storage based on user and group events that occur in the directory, including user creations, group assignments, moves, renames, and deletions. When a change happens in the directory that affects a user’s file storage needs or user storage policy, Storage Manager applies the appropriate policy and makes the necessary changes at the file system level to address those storage needs.&lt;ref&gt;{{citation| title=Novell Storage Manager for Novell eDirectory | year=2009 | page=4 | url=http://www.novell.com/docrep/2009/04/Novell_Storage_Manager_for_Novell_eDirectory_White_Paper_en.pdf}}&lt;/ref&gt;

The following key components comprise Novell Storage Manager's identity and policy-driven [[state machine]] architecture: Directory services; Storage policies; Novell Storage Manager event monitors; Novell Storage Manager policy engine; Novell Storage Manager agents; and Action objects. This state machine architecture enables the engine to properly deal with transient waits with directory synchronization issues. It also allows recovery from failures involving network communications, a target server or a server running a component of Storage Manager—including the policy engine itself. If a failure or interruption occurs at any point during operation, Storage Manager will be able to successfully continue the operation from where it was when the interruption occurred.

==Reviews==

Jon Toigo called Novell Storage Manager "a robust and smart approach to corralling user files... into an organized and efficient management scheme".&lt;ref&gt;{{Citation| last = Toigo | first = Jon William | title = Novell Storage Manager Strikes Data Management Gold | url= http://esj.com/articles/2009/04/28/novell-storage-mgr.aspx | accessdate = 26 July 2010}}&lt;/ref&gt; He also said it was "best in class" of the products he'd reviewed.&lt;ref&gt;{{Citation| last = Toigo | first = Jon William | title = Everything We Need to Know About How to Screw Up IT… | url=http://www.drunkendata.com/?p=2916 | accessdate = 30 July 2010}}&lt;/ref&gt;

==References==
{{reflist}}

==External links==
*[http://www.novell.com/products/storage-manager/ Novell Storage Manager: Product homepage] - Overview, features, and technical information
*[http://www.storagemanagersupport.com/nsm/ Novell Storage Manager: Support]

{{Novell}}

[[Category:Novell]]
[[Category:Novell software]]
[[Category:Storage software]]
[[Category:Data management]]
[[Category:Identity management]]</text>
      <sha1>ml7elmxzuq5fkgtaicsnq59ruqqw0tx</sha1>
    </revision>
  </page>
  <page>
    <title>DMAPI</title>
    <ns>0</ns>
    <id>8947566</id>
    <revision>
      <id>621913548</id>
      <parentid>621913486</parentid>
      <timestamp>2014-08-19T13:31:55Z</timestamp>
      <contributor>
        <ip>119.151.72.60</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="809" xml:space="preserve">'''Data Management API''' ('''DMAPI''') is the interface defined in the [[X/Open]] document "Systems Management: Data Storage Management (XDSM) API" dated February 1997. [[XFS]], IBM [[JFS (file system)|JFS]], [[VxFS]], [[AdvFS]], [[StorNext]] and [[GPFS]] file systems support DMAPI for [[Hierarchical storage management|Hierarchical Storage Management]] (HSM).

== External links ==
* [http://pubs.opengroup.org/onlinepubs/9657099/ Systems Management: Data Storage Management (XDSM) API]
* [http://publib.boulder.ibm.com/infocenter/clresctr/vxrx/topic/com.ibm.cluster.gpfs34.dmapi.doc/bl1dmp_BookMap_xtoc.html GPFS V3.4 Data Management API Guide]
* [http://oss.sgi.com/projects/xfs/ Open Source XFS Source code with DMAPI Implementation and Test Suite ]

[[Category:Data management]]

{{compu-storage-stub}}</text>
      <sha1>61pohqdz3ytzm84stx64klc54r46rma</sha1>
    </revision>
  </page>
  <page>
    <title>Data custodian</title>
    <ns>0</ns>
    <id>28192799</id>
    <revision>
      <id>718091612</id>
      <parentid>705417314</parentid>
      <timestamp>2016-05-01T13:40:13Z</timestamp>
      <contributor>
        <username>Themightyquill</username>
        <id>1212157</id>
      </contributor>
      <comment>removed [[Category:Library science]]; added [[Category:Library occupations]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3014" xml:space="preserve">{{more footnotes|date=August 2010}}
{{merge|Data steward|date=February 2016}}
In [[Data governance|Data Governance]] groups, responsibilities for data management are increasingly divided between the business process owners and information technology (IT) departments.  Two functional titles commonly used for these roles are [[Data steward|Data Steward]] and Data Custodian. 

Data Stewards are commonly responsible for data content, context, and associated business rules. Data Custodians are responsible for the safe custody, transport, storage of the data and implementation of business rules.&lt;ref&gt;Carnegie Mellon - Information Security Roles and Responsibilities, http://www.cmu.edu/iso/governance/roles/data-custodian.html&lt;/ref&gt;&lt;ref&gt;''Policies, Regulations and Rules: Data Management Procedures - REG 08.00.3 - Information Technology'', , NC State University, http://www.ncsu.edu/policies/informationtechnology/REG08.00.3.php&lt;/ref&gt; Simply put, Data Stewards are responsible for what is stored in a data field, while Data Custodians are responsible for the technical environment and database structure. Common job titles for data custodians are Database Administrator (DBA), Data Modeler, and ETL Developer.

==Data Custodian Responsibilities==
A data custodian ensures:
# Access to the data is authorized and controlled
# Data stewards are identified for each data set
# Technical processes sustain data integrity
# Processes exist for data quality issue resolution in partnership with Data Stewards
# Technical controls safeguard data
# Data added to data sets are consistent with the common data model
# Versions of Master Data are maintained along with the history of changes
# Change management practices are applied in maintenance of the database
# Data content and changes can be audited

==See also==
* [[Data governance]]
* [[Data steward]]

==References==
&lt;references&gt;&lt;/references&gt;

==Related Links==
* ''Establishing data stewards'', by Jonathan G. Geiger, Teradata Magazine Online, September 2008, http://www.teradata.com/tdmo/v08n03/Features/EstablishingDataStewards.aspx
* '' A Rose By Any Other Name – Titles In Data Governance'', by Anne Marie Smith, Ph.D., EIMInstitute.ORG Archives, Volume 1, Issue 13, March 2008, http://www.eiminstitute.org/library/eimi-archives/volume-1-issue-13-march-2008-edition/a-rose-by-any-other-name-2013-titles-in-data-governance

{{DEFAULTSORT:Data Custodian}}
[[Category:Information technology governance]]
[[Category:Data management]]
[[Category:Knowledge representation]]
[[Category:Library occupations]]
[[Category:Metadata]]
[[Category:Technical communication]]

[[ar:ميتاداتا]]
[[cs:Metadata]]
[[da:Metadata]]
[[de:Data Steward]]
[[et:Metaandmed]]
[[es:Metadato]]
[[eo:Meta-dateno]]
[[fr:Métadonnée]]
[[it:Metadata]]
[[lv:Metadati]]
[[hu:Metaadat]]
[[nl:Metadata]]
[[ja:メタデータ]]
[[no:Metadata]]
[[pl:Metadane]]
[[pt:Metadados]]
[[ru:Метаданные]]
[[fi:Metatieto]]
[[sv:Metadata]]
[[th:เมทาดาตา]]
[[vi:Metadata]]</text>
      <sha1>c3km9wtkh88l53mzxmp6mlo8mvf4gi6</sha1>
    </revision>
  </page>
  <page>
    <title>Rainbow storage</title>
    <ns>0</ns>
    <id>8098559</id>
    <revision>
      <id>758203483</id>
      <parentid>758200665</parentid>
      <timestamp>2017-01-04T01:57:54Z</timestamp>
      <contributor>
        <username>Doctree</username>
        <id>14110041</id>
      </contributor>
      <comment>/* top */ per WP:NCCAP</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7189" xml:space="preserve">{{notability|date=November 2014}}
'''Rainbow storage''' is a developing [[paper]]-based [[data storage device|data storage]] technique first demonstrated by Indian student Sainul Abideen in November 2006.&lt;ref name="ArabNews"&gt;[http://www.arabnews.com/?page=4&amp;section=0&amp;article=88962&amp;d=18&amp;m=11&amp;y=2006 "Data Can Now Be Stored on Paper"] by M. A. Siraj, ''[[ArabNews]]'' (published November 18, 2006; accessed November 29, 2006)&lt;/ref&gt; Abideen received his [[Master of Computer Applications|MCA]] from [[MES College of Engineering|MES Engineering College]] in [[Kuttipuram]] in [[Kerala]]'s [[Malappuram]] district.

Initial newspaper reports of the technology were disputed by multiple technical sources, although Abideen says those reports were based on a misunderstanding of the technology. The paper meant to demonstrate the capability of storing relatively large amounts of data (and not necessarily in the gigabyte range) using textures and diagrams.&lt;ref name=theinq&gt;[http://www.theinquirer.net/default.aspx?article=36294 Paper storage man misunderstood] &amp;mdash; ''[[The Inquirer]]'' article, 12 December 2006 (retrieved 15 December 2006.&lt;/ref&gt;

The Rainbow data storage technology claims to use [[Geometry|geometric]] shapes such as triangles, circles and squares of various colors to store a large amount of data on ordinary paper or plastic surfaces. This would provide several advantages over current forms of [[Optical disc|optical-]] or [[Magnetic storage|magnetic]] [[data storage device|data storage]] like less environmental pollution due to the biodegradability of paper, low cost and high capacity. Data could be stored on "Rainbow Versatile Disk" (RVD) or plastic/paper cards of any form factor (like SIM cards).&lt;ref name="Techworld.com"&gt;[http://www.techworld.com/storage/news/index.cfm?newsID=7424 "Store 256GB on an A4 sheet"] by Chris Mellor, Techworld (published November 24, 2006; accessed November 29, 2006)&lt;/ref&gt;

==Criticism==
Following the wide media attention this news received, some of the claims have been disputed by various experts.&lt;ref name="ITSoup"&gt; [http://itsoup.blogspot.com/2006/11/scam-of-indian-student-developing.html IT Soup: Scam of Indian student developing technology to store 450 GB of data on a sheet of paper] By ITSoup (published November 25, 2006; accessed November 25, 2006)&lt;/ref&gt;	 &lt;ref name="ArsTechnica"&gt; [http://arstechnica.com/news.ars/post/20061126-8288.html "Can you get 256GB on an A4 sheet? No way!"]  By Chris Mellor, Techworld (published November 24, 2006; accessed November 29, 2006)&lt;/ref&gt;	

Printing at 1,200 dots per inch (DPI) leads to a theoretical maximum of 1,440,000 colored dots per square inch. If a scanner can reliably distinguish between 256 unique colors (thus encoding one byte per dot), the maximum possible storage is approximately 140 megabytes for a sheet of A4 paper&amp;ndash;much lower when the necessary error correction is employed. If the scanner were able to accurately distinguish between 16,777,216 colors (24 bits, or 3 bytes per dot), the capacity would triple, but it still falls well below the media stories' claims of several hundred gigabytes.

Printing this quantity of unique colors would require specialized equipment to generate many [[spot color]]s.  The [[process color]] model used by most printers provides only four colors, with additional colors simulated by a [[halftone|halftone pattern]].

At least one of three things must be true for the claim to be valid:
* The paper must be printed and scanned at a much higher resolution than 1,200 DPI, 	 
* the printer and scanner must be able to accurately produce and distinguish between an extraordinary number of distinct color values, or 	 
* the compression scheme must be a revolutionary [[lossless compression]] algorithm. 	 

The theory is: If Rainbow's "geometric" algorithm is to be encoded and decoded by a computer, it would equally viable to store the compressed data on a conventional disk rather than printing it to paper or other non-digital medium. Printing something as dots on a page rather than bits on a disk will not change the underlying compression ratio, so a lossless compression algorithm that could store 250 gigabytes within a few hundred megabytes of data would be revolutionary indeed. Likewise, data can be compressed with ''any'' algorithm and subsequently printed to paper as colored dots. The amount of data that can be reliably stored in this way is limited by the printer and scanner, as described above.

However Sainul Abideen says that the articles are based on misunderstandings. He claims, it as a method to store data in the form of colour, in any medium where colour can be represented, not only paper. Density of storage in paper will be very small and the density will be depends on the storage medium, capacity of colour representation and retrieval methods etc.

==Demonstrations==
Sainul Abdeen demonstrated his technology to the college and members of the Indian press in the MES College of Engineering computer lab, Kerala, and was able to compress 450 sheets plain text from [[Foolscap folio|foolscap paper]] into a 1 inch square. He also demonstrated a 45-second audio clip compressed using this technology on to an [[ISO 216|A4 sheet]]. Depending on the sampling frequency, bit depth, and audio compression (if any), a 45-second audio clip can consist of anywhere from a few kilobytes to a few megabytes of data.  Abideen claimed that the technology could be extended to 250 gigabytes by using specific materials and devices. {{Fact|date=June 2009}}

This technology is based on two principles:

;Principle I
:“Every color or color combinations can be converted into some values and from the values the colors or color combinations can be regenerated”.
;Principle II
:“Every different color or color combinations will produce different values”.

==References==
{{reflist}}


==Absolute Rainbow Dots==
Absolute rainbow dots are used to detect errors caused by scratches, and whether any fading has occurred. Absolute rainbow dots are predefined dots carrying a unique value. These dots can be inserted in the rainbow picture in pre-specified areas. If fading occurs these dot values will change accordingly, and at the reproduction stage this can be checked and corrected.
Absolute rainbow dots will be microscopically small so that they occupy very little space in the rainbow picture. These will be colored differently so that each dot will have its own fixed unique value.

==External links==
* [http://www.kerlontech.com/RandD.html Sainul Abideen's home page] (dead)
* [http://www.deccanherald.com/deccanherald/Sep62006/cyberspace163748200695.asp Deccan Herald's article on Rainbow Storage] (dead)
* [http://www.dailytech.com/article.aspx?newsid=5052 Article in DailyTech,]
* [http://itsoup.blogspot.com/2006/11/scam-of-indian-student-developing.html IT Soup: Scam of Indian student developing technology to store 450 GB of data on a sheet of paper]
* [http://www.theregister.co.uk/2006/11/23/rvd_system/ Article in The Register]
*[http://www.idm.net.au/storypages/storydata.asp?id=7749 IDM: Paper Storage Claims A Hoax?]  (dead)

[[Category:Data management]]
[[Category:Vaporware]]</text>
      <sha1>258zidpa1g6gzn5mdi6p82j47tl3m5u</sha1>
    </revision>
  </page>
  <page>
    <title>Project workforce management</title>
    <ns>0</ns>
    <id>7217055</id>
    <revision>
      <id>706908758</id>
      <parentid>689141010</parentid>
      <timestamp>2016-02-25T23:54:32Z</timestamp>
      <contributor>
        <username>Keith D</username>
        <id>2278355</id>
      </contributor>
      <comment>Remove plus.google from ref publisher field</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="15200" xml:space="preserve">'''Project workforce management''' is the practice of combining the coordination of all logistic elements of a project through a single [[software application]] (or [[workflow engine]]). This includes planning and tracking of schedules and mileposts, cost and revenue, resource allocation, as well as overall management of these project elements.  Efficiency is improved by eliminating manual processes, like [[spreadsheet]] tracking&lt;ref&gt;
{{Cite web
| author      = Seema Haji
| title       = Business Intelligence Cures the Spreadsheet Problem
| url         = http://www.refresher.com/asmhbi.html
| publisher   = Refresher Publications Inc.
| year        = 2009
| accessdate  = October 30, 2009
}}&lt;/ref&gt;  to monitor project progress. It also allows for at-a-glance status updates and ideally integrates with existing legacy applications in order to unify ongoing projects, [[enterprise resource planning]] (ERP) and broader organizational goals.&lt;ref&gt;
{{Cite web
| author      = Rudolf Melik
| title       = The Rise of the Project Workforce
| url         = http://www.projectworkforcebook.com/
| publisher   = Wiley: New York, NY
| year        = 2007
| accessdate  = October 30, 2009
}}&lt;/ref&gt; There are a lot of logistic elements in a project. Different team members are responsible for managing each element and often, the organisation may have a mechanism to manage some logistic areas as well.

By coordinating these various components of [[project management]], [[workforce management]] and financials through a single solution, the process of configuring and changing project and workforce details is simplified.

== Introduction ==
&lt;ref&gt;{{Citation|title = Project workforce management|url = http://www.google.com/patents/US20030236692|accessdate = 2015-11-04}}&lt;/ref&gt; A project workforce management system defines project tasks, project positions, and assigns personnel to the project positions. The project tasks and positions are correlated to assign a responsible project position or even multiple positions to complete each project task. Because each project position may be assigned to a specific person, the qualifications and availabilities of that person can be taken into account when determining the assignment. By associating project tasks and project positions, a manager can better control the assignment of the workforce and complete the project more efficiently.

When it comes to project workforce management, it is all about managing all the logistic aspects of a project or an organisation through a software application. Usually, this software has a workflow engine defined. Therefore, all the logistic processes take place in the workflow engine.

== About ==

=== Technical Field ===
This invention relates to project management systems and methods, more particularly to a software-based system and method for project and workforce management.&lt;ref&gt;{{Citation|title = Project workforce management Technical Field|url = http://www.google.com/patents/US20030236692|accessdate = 2015-11-04}}&lt;/ref&gt;

=== Software Usage ===
Due to the software usage, all the project workflow management tasks can be fully automated without leaving many tasks for the project managers. This returns high efficiency to the project management when it comes to project tracking proposes. In addition to different tracking mechanisms, project workforce management software also offer a dashboard for the project team. Through the dashboard, the project team has a glance view of the overall progress of the project elements.

Most of the times, project workforce management software can work with the existing legacy software systems such as ERP (enterprise resource planning) systems. This easy integration allows the organisation to use a combination of software systems for management purposes.&lt;ref&gt;{{Citation|title = Project workforce management Software Use|url = http://www.google.com/patents/US20030236692|accessdate = 2015-11-04}}&lt;/ref&gt;

=== Background ===
Good project management is an important factor for the success of a project. A project may be thought of as a collection of activities and tasks designed to achieve a specific goal of the organisation, with specific performance or quality requirements while meeting any subject time and cost constraints. Project management refers to managing the activities that lead to the successful completion of a project. Furthermore, it focuses on finite deadlines and objectives. A number of tools may be used to assist with this as well as with assessment.

Project management may be used when planning personnel resources and capabilities. The project may be linked to the objects in a professional services life cycle and may accompany the objects from the opportunity over quotation, contract, time and expense recording, billing, period-end-activities to the final reporting. Naturally the project gets even more detailed when moving through this cycle.&lt;ref&gt;{{Citation|title = Project workforce management background|url = http://www.google.com/patents/US20030236692|accessdate = 2015-11-04}}&lt;/ref&gt;

For any given project, several project tasks should be defined. Project tasks describe the activities and phases that have to be performed in the project such as writing of layouts, customising, testing. What is needed is a system that allows project positions to be correlated with project tasks. Project positions describe project roles like project manager, consultant, tester, etc. Project-positions are typically arranged linearly within the project. By correlating project tasks with project positions, the qualifications and availability of personnel assigned to the project positions may be considered.

== Benefits of Project Management ==
&lt;ref&gt;{{Cite web|title = The advantages of project management and how it can help your business|url = https://www.nibusinessinfo.co.uk/content/advantages-project-management-and-how-it-can-help-your-business|website = nibusinessinfo.co.uk|accessdate = 2015-11-04|last = Migrator}}&lt;/ref&gt; Good project management should:
* Reduce the chance of a project failing
* Ensure a minimum level of quality and that results meet requirements and expectations
* Free up other staff members to get on with their area of work and increase efficiency both on the project and within the business
* Make things simpler and easier for staff with a single point of contact running the overall project
* Encourage consistent communications amongst staff and suppliers
* Keep costs, timeframes and resources to budget

== Workflow Engine ==
When it comes to project workforce management, it is all about managing all the logistic aspects of a project or an organisation through a software application. Usually, this software has a workflow engine defined in them. So, all the logistic processes take place in the workflow engine.

The regular and most common types of tasks handled by project workforce management software or a similar workflow engine are:

=== Planning and Monitoring the Project Schedules and Milestones ===
Regularly monitoring your project’s schedule performance can provide early indications of possible activity-coordination problems, resource conflicts, and possible cost overruns. To monitor schedule performance. Collecting information and evaluating it ensure a project accuracy.&lt;ref&gt;{{Cite web|title = How to Monitor Project-Schedule Performance - For Dummies|url = http://www.dummies.com/how-to/content/how-to-monitor-schedule-performance.html|website = www.dummies.com|accessdate = 2015-11-04}}&lt;/ref&gt;

=== Tracking the Cost and Revenue aspects of Projects ===
The importance of tracking actual costs and resource usage in projects depends upon the project situation.

Tracking actual costs and resource usage is an essential aspect of the project control function.&lt;ref&gt;{{Cite web|title = Why Track Actual Costs and Resource Usage on Projects?|url = http://www.projecttimes.com/articles/why-track-actual-costs-and-resource-usage-on-projects.html|website = www.projecttimes.com|accessdate = 2015-11-04}}&lt;/ref&gt;

=== Resource Utilisation and Monitoring ===
Organisational profitability is directly connected to project management efficiency and optimal resource utilisation.To sum up, organisations that struggle with either or both of these core competencies typically experience cost overruns, schedule delays and unhappy customers.&lt;ref&gt;{{Cite web|title = Resource Utilization in Project Management|url = https://www.clarizen.com/work/resource-utilization-in-project-management|website = www.clarizen.com|accessdate = 2015-11-04}}&lt;/ref&gt;

The focus for project management is the analysis of project performance to determine whether a change is needed in the plan for the remaining project activities to achieve the project goals.&lt;ref&gt;{{Cite web|title = Project Management Guru Monitoring and Controlling Tools|url = http://www.projectmanagementguru.com/controlling.html|website = www.projectmanagementguru.com|accessdate = 2015-11-04}}&lt;/ref&gt;

=== Other Management Aspects of the Project Management&lt;ref&gt;{{Cite web|title = Project Management Guide - How to Manage a Project {{!}} TeamGantt|url = http://teamgantt.com/guide-to-project-management/|website = teamgantt.com|accessdate = 2015-11-04}}&lt;/ref&gt; ===

==== Project risk management ====
Risk identification consists of determining which risks are likely to affect the project and documenting the characteristics of each.

==== Project communication management ====
Project communication management is about how communication is carried out during the course of the project

==== Project quality management ====
It is of no use completing a project within the set time and budget if the final product is of poor quality. The project manager has to ensure that the final product meets the quality expectations of the stakeholders. This is done by good: 

===== ''Quality Planning:'' =====
Identifying what quality standards are relevant to the project and determining how to meet them.

===== ''Quality Assurance:'' =====
Evaluating overall project performance on a regular basis to provide confidence that the project will satisfy the relevant quality standards.

===== ''Quality Control:'' =====

Monitoring specific project results to determine if they comply with relevant quality standards and identifying ways to remove causes of poor performance.

==Project Workforce Management vs. Traditional Management==
There are three main differences between Project Workforce Management and traditional [[project management]] and [[workforce management]] disciplines and solutions:&lt;ref&gt;{{Cite web
|author = Rudolf Melik|title = The Rise of the Project Workforce|url = https://books.google.co.uk/books?id=0b2RB81RqyQC&amp;pg=PA121&amp;lpg=PA121&amp;dq=the+rise+of+project+workforce+pdf&amp;source=bl&amp;ots=_Io_xYQd2Q&amp;sig=4KO0i1Gr5m_XoybVwJHqfP0enHk&amp;hl=en&amp;sa=X&amp;ved=0CDIQ6AEwBGoVChMImJOZ3a33yAIVQ70aCh1yGAq4#v=onepage&amp;q=the%20rise%20of%20project%20workforce%20pdf&amp;f=false|publisher = Wiley: New York, NY|year = 2007|accessdate = November 4, 2015}}&lt;/ref&gt;

=== Workflow-driven ===
All project and workforce processes are designed, controlled and audited using a built-in graphical workflow engine. Users can design, control and audit the different processes involved in the project. The graphical workflow is quite attractive for the users of the system and allows the users to have a clear idea of the workflow engine.&lt;ref&gt;{{Cite book|title = Flexibility of Data-Driven Process Structures|url = http://link.springer.com/chapter/10.1007/11837862_19|publisher = Springer Berlin Heidelberg|date = 2006-09-04|isbn = 978-3-540-38444-1|pages = 181–192|series = Lecture Notes in Computer Science|first = Dominic|last = Müller|first2 = Manfred|last2 = Reichert|first3 = Joachim|last3 = Herbst|editor-first = Johann|editor-last = Eder|editor-first2 = Schahram|editor-last2 = Dustdar}}&lt;/ref&gt;

=== Organisation and Work Breakdown Structures ===
Project Workforce Management provides organization and work breakdown structures to create, manage and report on functional and approval hierarchies, and to track information at any level of detail. Users can create, manage, edit and report work breakdown structures. Work breakdown structures have different abstraction levels, so the information can be tracked at any level. Usually, project workforce management has approval hierarchies. Each workflow created will go through several records before it becomes an organisational or project standard. This helps the organisation to reduce the inefficiencies of the process, as it is audited by many stakeholders.&lt;ref&gt;{{Cite web|title = Organisational Breakdown Structure|url = http://www.successful-project-management.com/organisational-breakdown-structure.html|website = www.successful-project-management.com|accessdate = 2015-11-04}}&lt;/ref&gt;

=== Connected Project, Workforce and Financial Processes ===
Unlike traditional disconnected project, workforce and billing management systems that are solely focused on tracking IT projects, internal workforce costs or billable projects, Project Workforce Management is designed to unify the coordination of all project and workforce processes, whether internal, shared (IT) or billable.

== Summary ==
A project workforce management system defines project tasks, project positions and assigns personnel to the project positions. The project tasks and project positions are correlated to assign a responsible project position or positions to complete each project task. Because each project position may be assigned to a specific person, the qualification and availabilities of the person can be taken into account when determining the assignment. By correlating the project tasks and project positions, a manager can better control the assignment of the workforce and complete projects more efficiently.&lt;ref&gt;{{Citation|title = Project workforce management abstract|url = http://www.google.com/patents/US20030236692|accessdate = 2015-11-04}}&lt;/ref&gt;

Project workflow management is one of the best methods for managing different aspects of project. If the project is complex, then the outcomes for the project workforce management could be more effective.

For simple projects or small organisations, project workflow management may not add much value, but for more complex projects and big organisations, managing project workflow will make a big difference. This is because that small organisations or projects do not have a significant overhead when it comes to managing processes. There are many project workforce management, but many organisations prefer to adopt unique solutions.

Therefore, organisation gets software development companies to develop custom project workflow managing systems for them. This has proved to be the most suitable way of getting the best project workforce management system acquired for the company.

==Literature==
*{{Cite book
 | first = Rudolf
 | last = Melik
 | authorlink =
 | year = 2007
 | title = The Rise of the Project Workforce
 | edition =
 | publisher = Willey
 | location = New York, NY
 | isbn = 0-470-12430-X
}}

==References==
{{Wikiquote}}
{{Reflist}}

[[Category:Data management]]
[[Category:ERP software]]
[[Category:Project management]]
[[Category:Workflow technology]]</text>
      <sha1>jzl9pgkv8qovm896b97m5mo9g34pjq2</sha1>
    </revision>
  </page>
  <page>
    <title>Storage model</title>
    <ns>0</ns>
    <id>8288646</id>
    <revision>
      <id>415292239</id>
      <parentid>387012749</parentid>
      <timestamp>2011-02-22T09:19:21Z</timestamp>
      <contributor>
        <username>Malcolma</username>
        <id>320496</id>
      </contributor>
      <minor />
      <comment>cat</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="319" xml:space="preserve">{{Unreferenced|date=December 2006}}
A '''storage model''' is a model that captures key ''physical'' aspects of data structure in a data store. 

On the other hand, a [[data model]] is a model that captures key ''logical'' aspects of data structure in a database.



[[Category:Data management]]


{{Compu-storage-stub}}</text>
      <sha1>bpfq040ldvq0kamxpkqqlrjtg8q3xhf</sha1>
    </revision>
  </page>
  <page>
    <title>Enterprise bus matrix</title>
    <ns>0</ns>
    <id>29723359</id>
    <revision>
      <id>756513003</id>
      <parentid>749962200</parentid>
      <timestamp>2016-12-24T20:25:36Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 1 sources and tagging 1 as dead. #IABot (v1.2.7.1)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6846" xml:space="preserve">{{Multiple issues|
{{weasel|date=December 2010}}
{{orphan|date=February 2012}}
{{cleanup|date=December 2010}}
}}

The '''Enterprise Bus Matrix''' is a [[data Warehouse]] planning tool and model created by [[Ralph Kimball]], and is part of the Data Warehouse Bus Architecture. The Matrix is the logical definition of one of the core concepts of Kimball’s approach to Dimensional Modeling – Conformed dimensions.&lt;ref&gt;{{cite web|url=http://www.kimballgroup.com/2003/09/15/design-tip-49-off-the-bench/ |title=Design Tip #49: Off The Bench |publisher=Kimball Group |date=2003-09-15 |accessdate=2015-05-22 }}{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}&lt;/ref&gt;

The Bus Matrix defines part of the Data Warehouse Bus Architecture and is an output of the Business Requirements phase in [[The Kimball Lifecycle]]. It is applied in the following phases of [[dimensional modeling]] and development of the Data Warehouse . The matrix can be categorized as a hybrid model, being part technical design tool, part project management tool and part communication tool&lt;ref name="Kimball"&gt;Kimball, Ralph &amp; Ross, Margy; The Data Warehouse Toolkit: The Complete Guide to Dimensional Modeling, 2nd Edition John Wiley &amp; Sons, 2002&lt;/ref&gt;

==Background==
The Enterprise Bus Matrix stems from the issue of how one goes about creating the overall Data Warehouse environment.  Historically there has been the structure of the centralized and planned approach and the more loosely defined, department specific, solutions developed in a more independent matter. Autonomous projects can result in a range of isolated stove pipe data marts. Naturally each approach has its issues; the overall visionary approach often struggles with long delivery cycles and lack of reaction time as the formalities and scope issues is evident. On the other hand, the development of isolated data marts, leading to [[Stovepipe system]]s that lacks synergy in development. Over time this approach will lead to a so-called data-mart-in-a-box architecture&lt;ref&gt;[http://www.mimno.com/avoiding-mistakes3.html#6]  {{webarchive |url=https://web.archive.org/web/20100704220014/http://www.mimno.com/avoiding-mistakes3.html#6 |date=July 4, 2010 }}&lt;/ref&gt; where [[interoperability]] and lack of cohesion is apparent, and can hinder the realization of an overall enterprise Data Warehouse. As an attempt to handle this matter [[Ralph Kimball]] introduced the enterprise bus.

==Bus matrix==
The bus matrix purpose is one of high abstraction and visionary planning on the Data Warehouse architectural level. By dictating coherency in the development and implementation of an overall Data Warehouse the Bus Architecture approach enables an overall vision of the broader enterprise integration and consistency while at the same time dividing the problem into more manageable parts&lt;ref name="Kimball" /&gt; – all in a technology and software independent manner .&lt;ref&gt;{{cite web|url=http://www.b-eye-network.com/view/713 |title=Data Warehouse: Ralph Kimball’s Vision by Katherine Drewek |publisher=Beyenetwork |date=2005-03-16 |accessdate=2015-05-22}}&lt;/ref&gt;

The bus matrix and architecture builds upon the concept of conformed dimensions -  creating a structure of common dimensions that ideally can be used across the enterprise by all business processes related to the DW and the corresponding fact tables from which they derive their context. According to Kimball and Margy Ross's article  “Differences of Opinion”&lt;ref&gt;{{cite web|url=http://intelligent-enterprise.informationweek.com/showArticle.jhtml;jsessionid=0OVJNEHMPRXGRQE1GHRSKH4ATMY32JVN?articleID=17800088 |title=Enterprise Software News, Analysis, &amp; Advice - InformationWeek |publisher=Intelligent-enterprise.informationweek.com |date= |accessdate=2015-05-22}}&lt;/ref&gt; "''The Enterprise Data warehouse built on the bus architecture ”identifies and enforces the relationship between business process metrics (facts) and descriptive attributes (dimensions)''”.

The concept of a [[Bus (computing)|bus]] is well known in the language of [[Information Technology]], and is what reflects the conformed dimension concept in the Data Warehouse, creating the skeletal structure where all parts of a system connect, ensuring [[interoperability]] and consistency of data, and at the same time considers future expansion. This makes the conformed dimensions act as the integration ‘glue’, creating a robust backbone of the enterprise Data Warehouse.&lt;ref&gt;{{cite web|url=http://intelligent-enterprise.informationweek.com/showArticle.jhtml;jsessionid=GMS3H4SOBFQBBQE1GHOSKH4ATMY32JVN?articleID=17800088&amp;pgno=2 |title=Enterprise Software News, Analysis, &amp; Advice - InformationWeek |publisher=Intelligent-enterprise.informationweek.com |date= |accessdate=2015-05-22}}&lt;/ref&gt;

==Establishment and applicability==
Figure 1&lt;ref&gt;{{cite web|url=http://www.widama.us/Documents/Kimball-DimensionalModeling.PDF |format=PDF |title=Dimensional Modeling Overview |author=Bob Becker |publisher=Widama.is |accessdate=2015-05-22 |deadurl=yes |archiveurl=https://web.archive.org/web/20130322224742/http://www.widama.us:80/Documents/Kimball-DimensionalModeling.PDF |archivedate=2013-03-22 |df= }}&lt;/ref&gt; shows the base for a single document planning tool for the whole of the DW implementation - a graphical overview of the enterprises core business processes or events each correspond to a measurement table of facts, that typically is complemented by a major source system in the horizontal rows.  In the vertical columns the groups of contextual data is found as the common, conformed dimensions.

In this way the shared dimensions are defined, as each process indicates what dimensions it applies to through the cells figure 2.&lt;ref name="Kimball" /&gt; By this definition and coordination of conformed dimensions and processes the development of the overall data DW bus architecture is realized.&lt;ref name="Kimball" /&gt; The matrix identifies the shared dimensions related to processes and fact tables, and can be a tool for planning, prioritizing what needs to be approached, coordinating implementation and communicating the importance for conformed dimensions .

Kimball extends the matrix bus in detail as seen in figure 3&lt;ref name="Kimball" /&gt;  by introducing the other steps of the Datawarehouse Methodology; The Fact tables, Granularity, and at last the description of the needed facts.  description of the fact tables, granularity and fact instances of each process, structuring and specifying what is needed across the enterprise in a more specific matter, further exemplifying how the matrix can be used as a planning tool.

==References==
{{Reflist}}

{{DEFAULTSORT:Enterprise bus matrix}}
[[Category:Business intelligence]]
[[Category:Data management]]
[[Category:Data warehousing]]
[[Category:Information technology management]]</text>
      <sha1>kt996n0ouc3d3kj8dopy46863djn271</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Data centers</title>
    <ns>14</ns>
    <id>24125707</id>
    <revision>
      <id>588264584</id>
      <parentid>547201728</parentid>
      <timestamp>2013-12-29T21:10:37Z</timestamp>
      <contributor>
        <username>BotMultichill</username>
        <id>4080734</id>
      </contributor>
      <minor />
      <comment>Adding Commons category link to [[:Commons:Category:Data centers|category with the same name]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="151" xml:space="preserve">{{Commons category|Data centers}}
{{catmain|Data center}}

[[Category:Data management|Centers]]
[[Category:Servers (computing)]]
[[Category:Computers]]</text>
      <sha1>ogk6dlmn54xiyhroes9di4k1vi6r4of</sha1>
    </revision>
  </page>
  <page>
    <title>Global serializability</title>
    <ns>0</ns>
    <id>11861063</id>
    <revision>
      <id>762614467</id>
      <parentid>760531997</parentid>
      <timestamp>2017-01-29T20:58:54Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor />
      <comment>Reformat 1 archive link. [[User:Green Cardamom/WaybackMedic_2.1|Wayback Medic 2.1]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="35296" xml:space="preserve">{{Technical|date=January 2017}}
In [[concurrency control]] of ''[[database]]s'', ''[[transaction processing]]'' (''transaction management''), and other transactional [[Distributed computing|distributed applications]], '''Global serializability''' (or '''Modular serializability''') is a property of a ''global schedule'' of [[Database transaction|transactions]]. A global schedule is the unified [[schedule (computer science)|schedule]] of all the individual database (and other [[transactional object]]) schedules in a multidatabase environment (e.g., [[federated database]]). Complying with global serializability means that the global schedule is ''[[serializable (databases)|serializable]]'', has the ''[[serializability]]'' property, while each component database (module) has a serializable schedule as well. In other words, a collection of serializable components provides overall system serializability, which is usually incorrect. A need in correctness across databases in multidatabase systems makes global serializability a major goal for ''[[global concurrency control]]'' (or ''modular concurrency control''). With the proliferation of the [[Internet]], [[Cloud computing]], [[Grid computing]], and small, portable, powerful computing devices (e.g., [[smartphone]]s), as well as increase in [[systems management]] sophistication, the need for atomic distributed transactions and thus effective global serializability techniques, to ensure correctness in and among distributed transactional applications, seems to increase.

In a [[federated database system]] or any other more loosely defined multidatabase system, which are typically distributed in a communication network, transactions span multiple (and possibly [[Distributed database|distributed]]) databases. Enforcing global serializability in such system, where different databases may use different types of [[concurrency control]], is problematic. Even if every local schedule of a single database is serializable, the global schedule of a whole system is not necessarily serializable. The massive communication exchanges of conflict information needed between databases to reach [[Serializability#View and conflict serializability|conflict serializability]] globally would lead to unacceptable performance, primarily due to computer and communication [[latency (engineering)|latency]]. Achieving global serializability effectively over different types of concurrency control has been [[Open problem|open]] for several years. ''[[Commitment ordering]]'' (or Commit ordering; CO), a serializability technique publicly introduced in 1991 by [[Yoav Raz]] from [[Digital Equipment Corporation]] (DEC), provides an effective general solution for global ([[Serializability#View and conflict serializability|conflict]]) serializability across any collection of database systems and other [[transactional object]]s, with possibly different concurrency control mechanisms. CO does not need the distribution of conflict information, but rather utilizes the already needed (unmodified) [[atomic commitment]] protocol messages without any further communication between databases. It also allows [[Optimistic concurrency control|optimistic]] (non-blocking) implementations. CO generalizes ''[[Two-phase locking|Strong strict two phase locking]]'' (SS2PL), which in conjunction with the ''[[Two-phase commit protocol|Two-phase commit]]'' (2PC) protocol is the [[de facto standard]] for achieving global serializability across (SS2PL based) database systems. As a result, CO compliant database systems (with any, different concurrency control types) can transparently join existing SS2PL based solutions for global serializability. The same applies also to all other multiple (transactional) object systems that use atomic transactions and need global serializability for correctness (see examples above; nowadays such need is not smaller than with database systems, the origin of atomic transactions).

The most significant aspects of CO that make it a uniquely effective general solution for global serializability are the following:
#Seamless, low overhead integration with any concurrency control mechanism, with neither changing any transaction's operation scheduling or blocking it, nor adding any new operation.
#[[Heterogeneity]]: Global serializability is achieved across multiple [[transactional objects]] (e.g., [[database management system]]s) with different (any) concurrency control mechanisms, without interfering with the mechanisms' operations.
#[[Modularity]]: Transactional objects can be added and removed transparently.
#[[Commitment ordering#CO is a necessary condition for global serializability across autonomous database systems|Autonomy]] of transactional objects: No need of conflict or equivalent information distribution (e.g., local precedence relations, locks, timestamps, or tickets; no object needs other object's information).
#[[Scalability]]: With "normal" global transactions, [[computer network]] size and number of transactional objects can increase unboundedly with no impact on performance, and
#Automatic global deadlock resolution.

All these aspects, except the first two, are also possessed by the popular [[Two-phase locking|SS2PL]], which is a (constrained, blocking) special case of CO and inherits many of CO's qualities.

==The global serializability problem==

===Problem statement===

The difficulties described above translate into the following problem:
:Find an efficient (high-performance and [[fault tolerant]]) method to enforce ''Global serializability'' (global conflict serializability) in a heterogeneous distributed environment of multiple autonomous database systems. The database systems may employ different [[concurrency control]] methods. No limitation should be imposed on the operations of either local transactions (confined to a single database system) or [[distributed transaction|global transactions]] (span two or more database systems).

===Quotations===
Lack of an appropriate solution for the global serializability problem has driven researchers to look for alternatives to [[serializability]] as a correctness criterion in a multidatabase environment (e.g., see ''[[Global serializability#Relaxing global serializability|Relaxing global serializability]]'' below), and the problem has been characterized as difficult and ''[[open problem|open]]''. The following two quotations demonstrate the mindset about it by the end of the year 1991, with similar quotations in numerous other articles:

*"Without knowledge about local as well as global transactions, it is highly unlikely that efficient global concurrency control can be provided... Additional complications occur when different component DBMSs [Database Management Systems] and the FDBMSs [Federated Database Management Systems] support different concurrency mechanisms... It is unlikely that a theoretically elegant solution that provides conflict serializability without sacrificing performance (i.e., concurrency and/or response time) and [[availability]] exists."&lt;ref&gt;Amit Sheth, James Larson (1990): [http://www.informatik.uni-trier.de/~ley/db/journals/csur/ShethL90.html  "Federated Database Systems for Managing Distributed, Heterogeneous, and Autonomous Databases"], ''ACM Computing Surveys'', Vol. 22, No 3, pp. 183-236, September 1990 (quotation from page 227)&lt;/ref&gt;

[[Commitment ordering]],&lt;ref name=Raz1992/&gt;&lt;ref name=Raz1994/&gt; publicly introduced in May 1991 (see below), provides an efficient [[Elegance|elegant]] general solution, from both practical&lt;ref name=Raz1990/&gt;&lt;ref name=Raz1991/&gt; and [[Theory|theoretical]]&lt;ref name=Raz2009/&gt; points of view, to the global serializability problem across database systems with possibly different concurrency control mechanisms. It provides conflict serializability with no negative effect on availability, and with no worse performance than the [[de facto standard]] for global serializability, CO's special case [[Two-phase locking#Strong strict two-phase locking|Strong strict two-phase locking]] (SS2PL). It requires knowledge about neither local nor global transactions.

*"Transaction management in a heterogeneous, distributed database system is a difficult  issue. The main problem is that each of the local database management systems may be using a different type of concurrency control scheme. Integrating this is a challenging problem, made worse if we wish to preserve the local autonomy of each of the local databases, and allow local and global transactions to execute in parallel. One simple solution is to restrict global transactions to retrieve-only access. However, the issue of reliable transaction management in the general case, where global and local transactions are allowed to both read and write data, is [[open problem|still open]]."&lt;ref&gt;[[Abraham Silberschatz]], [[Michael Stonebraker]], and [[Jeffrey Ullman]] (1991): [http://www.informatik.uni-trier.de/~ley/db/journals/cacm/SilberschatzSU91.html  "Database Systems: Achievements and Opportunities"], ''Communications of the ACM'', Vol. 34, No. 10, pp. 110-120, October 1991 (quotation from page 120)&lt;/ref&gt;

The commitment ordering solution comprises effective integration of autonomous database management systems with possibly different concurrency control mechanisms. This while local and global transactions execute in parallel without restricting any read or write operation in either local or global transactions, and without compromising the systems' autonomy.

Even in later years, after the public introduction of the Commitment ordering general solution in 1991, the problem still has been considered by many unsolvable:

*"We present a transaction model for multidatabase systems with autonomous component systems, coined heterogeneous 3-level transactions. It has become evident that in such a system the requirements of guaranteeing full [[ACID]] properties and full local autonomy can not be reconciled..."&lt;ref&gt;Peter Muth (1997): [http://portal.acm.org/citation.cfm?id=264226  "Application Specific Transaction Management in Multidatabase Systems"], ''Distributed and Parallel Databases'', Volume 5, Issue 4, pp. 357 - 403, October 1997, {{ISSN|0926-8782}} (quotation from the article's Abstract)&lt;/ref&gt;

The quotation above is from a 1997 article proposing a relaxed global serializability solution (see ''[[Global serializability#Relaxing global serializability|Relaxing global serializability]]'' below), and referencing [[Commitment ordering]] (CO) articles. The CO solution supports effectively both full [[ACID]] properties and full local autonomy, as well as meeting the other requirements posed above in the ''[[Global serializability#Problem statement|Problem statement]]'' section, and apparently has been misunderstood.

Similar thinking we see also in the following quotation from a 1998 article:

*"The concept of serializability has been the traditionally accepted correctness criterion in database systems. However in multidatabase systems (MDBSs), ensuring global serializability is a difficult task. The difficulty arises due to the heterogeneity of the concurrency control protocols used by the participating local database management systems (DBMSs), and the desire to preserve the autonomy of the local DBMSs. In general, solutions to the global serializability problem result in executions with a low degree of concurrency. The alternative, relaxed serializability, may result in data inconsistency."&lt;ref name=Shar1998&gt;Sharad Mehrotra, Rajeev Rastogi, Henry Korth, [[Abraham Silberschatz]] (1998):
[http://portal.acm.org/citation.cfm?id=277629 "Ensuring Consistency in Multidatabases by Preserving Two-Level Serializability"], ''ACM Transactions on Database Systems'' (TODS), Vol. 23, No. 2, pp. 199-230, June 1998 (quotation from the article's Abstract)&lt;/ref&gt;

Also the above quoted article proposes a relaxed global serializability solution, while referencing the CO work. The CO solution for global serializability both bridges between different concurrency control protocols with no substantial concurrency reduction (and typically minor, if at all), and maintains the autonomy of local DBMSs. Evidently also here CO has been misunderstood. This misunderstanding continues to 2010 in a textbook by some of the same authors, where the same relaxed global serializability technique, ''Two level serializability'', is emphasized and described in detail, and CO is not mentioned at all.&lt;ref name=Silber2010&gt;[[Abraham Silberschatz|Avi Silberschatz]], Henry F Korth, S. Sudarshan (2010): [http://highered.mcgraw-hill.com/sites/0073523321/  ''Database System Concepts''], 6th Edition, McGraw-Hill, ISBN 0-07-295886-3&lt;/ref&gt;

On the other hand, the following quotation on CO appears in a 2009 book:&lt;ref name=Bern2009&gt;[[Phil Bernstein|Philip A. Bernstein]], Eric Newcomer (2009): [http://www.elsevierdirect.com/product.jsp?isbn=9781558606234 ''Principles of Transaction Processing'', 2nd Edition],  Morgan Kaufmann (Elsevier), June 2009, ISBN 978-1-55860-623-4 (quotation from page 145)&lt;/ref&gt;

*"Not all concurrency control algorithms use locks... Three other techniques are timestamp ordering, serialization graph testing, and commit ordering. '''Timestamp ordering''' assigns each transaction a timestamp and ensures that conflicting operations execute in timestamp order. '''Serialization graph testing''' tracks conflicts and ensures that the serialization graph is acyclic. '''Commit ordering''' ensures that conflicting operations are consistent with the relative order in which their transactions commit, which can enable interoperability of systems using different concurrency control mechanisms."

:'''Comments:'''
#Beyond the common locking based algorithm SS2PL, which is a CO variant itself, also additional variants of CO that use locks exist, (see below). However, generic, or "pure" CO does not use locks.
#Since CO mechanisms order the commit events according to conflicts that already have occurred, it is better to describe CO as "'''Commit ordering''' ensures that the relative order in which transactions commit is consistent with the order of their respective conflicting operations."

The characteristics and properties of the CO solution are discussed below.

===Proposed solutions===
Several solutions, some partial, have been proposed for the global serializability problem. Among them:

* ''Global [[serializability#Testing conflict serializability|conflict graph]]'' (serializability graph, [[precedence graph]]) ''checking''
* ''Distributed [[Two phase locking]]'' (Distributed 2PL)
* ''Distributed [[Timestamp-based concurrency control|Timestamp ordering]]''
* ''Tickets'' (local logical timestamps which define local total orders, and are propagated to determine global partial order of transactions)
* ''Commitment ordering''

===Technology perspective===
The problem of global serializability has been a quite intensively researched subject in the late 1980s and early 1990s. ''Commitment ordering'' (CO) has provided an effective general solution to the problem, insight into it, and understanding about possible generalizations of ''[[serializability#Common mechanism - SS2PL|strong strict two phase locking]]'' (SS2PL), which practically and almost exclusively has been utilized (in conjunction with the ''[[Two-phase commit protocol]]'' (2PC) ) since the 1980s to achieve global serializability across databases. An important side-benefit of CO is the automatic ''global deadlock'' resolution that it provides (this is applicable also to distributed SS2PL; though global deadlocks have been an important research subject for SS2PL, automatic resolution has been overlooked, except in the CO articles, until today (2009)). At that time quite many commercial database system types existed, many non-relational, and databases were relatively very small. Multi database systems were considered a key for database scalability by database systems interoperability, and global serializability was urgently needed. Since then the tremendous progress in computing power, storage, and communication networks, resulted in [[Order of magnitude|orders of magnitude]] increases in both centralized databases' sizes, transaction rates, and remote access to database capabilities, as well as blurring the boundaries between centralized computing and distributed one over fast, low-latency local networks (e.g., [[Infiniband]]). These, together with progress in database vendors' distributed solutions (primarily the popular SS2PL with 2PC based, a [[de facto standard]] that allows interoperability among different vendors' (SS2PL-based) databases; both SS2PL and 2PC technologies have gained substantial expertise and efficiency), [[workflow]] management systems, and [[database replication]] technology, in most cases have provided satisfactory and sometimes better [[information technology]] solutions without multi database atomic [[distributed transaction]]s over databases with different concurrency control (bypassing the problem above). As a result, the sense of urgency that existed with the problem at that period, and in general with high-performance distributed atomic transactions over databases with different concurrency control  types, has reduced. However, the need in concurrent distributed atomic transactions as a fundamental element of reliability exists in distributed systems also beyond database systems, and so the need in global serializability as a fundamental correctness criterion for such transactional systems (see also [[Serializability#Distributed serializability|Distributed serializability]] in [[Serializability]]). With the proliferation of the [[Internet]], [[Cloud computing]], [[Grid computing]], small, portable, powerful computing devices (e.g., [[smartphone]]s), and sophisticated [[systems management]] the need for effective global serializability techniques to ensure correctness in and among distributed transactional applications seems to increase, and thus also the need in Commitment ordering (including the popular for databases special case SS2PL; SS2PL, though, does not meet the requirements of many other transactional objects).

==The commitment ordering solution==
{{POV-section|Commitment ordering|date=November 2011}}
{{main|Commitment ordering}}
{{main|The History of Commitment Ordering}}

Commitment ordering&lt;ref name=Raz1992&gt;[[Yoav Raz]] (1992): [http://www.informatik.uni-trier.de/~ley/db/conf/vldb/Raz92.html "The Principle of Commitment Ordering, or Guaranteeing Serializability in a Heterogeneous Environment of Multiple Autonomous Resource Managers Using Atomic Commitment"] {{webarchive |url=https://web.archive.org/web/20070523182950/http://www.informatik.uni-trier.de/~ley/db/conf/vldb/Raz92.html |date=May 23, 2007 }}, ''Proc. of the Eighteenth Int. Conf. on Very Large Data Bases'' (VLDB), pp. 292-312, Vancouver, Canada, August 1992. (also DEC-TR 841, [[Digital Equipment Corporation]], November 1990) 
&lt;/ref&gt;&lt;ref name=Raz1994&gt;Yoav Raz (1994): [http://linkinghub.elsevier.com/retrieve/pii/0020019094900051 "Serializability by Commitment Ordering"], ''Information Processing Letters'', [http://www.informatik.uni-trier.de/~ley/db/journals/ipl/ipl51.html#Raz94 Volume 51, Number 5],  pp. 257-264, September 1994. (Received August 1991)&lt;/ref&gt; (or Commit ordering; CO) is the only high-performance, [[fault tolerant]], [[Serializability#View and conflict serializability|conflict serializability]] providing solution that has been proposed as a fully distributed (no central computing component or data-structure are needed), general mechanism that can be combined seamlessly with any local (to a database) [[concurrency control]] mechanism (see [[Commitment ordering#Summary|technical summary]]). Since the CO property of a schedule is a [[necessary condition]] for global serializability of [[Commitment ordering#CO is a necessary condition for global serializability across autonomous database systems|''autonomous databases'']] (in the context of concurrency control), it provides the only general solution for autonomous databases (i.e., if autonomous databases do not comply with CO, then global serializability may be violated). Seemingly by sheer luck, the CO solution possesses many attractive properties: 
 
#does not interfere with any transaction's operation, particularly neither block, restrict nor delay any data-access operation (read or write) for either local or [[distributed transaction|global]] transactions (and thus does not cause any extra aborts); thus allows seamless integration with any concurrency control mechanism.
#allows [[Optimistic concurrency control|optimistic]] implementations (''non-blocking'', i.e., non data access blocking).
#allows [[heterogeneity]]: Global serializability is achieved across multiple [[transactional objects]] with different (any) concurrency control mechanisms, without interfering with the mechanisms' operations.
#allows [[modularity]]: Transactional objects can be added and removed transparently.
#allows full [[ACID]] transaction support.
#maintains each database's [[Commitment ordering#CO is a necessary condition for global serializability across autonomous database systems|autonomy]], and does not need any concurrency control information distribution (e.g., local precedence relations, locks, timestamps, or tickets).
#does not need any knowledge about the transactions.
#requires no communication overhead since it only uses already needed, unmodified ''[[atomic commitment]]'' protocol messages (any such protocol; using [[fault tolerant]] atomic commitment protocols and database systems makes the CO solution fault tolerant).
#automatically resolves global [[deadlock]]s due to [[lock (computer science)|locking]].
#[[Scalability|scales up]] effectively with [[computer network]] size and number of databases, almost without any negative impact on performance, since each global transaction is typically confined to certain relatively small numbers of databases and network nodes.
#requires no additional, artificial transaction access operations (e.g., "take [[Timestamp-based concurrency control|timestamp]]" or "take ticket"), which typically result in  additional, artificial conflicts that reduce concurrency.
#requires low overhead.

The only overhead incurred by the CO solution is locally detecting conflicts (which is already done by any known serializability mechanism, both pessimistic and optimistic) and locally ordering in each database system both the (local) commits of local transactions and the voting for atomic commitment of global transactions. Such overhead is low. The net effect of CO may be some delays of commit events (but never more delay than SS2PL, and on the average less). This makes CO instrumental for global concurrency control of multidatabase systems (e.g., [[federated database system]]s). The underlying ''Theory of Commitment ordering'',&lt;ref name=Raz2009&gt;Yoav Raz (2009): [http://sites.google.com/site/yoavraz2/home/theory-of-commitment-ordering Theory of Commitment Ordering - Summary] GoogleSites - Site of Yoav Raz. Retrieved 1 Feb, 2011.&lt;/ref&gt; part of [[Serializability]] theory, is both sound and [[Scientific method#Hypothesis development|elegant]] (and even [[Mathematical beauty|"mathematically beautiful"]]; referring to structure and dynamics of conflicts, graph cycles, and deadlocks), with interesting implications for transactional [[Distributed computing|distributed applications]].

All the qualities of CO in the list above, except the first three, are also possessed by SS2PL, which is a special case of CO, but blocking and constraining. This partially explains the popularity of SS2PL as a solution (practically, the only solution, for many years) for achieving global serializability. However, property 9 above, automatic resolution of global deadlocks, has not been noticed for SS2PL in the database research literature until today (2009; except in the CO publications). This, since the phenomenon of voting-deadlocks in such environments and their automatic resolution by the [[atomic commitment]] protocol has been overlooked.

Most existing database systems, including all major commercial database systems, are ''[[serializability#Common mechanism - SS2PL|strong strict two phase locking (SS2PL)]]'' based and already CO compliant. Thus they can participate in a [[commitment ordering#Summary|CO based solution for global serializability in multidatabase environments]] without any modification (except for the popular ''[[Multiversion concurrency control|multiversioning]]'', where additional CO aspects should be considered). Achieving global serializability across SS2PL based databases using atomic commitment (primarily using ''[[two phase commit]], 2PC'') has been employed for many years (i.e., using the same CO solution for a specific special case; however, no reference is known prior to CO, that notices this special case's automatic global deadlock resolution by the atomic commitment protocol's [[commitment ordering#Exact characterization of voting-deadlocks by global cycles|augmented-conflict-graph]] global cycle elimination process). Virtually all existing distributed transaction processing environments and supporting products rely on SS2PL and provide 2PC. As a matter of fact SS2PL together with 2PC have become a [[de facto standard]]. This solution is a homogeneous concurrency control one, suboptimal (when both Serializability and [[Schedule (computer science)#Strict|Strictness]] are needed; see [[Commitment ordering#Strict CO (SCO)|Strict commitment ordering]]; SCO) but still quite effective in most cases, sometimes at the cost of increased computing power needed relatively to the optimum. (However, for better performance [[Serializability#Relaxing serializability|relaxed serializability]] is used whenever applications allow). It allows inter-operation among SS2PL-compliant different database system types, i.e., allows heterogeneity in aspects other than concurrency control. SS2PL is a very constraining schedule property, and "takes over" when combined with any other property. For example, when combined with any [[Concurrency control#Concurrency control mechanisms|optimistic property]], the result is not optimistic anymore, but rather characteristically SS2PL. On the other hand, CO does not change data-access scheduling patterns at all, and ''any'' combined property's characteristics remain unchanged. Since also CO uses atomic commitment (e.g., 2PC) for achieving global serializability, as SS2PL does, any CO compliant database system or transactional object can transparently join existing SS2PL based environments, use 2PC, and maintain global serializability without any environment change. This makes CO a straightforward, natural generalization of SS2PL for any conflict serializability based database system, for all practical purposes.

Commitment ordering has been quite widely known inside the ''[[transaction processing]]'' and ''[[database]]s'' communities at ''[[Digital Equipment Corporation]]'' (DEC) since 1990. It has been under ''company confidentiality'' due to [[patent]]ing&lt;ref name=Raz1990&gt;Yoav Raz (1990): [http://yoavraz.googlepages.com/DEC-CO-MEMO-90-11-16.pdf  ''On the Significance of Commitment Ordering''] - Call for patenting, Memorandum, [[Digital Equipment Corporation]], November 1990.&lt;/ref&gt;
&lt;ref name=Raz1991&gt;
Yoav Raz: US patents [http://patft1.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=3&amp;f=G&amp;l=50&amp;co1=AND&amp;d=PTXT&amp;s1=%22commitment+ordering%22.TI.&amp;OS=TTL/ 5,504,899]  [http://patft1.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=2&amp;f=G&amp;l=50&amp;co1=AND&amp;d=PTXT&amp;s1=%22commitment+ordering%22.TI.&amp;OS=TTL/ 5,504,900]   [http://patft1.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;co1=AND&amp;d=PTXT&amp;s1=%22commitment+ordering%22.TI.&amp;OS=TTL/ 5,701,480]&lt;/ref&gt; processes. CO was disclosed outside of DEC by lectures and technical reports' distribution to database researches in May 1991, immediately after its first patent filing. It has been misunderstood by many database researchers years after its introduction, which is evident by the quotes above from articles in 1997-1998 referencing Commitment ordering articles. On the other hand, CO has been utilized extensively as a solution for global serializability in works on [[Transactional processes]],
&lt;ref&gt;Heiko Schuldt, Hans-Jörg Schek, and Gustavo Alonso (1999): [http://portal.acm.org/citation.cfm?id=853907 "Transactional Coordination Agents for Composite Systems"], In ''Proceedings of the 3rd International Database Engineering and Applications Symposium'' (IDEAS’99), IEEE Computer Society Press, Montrteal, Canada, pp. 321–331.&lt;/ref&gt;
&lt;ref&gt;Klaus Haller, Heiko Schuldt, Can Türker (2005): [http://portal.acm.org/citation.cfm?doid=1099554.1099563 "Decentralized coordination of transactional processes in peer-to-peer environments",] ''Proceedings of the 2005 ACM CIKM, International Conference on Information and Knowledge Management'', pp. 28-35, Bremen, Germany, October 31 - November 5, 2005, ISBN 1-59593-140-6&lt;/ref&gt; and more recently in the related '''Re:GRIDiT''', 
&lt;ref&gt;Laura Cristiana Voicu, Heiko Schuldt, Fuat Akal, Yuri Breitbart, Hans Jörg Schek (2009): [http://dbis.cs.unibas.ch/publications/2009/grid2009/dbis_publication_view  "Re:GRIDiT – Coordinating Distributed Update Transactions on Replicated Data in the Grid"], ''10th IEEE/ACM International Conference on Grid Computing (Grid 2009)'', Banff, Canada, 2009/10.&lt;/ref&gt;
&lt;ref&gt;Laura Cristiana Voicu and Heiko Schuldt (2009): [http://dbis.cs.unibas.ch/publications/2009/clouddb09/dbis_publication_view  "How Replicated Data Management in the Cloud can benefit from a Data Grid Protocol — the Re:GRIDiT Approach"], ''Proceedings of the 1st International Workshop on Cloud Data Management (CloudDB 2009)'', Hong Kong, China, 2009/11.&lt;/ref&gt;
which is an approach for transaction management in the converging [[Grid computing]] and [[Cloud computing]]. 
See more in ''[[The History of Commitment Ordering]]''.

==Relaxing global serializability==
Some techniques have been developed for '''relaxed global serializability''' (i.e., they do not guarantee global serializability; see also ''[[Serializability#Relaxing serializability|Relaxing serializability]]''). Among them (with several publications each):

* ''Quasi serializability''&lt;ref name=Du1989&gt;Weimin Du and Ahmed K. Elmagarmid (1989): [http://www.informatik.uni-trier.de/~ley/db/conf/vldb/DuE89.html  "Quasi Serializability: a Correctness Criterion for Global Concurrency Control in InterBase"], ''Proceedings of the Fifteenth International Conference on Very Large Data Bases'' (VLDB), August 22–25, 1989, Amsterdam, The Netherlands, pp. 347-355, Morgan Kaufmann, ISBN 1-55860-101-5&lt;/ref&gt;
* ''Two-level serializability''&lt;ref name=Shar1998 /&gt;

While local (to a database system) relaxed serializability methods compromise ''serializability'' for performance gain (and are utilized only when the application can tolerate possible resulting inaccuracies, or its integrity is unharmed), it is unclear that various proposed ''relaxed global serializability'' methods which compromise ''global serializability'', provide any performance gain over ''commitment ordering'' which guarantees global serializability. Typically, the declared intention of such methods has not been performance gain over effective global serializability methods (which apparently have been unknown to the inventors), but rather correctness criteria alternatives due to lack of a known effective global serializability method. Oddly, some of them were introduced years after CO had been introduced, and some even quote CO without realizing that it provides an effective global serializability solution, and thus without providing any performance comparison with CO to justify them as alternatives to global serializability for some applications (e.g., ''Two-level serializability''&lt;ref name=Shar1998 /&gt;). ''Two-level serializability'' is even presented as a major global concurrency control method in a 2010 edition of a text-book on databases&lt;ref name=Silber2010/&gt; (authored by two of the original authors of Two-level serializability, where one of them, [[Abraham Silberschatz|Avi Silberschatz]], is also an author of the original ''[[The History of Commitment Ordering#AESO is modified to Strong recoverability (CO)|Strong recoverability]]'' articles). This book neither mentions CO nor references it, and strangely, apparently does not consider CO a valid ''Global serializability'' solution.

Another common reason nowadays for Global serializability relaxation is the requirement of [[availability]] of [[internet]] products and [[Internet service provider|services]]. This requirement is typically answered by large scale data [[Replication (computer science)|replication]]. The straightforward solution for synchronizing replicas' updates of a same database object is including all these updates in a single atomic [[distributed transaction]]. However, with many replicas such a transaction is very large, and may span several [[computer]]s and [[computer network|networks]] that some of them are likely to be unavailable. Thus such a transaction is likely to end with abort and miss its purpose.&lt;ref name=Gray1996&gt;{{cite conference
 | author = [[Jim Gray (computer scientist)|Gray, J.]]
 |author2=Helland, P. |author3=[[Patrick O'Neil|O’Neil, P.]] |author4=[[Dennis Shasha|Shasha, D.]]
 | year = 1996
 | title = The dangers of replication and a solution
 | conference = Proceedings of the 1996 [[ACM SIGMOD International Conference on Management of Data]]
 | pages = 173–182
 | url = ftp://ftp.research.microsoft.com/pub/tr/tr-96-17.pdf
 | doi = 10.1145/233269.233330
 }}&lt;/ref&gt;
Consequently, [[Optimistic replication]] (Lazy replication) is often utilized (e.g., in many products and services by [[Google]], [[Amazon.com|Amazon]], [[Yahoo]], and alike), while Global serializability is relaxed and compromised for [[Eventual consistency]]. In this case relaxation is done only for applications that are not expected to be harmed by it.

Classes of schedules defined by ''relaxed global serializability'' properties either contain the global serializability class, or are incomparable with it. What differentiates techniques for ''relaxed global conflict serializability'' (RGCSR) properties from those of ''relaxed conflict serializability'' (RCSR) properties that are not RGCSR is typically the different way ''global cycles'' (span two or more databases) in the ''global conflict graph'' are handled. No distinction between global and local cycles exists for RCSR properties that are not RGCSR. RCSR contains RGCSR. Typically RGCSR techniques eliminate local cycles, i.e., provide ''local serializability'' (which can be achieved effectively by regular, known [[concurrency control]] methods), however, obviously they do not eliminate all global cycles (which would achieve global serializability).

==References==
{{reflist|33em}}

{{DEFAULTSORT:Global Serializability}}
[[Category:Data management]]
[[Category:Databases]]
[[Category:Transaction processing]]
[[Category:Concurrency control]]</text>
      <sha1>6kbogavwkurhbujphsrcdyn24s4r9ms</sha1>
    </revision>
  </page>
  <page>
    <title>Master data management</title>
    <ns>0</ns>
    <id>15103022</id>
    <revision>
      <id>761409994</id>
      <parentid>759853047</parentid>
      <timestamp>2017-01-22T21:05:13Z</timestamp>
      <contributor>
        <username>RickBeesley</username>
        <id>29355407</id>
      </contributor>
      <comment>Added a paragraph about a key problem that frequently defeats efforts to reduce overhead through MDM. May be a bit long-winded. Based on my own experience in 20 years working at a major global consulting brand.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11326" xml:space="preserve">{{refimprove|date=April 2012}}
In business, '''master data management''' ('''MDM''') comprises the processes, governance, policies, standards and tools that consistently define and manage the critical data of an [[organization]] to provide a single point of reference.&lt;ref&gt;"What is Master Data" SearchDataManagement, TechTarget, 22 November 2010, http://searchdatamanagement.techtarget.com/definition/master-data-management&lt;/ref&gt;

The data that is mastered may include:

* [[reference data]] &amp;ndash; the business objects for transactions, and the dimensions for analysis
* analytical data &amp;ndash; supports decision making&lt;ref&gt;"Introduction to Master Data Management", Mark Rittman, Director, Rittman Mead Consulting, 9 May 2008 https://s3.amazonaws.com/rmc_docs/Introduction%20to%20Oracle%20Master%20Data%20Management.pdf&lt;/ref&gt;&lt;ref&gt;"[http://www.b-eye-network.com/view/2918 "Defining Master Data"], David Loshin, BeyeNetwork, May 2006&lt;/ref&gt;

In [[computing]], a master data management tool can be used to support master data management by removing duplicates, standardizing data (mass maintaining), and incorporating rules to eliminate incorrect data from entering the system in order to create an authoritative source of master data. Master data are the products, accounts and parties for which the business transactions are completed. The root cause problem stems from business unit and product line segmentation, in which the same customer will be serviced by different product lines, with redundant data being entered about the customer (a.k.a. party in the role of customer) and account in order to process the transaction. The redundancy of party and account data is compounded in the front to back office life cycle, where the authoritative single source for the party, account and product data is needed but is often once again redundantly entered or augmented.

Master data management has the objective of providing processes for collecting, aggregating, matching, consolidating, quality-assuring, persisting and distributing such data throughout an organization to ensure consistency and control in the ongoing maintenance and application use of this information.

The term recalls the concept of a ''master file'' from an earlier computing era.

==Definition==
Master data management (MDM) is a comprehensive method of enabling an enterprise to link all of its critical data to one file, called a master file, that provides a common point of reference. When properly done, master data management streamlines data sharing among personnel and departments. In addition, master data management can facilitate computing in multiple system architectures, platforms and applications.&lt;ref&gt;{{cite web|title=Master data management|url=http://www.ibm.com/software/data/master-data-management/overview.html|publisher=[[IBM]]}}&lt;/ref&gt;

At its core Master Data Management (MDM) can be viewed as a "discipline for specialized quality improvement"&lt;ref&gt;DAMA-DMBOK Guide,2010 DAMA International&lt;/ref&gt; defined by the policies and procedures put in place by a data governance organization.  The ultimate goal being to provide the end user community with a "trusted single version of the truth" from which to base decisions.

==Issues==
At a basic level, master data management seeks to ensure that an organization does not use multiple (potentially [[Consistency (database systems)|inconsistent]]) versions of the same master data in different parts of its operations, which can occur in large organizations. A typical example of poor master data management is the scenario of a bank at which a [[customer]] has taken out a [[Mortgage loan|mortgage]] and the bank begins to send mortgage solicitations to that customer, ignoring the fact that the person already has a mortgage account relationship with the bank. This happens because the customer information used by the marketing section within the bank lacks integration with the customer information used by the customer services section of the bank.  Thus the two groups remain unaware that an existing customer is also considered a sales lead. The process of [[record linkage]] is used to associate different records that correspond to the same entity, in this case the same person.

Other problems include (for example) issues with the [[data quality|quality of data]], consistent [[classification]] and identification of data, and [[Data validation and reconciliation|data-reconciliation]] issues.  Master data management of disparate data systems requires [[data transformation]]s as the data extracted from the disparate source data system is transformed and loaded into the master data management hub.  To synchronize the disparate source master data, the managed master data extracted from the master data management hub is again transformed and loaded into the disparate source data system as the master data is updated.  As with other [[Extract, Transform, Load]]-based data movement, these processes are expensive and inefficient to develop and to maintain which greatly reduces the [[return on investment]] for the master data management product.

One of the most common reasons some large corporations experience massive issues with master data management is growth through [[merger]]s or [[Takeover|acquisitions]].  Any organizations which merge will typically create an entity with duplicate master data (since each likely had at least one master database of its own prior to the merger).  Ideally, [[database administrator]]s resolve this problem through [[Data deduplication|deduplication]] of the master data as part of the merger. In practice, however, reconciling several master data systems can present difficulties because of the dependencies that existing applications have on the master databases.  As a result, more often than not the two systems do not fully merge, but remain separate, with a special reconciliation process defined that ensures consistency between the data stored in the two systems.  Over time, however, as further mergers and acquisitions occur, the problem multiplies, more and more master databases appear, and data-reconciliation processes become extremely complex, and consequently unmanageable and unreliable. Because of this trend, one can find organizations with 10, 15, or even as many as 100 separate, poorly integrated master databases, which can cause serious operational problems in the areas of [[customer satisfaction]], operational efficiency, [[decision support]], and regulatory compliance.

Another problem concerns determining the proper degree of detail and normalization to include in the master data schema. For example, in a federated HR environment, the enterprise may focus on storing people data as a current status, adding a few fields to identify date of hire, date of last promotion, etc. However this simplification can introduce business impacting errors into dependent systems for planning and forecasting. The stakeholders of such systems may be forced to build a parallel network of new interfaces to track onboarding of new hires, planned retirements, and divestment, which works against one of the aims of master data management.  
==Solutions==
Processes commonly seen in master data management include source identification, data collection, [[data transformation]], [[database normalization|normalization]], rule administration, error detection and correction, data consolidation, [[data storage device|data storage]], data distribution, data classification, taxonomy services, item master creation, schema mapping, product codification, data enrichment and [[data governance]].

The selection of entities considered for master data management depends somewhat on the nature of an organization. In the common case of commercial enterprises, master data management may apply to such entities as customer ([[customer data integration]]), product ([[product information management]]), employee, and vendor. Master data management processes identify the sources from which to collect descriptions of these entities. In the course of transformation and normalization, administrators adapt descriptions to conform to standard formats and data domains, making it possible to remove duplicate instances of any entity. Such processes generally result in an organizational master data management repository, from which  all requests for a certain entity instance produce the same description, irrespective of the originating sources and the requesting destination.

The tools include [[data networks]], [[file systems]], a [[data warehouse]], [[data mart]]s, an [[operational data store]], [[data mining]], [[data analysis]], [[data visualization]], [[Federated database system|data federation]] and [[data virtualization]]. One of the newest tools, virtual master data management utilizes data virtualization and a persistent metadata server to implement a multi-level automated master data management hierarchy.

==Transmission of master data==
There are several ways in which master data may be collated and distributed to other systems.&lt;ref&gt;[http://dama-ny.com/images/meeting/101509/damanyc_mdmprint.pdf "Creating the Golden Record: Better Data Through Chemistry"], DAMA, slide 26, Donald J. Soulsby, 22 October 2009&lt;/ref&gt; This includes:

* Data consolidation – The process of capturing master data from multiple sources and integrating into a single hub ([[operational data store]]) for replication to other destination systems.
* [[Federated database system|Data federation]] – The process of providing a single virtual view of master data from one or more sources to one or more destination systems.
* Data propagation – The process of copying master data from one system to another, typically through point-to-point interfaces in legacy systems.

==See also==
* [[Reference data]]
* [[Master data]]
* [[Record linkage]]
* [[Data steward]]
* [[Data visualization]]
* [[Customer data integration]]
* [[Data integration]]
* [[Product information management]]
* [[Identity resolution]]
* [[Enterprise information integration]]
* [[Linked data]]
* [[Semantic Web]]
* [[Data governance]]
* [[Operational data store]]
* [[Single customer view]]

==References==
{{reflist}}

==External links==
* [http://msdn2.microsoft.com/en-us/library/bb190163.aspx#mdm04_topic4 Microsoft: The What, Why, and How of Master Data Management]
* [http://msdn.microsoft.com/en-us/library/bb410798.aspx Microsoft: Master Data Management (MDM) Hub Architecture]
* [http://mike2.openmethodology.org/wiki/Master_Data_Management_Solution_Offering Open Methodology for Master Data Management]
* [http://www.semarchy.com/overview/why-do-i-need-mdm/ Semarchy: Why do I Need MDM? (Video)]
* [http://www.mdmalliancegroup.com/ MDM Community]
* [http://www.stibosystems.com/Global/explore-stibo-systems/master-data-management.aspx Multidomain Master Data Management]
* [http://blogs.gartner.com/andrew_white/2014/06/05/reprise-when-is-master-data-and-mdm-not-master-data-or-mdm/ Reprise: When is Master Data and MDM Not Master Data or MDM?]
* [http://www.orchestranetworks.com/mdm/ Master Data Management (Multidomain)]

{{Data warehouse}}
{{databases}}

{{DEFAULTSORT:Master Data Management}}
[[Category:Business intelligence]]
[[Category:Data management]]
[[Category:Data warehousing]]
[[Category:Information technology management]]</text>
      <sha1>3w38tur3y0z1jqttj6nhejirkeluyxs</sha1>
    </revision>
  </page>
  <page>
    <title>Social information architecture</title>
    <ns>0</ns>
    <id>31377324</id>
    <revision>
      <id>532217079</id>
      <parentid>463731820</parentid>
      <timestamp>2013-01-09T18:15:27Z</timestamp>
      <contributor>
        <ip>69.106.238.83</ip>
      </contributor>
      <comment>/* See also */ Category:Internet -&gt; World Wide Web</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6018" xml:space="preserve">{{orphan|date=April 2011}}
'''Social information architecture''' is a sub-domain of [[information architecture]] which deals with the social aspects of conceptualizing, modeling and organizing information. Social Information Architecture, also known as Social iA &lt;ref&gt;[http://sweetinformationarchitecture.net/social-information-architecture/%20 Sweet Information Architecture]&lt;/ref&gt; has become more relevant because of the rise of [[Social Media]] and [[Web 2.0]] in recent times.

== Approach ==
There are different approaches to the explanation of Social iA. 

===A) The architecture model (internal space)===

Architects designing a physical community space, have to consider how the architecture will shape social interactions. A long hallway of offices creates an utterly different dynamic than desks with arranged in an open space. One might foster individuality, privacy, propriety; the other: collaboration, distraction, communalism.

Still, physical spaces can be flexibly repurposed and worked around if the inhabitants desire a social dynamic not instantly afforded by the space. Office doors can be left open to invite easier interaction. Partitions can be raised between adjacent desks to limit distraction and increase privacy.

That’s physical architecture. The information architectures of online communities are far more deterministic and far less flexible. They literally define the social architecture by pre-specifying in immutable computer code what information you have access to, who you can talk to, where you can go. In the online world, information architecture = social architecture.&lt;ref&gt;http://www.steinbock.org/&lt;/ref&gt;

===B) The social dialogue and information model (external space)===

All  major brands use information architecture to market their products online, it is then commonly wrapped under the umbrella phrase 'digital strategy'. Information architecture used for strategic purposes encompasses brand [[SEO]], strategic placement of virals, social media presence etc. 

Charities, news outlets and social dialogue forums can make a much more specific use of the same tools for positive and  important social purposes. Social Information Architecture is perceived as the socially conscious wing of  commercial information architecture &lt;ref&gt;http://www.sweetinformationarchitecture.net&lt;/ref&gt; and function to exchange information and ideas between people and groups. 

Social iA can pick up on conflicting issues that are treated with misunderstanding between  cultures and leaves individuals and societies vulnerable to exploitation and manipulation. Since the net has such a far reach it is obvious to use it for meaningful and coordinated social dialogue. 

Example of such issues are faith, environment,  politics, climate change, war, injustice and other social challenges. Information architecture can  help create frameworks in which sharing information brings people together, inspires and encourages them to participate in a forward thinking and unfragmented way. One of its core activities is to spread messages that bring people from opposite sites of social  and cultural spectrums together and to confront uncomfortable subject head on.

== How does social information architecture work? ==
Social iA utilizes a variety of [[Web2.0]] applications to filter relevant or valuable information and weave them in appropriate information repository or provide feedback to interesting channels. Social iA makes strategic use of Search Engines, Social Media, Google Algorithms, as well as websites, video &amp; news channels. It ‘reads’ or 'listens' to social conversations and [[search engine]] queries and engages with the net actively to gather clues about the world’s pulse on the internet. It assesses data, social &amp; political trends, and respond with targeted campaigns to give people ideas, as well as help people with making sense of information.

== Principals ==
Dan Brown in his paper 8 Principals of Social Information Architecture &lt;ref&gt;[http://socialinformationarchitecture.org.uk/paper/8principal_infoarchi.pdf Eight Principles of Information Architecture], Dan Brown. Published in the Bulletin of the American Society for Information Science and Technology – August/September 2010 – Volume 36, Number 6&lt;/ref&gt; enlists the following principals:&lt;br /&gt;
1. The principle of objects: Treat content as a living, breathing thing,
with a lifecycle, behaviors and attributes.&lt;br /&gt;
2. The principle of choices: Create pages that offer meaningful choices to users, keeping the range of choices available focused on a particular task. &lt;br /&gt;
3. The principle of disclosure: Show only enough information to help
people understand what kinds of information they’ll find as they dig
deeper. &lt;br /&gt;
4. The principle of exemplars: Describe the contents of categories by
showing examples of the contents.&lt;br /&gt;
5. The principle of front doors: Assume at least half of the website’s
visitors will come through some page other than the home page.&lt;br /&gt;
6. The principle of multiple classification: Offer users several different classification schemes to browse the site’s content.&lt;br /&gt;
7. The principle of focused navigation: Don’t mix apples and oranges
in your navigation scheme.&lt;br /&gt;
8. The principle of growth: Assume the content you have today is a
small fraction of the content you will have tomorrow.

== What can social information architecture achieve? ==
Social information architecture has many potentials in terms of fostering social connections and how information is shared in social spaces on the web.

== References==
{{Reflist}}

== See also ==
Wodtke, Christina and Govella, Austin '''Information Architecture: Blueprints for the Web''' (2009) Second Edition, Published by New Riders

{{Semantic Web}}

[[Category:Information architects|*Information architecture]]
[[Category:World Wide Web]]
[[Category:Data management]]
[[Category:Information science]]
[[Category:Information technology]]
[[Category:Digital technology]]
[[Category:New media]]</text>
      <sha1>j0n390mqdmaso1cz1312utiqia6pjii</sha1>
    </revision>
  </page>
  <page>
    <title>Data management plan</title>
    <ns>0</ns>
    <id>31808302</id>
    <revision>
      <id>683497752</id>
      <parentid>683496667</parentid>
      <timestamp>2015-09-30T17:39:05Z</timestamp>
      <contributor>
        <username>Phoebe</username>
        <id>19217</id>
      </contributor>
      <comment>/* References */  added further reading section</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11857" xml:space="preserve">A '''data management plan''' or '''DMP''' is a formal document that outlines how you will handle your [[data]] both during your research, and after the project is completed.&lt;ref&gt;http://www2.lib.virginia.edu/brown/data/plan.html&lt;/ref&gt; The goal of a data management plan is to consider the many aspects of [[data management]], [[metadata]] generation, data preservation, and analysis before the project begins; this ensures that data are well-managed in the present, and prepared for preservation in the future.

== Importance ==

Preparing a data management plan before data are collected ensures that data are in the correct format, organized well, and better annotated.&lt;ref&gt;http://libraries.mit.edu/data-management/plan/why/&lt;/ref&gt; This saves time in the long term because there is no need to re-organize, re-format, or try to remember details about data. It also increases research efficiency since both the data collector and other researchers will be able to understand and use well-annotated data in the future.  One component of a good data management plan is data archiving and preservation. By deciding on an archive ahead of time, the data collector can format data during collection to make its future submission to a database easier. If data are preserved, they are more relevant since they can be re-used by other researchers.  It also allows the data collector to direct requests for data to the database, rather than address requests individually.  Data that are preserved have the potential to lead to new, unanticipated discoveries, and they prevent duplication of scientific studies that have already been conducted. Data archiving also provides insurance against loss by the data collector.

Funding agencies are beginning to require data management plans as part of the proposal and evaluation process.&lt;ref&gt;http://www.nsf.gov/bfa/dias/policy/dmpfaqs.jsp&lt;/ref&gt;

== Major Components ==

=== Information about data &amp; data format ===

* Include a description of data to be produced by the project.&lt;ref&gt;{{Cite web|title = Elements of a Data Management Plan|url = http://www.icpsr.umich.edu/icpsrweb/content/datamanagement/dmp/elements.html|website = www.icpsr.umich.edu|accessdate = 2015-09-30}}&lt;/ref&gt; This might include (but is not limited to) data that are:
** Experimental
** Observational
** Raw or derived
** Physical collections
** Models
** Simulations
** Curriculum materials
** Software
** Images
* How will the data be acquired? When and where will they be acquired?
* After collection, how will the data be processed? Include information about
** Software used
** Algorithms
** [[workflow|Scientific workflows]]
* Describe the file formats that will be used, justify those formats, and describe the naming conventions used.
* Identify the quality assurance &amp; quality control measures that will be taken during sample collection, analysis, and processing.
* If existing data are used, what are their origins? How will the data collected be combined with existing data? What is the relationship between the data collected and existing data?
* How will the data be managed in the short-term? Consider the following:
** [[Version control]] for files
** Backing up data and data products
** Security &amp; protection of data and data products
** Who will be responsible for management

=== Metadata content and format ===

[[Metadata]] are the contextual details, including any information important for using data. This may include descriptions of temporal and spatial details, instruments, parameters, units, files, etc. Metadata is commonly referred to as “data about data”.&lt;ref&gt;Michener,WK and JW Brunt. 2000. ''Ecological Data: Design, Management and Processing''. Blackwell Science, 180p.&lt;/ref&gt; Consider the following:
* What metadata are needed? Include any details that make data meaningful.
* How will the metadata be created and/or captured? Examples include lab notebooks, GPS hand-held units, Auto-saved files on instruments, etc.
* What format will be used for the metadata? Consider the [[metadata standards]] commonly used in the scientific discipline that contains your work. There should be justification for the format chosen.

=== Policies for access, sharing, and re-use ===

* Describe any obligations that exist for sharing data collected. These may include obligations from funding agencies, institutions, other professional organizations, and legal requirements.
* Include information about how data will be shared, including when the data will be accessible, how long the data will be available, how access can be gained, and any rights that the data collector reserves for using data.
* Address any ethical or privacy issues with data sharing
* Address [[intellectual property]] &amp; [[copyright]] issues. Who owns the copyright? What are the institutional, publisher, and/or funding agency policies associated with intellectual property? Are there embargoes for political, commercial, or patent reasons?
* Describe the intended future uses/users for the data
* Indicate how the data should be cited by others. How will the issue of persistent citation be addressed? For example, if the data will be deposited in a public archive, will the dataset have a [[digital object identifier]] (doi) assigned to it?

=== Long-term storage and data management ===

* Researchers should identify an appropriate archive for long-term preservation of their data. By identifying the archive early in the project, the data can be formatted, transformed, and documented appropriately to meet the requirements of the archive. Researchers should consult colleagues and professional societies in their discipline to determine the most appropriate database, and include a backup archive in their data management plan in case their first choice goes out of existence.
* Early in the project, the primary researcher should identify what data will be preserved in an archive. Usually, preserving the data in its most raw form is desirable, although data derivatives and products can also be preserved.
* An individual should be identified as the primary contact person for archived data, and ensure that contact information is always kept up-to-date in case there are requests for data or information about data.

=== Budget ===

Data management and preservation costs may be considerable, depending on the nature of the project. By anticipating costs ahead of time, researchers ensure that the data will be properly managed and archived. Potential expenses that should be considered are
* Personnel time for data preparation, management, documentation, and preservation
* Hardware and/or software needed for data management, backing up, security, documentation, and preservation
* Costs associated with submitting the data to an archive
The data management plan should include how these costs will be paid.

== NSF Data Management Plan ==

All grant proposals submitted to [[National Science Foundation|NSF]] must include a Data Management Plan that is no more than two pages.&lt;ref&gt;http://www.nsf.gov/pubs/policydocs/pappguide/nsf11001/gpg_2.jsp#dmp&lt;/ref&gt; This is a supplement (not part of the 15 page proposal) and should describe how the proposal will conform to the Award and Administration Guide policy (see below). It may include the following:
# The types of data
# The standards to be used for data and metadata format and content
# Policies for access and sharing
# Policies and provisions for re-use
# Plans for archiving data

Policy summarized from the [[National Science Foundation|NSF]] Award and Administration Guide, Section 4 (Dissemination and Sharing of Research Results):&lt;ref&gt;http://www.nsf.gov/bfa/dias/policy/dmp.jsp&lt;/ref&gt;
# Promptly publish with appropriate authorship
# Share data, samples, physical collections, and supporting materials with others, within a reasonable time frame
# Share software and inventions
# Investigators can keep their legal rights over their intellectual property, but they still have to make their results, data, and collections available to others
# Policies will be implemented via
## Proposal review
## Award negotiations and conditions
## Support/incentives

== ESRC Data Management Plan ==

Since 1995, the UK's [[Economic and Social Research Council]] (ESRC) have had a research data policy in place. The current ESRC Research Data Policy states that research data created as a result of ESRC-funded research should be openly available to the scientific community to the maximum extent possible, through long-term preservation and high quality data management.&lt;ref&gt;[http://www.esrc.ac.uk/about-esrc/information/data-policy.aspx ESRC Research Data Policy 2010]&lt;/ref&gt;

ESRC requires a data management plan for all research award applications where new data are being created. Such plans are designed to promote a structured approach to data management throughout the data lifecycle, resulting in better quality data that is ready to archive for sharing and re-use. The [[UK Data Service]], the ESRC's flagship data service, provides practical guidance on research data management planning suitable for social science researchers in the UK and around the world.&lt;ref&gt;[http://ukdataservice.ac.uk/manage-data.aspx Prepare and manage data: Guidance from the UK Data Service]&lt;/ref&gt;&lt;ref&gt;[http://www.sagepub.com/books/Book240297 SAGE handbook: Managing and Sharing Data: A Guide to Good Practice]&lt;/ref&gt;

ESRC has a longstanding arrangement with the [[UK Data Archive]], based at the [[University of Essex]], as a place of deposit for research data, with award holders required to offer data resulting from their research grants via the UK Data Service.&lt;ref&gt;[http://www.data-archive.ac.uk/deposit/who UK Data Archive: Who can deposit data?]&lt;/ref&gt; The Archive enables data re-use by preserving data and making them available to the research and teaching communities.

== References ==
{{Reflist}}

== Further reading ==
{{Cite book|title = Delivering research data management services|last = Pryor|first = Graham|publisher = Facet Publishing|year = 2014|isbn = 9781856049337|location = |pages = }}

== External links ==
* [http://www.sagepub.com/books/Book240297?&amp;subject=B00&amp;sortBy=defaultPubDate%20desc&amp;fs=1 SAGE handbook]: Managing and Sharing Research Data: A Guide to Good Practice
* [http://dmp.cdlib.org DMPTool]: Guidance and resources for data management plans
* [http://www.cdlib.org/services/uc3/dmp/index.html California Digital Library], University of California Curation Center (UC3)
* [http://www.dataone.org/plans DataONE]
* [http://www2.lib.virginia.edu/brown/data/plan.html University of Virginia Library]
* [https://dmponline.dcc.ac.uk/ DMPonline]
* [http://www.dcc.ac.uk/resources/data-management-plans Digital Curation Centre]
* [http://www.lib.umich.edu/research-data-management-and-publishing-support/nsf-data-management-plans#directorate_guide University of Michigan Library]
* [http://www.nsf.gov/pubs/policydocs/pappguide/nsf11001/gpg_2.jsp#dmp NSF Grant Proposal Guidelines]
* [http://www.icpsr.umich.edu/icpsrweb/ICPSR/dmp/index.jsp Inter-University Consortium for Political and Social Research]
* [http://lno.lternet.edu/node/269 LTER Blog: How to write a data management plan]
* [http://www.gesis.org/en/archive-and-data-management-training-and-information-center/research-data-management/data-management-plan/ More information about data management plans at [[GESIS – Leibniz Institute for the Social Sciences]]]
* [http://ukdataservice.ac.uk/manage-data.aspx UK Data Service]: Prepare and Manage Data: Guidance and tools for social science researchers
* [http://www.consorciomadrono.es/pagoda Plan de Gestión de Datos PaGoDa]: DMP Toolkit of The Consortium of Universities of the Region of Madrid and the UNED for Library Cooperation (Madroño - Spain) 

&lt;!--- Categories ---&gt;
[[Category:Articles created via the Article Wizard]]
[[Category:Data management]]</text>
      <sha1>1dip9zeham0b8vpglky4my08onu4j7v</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Object-oriented database management systems</title>
    <ns>14</ns>
    <id>2595964</id>
    <revision>
      <id>435516766</id>
      <parentid>349220324</parentid>
      <timestamp>2011-06-21T19:54:37Z</timestamp>
      <contributor>
        <username>RickBeton</username>
        <id>24644</id>
      </contributor>
      <comment>additional related category</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="154" xml:space="preserve">Articles in this category are pure [[object-oriented database management system]]s.

[[Category:Data management]]
[[Category:Database management systems]]</text>
      <sha1>2f4arafq6jbmjrrhxc83mb8xif0t00c</sha1>
    </revision>
  </page>
  <page>
    <title>Cloud Data Management Interface</title>
    <ns>0</ns>
    <id>32115812</id>
    <revision>
      <id>677599436</id>
      <parentid>671762229</parentid>
      <timestamp>2015-08-24T09:54:52Z</timestamp>
      <contributor>
        <ip>79.35.205.170</ip>
      </contributor>
      <comment>Changed latest version (as it is written in www.snia.org/cdmi)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7077" xml:space="preserve">{{Infobox standardref
| title             = Cloud Data Management Interface
| status            = Published
| year_started      = 2009
| version           = 1.1.1
| organization      = [[Storage Networking Industry Association]]
| base_standards    = [[Hypertext Transfer Protocol]]
| related_standards = [[Network File System]]
| abbreviation      = CDMI
| domain            = [[Cloud computing]]
| license           = 
| website           = [http://www.snia.org/cloud CDMI Technical Working Group]
}}

The '''Cloud Data Management Interface''' ('''CDMI''') is a [http://www.snia.org SNIA] standard that specifies a protocol for self-provisioning, administering and accessing [[cloud storage]].&lt;ref&gt;{{cite web|title=Cloud Data Management Interface|url=http://www.snia.org/cdmi|publisher=SNIA|accessdate=26 June 2011}}&lt;/ref&gt;

CDMI defines [[REST]]ful [[HTTP]] operations for assessing the capabilities of the cloud storage system, allocating and accessing containers and objects, managing users and groups, implementing access control, attaching metadata, making arbitrary queries, using persistent queues, specifying retention intervals and holds for compliance purposes, using a logging facility, billing, moving data between cloud systems, and exporting data via other protocols such as [[iSCSI]] and [[Network File System (protocol)|NFS]]. Transport security is obtained via [[Transport Layer Security|TLS]].

==Capabilities==
Compliant implementations must provide access to a set of configuration parameters known as ''capabilities''.
These are either boolean values that represent whether or not a system supports things such as queues, export via other protocols, path-based storage and so on, or numeric values expressing system limits, such as how much metadata may be placed on an object.  As a minimal compliant implementation can be quite small, with few features, clients need to check the cloud storage system for a capability before attempting to use the functionality it represents.

==Containers==
A CDMI client may access objects, including containers, by either name or object id (OID), assuming the CDMI server supports both methods.  When storing objects by name, it is natural to use nested named containers; the resulting structure corresponds exactly to a traditional filesystem directory structure.

==Objects==
Objects are similar to files in a traditional file system, but are enhanced with an increased amount of and capacity for [[metadata]].  As with containers, they may be accessed by either name or OID.  When accessed by name, clients use [[Uniform Resource Locator|URLs]] that contain the full pathname of objects to [[create, read, update and delete]] them. When accessed by OID, the URL specifies an OID string in the '''cdmi-objectid''' container; this container presents a flat name space conformant with standard object storage system semantics.

Subject to system limits, objects may be of any size or type and have arbitrary user-supplied metadata attached to them. Systems that support query allow arbitrary queries to be run against the metadata.

==Domains, Users and Groups==
CDMI supports the concept of a ''domain'', similar in concept to a domain in the [[Windows]] [[Active Directory]] model. Users and groups created in a domain share a common administrative database and are known to each other on a "first name" basis, i.e. without reference to any other domain or system.

Domains also function as containers for usage and billing summary data.

==Access Control==
CDMI exactly follows the [[Access Control List|ACL]] and [[Access Control Entry|ACE]] model used for file authorization operations by [[NFSv4#NFSv4|NFSv4]]. This makes it also compatible with [[Microsoft Windows]] systems.

==Metadata==
CDMI draws much of its metadata model from the [[XAM]] specification. Objects and containers have "storage system metadata", "data system metadata" and arbitrary user specified metadata, in addition to the metadata maintained by an ordinary filesystem (atime etc.).

==Queries==
CDMI specifies a way for systems to support arbitrary queries against CDMI containers, with a rich set of comparison operators, including support for [[regular expression]]s.

==Queues==
CDMI supports the concept of persistent [[FIFO (computing and electronics)|FIFO]] (first-in, first-out) queues. These are useful for job scheduling,  order processing and other tasks in which lists of things must be processed in order.

==Compliance==
Both retention intervals and retention holds are supported by CDMI.  A retention interval consists of a start time and a retention period.  During this time interval, objects are preserved as immutable and may not be deleted. A retention hold is usually placed on an object because of judicial action and has the same effect: objects may not be changed nor deleted until all holds placed on them are removed.

==Logging==
CDMI clients can sign up for logging of system, security and object access events on servers that support it.  This feature allows clients to see events locally as the server logs them.

==Billing==
Summary information suitable for billing clients for on-demand services can be obtained by authorized users from systems that support it.

==Serialization==
Serialization of objects and containers allows export of all data and metadata on a system and importation of that data into another cloud system.

==Foreign Protocols==
CDMI supports export of containers as NFS or CIFS shares.  Clients that mount these shares see the container hierarchy as an ordinary filesystem directory hierarchy, and the objects in the containers as normal files. Metadata outside of ordinary filesystem metadata may or may not be exposed.

Provisioning of iSCSI LUNs is also supported.

== Client SDKs ==
* [http://www.snia.org/forums/csi/programs/CDMIportal CDMI Reference Implementation]
* [https://github.com/scality/Droplet Droplet]
* [https://github.com/livenson/libcdmi-java libcdmi-java]
* [https://github.com/livenson/libcdmi-python libcdmi-python]
* [https://github.com/projectpvg1/.net-SDK .NET SDK]

== See also ==
[[Comparison of CDMI server implementations]]

== References ==
{{reflist}}

== External links ==
* [http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=40874 ISO-8601]  International Organization for Standardization, "Data elements and interchange formats -- Information interchange -- Representation of dates and times”, ISO 8601:20044
* [http://www.itu.int/ITU-T/publications/recs.html ITU-T509]  International Telecommunications Union Telecommunication Standardization Sector (ITU-T), Recommendation X.509: Information technology - Open Systems Interconnection - The Directory: Public-key and attribute certificate frameworks, May 2000. Specification and technical corrigenda -
* [http://www.unix.org/version3/ieee_std.html POSIX ERE] The Open Group, Base Specifications Issue 6, IEEE Std 1003.1, 2004 Edition
* [http://www.cloudplugfest.org/ Cloud Interoperability Plugfest project]

[[Category:Cloud storage]]
[[Category:Data management]]</text>
      <sha1>go51qrlype33klzw8jsvbobig4qfpfi</sha1>
    </revision>
  </page>
  <page>
    <title>Meta-data management</title>
    <ns>0</ns>
    <id>3198327</id>
    <revision>
      <id>751386758</id>
      <parentid>681726753</parentid>
      <timestamp>2016-11-25T09:29:54Z</timestamp>
      <contributor>
        <username>Jane023</username>
        <id>1213535</id>
      </contributor>
      <comment>/* Wikipedia metadata */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4754" xml:space="preserve">[[File:Linnaeus - Regnum Animale (1735).png|thumb|[[Linnaean taxonomy]], a metadata system used historically for grouping animals in zoos, first published in 1735]]
[[File:6123034166 card catalog.jpg|thumb|[[Card catalog]] and digital media access point]]
'''Meta-data management''' (also known as [[metadata]] management, without the hyphen) involves managing [[data]] about ''other data'', whereby this "other data" is generally referred to as ''content'' data. The term is used most often in relation to [[Digital media]], but older forms of metadata are catalogs, dictionaries, and taxonomies. For example, the [[Dewey Decimal Classification]] is a metadata management system for books developed in 1876 for libraries.

==Metadata schema==
Metadata management can be defined as the end-to-end process and governance framework for creating, controlling, enhancing, attributing, defining and managing a metadata schema, model or other structured aggregation system, either independently or within a repository and the associated supporting processes (often to enable the management of content). For web-based systems, [[Uniform Resource Locator|URL]]s, images, video etc. may be referenced from a triples table of object, attribute and value.
==Scope==
With specific [[knowledge domain]]s, the boundaries of the metadata for each must be managed, since a general [[ontology]] is not useful to experts in one field whose language is knowledge-domain specific.
==Metadata manager==
If one is in the process of making a knowledge management solution, creating a metadata schema and developing a system in which metadata is managed are very important. In such a project, a dedicated metadata manager may be appointed in order to maintain adherence to metadata and information management standards. {{Citation needed|date=January 2011}} This is a person who will be responsible for the metadata strategy, and possibly, the implementation. A metadata manager does not need to know about and be involved with everything concerning the solution, but it does help to have an understanding of as much of the process as possible to make sure a relevant schema is developed.
==Metadata management over time==

Managing the metadata in a knowledge management solution is an important step in a metadata strategy. It is part of the strategy to make sure that the metadata are complete, current and correct at any given time. Managing a metadata project is also about making sure that users of the system are aware of the possibilities allowed by a well-designed metadata system and how to maximize the benefits of metadata. Regularly monitoring the metadata to ensure that the schema remains relevant is advised.

===Wikipedia metadata===
Wikipedia is a project that actively manages metadata for its articles and files. For example, volunteer editors carefully curate new biographical articles based on the notability (''claim to fame''), name, birth, and/or death dates.&lt;ref&gt;See the internal Wikipedia project on the English Wikipedia called [[Wikipedia:WikiProject Biography]]&lt;/ref&gt; Similarly, volunteer editors carefully curate new architectural articles based on name, municipality, or [[geo coordinates]].&lt;ref&gt;See [[Wikipedia:WikiProject Architecture]]&lt;/ref&gt; When new articles with a valid alternate spelling are added to Wikipedia that match up to existing articles based on metadata, these are then manually checked and if needed, tagged for merging.&lt;ref&gt;See [[Wikipedia:WikiProject Merge]]&lt;/ref&gt; When new articles are added that are considered out of scope or otherwise unfit for Wikipedia, these are nominated for deletion.&lt;ref&gt;See [[Wikipedia:Articles for deletion]]&lt;/ref&gt; To help keep track of metadata on Wikipedia, the new Wikimedia project [[Wikidata]] was established in 2012. Click on the pictures to view more metadata about these images:
&lt;gallery&gt;
File:Sta-eulalia.jpg|This picture of the [[Barcelona Cathedral]] was uploaded to the English Wikipedia in 2003 to illustrate its Wikipedia article, and was transferred to [[Wikimedia Commons]] in 2007 so it could be used in other language versions of Wikipedia.
File:Article catedral pantalla estreta.png|This screenprint of the [[Catalan Wikipedia]] page on the cathedral features several photos including this one. The screenprint was uploaded to Wikimedia Commons in 2007 soon after the photo was available there, but [[:ca:Catedral de Barcelona|that article]] on the Catalan Wikipedia has since been expanded.
&lt;/gallery&gt;

== See also ==
* [[Data Defined Storage]] 
* [[Metadata discovery]]
* [[Metadata publishing]]
* [[Metadata registry]]
* [[ISO/IEC 11179]]
* [[Dublin core]]

 
==References==
{{reflist}}
{{DEFAULTSORT:Meta-Data Management}}
[[Category:Metadata]]
[[Category:Data management]]</text>
      <sha1>arwzqjj1zibrqba3j0m5xw786h4jdm2</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Data partitioning</title>
    <ns>14</ns>
    <id>19073984</id>
    <revision>
      <id>464076617</id>
      <parentid>419220774</parentid>
      <timestamp>2011-12-04T18:43:21Z</timestamp>
      <contributor>
        <username>Starcheerspeaksnewslostwars</username>
        <id>11554556</id>
      </contributor>
      <comment>removed [[Category:Data managementPartitioning]]; added [[Category:Data management]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="28" xml:space="preserve">[[Category:Data management]]</text>
      <sha1>tpzfnyay41jj855s8j16rxxvd2yg34n</sha1>
    </revision>
  </page>
  <page>
    <title>DMAIC</title>
    <ns>0</ns>
    <id>3733991</id>
    <revision>
      <id>744294342</id>
      <parentid>741433737</parentid>
      <timestamp>2016-10-14T10:04:20Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* Additional Steps */http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5929" xml:space="preserve">{{sources|date=April 2012}}
'''DMAIC''' (an acronym for ''Define, Measure, Analyze, Improve and Control'') (pronounced ''də-MAY-ick'') refers to a data-driven improvement cycle used for improving, optimizing and stabilizing business processes and designs. The DMAIC improvement cycle is the core tool used to drive [[Six Sigma]] projects. However, DMAIC is not exclusive to Six Sigma and can be used as the framework for other improvement applications.

==Steps==
DMAIC is an abbreviation of the five improvement steps it comprises: Define, Measure, Analyze, Improve and Control. All of the DMAIC process steps are required and always proceed in the given order.
[[File:DMAICWebdingsII.png|thumbnail|right|400px|The five steps of DMAIC]]

===Define===
The purpose of this step is to clearly articulate the business problem, goal, potential resources, project scope and high-level project timeline.  This information is typically captured within project charter document.  Write down what you currently know. Seek to clarify facts, set objectives and form the project team. Define the following:

* A problem 
* The customer(s)
* [[Voice of the customer]] (VOC) and  [[Critical to Quality]] (CTQs) — what are the critical process outputs?

===Measure===
The purpose of this step is to objectively establish current baselines as the basis for improvement.  This is a data collection step, the purpose of which is to establish process performance baselines.  The performance metric baseline(s) from the Measure phase will be compared to the performance metric at the conclusion of the project to determine objectively whether significant improvement has been made.  The team decides on what should be measured and how to measure it. It is usual for teams to invest a lot of effort into assessing the suitability of the proposed measurement systems. Good data is at the heart of the DMAIC process:

===Analyze===
The purpose of this step is to identify, validate and select root cause for elimination.  A large number of potential root causes (process inputs, X) of the project problem are identified via root cause analysis (for example a [[Ishikawa diagram|fishbone diagram]]).  The top 3-4 potential root causes are selected using multi-voting or other consensus tool for further validation.  A data collection plan is created and data are collected to establish the relative contribution of each root causes to the project metric, Y.  This process is repeated until "valid" root causes can be identified.  Within Six Sigma, often complex analysis tools are used. However, it is acceptable to use basic tools if these are appropriate.  Of the "validated" root causes, all or some can be

* List and prioritize potential causes of the problem
* Prioritize the root causes (key process inputs) to pursue in the Improve step
* Identify how the process inputs (Xs) affect the process outputs (Ys).  Data are analyzed to understand the magnitude of contribution of each root cause, X, to the project metric, Y.  Statistical tests using p-values accompanied by Histograms, Pareto charts, and line plots are often used to do this.
* Detailed process maps can be created to help pin-point where in the process the root causes reside, and what might be contributing to the occurrence.

===Improve===
The purpose of this step is to identify, test and implement a solution to the problem; in part or in whole. This depends on the situation. Identify creative solutions to eliminate the key root causes in order to fix and prevent process problems. Use brainstorming or techniques like [[Six Thinking Hats]] and [[Random stimulus|Random Word]]. Some projects can utilize complex analysis tools like DOE ([[Design of Experiments]]), but try to focus on obvious solutions if these are apparent. However, the purpose of this step can also be to find solutions without implementing them.

* Create
* Focus on the simplest and easiest solutions
* Test solutions using [[PDCA|Plan-Do-Check-Act]] (PDCA) cycle
* Based on PDCA results, attempt to anticipate any avoidable risks associated with the "improvement" using [[Failure mode and effects analysis|FMEA]]
* Create a detailed implementation plan
* Deploy improvements

===Control===
The purpose of this step is to sustain the gains.  Monitor the improvements to ensure continued and sustainable success. Create a control plan. Update documents, business process and training records as required.

A [[Control chart]] can be useful during the Control stage to assess the stability of the improvements over time by serving as 1. a guide to continue monitoring the process and 2. provide a response plan for each of the measures being monitored in case the process becomes unstable.

===Replicate and thank the teams===
This is additional to the standard DMAIC steps but it should be considered. Think about replicating the changes in other processes. Share your new knowledge within and outside of your organization.  It is very important to always provide positive morale support to team members in an effort to maximize the effectiveness of DMAIC.

Replicating the improvements, sharing your success and thanking your team members helps build buy-in for future DMAIC or improvement initiatives.

===Additional Steps===
Some organizations add a '''''R'''ecognize'' step at the beginning, which is to recognize the right problem to work on, thus yielding an RDMAIC methodology.&lt;ref name="WebberWallace2006p43"&gt;{{cite book | first1=Larry | last1=Webber | first2=Michael | last2=Wallace | title=Quality Control for Dummies | url=https://books.google.com/books?id=9BWkxto2fcEC&amp;pg=PA43 | accessdate=2012-05-16 | date=15 December 2006 | publisher=For Dummies | isbn=978-0-470-06909-7 | pages=42–43 }}&lt;/ref&gt;

==See also==
*[[Design for Six Sigma|DFSS]]
*[[Industrial engineering]]
*[[Kaizen]]
*[[PDCA]]
*[[Six Sigma]]

==References==
{{Reflist}}

[[Category:Data management]]
[[Category:Six Sigma]]</text>
      <sha1>4kru109yyc7n246jf8gczh735ctrvwy</sha1>
    </revision>
  </page>
  <page>
    <title>Database server</title>
    <ns>0</ns>
    <id>815760</id>
    <revision>
      <id>760952346</id>
      <parentid>760949520</parentid>
      <timestamp>2017-01-20T00:42:16Z</timestamp>
      <contributor>
        <username>AnomieBOT</username>
        <id>7611264</id>
      </contributor>
      <minor />
      <comment>Dating maintenance tags: {{Cn}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4086" xml:space="preserve">{{refimprove|date=September 2014}}
A '''database server''' is a [[computer program]] that provides [[database]] services to other computer programs or to [[computer]]s, as defined by the [[client–server]] [[software modeling|model]].{{cn|date=January 2017}} The term may also refer to a computer dedicated to running such a program. [[Database management system]]s frequently provide database-server functionality, and some [[database management system]]s (DBMSs) (such as [[MySQL]]) rely exclusively on the client–server model for database access.

Users access a database server either through a "[[Front and back ends|front end]]" running on the user's computer - which displays requested data - or through the "[[Front and back ends|back end]]", which runs on the server and handles tasks such as data analysis and storage.

In a [[Master-slave (technology)|master-slave]] model, database master servers are central and primary locations of data while database slave servers are synchronized backups of the master acting as [[proxy server|proxies]].

Most database servers respond to a [[query language]]. Each database understands its query language and converts each submitted [[query (disambiguation) | query]] to server-readable form and executes it to retrieve results.

Examples of proprietary database servers include [[Oracle Database|Oracle]], [[IBM DB2|DB2]], [[Informix]], and [[Microsoft SQL Server]]. Examples of [[GNU General Public Licence]] database servers include [[Ingres (database)|Ingres]] and [[MySQL]].  Every server uses its own query logic and structure. The [[SQL]] (Structured Query Language) query language is more or less the same on all [[relational database]] servers.

[[DB-Engines]] lists over 200 DBMSs in its ranking.&lt;ref&gt;
{{cite web
|url= http://db-engines.com/en/ranking 
|title= DB-Engines Ranking 
|publisher= DB-Engines.com 
|date= 2013-12-01 
|accessdate= 2013-12-28
}}
&lt;/ref&gt;

==History ==
The foundations for modeling large sets of data were first introduced by [[Charles Bachman]] in 1969.&lt;ref name="dbhist"&gt;[http://knol.google.com/k/databases-history-early-development# Databases - History &amp; Early Development]&lt;/ref&gt; Bachman introduced [[Data structure diagram|Data Structure Diagrams (DSDs)]] as a means to graphically represent data. DSDs provided a means to represent the relationships between different data entities. In 1970, [[Edgar F. Codd|Codd]] introduced the concept that users of a database should be ignorant of the "inner workings" of the database.&lt;ref name="dbhist"/&gt; Codd proposed the "relational view" of data which later evolved into the [[Relational Model]] which most databases use today. In 1971, the Database Task Report Group of [[CODASYL]] (the driving force behind the development of the programming language [[COBOL]]) first proposed a "data description language for describing a database, a data description language for describing that part of the data base known to a program, and a data manipulation language." &lt;ref name="dbhist"/&gt; Most of the research and development of databases focused on the relational model during the 1970s.

In 1975 Bachman demonstrated how the relational model and the data structure set were similar and "congruent" ways of structuring data while working for the [[Honeywell]].&lt;ref name="dbhist"/&gt; The [[Entity-relationship model]] was first proposed in its current form by [[Peter Chen]] in 1976 while he was conducting research at [[MIT]].&lt;ref&gt;[http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.123.1085 The Entity-Relationship Model: Toward a Unified View of Data (1976)]&lt;/ref&gt; This model became the most frequently used model to describe relational databases. Chen was able to propose a model that was superior to the navigational model and was more applicable to the "real world" than the relational model proposed by Codd.&lt;ref name="dbhist"/&gt;

== References ==
{{Reflist}}

==See also==
* [[Replication (computer science)#Database replication|Database replication]]

{{Database}}

[[Category:Data management]]
[[Category:Servers (computing)]]
[[Category:Databases]]</text>
      <sha1>cf6mtdep12vqq18ocaal52q9q9t5khq</sha1>
    </revision>
  </page>
  <page>
    <title>Data warehouse</title>
    <ns>0</ns>
    <id>7990</id>
    <revision>
      <id>759338969</id>
      <parentid>755857297</parentid>
      <timestamp>2017-01-10T16:14:47Z</timestamp>
      <contributor>
        <username>Paul2520</username>
        <id>19295592</id>
      </contributor>
      <comment>filled in references</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="29470" xml:space="preserve">{{multiple issues|
{{Refimprove|date=February 2008}}
{{Citation style|date=September 2013}}
}}

[[File:Data warehouse overview.JPG|thumb|200px|Data Warehouse Overview]]

In [[computing]], a '''data warehouse''' ('''DW''' or '''DWH'''), also known as an '''enterprise data warehouse''' ('''EDW'''), is a system used for [[Business reporting|reporting]] and [[data analysis]], and is considered a core component of [[business intelligence]].&lt;ref&gt;Dedić, N. and Stanier C., 2016., "An Evaluation of the Challenges of Multilingualism in Data Warehouse Development" in 18th International Conference on Enterprise Information Systems - ICEIS 2016, p. 196.&lt;/ref&gt; DWs are central repositories of integrated data from one or more disparate sources. They store current and historical data and are used for creating analytical reports for knowledge workers throughout the enterprise. Examples of reports could range from annual and quarterly comparisons and trends to detailed daily sales analysis.

The data stored in the warehouse is [[upload]]ed from the [[operational system]]s (such as marketing or sales). The data may pass through an [[operational data store]] for additional operations before it is used in the DW for reporting.

==Types of systems==
;[[Data mart]]:  A data mart is a simple form of a data warehouse that is focused on a single subject (or functional area), hence they draw data from a limited number of sources such as sales, finance or marketing. Data marts are often built and controlled by a single department within an organization. The sources could be internal operational systems, a central data warehouse, or external data.&lt;ref&gt;{{cite web |url=http://docs.oracle.com/html/E10312_01/dm_concepts.htm |title=Data Mart Concepts |publisher=Oracle |year=2007}}&lt;/ref&gt; Denormalization is the norm for data modeling techniques in this system. Given that data marts generally cover only a subset of the data contained in a data warehouse, they are often easier and faster to implement.

{| class="wikitable"
|+ Difference between data warehouse and {{nowrap|data mart}}
|-
! Data warehouse
! Data mart
|-
| enterprise-wide data
| department-wide data
|-
| multiple subject areas
| single subject area
|-
| difficult to build
| easy to build
|-
| takes more time to build
| less time to build
|-
| larger memory
| limited memory
|}

'''Types of data marts'''
* Dependent data mart
* Independent data mart
* Hybrid data mart
;[[Online analytical processing]] (OLAP): OLAP is characterized by a relatively low volume of transactions. Queries are often very complex and involve aggregations. For OLAP systems, response time is an effectiveness measure. OLAP applications are widely used by [[Data Mining]] techniques. OLAP databases store aggregated, historical data in multi-dimensional schemas (usually star schemas). OLAP systems typically have data latency of a few hours, as opposed to data marts, where latency is expected to be closer to one day. The OLAP approach is used to analyze multidimensional data from multiple sources and perspectives. The three basic operations in OLAP are : Roll-up (Consolidation), Drill-down and Slicing &amp; Dicing.&lt;ref name=dwh&gt;{{cite web |url=https://intellipaat.com/tutorial/data-warehouse-tutorial/ |title=Data Warehousing Tutorial For Beginners |publisher=Intellipaat}}&lt;/ref&gt;

;[[Online transaction processing]] (OLTP): OLTP is characterized by a large number of short on-line transactions (INSERT, UPDATE, DELETE). OLTP systems emphasize very fast query processing and maintaining [[data integrity]] in multi-access environments. For OLTP systems, effectiveness is measured by the number of transactions per second. OLTP databases contain detailed and current data. The schema used to store transactional databases is the entity model (usually [[Third normal form|3NF]]).&lt;ref&gt;{{cite web |url=http://datawarehouse4u.info/OLTP-vs-OLAP.html |title=OLTP vs. OLAP |year=2009 |website=Datawarehouse4u.Info |quote=We can divide IT systems into transactional (OLTP) and analytical (OLAP). In general we can assume that OLTP systems provide source data to data warehouses, whereas OLAP systems help to analyze it.}}&lt;/ref&gt; Normalization is the norm for data modeling techniques in this system.

;Predictive analysis: Predictive analysis is about [[pattern recognition|finding]] and quantifying hidden patterns in the data using complex mathematical models that can be used to [[prediction|predict]] future outcomes.  Predictive analysis is different from OLAP in that OLAP focuses on historical data analysis and is reactive in nature, while predictive analysis focuses on the future. These systems are also used for CRM ([[customer relationship management]]).

==Software tools==
The typical extract-transform-load ([[Extract, transform, load|ETL]])-based data warehouse uses [[Staging (data)|staging]], [[data integration]], and access layers to house its key functions. The staging layer or staging database stores raw data extracted from each of the disparate source data systems. The integration layer integrates the disparate data sets by transforming the data from the staging layer often storing this transformed data in an [[operational data store]] (ODS) database. The integrated data are then moved to yet another database, often called the data warehouse database, where the data is arranged into hierarchical groups often called dimensions and into facts and aggregate facts. The combination of facts and dimensions is sometimes called a [[star schema]].  The access layer helps users retrieve data.&lt;ref name=IJCA96Patil&gt;{{cite journal |url=http://www.ijcaonline.org/proceedings/icwet/number9/2131-db195 |author1=Patil, Preeti S. |author2=Srikantha Rao |author3=Suryakant B. Patil |title=Optimization of Data Warehousing System: Simplification in Reporting and Analysis |work=IJCA Proceedings on International Conference and workshop on Emerging Trends in Technology (ICWET) |year=2011 |volume=9 |issue=6 |pages=33–37 |publisher=Foundation of Computer Science}}&lt;/ref&gt;

This definition of the data warehouse focuses on data storage. The main source of the data is cleaned, transformed, catalogued and made available for use by managers and other business professionals for [[data mining]], [[OLAP|online analytical processing]], [[market research]] and [[decision support]].&lt;ref&gt;Marakas &amp; O'Brien 2009&lt;/ref&gt; However, the means to retrieve and analyze data, to [[Extract, transform, load|extract, transform, and load]] data, and to manage the [[data dictionary]] are also considered essential components of a data warehousing system. Many references to data warehousing use this broader context. Thus, an expanded definition for data warehousing includes [[business intelligence tools]], tools to extract, transform, and load data into the repository, and tools to manage and retrieve [[metadata]].

==Benefits==
A data warehouse maintains a copy of information from the source transaction systems. This architectural complexity provides the opportunity to:
* Integrate data from multiple sources into a single database and data model. Mere congregation of data to single database so a single query engine can be used to present data is an ODS.
* Mitigate the problem of database isolation level lock contention in transaction processing systems caused by attempts to run large, long running, analysis queries in transaction processing databases.
* Maintain [[Provenance#Data provenance|data history]], even if the source transaction systems do not.
* Integrate data from multiple source systems, enabling a central view across the enterprise. This benefit is always valuable, but particularly so when the organization has grown by merger.
* Improve [[data quality]], by providing consistent codes and descriptions, flagging or even fixing bad data.
* Present the organization's information consistently.
* Provide a single common data model for all data of interest regardless of the data's source.
* Restructure the data so that it makes sense to the business users.
* Restructure the data so that it delivers excellent query performance, even for complex analytic queries, without impacting the [[operational system]]s.
* Add value to operational business applications, notably [[customer relationship management]] (CRM) systems.
*Make decision–support queries easier to write.
*Optimized data warehouse architectures allow data scientists to organize and disambiguate repetitive data.&lt;ref&gt;{{Cite web|url=https://www.idera.com/resourcecentral/whitepapers/modern-data-architecture|title=Modern Data Architecture {{!}} IDERA|last=|first=|date=|website=www.idera.com|publisher=|access-date=2016-09-18}}&lt;/ref&gt;

==Generic environment==

The environment for data warehouses and marts includes the following:

* Source systems that provide data to the warehouse or mart;
* Data integration technology and processes that are needed to prepare the data for use;
* Different architectures for storing data in an organization's data warehouse or data marts;
*Different tools and applications for the variety of users;
*Metadata, data quality, and governance processes must be in place to ensure that the warehouse or mart meets its purposes.

In regards to source systems listed above, Rainer{{clarify|reason=Who is Rainer?|date=December 2014}} states, "A common source for the data in data warehouses is the company's operational databases, which can be relational databases".&lt;ref name=rainer2012&gt;{{cite book|last=Rainer|first=R. Kelly|title=Introduction to Information Systems: Enabling and Transforming Business, 4th Edition (Kindle Edition)|date=2012-05-01|publisher=Wiley|pages=127, 128, 130, 131, 133}}&lt;/ref&gt;

Regarding data integration, Rainer states, "It is necessary to extract data from source systems, transform them, and load them into a data mart or warehouse".&lt;ref name=rainer2012/&gt;

Rainer discusses storing data in an organization’s data warehouse or data marts.&lt;ref name=rainer2012 /&gt;

Metadata are data about data. "IT personnel need information about data sources; database, table, and column names; refresh schedules; and data usage measures".&lt;ref name=rainer2012 /&gt;

Today, the most successful companies are those that can respond quickly and flexibly to market changes and opportunities. A key to this response is the effective and efficient use of data and information by analysts and managers.&lt;ref name=rainer2012 /&gt; A "data warehouse" is a repository of historical data that are organized by subject to support decision makers in the organization.&lt;ref name=rainer2012 /&gt; Once data are stored in a data mart or warehouse, they can be accessed.

==History==
The concept of data warehousing dates back to the late 1980s&lt;ref&gt;{{cite web|url=http://www.computerworld.com/databasetopics/data/story/0,10801,70102,00.html |title=The Story So Far |date=2002-04-15 |accessdate=2008-09-21 |deadurl=yes |archiveurl=https://web.archive.org/web/20080708182105/http://www.computerworld.com/databasetopics/data/story/0,10801,70102,00.html |archivedate=2008-07-08 |df= }}&lt;/ref&gt; when IBM researchers Barry Devlin and Paul Murphy developed the "business data warehouse". In essence, the data warehousing concept was intended to provide an architectural model for the flow of data from operational systems to [[decision support system|decision support environments]]. The concept attempted to address the various problems associated with this flow, mainly the high costs associated with it. In the absence of a data warehousing architecture, an enormous amount of redundancy was required to support multiple decision support environments. In larger corporations it was typical for multiple decision support environments to operate independently. Though each environment served different users, they often required much of the same stored data. The process of gathering, cleaning and integrating data from various sources, usually from long-term existing operational systems (usually referred to as [[legacy system]]s), was typically in part replicated for each environment. Moreover, the operational systems were frequently reexamined as new decision support requirements emerged. Often new requirements necessitated gathering, cleaning and integrating new data from "[[data mart]]s" that were tailored for ready access by users.

Key developments in early years of data warehousing were:
* 1960s – [[General Mills]] and [[Dartmouth College]], in a joint research project, develop the terms ''dimensions'' and ''facts''.&lt;ref name="kimball16"&gt;Kimball 2002, pg. 16&lt;/ref&gt;
* 1970s – [[ACNielsen]] and IRI provide dimensional data marts for retail sales.&lt;ref name="kimball16" /&gt;
* 1970s – [[Bill Inmon]] begins to define and discuss the term: Data Warehouse.{{citation needed|date=June 2014}}
* 1975 – [[Sperry Univac]] introduces [[MAPPER]] (MAintain, Prepare, and Produce Executive Reports) is a database management and reporting system that includes the world's first [[Fourth-generation programming language|4GL]]. First platform designed for building Information Centers (a forerunner of contemporary Enterprise [[Data Warehousing]] platforms)
* 1983 – [[Teradata]] introduces a database management system specifically designed for decision support.
* 1984 – [[Metaphor Computer Systems]], founded by [[David Liddle]] and Don Massaro, releases Data Interpretation System (DIS). DIS was a hardware/software package and GUI for business users to create a database management and analytic system.
* 1988 – Barry Devlin and Paul Murphy publish the article ''An architecture for a business and information system'' where they introduce the term "business data warehouse".&lt;ref&gt;{{cite journal|url=http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=5387658|title=An architecture for a business and information system|journal=IBM Systems Journal | doi=10.1147/sj.271.0060|volume=27|pages=60–80}}&lt;/ref&gt;
* 1990 – Red Brick Systems, founded by [[Ralph Kimball]], introduces Red Brick Warehouse, a database management system specifically for data warehousing.
* 1991 – Prism Solutions, founded by [[Bill Inmon]], introduces Prism Warehouse Manager, software for developing a data warehouse.
* 1992 – [[Bill Inmon]] publishes the book ''Building the Data Warehouse''.&lt;ref&gt;{{cite book|last=Inmon|first=Bill|title=Building the Data Warehouse|year=1992|publisher=Wiley|isbn=0-471-56960-7}}&lt;/ref&gt;
* 1995 – The Data Warehousing Institute, a for-profit organization that promotes data warehousing, is founded.
* 1996 – [[Ralph Kimball]] publishes the book ''The Data Warehouse Toolkit''.&lt;ref name=":0"&gt;{{cite book|title=The Data Warehouse Toolkit|last=Kimball|first=Ralph|publisher=Wiley|year=2011|isbn=9780470149775|page=237}}&lt;/ref&gt;
* 2012 – [[Bill Inmon]] developed and made public technology known as "textual disambiguation". Textual disambiguation applies context to raw text and reformats the raw text and context into a standard data base format. Once raw text is passed through textual disambiguation, it can easily and efficiently be accessed and analyzed by standard business intelligence technology. Textual disambiguation is accomplished through the execution of textual ETL. Textual disambiguation is useful wherever raw text is found, such as in documents, Hadoop, email, and so forth.

==Information storage==

===Facts===
A fact is a value or measurement, which represents a fact about the managed entity or system.

Facts as reported by the reporting entity are said to be at raw level. E.g. in a mobile telephone system, if a BTS (base transceiver station) received 1,000 requests for traffic channel allocation, it allocates for 820 and rejects the remaining then it would report 3 '''facts''' or measurements to a management system:
* tch_req_total = 1000
* tch_req_success = 820
* tch_req_fail = 180

Facts at the raw level are further aggregated to higher levels in various [[Dimension (data warehouse)|dimensions]] to extract more service or business-relevant information from it. These are called aggregates or summaries or aggregated facts.

For instance, if there are 3 BTSs in a city, then the facts above can be aggregated from the BTS to the city level in the network dimension. For example:

* &lt;math&gt;tch\_req\_success\_city = tch\_req\_success\_bts1 + tch\_req\_success\_bts2 + tch\_req\_success\_bts3&lt;/math&gt;
* &lt;math&gt;avg\_tch\_req\_success\_city = (tch\_req\_success\_bts1 + tch\_req\_success\_bts2 + tch\_req\_success\_bts3) / 3&lt;/math&gt;

===Dimensional versus normalized approach for storage of data===
There are three or more leading approaches to storing data in a data warehouse&amp;nbsp;— the most important approaches are the dimensional approach and the normalized approach.

The dimensional approach refers to [[Ralph Kimball]]’s approach in which it is stated that the data warehouse should be modeled using a Dimensional Model/[[star schema]]. The normalized approach, also called the [[Third normal form|3NF]] model (Third Normal Form) refers to Bill Inmon's approach in which it is stated that the data warehouse should be modeled using an E-R model/normalized model.

In a [[Star schema|dimensional approach]], [[transaction data]] are partitioned into "facts", which are generally numeric transaction data, and "[[dimension (data warehouse)|dimensions]]", which are the reference information that gives context to the facts. For example, a sales transaction can be broken up into facts such as the number of products ordered and the total price paid for the products, and into dimensions such as order date, customer name, product number, order ship-to and bill-to locations, and salesperson responsible for receiving the order.

A key advantage of a dimensional approach is that the data warehouse is easier for the user to understand and to use. Also, the retrieval of data from the data warehouse tends to operate very quickly.&lt;ref name=":0" /&gt; Dimensional structures are easy to understand for business users, because the structure is divided into measurements/facts and context/dimensions. Facts are related to the organization’s business processes and operational system whereas the dimensions surrounding them contain context about the measurement (Kimball, Ralph 2008). Another advantage offered by dimensional model is that it does not involve a relational database every time. Thus, this type of modeling technique is very useful for end-user queries in data warehouse.&lt;ref name=dwh /&gt;

The main disadvantages of the dimensional approach are the following:
# In order to maintain the integrity of facts and dimensions, loading the data warehouse with data from different operational systems is complicated.
# It is difficult to modify the data warehouse structure if the organization adopting the dimensional approach changes the way in which it does business.

In the normalized approach, the data in the data warehouse are stored following, to a degree, [[database normalization]] rules. Tables are grouped together by ''subject areas'' that reflect general data categories (e.g., data on customers, products, finance, etc.). The normalized structure divides data into entities, which creates several tables in a relational database. When applied in large enterprises the result is dozens of tables that are linked together by a web of joins. Furthermore, each of the created entities is converted into separate physical tables when the database is implemented (Kimball, Ralph 2008){{Citation needed|date=November 2013}}.
The main advantage of this approach is that it is straightforward to add information into the database. Some disadvantages of this approach are that, because of the number of tables involved, it can be difficult for users to join data from different sources into meaningful information and to access the information without a precise understanding of the sources of data and of the [[data structure]] of the data warehouse.

Both normalized and dimensional models can be represented in entity-relationship diagrams as both contain joined relational tables. The difference between the two models is the degree of normalization (also known as [[Database normalization#Normal forms|Normal Forms]]). These approaches are not mutually exclusive, and there are other approaches. Dimensional approaches can involve normalizing data to a degree (Kimball, Ralph 2008).

In ''Information-Driven Business'',&lt;ref&gt;{{cite book|last=Hillard|first=Robert|title=Information-Driven Business|year=2010|publisher=Wiley|isbn=978-0-470-62577-4}}&lt;/ref&gt; Robert Hillard proposes an approach to comparing the two approaches based on the information needs of the business problem. The technique shows that normalized models hold far more information than their dimensional equivalents (even when the same fields are used in both models) but this extra information comes at the cost of usability. The technique measures information quantity in terms of [[Entropy (information theory)|information entropy]] and usability in terms of the Small Worlds data transformation measure.&lt;ref&gt;{{cite web|url=http://mike2.openmethodology.org/wiki/Small_Worlds_Data_Transformation_Measure |title=Information Theory &amp; Business Intelligence Strategy - Small Worlds Data Transformation Measure - MIKE2.0, the open source methodology for Information Development |publisher=Mike2.openmethodology.org |date= |accessdate=2013-06-14}}&lt;/ref&gt;

==Design methods==
{{refimprove section|date=July 2015}}

===Bottom-up design===
In the ''bottom-up'' approach, [[data mart]]s are first created to provide reporting and analytical capabilities for specific [[business process]]es. These data marts can then be integrated to create a comprehensive data warehouse. The data warehouse bus architecture is primarily an implementation of "the bus", a collection of [[Dimension (data warehouse)#Types|conformed dimension]]s and [[Facts (data warehouse)#Types|conformed fact]]s, which are dimensions that are shared (in a specific way) between facts in two or more data marts.&lt;ref&gt;{{Cite web|url=http://decisionworks.com/2003/09/the-bottom-up-misnomer/|title=The Bottom-Up Misnomer - DecisionWorks Consulting|website=DecisionWorks Consulting|language=en-US|access-date=2016-03-06}}&lt;/ref&gt;

===Top-down design===
The ''top-down'' approach is designed using a normalized enterprise [[data model]]. [[Data element|"Atomic" data]], that is, data at the greatest level of detail, are stored in the data warehouse. Dimensional data marts containing data needed for specific business processes or specific departments are created from the data warehouse.&lt;ref name="ReferenceA"&gt;Gartner, Of Data Warehouses, Operational Data Stores, Data Marts and Data Outhouses, Dec 2005&lt;/ref&gt;

===Hybrid design===
Data warehouses (DW) often resemble the [[hub and spokes architecture]]. [[Legacy system]]s feeding the warehouse often include [[customer relationship management]] and [[enterprise resource planning]], generating large amounts of data. To consolidate these various data models, and facilitate the [[extract transform load]] process, data warehouses often make use of an [[operational data store]], the information from which is parsed into the actual DW. To reduce data redundancy, larger systems often store the data in a normalized way. Data marts for specific reports can then be built on top of the DW.

The DW database in a hybrid solution is kept on [[third normal form]] to eliminate [[data redundancy]]. A normal relational database, however, is not efficient for business intelligence reports where dimensional modelling is prevalent. Small data marts can shop for data from the consolidated warehouse and use the filtered, specific data for the fact tables and dimensions required. The DW provides a single source of information from which the data marts can read, providing a wide range of business information. The hybrid architecture allows a DW to be replaced with a [[master data management]] solution where operational, not static information could reside.

The [[Data Vault Modeling]] components follow hub and spokes architecture. This modeling style is a hybrid design, consisting of the best practices from both third normal form and [[star schema]]. The Data Vault model is not a true third normal form, and breaks some of its rules, but it is a top-down architecture with a bottom up design. The Data Vault model is geared to be strictly a data warehouse. It is not geared to be end-user accessible, which when built, still requires the use of a data mart or star schema based release area for business purposes.

==Versus operational system==
Operational systems are optimized for preservation of [[data integrity]] and speed of recording of business transactions through use of [[database normalization]] and an [[entity-relationship model]]. Operational system designers generally follow the [[Codd's 12 rules|Codd rules]] of [[database normalization]] in order to ensure data integrity. Codd defined five increasingly stringent rules of normalization. Fully normalized database designs (that is, those satisfying all five Codd rules) often result in information from a business transaction being stored in dozens to hundreds of tables. [[Relational database]]s are efficient at managing the relationships between these tables. The databases have very fast insert/update performance because only a small amount of data in those tables is affected each time a transaction is processed. Finally, in order to improve performance, older data are usually periodically purged from operational systems.

Data warehouses are optimized for analytic access patterns.  Analytic access patterns generally involve selecting specific fields and rarely if ever 'select *' as is more common in operational databases.  Because of these differences in access patterns, operational databases (loosely, OLTP) benefit from the use of a row-oriented DBMS whereas analytics databases (loosely, OLAP) benefit from the use of a [[column-oriented DBMS]].  Unlike operational systems which maintain a snapshot of the business, data warehouses generally maintain an infinite history which is implemented through ETL processes that periodically migrate data from the operational systems over to the data warehouse.

==Evolution in organization use==
These terms refer to the level of sophistication of a data warehouse:

; Offline operational data warehouse: Data warehouses in this stage of evolution are updated on a regular time cycle (usually daily, weekly or monthly) from the operational systems and the data is stored in an integrated reporting-oriented data
; Offline data warehouse: Data warehouses at this stage are updated from data in the operational systems on a regular basis and the data warehouse data are stored in a data structure designed to facilitate reporting.
; On time data warehouse: Online Integrated Data Warehousing represent the real time Data warehouses stage data in the warehouse is updated for every transaction performed on the source data
; Integrated data warehouse: These data warehouses assemble data from different areas of business, so users can look up the information they need across other systems.&lt;ref&gt;{{cite web |url=http://www.tech-faq.com/data-warehouse.html |title=Data Warehouse }}&lt;/ref&gt;

==See also==
{{colbegin|3}}
*[[Accounting intelligence]]
*[[Anchor modeling]]
*[[Business intelligence]]
*[[Business intelligence tools]]
*[[Data integration]]
*[[Data mart]]
*[[Data mining]]
*[[Data presentation architecture]]
*[[Data scraping]]
*[[Data warehouse appliance]]
*[[Database management system]]
*[[Decision support system]]
*[[Data vault modeling]]
*[[Executive information system]]
*[[Extract, transform, load]]
*[[Master data management]]
*[[Online analytical processing]]
*[[Online transaction processing]]
*[[Operational data store]]
*[[Semantic warehousing]]
*[[Snowflake schema]]
*[[Software as a service]]
*[[Star schema]]
*[[Slowly changing dimension]]
*[[Data warehouse automation]]
{{colend}}

==References==
{{Reflist|30em|&lt;!--refs=
&lt;ref name=ahsan&gt;
{{Cite journal
|last1=Abdullah
|first1=Ahsan
|title=Analysis of mealybug incidence on the cotton crop using ADSS-OLAP (Online Analytical Processing) tool, Volume 69, Issue 1
|journal= Computers and Electronics in Agriculture
|year=2009
|pages=59–72
|doi=10.1016/j.compag.2009.07.003
|volume=69
}}
&lt;/ref&gt;--&gt;
}}

==Further reading==
* Davenport, Thomas H. and Harris, Jeanne G. ''Competing on Analytics: The New Science of Winning'' (2007) Harvard Business School Press. ISBN 978-1-4221-0332-6
* Ganczarski, Joe. ''Data Warehouse Implementations: Critical Implementation Factors Study'' (2009) [[VDM Verlag]] ISBN 3-639-18589-7 ISBN 978-3-639-18589-8
* Kimball, Ralph and Ross, Margy. ''The Data Warehouse Toolkit'' Third Edition (2013) Wiley, ISBN 978-1-118-53080-1
* Linstedt, Graziano, Hultgren. ''The Business of Data Vault Modeling'' Second Edition (2010) Dan linstedt, ISBN 978-1-4357-1914-9
* William Inmon. ''Building the Data Warehouse'' (2005) John Wiley and Sons, ISBN 978-8-1265-0645-3

==External links==
* [http://www.kimballgroup.com/html/articles.html Ralph Kimball articles]
* [http://www.ijcaonline.org/archives/number3/77-172 International Journal of Computer Applications]
* [http://dwreview.com/DW_Overview.html Data Warehouse Introduction]

{{data}}
{{Data warehouse}}

{{Authority control}}

{{DEFAULTSORT:Data Warehouse}}
[[Category:Business intelligence]]
[[Category:Data management]]
[[Category:Data warehousing| ]]
[[Category:Information technology management]]</text>
      <sha1>ojlbhcfc2n3zvyh79lllk6fwpoiqxu2</sha1>
    </revision>
  </page>
  <page>
    <title>Disaster recovery plan</title>
    <ns>0</ns>
    <id>6309764</id>
    <revision>
      <id>754651990</id>
      <parentid>754605324</parentid>
      <timestamp>2016-12-13T19:56:11Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 5 sources and tagging 0 as dead. #IABot (v1.2.7.1)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="34601" xml:space="preserve">{{merge to|Business continuity planning|date=June 2015}}
A '''disaster recovery plan''' (DRP) is a documented process or set of procedures to recover and protect a business [[Information_technology|IT]] infrastructure in the event of a [[disaster]].&lt;ref name="5 tips"&gt;{{cite web |url=http://www.smallbusinesscomputing.com/News/ITManagement/5-tips-to-build-an-effective-disaster-recovery-plan.html |title=5 Tips to Build an Effective Disaster Recovery Plan |publisher=Small Business Computing |date=14 June 2012 |accessdate=9 August 2012 |first1=Bill |last1=Abram}}&lt;/ref&gt;  Such a plan, ordinarily documented in written form, specifies  procedures an organization is to follow in the event of a disaster. It is "a comprehensive statement of consistent actions to be taken before, during and after a disaster."&lt;ref name="DR journal"&gt;{{cite web |url=http://www.drj.com/new2dr/w2_002.htm |title=Disaster Recovery Planning Process |first1=Geoffrey H. |last1=Wold |work=Disaster Recovery Journal |series=Adapted from Volume 5 #1 | publisher=Disaster Recovery World |year=1997 |accessdate=8 August 2012}}&lt;/ref&gt; The disaster could be [[Natural disaster|natural]], [[Environmental disaster|environmental]] or [[Anthropogenic hazard|man-made]]. Man-made disasters could be intentional (for example, an act of a terrorist) or unintentional (that is, accidental, such as the breakage of a man-made dam).

Given organizations' increasing dependency on [[information technology]] to run their operations, a disaster recovery plan, sometimes erroneously called a [[Continuity of Operations]] Plan (COOP), is increasingly associated with the recovery of information technology data, assets, and facilities.

==Objectives==

Organizations cannot always avoid disasters, but with careful planning the effects of a disaster can be minimized. The objective of a disaster recovery plan is to minimize downtime and data loss.&lt;ref&gt;[http://www.comp-soln.com/DRP_whitepaper.pdf ''An Overview of the Disaster Recovery Planning Process - From Start to Finish.''] Comprehensive Consulting Solutions Inc.( "Disaster Recovey Planning, An Overview: White Paper." )March 1999. Retrieved 8 August 2012.&lt;/ref&gt; The primary objective is to protect the organization in the event that all or part of its operations and/or computer services are rendered unusable. The plan minimizes the disruption of operations and ensures that some level of organizational stability and an orderly recovery after a disaster will prevail.&lt;ref name="DR journal" /&gt;  Minimizing downtime and data loss is measured in terms of two concepts: the [[recovery time objective]] (RTO) and the [[recovery point objective]] (RPO).
 
The recovery time objective is the time within which a business process must be restored, after a [[disaster|major incident]] (MI) has occurred, in order to avoid unacceptable consequences associated with a break in [[business continuity]]. The recovery point objective (RPO) is the age of files that must be recovered from backup storage for normal operations to resume if a computer, system, or network goes down as a result of a MI. The RPO is expressed backwards in time (that is, into the past) starting from the instant at which the MI occurs, and can be specified in seconds, minutes, hours, or days.&lt;ref&gt;[http://whatis.techtarget.com/definition/recovery-point-objective-RPO ''Definition: Recovery point objective (RPO).''] Retrieved 10 August 2012.&lt;/ref&gt; The recovery point objective (RPO) is thus the maximum acceptable amount of data loss measured in time. It is the age of the files or data in backup storage required to resume normal operations after the MI.&lt;ref&gt;{{cite web |url=http://www.techopedia.com/definition/1032/recovery-point-objective-rpo |title=Recovery Point Objective (RPO): Definition - What does Recovery Point Objective (RPO) mean? |work=Techopedia |publisher=Janalta Interactive Inc. |year=2012 |accessdate=10 August 2012 }}&lt;/ref&gt;

[[File:Schematic ITSC and RTO, RPO, MI.jpg|frame|left|A DR plan illustrating the chronology of the '''{{color|#bd00e0|RPO}}''' and the '''{{color|#ff7f7c|RTO}}''' with respect to the '''{{color|#fe0000|MI}}'''.]]
{{clear}}

==Relationship to the Business Continuity Plan==

According to the SANS institute, the [[Business continuity planning|Business Continuity Plan]] (BCP) is a comprehensive organizational plan that includes the disaster recovery plan. The Institute further states that a Business Continuity Plan (BCP) consists of the five component plans:&lt;ref name="The Disaster Recovery Plan."&gt;[http://www.sans.org/reading_room/whitepapers/recovery/disaster-recovery-plan_1164 ''The Disaster Recovery Plan.''] Chad Bahan. GSEC Practical Assignment version 1.4b. SANS Institute InfoSec Reading Room. June 2003. Retrieved 24 August 2012.&lt;/ref&gt;

* Business Resumption Plan
* Occupant Emergency Plan
* Continuity of Operations Plan
* Incident Management Plan
* Disaster Recovery Plan

The Institute states that the first three plans (Business Resumption, Occupant Emergency, and Continuity of Operations Plans) do not deal with the IT infrastructure. They further state that the Incident Management Plan (IMP) does deal with the IT infrastructure, but since it establishes structure and procedures to address cyber attacks against an organization’s IT systems, it generally does not represent an agent for activating the Disaster Recovery Plan, leaving The Disaster Recovery Plan as the only BCP component of interest to IT.&lt;ref name="The Disaster Recovery Plan."/&gt;

[[Disaster Recovery Institute]] International states that disaster recovery is the area of business continuity that deals with ''technology'' recovery as opposed to the recovery of business operations.&lt;ref&gt;https://www.drii.org/glossary.php&lt;/ref&gt;

==Benefits==

Like every insurance plan, there are benefits that can be obtained from the drafting of a disaster recovery plan. Some of these benefits are:&lt;ref name="DR journal" /&gt;

* Providing a sense of security
* Minimizing risk of delays
* Guaranteeing the reliability of standby systems
* Providing a standard for testing the plan
* Minimizing decision-making during a disaster
* Reducing potential legal liabilities
* Lowering unnecessarily stressful work environment

==Types of plans==

There is no one right type of disaster recovery plan,&lt;ref name=MSU&gt;{{cite web |url=http://www.drp.msu.edu/documentation/stepbystepguide.htm |publisher=Michigan State University |title=Disaster Recovery Planning - Step by Step Guide |accessdate=9 May 2014 }}&lt;/ref&gt; nor is there a one-size-fits-all disaster recovery plan.&lt;ref name="5 tips" /&gt;&lt;ref name=MSU /&gt; However, there are three basic strategies that feature in all disaster recovery plans: (1) preventive measures, (2) detective measures, and (3) corrective measures.&lt;ref&gt;{{cite web |url=http://emailarchivingandremotebackup.com/backup-disaster-recovery.html |title=Backup Disaster Recovery |publisher=Email Archiving and Remote Backup |year=2010 |accessdate=9 May 2014}}&lt;/ref&gt; Preventive measures will try to prevent a disaster from occurring. These measures seek to identify and reduce risks. They are designed to mitigate or prevent an event from happening. These measures may include keeping data backed up and off site, using surge protectors, installing generators and conducting routine inspections. Detective measures are taken to discover the presence of any unwanted events within the IT infrastructure.  Their aim is to uncover new potential threats. They may detect or uncover unwanted events. These measures include installing fire alarms, using up-to-date antivirus software, holding employee training sessions, and installing server and [[network monitoring]] software.  Corrective measures are aimed to restore a system after a disaster or otherwise unwanted event takes place. These measures focus on fixing or restoring the systems after a disaster. Corrective measures may include keeping critical documents in the Disaster Recovery Plan or securing proper [[insurance policy|insurance policies]], after a "lessons learned" brainstorming session.&lt;ref name="5 tips" /&gt;&lt;ref&gt;{{cite web|url=http://www.stonecrossingsolutions.com/technical-solutions/disaster-recovery/ |title=Disaster Recovery &amp; Business Continuity Plans |publisher=Stone Crossing Solutions |date=2012 |accessdate=9 August 2012 |deadurl=yes |archiveurl=https://web.archive.org/web/20120823045007/http://www.stonecrossingsolutions.com/technical-solutions/disaster-recovery/ |archivedate=23 August 2012 |df= }}&lt;/ref&gt;

A disaster recovery plan must answer at least three basic questions: (1) what is its objective and purpose, (2) who will be the people or teams who will be responsible in case any disruptions happen, and (3) what will these people do (the procedures to be followed) when the disaster strikes.&lt;ref&gt;{{cite web|url=http://www.continuitycompliance.org/disaster-recovery-planning-on-virtual-and-cloud-platforms-survey-results-now-available/ |title=Disaster Recovery – Benefits of Getting Disaster Planning Software and Template and Contracting with Companies Offering Data Disaster Recovery Plans, Solutions and Services: Why Would You Need a Disaster Recovery Plan? |publisher=Continuity Compliance |date=7 June 2011 |accessdate=14 August 2012 |archivedate=9 May 2014 |archiveurl=http://www.webcitation.org/6PQaoed5G?url=http://www.continuitycompliance.org/disaster-recovery-planning-on-virtual-and-cloud-platforms-survey-results-now-available/ |deadurl=yes |df= }}&lt;/ref&gt;

==Types of disasters==
[[Image:SH-60B helicopter flies over Sendai.jpg|thumb|right|200px|The tsunami that affected Japan in 2011, a type of natural disaster]]
[[Image:UA Flight 175 hits WTC south tower 9-11 edit.jpeg|thumb|right|200px|September 11, 2001, in New York City, a type of man-made disaster: it caused pollution, loss of lives, property damage, and considerable [[data loss]]]]

Disasters can be [[Natural disaster|natural]] or [[Anthropogenic hazard|man-made]]. Man-made disasters could be intentional (for example, sabotage or an act of [[terrorism]]) or unintentional (that is, accidental, such as the breakage of a man-made dam).  Disasters may encompass more than weather. They may involve Internet threats or take on other man-made manifestations such as theft.&lt;ref name="5 tips" /&gt;

===Natural disaster===
{{Main article|Natural disaster}}
A natural disaster is a major adverse event resulting from the earth's natural hazards. Examples of natural disasters are [[flood]]s, [[tsunami]]s, [[tornado]]es, [[hurricane|hurricanes/cyclones]], [[volcanic eruption]]s, [[earthquake]]s, [[heat wave]]s, and [[landslide]]s.  Other types of disasters include the more [[End time|cosmic]] scenario of an [[Impact event|asteroid hitting the Earth]].

===Man-made disasters===
{{Main article|Man-made disasters}}
Man-made disasters are the consequence of technological or human hazards. Examples include [[stampede]]s, [[fire|urban fires]], [[industrial accident]]s, [[oil spill]]s, [[nuclear explosion]]s/[[nuclear radiation]] and acts of [[war]].  Other types of man-made disasters include the more cosmic scenarios of catastrophic [[global warming]], [[nuclear war]], and [[bioterrorism]].

The following table categorizes some disasters and notes first response initiatives. Note that whereas the sources of a disaster may be natural (for example, heavy rains) or man-made (for example, a broken dam), the results may be similar (flooding).&lt;ref&gt;[http://www.nten.org/sites/nten/files/Sample%20Disaster%20Recovery%20Plan.doc ''Business Continuity Planning (BCP): Sample Plan For Nonprofit Organizations.''] {{wayback|url=http://www.nten.org/sites/nten/files/Sample%20Disaster%20Recovery%20Plan.doc |date=20100602065521 }} Pages 11-12. Retrieved 8 August 2012.&lt;/ref&gt;

{| class="wikitable"
! rowspan="16" | Natural
! colspan="3"  | Disaster
|- bgcolor="#CCCCCC"
!Example|| Profile || First Response
|-
|[[Avalanche]]||The sudden, drastic flow of snow down a slope, occurring when either natural triggers, such as loading from new snow or rain, or artificial triggers, such as explosives or backcountry skiers, overload the snowpack||Shut off utilities; Evacuate building if necessary; Determine impact on the equipment and facilities and any disruption
|-
|[[Blizzard]]||A severe snowstorm characterized by very strong winds and low temperatures||Power off all equipment; listen to blizzard advisories; Evacuate area, if unsafe; Assess damage
|-
|[[Earthquake]]||The shaking of the earth’s crust, caused by underground volcanic forces of breaking and shifting rock beneath the earth’s surface||Shut off utilities; Evacuate building if necessary; Determine impact on the equipment and facilities and any disruption
|-
|[[Fire|Fire (wild)]]||Fires that originate in uninhabited areas and which pose the risk to spread to inhabited areas||Attempt to suppress fire in early stages; Evacuate personnel on alarm, as necessary; Notify fire department; Shut off utilities; Monitor weather advisories
|-
|[[Flood]]||Flash flooding: Small creeks, gullies, dry streambeds, ravines, culverts or even low-lying areas flood quickly||Monitor flood advisories; Determine flood potential to facilities; Pre-stage emergency power generating equipment; Assess damage
|-
|[[Freezing Rain]]||Rain occurring when outside surface temperature is below freezing||Monitor weather advisories; Notify employees of business closure; home; Arrange for snow and ice removal
|-
|[[Heat wave]]||A prolonged period of excessively hot weather relative to the usual weather pattern of an area and relative to normal temperatures for the season||Listen to weather advisories; Power-off all servers after a graceful shutdown if there is imminent potential of power failure; Shut down main electric circuit usually located in the basement or the first floor
|-
|[[Hurricane]]||Heavy rains and high winds||Power off all equipment; listen to hurricane advisories; Evacuate area, if flooding is possible; Check gas, water and electrical lines for damage; Do not use telephones, in the event of severe lightning; Assess damage
|-
|[[Landslide]]||Geological phenomenon which includes a range of ground movement, such as rock falls, deep failure of slopes and shallow debris flows||Shut off utilities; Evacuate building if necessary; Determine impact on the equipment and facilities and any disruption
|-
|[[Lightning strike]]||An electrical discharge caused by lightning, typically during thunderstorms||Power off all equipment; listen to hurricane advisories; Evacuate area, if flooding is possible; Check gas, water and electrical lines for damage; Do not use telephones, in the event of severe lightning; Assess damage
|-
|[[Limnic eruption]]||The sudden eruption of carbon dioxide from deep lake water||Shut off utilities; Evacuate building if necessary; Determine impact on the equipment and facilities and any disruption
|-
|[[Tornado]]||Violent rotating columns of air which descent from severe thunderstorm cloud systems||Monitor tornado advisories; Power off equipment; Shut off utilities (power and gas); Assess damage once storm passes
|-
|[[Tsunami]]||A series of water waves caused by the displacement of a large volume of a body of water, typically an ocean or a large lake, usually caused by earthquakes, volcanic eruptions, underwater explosions, landslides, glacier calvings, meteorite impacts and other disturbances above or below water||Power off all equipment; listen to tsunami advisories; Evacuate area, if flooding is possible; Check gas, water and electrical lines for damage; Assess damage
|-
|[[Volcanic eruption]]||The release of hot magma, volcanic ash and/or gases from a volcano||Shut off utilities; Evacuate building if necessary; Determine impact on the equipment and facilities and any disruption
|-
! rowspan="6" | Man-made
|[[Bioterrorism]]||The intentional release or dissemination of biological agents as a means of coercion||Get information immediately from your [[Public Health]] officials via the news media as to the right course of action; If you think you have been exposed, quickly remove your clothing and wash off your skin; Also put on a [[HEPA]] to help prevent inhalation of the agent&lt;ref&gt;[http://answers.webmd.com/answers/1176206/what-should-i-do-if-there ''What should I do if there has been a bioterrorism attack?.''] Edmond A. Hooker. WebMD. 9 October 2007. Retrieved 18 September 2012.&lt;/ref&gt; 
|-
|[[Civil unrest]]||A disturbance caused by a group of people that may include [[sit-in]]s and other forms of obstructions, riots, sabotage and other forms of crime, and which is intended to be a demonstration to the public and the government, but can escalate into general chaos||Contact local police or law enforcement&lt;ref&gt;[http://www.usfa.fema.gov/downloads/pdf/publications/fa-142.pdf ''Report of the Joint Fire/Police Task Force on Civil Unrest (FA-142): Recommendations for Organization and Operations During Civil Disturbance.''] Page 55. FEMA. Retrieved 21 October 2012.&lt;/ref&gt;&lt;ref&gt;[http://www.xanaboo.com/BCP%20-%20Developing%20a%20Strategy%20to%20Minimize%20Risk%20and%20Maintain%20Operations.pdf ''Business Continuity Planning: Developing a Strategy to Minimize Risk and Maintain Operations.''] {{wayback|url=http://www.xanaboo.com/BCP%20-%20Developing%20a%20Strategy%20to%20Minimize%20Risk%20and%20Maintain%20Operations.pdf |date=20140327234742 }} Adam Booher. Retrieved 19 September 2012.&lt;/ref&gt; 
|-
|[[Fire|Fire (urban)]]||Even with strict building fire codes, people still perish needlessly in fires||Attempt to suppress fire in early stages; Evacuate personnel on alarm, as necessary; Notify fire department; Shut off utilities; Monitor weather advisories
|-
|[[Hazardous material|Hazardous material spills]]||The escape of solids, liquids, or gases that can harm people, other living organisms, property or the environment, from their intended controlled environment such as a container.||Leave the area and call the local fire department for help.&lt;ref&gt;[http://www.tnema.org/public/hazmat.html ''Hazardous Materials.''] {{wayback|url=http://www.tnema.org/public/hazmat.html |date=20121011150052 }} Tennessee Emergency Management Office. Retrieved 7 September 2012.&lt;/ref&gt; If anyone was affected by the spill, call the your local Emergency Medical Services line&lt;ref&gt;[http://www.atsdr.cdc.gov/MHMI/index.asp ''Managing Hazardous Materials Incidents (MHMIs).''] Center for Disease Control. Retrieved 7 September 2012.&lt;/ref&gt;
|-
||[[Nuclear and radiation accidents|Nuclear and Radiation Accidents]]||An event involving significant release of radioactivity to the environment or a reactor core meltdown and which leads to major undesirable consequences to people, the environment, or the facility||Recognize that a CBRN incident has or may occur. Gather, assess and disseminate all available information to first responders. Establish an overview of the affected area. Provide and obtain regular updates to and from first responders.&lt;ref&gt;[http://www.nato.int/docu/cep/cep-cbrn-response-e.pdf ''Guidelines for First Response to a CBRN Incident.''] Project on Minimum Standards and Non-Binding Guidelines for First Responders Regarding Planning, Training, Procedure and Equipment for Chemical, Biological, Radiological and Nuclear (CBRN) Incidents.] NATO. Emergency Management. Retrieved 21 October 2012.&lt;/ref&gt;
|-
|[[Power Failure]]||Caused by summer or winter storms, lightning or construction equipment digging in the wrong location||Wait 5–10 minutes; Power-off all Servers after a graceful shutdown; Do not use telephones, in the event of severe lightning; Shut down main electric circuit usually located in the basement or the first floor
|-
|}

In the realm of information technology per se, disasters may also be the result of a computer security exploit. Some of these are: [[computer virus]]es, [[cyberattack]]s, [[denial-of-service attack]]s, [[hacker (computer security)|hacking]], and [[malware]] exploits. These are ordinarily attended to by [[information security]] experts.

==Planning methodology==

According to Geoffrey H. Wold of the Disaster Recovery Journal, the entire process involved in developing a Disaster Recovery Plan consists of 10 steps:&lt;ref name="DR journal" /&gt;

===Obtaining top management commitment===
For a disaster recovery plan to be successful, the central responsibility for the plan must reside on [[Management#Top-level managers|top management]]. Management is responsible for coordinating the disaster recovery plan and ensuring its effectiveness within the organization. It is also responsible for allocating adequate time and resources required in the development of an effective plan. Resources that management must allocate include both financial considerations and the effort of all personnel involved.

===Establishing a planning committee===
A [[plan]]ning [[committee]] is appointed to oversee the development and implementation of the plan. The planning committee includes representatives from all functional areas of the organization. Key committee members customarily include the operations manager and the data processing manager. The committee also defines the scope of the plan.

===Performing a risk assessment===
The planning committee prepares a [[Probabilistic risk assessment|risk analysis]] and a [[business impact analysis]] (BIA) that includes a range of possible disasters, including natural, technical and human threats. Each functional area of the organization is analyzed to determine the potential consequence and impact associated with several disaster scenarios. The risk assessment process also evaluates the safety of critical documents and vital records. Traditionally, fire has posed the greatest threat to an organization. Intentional human destruction, however, should also be considered. A thorough plan provides for the “worst case” situation: destruction of the main building. It is important to assess the impacts and consequences resulting from loss of information and services. The planning committee also analyzes the costs related to minimizing the potential exposures.

===Establishing priorities for processing and operations===
At this point, the critical needs of each department within the organization are evaluated in order to prioritize them. Establishing [[Wiktionary:priority|priorities]] is important because no organization possesses infinite resources and criteria must be set as to where to allocate resources first. Some of the areas often reviewed during the prioritization process are functional operations, key personnel and their functions, information flow, processing systems used, services provided, existing documentation, historical records, and the department's policies and procedures.

Processing and operations are analyzed to determine the maximum amount of time that the department and organization can operate without each critical system. This will later get mapped into the [[Recovery Time Objective]]. A critical system is defined as that which is part of a system or procedure necessary to continue operations should a department, computer center, main facility or a combination of these be destroyed or become inaccessible. A method used to determine the critical needs of a department is to document all the functions performed by each department. Once the primary functions have been identified, the operations and processes are then ranked in order of priority: essential, important and non-essential.

===Determining recovery strategies===
During this phase, the most practical alternatives for processing in case of a disaster are researched and evaluated. All aspects of the organization are considered, including [[Building|physical facilities]], [[computer hardware]] and [[software]], [[communications link]]s, [[data file]]s and [[database]]s, [[customer service]]s provided, user operations, the overall [[management information system]]s (MIS) structure, [[end-user]] systems, and any other processing operations.

Alternatives, dependent upon the evaluation of the computer function, may include: [[hot site]]s, [[warm site]]s, [[cold site]]s, [[reciprocal agreement (disaster preparedness)|reciprocal agreements]], the provision of more than one data center, the installation and deployment of multiple computer system, duplication of service center, [[consortium]] arrangements, lease of equipment, and any combinations of the above.

Written [[Contract|agreements]] for the specific recovery alternatives selected are prepared, specifying contract duration, termination conditions, [[system testing]], [[cost]], any special security procedures, procedure for the notification of system changes, hours of operation, the specific hardware and other equipment required for processing, personnel requirements, definition of the circumstances constituting an [[emergency]], process to negotiate service extensions, guarantee of [[Computer compatibility|compatibility]], [[availability]], non-mainframe resource requirements, priorities, and other contractual issues.

===Collecting data===
In this phase, data collection takes place. Among the recommended data gathering materials and documentation often included are
various lists (employee backup position listing, critical telephone numbers list, master call list, master vendor list, notification checklist), inventories (communications equipment, documentation, office equipment, forms, [[insurance policy|insurance policies]], workgroup and data center computer hardware, [[microcomputer]] hardware and software, [[office supplies|office supply]], off-site storage location equipment, telephones, etc.), distribution register, software and data files backup/retention schedules, temporary location specifications, any other such other lists, materials, inventories and documentation. Pre-formatted forms are often used to facilitate the data gathering process.

===Organizing and documenting a written plan===
Next, an outline of the plan’s contents is prepared to guide the development of the detailed procedures. Top management reviews and approves the proposed plan. The outline can ultimately be used for the [[table of contents]] after final revision. Other four benefits of this approach are that (1) it helps to organize the detailed procedures, (2) identifies all major steps before the actual writing process begins, (3) identifies redundant procedures that only need to be written once, and (4) provides a [[plan|road map]] for developing the procedures.

It is often considered [[best practice]] to develop a standard format for the disaster recovery plan so as to facilitate the writing of detailed procedures and the documentation of other information to be included in the plan later. This helps ensure that the disaster plan follows a consistent format and allows for its ongoing future maintenance. [[Standardization]] is also important if more than one person is involved in writing the procedures.

It is during this phase that the actual written plan is developed in its entirety, including all detailed procedures to be used before, during, and after a disaster. The procedures include methods for maintaining and updating the plan to reflect any significant internal, external or systems changes. The procedures allow for a regular review of the plan by key personnel within the organization. The disaster recovery plan is structured using a team approach. Specific responsibilities are assigned to the appropriate team for each functional area of the organization. Teams responsible for administrative functions, [[building|facilities]], [[logistics]], user support, [[backup|computer backup]], restoration and other important areas in the organization are identified.

The structure of the contingency organization may not be the same as the existing organization chart. The contingency organization is usually structured with teams responsible for major functional areas such as administrative functions, facilities, logistics, user support, computer backup, restoration, and any other important area.

The [[management team]] is especially important because it coordinates the recovery process. The team assesses the disaster, activates the recovery plan, and contacts team managers. The management team also oversees, documents and monitors the recovery process. It is helpful when management team members are the final decision-makers in setting priorities, policies and procedures. Each team has specific responsibilities that are completed to ensure successful execution of the plan. The teams have an assigned manager and an alternate in case the team manager is not available. Other team members may also have specific assignments where possible.

===Developing testing criteria and procedures===
Best practices dictate that DR plans be thoroughly tested and evaluated on a regular basis (at least annually). Thorough DR plans include documentation with the procedures for testing the plan. The tests will provide the organization with the assurance that all necessary steps are included in the plan. Other reasons for testing include:
* Determining the feasibility and compatibility of backup facilities and procedures.
* Identifying areas in the plan that need modification.
* Providing training to the team managers and team members.
* Demonstrating the ability of the organization to recover.
* Providing motivation for maintaining and updating the disaster recovery plan.

===Testing the plan===
After testing procedures have been completed, an initial "[[Dry run (testing)|dry run]]" of the plan is performed by conducting a structured walk-through test. The test will provide additional information regarding any further steps that may need to be included, changes in procedures that are not effective, and other appropriate adjustments. These may not become evident unless an actual dry-run test is performed. The plan is subsequently updated to correct any problems identified during the test. Initially, testing of the plan is done in sections and after normal business hours to minimize disruptions to the overall operations of the organization. As the plan is further polished, future tests occur during normal business hours.

Types of tests include: checklist tests, simulation tests, parallel tests, and full interruption tests.

===Obtaining plan approval===
Once the disaster recovery plan has been written and tested, the plan is then submitted to management for approval. It is top management’s ultimate responsibility that the organization has a documented and tested plan. Management is responsible for (1) establishing the policies, procedures and responsibilities for comprehensive [[contingency plan]]ning, and (2) reviewing and approving the contingency plan annually, documenting such reviews in writing.

Organizations that receive information processing from [[service bureau]]s will, in addition, also need to (1) evaluate the adequacy of contingency plans for its service bureau, and (2)ensure that its contingency plan is compatible with its service bureau’s plan.

==Caveats/controversies==

Due to its high cost, disaster recovery plans are not without critics. [[Cormac Foster]] has identified five "common mistakes" organizations often make related to disaster recovery planning:&lt;ref&gt;[https://web.archive.org/web/20130116112225/http://content.dell.com/us/en/enterprise/d/large-business/mistakes-that-kill-disaster.aspx ''Five Mistakes That Can Kill a Disaster Recovery Plan. In archive.org''] Cormac Foster. Dell Corporation. 25 October 2010. Retrieved 8 August 2012.&lt;/ref&gt;

===Lack of buy-in===
One factor is the perception by executive management that DR planning is "just another fake earthquake drill" or CEOs that fail to make DR planning and preparation a priority, are often significant contributors to the failure of a DR plan.

===Incomplete RTOs and RPOs===
Another critical point is failure to include each and every important business process or a block of data. "Every item in your DR plan requires a Recovery Time Objective (RTO) defining maximum process downtime or a Recovery Point Objective (RPO) noting an acceptable restore point. Anything less creates ripples that can extend the disaster's impact." As an example, "payroll, accounting and the weekly customer newsletter may not be mission-critical in the first 24 hours, but left alone for several days, they can become more important than any of your initial problems."

===Systems myopia===
A third point of failure involves focusing only on DR without considering the larger business continuity needs: "Data and systems restoration after a disaster are essential, but every business process in your organization will need IT support, and that support requires planning and resources." As an example, corporate office space lost to a disaster can result in an instant pool of teleworkers which, in turn, can overload a company's [[VPN]] overnight, overwork the IT support staff at the blink of an eye and cause serious bottlenecks and monopolies with the dial-in PBX system.

===Lax security===
When there is a disaster, an organization's data and business processes become vulnerable. As such, security can be more important than the raw speed involved in a disaster recovery plan's RTO. The most critical consideration then becomes securing the new data pipelines: from new VPNs to the connection from offsite backup services. Another security concern includes documenting every step of the recovery process—something that is especially important in highly regulated industries, government agencies, or in disasters requiring post-mortem forensics. Locking down or remotely wiping lost handheld devices is also an area that may require addressing.

===Outdated plans===
Another important aspect that is often overlooked involves the frequency with which DR Plans are updated. Yearly updates are recommended but some industries or organizations require more frequent updates because business processes evolve or because of quicker data growth. To stay relevant, disaster recovery plans should be an integral part of all [[business analysis]] processes, and should be revisited at every major corporate acquisition, at every new product launch and at every new system development milestone.

==See also==
* [[Disaster recovery]]
* [[Business continuity planning]]
* [[Federal Emergency Management Agency]]
* [[Backup rotation scheme]]
* [[Seven tiers of disaster recovery]]

==References==
{{reflist|2}}

{{DEFAULTSORT:Disaster recovery plan}}
[[Category:Disaster recovery]]
[[Category:Data management]]
[[Category:Backup]]
[[Category:IT risk management]]
[[Category:Planning]]</text>
      <sha1>skazw29sfrdej87sp7soa1jkqe4yhq6</sha1>
    </revision>
  </page>
  <page>
    <title>Data access</title>
    <ns>0</ns>
    <id>1582494</id>
    <revision>
      <id>703967160</id>
      <parentid>697159929</parentid>
      <timestamp>2016-02-08T19:02:20Z</timestamp>
      <contributor>
        <username>Roman.korpachyov</username>
        <id>20212030</id>
      </contributor>
      <minor />
      <comment>Undid revision 682421722 by [[Special:Contributions/89.41.154.245|89.41.154.245]] ([[User talk:89.41.154.245|talk]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2194" xml:space="preserve">{{Refimprove|date=September 2014}}
'''Data access''' typically refers to software and activities related to storing, retrieving, or acting on [[data]] housed in a [[database]] or other [[Information repository|repository]]. Two fundamental types of data access exist:

# [[sequential access]] (as in [[Magnetic tape data storage|magnetic tape]], for example)
# [[random access]] (as in indexed [[Digital media|media]])

Data access crucially involves [[authorization]] to access different data repositories. Data access can help distinguish the abilities of administrators and users. For example, administrators may have the ability to remove, edit and add data, while general users may not even have "read" rights if they lack access to particular information.

Historically, each repository (including each different database, [[file system]], etc.), might require the use of different [[Method (computer science)|methods]] and [[languages]], and many of these repositories stored their content in different and incompatible formats.

Over the years standardized languages, methods, and formats, have developed to serve as interfaces between the often proprietary, and always idiosyncratic, specific languages and methods.  Such standards include [[SQL]] (1974- ), [[ODBC]] (ca 1990- ), [[JDBC]], [[XQuery API for Java|XQJ]], [[ADO.NET]], [[XML]], [[XQuery]], [[XPath]] (1999- ), and [[Web Services]].

Some of these standards enable translation of data from [[unstructured data|unstructured]] (such as HTML or free-text files) to [[structured data|structured]] (such as [[XML]] or [[SQL]]).

Structures such as [[connection string]]s and DBURLs&lt;ref&gt;
{{cite web
| url           = http://www.quickprogrammingtips.com/java/connecting-to-oracle-database-in-java.html
| title         = Connecting to Oracle Database in Java
| accessdate    = 2014-07-18
| quote         = DBURL is of the form [...] jdbc:oracle:thin:@machinename:1521:databasename [...]
}}
&lt;/ref&gt;
can attempt to standardise methods of [[Database connection|connecting to databases]].

== References ==
{{reflist}}

{{DEFAULTSORT:Data Access}}
[[Category:Data management]]
[[Category:Data access technologies| ]]


{{Database-stub}}</text>
      <sha1>rr2m2guq8q1xcgug8tjnboi0mp40gxr</sha1>
    </revision>
  </page>
  <page>
    <title>Inverted index</title>
    <ns>0</ns>
    <id>3125116</id>
    <revision>
      <id>750712977</id>
      <parentid>750712463</parentid>
      <timestamp>2016-11-21T11:49:17Z</timestamp>
      <contributor>
        <username>Zazpot</username>
        <id>632368</id>
      </contributor>
      <comment>/* Bibliography */ Improve poorly formatted references</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6701" xml:space="preserve">In [[computer science]], an '''inverted index''' (also referred to as '''postings file''' or '''inverted file''') is an [[index (database)|index data structure]] storing a mapping from content, such as words or numbers, to its locations in a [[Table (database)|database file]], or in a document or a set of documents (named in contrast to a [[Forward Index]], which maps from documents to content).  The purpose of an inverted index is to allow fast [[full text search]]es, at a cost of increased processing when a document is added to the database.  The inverted file may be the database file itself, rather than its [[Index (database)|index]]. It is the most popular data structure used in [[document retrieval]] systems,&lt;ref&gt;{{Harvnb |Zobel|Moffat|Ramamohanarao|1998| Ref=none }}&lt;/ref&gt; used on a large scale for example in [[search engine]]s.  Additionally, several significant general-purpose [[Mainframe computer|mainframe]]-based [[database management systems]] have used inverted list architectures, including [[ADABAS]], [[DATACOM/DB]], and [[Model 204]].

There are two main variants of inverted indexes: A '''record-level inverted index''' (or '''inverted file index''' or just '''inverted file''') contains a list of references to documents for each word. A '''word-level inverted index''' (or '''full inverted index''' or '''inverted list''') additionally contains the positions of each word within a document.&lt;ref name="isbn0-201-39829-X-p192"&gt;{{Harvnb |Baeza-Yates|Ribeiro-Neto|1999| p=192 | Ref=BYR99 }}&lt;/ref&gt; The latter form offers more functionality (like [[phrase search]]es), but needs more processing power and space to be created.

==Applications==

The inverted index [[data structure]] is a central component of a typical [[Index (search engine)|search engine indexing algorithm]]. A goal of a search engine implementation is to optimize the speed of the query: find the documents where word X occurs. Once a [[Search engine indexing#The forward index|forward index]] is developed, which stores lists of words per document, it is next inverted to develop an inverted index. Querying the forward index would require sequential iteration through each document and to each word to verify a matching document. The time, memory, and processing resources to perform such a query are not always technically realistic.  Instead of listing the words per document in the forward index, the inverted index data structure is developed which lists the documents per word.

With the inverted index created, the query can now be resolved by jumping to the word ID (via [[random access]]) in the inverted index.

In pre-computer times, [[Concordance (publishing)|concordances]] to important books were manually assembled.  These were effectively inverted indexes with a small amount of accompanying commentary that required a tremendous amount of effort to produce.

In bioinformatics, inverted indexes are very important in the [[sequence assembly]] of short fragments of sequenced DNA. One way to find the source of a fragment is to search for it against a reference DNA sequence. A small number of mismatches (due to differences between the sequenced DNA and reference DNA, or errors) can be accounted for by dividing the fragment into smaller fragments—at least one subfragment is likely to match the reference DNA sequence. The matching requires constructing an inverted index of all substrings of a certain length from the reference DNA sequence. Since the human DNA contains more than 3 billion base pairs, and we need to store a DNA substring for every index and a 32-bit integer for index itself, the storage requirement for such an inverted index would probably be in the tens of gigabytes.

==See also==
* [[Index (search engine)]]
* [[Reverse index]]
* [[Vector space model]]

== Bibliography ==
*{{cite book |last= Knuth |first= D. E. |authorlink= Donald Knuth |title= [[The Art of Computer Programming]] |publisher= [[Addison-Wesley]] |edition= Third |year= 1997 |origyear= 1973 |location= [[Reading, Massachusetts]] |isbn= 0-201-89685-0 |ref= Knu97 |chapter= 6.5. Retrieval on Secondary Keys}}
*{{cite journal|last1= Zobel |first1= Justin |last2=Moffat |first2=Alistair |last3=Ramamohanarao |first3=Kotagiri |date=December 1998 |title= Inverted files versus signature files for text indexing |journal= ACM Transactions on Database Systems |volume= 23 |issue= 4 |pages=453–490 |publisher= [[Association for Computing Machinery]] |location= New York |doi= 10.1145/296854.277632 |url= |accessdate= }}
*{{cite journal|last1= Zobel |first1= Justin |last2=Moffat |first2=Alistair |date=July 2006 |title= Inverted Files for Text Search Engines |journal= ACM Computing Surveys |volume= 38 |issue= 2 |page=6|publisher= [[Association for Computing Machinery]] |location= New York |doi= 10.1145/1132956.1132959 |url= |accessdate= }}
*{{cite book |last= Baeza-Yates | first = Ricardo |authorlink=Ricardo Baeza-Yates |author2=Ribeiro-Neto, Berthier |title= Modern information retrieval |publisher= Addison-Wesley Longman |location= [[Reading, Massachusetts]] |year= 1999 |isbn= 0-201-39829-X |oclc= |doi= |ref= BYR99 |page= 192 }}
*{{cite journal |last= Salton | first = Gerard |author2=Fox, Edward A. |author3=Wu, Harry  |title= Extended Boolean information retrieval |publisher= ACM |year= 1983
|journal = Commun. ACM |volume = 26 |issue = 11 |doi= 10.1145/182.358466 |page=1022 }}
*{{cite book |title=Information Retrieval: Implementing and Evaluating Search Engines  |url=http://www.ir.uwaterloo.ca/book/ |publisher=MIT Press |year=2010 |location=Cambridge, Massachusetts |isbn= 978-0-262-02651-2 |author8=Stefan B&amp;uuml;ttcher, Charles L. A. Clarke, and Gordon V. Cormack}}

==References==
{{Reflist}}

==External links==
*[https://xlinux.nist.gov/dads/HTML/invertedIndex.html NIST's Dictionary of Algorithms and Data Structures: inverted index]
*[http://mg4j.di.unimi.it Managing Gigabytes for Java] a free full-text search engine for large document collections written in Java.
*[http://lucene.apache.org/java/docs/ Lucene] - Apache Lucene is a full-featured text search engine library written in Java.
*[http://sphinxsearch.com/ Sphinx Search] - Open source high-performance, full-featured text search engine library used by craigslist and others employing an inverted index.
*[http://rosettacode.org/wiki/Inverted_Index Example implementations] on [[Rosetta Code]]
* [http://www.vision.caltech.edu/malaa/software/research/image-search/ Caltech Large Scale Image Search Toolbox]: a Matlab toolbox implementing Inverted File Bag-of-Words image search.

[[Category:Data management]]
[[Category:Search algorithms]]
[[Category:Database index techniques]]
[[Category:Substring indices]]</text>
      <sha1>0ake1ml4qryliozkopdv1puxvpmyr05</sha1>
    </revision>
  </page>
  <page>
    <title>Clustered file system</title>
    <ns>0</ns>
    <id>10459749</id>
    <revision>
      <id>758821001</id>
      <parentid>746471842</parentid>
      <timestamp>2017-01-07T19:41:02Z</timestamp>
      <contributor>
        <username>Goose121</username>
        <id>26303422</id>
      </contributor>
      <comment>Removed merge template</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="13843" xml:space="preserve">{{distinguish|Data cluster}}
{{redirect2|Network filesystem|Parallel file system|the Sun NFS protocol|Network File System|the IBM GPFS protocol|IBM General Parallel File System}}
{{multiple|
{{refimprove|date=December 2015}}
{{cleanup|date=December 2013|reason=Merges need to be smoothed over}}
}}

A '''clustered file system''' is a [[file system]] which is shared by being simultaneously [[Mount (computing)|mounted]] on multiple [[Server (computing)|servers]].  There are several approaches to [[computer cluster|clustering]], most of which do not employ a clustered file system (only [[direct attached storage]] for each node).  Clustered file systems can provide features like location-independent addressing and redundancy which improve reliability or reduce the complexity of the other parts of the cluster.  '''Parallel file systems''' are a type of clustered file system that spread data across multiple storage nodes, usually for redundancy or performance.&lt;ref&gt;http://www.dell.com/downloads/global/power/ps2q05-20040179-Saify-OE.pdf&lt;/ref&gt;

== {{Anchor|SHARED-DISK}}Shared-disk file system ==
A '''shared-disk file system''' uses a [[storage area network|storage-area network]] (SAN) to allow multiple computers to gain  direct disk access at the [[Block (data storage)|block level]].  Access control and translation from file-level operations that applications use to block-level operations used by the SAN must take place on the client node.  The most common type of clustered file system, the shared-disk file system &amp;mdash;by adding mechanisms for [[concurrency control]]&amp;mdash;provides a consistent and [[serialization|serializable]] view of the file system, avoiding corruption and unintended [[data loss]] even when multiple clients try to access the same files at the same time. Shared-disk file-systems commonly employ some sort of [[Fencing (computing)|fencing]] mechanism to prevent data corruption in case of node failures, because an unfenced device can cause data corruption if it loses communication with its sister nodes and tries to access the same information other nodes are accessing.

The underlying storage area network may use any of a number of block-level protocols, including [[SCSI]], [[iSCSI]], [[HyperSCSI]], [[ATA over Ethernet]] (AoE), [[Fibre Channel]], [[network block device]], and [[InfiniBand]].

There are different architectural approaches to a shared-disk filesystem. Some distribute file information across all the servers in a cluster (fully distributed). Others utilize a centralized [[metadata]] server. Both achieve the same result of enabling all servers to access all the data on a shared storage device.{{Citation needed|date=December 2009}}

=== Examples ===
{{Div col||25em}}
* [[Blue Whale Clustered file system]] (BWFS)
* [[Silicon Graphics]] (SGI) clustered file system ([[CXFS]])
* [[Veritas Cluster File System]]
* DataPlow [[Nasan]] File System
* [[IBM General Parallel File System]] (GPFS)
* [[LizardFS]]
* [[Lustre (file system)|Lustre]]
* Microsoft [[Cluster Shared Volumes]] (CSV)
* [[OCFS|Oracle Cluster File System]] (OCFS)
* PolyServe storage solutions
* [[Quantum Corporation|Quantum]] [[StorNext File System|StorNext]] File System (SNFS), ex ADIC, ex CentraVision File System (CVFS)
* Red Hat [[Global File System]] (GFS)
* Sun [[QFS]]
* TerraScale Technologies TerraFS
* Versity VSM
* [[VMware VMFS]]
* [[Xsan]]
{{Div col end}}

=={{Anchor|DISTRIBUTED-FS}}Distributed file systems==
''Distributed file systems'' do not share block level access to the same storage but use a network [[protocol (computing)|protocol]].&lt;ref&gt;Silberschatz, Galvin (1994). ''Operating System concepts'', chapter 17 ''Distributed file systems''. Addison-Wesley Publishing Company. ISBN 0-201-59292-4.&lt;/ref&gt;&lt;ref name="ostep-1"&gt;{{citation|title=Sun's Network File System|url=http://pages.cs.wisc.edu/~remzi/OSTEP/dist-nfs.pdf|publisher= Arpaci-Dusseau Books|date = 2014|first1 = Remzi H.|last1 =Arpaci-Dusseau|first2=Andrea C.|last2 = Arpaci-Dusseau}}&lt;/ref&gt;  These are commonly known as ''network file systems'', even though they are not the only file systems that use the network to send data.{{citation needed|date=March 2013}}  Distributed file systems can restrict access to the file system depending on [[access list]]s or [[Capability-based security|capabilities]] on both the servers and the clients, depending on how the protocol is designed.

The difference between a distributed file system and a [[distributed data store]] is that a distributed file system allows files to be accessed using the same interfaces and semantics as local files{{snd}} for example, mounting/unmounting, listing directories, read/write at byte boundaries, system's native permission model.  Distributed data stores, by contrast, require using a different API or library and have different semantics (most often those of a database).{{cn|date=June 2016}}

A distributed file system may also be created by software implementing IBM's [[Distributed Data Management Architecture]] (DDM), in which programs running on one computer use local interfaces and semantics to create, manage and access files located on other networked computers.  All such client requests are trapped and converted to equivalent messages defined by the DDM. Using protocols also defined by the DDM, these messages are transmitted to the specified remote computer on which a DDM server program interprets the messages and uses the file system interfaces of that computer to locate and interact with the specified file.

===Design goals===
Distributed file systems may aim for "transparency" in a number of aspects.  That is, they aim to be "invisible" to client programs, which "see" a system which is similar to a local file system.  Behind the scenes, the distributed file system handles locating files, transporting data, and potentially providing other features listed below.

* ''Access transparency'' is that clients are unaware that files are distributed and can access them in the same way as local files are accessed.
* ''Location transparency''; a consistent name space exists encompassing local as well as remote files. The name of a file does not give its location.
* ''Concurrency transparency''; all clients have the same view of the state of the file system. This means that if one process is modifying a file, any other processes on the same system or remote systems that are accessing the files will see the modifications in a coherent manner.
* ''Failure transparency''; the client and client programs should operate correctly after a server failure.
* ''Heterogeneity''; file service should be provided across different hardware and operating system platforms.
* ''Scalability''; the file system should work well in small environments (1 machine, a dozen machines) and also scale gracefully to huge ones (hundreds through tens of thousands of systems).
* ''Replication transparency''; to support scalability, we may wish to replicate files across multiple servers. Clients should be unaware of this.
* ''Migration transparency''; files should be able to move around without the client's knowledge.

===History===
The [[Incompatible Timesharing System]] used virtual devices for transparent inter-machine file system access in the 1960s.  More file servers were developed in the 1970s. In 1976 [[Digital Equipment Corporation]] created the [[File Access Listener]] (FAL), an implementation of the [[Data Access Protocol]] as part of [[DECnet]] Phase II which became the first widely used network file system. In 1985 [[Sun Microsystems]] created the file system called "[[Network File System (protocol)|Network File System]]" (NFS) which became the first widely used [[Internet Protocol]] based network file system.&lt;ref name="ostep-1" /&gt;  Other notable network file systems are [[Andrew File System]] (AFS), [[Apple Filing Protocol]] (AFP), [[NetWare Core Protocol]] (NCP), and [[Server Message Block]] (SMB) which is also known as Common Internet File System (CIFS).

In 1986, [[IBM]] announced client and server support for Distributed Data Management Architecture (DDM) for the [[System/36]], [[System/38]], and IBM mainframe computers running [[CICS]].  This was followed by the support for [[IBM Personal Computer]], [[AS/400]], IBM mainframe computers under the [[MVS]] and [[VSE (operating system)|VSE]] operating systems, and [[FlexOS]].   DDM also became the foundation for [[DRDA|Distributed Relational Database Architecture]], also known as DRDA.

=== Examples ===
{{Main|List of file systems#Distributed file systems|l1 = List of distributed file systems}}
{{Div col||25em}}
* [[BeeGFS]] (Fraunhofer)
* [[Ceph (software)|Ceph]] (Inktank, Red Hat, SUSE)
* [[Distributed File System (Microsoft)|Windows Distributed File System (DFS)]] (Microsoft)
* [[Infinit (file system)|Infinit]]
* [[Gfarm file system|GfarmFS]]
* [[GlusterFS]] (Red Hat)
* [[Google file system|GFS]] (Google Inc.)
* [[Hadoop distributed file system|HDFS]] (Apache Software Foundation)
* [[InterPlanetary File System|IPFS]]
* iRODS
* [[LizardFS]] (Skytechnology)
* [[Moose File System|MooseFS]] (Core Technology / Gemius)
* [[ObjectiveFS]]
* [[OneFS]] (EMC Isilon)
* OpenIO
* [[OrangeFS]] (Clemson University, Omnibond Systems), formerly [[Parallel Virtual File System]]
* [[Panasas|Panfs]] (Panasas)
* [[Parallel Virtual File System]] (Clemson University, Argonne National Laboratory, Ohio Supercomputer Center)
* [[RozoFS]] (Rozo Systems)
* Torus (CoreOS)
* [[XtreemFS]]
{{Div col end}}

==Network-attached storage==
{{Main|Network-attached storage}}

Network-attached storage (NAS) provides both storage and a file system, like a shared disk file system on top of a storage area network (SAN).  NAS typically uses file-based protocols (as opposed to block-based protocols a SAN would use) such as [[Network File System (protocol)|NFS]] (popular on [[UNIX]] systems), SMB/CIFS ([[Server Message Block|Server Message Block/Common Internet File System]]) (used with MS Windows systems), [[Apple Filing Protocol|AFP]] (used with [[Macintosh|Apple Macintosh]] computers), or NCP (used with [[Novell Open Enterprise Server|OES]] and [[NetWare|Novell NetWare]]).

==Design considerations==

===Avoiding single point of failure===

The failure of disk hardware or a given storage node in a cluster can create a [[single point of failure]] that can result in [[data loss]] or unavailability.  [[Fault tolerance]] and high availability can be provided through [[Replication (computing)|data replication]] of one sort or another, so that data remains intact and available despite the failure of any single piece of equipment.  For examples, see the lists of [[List of file systems#Distributed fault-tolerant file systems|distributed fault-tolerant file systems]] and [[List of file systems#Distributed parallel fault-tolerant file systems|distributed parallel fault-tolerant file systems]].

===Performance===

A common [[performance]] [[measurement]] of a clustered file system is the amount of time needed to satisfy service requests.  In conventional systems, this time consists of a disk-access time and a small amount of [[Central processing unit|CPU]]-processing time.  But in a clustered file system, a remote access has additional overhead due to the distributed structure.  This includes the time to deliver the request to a server, the time to deliver the response to the client, and for each direction, a CPU overhead of running the [[communication protocol]] [[software]].

===Concurrency===

Concurrency control becomes an issue when more than one person or client is accessing the same file or block and want to update it.  Hence updates to the file from one client should not interfere with access and updates from other clients. This problem is more complex with file systems due to concurrent overlapping writes, where different writers write to overlapping regions of the file concurrently.&lt;ref&gt;Pessach, Yaniv (2013). ''Distributed Storage: Concepts, Algorithms, and Implementations''. ISBN 978-1482561043.&lt;/ref&gt; This problem is usually handled by [[concurrency control]] or [[lock (computer science)|locking]] which may either be built into the file system or provided by an add-on protocol.

==History==
IBM mainframes in the 1970s could share physical disks and file systems if each machine had its own channel connection to the drives' control units. In the 1980s, [[Digital Equipment Corporation]]'s [[TOPS-20]] and [[OpenVMS]] clusters (VAX/ALPHA/IA64) included shared disk file systems.{{Citation needed|date=May 2016}}

==See also==
{{Div col||25em}}
* [[Network-attached storage]]
* [[Storage area network]]
* [[Shared resource]]
* [[Direct-attached storage]]
* [[Peer-to-peer file sharing]]
* [[Disk sharing]]
* [[Distributed data store]]
* [[Distributed file system for cloud]]
* [[Global file system]]
* [[Gopher (protocol)]]
* [[List of file systems#Distributed file systems|List of distributed file systems]]
* [[CacheFS]]
* [[RAID]]
{{Div col end}}

==References==
{{reflist}}

==Further reading==
* [http://www.cloudbus.org/reports/DistributedStorageTaxonomy.pdf A Taxonomy of Distributed Storage Systems]
* [http://trac.nchc.org.tw/grid/raw-attachment/wiki/jazz/09-05-22/A_Taxonomy_and_Survey_on_Distributed_File_Systems.pdf A Taxonomy and Survey on Distributed File Systems]
* [http://www.cis.upenn.edu/~bcpierce/courses/dd/papers/satya89survey.ps A survey of distributed file systems]
* [http://www.snia-europe.org/objects_store/Christian_Bandulet_SNIATutorial%20Basics_EvolutionFileSystems.pdf The Evolution of File Systems]

{{File systems|state=collapsed}}

[[Category:Computer file systems]]
[[Category:Shared disk file systems| ]]
[[Category:Storage area networks]]
[[Category:Distributed file systems| ]]
[[Category:Data management]]
[[Category:Distributed data storage]]
[[Category:Network file systems]]</text>
      <sha1>gf3rwvd42jmdodkazhwpy8r4npx3isw</sha1>
    </revision>
  </page>
  <page>
    <title>Write–write conflict</title>
    <ns>0</ns>
    <id>217748</id>
    <revision>
      <id>731667758</id>
      <parentid>664943200</parentid>
      <timestamp>2016-07-26T19:39:12Z</timestamp>
      <contributor>
        <username>Chris the speller</username>
        <id>525927</id>
      </contributor>
      <minor />
      <comment>cap, punct</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1284" xml:space="preserve">In [[computer science]], in the field of [[database]]s, '''write–write conflict''', also known as '''overwriting [[commit (data management)|uncommitted]] data''' is a computational anomaly associated with interleaved execution of [[Database transaction|transactions]].

Given a [[Schedule (computer science)|schedule]] S

&lt;math&gt;S = \begin{bmatrix}
T1 &amp; T2 \\
W(A) &amp; \\
 &amp; W(B) \\
W(B) &amp; \\
Com. &amp; \\
 &amp; W(A)\\
 &amp; Com. \end{bmatrix}&lt;/math&gt;

note that there is no read in this schedule. The writes are called '''''blind writes'''''.

We have a '''''lost update'''''.  Any attempts to make this schedule serial would give off two different results (either T1's version of A and B is shown, or T2's version of A and B is shown), and would not be the same as the above schedule.  This schedule would not be [[Serializability|serializable]].

[[Strict two-phase locking|Strict 2PL]] overcomes this inconsistency by locking T1 out from B.  Unfortunately, [[deadlock]]s are something Strict 2PL does not overcome all the time.

== See also ==

* [[Concurrency control]]
* [[Read–write conflict]]
* [[Write–read conflict]]

==References==
{{reflist}}
{{Unreferenced|date=August 2009}}

{{DEFAULTSORT:Write-write conflict}}
[[Category:Data management]]
[[Category:Transaction processing]]</text>
      <sha1>j4999757pqsyb3seqq7ty4tz35f56g1</sha1>
    </revision>
  </page>
  <page>
    <title>XSA</title>
    <ns>0</ns>
    <id>15323614</id>
    <revision>
      <id>667005266</id>
      <parentid>667005265</parentid>
      <timestamp>2015-06-15T05:31:30Z</timestamp>
      <contributor>
        <username>ClueBot NG</username>
        <id>13286072</id>
      </contributor>
      <minor />
      <comment>Reverting possible vandalism by [[Special:Contributions/59.32.95.83|59.32.95.83]] to version by X2Fusion. False positive? [[User:ClueBot NG/FalsePositives|Report it]]. Thanks, [[User:ClueBot NG|ClueBot NG]]. (2273970) (Bot)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1877" xml:space="preserve">In computer science, '''XSA''' (better known as '''Cross-Server Attack''') is a networking security intrusion method which allows for a malicious client to compromise security over a website or service on a server by using implemented services on the server that may not be secure.

In general, XSA is demonstrated against websites, yet sometimes it is used in conjunction with other services located on the same server.

== Basics ==
XSA is a method that allows for a malicious client to use services that a remote server implements in order to attack another service on the same server or network.

Most website hosting companies that offer hosting for large or even little amounts of separate websites are vulnerable to this method of attack, because of the amount of access services such as [[PHP]] and the webserver itself give to a client that allows the client to access other website configurations, files, passwords and the like.

== History ==

The term 'XSA' was first coined by DeadlyData, a prominent [[Computer hacker]] during the early 2000s, over the voice communications software [[TeamSpeak]]. While he had not invented or pioneered this method of intrusion, he coined it as a shortened term to describe the act of performing Cross-Server Attacks (XSAs).

It was then used further in the community and now supports for most of the methods and subsets of the method that give both [[Computer hacker]] and malicious individuals the terminology to attack websites using software that is located on the same server.

== See also ==
{{Portal|Software Testing}}
*[[SQL Injection]] (SQLi)
*[[Cross-Site Scripting]] (XSS)
*[[Cross-Site Request Forgery]] (CSRF)
*[[Buffer Overflow]]

{{DEFAULTSORT:XSA}}
[[Category:Data management]]
[[Category:Computer security exploits]]
[[Category:Computer network security]]
[[Category:World Wide Web]]
[[Category:Web development]]</text>
      <sha1>3osnpcdw5oa7o192dg5145tii64e33b</sha1>
    </revision>
  </page>
  <page>
    <title>Online analytical processing</title>
    <ns>0</ns>
    <id>189239</id>
    <revision>
      <id>757186245</id>
      <parentid>752282782</parentid>
      <timestamp>2016-12-29T08:23:03Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor />
      <comment>/* Comparison */Remove blank line(s) between list items per [[WP:LISTGAP]] to fix an accessibility issue for users of [[screen reader]]s. Do [[WP:GENFIXES]] and cleanup if needed. Discuss this at [[Wikipedia talk:WikiProject Accessibility/LISTGAP]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="29193" xml:space="preserve">'''Online analytical processing''', or '''OLAP''' ({{IPAc-en|ˈ|oʊ|l|æ|p}}), is an approach to answering [[multi-dimensional analytical]] (MDA) queries swiftly in [[computing]], .&lt;ref name=Codd1993&gt;{{cite web
  |url=http://www.sgpnyc.com/us/products/dataquest/whitepapers/OLAP_wp_efcodd.pdf
  |title=Providing OLAP (On-line Analytical Processing) to User-Analysts: An IT Mandate
  |publisher=Codd &amp; Date, Inc
  |author1=Codd E.F. |author2=Codd S.B. |author3=Salley C.T.  |last-author-amp=yes |year=1993
  |accessdate=2008-03-05 }}&lt;/ref&gt; OLAP is part of the broader category of [[business intelligence]], which also encompasses [[relational database]], report writing and [[data mining]].&lt;ref&gt;{{cite book
  |url=https://books.google.com/books?id=M-UOE1Cp9OEC
  |title=Business Intelligence for Telecommunications
  |publisher=CRC Press
  |author=Deepak Pareek
  |year=2007
  |pages=294 pp
  |isbn=0-8493-8792-2
  |accessdate=2008-03-18
}}&lt;/ref&gt;  Typical applications of OLAP include [[business reporting]] for sales, [[marketing]], management reporting, [[business process management]] (BPM),&lt;ref&gt;{{cite book
  |url=http://www.google.com/products?q=9783639222166
  |title=Business Process Management:A Data Cube To Analyze Business Process Simulation Data For Decision Making
  |publisher=[[VDM Verlag|VDM Verlag Dr. Müller e.K.]]
  |author=Apostolos Benisis
  |year=2010
  |pages=204 pp
  |isbn=978-3-639-22216-6
}}&lt;/ref&gt; [[budget]]ing and [[forecasting|forecast]]ing, [[financial reporting]] and similar areas, with new applications coming up, such as [[agriculture]].&lt;ref name=ahsan/&gt; The term ''OLAP'' was created as a slight modification of the traditional database term [[online transaction processing]] (OLTP).&lt;ref&gt;{{cite web
  |url=http://www.symcorp.com/downloads/OLAP_CouncilWhitePaper.pdf
  |format=PDF|title=OLAP Council White Paper
  |publisher=OLAP Council
  |year=1997

  |accessdate=2008-03-18
}}&lt;/ref&gt;

OLAP tools enable users to analyze multidimensional data interactively from multiple perspectives. OLAP consists of three basic analytical operations: consolidation (roll-up), drill-down, and slicing and dicing.&lt;ref&gt;O'Brien &amp; Marakas, 2011, p. 402-403&lt;/ref&gt; Consolidation involves the aggregation of data that can be accumulated and computed in one or more dimensions. For example, all sales offices are rolled up to the sales department or sales division to anticipate sales trends. By contrast, the drill-down is a technique that allows users to navigate through the details. For instance, users can view the sales by individual products that make up a region's sales. Slicing and dicing is a feature whereby users can take out (slicing) a specific set of data of the [[OLAP cube]] and view (dicing) the slices from different viewpoints.  These viewpoints are sometimes called dimensions (such as looking at the same sales by salesperson or by date or by customer or by product or by region, etc.)

[[Database]]s configured for OLAP use a multidimensional data model, allowing for complex analytical and [[ad hoc]] queries with a rapid execution time.&lt;ref&gt;{{cite web
  |url=http://www.dwreview.com/OLAP/Introduction_OLAP.html
  |title=Introduction to OLAP – Slice, Dice and Drill!
  |publisher=Data Warehousing Review
  |author=Hari Mailvaganam
  |year=2007  |accessdate=2008-03-18
}}&lt;/ref&gt;  They borrow aspects of [[navigational database]]s, [[hierarchical database]]s and relational databases.

OLAP is typically contrasted to [[Online transaction processing|OLTP]] (online transaction processing), which is generally characterized by much less complex queries, in a larger volume, to process transactions rather than for the purpose of business intelligence or reporting. Whereas OLAP systems are mostly optimized for read, OLTP has to processes all kinds of queries (read, insert, update and delete).

== Overview of OLAP systems ==
At the core of any OLAP system is an [[OLAP cube]] (also called a 'multidimensional cube' or a [[hypercube]]). It consists of numeric facts called ''measures'' that are categorized by ''[[Dimension (data warehouse)|dimensions]]''. The measures are placed at the intersections of the hypercube, which is spanned by the dimensions as a [[vector space]]. The usual interface to manipulate an OLAP cube is a matrix interface, like [[Pivot table]]s in a spreadsheet program, which performs projection operations along the dimensions, such as aggregation or averaging.

The cube metadata is typically created from a [[star schema]] or [[snowflake schema]] or [[fact constellation]] of tables in a [[relational database]]. Measures are derived from the records in the [[fact table]] and dimensions are derived from the [[dimension table]]s.

Each ''measure'' can be thought of as having a set of ''labels'', or meta-data associated with it. A ''dimension'' is what describes these ''labels''; it provides information about the ''measure''.

A simple example would be a cube that contains a store's sales as a ''measure'', and Date/Time as a ''dimension''. Each Sale has a Date/Time ''label'' that describes more about that sale.

For example:
  Sale Fact Table
 +-------------+----------+
 | sale_amount | time_id  |
 +-------------+----------+            Time Dimension
 |      2008.10|     1234 |----+     +---------+-------------------+
 +-------------+----------+    |     | time_id | timestamp         |
                               |     +---------+-------------------+
                               +----&gt;|   1234  | 20080902 12:35:43 |
                                     +---------+-------------------+

=== Multidimensional databases ===
Multidimensional structure is defined as "a variation of the relational model that uses multidimensional structures to organize data and express the relationships between data".&lt;ref&gt;O'Brien &amp; Marakas, 2009, pg 177&lt;/ref&gt;  The structure is broken into cubes and the cubes are able to store and access data within the confines of each cube. "Each cell within a multidimensional structure contains aggregated data related to elements along each of its dimensions".&lt;ref&gt;O'Brien &amp; Marakas, 2009, pg 178&lt;/ref&gt;  Even when data is manipulated it remains easy to access and continues to constitute a compact database format.  The data still remains interrelated.
Multidimensional structure is quite popular for analytical databases that use online analytical processing (OLAP) applications.&lt;ref&gt;(O'Brien &amp; Marakas, 2009)&lt;/ref&gt;  Analytical databases use these databases because of their ability to deliver answers to complex business queries swiftly.  Data can be viewed from different angles, which gives a broader perspective of a problem unlike other models.&lt;ref&gt;Williams, C., Garza, V.R., Tucker, S, Marcus, A.M. (1994, January 24). Multidimensional models boost viewing options. InfoWorld, 16(4)&lt;/ref&gt;

=== Aggregations ===
It has been claimed that for complex queries OLAP cubes can produce an answer in around 0.1% of the time required for the same query on [[OLTP]] relational data.&lt;ref&gt;{{cite web
  | author=MicroStrategy, Incorporated
  | year=1995
  | title=The Case for Relational OLAP
  | url=http://www.cs.bgu.ac.il/~onap052/uploads/Seminar/Relational%20OLAP%20Microstrategy.pdf

  |format=PDF| accessdate=2008-03-20
}}&lt;/ref&gt;&lt;ref&gt;{{cite journal
  |author1=Surajit Chaudhuri  |author2=Umeshwar Dayal
   |lastauthoramp=yes | title = An overview of data warehousing and OLAP technology
  | journal = SIGMOD Rec.
  | publisher = [[Association for Computing Machinery|ACM]]
  | volume = 26
  | issue = 1
  | year = 1997

  | pages = 65
  | url = http://doi.acm.org/10.1145/248603.248616
  | doi = 10.1145/248603.248616

  | accessdate=2008-03-20
}}&lt;/ref&gt;  The most important mechanism in OLAP which allows it to achieve such performance is the use of ''aggregations''. Aggregations are built from the fact table by changing the granularity on specific dimensions and aggregating up data along these dimensions. The number of possible aggregations is determined by every possible combination of dimension granularities.

The combination of all possible aggregations and the base data contains the answers to every query which can be answered from the data.&lt;ref&gt;{{cite journal
  | last1 = Gray | first1 = Jim
  | author1-link = Jim Gray (computer scientist)
  | last2 = Chaudhuri | first2 = Surajit
  | last3 = Layman | first3 = Andrew
  | last4 = Reichart | first4 = Don
  | last5 = Venkatrao | first5 = Murali
  | last6 = Pellow | first6 = Frank
  | last7 = Pirahesh | first7 = Hamid
  | title = Data Cube: {A} Relational Aggregation Operator Generalizing Group-By, Cross-Tab, and Sub-Totals
  | journal = J. Data Mining and Knowledge Discovery
  | volume = 1
  | issue = 1
  | pages = 29–53
  | year = 1997
  | url = http://citeseer.ist.psu.edu/gray97data.html

  | accessdate=2008-03-20
}}&lt;/ref&gt;

Because usually there are many aggregations that can be calculated, often only a predetermined number are fully calculated; the remainder are solved on demand.  The problem of deciding which aggregations (views) to calculate is known as the view selection problem.  View selection can be constrained by the total size of the selected set of aggregations, the time to update them from changes in the base data, or both.  The objective of view selection is typically to minimize the average time to answer OLAP queries, although some studies also minimize the update time.  View selection is [[NP-Complete]]. Many approaches to the problem have been explored, including [[greedy algorithm]]s, randomized search, [[genetic algorithm]]s and [[A* search algorithm]].

==Types==
OLAP systems have been traditionally categorized using the following taxonomy.&lt;ref name=Pendse2006&gt;{{cite web|url=http://www.olapreport.com/Architectures.htm |title=OLAP architectures |publisher=OLAP Report |author=Nigel Pendse |date=2006-06-27 |accessdate=2008-03-17 |deadurl=yes |archiveurl=https://web.archive.org/web/20080124155954/http://www.olapreport.com/Architectures.htm |archivedate=January 24, 2008 }}&lt;/ref&gt;

===Multidimensional OLAP (MOLAP)===
MOLAP (multi-dimensional online analytical processing) is the classic form of OLAP and is sometimes referred to as just OLAP. MOLAP stores this data in an optimized multi-dimensional array storage, rather than in a relational database.

Some MOLAP tools require the [[pre-computation]] and storage of derived data, such as consolidations – the operation known as processing. Such MOLAP tools generally utilize a pre-calculated data set referred to as a data cube. The data cube contains all the possible answers to a given range of questions. As a result, they  have a very fast response to queries. On the other hand, updating can take a long time depending on the degree of pre-computation. Pre-computation can also lead to what is known as data explosion.

Other MOLAP tools, particularly those that implement the [[Functional Database Model|functional database model]] do not pre-compute derived data but make all calculations on demand other than those that were previously requested and stored in a cache.

'''Advantages of MOLAP'''
* Fast query performance due to optimized storage, multidimensional indexing and caching.
* Smaller on-disk size of data compared to data stored in [[relational database]] due to compression techniques.
* Automated computation of higher level aggregates of the data.
* It is very compact for low dimension data sets.
* Array models provide natural indexing.
* Effective data extraction achieved through the pre-structuring of aggregated data.

'''Disadvantages of MOLAP'''
* Within some MOLAP Solutions the processing step (data load) can be quite lengthy, especially on large data volumes. This is usually remedied by doing only incremental processing, i.e., processing only the data which have changed (usually new data) instead of reprocessing the entire data set.
* Some MOLAP methodologies introduce data redundancy.

====Products====
Examples of commercial products that use MOLAP are [[Cognos]] Powerplay, [[Oracle OLAP|Oracle Database OLAP Option]], [[MicroStrategy]], [[Microsoft Analysis Services]], [[Essbase]], [[Applix|TM1]], [[Jedox]], and [[icCube]].

===Relational OLAP (ROLAP)===
'''ROLAP''' works directly with relational databases and does not require pre-computation. The base data and the dimension tables are stored as relational tables and new tables are created to hold the aggregated information. It depends on a specialized schema design. This methodology relies on manipulating the data stored in the relational database to give the appearance of traditional OLAP's slicing and dicing functionality. In essence, each action of slicing and dicing is equivalent to adding a "WHERE" clause in the SQL statement. ROLAP tools do not use pre-calculated data cubes but instead pose the query to the standard relational database and its tables in order to bring back the data required to answer the question. ROLAP tools feature the ability to ask any question because the methodology does not limit to the contents of a cube.  ROLAP also has the ability to drill down to the lowest level of detail in the database.

While ROLAP uses a relational database source, generally the database must be carefully designed for ROLAP use.  A database which was designed for [[OLTP]] will not function well as a ROLAP database.  Therefore, ROLAP still involves creating an additional copy of the data.  However, since it is a database, a variety of technologies can be used to populate the database.

==== Advantages of ROLAP ====
&lt;!--Note to editors:
Please review the discussion page before making changes to the advantages or disadvantages. Thank you.
--&gt;

* ROLAP is considered to be more scalable in handling large data volumes, especially models with [[Dimension (data warehouse)|dimensions]] with very high [[cardinality]] (i.e., millions of members).
* With a variety of data loading tools available, and the ability to fine-tune the [[Extract, transform, load|ETL]] code to the particular data model, load times are generally much shorter than with the automated [[#Multidimensional_OLAP_.28MOLAP.29|MOLAP]] loads.
* The data are stored in a standard [[relational database]] and can be accessed by any [[SQL]] reporting tool (the tool does not have to be an OLAP tool).
* ROLAP tools are better at handling ''non-aggregatable facts'' (e.g., textual descriptions).  [[#Multidimensional_OLAP_.28MOLAP.29|MOLAP]] tools tend to suffer from slow performance when querying these elements.
* By [[Decoupling (electronics)|decoupling]] the data storage from the multi-dimensional model, it is possible to successfully model data that would not otherwise fit into a strict dimensional model.
* The ROLAP approach can leverage [[database]] authorization controls such as row-level security, whereby the query results are filtered depending on preset criteria applied, for example, to a given user or group of users ([[SQL]] WHERE clause).

==== Disadvantages of ROLAP ====
&lt;!--Note to editors:
Please review the discussion page before making changes to the advantages or disadvantages. Thank you.
--&gt;

* There is a consensus in the industry that ROLAP tools have slower performance than MOLAP tools. However, see the discussion below about ROLAP performance.
* The loading of ''aggregate tables'' must be managed by custom [[Extract, transform, load|ETL]] code.  The ROLAP tools do not help with this task.  This means additional development time and more code to support.
* When the step of creating aggregate tables is skipped, the query performance then suffers because the larger detailed tables must be queried. This can be partially remedied by adding additional aggregate tables, however it is still not practical to create aggregate tables for all combinations of dimensions/attributes.
* ROLAP relies on the general purpose database for querying and caching, and therefore several special techniques employed by [[MOLAP]] tools are not available (such as special hierarchical indexing).  However, modern ROLAP tools take advantage of latest improvements in [[SQL]] language such as CUBE and ROLLUP operators, DB2 Cube Views, as well as other SQL OLAP extensions.  These SQL improvements can mitigate the benefits of the [[MOLAP]] tools.
* Since ROLAP tools rely on [[SQL]] for all of the computations, they are not suitable when the model is heavy on calculations which don't translate well into [[SQL]]. Examples of such models include budgeting, allocations, financial reporting and other scenarios.

==== Performance of ROLAP ====

In the OLAP industry ROLAP is usually perceived as being able to scale for large data volumes, but suffering from slower query performance as opposed to [[#Multidimensional_OLAP_.28MOLAP.29|MOLAP]]. The [http://www.olapreport.com/survey.htm OLAP Survey], the largest independent survey across all major OLAP products, being conducted for 6 years (2001 to 2006) have consistently found that companies using ROLAP report slower performance than those using MOLAP even when data volumes were taken into consideration.

However, as with any survey there are a number of subtle issues that must be taken into account when interpreting the results.
* The survey shows that ROLAP tools have 7 times more users than [[#Multidimensional_OLAP_.28MOLAP.29|MOLAP]] tools within each company.  Systems with more users will tend to suffer more performance problems at peak usage times.
* There is also a question about complexity of the model, measured both in number of dimensions and richness of calculations. The survey does not offer a good way to control for these variations in the data being analyzed.

==== Downside of flexibility ====

Some companies select ROLAP because they intend to re-use existing relational database tables—these tables will frequently not be optimally designed for OLAP use.  The superior flexibility of ROLAP tools allows this less than optimal design to work, but performance suffers.  [[#Multidimensional_OLAP_.28MOLAP.29|MOLAP]] tools in contrast would force the data to be re-loaded into an optimal OLAP design.

===Hybrid OLAP (HOLAP)===
The undesirable trade-off between additional [[Extract, transform, load|ETL]] cost and slow query performance has ensured that most commercial OLAP tools now use a "Hybrid OLAP" (HOLAP) approach, which allows the model designer to decide which portion of the data will be stored in [[#Multidimensional_OLAP_.28MOLAP.29|MOLAP]] and which portion in ROLAP.

There is no clear agreement across the industry as to what constitutes "Hybrid OLAP", except that a database will divide data between relational and specialized storage.&lt;ref name="ieee_cite"&gt;{{cite journal
  | last1 = Bach Pedersen | first1 = Torben
  | last2 = S. Jensen 
  | title = Multidimensional Database Technology
  | journal = Distributed Systems Online
  | volume = 
  | issue = 
  | issn = 0018-9162
  | pages = 40–46
  | publisher = [[IEEE]]
  | location = 
  | date = December 2001
  | url = http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=00970558
  | doi = 
  | id = 
  | accessdate = | first2 = Christian }}
&lt;/ref&gt; For example, for some vendors, a HOLAP database will use relational tables to hold the larger quantities of detailed data, and use specialized storage for at least some aspects of the smaller quantities of more-aggregate or less-detailed data. HOLAP addresses the shortcomings of [[#Multidimensional_OLAP_.28MOLAP.29|MOLAP]] and [[#Relational_OLAP_.28ROLAP.29|ROLAP]] by combining the capabilities of both approaches. HOLAP tools can utilize both pre-calculated cubes and relational data sources.

==== Vertical partitioning ====

In this mode HOLAP stores ''aggregations'' in [[#Multidimensional_OLAP_.28MOLAP.29|MOLAP]] for fast query performance, and detailed data in [[#Relational_OLAP_.28ROLAP.29|ROLAP]] to optimize time of cube ''processing''.

==== Horizontal partitioning ====

In this mode HOLAP stores some slice of data, usually the more recent one (i.e. sliced by Time dimension) in [[#Multidimensional_OLAP_.28MOLAP.29|MOLAP]] for fast query performance, and older data in [[#Relational_OLAP_.28ROLAP.29|ROLAP]]. Moreover, we can store some dices in [[#Multidimensional_OLAP_.28MOLAP.29|MOLAP]] and others in [[#Relational_OLAP_.28ROLAP.29|ROLAP]], leveraging the fact that in a large cuboid, there will be dense and sparse subregions.&lt;ref&gt;Owen Kaser and Daniel Lemire, [http://arxiv.org/abs/cs.DB/0702143 Attribute Value Reordering for Efficient Hybrid OLAP], Information Sciences, Volume 176, Issue 16, pages 2279-2438, 2006.&lt;/ref&gt;

==== Products ====
The first product to provide HOLAP storage was [[Holos]], but the technology also became available in other commercial products such as [[Microsoft Analysis Services]], [[Oracle OLAP|Oracle Database OLAP Option]], [[MicroStrategy]] and [[SAP AG]] BI Accelerator. The hybrid OLAP approach combines ROLAP and MOLAP technology, benefiting from the greater scalability of ROLAP and the faster computation of MOLAP. For example, a HOLAP server may allow large volumes of detail data to be stored in a relational database, while aggregations are kept in a separate MOLAP store. The Microsoft SQL Server 7.0 OLAP Services supports a hybrid OLAP server

===Comparison===
Each type has certain benefits, although there is disagreement about the specifics of the benefits between providers.

* Some MOLAP implementations are prone to database explosion, a phenomenon causing vast amounts of storage space to be used by MOLAP databases when certain common conditions are met: high number of dimensions, pre-calculated results and sparse multidimensional data.
* MOLAP generally delivers better performance due to specialized indexing and storage optimizations. MOLAP also needs less storage space compared to ROLAP because the specialized storage typically includes [[Data compression|compression]] techniques.&lt;ref name="ieee_cite"/&gt;
* ROLAP is generally more scalable.&lt;ref name="ieee_cite"/&gt; However, large volume pre-processing is difficult to implement efficiently so it is frequently skipped.  ROLAP query performance can therefore suffer tremendously.
* Since ROLAP relies more on the database to perform calculations, it has more limitations in the specialized functions it can use.
* HOLAP encompasses a range of solutions that attempt to mix the best of ROLAP and MOLAP.  It can generally pre-process swiftly, scale well, and offer good function support.

===Other types===
The following acronyms are also sometimes used, although they are not as widespread as the ones above:

* '''WOLAP''' – Web-based OLAP
* '''DOLAP''' – [[Desktop computer|Desktop]] OLAP
* '''[[Rtolap|RTOLAP]]''' – Real-Time OLAP

==APIs and query languages==
Unlike [[relational databases]], which had SQL as the standard query language, and widespread [[Application programming interface|API]]s such as [[ODBC]], [[JDBC]] and [[OLEDB]], there was no such unification in the OLAP world for a long time. The first real standard API was [[OLE DB for OLAP]] specification from [[Microsoft]] which appeared in 1997 and introduced the [[Multidimensional Expressions|MDX]] query language. Several OLAP vendors – both server and client – adopted it. In 2001 Microsoft and [[Hyperion Solutions Corporation|Hyperion]] announced the [[XML for Analysis]] specification, which was endorsed by most of the OLAP vendors. Since this also used MDX as a query language, MDX became the de facto standard.&lt;ref&gt;{{cite web|url=http://www.olapreport.com/Comment_APIs.htm |title=Commentary: OLAP API wars |publisher=OLAP Report |author=Nigel Pendse |date=2007-08-23 |accessdate=2008-03-18 |deadurl=yes |archiveurl=https://web.archive.org/web/20080528220113/http://www.olapreport.com/Comment_APIs.htm |archivedate=May 28, 2008 }}&lt;/ref&gt;
Since September-2011 [[LINQ]] can be used to query [[Microsoft Analysis Services|SSAS]] OLAP cubes from Microsoft .NET.&lt;ref&gt;{{cite web|url=http://www.agiledesignllc.com/Products|title=SSAS Entity Framework Provider for LINQ to SSAS OLAP}}&lt;/ref&gt;

==Products==

===History===
The first product that performed OLAP queries was ''Express,'' which was released in 1970 (and acquired by [[Oracle Corporation|Oracle]] in 1995 from Information Resources).&lt;ref&gt;{{cite web|title=The origins of today's OLAP products |url=http://olapreport.com/origins.htm |publisher=OLAP Report |date=2007-08-23 |author=Nigel Pendse |accessdate=November 27, 2007 |deadurl=yes |archiveurl=https://web.archive.org/web/20071221044811/http://www.olapreport.com/origins.htm |archivedate=December 21, 2007 }}&lt;/ref&gt; However, the term did not appear until 1993 when it was coined by [[Edgar F. Codd]], who has been described as "the father of the relational database". Codd's paper&lt;ref name=Codd1993/&gt; resulted from a short consulting assignment which Codd undertook for former Arbor Software (later [[Hyperion Solutions]], and in 2007 acquired by Oracle), as a sort of marketing coup.  The company had released its own OLAP product, ''[[Essbase]]'', a year earlier. As a result, Codd's "twelve laws of online analytical processing" were explicit in their reference to Essbase. There was some ensuing controversy and when Computerworld learned that Codd was paid by Arbor, it retracted the article.
OLAP market experienced strong growth in late 90s with dozens of commercial products going into market. In 1998, Microsoft released its first OLAP Server – [[Microsoft Analysis Services]], which drove wide adoption of OLAP technology and moved it into mainstream.

===Product comparison===
{{Main|Comparison of OLAP Servers}}

===OLAP Clients===
OLAP clients include many spreadsheet programs like Excel, web application, sql,dashboard tools, etc.

===Market structure===
Below is a list of top OLAP vendors in 2006, with figures in millions of [[US Dollar]]s.&lt;ref&gt;{{cite web
  |url=http://www.olapreport.com/market.htm
  |title=OLAP Market
  |publisher=OLAP Report
  |author=Nigel Pendse
  |year=2006

  |accessdate=2008-03-17
}}&lt;/ref&gt;
{| class="wikitable sortable"
|- bgcolor="#CCCCCC" align="center"
! Vendor !! Global Revenue  !! Consolidated company
|-
| [[Microsoft Corporation]] || 1,806   || Microsoft
|-
| [[Hyperion Solutions Corporation]] || 1,077  || Oracle
|-
| [[Cognos]] || 735  || IBM
|-
| [[Business Objects (company)|Business Objects]] || 416 || SAP
|-
| [[MicroStrategy]] || 416 || MicroStrategy
|-
| [[SAP AG]] || 330 || SAP
|-
| Cartesis ([[SAP AG|SAP]]) || 210  || SAP
|-
| [[Applix]] || 205  || IBM
|-
| [[Infor]] || 199  || Infor
|-
| [[Oracle Corporation]] || 159 || Oracle
|-
| Others || 152  || Others
|-
| '''Total''' || '''5,700'''
|}

=== Open-source ===
* [[Druid (open-source data store)]] is a popular [[open-source]] distributed data store for OLAP queries that is used at scale in production by various organizations.
* [[Apache Kylin]] is a distributed data store for OLAP queries originally developed by eBay.
* [[Cubes (OLAP server)]] is another light-weight [[open-source]] toolkit implementation of OLAP functionality in the [[Python (programming language)|Python programming language]] with built-in ROLAP.
* [[Linkedin Pinot]] is used at LinkedIn to deliver scalable real time analytics with low latency.&lt;ref&gt;{{cite news |last= Yegulalp |first=Serdar |date=2015-06-11 |title= LinkedIn fills another SQL-on-Hadoop niche |url=http://www.infoworld.com/article/2934506/olap/linkedins-pinot-fills-another-sql-on-hadoop-niche.html |magazine=InfoWorld |access-date=2016-11-19}}&lt;/ref&gt; It can ingest data from offline data sources (such as Hadoop and flat files) as well as online sources (such as Kafka). Pinot is designed to scale horizontally.

== See also ==
{{portal|Computer science}}
* [[Comparison of OLAP Servers]]
* [[Data warehouse]]
* [[Online transaction processing]] (OLTP)
* [[Business analytics]]
* [[Predictive analytics]]
* [[Data Mining]]
* [[Thomsen Diagrams]]
* [[Functional Database Model]]

==Bibliography==
* {{cite web
  |url= http://www.daniel-lemire.com/OLAP/
  |title= Data Warehousing and OLAP-A Research-Oriented Bibliography
  |author= Daniel Lemire
  |date= December 2007
  }}

* {{cite book
  | title = OLAP Solutions: Building Multidimensional Information Systems, 2nd Edition
  | publisher = John Wiley &amp; Sons
  | series =
  | year = 1997
  | isbn = 978-0-471-14931-6
  | author = Erik Thomsen. }}

* Ling Liu and Tamer M. Özsu (Eds.) (2009).  "[http://www.springer.com/computer/database+management+&amp;+information+retrieval/book/978-0-387-49616-0 Encyclopedia of Database Systems], 4100 p.&amp;nbsp;60 illus. ISBN 978-0-387-49616-0.
* O'Brien, J. A., &amp; Marakas, G. M. (2009). Management information systems (9th ed.). Boston, MA: McGraw-Hill/Irwin.

==References==
{{Reflist|30em|refs=
&lt;ref name=ahsan&gt;
{{cite journal
|last1=Abdullah
|first1=Ahsan
|title=Analysis of mealybug incidence on the cotton crop using ADSS-OLAP (Online Analytical Processing) tool
|journal=Computers and Electronics in Agriculture |date=November 2009 |volume=69 |issue=1 |pages=59–72 |doi=10.1016/j.compag.2009.07.003
}}
&lt;/ref&gt;
}}

{{Data warehouse}}

{{Authority control}}

{{DEFAULTSORT:Online Analytical Processing}}
[[Category:Online analytical processing| ]]
[[Category:Data management]]
[[Category:Information technology management]]</text>
      <sha1>67ic4aaez06hwtkm53r0tes3cfz4jn7</sha1>
    </revision>
  </page>
  <page>
    <title>Managed Memory Computing</title>
    <ns>0</ns>
    <id>40119072</id>
    <revision>
      <id>737441080</id>
      <parentid>737258320</parentid>
      <timestamp>2016-09-02T20:11:57Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <comment>removed parent category of [[Category:Business intelligence]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1956" xml:space="preserve">{{multiple issues|
{{Orphan|date=November 2013}}
{{unreferenced|date=November 2013}}
}}

== This article pertains to a new technology used in Business Intelligence. Managed Memory Computing uses aggregated data for in-memory analytics   ==

Aggregated data cubes are the most effective form of storage of aggregated or summarized data for quick analysis. This technology is driven by [[Online Analytical Processing|Online Analytical Processing technology]]. Utilizing these data cubes involves intense disk I/O operations. This at times lowers the speed for users of data.

Conventional, [[In-Memory Processing|in-memory processing]] does not rely on stored and summarized or aggregated data but brings all the relevant data to the memory. This technology then utilizes intense processing and large amounts of memory to perform all calculations and aggregations while in memory.

Managed Memory Computing blends the best of both methods, allowing users to define data cubes with per-structured and aggregated data, providing a logical business layer to users, and offering in-memory computation. These features make the response time for user interactions far superior and enable the most balanced approach between disk I/O and in-memory processing.

The hybrid approach of Managed Memory Computing provides analysis, dashboards, graphical interaction, ad hoc querying, presentation, and discussion driven analytic at blazing speeds, making the [[Business intelligence|Business Intelligence Tool]] ready for everything from an interactive session in the boardroom to a [[production planning]] meeting on the factory floor.

== References ==

[http://www.cioreview.in/magazine/ElegantJ-BI-Managed-Memory-Computing--Business-Intelligence-Redefined-CZSI499492332.html Introduction of Managed Memory Computing in CIOReview]

[[Category:Business intelligence]]
[[Category:Financial data analysis]]
[[Category:Data management]]
[[Category:Computer architecture]]</text>
      <sha1>13ljlz53hpih04dcof8eqxdh6y76vq3</sha1>
    </revision>
  </page>
  <page>
    <title>Query Rewriting</title>
    <ns>0</ns>
    <id>43435003</id>
    <revision>
      <id>621044622</id>
      <parentid>619196384</parentid>
      <timestamp>2014-08-13T11:10:14Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor />
      <comment>[[WP:CHECKWIKI]] error fixes, added [[CAT:O|orphan]] tag using [[Project:AWB|AWB]] (10369)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1516" xml:space="preserve">{{Orphan|date=August 2014}}

'''Query Rewriting''' is a technique used in mediation based [[data integration]] systems for translating the queries formulated over the mediated schema to a query over the various sources by making use of the view definitions.&lt;ref name="refone"&gt;{{cite conference | author=[[Alon Y. Halevy]] | title=Answering queries using views: A survey | booktitle=The VLDB Journal | year=2001 | pages=270–294}}&lt;/ref&gt; Mediation based data integration system hides from the end user the underlying heterogeneity of the various data providing sources linked to it by providing a uniform query interface in the form of a mediated schema. This schema is also referred to as the global schema whereas the schema of the various data sources is collectively referred to as the local schema. The local schema and the mediated schema are mapped to each other using view definitions. The queries formulated on the mediated schema cannot be directly used to query the sources. Therefore query rewriting translates such a query formulated over the global schema to a query over the various data sources. Examples include bucket algorithm, Minicon algorithm, inverse rules algorithm.&lt;ref name="refone"/&gt; This rewritten query is then evaluated to obtain the query response making use of the data obtained by querying the data sources.

==See also==
* [[Data integration]]
* [[Schema Matching]]
* [[Data Virtualization]]

==References==
&lt;references/&gt;

{{DEFAULTSORT:Query Rewriting}}
[[Category:Data management]]</text>
      <sha1>me48d1rrh0z50zrn74bi0g1ks1xa8s9</sha1>
    </revision>
  </page>
  <page>
    <title>NCSA Brown Dog</title>
    <ns>0</ns>
    <id>43444201</id>
    <revision>
      <id>731682627</id>
      <parentid>698306369</parentid>
      <timestamp>2016-07-26T21:15:11Z</timestamp>
      <contributor>
        <username>Sbrad77</username>
        <id>28839370</id>
      </contributor>
      <minor />
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11863" xml:space="preserve">
'''NCSA Brown Dog''' is a research project to develop a method for easily accessing historic research data stored in order to maintain the long-term viability of large bodies of scientific research. It is supported by the [[National Center for Supercomputing Applications]] (NCSA) that is funded by the [[National Science Foundation]] (NSF).&lt;ref name=bdweb&gt;{{cite web|title=Brown Dog|url=http://browndog.ncsa.illinois.edu|website=NCSA Brown Dog|accessdate=31 July 2014}}&lt;/ref&gt; 

==History==
Brown Dog is part of the [[Datanet|DataNet]] partners program funded by NSF in 2008. DataNet was conceived to address the increasingly digital and data-intensive nature of science, engineering and education. Brown Dog is part of a follow-on effort called [[Data Infrastructure Building Blocks (DIBBs)]], focused on building software to support DataNet. The project was proposed by researchers at NCSA and the [[University of Illinois Urbana-Champaign]] as well as researchers from [[Boston University]] and the [[University of North Carolina at Chapel Hill]].

==Unstructured, uncurated, long tail data==
Much scientific data is smaller, [[Unstructured data|unstructured]] and uncurated and thus not easily shared. Such data is sometimes referred to as "long tail" data. This borrows a term from statistics and refers to the tail of the distribution of project sizes. The majority of smaller projects lack the resources to properly steward the data they produce. This so-called “long tail” data, both past and present, has the potential to inform future research in many study areas. Much of this data has become inaccessible due to obsolete software and file formats. The resulting impossibility of reviewing data from older research disrupts the overall scientific research project.&lt;ref&gt;{{cite web|title=DataUp—Data Curation for the Long Tail of Science|url=http://blogs.msdn.com/b/msr_er/archive/2012/10/02/dataup-data-curation-for-the-long-tail-of-science.aspx|website=Microsoft Research Connections Blog|publisher=Microsoft Research Connections Team|accessdate=7 August 2014}}&lt;/ref&gt;

==Approach==
Brown Dog describes itself as the “super mutt” of software&lt;ref&gt;{{cite web|last1=Woodie|first1=Alex|title=NCSA Project Aims to Create a DNS-Like Service for Data|url=http://www.datanami.com/2014/01/06/ncsa_project_aims_to_create_a_dns-like_service_for_data/|website=datanami|accessdate=7 August 2014}}&lt;/ref&gt; (thus the name “Brown Dog”), serving as a low-level data infrastructure to interface digital data content across the internet.  Its approach is to use every possible source of automated help (i.e., software) in existence in a robust and provenance-preserving manner to create a service that can deal with as much of this data as possible.&lt;ref&gt;{{cite web|last1=Pletz|first1=John|title=U of I researchers get millions for 'super mutt' to sniff out big-data trends|url=http://www.chicagobusiness.com/article/20131202/blogs11/131129794/u-of-i-researchers-get-millions-for-super-mutt-to-sniff-out-big-data-trends|website=Chicago Business|publisher=Crain Communications, Inc.|accessdate=7 August 2014}}&lt;/ref&gt; The project sees the broader impact of its work in its potential to serve the general public as a sort of “DNS for data”, with the goal of making all data and all file formats as accessible as webpages are today.

==Technology==
Brown Dog seeks to address problems involving the use of uncurated and unstructured data collections through the development of two services: the Data Access Proxy (DAP) to aid in the conversion of file formats and the Data Tilling Services (DTS) for the automatic extraction of metadata from file contents. Once developed, researchers and general public users will be able to download browser plugins and other tools from the Brown Dog tool catalog.&lt;ref name="bdweb" /&gt;&lt;ref&gt;{{cite web|last1=Jewett|first1=Barbara|title=DATA SET FREE|url=http://www.ncsa.illinois.edu/news/stories/KentonMcHenry/|website=NCSA Access Magazine|publisher=NCSA|accessdate=7 August 2014}}&lt;/ref&gt;

===Data Tilling Service===
Data Tilling Service (DTS) will allow users to search data collections using an existing file to discover other similar files in a collection. A DTS search field will be appended to configured browsers where example files can be dropped. This tells DTS to search all the files under a given [[URL]] for files similar to the dropped file. For example, while browsing an online image collection, a user could drop an image of three people into the search field, and the DTS would return all images in the collection that also contain three people. If DTS encounters a foreign file format, it will utilize DAP to make the file accessible. DTS also indexes the data and extract and appends metadata to files and collections enabling users to gain some sense of the type of data they are encountering.

This service runs on port 9443.

===Data Access Proxy===
Data Access Proxy (DAP) allows users to access data files that would otherwise be unreadable. Similar to an internet gateway or [[Domain Name System|Domain Name Service]], the DAP configuration would be entered into a user’s machine and browser settings. Data requests over [[HTTP]] would first be examined by DAP to determine if the native file format is readable on the client device. If not, DAP converts the file into the best available format readable by the client machine.  Alternatively, the user could specify the desired format themselves.

This service runs on port 8184.

==Use cases==
Brown Dog targets three [[use cases]] proposed by groups within the [http://earthcube.org EarthCube] research communities. Developers and researchers from these communities will work together on use cases that span [[geoscience]], [[engineering]], [[biology]] and [[social science]].

===Long tail vegetation data in ecology and global change biology===
This use case is led by [http://people.bu.edu/dietze/ '''Michael Dietze'''], [http://www.bu.edu/ Boston University]

&lt;blockquote&gt;Data on the abundance, species composition, and size structure of vegetation is critically important for a wide array of sub-disciplines in ecology, conservation, natural resource management, and global change biology. However, addressing many of the pressing questions in these disciplines will require that terrestrial biosphere and hydrologic models are able to assimilate the large amount of long-tail data that exists but is largely inaccessible. The Brown Dog team in cooperation with researches from Dietze's lab will facilitate the capture of a huge body of smaller research-oriented vegetation data sets collected over many decades and historical vegetation data embedded in Public Land Survey data dating back to 1785. This data will be used as initial conditions for models, to make sense of other large data sets and for model calibration and validation.&lt;ref name=bdweb /&gt;&lt;ref name=newswise&gt;{{cite web|title=BU Scientist, Collaborators Get $10.5 Million Grant to Develop Software for un-Curated Data|url=http://www.newswise.com/articles/bu-scientist-collaborators-get-10-5-million-grant-to-develop-software-for-un-curated-data|website=www.newswise.com|publisher=Boston University College of Arts and Sciences|accessdate=7 August 2014}}&lt;/ref&gt;&lt;/blockquote&gt;

===Designing green infrastructure considering storm water and human requirements===
This use case is led by [http://eisa.ncsa.illinois.edu/ Barbara Minsker], [http://illinois.edu University of Illinois at Urbana-Champaign];  [http://willsull.net/research/ William Sullivan], University of Illinois at Urbana-Champaign; [http://cee.illinois.edu/faculty/arthurschmidt Arthur Schmidt], University of Illinois at Urbana-Champaign

&lt;blockquote&gt;This case study involves developing novel green infrastructure design criteria and models that integrate requirements for storm water management and ecosystem and human health and well being. To address the scientific and social problems associated with the design of green spaces, data accessibility and availability is a major challenge.  This study will focus on identified areas of the Green Healthy Neighborhood Planning region within the City of Chicago where existing local sewer performance is most deficient and where changes in impervious area through green infrastructure would be beneficial to under served neighborhoods. Brown Dog will be used to extract long-tail experimental data on human landscape preferences and health impacts. This data will be used to develop a human health impacts model that will then be linked together with a terrestrial biosphere model and a storm water model using Brown Dog technology.&lt;ref name=bdweb /&gt;&lt;/blockquote&gt;

===Development and application for critical zone studies===
This use case is led by [http://hydrocomplexity.net/index.html Praveen Kumar], University of Illinois at Urbana-Champaign

&lt;blockquote&gt;[[Earth's Critical Zone|Critical Zone]] (CZ) is the “skin” of the earth that extends from the treetops to the bedrock that is created by life processes working at scales from microbes to biomes. The Critical Zone supports all terrestrial living systems. Its upper part is the bio-mantle. This is where terrestrial biota live, reproduce, use and expend energy, and where their wastes and remains accumulate and decompose. It encompasses the soil, which acts as a geomembrane through which water and solutes, energy, gases, solids, and organisms interact with the atmosphere, biosphere, hydrosphere, and lithosphere. A variety of drivers affect this bio-dynamic zone, ranging from climate and deforestation to agriculture, grazing and human development. Understanding and predicting these effects is central to managing and sustaining vital ecosystem services such as soil fertility, water purification, and production of food resources, and, at larger scales, global carbon cycling and carbon sequestration.
The CZ provides a unifying framework for integrating terrestrial surface and near-surface environments, and reflects an intricate web of biological and chemical processes and human impacts occurring at vastly different temporal and spatial scales. The nature of these data create significant challenges for inter-disciplinary studies of the CZ because integration of the variety and number of data products and models has been a barrier. On the other hand, CZ data provides an excellent opportunity for defining, testing and implementing Brown Dog technologies. In this context “unstructured” data is viewed broadly as consisting of a collection of heterogeneous data with formats that reflect temporal and disciplinary legacies, data from emerging low cost open hardware based sensors and embedded sensor networks that lack well defined metadata and sensor characteristics, as well as data that are available as maps, images and text.&lt;ref name=bdweb /&gt;&lt;/blockquote&gt;

==NSF Award==
CIF21 DIBBs: Brown Dog was awarded in the winter of 2013 with a start date of October 1, 2013. Estimated expiration date is September 30, 2018.&lt;ref&gt;{{cite web|title=Award#1261582 - CIF21 DIBBs: Brown Dog|url=http://www.nsf.gov/awardsearch/showAward?AWD_ID=1261582&amp;HistoricalAwards=false|website=nsf.gov|accessdate=31 July 2014}}&lt;/ref&gt;

The award amount was $10,519,716.00, the largest DIBB award. The principal investigator is Kenton McHenry of NCSA at the University of Illinois at Urbana-Champaign. Coleaders are Jong Lee NCSA/UIUC; Barbara Minsker, Civil and Environmental Engineering, University of Illinois at Urbana-Champaign; Praveen Kumar, Civil and Environmental Engineering, University of Illinois at Urbana-Champaign; Michael Dietze, Department of Earth and Environment, Boston University.

==References==
{{reflist|30em}}

==External links==
* {{official website|http://browndog.ncsa.illinois.edu}}

[[Category:Data management]]
[[Category:National Science Foundation]]
[[Category:Research projects]]</text>
      <sha1>rsom5tobksrht6c33gpez3hmoo7hpku</sha1>
    </revision>
  </page>
  <page>
    <title>Rtolap</title>
    <ns>0</ns>
    <id>2878165</id>
    <revision>
      <id>735451509</id>
      <parentid>634627779</parentid>
      <timestamp>2016-08-20T20:24:48Z</timestamp>
      <contributor>
        <username>W Nowicki</username>
        <id>9177019</id>
      </contributor>
      <comment>remove uncited promotion</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2014" xml:space="preserve">{{multiple issues|
{{Unreferenced|date=October 2008}}
{{Original research|date=October 2008}}
}}

==RTOLAP - Real Time OLAP==

Whilst many [[OLAP]] Servers like [[Microsoft Analysis Services]] store pre-calculating consolidations and calculated elements to achieve rapid response times. A Real Time OLAP Server will calculate the values on the fly, when they are required. 
The essential characteristic of RTOLAP system is in holding all the data in RAM.

It is a protocol which analyzes fly values when required. It saves every bit of information in RAM. The calculations are executed in a “right-away” manner which reduces the setback linked with “information outburst” since it only saves information under the RAM size standard.

== Advantages ==
* Since precalculated values aren't stored, the size of a cube in an RTOLAP system is smaller than of an OLAP product which resorts to precalculation. RTOLAP often reduces the problem which may be associated with "Data explosion", by  means of storing less data. 
* RTOLAP essentially performs calculations "just-in-time" by only calculating values when they are needed space can be saved, since in a precalculated system, a great deal of calculations will be stored which may well never be called up.
* Incremental updates are available once they are loaded, and any modifications to data will flow through the system immediately. With RTOLAP when a change is made, everyone sees the result. This isn't a unique characteristic of RTOLAP, since other OLAP systems (e.g. [[SAS Institute]], [[Microsoft Analysis Services]], [[MicroStrategy]]) behave the same way.

== Disadvantages ==
* Since RTOLAP stores the entire cube in RAM, it doesn't scale to the data volumes larger than the RAM size
* Performance of queries can be slower since the values need to be calculated on the fly instead of being accessed from the precalculated storage

[[Category:Data management]]
[[Category:Information technology management]]
[[Category:Online analytical processing]]</text>
      <sha1>aa6miwaxvdmsypkaoh8k4thu2yc0jqa</sha1>
    </revision>
  </page>
  <page>
    <title>Relational data stream management system</title>
    <ns>0</ns>
    <id>44046965</id>
    <revision>
      <id>715081210</id>
      <parentid>636904926</parentid>
      <timestamp>2016-04-13T16:04:02Z</timestamp>
      <contributor>
        <username>Dewritech</username>
        <id>11498870</id>
      </contributor>
      <comment>/* RDSMS SQL Query Examples */clean up, [[WP:AWB/T|typo(s) fixed]]: one second → one-second (2) using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3224" xml:space="preserve">A '''relational data stream management system (RDSMS)''' is a distributed, in-memory [[data stream management system]] (DSMS) that is designed to use standards-compliant [[SQL]] queries to process unstructured and structured data streams in real-time. Unlike [[SQL]] queries executed in a traditional [[RDBMS]], which return a result and exit, SQL queries executed in a RDSMS do not exit, generating results continuously as new data become available. Continuous SQL queries in a RDSMS use the [[SQL]] Window function to analyze, join and aggregate data streams over fixed or sliding windows. Windows can be specified as time-based or row-based.

== RDSMS SQL Query Examples ==

Continuous [[SQL]] queries in a RDSMS conform to the [[ANSI]] [[SQL]] standards. The most common RDSMS SQL query is performed with the declarative &lt;code&gt;SELECT&lt;/code&gt; statement. A continuous SQL &lt;code&gt;SELECT&lt;/code&gt; operates on data across one or more data streams, with optional keywords and clauses that include &lt;code&gt;FROM&lt;/code&gt; with an optional &lt;code&gt;JOIN&lt;/code&gt; subclause to specify the rules for joining multiple data streams, the &lt;code&gt;WHERE&lt;/code&gt; clause and comparison predicate to restrict the records returned by the query, &lt;code&gt;GROUP BY&lt;/code&gt; to project streams with common values into a smaller set, &lt;code&gt;HAVING&lt;/code&gt; to filter records resulting from a &lt;code&gt;GROUP BY&lt;/code&gt;, and &lt;code&gt;ORDER BY&lt;/code&gt; to sort the results.

The following is an example of a continuous data stream aggregation using a &lt;code&gt;SELECT&lt;/code&gt; query that aggregates a sensor stream from a weather monitoring station. The &lt;code&gt;SELECT&lt;/code&gt;query aggregates the minimum, maximum and average temperature values over a one-second time period, returning a continuous stream of aggregated results at one second intervals.
 
&lt;source lang="sql"&gt;
SELECT STREAM
    FLOOR(WEATHERSTREAM.ROWTIME to SECOND) AS FLOOR_SECOND,
    MIN(TEMP) AS MIN_TEMP,
    MAX(TEMP) AS MAX_TEMP,
    AVG(TEMP) AS AVG_TEMP
FROM WEATHERSTREAM
GROUP BY FLOOR(WEATHERSTREAM.ROWTIME TO SECOND);
&lt;/source&gt;

RDSMS SQL queries also operate on data streams over time or row-based windows. The following example shows a second continuous SQL query using the &lt;code&gt;WINDOW&lt;/code&gt; clause with a one-second duration. The &lt;code&gt;WINDOW&lt;/code&gt; clause changes the behavior of the query, to output a result for each new record as it arrives. Hence the output is a stream of incrementally updated results with zero result latency.

&lt;source lang="sql"&gt;
SELECT STREAM
    ROWTIME,
    MIN(TEMP) OVER W1 AS WMIN_TEMP,
    MAX(TEMP) OVER W1 AS WMAX_TEMP,
    AVG(TEMP) OVER W1 AS WAVG_TEMP
FROM WEATHERSTREAM
WINDOW W1 AS ( RANGE INTERVAL '1' SECOND PRECEDING );
&lt;/source&gt;

== See also ==
* [[SQL]]
* [[NoSQL]]
* [[NewSQL]]

== External links ==
* [http://www.sqlstream.com/stream-processing/ Stream processing with SQL]
* [http://researcher.watson.ibm.com/researcher/view_group.php?id=2531 IBM System S]
* [http://www.mcjones.org/System_R/SQL_Reunion_95/sqlr95.html ''1995 SQL Reunion: People, Projects, and Politics'', by Paul McJones (ed.)]: transcript of a reunion meeting devoted to the personal history of relational databases, SQL System R.

[[Category:Data management]]
[[Category:Relational model]]</text>
      <sha1>eszymn8xnzpfp24x0qvrd3t2n6i5lym</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Object storage</title>
    <ns>14</ns>
    <id>44628227</id>
    <revision>
      <id>637037146</id>
      <timestamp>2014-12-07T16:30:32Z</timestamp>
      <contributor>
        <username>Leoinspace</username>
        <id>14273534</id>
      </contributor>
      <comment>Making Object storage a sub-category of Data management</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="28" xml:space="preserve">[[Category:Data management]]</text>
      <sha1>tpzfnyay41jj855s8j16rxxvd2yg34n</sha1>
    </revision>
  </page>
  <page>
    <title>Semantic query</title>
    <ns>0</ns>
    <id>44626050</id>
    <revision>
      <id>759242474</id>
      <parentid>749670137</parentid>
      <timestamp>2017-01-10T01:20:33Z</timestamp>
      <contributor>
        <ip>128.189.133.101</ip>
      </contributor>
      <comment>/* Background */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8031" xml:space="preserve">'''Semantic queries''' allow for queries and analytics of associative and contextual nature. Semantic queries enable the retrieval of both explicitly and implicitly derived information based on syntactic, semantic and structural information contained in data. They are designed to deliver precise results (possibly the distinctive selection of one single piece of information) or to answer more fuzzy and wide open questions through [[pattern matching]] and [[Reasoning system|digital reasoning]].

Semantic queries work on [[named graphs]], [[Linked Data|linked-data]] or [[Semantic triple|triples]]. This enables the query to process the actual relationships between information and ''infer'' the answers from the ''network of data''. This is in contrast to [[semantic search]], which uses [[semantics]] (the science of meaning) in [[Unstructured data|unstructured text]] to produce a better search result (see [[natural language processing]]).

From a technical point of view semantic queries are precise relational-type operations much like a [[SQL|database query]]. They work on structured data and therefore have the possibility to utilize comprehensive features like operators (e.g. &gt;, &lt; and =), namespaces, [[pattern matching]], [[Type inheritance|subclassing]], [[transitive relation]]s, [[Semantic Web Rule Language|semantic rules]] and contextual [[Full-text index|full text search]]. The [[semantic web]] technology stack of the [[W3C]] is offering [[SPARQL]]&lt;ref name="XML.com"&gt;{{cite web|url=http://www.xml.com/pub/a/2005/11/16/introducing-sparql-querying-semantic-web-tutorial.html |title=Introducing SPARQL: Querying the Semantic Web |publisher=XML.com|date=2005}}&lt;/ref&gt;&lt;ref name="W3C"&gt;{{cite web|url=http://www.w3.org/TR/rdf-sparql-query |title=SPARQL Query Language for RDF |publisher=W3C|date=2008}}&lt;/ref&gt; to formulate semantic queries in a syntax similar to [[SQL]]. Semantic queries are used in [[triplestore]]s, [[graph databases]], [[semantic wiki]]s, natural language and artificial intelligence systems.

== Background ==

[[Relational database]]s contain all relationships between data in an ''implicit'' manner only.&lt;ref name="ACM-DL"&gt;{{cite web|url=http://portal.acm.org/citation.cfm?id=1646157 |title=Semantic queries in databases: problems and challenges |publisher=ACM Digital Library|date=2009}}&lt;/ref&gt;&lt;ref name="ESWC"&gt;{{cite web|url=http://2012.eswc-conferences.org/sites/default/files/eswc2012_submission_357.pdf |title=Karma: A System for Mapping Structured Sources into the Semantic Web |publisher=eswc-conferences.org|date=2012}}&lt;/ref&gt; For example, the relationships between customers and products (stored in two content-tables and connected with an additional link-table) only come into existence in a query statement ([[SQL]] in the case of relational databases) written by a developer. Writing the query demands exact knowledge of the [[database schema]].&lt;ref name="IEEE"&gt;{{cite web|url=http://www-scf.usc.edu/~taheriya/papers/taheriyan14-icsc-paper.pdf |title=A Scalable Approach to Learn Semantic Models of Structured Sources |publisher=8th IEEE International Conference on Semantic Computing|date=2014}}&lt;/ref&gt;&lt;ref name="AAAI"&gt;{{cite web|url=http://www.isi.edu/integration/papers/knoblock13-sbd.pdf |title=Semantics for Big Data Integration and Analysis |publisher=AAAI Fall Symposium on Semantics for Big Data|date=2013}}&lt;/ref&gt;

[[Linked Data|Linked-Data]] contain all relationships between data in an ''explicit'' manner. In the above example no query code needs to be written. The correct product for each customer can be fetched automatically. Whereas this simple example is trivial, the real power of linked-data comes into play when a ''network of information'' is created (customers with their geo-spatial information like city, state and country; products with their categories within sub- and super-categories). Now the system can automatically answer more complex queries and analytics that look for the connection of a particular location with a product category. The development effort for this query is omitted. Executing a semantic query is conducted by ''walking'' the network of information and finding matches (also called ''Data Graph Traversal'').

Another important aspect of semantic queries is that the type of the relationship can be used to incorporate intelligence into the system. The relationship between a customer and a product has a fundamentally different nature than the relationship between a neighbourhood and its city. The latter enables the semantic query engine to ''infer'' that a customer ''living in Manhattan is also living in New York City'' whereas other relationships might have more complicated patterns and "contextual analytics". This process is called inference or reasoning and is the ability of the software to derive new information based on given facts.

== Articles ==

* {{Cite web
| last =  Velez
| first = Golda
| year = 2008
| url = http://www.wallstreetandtech.com/data-management/showArticle.jhtml?articleID=208700210&amp;pgno=2
| title = Semantics Help Wall Street Cope With Data Overload
| publisher = wallstreetandtech.com
}}
* {{Cite web
| last =  Zhifeng
| first = Xiao
| year = 2009
| url = http://adsabs.harvard.edu/abs/2009SPIE.7492E..60X
| title = Spatial information semantic query based on SPARQL
| publisher = International Symposium on Spatial Analysis
}}
* {{Cite web
| last = Aquin
| first = Mathieu
| year = 2010
| url = http://www.semantic-web-journal.net/sites/default/files/swj96_1.pdf
| title = Watson, more than a Semantic Web search engine
| publisher = Semantic Web Journal
}}
* {{Cite web
| last =  Prudhommeaux
| first = Eric
| year = 2010
| url = http://www.cambridgesemantics.com/semantic-university/sparql-vs-sql-intro
| title = SPARQL vs. SQL - Introduction
| publisher = Cambridge Semantics
}}
* {{Cite web
| last = Dworetzky
| first = Tom
| year = 2011
| url = http://www.ibtimes.com/how-siri-works-iphones-brain-comes-natural-language-processing-stanford-professors-teach-free-online
| title = How Siri Works: iPhone's 'Brain' Comes from Natural Language Processing
| publisher = International Business Times
}}
* {{Cite web
| last =  Horwitt
| first = Elisabeth
| year = 2011
| url = http://www.computerworld.com/s/article/9209118/The_semantic_Web_gets_down_to_businessarticleID=208700210&amp;pgno=2
| title = The semantic Web gets down to business
| publisher = computerworld.com
}}
* {{Cite web
| last = Rodriguez
| first = Marko
| year = 2011
| url = http://markorodriguez.com/2011/06/15/graph-pattern-matching-with-gremlin-1-1/
| title = Graph Pattern Matching with Gremlin
| publisher = markorodriguez.com on Graph Computing
}}
* {{Cite web
| last = Sequeda
| first = Juan
| year = 2011
| url = http://www.cambridgesemantics.com/semantic-university/sparql-nuts-and-bolts
| title = SPARQL Nuts &amp; Bolts
| publisher = Cambridge Semantics
}}
* {{Cite web
| last = Freitas
| first = Andre
| year = 2012
| url = https://www.deri.ie/sites/default/files/publications/freitas_ic_12.pdf
| title = Querying Heterogeneous Datasets on the Linked Data Web
| publisher = IEEE Internet Computing
}}
* {{Cite web
| last = Kauppinen
| first = Tomi
| year = 2012
| url = http://linkedscience.org/tools/sparql-package-for-r/tutorial-on-sparql-package-for-r/
| title = Using the SPARQL Package in R to handle Spatial Linked Data
| publisher = linkedscience.org
}}
* {{Cite web
| last = Lorentz
| first = Alissa
| year = 2013
| url = http://www.wired.com/2013/04/with-big-data-context-is-a-big-issue/
| title = With Big Data, Context is a Big Issue
| publisher = Wired
}}

== See also ==

* [[Dataspaces]]
* [[Knowledge Representation]]
* [[Linked Data]]
* [[Ontology alignment]]
* [[Semantic Integration]]
* [[Semantic publishing]]
* [[Semantics of Business Vocabulary and Business Rules]]
* [[SPARQL]]

== References ==
{{reflist}}

==External links==
* [http://www.w3.org/standards/semanticweb/query W3C Semantic Web Standards - Query]

[[Category:Data management]]
[[Category:Query languages]]
[[Category:Semantic Web]]</text>
      <sha1>t6iq6sloydr16gbc294tbgjrz3zygfu</sha1>
    </revision>
  </page>
  <page>
    <title>Data lineage</title>
    <ns>0</ns>
    <id>44783487</id>
    <revision>
      <id>756078193</id>
      <parentid>730386840</parentid>
      <timestamp>2016-12-21T21:37:33Z</timestamp>
      <contributor>
        <ip>74.118.24.163</ip>
      </contributor>
      <comment>casing on the second word in the sentence: The</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="38929" xml:space="preserve">{{peacock|date=May 2015}}

'''Data lineage''' is defined as a data life cycle that includes the data's origins and where it moves over time.&lt;ref&gt;http://www.techopedia.com/definition/28040/data-lineage&lt;/ref&gt; It describes what happens to data as it goes through diverse processes. It helps provide visibility into the analytics pipeline and simplifies tracing errors back to their sources. It also enables replaying specific portions or inputs of the dataflow for step-wise debugging or regenerating lost output. In fact, database systems have used such information, called data provenance, to address similar validation and debugging challenges already.&lt;ref name="DeSoumyarupa"&gt;De, Soumyarupa. (2012). Newt : an architecture for lineage based replay and debugging in DISC systems. UC San Diego: b7355202. Retrieved from: https://escholarship.org/uc/item/3170p7zn&lt;/ref&gt;

'''Data Lineage''' provides a visual representation to discover the data flow/movement from its source to destination via various changes and hops on its way in the enterprise environment. 
''Data lineage'' represents: how the data hops between various data points, how the data gets transformed along the way, how the representation and parameters change, and how the data splits or converges after each hop. Easier representation of the ''Data Lineage'' can be shown with dots and lines, where dot represents a data container for data point(s) and lines connecting them represents the transformation(s) the data point under goes, between the data containers.

&lt;!-- Deleted image removed: [[File:DataLineage Dots lines.png]] --&gt;

Representation of ''Data Lineage'' broadly depends on scope of the ''[[Meta-data management|Metadata Management]]'' and reference point of interest. ''Data Lineage'' provides sources of the data and intermediate data flow hops from the reference point with '''Backward data lineage''', leads to the final destination's data points and its intermediate data flows with '''Forward data lineage'''.  These views can be combined with '''End to End Lineage''' for a reference point that provides complete audit trail of that data point of interest from source(s) to its final destination(s). As the data points or hops increases, the complexity of such representation becomes incomprehensible. Thus, the best feature of the data lineage view would be to be able to simplify the view by temporarily ''Masking'' unwanted peripheral data points. Tools that have the '''masking''' feature enables scalability of the view and enhances analysis with best user experience for both Technical and business users alike.

'''Scope of the data lineage''' determines the volume of metadata required to represent its data lineage. Usually, [[Data governance|Data Governance]], and [[Data management|Data Management]] determines the scope of the data lineage based on their [[regulation]]s, ''enterprise data management strategy'', ''data impact'', ''reporting attributes'', and ''critical [[data element]]s'' of the organization.

''Data Lineage'' provides the audit trail of the data points at the lowest granular level,but presentation of the lineage may be done at various zoom levels to simplify the vast information, similar to the ''analytic web maps''. ''Data Lineage'' can be visualized at various levels based on the granularity of the view. At a very high level ''data lineage'' provides what systems the data interacts before it reaches destination. As the granularity increases it goes up to the data point level where it can provide the details of the data point and its historical behavior, attribute properties, and trends and ''[[Data quality|Data Quality]]'' of the data passed through that specific data point in the ''data lineage''.

''[[Data governance|Data Governance]]'' plays a key role in metadata management for guidelines, strategies, policies, implementation. ''[[Data quality|Data Quality]]'', and ''[[Master data management|Master Data Management]]'' helps in enriching the data lineage with more business value. Even though the final representation of ''Data lineage'' is provided in one interface but the way the metadata is harvested and exposed to the data lineage '''[[Graphical user interface|User Interface (UI)]]''' could be entirely different. Thus, ''Data lineage'' can be broadly divided into three categories based on the way metadata is harvested:Data lineage involving ''software packages for structured data'', ''Programming Languages'', and ''Big Data''.

''Data lineage'' expects to view at least the technical metadata involving the data points and its various transformations. Along with technical data, ''Data Lineage'' may enrich the metadata with their corresponding Data Quality results,Reference Data values, [[Data model|Data Models]], [[Glossary of business and management terms|Business Vocabulary]], [[Data steward|People]], [[Program management|Programs]], and [[Enterprise system|Systems]] linked to the data points and transformations. Masking feature in the data lineage visualization allows the tools to incorporate all the enrichments that matter for the specific use case.  
Metadata normalization may be done in data lineage to represent disparate systems into one common view.

'''Data provenance''' documents the inputs, entities, systems, and processes that influence data of interest, in effect providing a historical record of the data and its origins. The generated evidence supports essential forensic activities such as data-dependency analysis, error/compromise detection and recovery, and auditing and compliance analysis. "'''Lineage''' is a simple type of '''why provenance'''."&lt;ref name="DeSoumyarupa"/&gt;

==Case for Data Lineage==
The world of [[big data]] is changing dramatically right before our eyes. Statistics say that Ninety percent (90%) of the world’s data has been created in the last two years alone.&lt;ref&gt;http://newstex.com/2014/07/12/thedataexplosionin2014minutebyminuteinfographic/&lt;/ref&gt; This explosion of data has resulted in the ever-growing number of systems and automation at all levels in all sizes of organizations.

Today, distributed systems like Google [[Map Reduce]],&lt;ref&gt;Jeffrey Dean and Sanjay Ghemawat. Mapreduce: simplified data processing on
large clusters. Commun. ACM, 51(1):107–113, January 2008.&lt;/ref&gt; Microsoft Dryad,&lt;ref&gt;Michael Isard, Mihai Budiu, Yuan Yu, Andrew Birrell, and Dennis Fetterly.
Dryad: distributed data-parallel programs from sequential building blocks. In Proceedings of the 2nd ACM SIGOPS/EuroSys European Conference onComputer
Systems 2007, EuroSys ’07, pages 59–72, New York, NY, USA, 2007. ACM.&lt;/ref&gt; Apache Hadoop &lt;ref&gt;Apache Hadoop. http://hadoop.apache.org.&lt;/ref&gt;(an open-source project) and Google Pregel&lt;ref&gt;Grzegorz Malewicz, Matthew H. Austern, Aart J.C Bik, James C. Dehnert, Ilan Horn, Naty Leiser, and Grzegorz Czajkowski. Pregel: a system for largescale graph processing. In Proceedings of the 2010 international conference on Managementof data, SIGMOD ’10, pages 135–146, New York, NY, USA, 2010. ACM.&lt;/ref&gt; provide such platforms for businesses and users. However, even with these systems, [[big data]] analytics can take several hours, days or weeks to run, simply due to the data volumes involved. For example, a ratings prediction algorithm for the Netflix Prize challenge took nearly 20 hours to execute on 50 cores, and a large-scale image processing task to estimate geographic information took 3 days to complete using 400 cores.&lt;ref&gt;Shimin Chen and Steven W. Schlosser. Map-reduce meets wider varieties of
applications. Technical report, Intel Research, 2008.&lt;/ref&gt; "The Large Synoptic Survey Telescope is expected to generate terabytes of data every night and eventually store more than 50 petabytes, while in the bioinformatics sector, the largest genome 12 sequencing houses in the world now store petabytes of data apiece."&lt;ref&gt;The data deluge in genomics. https://www-304.ibm.com/connections/blogs/ibmhealthcare/entry/data overload in genomics3?lang=de, 2010.&lt;/ref&gt;
Due to the humongous size of the [[big data]], there could be features in the data that are not considered in the machine learning algorithm, possibly even outliers. It is very difficult for a data scientist to trace an unknown or an unanticipated result.

===Big Data Debugging===

[[Big data]] analytics is the process of examining large data sets to uncover hidden patterns, unknown correlations, market trends, customer preferences and other useful business information. They apply machine learning algorithms etc. to the data which transform the data. Due to the humongous size of the data, there could be unknown features in the data, possibly even outliers. It is pretty difficult for a data scientist to actually debug an unexpected result.

The massive scale and unstructured nature of data, the complexity of these analytics pipelines, and long runtimes pose significant manageability and debugging challenges. Even a single error in these analytics can be extremely difficult to identify and remove. While one may debug them by re-running the entire analytics through a debugger for step-wise debugging, this can be expensive due to the amount of time and resources needed. Auditing and data validation are other major problems due to the growing ease of access to relevant data sources for use in experiments, sharing of data between scientific communities and use of third-party data in business enterprises.&lt;ref&gt;Yogesh L. Simmhan, Beth Plale, and Dennis Gannon. A survey of data prove-
nance in e-science. SIGMOD Rec., 34(3):31–36, September 2005.&lt;/ref&gt;&lt;ref name="IanFosterJensVockler"&gt;Ian Foster, Jens Vockler, Michael Wilde, and Yong Zhao. Chimera: A Virtual Data System for Representing, Querying, and Automating Data Derivation. In 14th International Conference on Scientific and Statistical Database Management, July 2002.&lt;/ref&gt;&lt;ref name="Benjamim&amp;Luiz"&gt;Benjamin H. Sigelman, Luiz Andr Barroso, Mike Burrows, Pat Stephenson, Manoj Plakal, Donald Beaver, Saul Jaspan, and Chandan Shanbhag. Dapper, a large-scale distributed systems tracing infrastructure. Technical report, Google Inc, 2010.&lt;/ref&gt;&lt;ref name="PeterBuneman"&gt;Peter Buneman, Sanjeev Khanna, and Wang Chiew Tan. Data provenance: Some basic issues. In Proceedings of the 20th Conference on Foundations of SoftwareTechnology and Theoretical Computer Science, FST TCS 2000, pages 87–93, London, UK, UK, 2000. Springer-Verlag&lt;/ref&gt; These problems will only become larger and more acute as these systems and data continue to grow. As such, more cost-efficient ways of analyzing [[data-intensive computing|data intensive scalable computing]] (DISC) are crucial to their continued effective use.

===Challenges in [[Big Data]] Debugging===

====Massive Scale====
According to an EMC/IDC study:&lt;ref&gt;http://www.emc.com/about/news/press/2012/20121211-01.htm&lt;/ref&gt;
* 2.8ZB of data were created and replicated in 2012,
* the digital universe will double every two years between now and 2020, and
* there will be approximately 5.2TB of data for every man, woman and child on earth in 2020.
Working with this scale of data has become very challenging.

====Unstructured Data====
The phrase [[unstructured data]] usually refers to information that doesn't reside in a traditional row-column database. Unstructured data files often include text and multimedia content. Examples include e-mail messages, word processing documents, videos, photos, audio files, presentations, webpages and many other kinds of business documents. Note that while these sorts of files may have an internal structure, they are still considered "unstructured" because the data they contain doesn't fit neatly in a database.
Experts estimate that 80 to 90 percent of the data in any organization is unstructured. And the amount of unstructured data in enterprises is growing significantly often many times faster than structured databases are growing. "[[Big data]] can include both structured and unstructured data, but IDC estimates that 90 percent of [[big data]] is unstructured data."&lt;ref&gt;Webopedia http://www.webopedia.com/TERM/U/unstructured_data.html&lt;/ref&gt;

====Long Runtime====
In today’s hyper competitive business environment, companies not only have to find and analyze the relevant data they need, they must find it quickly. The challenge is going through the sheer volumes of data and accessing the level of detail needed, all at a high speed. The challenge only grows as the degree of granularity increases. One possible solution is hardware. Some vendors are using increased memory and powerful parallel processing to crunch large volumes of data extremely quickly. Another method is putting data in-memory but using a grid computing approach, where many machines are used to solve a problem. Both approaches allow organizations to explore huge data volumes. Even this level of sophisticated hardware and software, few of the image processing tasks in large scale take a few days to few weeks.&lt;ref&gt;SAS. http://www.sas.com/resources/asset/five-big-data-challenges-article.pdf&lt;/ref&gt; Debugging of the data processing is extremely hard due to long run times.

====Complex Platform====
[[Big Data]] platforms have a very complicated structure. Data is distributed among several machines. Typically the jobs are mapped into several machines and results are later combined by reduce operations. Debugging of a [[big data]] pipeline becomes very challenging because of the very nature of the system. It will not be an easy task for the data scientist to figure out which machine's data has the outliers and unknown features causing a particular algorithm to give unexpected results.

====Proposed Solution====
Data provenance or data lineage can be used to make the debugging of [[big data]] pipeline easier. This necessitates the collection of data about data transformations. The below section will explain data provenance in more detail.

==Data Provenance==
Data Provenance provides a historical record of the data and its origins. The provenance of data which is generated by complex transformations such as workflows is of considerable value to scientists. From it, one can ascertain the quality of the data based on its ancestral data and derivations, track back sources of errors, allow automated re-enactment of derivations to update a data, and provide attribution of data sources. Provenance is also essential to the business domain where it can be used to drill down to the source of data in a data warehouse, track the creation of intellectual property, and provide an audit trail for regulatory purposes.

The use of data provenance is proposed in distributed systems to trace records through a dataflow, replay the dataflow on a subset of its original inputs and debug data flows. To do so, one needs to keep track of the set of inputs to each operator, which were used to derive each of its outputs. Although there are several forms of provenance, such as copy-provenance and how-provenance,&lt;ref name="PeterBuneman" /&gt;&lt;ref&gt;Robert Ikeda and Jennifer Widom. Data lineage: A survey. Technical report, Stanford University, 2009.&lt;/ref&gt; the information we need is a simple form of '''why-provenance, or lineage''', as defined by Cui et al.&lt;ref name="YCui"&gt;Y. Cui and J. Widom. Lineage tracing for general data warehouse transformations. VLDB Journal, 12(1), 2003.&lt;/ref&gt;

==Lineage Capture==
Intuitively, for an operator T producing output o, lineage consists of triplets of form {I, T, o}, where I is the set of inputs to T used to derive o. Capturing lineage for each operator T in a dataflow enables users to ask questions such as “Which outputs were produced by an input i on operator T ?” and “Which inputs produced output o in operator T ?”&lt;ref name="DeSoumyarupa"/&gt; A query that finds the inputs deriving an output is called a backward tracing query, while one that finds the outputs produced by an input is called a forward tracing query.&lt;ref name="RobertIkedaHyunjung"&gt;Robert Ikeda, Hyunjung Park, and Jennifer Widom. Provenance for generalized map and reduce workflows. In Proc. of CIDR, January 2011.&lt;/ref&gt; Backward tracing is useful for debugging, while forward tracing is useful for tracking error propagation.&lt;ref name="RobertIkedaHyunjung" /&gt; Tracing queries also form the basis for replaying an original dataflow.&lt;ref name="IanFosterJensVockler" /&gt;&lt;ref name="YCui" /&gt;&lt;ref name="RobertIkedaHyunjung" /&gt; However, to efficiently use lineage in a DISC system, we need to be able to capture lineage at multiple levels (or granularities) of operators and data, capture accurate lineage for DISC processing constructs and be able to trace through multiple dataflow stages efficiently.

DISC system consists of several levels of operators and data, and different use cases of lineage can dictate the level at which lineage needs to be captured.  Lineage can be captured at the level of the job, using files and giving lineage tuples of form {IF i, M RJob, OF i }, lineage can also be captured at the level of each task, using records and giving, for example, lineage tuples of form {(k rr, v rr ), map, (k m, v m )}. The first form of lineage is called coarse-grain lineage, while the second form is called fine-grain lineage. Integrating lineage across different granularities enables users to ask questions such as “Which file read by a MapReduce job produced this particular output record?” and can be useful in debugging across different operator and data granularities within a dataflow.&lt;ref name="DeSoumyarupa" /&gt;
[[File:Map Reduce Job -1.png|thumb|center|500px|Map Reduce Job showing containment relationships]]

To capture end-to-end lineage in a DISC system, we use the Ibis model,&lt;ref&gt;C. Olston and A. Das Sarma. Ibis: A provenance manager for multi-layer
systems. In Proc. of CIDR, January 2011.&lt;/ref&gt; which introduces the notion of containment hierarchies for operators and data. Specifically, Ibis proposes that an operator can be contained within another and such a relationship between two operators is called '''operator containment'''. "Operator containment implies that the contained (or child) operator performs a part of the logical operation of the containing (or parent) operator."&lt;ref name="DeSoumyarupa" /&gt; For example, a MapReduce task is contained in a job. Similar containment relationships exist for data as well, called data containment. Data containment implies that the contained data is a subset of the containing data (superset).
[[File:Containment Hierarchy.png|thumb|center|500px|Containment Hierarchy]]

==Prescriptive Data Lineage==

The concept of '''Prescriptive Data Lineage''' combines both the logical model (entity) of how that data should flow with the actual lineage for that instance.&lt;ref&gt;http://info.hortonworks.com/rs/549-QAL-086/images/Hadoop-Governance-White-Paper.pdf&lt;/ref&gt;

Data lineage and provenance typically refers to the way or the steps a dataset came to its current state Data lineage, as well as all copies or derivatives. However, simply looking back at only audit or log correlations to determine lineage from a forensic point of view is flawed for certain data management cases.  For instance, it is impossible to determine with certainty if the route a data workflow took was correct or in compliance without the logic model.

Only by combining the a logical model with atomic forensic events can proper activities be validated:
#Authorized copies, joins, or CTAS operations
#Mapping of processing to the systems that those process are run on
#Ad-Hoc versus established processing sequences

Many certified compliance reports require provenance of data flow as well as the end state data for a specific instance. With these types of situations, any deviation from the prescribed path need to be accounted for and potentially remediated.&lt;ref&gt;[https://www.sec.gov/info/smallbus/secg/bd-small-entity-compliance-guide.htm SEC Small Entity Compliance Guide]&lt;/ref&gt;   This is marks a shift in thinking from purely a look back model  to a framework which is better suited to capture compliance workflows.

==Active vs Lazy Lineage==
Lazy lineage collection typically captures only coarse-grain lineage at run time. These systems incur low capture overheads due to the small amount of lineage they capture. However, to answer fine-grain tracing queries, they must replay the data flow on all (or a large part) of its input and collect fine-grain lineage during the replay. This approach is suitable for forensic systems, where a user wants to debug an observed bad output.

Active collection systems capture entire lineage of the data flow at run time. The kind of lineage they capture may be coarse-grain or fine-grain, but they do
not require any further computations on the data flow after its execution. Active fine-grain lineage collection systems incur higher capture overheads than lazy collection systems. However, they enable sophisticated replay and debugging.&lt;ref name="DeSoumyarupa" /&gt;

==Actors==
An actor is an entity that transforms data; it may be a Dryad vertex, individual map and reduce operators, a MapReduce job, or an entire dataflow pipeline. Actors act as black-boxes and the inputs and outputs of an actor are tapped to capture lineage in the form of associations, where an association is a triplet {i, T, o} that relates an input i with an output o for an actor T . The instrumentation thus captures lineage in a dataflow one actor at a time, piecing it into a set of associations for each actor. The system developer needs to capture the data an actor reads (from other actors) and the data an actor writes (to other actors). For example, a developer can treat the Hadoop Job Tracker as an actor by recording the set of files read and written by each job.
&lt;ref name="mainPaper"&gt;Dionysios Logothetis, Soumyarupa De, and Kenneth Yocum. 2013. Scalable lineage capture for debugging DISC analytics. In Proceedings of the 4th annual Symposium on Cloud Computing (SOCC '13). ACM, New York, NY, USA, , Article 17 , 15 pages.&lt;/ref&gt;

==Associations==
Association is a combination of the inputs, outputs and the operation itself. The operation is represented in terms of a black box also known as the actor. The associations describe the transformations that are applied on the data. The associations are stored in the association tables. Each unique actor is represented by its own association table. An association itself looks like {i, T, o} where i is the set of inputs to the actor T and o is set of outputs given produced by the actor. Associations are the basic units of Data Lineage. Individual associations are later clubbed together to construct the entire history of transformations that were applied to the data.&lt;ref name="DeSoumyarupa"/&gt;

==Architecture==
[[Big data]] systems scale horizontally i.e. increase capacity by adding new hardware or software entities into the distributed system. The distributed system acts as a single entity in the logical level even though it comprises multiple hardware and software entities. The system should continue to maintain this property after horizontal scaling. An important advantage of horizontal scalability is that it can provide the ability to increase capacity on the fly. The biggest plus point is that horizontal scaling can be done using commodity hardware.

The horizontal scaling feature of [[Big Data]] systems should be taken into account while creating the architecture of lineage store. This is essential because the lineage store itself should also be able to scale in parallel with the [[Big data]] system. The number of associations and amount of storage required to store lineage will increase with the increase in size and capacity of the system. The architecture of [[Big data]] systems makes the use of a single lineage store not appropriate and impossible to scale. The immediate solution to this problem is to distribute the lineage store itself.&lt;ref name="DeSoumyarupa"/&gt;

The best case scenario is to use a local lineage store for every machine in the distributed system network. This allows the lineage store also to scale horizontally. In this design, the lineage of data transformations applied to the data on a particular machine is stored on the local lineage store of that specific machine. The lineage store typically stores association tables. Each actor is represented by its own association table. The rows are the associations themselves and columns represent inputs and outputs. This design solves 2 problems. It allows horizontal scaling of the lineage store. If a single centralized lineage store was used, then this information had to be carried over the network, which would cause additional network latency. The network latency is also avoided by the use of a distributed lineage store.&lt;ref name="mainPaper"/&gt;

[[File:Selection 065.png|thumb|center|500px|Architecture of Lineage Systems]]

==Data flow Reconstruction==
The information stored in terms of associations needs to be combined by some means to get the data flow of a particular job. In a distributed system a job is broken down into multiple tasks. One or more instances run a particular task. The results produced on these individual machines are later combined together to finish the job. Tasks running on different machines perform multiple transformations on the data in the machine. All the transformations applied to the data on a machines is stored in the local lineage store of that machines. This information needs to be combined together to get the lineage of the entire job. The lineage of the entire job should help the data scientist understand the data flow of the job and he/she can use the data flow to debug the [[big data]] pipeline. The data flow is reconstructed in 3 stages.

===Association tables===
The first stage of the data flow reconstruction is the computation of the association tables. The association tables exists for each actor in each local lineage store. The entire association table for an actor can be computed by combining these individual association tables. This is generally done using a series of equality joins based on the actors themselves. In few scenarios the tables might also be joined using inputs as the key. Indexes can also be used to improve the efficiency of a join.The joined tables need to be stored on a single instance or a machine to further continue processing. There are multiple schemes that are used to pick a machine where a join would be computed. The easiest one being the one with minimum CPU load. Space constraints should also be kept in mind while picking the instance where join would happen.

===Association Graph===
The second step in data flow reconstruction is computing an association graph from the lineage information. The graph represents the steps in the data flow. The actors act as vertices and the associations act as edges. Each actor T is linked to its upstream and downstream actors in the data flow. An upstream actor of T is one that produced the input of T, while a downstream actor is one that consumes the output of T . Containment relationships are always considered while creating the links. The graph consists of three types of links or edges.

====Explicitly specified links====
The simplest link is an explicitly specified link between two actors. These links are explicitly specified in the code of a machine learning algorithm. When an actor is aware of its exact upstream or downstream actor, it can communicate this information to lineage API. This information is later used to link these actors during the tracing query. For example, in the [[MapReduce]] architecture, each map instance knows the exact record reader instance whose output it consumes.&lt;ref name="DeSoumyarupa"/&gt;

====Logically inferred links====
Developers can attach data flow [[archetypes]] to each logical actor. A data flow archetype explains how the children types of an actor type arrange themselves in a data flow. With the help of this information, one can infer a link between each actor of a source type and a destination type. For example, in the [[MapReduce]] architecture, the map actor type is the source for reduce, and vice versa. The system infers this from the data flow archetypes and duly links map instances with reduce instances. However, there may be several [[MapReduce]] jobs in the data flow, and linking all map instances with all reduce instances can create false links. To prevent this, such links are restricted to actor instances contained within a common actor instance of a containing (or parent) actor type. Thus, map and reduce instances are only linked to each other if they belong to the same job.&lt;ref name="DeSoumyarupa"/&gt;

====Implicit links through data set sharing====
In distributed systems, sometimes there are implicit links, which are not specified during execution. For example, an implicit link exists between an actor that wrote to a file and another actor that read from it. Such links connect actors which use a common data set for execution. The dataset is the output of the first actor and is the input of the actor following it.&lt;ref name="DeSoumyarupa"/&gt;

===Topological Sorting===
The final step in the data flow reconstruction is the [[Topological sorting]] of the association graph. The directed graph created in the previous step is topologically sorted to obtain the order in which the actors have modified the data. This inherit order of the actors defines the data flow of the big data pipeline or task.

==Tracing &amp; Replay==
This is the most crucial step in [[Big Data]] debugging. The  captured lineage is combined and processed to obtain the data flow of the pipeline. The data flow helps the data  scientist or a developer to look deeply into the actors and their transformations. This step allows the data scientist to figure out the part of the algorithm that is generating the unexpected output. A [[big data]] pipeline can go wrong in 2 broad ways. The first is a presence of a suspicious actor in the data-flow. The second being the existence of outliers in the data.

The first case can be debugged by tracing the data-flow. By using lineage and data-flow information together a data scientist can figure out how the inputs are converted into outputs. During the process actors that behave unexpectedly can be caught. Either these actors can be removed from the data flow or they can be augmented by new actors to change the data-flow. The improved data-flow can be replayed to test the validity of it. Debugging faulty actors include recursively performing coarse-grain replay on actors in the data-flow,&lt;ref&gt;Wenchao Zhou, Qiong Fei, Arjun Narayan, Andreas Haeberlen, Boon Thau Loo, and Micah Sherr. Secure network provenance. In Proceedings of 23rd ACM Symposium on Operating System Principles (SOSP), December 2011.&lt;/ref&gt; which can be expensive in resources for long dataflows. Another approach is to manually inspect lineage logs to find anomalies,&lt;ref name="Benjamim&amp;Luiz" /&gt;&lt;ref&gt;Rodrigo Fonseca, George Porter, Randy H. Katz, Scott Shenker, and Ion Stoica. X-trace: A pervasive network tracing framework. In In Proceedings of NSDI’07, 2007.&lt;/ref&gt; which can be tedious and time-consuming across several stages of a data-flow. Furthermore, these approaches work only when the data scientist can discover bad outputs. To debug analytics without known bad outputs, the data scientist need to analyze the data-flow for suspicious behavior in general. However, often, a user may not know the expected normal behavior and cannot specify predicates. This section describes a debugging methodology for retrospectively analyzing lineage to identify faulty actors in a multi-stage data-flow. We believe that sudden changes in an actor’s behavior, such as its average selectivity, processing rate or output size, is characteristic of an anomaly. Lineage can reflect such changes in actor behavior over time and across different actor instances. Thus, mining lineage to identify such changes can be useful in debugging faulty actors in a data-flow.
[[File:Tracing Anomalous Actors.png|thumb|center|400px|Tracing Anomalous Actors]]

The second problem i.e. the existence of outliers can also be identified by running the data-flow step wise and looking at the transformed outputs. The data scientist finds a subset of outputs that are not in accordance to the rest of outputs. The inputs which are causing these bad outputs are the outliers in the data. This problem can be solved by removing the set of outliers from the data and replaying the entire data-flow. It can also be solved by modifying the machine learning algorithm by adding, removing or moving actors in the data-flow. The changes in the data-flow are successful if the replayed data-flow does not produce bad outputs.
[[File:Tracing Outliers in the data.png|thumb|center|400px|Tracing Outliers in the data]]

==Challenges==
Even though use data lineage is a novel way of debugging of [[big data]] pipelines, the process is not simple. The challenges are scalability of lineage store, fault tolerance of the lineage store, accurate capture of lineage for black box operators and many others. These challenges must be considered carefully and trade offs between them need to be evaluated to make a realistic design for data lineage capture.

===Scalability===
DISC systems are primarily batch processing systems designed for high throughput. They execute several jobs per analytics, with several tasks per job. The overall number of operators executing at any time in a cluster can range from hundreds to thousands depending on the cluster size. Lineage capture for
these systems must be able scale to both large volumes of data and numerous operators to avoid being a bottleneck for the DISC analytics.

===Fault tolerance===
Lineage capture systems must also be fault tolerant to avoid rerunning data flows to capture lineage. At the same time, they must also accommodate failures in the DISC system. To do so, they must be able to identify a failed DISC task and avoid storing duplicate copies of lineage between the partial lineage generated by the failed task and duplicate lineage produced by the restarted task. A lineage system should also be able to gracefully handle multiple instances of local lineage systems going down. This can achieved by storing replicas of lineage associations in multiple machines. The replica can act like a backup in the event of the real copy being lost.

===Black-box operators===
Lineage systems for DISC dataflows must be able to capture accurate lineage across black-box operators to enable fine-grain debugging. Current approaches to this include Prober, which seeks to find the minimal set of inputs that can produce a specified output for a black-box operator by replaying the data-flow several times to deduce the minimal set,&lt;ref&gt;Anish Das Sarma, Alpa Jain, and Philip Bohannon. PROBER: Ad-Hoc Debugging of Extraction and Integration Pipelines. Technical report, Yahoo, April 2010.&lt;/ref&gt; and dynamic slicing, as used by Zhang et al.&lt;ref&gt;Mingwu Zhang, Xiangyu Zhang, Xiang Zhang, and Sunil Prabhakar. Tracing lineage beyond relational operators. In Proc. Conference on Very Large Data Bases (VLDB), September 2007.&lt;/ref&gt; to capture lineage for [[NoSQL]] operators through binary rewriting to compute dynamic slices. Although producing highly accurate lineage, such techniques can incur significant time overheads for capture or tracing, and it may be preferable to instead trade some accuracy for better performance. Thus, there is a need for a lineage collection system for DISC dataflows that can capture lineage from arbitrary operators with reasonable accuracy, and without significant overheads in capture or tracing.

===Efficient tracing===
Tracing is essential for debugging, during which, a user can issue multiple tracing queries. Thus, it is important that tracing has fast turnaround times. Ikeda et al.&lt;ref name="RobertIkedaHyunjung" /&gt; can perform efficient backward tracing queries for MapReduce dataflows, but are not generic to different DISC systems and do not perform efficient forward queries. Lipstick,&lt;ref&gt;Yael Amsterdamer, Susan B. Davidson, Daniel Deutch, Tova Milo, and Julia Stoyanovich. Putting lipstick on a pig: Enabling database-style workflow provenance. In Proc. of VLDB, August 2011.&lt;/ref&gt; a lineage system for Pig,&lt;ref&gt;Christopher Olston, Benjamin Reed, Utkarsh Srivastava, Ravi Kumar, and Andrew Tomkins. Pig latin: A not-so-foreign language for data processing. In Proc. of ACM SIGMOD, Vancouver, Canada, June 2008.&lt;/ref&gt; while able to perform both backward and forward tracing, is specific to Pig and SQL operators and can only perform coarse-grain tracing for black-box operators. Thus, there is a need for a lineage system that enables efficient forward and backward tracing for generic DISC systems and dataflows with black-box operators.

===Sophisticated replay===
Replaying only specific inputs or portions of a data-flow is crucial for efficient debugging and simulating what-if scenarios. Ikeda et al. present a methodology for lineage-based refresh, which selectively replays updated inputs to recompute affected outputs.&lt;ref&gt;Robert Ikeda, Semih Salihoglu, and Jennifer Widom. Provenance-based refresh in data-oriented workflows. In Proceedings of the 20th ACM international conference on Information and knowledge management, CIKM ’11, pages 1659–1668, New York, NY, USA, 2011. ACM.&lt;/ref&gt; This is useful during debugging for re-computing outputs when a bad input has been fixed. However, sometimes a user may want to remove the bad input and replay the lineage of outputs previously affected by the error to produce error-free outputs. We call this exclusive replay. Another use of replay in debugging involves replaying bad inputs for step-wise debugging (called selective replay). Current approaches to using lineage in DISC systems do not address these. Thus, there is a need for a lineage system that can perform both exclusive and selective replays to address different debugging needs.

===Anomaly detection===
One of the primary debugging concerns in DISC systems is identifying faulty operators. In long dataflows with several hundreds of operators or tasks, manual inspection can be tedious and prohibitive. Even if lineage is used to narrow the subset of operators to examine, the lineage of a single output can still span several operators. There is a need for an inexpensive automated debugging system, which can substantially narrow the set of potentially faulty operators, with reasonable accuracy, to minimize the amount of manual examination required.

==See also==
&lt;!-- please do not list specific implementations here --&gt;
* [[Provenance]]
* [[Big Data]]
* [[Topological Sorting]]
* [[Debugging]]
* [[NoSQL]]
* [[Scalability]]
* [[Directed acyclic graph]]

==References==
{{Reflist|33em}}

[[Category:Data management]]
[[Category:Distributed computing problems]]
[[Category:Big data]]</text>
      <sha1>tb0lw1nkqukwtx7unlwg9usuakcxbnd</sha1>
    </revision>
  </page>
  <page>
    <title>Multi-model database</title>
    <ns>0</ns>
    <id>44971098</id>
    <revision>
      <id>763089810</id>
      <parentid>762923464</parentid>
      <timestamp>2017-02-01T07:29:22Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor />
      <comment>/* Background */[[WP:CHECKWIKI]] error fix for #61.  Punctuation goes before References. Do [[Wikipedia:GENFIXES|general fixes]] if a problem exists. -</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8405" xml:space="preserve">Most database management systems are organized around a single [[Database model|data model]] that determines how data can be organized, stored, and manipulated. In contrast, a '''multi-model database''' is designed to support multiple data models against a single, integrated backend.&lt;ref name="neither"&gt;[http://blogs.the451group.com/information_management/2013/02/08/neither-fish-nor-fowl/ The 451 Group, "Neither Fish Nor Fowl: The Rise of Multi-Model Databases"]&lt;/ref&gt; Document, graph, relational, and key-value models are examples of data models that may be supported by a multi-model database.

== Background ==

The [[relational model|relational]] data model became popular after its publication by [[Edgar F. Codd]] in 1970. Due to increasing requirements for [[Scalability#Horizontal and vertical scaling|horizontal scalability]] and [[fault tolerance]], [[NoSQL]] databases became prominent after 2009. NoSQL databases use a variety of data models, with [[Document-oriented database|document]], [[Graph database|graph]], and key-value models being popular.&lt;ref name="rise"&gt;[http://www.infoworld.com/article/2861579/database/the-rise-of-the-multimodel-database.html Infoworld, "The Rise of the Multi-Model Database"]&lt;/ref&gt;

A Multi-model database is a database that can store, index and query data in more than one model. For some time, databases have primarily supported only one model, such as: [[Relational database]], [[Document-oriented database]], [[Graph database]] or [[Triplestore]]. A database that combines many of these is multi-model.

For some time, it was all but forgotten (or considered irrelevant) that there were any other database models besides Relational. The Relational model and notion of [[Third normal form]] were the de facto standard for all data storage. However, prior to the dominance of Relational data modeling from about 1980 to 2005 the [[Hierarchical database model]] was commonly used, and since 2000 or 2010, many [[NoSQL]] models that are non-relational including Documents, triples, key-value stores and graphs are popular. Arguably, geospatial data, temporal data and text data are also separate models, though indexed, queryable text data is generally termed a "[[search engine]]" rather than a database.

The first time the word "Multi-Model" has been associated to the databases was on May 30, 2012 in Cologne, Germany, during the Luca Garulli's key note "''NoSQL Adoption – What’s the Next Step?''".&lt;ref&gt;{{Cite journal|date=2012-06-01|title=Multi-Model storage 1/2 one product,|url=http://www.slideshare.net/lvca/no-sql-matters2012keynote/47-MultiModel_storage_12_one_product}}&lt;/ref&gt;&lt;ref&gt;{{Cite web|url=https://2012.nosql-matters.org/cgn/index.html?p=1202.html#luca_garulli_keynote|title=Nosql Matters Conference 2012 {{!}} NoSQL Matters CGN 2012|website=2012.nosql-matters.org|access-date=2017-01-12}}&lt;/ref&gt; Luca Garulli envisioned the evolution of the 1st generation NoSQL products into new products with more features able to be used by multiple use cases.

A Multi-model database is most directly a response to the "[[Polyglot Persistence]]" approach of knitting together multiple database products, each handing a different model, to achieve a multi-model capability as described by Martin Fowler.&lt;ref name="polyglot"&gt;[http://martinfowler.com/bliki/PolyglotPersistence.html Polyglot Persistence]&lt;/ref&gt; This strategy has two major disadvantages: it leads to a significant increase in operational complexity, and there is no support for maintaining data consistency across the separate data stores, so Multi-model databases have begun to fill in this gap.

Some stories of overcomplicated systems from un-necessary "frankenbeast" database integrations are found on the web.&lt;ref name="frankenbeast"&gt;[http://www.marklogic.com/blog/polyglot-persistence-done-right/ MarkLogic, "Avoiding the Frankenbeast"]&lt;/ref&gt;&lt;ref name="boring"&gt;[http://mcfunley.com/choose-boring-technology McKinley, "Choose Boring Technology"]&lt;/ref&gt;

Multi-model databases are intended to offer the data modeling advantages of [[Polyglot Persistence]],&lt;ref name="polyglot"/&gt; without its disadvantages. Operational complexity, in particular, is reduced through the use of a single data store.&lt;ref name="rise"/&gt;

== Databases ==
Multi-model databases include (in alphabetic order):
* [[ArangoDB]] - Document (JSON), Graph, Key-value
* [[CouchBase]] - Relational (SQL), Document
* [[CrateDB]] - Relational (SQL), Document (Lucene)
* [[MarkLogic]] - Document (XML and JSON), Graph (RDF with OWL/RDFS), text, geospatial, binary, SQL
* [[OrientDB]] - Document (JSON), Graph, Key-value, Text, Geospatial, Binary, SQL, Reactive

Note that the level of support for the various models varies widely, including the ability to query across models, fully index the internal structure of a model, transactional support, and optimization or query planning across models.
The first multi-model database was [[OrientDB]], created in 2010 as an answer to the fragmented NoSQL environment, with the goal of providing one product to replace multiple NoSQL databases.

== Architecture ==

The main difference between the available multi-model databases is related to their architectures. Multi-model databases can support different models either within the engine or via different layers on top of the engine. Some products may provide an engine which supports documents and graphs while others provide layers on top of a key-key store.&lt;ref&gt;[http://blog.foundationdb.com/7-things-that-make-google-f1-and-the-foundationdb-sql-layer-so-strikingly-similar, "layer"]&lt;/ref&gt; With a layered architecture, each data model is provided via its own [[Component-based software engineering|component]].

== User-defined data models ==

In addition to offering multiple data models in a single data store, some databases allow developers to easily define custom data models. This capability is enabled by ACID transactions with high performance and scalability. In order for a custom data model to support concurrent updates, the database must be able to synchronize updates across multiple keys. ACID transactions, if they are sufficiently performant,  allow such synchronization.&lt;ref name="multiple"&gt;[http://www.odbms.org/wp-content/uploads/2014/04/Multiple-Data-Models.pdf ODBMS, "Polyglot Persistence or Multiple Data Models?"]&lt;/ref&gt; JSON documents, graphs, and relational tables can all be implemented in a manner that inherits the horizontal scalability and fault-tolerance of the underlying data store.

== See also ==
&lt;!-- please do not list specific implementations here --&gt;
* [[Comparison of multi-model databases]]
* [[ACID]]
* [[NoSQL]]
* [[Comparison of structured storage software]]
* [[Database transaction]]
* [[Distributed database]]
* [[Distributed transaction]]
* [[Document-oriented database]]
* [[Graph database]]
* [[Relational model]]

== References ==
{{Reflist|2}}

== External links ==
* [http://www.orientechnologies.com/docs/last/orientdb.wiki/Tutorial-Document-and-graph-model.html OrientDB Document and Graph Model]
* [https://www.arangodb.com/key-features ArangoDB Key Features]
* [https://foundationdb.com/try/multi-model FoundationDB Multi-Model Architecture]
* [http://martinfowler.com/bliki/PolyglotPersistence.html Polyglot Persistence]
* [http://blogs.the451group.com/information_management/2013/02/08/neither-fish-nor-fowl/ The 451 Group, "Neither Fish Nor Fowl: The Rise of Multi-Model Databases"]
* [http://www.odbms.org/blog/2013/10/on-multi-model-databases-interview-with-martin-schonert-and-frank-celler/ ODBMS, "On Multi-Model Databases. Interview with Martin Schönert and Frank Celler."]
* [http://www.odbms.org/wp-content/uploads/2014/04/Multiple-Data-Models.pdf ODBMS, "Polyglot Persistence or Multiple Data Models?"]
* [http://www.infoworld.com/article/2861579/database/the-rise-of-the-multimodel-database.html Infoworld, "The Rise of the Multi-Model Database"]
* [https://crate.io/docs/reference/storage_consistency.html, Crate.IO Storage and Consistency]
* [http://www.marklogic.com/blog/tag/multi-model-database/, MarkLogic on Multi-model databases]

{{DEFAULTSORT:Multi-model Database}}
[[Category:Applications of distributed computing]]
[[Category:Databases]]
[[Category:Data management]]
[[Category:Distributed computing architecture]]
[[Category:Distributed data stores]]
[[Category:NoSQL]]
[[Category:Structured storage]]
[[Category:Transaction processing]]</text>
      <sha1>smn3zf81lfkiey4voug5gpdsmiayms5</sha1>
    </revision>
  </page>
  <page>
    <title>Media aggregation platform</title>
    <ns>0</ns>
    <id>46201390</id>
    <revision>
      <id>723519281</id>
      <parentid>711264718</parentid>
      <timestamp>2016-06-03T14:30:57Z</timestamp>
      <contributor>
        <username>Dcirovic</username>
        <id>11795905</id>
      </contributor>
      <minor />
      <comment>/* top */clean up using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2980" xml:space="preserve">A '''Media Aggregation Platform''' or '''Media Aggregation Portal''' (MAP) is an over-the-top service for distributing web-based streaming media content from multiple sources to a large audience. MAPs consist of networks of sources who host their own content which viewers can choose and access directly from a larger variety of content to choose from than a single source can offer.&lt;ref&gt;{{cite web | url=https://medium.com/@bmobley/over-the-top-of-ott-need-a-map-9931096775c2 | title=Over the Top of OTT… Need a MAP? | publisher=[[Medium.com]] | accessdate=23 March 2015}}&lt;/ref&gt;&lt;ref&gt;{{cite book | title=Building Next-Generation Converged Networks: Theory and Practice | publisher=CRC Press |author1=Al-Sakib Khan Pathan |author2=Muhammad Mostafa Monowar |author3=Zubair Md. Fadlullah | year=2013 | isbn=1466507616}}&lt;/ref&gt; The service is used by content providers, looking to extend the reach of their content.

Unlike multichannel video programming distributor ([[MVPD]]) or multiple-system operator (MSO), MAPs rely on the Internet rather than cables or satellite. As more network television channels have moved online in the early 21st century,&lt;ref&gt;{{cite web | url=http://www.exchange4media.com/59377_ott-platforms-to-be-key-growth-area-for-tv-broadcasters.html | title=OTT platforms to be key growth area for TV broadcasters | accessdate=23 March 2015}}&lt;/ref&gt; joining web-native channels like [[Netflix]], MAPs aggregates content the way that MSOs and MVPDs have used cable, and to a lesser extent satellite and IPTV infrastructure. There are companies that offer a similar service for free, including [[Yidio]] and TV.com, while others charge a subscription fee like as [[FreeCast Inc]]'s Rabbit TV Plus.&lt;ref&gt;{{cite web | url=http://rabbittvplus.com/ | title=FreeCast Inc Rabbit TV Plus | accessdate=23 March 2015}}&lt;/ref&gt; When compared with MSOs and MVPDs, MPAs network have much lower cost due to lack of physical infrastructure. The majority of revenues from their services is retained by the content creators and revenues are from advertisements, [[pay-per-view]], and subscription-based content offerings instead of by licensing and reselling content. MAPs service consumers directly with the content source and they purchase content directly from its source, without the markup added by a middleman.&lt;ref&gt;{{cite web | url=https://www.ncta.com/industry-data | title=NCTA Industry Data | accessdate=23 March 2015}}&lt;/ref&gt;

==See also==
* [[Multichannel video programming distributor|Multichannel Video Programming Distributor (MVPD)]]
* [[Multiple system operator|Multiple System Operator (MSO)]]
* [[Internet protocol television|Internet Protocol Television (IPTV)]]
* [[Over-the-top content|Over-the-top (OTT)]]
* [[Over-the-air television|Over-the-air (OTA)]]
* [[Video on demand|Video on demand (VOD)]] 
* [[Broadcast networks|Broadcast Networks]] 
* [[Internet Television]]
* [[Streaming Media]]
* [[Pay TV]]

==References==
{{reflist}}

[[Category:Data management]]</text>
      <sha1>548lw4ztr4kisy9wmjuxnr2rjp2mbyv</sha1>
    </revision>
  </page>
  <page>
    <title>Data architect</title>
    <ns>0</ns>
    <id>46362818</id>
    <revision>
      <id>741240349</id>
      <parentid>739031003</parentid>
      <timestamp>2016-09-26T07:40:00Z</timestamp>
      <contributor>
        <ip>2400:4020:89C7:2F00:5C4A:79A7:ADBF:EDDD</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4742" xml:space="preserve">{{expert-subject|Computer science|talk=Copyright issues and need for expert attention}}
A ''data architect'' is a practitioner of [[data architecture]], an information technology discipline concerned with designing, creating, deploying and managing an organization's data architecture. Data architects define how the data will be stored, consumed, integrated and managed by different data entities and IT systems, as well as any applications using or processing that data in some way.&lt;ref&gt;{{cite web|title=Definition of Data Architect|url=http://stage.web.techopedia.com/definition/29452/data-architect|website=Techopedia}}&lt;/ref&gt;  It is closely allied with [[business architecture]] and is considered to be one of the four domains of [[enterprise architecture]].

==Role==
According to the Data Management Body of Knowledge,&lt;ref&gt;{{cite web|title=Data Management Body of Knowledge|url=http://www.dama.org/content/body-knowledge|publisher=Data Management Association}}&lt;/ref&gt; the data architect “provides a standard common business vocabulary, expresses strategic data requirements, outlines high level integrated designs to meet these requirements, and aligns with enterprise strategy and related business architecture.”

According to the Open Group Architecture Framework (TOGAF), a data architect is expected to set data architecture principles, create models of data that enable the implementation of the intended business architecture, create diagrams showing key data entities, and create an inventory of the data needed to implement the architecture vision.&lt;ref name=TOGAF&gt;{{cite book|title=The Open Group Architectural Framework (TOGAF 9.1)|publisher=The Open Group|location=Chapter 10 - Data Architecture|url=http://pubs.opengroup.org/architecture/togaf9-doc/arch/chap10.html|accessdate=1 March 2015}}&lt;/ref&gt;

==Responsibilities==
# Organizes data at the macro level (i.e. which subject areas are managed in which goldensources) 
# Organizes data at the micro level, data models, for a new application. 
# Provides a logical data model as a standard for the goldensource and for consuming applications to inherit. 
# Provides a logical data model with elements and business rules needed for the creation of DQ rules.

==Skills==
Bob Lambert, Director of Data Architecture at consulting firm CapTech, describes the necessary skills of a Data Architect as follows:&lt;ref&gt;{{cite web|last1=Lambert|first1=Bob|title=Skills of a Data Architect|url=http://www.captechconsulting.com/blog/bob-lambert/skills-the-data-architect|website=Captech}}&lt;/ref&gt;

* Foundation in systems development: the data architect should understand the system development life cycle; software project management approaches; and requirements, design, and test techniques. The data architect is asked to conceptualize and influence application and interface projects, and therefore must understand what advice to give and where to plug in to steer toward desirable outcomes.
* Depth in data modeling and database design: This is the core skill of the data architect, and the most requested in data architect job descriptions. The effective data architect is sound across all phases of data modeling, from conceptualization to database optimization. In his experience this skill extends to SQL development and perhaps database administration.
* Breadth in established and emerging data technologies: In addition to depth in established data management and reporting technologies, the data architect is either experienced or conversant in emerging tools like columnar and NoSQL databases, predictive analytics, data visualization, and unstructured data. While not necessarily deep in all of these technologies, the data architect hopefully is experienced in one or more, and must understand them sufficiently to guide the organization in understanding and adopting them.
* Ability to conceive and portray the big data picture: When the data architect initiates, evaluates, and influences projects he or she does so from the perspective of the entire organization. The data architect maps the systems and interfaces used to manage data, sets standards for data management, analyzes current state and conceives desired future state, and conceives projects needed to close the gap between current state and future goals.
* Ability to astutely operate in the organization: Well respected and influential, Able to emphasize methodology, modeling, and governance, Technologically and politically neutral, Articulate, persuasive, and a good salesperson, and Enthusiastic

==References==
{{reflist}}

==See also==
* [[Data Architecture]]
* [[Information Architect]]
* [[Enterprise Architecture]]

[[Category:Data management]]
[[Category:Data modeling]]
[[Category:Data security]]</text>
      <sha1>1pynysiny7kdpvw2tywx6vkd2m0292f</sha1>
    </revision>
  </page>
  <page>
    <title>Data based decision making</title>
    <ns>0</ns>
    <id>46235825</id>
    <revision>
      <id>748811865</id>
      <parentid>699147434</parentid>
      <timestamp>2016-11-10T14:55:27Z</timestamp>
      <contributor>
        <username>CitationCleanerBot</username>
        <id>15270283</id>
      </contributor>
      <minor />
      <comment>/* General references */clean up, url redundant with jstor, and/or remove accessdate if no url using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8466" xml:space="preserve">{{more footnotes|date=May 2015}}

'''Data based decision making''' or data driven decision making refers to educator’s  ongoing process of collecting and analyzing different types of data, including demographic, student achievement test, satisfaction, process data to guide decisions towards improvement of educational process. DDDM becomes more important in education since federal and state test-based  accountability policies. [[No Child Left Behind Act]] opens broader opportunities and incentives in using [[data]] by educational organizations by requiring schools and districts to analyze additional components of data, as well as pressing them to increase student test scores. Information makes schools accountable for year by year improvement various student groups. DDDM helps to recognize the problem and who is affected by the problem; therefore, DDDM can find a solution of the problem

==Purpose==

The purpose of DDDM is to help educators, schools, districts, and states to use information they have to actionable knowledge to improve student outcomes. DDDM requires high-quality data and possibly technical assistance; otherwise, data can misinform and lead to unreliable inferences. [[Data management]] techniques can improve teaching and learning in schools. Test scores are used by many principals to identify “bubble kids”, students whose results are just below proficiency level in reading and mathematics.&lt;ref name=r1&gt;{{cite journal|last1=Mandinach|first1=Ellen|title=A perfect time for data use|journal=Educational Psychologist|date=April 23, 2012|volume=47|page=2|doi=10.1080/00461520.2012.667064}}&lt;/ref&gt;

==Types of data used in education==

There are 4 major types of data used in education: demographics data, perceptions data, student learning data, and school processes data.&lt;ref name=Bernhardt&gt;{{cite book|last1=Bernhardt|first1=Victoria|title=Data analysis for continuous school improvement|date=2013|publisher=Routledge|location=711 Third Avenue, New York, 10017|isbn=978-1-59667-252-9|pages=27–80}}&lt;/ref&gt;

1. Demographics data in educational organizations answers the question, "Who are we?". Demographics show the current context of the school and shows the trends. Trends help to predict and plan for the future, along with seeing measures where leaders work towards continuous school improvement. Thorough demographic data explains the structure of school, system, and the leadership. In education demographic data to the next items: number of students in the school, number of students with special needs, number of English learners, age or grade of students in cohorts, socio-economical status of students, attendance rates, [[ethnicity]]/[[race (human classification)|race]]/[[religious beliefs]], graduation rates, dropout rates, experience information of teachers, information about parents of students.&lt;ref name="Bernhardt"/&gt;

2. Perception data tells us what students, staff, and parents think about a school and answers the question, "How do we do business?". School culture, climate, and organizational processes are assessed by perception data. Perception data includes values, beliefs, perceptions, opinions, observations. Perception data is collected mostly questionnaires. Perception data can be differentiate by two groups: 1- staff, 2 - students and parents. Staff are being asked if any changes in instruction or [[curriculum]] need to take place. Student and parent are questioned to report their interests, how difficult it take them to learn, how are they taught and treated.&lt;ref name="Bernhardt"/&gt;

3. Student learning data answers two questions: How are our students doing? and Where are we now? Student learning data requires information from all subject areas, disaggregated by demographic groups, by teachers, by grade level, by cohorts over time, and individual student growth. This type of data helps to address additional help to students who are not proficient, deepening into what they know and what they don't know to become proficient. Student learning data connects with [[curriculum]], [[Teaching|instruction]], and [[Educational assessment|assessment]] in order to improve outcomes. Student learning data can clearly state the effectiveness of a single educator or the entire school. SLD can be gathered by looking at diagnostic tests, formative assessments, performance assessments, standardized tests, non-referenced tests, summative assessments, teacher-assigned tests, and others.&lt;ref name="Bernhardt"/&gt;

4. School processes refer to actions of administrators and teachers to achieve the purpose of the school. Teachers' habits, customs, knowledge, and professionalism are the things leading towards progress inside organizations. School processes data tell us what works, what doesn't, the results of educational process, and answers the question, "What are our processes are?". School processes produce school and class results. There are 4 major types of school processes: 1. instructional processes, 2. Organizational processes, 3. Administrative processes, 4. Continuous school improvement processes.   
&lt;ref name="Bernhardt"/&gt;

==Use in educational organizations==

[[The U.S. Department of Education]] and the [[Institute of Education Sciences]] require to use data and DDDM in past decades to run educational organizations. Hard evidence and the use of data are emphasized to inform decisions. The data in educational organizations means more than analyzing test scores. Educational data movement is considered as a sociotechnical revolution. Educational data systems involve technologies and evidence to explain districts', schools', classrooms' tendencies. DDDM is used to explain complexity of education, support collaboration, creating new designs of teaching. Student performance is central in DDDM. NCLB provided boost in the collection and use of educational information.&lt;ref name=Piety&gt;{{cite book|last1=Piety|first1=Philip|title=Assessing the educational data movement|date=2013|publisher=Teachers college press|location=New york|isbn=978-0-8077-5426-9|pages=1–20}}&lt;/ref&gt;

For example, in a rural area educators tried to understand why a particular subset of students were struggling academically. Data analysts collected students performance data, medical records, behavioral data, attendance, and other data less qualitative information. After not finding direct correlation between collected data and student outcomes they decided to include transportation data into the research. As result, educators found that students who had longer way from houses to the school were struggling the most. According to the finding administrators modified transportation arrangements to make the way shorter for students as well as installing Internet access in buses so students could concentrate on doing homework. DDDM in this particular case helped to improve student results.&lt;ref name="r1"/&gt;

==Effects on schools==

Effective schools showing outstanding gains in academic measures report that the wide and wise use of data has a positive effect on student achievement and progress. DDDM is suggested to be a main tool to move educational organizations towards school improvement and [[educator effectiveness]]. Data can be used to measure growth over time, program evaluation along with identifying root causes of problems connected to education. Involving school teachers in data inquiry causes more collaborative work from staff. Data provides increasing communication and knowledge which has a positive effect on altering educator attitudes towards groups inside schools which are underperforming 
&lt;ref&gt;{{cite journal|last1=Wayman|first1=Jeffrey|title=Involving teachers in data driven decision making:Using computer data systems to support teacher inquiry and reflection|journal=Journal of education for students placed at risk|date=2005|pages=296–300}}&lt;/ref&gt;

==Notes==
{{Reflist}}

==General references==
* {{cite journal|last1=Spillane|first1=James P.|title=Data in Practice: Conceptualizing the Data-Based Decision-Making Phenomena|journal=American Journal of Education|date=2012|volume=118|issue=2|pages=113–141|jstor=10.1086/663283|doi=10.1086/663283}}
* {{cite journal|last1=Reeves|first1=Patricia L.|last2=Burt|first2=Walter L.|title=Challenges in Data-based Decision-making: Voices from Principals|journal=Educational Horizons|date=2006|volume=85|issue=1|pages=65–71|jstor=42925967}}

[[Category:Data management]]
[[Category:Standards-based education]]</text>
      <sha1>qdox7jvyqh68mqbl09d1er0hnienmwx</sha1>
    </revision>
  </page>
  <page>
    <title>Data lake</title>
    <ns>0</ns>
    <id>46626475</id>
    <revision>
      <id>759623152</id>
      <parentid>759457989</parentid>
      <timestamp>2017-01-12T07:01:28Z</timestamp>
      <contributor>
        <ip>125.18.177.100</ip>
      </contributor>
      <comment>/* Criticism */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5993" xml:space="preserve">{{Use dmy dates|date=May 2016}}
A '''data lake''' is a method of storing [[data]] within a system or repository, in its natural format,&lt;ref&gt;[http://blogs.sas.com/content/datamanagement/2016/11/21/growing-import-big-data-quality/ The growing importance of big data quality]&lt;/ref&gt; that facilitates the collocation of data in various schemata and structural forms, usually object blobs or files. 

== Invention ==
James Dixon, then chief technology officer at [[Pentaho]] coined the term&lt;ref name="woods2011"&gt;{{cite news | title=Big data requires a big architecture |last=Woods |first=Dan |work=Forbes |date=21 July 2011 |department=Tech |url=http://www.forbes.com/sites/ciocentral/2011/07/21/big-data-requires-a-big-new-architecture/ }}&lt;/ref&gt; to contrast it with [[data mart]], which is a smaller repository of interesting attributes extracted from raw data.&lt;ref name="dixon2010"&gt;{{cite web | last=Dixon|first=James|title=Pentaho, Hadoop, and Data Lakes|url=https://jamesdixon.wordpress.com/2010/10/14/pentaho-hadoop-and-data-lakes/|website=James Dixon’s Blog|publisher=James|accessdate=7 November 2015 |quote=If you think of a datamart as a store of bottled water – cleansed and packaged and structured for easy consumption – the data lake is a large body of water in a more natural state. The contents of the data lake stream in from a source to fill the lake, and various users of the lake can come to examine, dive in, or take samples.}}&lt;/ref&gt; He argued that data marts have several inherent problems, and that data lakes are the optimal solution. These problems are often referred to as [[information silo]]ing. [[PricewaterhouseCoopers]] said that data lakes could "put an end to data silos.&lt;ref name="stein2014"&gt;{{cite report | url=http://www.pwc.com/en_US/us/technology-forecast/2014/cloud-computing/assets/pdf/pwc-technology-forecast-data-lakes.pdf |format=pdf |title=Data lakes and the promise of unsiloed data |last2=Morrison |first2=Alan |last=Stein |first=Brian |publisher=PricewaterhouseCooper |series=Technology Forecast: Rethinking integration |year=2014 }}&lt;/ref&gt; In their study on data lakes they noted that enterprises were "starting to extract and place data for analytics into a single, Hadoop-based repository."

==Characteristics==
The idea of data lake is to have a single store of all data in the enterprise ranging from raw data (which implies exact copy of source system data) to transformed data which is used for various tasks including [[Data reporting|reporting]], [[data visualization|visualization]], [[data analytics|analytics]] and [[machine learning]].

The data lake includes structured data from relational databases (rows and columns), semi-structured data (CSV, logs, XML, JSON), unstructured data (emails, documents, PDFs) and even binary data (images, audio, video) thus creating a centralized data store accommodating all forms of data.

== Examples ==

One example of a data lake is the distributed file system [[Apache Hadoop]].

Many companies also use cloud storage services such as [[Amazon S3]].&lt;ref name="tuulos2015"&gt;{{cite web | title=Petabyte-Scale Data Pipelines with Docker, Luigi and Elastic Spot Instances |last=Tuulos |first=Ville |date=22 September 2015 |url=http://tech.adroll.com/blog/data/2015/09/22/data-pipelines-docker.html}}&lt;/ref&gt; There is a gradual academic interest in the concept of data lakes, for instance, [http://www.researchgate.net/publication/283053696_Personal_Data_Lake_With_Data_Gravity_Pull Personal DataLake]&lt;ref&gt;http://ieeexplore.ieee.org/xpl/abstractAuthors.jsp?reload=true&amp;arnumber=7310733&lt;/ref&gt; an ongoing project at Cardiff University to create a new type of data lake which aims at managing big data of individual users by providing a single point of collecting, organizing, and sharing personal data.&lt;ref&gt;http://www.researchgate.net/publication/283053696_Personal_Data_Lake_With_Data_Gravity_Pull&lt;/ref&gt;

The earlier data lake (Hadoop 1.0) had limited capabilities with its batch oriented processing (Map Reduce) and was the only processing paradigm associated with it. Interacting with the data lake meant you had to have expertise in Java with map reduce and higher level tools like Pig &amp; Hive (which by themselves were batch oriented). With the dawn of Hadoop 2.0 and separation of duties with Resource Management taken over by YARN (Yet another resource negotiator), new processing paradigms like Streaming, interactive, on-line have become available via Hadoop and the Data Lake.

== Criticism ==
{{criticism section|date=December 2015}}

In June 2015, David Needle characterized "so-called data lakes" as "one of the more controversial ways to manage [[big data]]".&lt;ref name="needle2015"&gt;{{cite news |  last=Needle |first=David | title=Hadoop Summit: Wrangling Big Data Requires Novel Tools, Techniques |date=10 June 2015 | work=eWeek | url= http://www.eweek.com/enterprise-apps/hadoop-summit-wrangling-big-data-requires-novel-tools-techniques-2.html | department=Enterprise Apps | access-date = 1 November 2015 | quote = Walter Maguire, chief field technologist at HP's Big Data Business Unit, discussed one of the more controversial ways to manage big data, so-called data lakes. }}&lt;/ref&gt; [[PricewaterhouseCoopers]] were also careful to note in their research that not all data lake initiatives are successful. They quote Sean Martin, CTO of [[Cambridge Semantics]],
{{quote|sign=|source=|We see customers creating big data graveyards, dumping everything into HDFS [Hadoop Distributed File System] and hoping to do something with it down the road. But then they just lose track of what’s there.&lt;ref name="stein2014"/&gt;}}
They advise that "The main challenge is not creating a data lake, but taking advantage of the opportunities it presents."&lt;ref name="stein2014"/&gt; They describe companies that build successful data lakes as gradually maturing their lake as they figure out which data and metadata are important to the organization.

== References ==
&lt;references/&gt;

[[Category:Data management]]</text>
      <sha1>ocq95239jxte2dn83dy0p8rd6eeu08j</sha1>
    </revision>
  </page>
  <page>
    <title>Cleo (company)</title>
    <ns>0</ns>
    <id>42965868</id>
    <revision>
      <id>751529860</id>
      <parentid>714068884</parentid>
      <timestamp>2016-11-26T08:25:42Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 0 sources and tagging 1 as dead. #IABot (v1.2.7.1)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10388" xml:space="preserve">{{Infobox company
| logo             = [[File:Cleo (company) logo, 2014.png]]
| name             = Cleo
| type             = [[Privately held company]]
| foundation       =  1976
| location_city    = [[Loves Park, Illinois]]
| location_country = [[United States of America]]
| key_people       =
Mahesh Rajasekharan &lt;small&gt;([[Chief executive officer|CEO]])&lt;/small&gt;&lt;br/&gt;Sumit Garg &lt;small&gt;(President)&lt;/small&gt;
| num_employees    = 200+
| industry         = [[Managed file transfer]], data integration, [[network management]] and secure file sharing
| homepage         = {{url|http://cleo.com}}
}}

'''Cleo''' is an [[enterprise software]] company that provides [[electronic data interchange]] (EDI), and application-to-application (A2A), [[business-to-business]] (B2B), and [[big data]] integration services to organizations with [[managed file transfer]] needs. The company, formerly known as Cleo Communications, was founded in 1976. Cleo was acquired by investment firm Globe Equity Partners in 2012. Mahesh Rajasekharan is Cleo's [[CEO]], and Sumit Garg serves as Cleo's president.&lt;ref&gt;{{cite web|author=Alex Gary |url=http://www.rrstar.com/x1364621329/Private-equity-firm-acquires-Loves-Park-company |title=Private equity firm acquires Loves Park company - Blogs - Rockford Register Star |publisher=Rrstar.com |date= |accessdate=2014-06-05}}&lt;/ref&gt;

== Business ==
Cleo originally began as a division of Phone 1 Inc., a voice data gathering systems manufacturer, and built data concentrators and [[terminal emulator]]s — multi-bus computers, modems, and terminals to interface with [[IBM]] mainframes via [[Binary Synchronous Communications|bisynchronous communications]]. The company then began developing [[mainframe]] middleware in the 1980s, and with the rise of the [[Personal computer|PC]], moved into B2B data communications and [[file transfer]] software.&lt;ref&gt;{{cite web|url=https://books.google.com/books?id=JNJWAAAAMAAJ&amp;q=cleo+%22phone+1%22&amp;dq=cleo+%22phone+1%22&amp;hl=en&amp;sa=X&amp;ei=6I65VLiABc6yogTf0IKABA&amp;ved=0CEcQ6AEwBTge |title=Kelly/Grimes IBM PC compatible computer directory - Brian W. Kelly, Dennis J. Grimes - Google Books |publisher=Books.google.com |date=2008-01-28 |accessdate=2015-04-02}}&lt;/ref&gt;

Cleo's portfolio features big data, extreme file transfer, [[data transformation]], person-to-person collaboration, and file sharing solutions,&lt;ref&gt;http://www.channelworld.in/interviews/high-speed-data-transfer-is-more-critical-than-ever%3A-mahesh-rajasekharan%2C-cleo&lt;/ref&gt; and its product line includes software for secure file transfer, exchange, and [[Cloud collaboration|collaboration]]; secure email, text, and voice messaging; and others.&lt;ref&gt;[http://investing.businessweek.com/research/stocks/private/snapshot.asp?privcapId=204651549 Cleo Communications, Inc.: Private Company Information - Businessweek&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;  Cleo products use the [[AS2]] specification and other protocols for connectivity and community management.&lt;ref&gt;{{cite web|author=|url=http://www.filetransferconsulting.com/forresters-managed-file-transfer-good-bad-ugly/ |title=Forrester’s "Managed File Transfer Solutions" – Good, Bad and Ugly |publisher=Filetransferconsulting.com |date=2011-07-14 |accessdate=2014-06-05}}&lt;/ref&gt; Cleo VersaLex is the engine behind its software offerings, which include Cleo LexiCom,&lt;ref&gt;[http://www.itjungle.com/fhs/fhs030408-story10.html Four Hundred Stuff-Cleo Updates B2B Communications Software&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; Cleo VLTrader,&lt;ref&gt;[http://www.itjungle.com/fhs/fhs021610-story08.html Four Hundred Stuff-Stonebranch Taps Cleo for B2B Expertise&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; and Cleo Harmony, which supports the streamlining of [[data integration]].&lt;ref&gt;[http://webcache.googleusercontent.com/search?q=cache:http://lerablog.org/technology/software/ftp-tools-to-help-with-large-file-transfers/ Best FTP Tools for Large File Transfers&lt;!-- Bot generated title --&gt;]{{dead link|date=November 2016 |bot=InternetArchiveBot |fix-attempted=yes }}&lt;/ref&gt; The company also developed the Cleo Unify and Cleo Trust secure [[file sharing]] and [[email]] messaging solutions that work independently or in conjunction with Cleo's data integration platform.&lt;ref&gt;{{cite web|url=http://www.rrstar.com/article/20150209/News/150209528 |title=Cleo releases new software - Rockford Register Star |publisher=rrstar.com |date=2015-02-09 |accessdate=2015-02-19}}&lt;/ref&gt; In 2015, Cleo introduced the Cleo Jetsonic high-speed data transfer software solution.&lt;ref&gt;{{cite web|url=http://www.rrstar.com/article/20150708/NEWS/150709580/-1/json |title=Cleo announces new data solution - Rockford Register Star |publisher=rrstar.com |date= |accessdate=2015-07-09}}&lt;/ref&gt;

The City of [[Atlanta]] adopted Cleo's [[fax]] technology, Cleo Streem, in 2006 to accommodate its communication needs,&lt;ref&gt;http://citycouncil.atlantaga.gov/2013/images/proposed/13R3556.pdf&lt;/ref&gt; and the [[U.S. Department of Veterans Affairs]] did the same in 2013 when in need of [[FIPS 140-2]]-compliant technology to protect information.&lt;ref&gt;[http://www.va.gov/TRM/ToolPage.asp?tid=6568 One-VA Technical Reference Model&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; Cleo also serves U.S. transportation [[logistics]] company MercuryGate International&lt;ref&gt;{{cite web|url=http://www.rrstar.com/article/20140820/ENTERTAINMENTLIFE/140829886/10487/BUSINESS |title=Loves Park business selected to provide online services - Rockford Register Star |publisher=rrstar.com |date=2014-08-20 |accessdate=2015-02-19}}&lt;/ref&gt; as a customer and partners with [[Hortonworks]]&lt;ref&gt;[http://hortonworks.com/blog/secure-reliable-hadoop-data-transfer-option-cleo-mft/ Secure, reliable Hadoop data transfer with Cleo MFT - Hortonworks&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; for big data integration and [[Tech Data]] for software distribution.&lt;ref&gt;[http://logistics.cioreview.com/news/cleo-s-data-transfer-solutions-now-available-on-the-tech-data-online-store-nid-2119-cid-33.html Cleo's Data Transfer Solutions Now Available on the Tech Data Online Store&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; Cleo software also powers the architecture for several major supply chain companies, such as [[JDA Software]] and [[SAP SE|SAP]].&lt;ref&gt;{{cite web|last=Grackin |first=Ann |url=http://searchmanufacturingerp.techtarget.com/tip/Smart-sensors-bring-the-supply-chain-to-life |title=Smart sensors bring the supply chain to life |publisher=Searchmanufacturingerp.techtarget.com |date= |accessdate=2015-07-08}}&lt;/ref&gt;

In 2009, Cleo was added to the [[Gartner]] [[Magic Quadrant]] for managed file transfer.&lt;ref&gt;http://www.servicecatalog.dts.ca.gov/services/sft/docs/MFT_Quad_2009_axway_3183.pdf&lt;/ref&gt;

== Expansion ==
In June 2014, Cleo opened an office in [[Chicago]] for members of its support and engineering teams.&lt;ref&gt;[http://rockrivertimes.com/2014/07/16/cleo-continues-to-grow-expands-operations-into-chicago-office/ Cleo continues to grow, expands operations into Chicago office | The Rock River Times&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; The company in 2014 hired Jorge Rodriguez as senior vice president of product development&lt;ref&gt;{{cite web|author=|url=http://www.marketwatch.com/story/jorge-rodriguez-joins-cleo-as-senior-vice-president-of-product-development-2014-02-04 |title=Jorge Rodriguez Joins Cleo as Senior Vice President of Product Development |publisher=MarketWatch |date=2014-02-04 |accessdate=2015-02-19}}&lt;/ref&gt; and John Thielens as vice president of technology.&lt;ref&gt;{{cite web|author=|url=http://www.marketwatch.com/story/john-thielens-joins-cleo-as-vice-president-of-technology-2014-01-31 |title=John Thielens Joins Cleo as Vice President of Technology |publisher=MarketWatch |date=2014-01-31 |accessdate=2015-02-19}}&lt;/ref&gt; And in 2015, Cleo hired Dave Brunswick as vice president of solutions for North America.&lt;ref&gt;{{cite web|url=http://www.rrstar.com/article/20150705/NEWS/150709917 |title=Cleo announces new hire - Rockford Register Star |publisher=rrstar.com |date= |accessdate=2015-07-08}}&lt;/ref&gt; Cleo also opened its Center of Innovation product development facility in [[Bengaluru, India]], in 2015.&lt;ref&gt;http://www.deccanherald.com/content/500091/cleo-bengaluru-centre-plans-co.html&lt;/ref&gt;

In 2016, Cleo acquired [[Extol International|EXTOL International]], a [[Pottsville, Pennsylvania|Pottsville, Pa.]]-based business and EDI integration and data transformation company for an undisclosed amount. The Pottsville office will operate under the Cleo name.&lt;ref&gt;http://www.lvb.com/article/20160406/LVB01/160409929/pottsville-tech-firm-acquired-by-illinois-company&lt;/ref&gt;

== Certification ==
Cleo regularly submits its products to Drummond Group's interoperability software testing for AS2,&lt;ref&gt;[http://www.supplychainbrain.com/content/technology-solutions/supplier-relationship-mgmt/single-article-page/article/cleos-versalex-wins-drummond-certification-for-as2-interoperability/ Cleo's VersaLex Wins Drummond Certification for AS2 Interoperability&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; AS3&lt;ref&gt;{{cite web|url=http://www.drummondgroup.com/index.php/newsevents/press-releases/341-as3-secure-messaging-products-are-drummond-certified-in-1q14-interoperability-test-event |title=AS3 Secure Messaging Products are Drummond Certified™ in 1Q14 Interoperability Test Event |publisher=Drummond Group |date=2014-02-19 |accessdate=2014-06-05}}&lt;/ref&gt; and ebMS 2.0.&lt;ref&gt;{{cite web|url=http://www.drummondgroup.com/index.php/newsevents/press-releases/339-newest-ebms-20-secure-messaging-products-are-drummond-certified |title=Newest ebMS 2.0 Secure Messaging Products are Drummond Certified™ |publisher=Drummond Group |date=2013-09-09 |accessdate=2014-06-05}}&lt;/ref&gt;

== Awards ==
Cleo has been given a [[Xerox]] partner of the year award for each of the past five years. The Cleo Streem solution integrates with Xerox multi-function products, providing customers with comprehensive solutions for network fax and interactive messaging needs.&lt;ref&gt;{{cite web|url=http://www.rrstar.com/article/20150330/NEWS/150339927/10447/NEWS |title=Cleo Wins Xerox Partner of the Year Award - News - Rockford Register Star |publisher=rrstar.com |date= |accessdate=2015-04-02}}&lt;/ref&gt;

== References ==
{{Reflist|3}}

[[Category:EDI software companies]]
[[Category:Software companies based in Illinois]]
[[Category:Network management]]
[[Category:Managed file transfer]]
[[Category:File transfer protocols]]
[[Category:Data management]]</text>
      <sha1>79mk6zmwc3zpl3zytcu7s1g3lji80l7</sha1>
    </revision>
  </page>
  <page>
    <title>Information integration</title>
    <ns>0</ns>
    <id>2714749</id>
    <revision>
      <id>760754830</id>
      <parentid>760746092</parentid>
      <timestamp>2017-01-18T22:04:19Z</timestamp>
      <contributor>
        <username>Sir mba</username>
        <id>11947451</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2896" xml:space="preserve">{{Refimprove|date=September 2014}}
'''Information integration''' ('''II''') (also called [[referential integrity]]) is the merging of information from heterogeneous sources with differing conceptual, contextual and typographical representations. It is used in [[data mining]] and consolidation of data from unstructured or semi-structured resources. Typically, ''information integration'' refers to textual representations of knowledge but is sometimes applied to [[rich-media]] content. '''Information fusion''', which is a related term, involves the combination of information into a new set of information towards reducing redundancy and uncertainty.&lt;ref name="dca"&gt;M. Haghighat, M. Abdel-Mottaleb, &amp;  W. Alhalabi (2016). [http://dx.doi.org/10.1109/TIFS.2016.2569061 Discriminant Correlation Analysis: Real-Time Feature Level Fusion for Multimodal Biometric Recognition]. IEEE Transactions on Information Forensics and Security, 11(9), 1984-1996.&lt;/ref&gt;

Examples of [[Technology|technologies]] available to integrate information include [[data deduplication|deduplication]], and [[string metrics]] which allow the detection of similar text in different data sources by [[fuzzy string searching|fuzzy matching]]. A host of methods for these research areas are available such as those presented in the International Society of Information Fusion.

==See also==
* [[Data fusion]] (is a subset of Information integration)
* [[Sensor fusion]]
* [[Data integration]]
* [[Image fusion]]

==External links==
* [https://github.com/mhaghighat/dcaFuse Discriminant Correlation Analysis (DCA)]&lt;ref name="dca"&gt;&lt;/ref&gt;
* [http://webcache.googleusercontent.com/search?q=cache:OrNCxOpaXAMJ:infolab.stanford.edu/pub/papers/integration-using-views.ps+information+integration&amp;cd=5&amp;hl=en&amp;ct=clnk&amp;gl=us&amp;client=firefox-a Information Integration Using Logical View] LNCS 1997.
* [http://www.isif.org/ International Society of Information Fusion]

==Books==
* Liggins, Martin E., David L. Hall, and James Llinas. Multisensor Data Fusion, Second Edition Theory and Practice (Multisensor Data Fusion). CRC, 2008. ISBN 978-1-4200-5308-1
* David L. Hall, Sonya A. H. McMullen, Mathematical Techniques in Multisensor Data Fusion (2004), ISBN 1-58053-335-3
* Springer, Information Fusion in Data Mining (2003), ISBN 3-540-00676-1
* H. B. Mitchell, Multi-sensor Data Fusion – An Introduction (2007) Springer-Verlag, Berlin, ISBN 978-3-540-71463-7
* S. Das, High-Level Data Fusion (2008), Artech House Publishers, Norwood, MA, ISBN 978-1-59693-281-4 and 1596932813
* Erik P. Blasch, Eloi Bosse, and Dale A. Lambert, High-Level Information Fusion Management and System Design (2012), Artech House Publishers, Norwood, MA. ISBN 1608071510 | ISBN 978-1608071517

==References==
{{Reflist}}


{{DEFAULTSORT:Information Integration}}
[[Category:Data management]]

[[ar:تكامل البيانات]]
[[de:Informationsintegration]]</text>
      <sha1>f0twc7xm6gj48bktb0k1mbyy34rrzw4</sha1>
    </revision>
  </page>
  <page>
    <title>Chunked transfer encoding</title>
    <ns>0</ns>
    <id>7061159</id>
    <revision>
      <id>756992059</id>
      <parentid>756988961</parentid>
      <timestamp>2016-12-28T03:16:29Z</timestamp>
      <contributor>
        <username>StrokeOfMidnight</username>
        <id>28125651</id>
      </contributor>
      <comment>Undid revision 756988961 by [[Special:Contributions/2600:8805:D880:844:4462:6DA1:7D2:E25A|2600:8805:D880:844:4462:6DA1:7D2:E25A]] ([[User talk:2600:8805:D880:844:4462:6DA1:7D2:E25A|talk]]): not a typo; chunk size is in hex format</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7142" xml:space="preserve">{{refimprove|date=June 2014}}
'''Chunked transfer encoding''' is a data transfer mechanism in version 1.1 of the [[Hypertext Transfer Protocol]] (HTTP) in which data is sent in a series of "chunks". It uses the [[List of HTTP header fields#transfer-encoding-response-header|Transfer-Encoding]] HTTP header in place of the [[Content-Length]] header, which the earlier version of the protocol would otherwise require.&lt;ref&gt;http://tools.ietf.org/html/rfc1945#section-7.2&lt;/ref&gt; Because the Content-Length header is not used, the sender does not need to know the length of the content before it starts transmitting a response to the receiver. Senders can begin transmitting dynamically-generated content before knowing the total size of that content.

The size of each chunk is sent right before the chunk itself so that the receiver can tell when it has finished receiving data for that chunk. The data transfer is terminated by a final chunk of length zero.

An early form of the chunked encoding was proposed in 1994.&lt;ref&gt;{{cite web|last=Connolly|first=Daniel|title=Content-Transfer-Encoding: packets for HTTP|url=http://1997.webhistory.org/www.lists/www-talk.1994q3/1147.html|accessdate=13 September 2013|date=27 Sep 1994|id=&amp;lt;9409271503.AA27488@austin2.hal.com&amp;gt;}}&lt;/ref&gt; Later it was standardized in HTTP 1.1.

==Rationale==
The introduction of chunked encoding provided various benefits:

* Chunked transfer encoding allows a server to maintain an [[HTTP persistent connection]] for dynamically generated content. In this case, the HTTP Content-Length header cannot be used to delimit the content and the next HTTP request/response, as the content size is as yet unknown. Chunked encoding has the benefit that it is not necessary to generate the full content before writing the header, as it allows streaming of content as chunks and explicitly signaling the end of the content, making the connection available for the next HTTP request/response.
* Chunked encoding allows the sender to send additional header fields after the message body. This is important in cases where values of a field cannot be known until the content has been produced, such as when the content of the message must be digitally signed. Without chunked encoding, the sender would have to buffer the content until it was complete in order to calculate a field value and send it before the content.

==Applicability==
For version 1.1 of the HTTP protocol, the chunked transfer mechanism is considered to be always and anyways acceptable, even if not listed in the [[List of HTTP header fields#te-request-header|TE]] (transfer encoding) request header field, and when used with other transfer mechanisms, should always be applied last to the transferred data and never more than one time. This transfer coding method also allows additional entity header fields to be sent after the last chunk if the client specified the "trailers" parameter as an argument of the TE field. The origin server of the response can also decide to send additional entity trailers even if the client did not specify the "trailers" option in the TE request field, but only if the metadata is optional (i.e. the client can use the received entity without them). Whenever the trailers are used, the server should list their names in the Trailer header field; 3 header field types are specifically prohibited from appearing as a trailer field:  [[List of HTTP header fields#transfer-encoding-response-header|Transfer-Encoding]], [[List of HTTP header fields#content-length-response-header|Content-Length]] and [[List of HTTP header fields#trailer-response-header|Trailer]].

==Format==
If a &lt;tt&gt;Transfer-Encoding&lt;/tt&gt; field with a value of "&lt;tt&gt;chunked&lt;/tt&gt;" is specified in an HTTP message (either a request sent by a client or the response from the server), the body of the message consists of an unspecified number of chunks, a terminating chunk, trailer, and a final CRLF sequence (i.e. [[carriage return]] followed by [[line feed]]).

Each chunk starts with the number of [[Octet (computing)|octets]] of the data it embeds expressed as a [[hexadecimal]] number in [[ASCII]] followed by optional parameters (''chunk extension'') and a terminating CRLF sequence, followed by the chunk data. The chunk is terminated by CRLF.

If chunk extensions are provided, the chunk size is terminated by a semicolon and followed by the parameters, each also delimited by semicolons. Each parameter is encoded as an extension name followed by an optional equal sign and value. These parameters could be used for a running [[message digest]] or [[digital signature]], or to indicate an estimated transfer progress, for instance.

The terminating chunk is a regular chunk, with the exception that its length is zero. It is followed by the trailer, which consists of a (possibly empty) sequence of entity header fields. Normally, such header fields would be sent in the message's header; however, it may be more efficient to determine them after processing the entire message entity. In that case, it is useful to send those headers in the trailer.

Header fields that regulate the use of trailers are ''TE'' (used in requests), and ''Trailers'' (used in responses).

==Use with compression==

HTTP servers often use [[data compression|compression]] to optimize transmission, for example with &lt;tt&gt;Content-Encoding: [[gzip]]&lt;/tt&gt; or &lt;tt&gt;Content-Encoding: [[deflate]]&lt;/tt&gt;. If both compression and chunked encoding are enabled, then the content stream is first compressed, then chunked; so the chunk encoding itself is not compressed, and the data in each chunk is not compressed individually. The remote endpoint then decodes the stream by concatenating the chunks and uncompressing the result.

==Example==

===Encoded data===
In the following example, three chunks of length 4, 5 and 14 are shown. The chunk size is transferred as a hexadecimal number followed by \r\n as a line separator, followed by a chunk of data of the given size.

&lt;pre&gt;
4\r\n
Wiki\r\n
5\r\n
pedia\r\n
E\r\n
 in\r\n
\r\n
chunks.\r\n
0\r\n
\r\n
&lt;/pre&gt;

Note: the chunk size indicates the size of the chunk data and excludes the trailing CRLF ("\r\n").&lt;ref&gt;http://skrb.org/ietf/http_errata.html&lt;/ref&gt; In this particular example, the CRLF following "in" is counted toward the chunk size of 0xE (14). The CRLF in its own line is also counted toward the chunk size.
The period character at the end of "chunks" is the 14th character, so it is the
last data character in that chunk. The CRLF following the period is
the trailing CRLF, so it is not counted toward the chunk size of 0xE (14).

===Decoded data===
&lt;pre&gt;
Wikipedia in

chunks.
&lt;/pre&gt;

==See also==
* [[List of HTTP header fields]]

==References==
{{Reflist}}
{{Refbegin}}
* See [http://tools.ietf.org/html/rfc7230#section-4.1 RFC 7230 section 4.1] for further details of chunked encoding.
* The previous (obsoleted) version is at [https://tools.ietf.org/html/rfc2616#section-3.6.1 RFC 2616 section 3.6.1].
{{Refend}}

{{DEFAULTSORT:Chunked Transfer Encoding}}
[[Category:Data management]]
[[Category:Hypertext Transfer Protocol]]
[[Category:Hypertext Transfer Protocol headers]]</text>
      <sha1>el48fz3d726vzdy68dkbp2m8cuemjpn</sha1>
    </revision>
  </page>
  <page>
    <title>Data availability</title>
    <ns>0</ns>
    <id>46877023</id>
    <revision>
      <id>751536610</id>
      <parentid>751376809</parentid>
      <timestamp>2016-11-26T09:24:23Z</timestamp>
      <contributor>
        <username>Materialscientist</username>
        <id>7852030</id>
      </contributor>
      <comment>Reverted 1 [[WP:AGF|good faith]] edit by [[Special:Contributions/136.186.67.94|136.186.67.94]] using [[WP:STiki|STiki]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2803" xml:space="preserve">{{cleanup reorganize|date=June 2015}}
Data availability&lt;ref&gt;http://searchstorage.techtarget.com/definition/data-availability&lt;/ref&gt; is a term used by computer storage manufacturers and storage service providers (SSPs) to describe products and services that ensure that data continues to be available at a required level of performance in situations ranging from normal through "disastrous."

Anytime a server loses power, for example, it has to reboot, recover data and repair corrupted data. The time it takes to recover, known as the mean time to recover (MTR), could be minutes, hours or days.&lt;ref&gt;http://blog.schneider-electric.com/datacenter/2012/10/12/understanding-data-center-reliability-availability-and-the-cost-of-downtime/&lt;/ref&gt;

==Data Center Standards==
The two organizations in the United States that publish data center standards are the [[Telecommunications Industry Association]] (TIA) and the [[Uptime Institute]].

===TIA - Data Center Standards===
See wiki entry on [[TIA-942]].

===Uptime Institute - Data Center Tier Standards===
'''Tier I Requirements'''&lt;ref&gt;http://www.firstcomm.com/overview-of-data-center-availability-tiers/&lt;/ref&gt;
* Single non-redundant distribution path serving the IT equipment
*  Non-redundant capacity components
* Basic site infrastructure with expected availability of 99.671%

'''Tier II Requirements'''
* Meets or exceeds all Tier I requirements
* Redundant site infrastructure capacity components with expected availability of 99.741%

'''Tier III Requirements'''
* Meets or exceeds all Tier I and Tier II requirements
* Multiple independent distribution paths serving the IT equipment
* All IT equipment must be dual-powered and fully compatible with the topology of a site’s architecture
* Concurrently maintainable site infrastructure with expected availability of 99.982%

'''Tier IV Requirements'''
* Meets or exceeds all Tier I, Tier II and Tier III requirements
* All cooling equipment is independently dual-powered, including chillers and heating, ventilating and air-conditioning (HVAC) systems
* Fault-tolerant site infrastructure with electrical power storage and distribution facilities with expected availability of 99.995%

The Uptime Institute’s tier system allows for the following minutes of downtime annually:
* Tier I (99.671% minimum uptime) (1729 minutes maximum annual downtime)
* Tier II (99.741% minimum uptime) (1361 minutes maximum annual downtime)
* Tier III (99.982% minimum uptime) (95 minutes maximum annual downtime)
* Tier IV (99.995% minimum uptime) (26 minutes maximum annual downtime)

==See also==
* Data Center Tiers [[Data center#Data center tiers|Data center tiers]]

==References==
{{reflist}}

[[Category:Data management]]
[[Category:Distributed data storage]]
[[Category:Distributed data storage systems]]</text>
      <sha1>eyw1evyl91100yk9uk86y68gwjw8m1j</sha1>
    </revision>
  </page>
  <page>
    <title>Skyline operator</title>
    <ns>0</ns>
    <id>46213978</id>
    <revision>
      <id>748210532</id>
      <parentid>747078399</parentid>
      <timestamp>2016-11-07T00:46:55Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor />
      <comment>/* Implementation */Removed invisible unicode characters + other fixes, replaced: →   (2) using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3294" xml:space="preserve">The '''Skyline operator''' is used in a query and performs a filtering of results from a database so that it keeps only those objects that are not worse than any other.

This operator is an extension to [[SQL]] proposed by Börzsönyi et al.&lt;ref name=borzsony2001skyline&gt;{{cite journal|last1=Borzsonyi|first1=Stephan|last2=Kossmann|first2=Donald|last3=Stocker|first3=Konrad|title=The Skyline Operator|journal=Proceedings 17th International Conference on Data Engineering|date=2001|pages=421–430|doi=10.1109/ICDE.2001.914855}}&lt;/ref&gt;  A classic example of application of the Skyline operator involves selecting a hotel for a holiday. The user wants the hotel to be both cheap and close to the beach. However, hotels that are close to the beach may also be expensive. In this case, the Skyline operator would only present those hotels that are not worse than any other hotel in both price and distance to the beach.

== Proposed syntax ==

To give an example in SQL: Börzsönyi et al.&lt;ref name=borzsony2001skyline/&gt; proposed the following syntax for the Skyline operator:

&lt;source lang="sql"&gt;
SELECT ... FROM ... WHERE ...
GROUP BY ... HAVING ...
SKYLINE OF [DISTINCT] d1 [MIN | MAX | DIFF],
                 ..., dm [MIN | MAX | DIFF]
ORDER BY ...
&lt;/source&gt;
where d&lt;sub&gt;1&lt;/sub&gt;, ... d&lt;sub&gt;m&lt;/sub&gt; denote the dimensions of the Skyline and MIN, MAX and DIFF specify whether the value in that dimension should be minimised, maximised or simply be different.

== Implementation ==
The Skyline operator can be implemented directly in SQL using current SQL constructs, however this has been shown to be very slow.&lt;ref name=borzsony2001skyline/&gt; Other algorithms have been proposed that make use of divide and conquer, indices,&lt;ref name=borzsony2001skyline/&gt; [[MapReduce]]&lt;ref&gt;{{cite journal|last1=Mullesgaard|first1=Kasper|last2=Pedersen|first2=Jens Laurits|last3=Lu|first3=Hua|last4=Zhou|first4=Yongluan|title=Efficient Skyline Computation in MapReduce|journal=Proc. 17th International Conference on Extending Database Technology (EDBT)|date=2014|pages=37–48|url=http://www.openproceedings.eu/2014/conf/edbt/MullesgaardPLZ14.pdf}}&lt;/ref&gt; and [[General-purpose computing on graphics processing units|general-purpose computing on graphics cards]].&lt;ref&gt;{{cite journal|last1=Bøgh|first1=Kenneth S|last2=Assent|first2=Ira|last3=Magnani|first3=Matteo|title=Efficient GPU-based skyline computation|journal=Proceedings of the Ninth International Workshop on Data Management on New Hardware|date=2013|pages=5:1–5:6|doi=10.1145/2485278.2485283}}&lt;/ref&gt; Skyline queries on data streams (i.e. continuous skyline queries) have been studied in the context of parallel query processing on multicores, owing to their wide diffusion in real-time decision making problems and data streaming analytics.&lt;ref&gt;{{cite journal|last1=De Matteis|first1=Tiziano|last2=Di Girolamo|first2=Salvatore|last3=Mencagli|first3=Gabriele|title=Continuous skyline queries on multicore architectures|journal=Concurrency and Computation: Practice and Experience|date=25 August 2016|volume=28|issue=12|pages=3503–3522|doi=10.1002/cpe.3866}}&lt;/ref&gt;

==References==
&lt;references /&gt;

[[Category:Data management]]
[[Category:Query languages]]
[[Category:Relational database management systems]]
[[Category:SQL]]


{{database-software-stub}}</text>
      <sha1>26swzixi1cieylt3so6y3uglsmd4rt3</sha1>
    </revision>
  </page>
  <page>
    <title>Couchbase Server</title>
    <ns>0</ns>
    <id>28366048</id>
    <revision>
      <id>754348871</id>
      <parentid>749524126</parentid>
      <timestamp>2016-12-12T04:48:30Z</timestamp>
      <contributor>
        <username>Rkeshav.murthy</username>
        <id>29873708</id>
      </contributor>
      <minor />
      <comment>Expanded the description of N1QL</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="12158" xml:space="preserve">{{Infobox software
| name                   = Couchbase Server
| logo                   = [[File:CouchbaseLogo.svg|224px]]
| screenshot             = Couchbase Server Screenshot.jpg
| caption                =
| developer              = [[Couchbase, Inc.]]
| released               = {{Start date|2010|08}}
| latest release version = 4.5
| latest release date    = {{release date|2016|06|22}}
| status                 = active
| programming language   = [[C++]], [[Erlang (programming language)|Erlang]], [[C (programming language)|C]],&lt;ref&gt;{{cite web |author= Damien Katz|url=http://damienkatz.net/2013/01/the_unreasonable_effectiveness_of_c.html |title=The Unreasonable Effectiveness of C |date= January 8, 2013 |accessdate= September 30, 2016 }}&lt;/ref&gt; [[Go (programming language)|Go]]
| operating system       = [[Cross-platform]]
| genre                  = [[Multi-model database]] / [[Key-value database|Distributed Key-Value]] / [[Document-oriented database]]
| license                = [[Apache License]] (Open Source edition), [[Proprietary software|Proprietary]] (Free Community edition and Paid Enterprise edition)
| website                = {{URL|http://www.couchbase.com/}}
| frequently updated     = yes
}}

'''Couchbase Server''', originally known as '''Membase''', is an [[open-source]], distributed ([[shared-nothing architecture]]) [[Multi-model database|multi-model]] [[NoSQL]] [[document-oriented database]] software package that is optimized for interactive applications. These applications may serve many [[concurrent user]]s by creating, storing, retrieving, aggregating, manipulating and presenting data. In support of these kinds of application needs, Couchbase Server is designed to provide easy-to-scale key-value or JSON document access with low latency and high sustained throughput. It is designed to be [[Cluster (computing)|clustered]] from a single machine to very large-scale deployments spanning many machines.
A version originally called '''Couchbase Lite''' was later marketed as Couchbase Mobile combined with other software.

Couchbase Server provided client protocol compatibility with [[memcached]],&lt;ref&gt;{{cite web|url=http://code.google.com/p/memcached/wiki/NewProtocols |title=NewProtocols - memcached - Klingon - Memcached - Google Project Hosting |publisher=Code.google.com |date=2011-08-22 |accessdate=2013-06-04}}&lt;/ref&gt; but added disk [[Persistence (computer science)|persistence]], [[data replication]], live cluster reconfiguration, rebalancing and [[multitenancy]] with [[Partition (database)|data partitioning]].

==Product history==
Membase was developed by several leaders of the [[memcached]] project, who had founded a company, NorthScale, to develop a [[key-value store]] with the simplicity, speed, and scalability of memcached, but also the storage, persistence and querying capabilities of a database. The original membase source code was contributed by NorthScale, and project co-sponsors [[Zynga]] and [[Naver Corporation]] (then known as NHN) to a new project on membase.org in June 2010.&lt;ref&gt;{{Cite book |title= Professional NoSQL |author= Shashank Tiwari  |publisher= John Wiley &amp; Sons |pages= 15–16 |isbn= 9781118167809 }}&lt;/ref&gt;

On February 8, 2011, the Membase project founders and Membase, Inc. announced a merger with CouchOne (a company with many of the principal players behind [[CouchDB]]) with an associated project merger. The merged company was called [[Couchbase, Inc.]] In January 2012, Couchbase released Couchbase Server 1.8. 
In September, 2012, [[Orbitz]] said it had changed some of its systems to use Couchbase.&lt;ref&gt;{{cite web |url= http://gigaom.com/cloud/balancing-oracle-and-open-source-at-orbitz/ |title= Balancing Oracle and open source at Orbitz |publisher=[[GigaOM]] |date= September 21, 2012 |accessdate= September 19, 2016 }}&lt;/ref&gt;
On December 2012, 
Couchbase Server 2.0 (announced in July 2011) was released and included a new [[JSON]] document store, indexing and querying, incremental [[MapReduce]] and [[Replication (computing)|replication]] across [[data center]]s.&lt;ref name="zd2"&gt;{{cite web |url= http://www.zdnet.com/couchbase-2-0-released-implements-json-document-store-7000008649/ |title= Couchbase 2.0 released; implements JSON document store |publisher= [[ZDNet]] |author=  Andrew Brust |date= December 12, 2012}}&lt;/ref&gt;&lt;ref&gt;{{Cite web |title= Couchbase goes 2.0, pushes SQL for NoSQL |author= Derrick Harris |date= July 29, 2011 |work= GigaOm |url= https://gigaom.com/2011/07/29/couchbase-2-0-unql-sql-nosql/ |accessdate= September 19, 2016 }}&lt;/ref&gt;

==Architecture==
Every Couchbase node consists of a data service, index service, query service, and cluster manager component. Starting with the 4.0 release, the three services can be distributed to run on separate nodes of the cluster if needed.
In the parlance of Eric Brewer’s [[CAP theorem]], Couchbase is normally a CP type system meaning it provides [[Consistency (database systems)|consistency]] and [[Network partitioning|partition tolerance]], or it can be set up as an AP system with multiple clusters.

===Cluster manager===
The cluster manager supervises the configuration and behavior of all the servers in a Couchbase cluster. It configures and supervises inter-node behavior like managing replication streams and re-balancing operations. It also provides metric aggregation and consensus functions for the cluster, and a [[REST]]ful cluster management interface. The cluster manager uses the [[Erlang (programming language)|Erlang programming language]] and the [[Open Telecom Platform]].

====Replication and fail-over====
[[Data replication]] within the nodes of a cluster can be controlled with several parameters.
In December 2012, replication was also supported between different [[data center]]s.&lt;ref name="zd2" /&gt;

===Data manager===
The data manager stores and retries documents in response to data operations from applications.
It asynchronously writes data to disk after acknowledging to the client.  In version 1.7 and later, applications can optionally ensure data is written to more than one server or to disk before acknowledging a write to the client.
Parameters define item ages that affect when data is persisted, and how max memory and migration from main-memory to disk is handled.
It supports working sets greater than a memory quota per "node" or "bucket".
External systems can subscribe to filtered data streams, supporting, for example, [[full text search]] indexing, [[data analytics]] or archiving.&lt;ref&gt;{{Cite web |url= http://blog.couchbase.com/want-know-what-your-memcached-servers-are-doing-tap-them |title= Want to know what your memcached servers are doing? Tap them |author= Trond Norbye |work= Couchbase blog |date= March 15, 2010}}&lt;/ref&gt;

====Data format====
A document is the most basic unit of data manipulation in Couchbase Server. Documents are stored in JSON document format with no predefined schemas.

====Object-managed cache====
Couchbase Server includes a built-in multi-threaded object-managed cache that implements memcached compatible APIs such as get, set, delete, append, prepend etc.

====Storage engine ====
Couchbase Server has a tail-append storage design that is immune to data corruption, [[OOM killer]]s or sudden loss of power.  Data is written to the data file in an append-only manner, which enables Couchbase to do mostly sequential writes for update, and provide an optimized access patterns for disk I/O.

=== Performance ===
A performance benchmark done by [[Altoros]] in 2012, compared Couchbase Server with other technologies.&lt;ref&gt;{{cite web |url= http://www.couchbase.com/nosql-resources/presentations/benchmarking-couchbase%5B2%5D.html |title= Benchmarking Couchbase |author= Frank Weigel |publisher=Couchbase |date= October 30, 2012 |accessdate= September 30, 2016 }}&lt;/ref&gt;
[[Cisco Systems]] published a benchmark that measured the latency and throughput of Couchbase Server with a mixed workload in 2012.&lt;ref&gt;{{cite web |url= http://www.cisco.com/en/US/prod/collateral/switches/ps9441/ps9670/white_paper_c11-708169.pdf |title=Cisco and Solarflare Achieve Dramatic Latency Reduction for Interactive Web Applications with Couchbase, a NoSQL Database |publisher=[[Cisco Systems]] |date= June 18, 2012 |archivedate=  August 13, 2012 |archiveurl= https://web.archive.org/web/20120813162214/http://www.cisco.com/en/US/prod/collateral/switches/ps9441/ps9670/white_paper_c11-708169.pdf |accessdate= October 7, 2016 }}&lt;/ref&gt;

== Licensing and support ==
Couchbase Server is a packaged version of Couchbase's [[open source software]] technology and is available in a community edition without recent bug fixes with Apache 2.0 license.&lt;ref&gt;{{cite web |url= http://developer.couchbase.com/open-source-projects |title=Couchbase Open Source Projects |work= Couchbase web site |accessdate= October 7, 2016 }}&lt;/ref&gt; and an edition for commercial use.&lt;ref&gt;{{cite web|url=http://www.couchbase.com/couchbase-server/editions|title=Couchbase Server Editions|publisher= Couchbase }}&lt;/ref&gt; 
Couchbase Server builds are available for Ubuntu, Debian, Red Hat, SUSE, Oracle Linux, [[Microsoft Windows]] and Mac OS X operating systems.

Couchbase has supported software developers' kits for the programming languages [[.NET Framework|.Net]], [[PHP]], [[Ruby (programming language)|Ruby]], [[Python (programming language)|Python]], [[C (programming language)|C]], [[Node.js]], [[Java (programming language)|Java]], and [[Go (programming language)|Go]].

==N1QL==
A [[query language]] called the non-first normal form query language, N1QL (pronounced nickel), is used for manipulating the JSON data in Couchbase, just like SQL manipulates data in RDBMS. It has SELECT, INSERT, UPDATE, DELETE, MERGE statements to operate on JSON data.
It was announced in March 2015 as "SQL for documents".&lt;ref&gt;{{Cite web |title= Ssssh!  don’t tell anyone but Couchbase is a serious contender: Couchbase Live Europe 2015 |author= Andrew Slater |date= March 24, 2015 |accessdate= September 19, 2016 }}&lt;/ref&gt;

The N1QL [[data model]] is [[Database normalization#Non-first normal form .28NF.C2.B2 or N1NF.29|non-first normal form]] (N1NF) with support for nested attributes and domain-oriented [[Database normalization|normalization]].  The N1QL data model is also a proper superset and generalization of the [[relational model]].

===Example===
&lt;source lang="json"&gt;
{
  "email":"testme@gmail.com",
  "friends":[
            {"name":"rick"},
            {"name":"cate"}
           ]
}
&lt;/source&gt;

;Like Query: {{code|2=sql|SELECT * FROM `bucket` WHERE LIKE "%@gmail.com";}}

;Array Query: {{code|2=sql|1=SELECT * FROM `bucket` WHERE ANY x IN friends SATISFIES x.name = "cate" END;}}

==Bibliography==
* {{cite book |last=Brown |first=MC |editor-first=|editor-last=|title=Getting Started with Couchbase Server (1st edition) |publisher=O'Reilly Media |date=June 22, 2012 |page=88 | isbn=978-1449331061}}
* {{citation
| first1    = David
| last1     = Ostrovsky
| first2   = Mohammed
| last2   = Haji
| first3  = Yaniv
| last3   = Rodenski
| date      = November 26, 2015
| title     = Pro Couchbase Server 2nd ed.
| edition   = 2nd
| publisher = [[Apress]]
| page     = 349
| isbn      = 978-1484211861
}}
* {{citation
| first1    = Henry
| last1     = Potsangbam
| date      = November 23, 2015
| title     = Learning Couchbase
| edition   = 1st
| publisher = [[Packt]]
| page     = 202
| isbn      = 978-1785288593
}}
* {{citation
| first1    = Deepak
| last1     = Vohra
| date      = August 3, 2015
| title     = Pro Couchbase Development: A NoSQL Platform for the Enterprise
| edition   = 1st
| publisher = [[Apress]]
| page     = 331
| isbn      = 978-1484214350
}}

==References==
{{Reflist}}

==External links==
*{{Official website}}

[[Category:Free database management systems]]
[[Category:Distributed computing architecture]]
[[Category:NoSQL]]
[[Category:Cross-platform software]]
[[Category:Structured storage]]
[[Category:Client-server database management systems]]
[[Category:Database-related software for Linux]]
[[Category:Applications of distributed computing]]
[[Category:Databases]]
[[Category:Data management]]
[[Category:Distributed data stores]]</text>
      <sha1>90d8mq5ts1ar436d5ir0uel4ynvae23</sha1>
    </revision>
  </page>
  <page>
    <title>National Data Repository</title>
    <ns>0</ns>
    <id>30966530</id>
    <revision>
      <id>760848524</id>
      <parentid>754608711</parentid>
      <timestamp>2017-01-19T12:50:15Z</timestamp>
      <contributor>
        <ip>81.29.45.182</ip>
      </contributor>
      <comment>Updated figures for Norwegian NDR</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="37149" xml:space="preserve">A '''National Data Repository''' ('''NDR''') is a data bank that seeks to preserve and promote a country’s natural resources data, particularly data related to the petroleum [[exploration and production]] (E&amp;P) sector.

A National Data Repository is normally established by an entity that governs, controls and supports the exchange, capture, transference and distribution of E&amp;P information, with the final target to provide the State with the tools and information to assure the growth, govern-ability, control, independence and sovereignty of the industry.

The two fundamental reasons for a country to establish an NDR are to '''preserve''' data generated inside the country by the industry, and to '''promote''' investments in the country by utilizing data to reduce the exploration, production, and transportation business risks.

Countries take different approaches towards preserving and promoting their natural resources data. The approach varies according to a country’s natural resources policies, level of openness, and its attitude towards foreign investment.

==Data types==
NDRs store a vast array of data related to a country’s natural resources. This includes wells, [[Well logging|well log data]], well reports, [[core sample]]s, [[seismic]] surveys, [[Seismic inversion#Post-stack seismic resolution inversion|post-stack seismic]], field data/tapes, seismic (acquisition/processing) reports, [[Oil production|production]] data, [[geological map]]s and reports, license data and [[geologic modeling|geological models]].

==Funding models==
Some NDRs are financed entirely by a country’s government. Others are industry-funded. Still some are hybrid systems, funded in part by industry and government.
NDRs typically charge fees for data requests and for data loading. The cost differs significantly between countries. In some cases an annual membership is charged to oil companies to store and access the data in the NDR.

==Standards body==
[[Energistics]] is the global energy standards resource center for the upstream oil and gas industry.

Energistics National Data Repository Work Group:
The standards body is Energistics.&lt;ref&gt;[http://energistics.org/energistics-standards-directory Energistics]&lt;/ref&gt;

===Energistics-standards-directory===
Global regulators of upstream oil and natural gas information, including seismic, drilling, production and reservoir data, formed the National Data Repository (NDR) Work Group in 2008 to collaborate on the development of data management standards and to assist emerging nations with hydrocarbon reserves to better collect, maintain and deliver oil and gas data to the public and to the industry.

Ten countries, led by the [[Netherlands]], [[Norway]] and the [[United Kingdom]], formed NDR to share best practices and to formalize the development and deployment of data management standards for regulatory agencies. The other countries involved in the NDR Work Group’s formation are [[Australia]], [[Canada]], [[India]], [[Kenya]], [[New Zealand]], [[South Africa]] and the [[United States]].

Annual NDR Conference: Approximately every 18 months Energistics organizes a National Data Repository Conference. The purpose is to provide government and regulatory agencies from around the world an opportunity to attend a series of workshops dedicated to developing data exchange standards, improving communications with the oil and gas industry and learning data management techniques for natural resources information.&lt;ref&gt;[http://www.energistics.org/regulatory/national-data-repository-ndr-work-group/ndr-meetings NDR Conference page on the Energistics website]&lt;/ref&gt;

===Society of Exploration Geophysicists and The International Oil and Gas Producers Association===
The SEG is the custodian of the SEG standards which are used for the exchange, retention and release of seismic data.  They are commonly used by National Data Repositories with the SEGD and SEGY being the field and processed exchange standards respectively.

==NDRs around the world==

[https://www.google.com/maps/d/viewer?mid=1by9vDDoWwnZD0f8vNt_le2TThTU Click here to see a map of the NDRs around the world]

{| class="wikitable sortable"
|-
! Country  !!  Name  !!  Agency  !!  Scope  !! Status !!   Purposes   !! Data types/volumes !! Standards used !! Funding !! Website
|-
| {{flagcountry|Algeria}} || Banque de Données Nationale "BDN" || Agence Nationale pour la Valorisation des Ressources en Hydrocarbures (ALNAFT) || Onshore and Offshore Algeria || Ongoing project - agency created by new law in 2005 || Custodian of all E&amp;P data of the country || Cultural, Seismic 2D &amp; 3D, Wells, Data Wells, Wells report, Production, Facilities, Economical and Fiscality, Interpretation, Physical assets index, Data drilling, Transcription, Vectorisation, digitalization || ASCII, SEGY, UKOOA, LAS, DLIS, LIS, PDS, BIT, RODE, PDF, TIF....etc || Government funding/Agency revenue || http://www.alnaft.gov.dz/
|-
| {{flagcountry|Colombia}} || EPIS || Agencia Nacional de Hidrocarburos (ANH) || Onshore and offshore Colombia || Created originally for Ecopetrol and transferred to ANH when it was established in 2003. New system launched December 2009 || Promote and preserve all the technical E&amp;P information assets of the country || wells, surveys, licenses, seismic sections, well reports, maps || REST Web services || Government funding ||http://www.epis.com.co
|-
|  {{flagcountry|Canada}} || CNSOPB || Nova Scotia Offshore Petroleum Board – Geoscience Research Centre- Digital Data Management Centre (DMC) || Offshore Nova Scotia, Canada ||  Operational since 2007 || To provide an effective &amp; efficient system for the management of digital petroleum data, assist explorers in easily obtaining access to large volumes of data via the web, Data Preservation and Data Distribution ||  Wells, well log curves, well reports, cores and samples, field data/tapes, seismic (acquisition/processing) reports, production data, interpretative maps and reports || LAS, DLIS, SEGY || Funded 50/50 by the Federal and Provincial Governments with some funds from industry through cost recovery  || http://www.cnsopb.ns.ca/
|-
| {{flagcountry|Australia}} || PIMS ||Geoscience Australia || ||Active  || Various online and web based systems exist for E &amp;P, geosciences
||Wells, well log curves, well reports, cores and samples, field data/tapes, seismic (acquisition/processing) reports, production data, interpretative maps and reports ||  ||  ||
http://dbforms.ga.gov.au/pls/www/npm.pims_web.search
|-
| {{flagcountry|Western Australia}} || WAPIMS ||Government of Western Australia   || || Active || WAPIMS is a petroleum, geothermal and minerals exploration database ||Contains data on titles, wells, geophysical surveys and other petroleum exploration and production data submitted to DMP by the petroleum industry.
  ||  ||  || http://dmp.wa.gov.au
|-
| {{flagcountry|New South Wales}} ||  ||Government of New South Wales ||  || Active  || Various online geoscience databases to assist New South Wales including DIGS  ||  ||  ||  ||
http://www.dpi.nsw.gov.au/minerals
http://digsopen.minerals.nsw.gov.au/
|-
| {{flagcountry|Northern Territory}} ||  || Government of Northern Territory  ||  || Active  || Various online geoscience databases to assist Northern Territories ||  ||Wells, well log curves, well reports, cores and samples, field data/tapes, seismic (acquisition/processing) reports, production data, interpretative maps and reports  ||  ||
http://www.nt.gov.au/d/Minerals_Energy/index.cfm?header=Petroleum	
|-
| {{flagcountry|Queensland}} ||  || Government of Queensland ||  || Active  || Various online geoscience databases to assist Queensland including Q-DEX ||  ||Wells, well log curves, well reports, cores and samples, field data/tapes, seismic (acquisition/processing) reports, production data, interpretative maps and reports  ||  ||
https://www.dnrm.qld.gov.au/
|-
| {{flagcountry|South Australia}} || SARIG || Government of South Australia  ||  || Active  || Various online geoscience databases to assist South Australia such as PEP-SA  || ||Wells, well log curves, well reports, cores and samples, field data/tapes, seismic (acquisition/processing) reports, production data, interpretative maps and reports  ||  ||
http://petroleum.statedevelopment.sa.gov.au/data_and_publications/sarig
https://sarig.pir.sa.gov.au/
|-
| {{flagcountry|Tasmania}} ||  ||  ||  ||   ||Various online geoscience databases to assist Tasmania   || || Active ||  ||
http://www.mrt.tas.gov.au/portal/page?_pageid=35,1&amp;_dad=portal&amp;_schema=PORTAL		
|-
| {{flagcountry|China }} || CNPC  || Chinese National Petroleum Corporation  ||  ||   || Various oil companies in China with CNPC the largest and parent of Petrochina ||  ||  ||  ||
http://www.cnpc.com.cn/en/
http://www.petrochina.com.cn/ptr/
http://www.cnooc.com.cn/
http://english.sinopec.com/index.shtml
|-
| {{flagcountry|Russia}} ||  ||  Sakhalin, DIGC RDC ||  || Various oil companies in Russia the largest being Rosneft which is state owned  ||  ||  ||  ||  ||
http://www.rosneft.com
http://www.lukoil.com
http://www.tnk-bp.com/en/
http://www.surgutneftegas.ru/
http://www.gazprom-neft.com/
http://www.tatneft.ru/wps/wcm/connect/tatneft/portal_rus/homepage/
|-
| {{flagcountry|Indonesia}} || Indonesia's National Data Centre (NDC) for petroleum, energy and minerals data || Agency for Research and Development in the Ministry of Energy and Mineral Resources of the Republic of Indonesia||Onshore &amp; Offshore || In 1997 Indonesia established Migas Data Management (MDM) operated by PT. Patra Nusa Data (PND)  || PND manages and promotes petroleum investment opportunities by compiling and value adding available petroleum data and information. ||  ||  ||  || http://www.patranusa.com/
|-
|  {{flagcountry|New Zealand}} || New Zealand Online Exploration Database || New Zealand Petroleum &amp; Minerals, Ministry of Business Innovation &amp; Employment || New Zealand onshore and offshore out to the outer continental shelf.  || Opened to public in April 2007. || Data preservation, Investment facilitation, aid in monitoring regulatory compliance, maximise the return to the nation by informing public policy and business strategy. || Wells, well log curves, petroleum reports (includes wells and surveys), mineral reports, coal reports, cores and samples, seismic surveys, post-stack seismic, field data/tapes, seismic acquisition/processing reports, geophysical and geochemical data acquired in mineral and coal exploration (incorporated as enclosures to reports), VSP (incorporated as enclosures to reports), Seismic survey observer logs. GIS data and projects (minerals and coal). Estimated total NDR Size: 2.5 TB loaded, 3.0 TB staged for loading, 40 TB field data offline.  || Closely follow Australian digital reporting standards. No naming standards for wells and surveys.  || 50% Government funding, 50% third party permit (license) fees paid by exploration companies. || https://data.nzpam.govt.nz
|-
| {{flagcountry|Jordan}} ||NRA  || Jordan Natural Resources Authority (NRA) ||Onshore || Active    || Online data room allows users to browse and select large data set quickly in a controlled and secure environment ||Reserves land records, field data, maps, engineering, seismic data, geological studies and well files.  ||  ||  || http://www.jordan.gov.jo
|-
| {{flagcountry|Angola}} ||  || Sonangol || Offshore Angola || Active || Promotion, Organisation &amp; Management of all Exploration &amp; Production (E&amp;P) Data of Angola || Wells, surveys, licenses, seismic sections, well reports, maps ||  || Norad/OfD and NPD assistance || http://www.sonangol.co.ao
|-
| {{flagcountry|France}} ||BEPH ||  || French Territory || || Interactive maps of French territory of oil data are available to Internet users which includes: Permits for petroleum exploration, seismic exploration, oil drilling  (data, documents available) || Wells, Surveys, Licenses, Seismic Sections, Well Reports, Maps ||  ||  || http://www.beph.net/
|-
| {{flagcountry|São Tomé and Príncipe }} || ANP-STP  || National Petroleum Agency of São Tomé &amp; Principe (ANP-STP)  || Offshore || ||  ||  ||  ||Norad/OfD and NPD assistance  || http://www.anp-stp.gov.st
|-
| {{flagcountry|Tanzania}} || TPDC  || Tanzania Petroleum Development Corp  ||  || Began in the early 1990s with Norwegian assistance || An E &amp; P data archive centre || Geophysical survey data, Geological studies, Well drilling and completion reports, Cores and drill steam data ||  || Norad/OfD and NPD assistance || http://www.tpdc-tz.com
|-
| {{flagcountry|Oman}} || OGDR || Department of Petroleum Concession, Ministry of Oil and Gas || Onshore &amp; Offshore || Operational, tendering OGDR as a managed service (fully outsourced) June 2015 || Preservation of E&amp;P data, support concession promotion. || Well-related Data: Header, deviation, tops, field and processed logs, well documents. Seismic-related Data: Field and processed 2D/3D, Gravimag, VSP. || OGDR Data Submission Standard that uses industry standards where possible i.e. DLIS, SEG, UKOOA. || Government &amp; concession holders.||  http://www.mog.gov.om/english/tabid/309/Default.aspx
|-
| {{flagcountry|Netherlands}} || DINO || The Geological Survey of the Netherlands, a division of [[Netherlands Organisation for Applied Scientific Research|TNO]] || The Netherlands including offshore waters ||Started in 2004. Currently BRO is being planned to succeed DINO. || To archive subsurface data of the Netherlands in one repository and provide easy access to the data to encourage multiple use of data.||  || WMS web services. DINO uses own naming conventions || 100% Government funding || http://www.nlog.nl/en/home/NLOGPortal.html
|-
| {{flagcountry|India}} ||DGH  || Directorate General of Hydrocarbons (DGH) ||  || Active - scheduled operation by April 2015
 ||  Establishing national data archival, improving data quality and access for quality exploration covering large area under exploration and providing basis for long term energy policy formulation as well as support OALP  ||Wells, Well Logs, Cores, Scanned core images, Seismic, Reports, production, Technical Reports
  ||  ||Government of India || http://www.dghindia.org/DataManagement.aspx#
|-
| {{flagcountry|Sri Lanka}} || PRDS || Ministry of Petroleum and Petroleum Resources Development ||  || Active since 2009 ||   The PRDS developed a website to disseminate petroleum data and information to public and to investors to assist promotion of offshore areas to attract investors for petroleum exploration ||Wells, surveys, licenses, seismic sections, well reports, maps. Data historic and current, archived on different media (paper, mylar, magnetic tape) || ||  || http://www.prds-srilanka.com/data/onlineData.faces
|-
| {{flagcountry|Argentina}} || ENARSA || Energia Argentina SA  || || Established in 2006  ||  ||  ||  ||  ||  http://www.enarsa.com.ar http://energia.mecon.gov.ar/upstream/US_Pterminados.asp
|-
| {{flagcountry|Peru}} || PeruPetro ||  ||  || Active ||  ||  || ||  || http://www.perupetro.com.pe
|-
| {{flagcountry|Kazakhstan}} ||  || Ministry of Energy and Mineral Resources of the Republic of Kazakhstan (MEMR) ||  || Active ||  ||  ||  ||  || http://www.petrodata.kz
|-
| {{flagcountry|Pakistan}} || PPEPDR || Directorate General Petroleum Concessions (DGPC)  ||  || Active since 2001 ||  || Repository contains more than 10 terabytes of secure petrotechnical data ||  ||  || http://www.ppepdr.net/
|-
| {{flagcountry|Nigeria}} ||Department of Petroleum Resources  ||  ||  || Active since December 2003.
 || Preserve, maintain the integrity and promote the National E&amp;P data assets with improved quality, efficiency and accessibility in the most rapid, secure and reliable manner|| || International and PetroBank data management standards || Funded by Establishment Costs - one-off funding by Government and Running Costs - Subscription &amp;  Transaction Fees by Operators ||http://ndr.dprnigeria.com/
|-
| {{flagcountry|Turkey}} || PetroBank MDS  || Turkish Petroleum Corporation (TPAO). It is NOC of Turkey. || 36˚-42˚ northern parallel and the 26-45˚ eastern meridian.  || Operational since 2007 || Data assets preservation, easy access to assets, assets access controlling and auditing, consolidation of assets, national archive, central management of all assets, standardization of assets according to international standards and naming conventions, working with the most convenient assets. || Wells, Well log curves, well reports, cores and samples, seismic surveys, post-stack seismic, field data/tapes and seismic acquisition/processing reports. || International and PetroBank data management standards || Funded fully by the Turkish Petroleum Corporation. Service usage is free of charge.  || http://www.tpao.gov.tr
|-
| {{flagcountry|Norway}} || DISKOS- Norwegian National Data Repository || Norwegian Petroleum Directorate (NPD) and DISKOS Group of oil companies || Norwegian continental shelf || Started in 1995 || To ensure compliance with NPD reporting regulations for digital E&amp;P data. To reduce data redundancy. To ensure that data is made generally available to the oil and gas industry and to society as a whole Long term preservation of data.
 || Wells, Well Log Curves, Seismic Surveys, field, pre-stack &amp;  post-stack seismic, seismic reports, production data (monthly allocated).Size of NDR estimated at more than 3 Petabytes.
 || SEG-D for seismic field data, SEG-Y for pre-stack and post-stack seismic data (currently only limited amounts of field and pre-stack data) All relevant well data standards such as LIS, DLIS, LAS, SPLA, SCAL etc. PDF and TIF are also used. || Costs are shared equally between all participating oil companies (around 50) in the Diskos consortium, including the NPD. In addition reporting companies pay to submit and download data. All Norwegian universities have free access to public data in Diskos. Non-oil companies can apply for Associated Membership, there are currently around 25 such members. ||http://www.diskos.no/ http://www.npd.no
|-
| {{flagcountry|United Kingdom}} || CDA || CDA Common Data Access Ltd  || UK Offshore Waters || Wells went live in 1995. Infrastructure started operations in 2000. Seismic went live in 2009. Estimated NDR size: 6 Terabytes||Save costs for licenses,Improve access to data,Comply with regulations || Well log curves, Well reports, Post-stack seismic, Seismic reports, VSP, deviation and test data. Estimated NDR size: 6 Terabytes || CDA has adopted DECC’s naming standards for wells and surveys and continues to work closely with DECC and industry to identify a range of standards (see the CDA and DECC websites for more on this) || Owned by the UK oil and gas industry ||
http://www.ukoilandgasdata.com
http://www.gov.uk/oil-and-gas-petroleum-operations-notices
http://www.cdal.com
|-
| {{flagcountry|United Kingdom}} || UKOGL || UK Onshore Geophysical Library || UK onshore || In operation since 1994. Managed and operated by Lynx Information Systems Ltd on behalf of UKOGL. || Custodian of all UK onshore seismic data || Seismic, well tops, logs, cultural. Current archive size approx 6TB || SEGY, UKOOA, LAS, DLIS || Self-funded through data sales ||
http://www.ukogl.org.uk
http://maps.lynxinfo.co.uk/UKOGL_LIVE/map.html
|-
| {{flagcountry|Brazil}} || ANP || Agência Nacional do Petróleo (ANP)||  || BDEP formed in May 2000
 || ||Stores seismic, well log, post stack and pre-stack seismic data and potential field data(Grav/Mag)  || ANP standards in place || Funded by Members || http://www.bdep.gov.br
|-
| {{flagcountry|Mexico}} || Ditep || Pemex ||  ||Established in 2002  || || Promotes and preserve all the technical E&amp;P information assets of the country ||  || ||http://www.pep.pemex.com/index.html
|-
|  {{flagcountry|Israel}} ||  || The Ministry of National Infrastructures  || || Exploratory ||  ||  ||  ||  || http://www.mni.gov.il/mni/en-US/NaturalResources/OilandgasExploration/OilMaps/
|-
| {{flagcountry|Cyprus}} || MCIT || Ministry of Commerce, Industry and Tourism-Energy Service || Offshore  || Promotional || Responsible for granting licences for prospecting, exploration and exploitation of hydrocarbons ||  ||  ||  || http://www.mcit.gov.cy/mcit/mcit.nsf/dmlhexploration_en/dmlhexploration_en?OpenDocument
|-
| {{flagcountry|South Africa}} ||   || Petroleum Agency of South Africa ||  || Active || Seismic data, Well data, Samples, reports and diagrams ||  ||Standards: Formats – SEGD, SEGY, LIS, LAS, PDF and TIFF, Media – 3480, 3590, DLT, 8mm Exabyte, DAT   || From 2010 funded by Government || http://www.petroleumagencysa.com
|-
| {{flagcountry|Kenya}} || National Data Center (NDC) || National Oil Corporation of Kenya || Offshore and Onshore || Began in 2007, system implemented in 2010.  || Digital data preservation, National archive, to implement integrated data management systems, provide easy access to quality-controlled data for internal and external customers, attract oil and gas exploration investment and to reduce data management costs. || Wells, well log curves, well reports, post-stack seismic, field data/tapes, seismic acquisition/processing reports, interpretive maps and reports. || Seismic data- SEGY. 3590 or 3592 data cartridges.  || 100% Government Funded || http://www.nockenya.co.ke/
|-
| {{flagcountry|United States}} || BOEMRE || [[Bureau of Ocean Energy Management, Regulation and Enforcement|Bureau of Ocean Energy, Management, Regulation and Enforcement (BOEMRE)]] || Gulf of Mexico || Has replaced the former Minerals Management Service (MMS) ||  ||  ||  ||  || http://www.gomr.boemre.gov/homepg/data_center.html
|-
| {{flagcountry|United States}} || NGRDS || National Geoscience Data Repository System (NGDRS) ||  ||  || NGRDS is a system of geoscience data repositories, providing information about their respective holdings accessible through a web-based supercatalog. ||  || geologic, geophysical, and engineering data, maps, well logs, and samples || DOE has provided funds for the NGDRS since 1993  ||
http://www.agiweb.org/ngdrs/index.html
http://www.energy.gov/
http://www.agiweb.org/index.html

List of Repositories in US listed also as directory
http://www.agiweb.org/ngdrs/overview/datadirectory.html
|-
| {{flagcountry|Cambodia}} ||CNPA ||Cambodia National Petroleum Authority  || Onshore &amp; Offshore  ||  ||Promotion and preservation of technical E&amp;P information assets of the country  || ||  ||Norad/OfD and NPD assistance || http://www.cnpa-cambodia.com/
|-
| {{flagcountry|Afghanistan}} ||MOM ||Ministry of Mines of the Islamic Republic of Afghanistan (MoM)
|| Onshore ||  ||Promotion and preservation of technical E&amp;P information assets of the country  || ||  ||Norad/OfD and NPD assistance ||http://mom.gov.af/en/news/1637
|-
| {{flagcountry|Bangladesh}} || MOEMR  ||Hydrocarbon Unit, Ministry of Power, Energy and Mineral Resources (MOEMR)|| Onshore &amp; Offshore || Active and ongoing via HCU unit since 2005
|| A mini-data bank has established in the HCU to handle Production data, Resource data by using Database &amp; GIS Software 2005 and promotion of technical E&amp;P information assets of the country  || ||Funded assistance    ||Norad/OfD and NPD assistance ||
http://www.hcu.org.bd/
http://www.petrobangla.org.bd
http://www.bapex.com.bd
|-
| {{flagcountry|Ethiopia}} || MOME  ||Ministry of Mines and Energy Ethiopia  ||  || Active and ongoing ||Promotion of technical E&amp;P information assets of the country   || ||  || ||
http://www.mome.gov.et/petroleum.html
|-
| {{flagcountry|Cameroon}} ||SNH ||SNH Cameroon   ||  || Active &amp; Ongoing || Preservation and promotion of technical E&amp;P information assets of the country  || ||  || || http://www.snh.cm
|-
| {{flagcountry|Malaysia}} ||PIRI  ||Petronas  ||  ||Yet to establish full NDR  || Promotion and preservation of technical E&amp;P information assets of the country  || ||  || || http://www.petronas.com.my
|-
| {{flagcountry|Spain}} || ATH  ||  ||  ||  ||Online GIS databases to geophyscial information SIGEOF and ATH (Archivo de Hydrocarbures) || ||  ||  ||
http://www.mityc.es/energia/petroleo/Exploracion/Paginas/Estadisticas.aspx
http://hidrocarburos.mityc.es/ath/
http://www.igme.es/internet/sigeof/INICIOsiGEOF.htm
|-
| {{flagcountry|Morocco}} ||ONHYM  || Office National des Hydrocarbures et des Mines ||  ||  || Promotion and preservation of technical E&amp;P information assets of the country || ||  ||  || http://www.onhym.com
|-
| {{flagcountry|Madagascar}} ||OMNIS  ||  ||  ||  || Promotion and preservation of technical E&amp;P information assets of the country || ||  ||Norad OfD and NPD assistance    ||
|-
| {{flagcountry|Sudan}} || PIC (Petroleum Information Center) || Ministry of Oil and Gas ||  || Active since 2000  || Preserve and promote E&amp;P data,managing Oil Museum || Wells, well log, well reports, cores and samples,seismic (acquisition/processing) reports, production data,GIS ||  ||  ||
http://www.spc.gov.sd
|-
| {{flagcountry|Morocco}} ||ONHYM  || Office National des Hydrocarbures et des Mines ||  ||  || Promotion and preservation of technical E&amp;P information assets of the country || ||  ||  || http://www.onhym.com
|-
| {{flagcountry|Nicaragua}} ||MEM || ||  ||  Active &amp; ongoing ||  || ||  || Norad OfD and NPD assistance   ||
http://www.ine.gob.ni
http://www.mem.gob.ni
|-
| {{flagcountry|Iraq}} ||MoO ||  Ministry of Oil Republic of Iraq||  || Active and Ongoing since 2005  || MoO establishing a centralized data base and NDR for Iraqi petroleum data and to ensure that data &amp; information from petroleum activities is made available and attract more investors by promoting the petroleum activities || Well logs, Maps, Magnetic tapes, Core &amp; cutting samples, Other geological and geophysical information ||  || Norad/OfD and NPD assistance || http://www.oil.gov.iq
|-
| {{flagcountry|Latvia}} ||LEGMC  ||Latvian Environment, Geology and Meteorology Centre    || Offshore &amp; Onshore || || An E &amp; P data archive centre which provides data available for purchase  || Geological (well and seismic data, maps, reports etc.)  ||  ||  ||  http://mapx.map.vgd.gov.lv/geo3/VGD_OIL_PAGE/index.htm
|-
|  {{flagcountry|Albania}} || AKBN  || National Agency of Natural Resources ||  ||  || Generates and promotes exploration opportunities in Albania, maintains archive of E &amp; P data. ||  ||  ||  || http://www.akbn.gov.al/index.php?ak=details&amp;cid=5&amp;lng=en
|-
| {{flagcountry|Uganda}} ||PEPD|| Petroleum Exploration &amp; Production Dept (PEPD) || Onshore || || || Technical E &amp; P data archive and information ||  ||Norad/Ofd assistance || http://www.statehouse.go.ug/government.php?catId=10 http://www.energyandminerals.go.ug
|-
| {{flagcountry|Zambia}} |||| Ministry of Mines and Minerals Development, Geological Survey Department (GSD)
 || Onshore ||Active and ongoing || || Technical E &amp; P data archive and information - Technical Records Unit||  ||Norad/Ofd &amp; NPD assistance ||
http://www.zambiageosurvey.gov.zm/
|-
| {{flagcountry|Ivory Coast}} ||MME|| Ministry of Mines &amp; Energy || Onshore &amp; Offshore || || || ||  ||Norad/Ofd and NPD assistance ||
http://www.cotedivoirepr.ci/
http://www.petroci.ci/index.php?numlien=31
|-
| {{flagcountry|Romania}} || || National Agency for Mineral Resources  ||  || ||Promotion and preservation of technical E&amp;P information assets of the country  || ||  ||  || http://www.namr.ro/main_en.htm
|-
| {{flagcountry|Fiji}} ||SOPAC ||Mineral Resources Dept Fiji     |||| Created as SOPAC Petroleum Data Bank a cooperative effort with Geoscience Australia  || SOPAC acts as custodian and primary point for E &amp; P data &amp; information preserved on behalf of Pacific Island member nations || Well logs, Maps, Magnetic tapes, Core &amp; cutting samples, Other geological and geophysical information ||  || In part externally managed ||
http://www.mrd.gov.fj/gfiji/
http://www.mrd.gov.fj/gfiji/petroleum/petroleum.html
http://www.ga.gov.au/energy/projects/pacific-islands-applied-geoscience-commission.html
http://www.sopac.org/index.php/member-countries/fiji-islands
|-
| {{flagcountry|Papua New Guinea}} ||SOPAC||Department of Petroleum and Energy ||  || Created as SOPAC Petroleum Data Bank a cooperative effort with Geoscience Australia   || SOPAC acts as custodian and primary point for E &amp; P data &amp; information preserved on behalf of Pacific Island member nations || Well logs, Maps, Magnetic tapes, Core &amp; cutting samples, Other geological and geophysical information ||  || Externally managed ||
http://www.petroleum.gov.pg
http://www.petrominpng.com.pg/about.html
http://www.ga.gov.au/energy/projects/pacific-islands-applied-geoscience-commission.html
http://www.sopac.org/index.php/member-countries/papua-new-guinea
|-
| {{flagcountry|Solomon Islands}} ||SOPAC ||  ||  || Created as SOPAC Petroleum Data Bank a cooperative effort with Geoscience Australia   || SOPAC acts as custodian and primary point for E &amp; P data &amp; information preserved on behalf of Pacific Island member nations  || Well logs, Maps, Magnetic tapes, Core &amp; cutting samples, Other geological and geophysical information ||  || Externally managed ||
http://www.ga.gov.au/energy/projects/pacific-islands-applied-geoscience-commission.html
http://www.sopac.org/index.php/member-countries/solomon-islands
|-
| {{flagcountry|Tonga}} ||SOPAC ||  ||  || Created as SOPAC Petroleum Data Bank a cooperative effort with Geoscience Australia    || SOPAC acts as custodian and primary point for E &amp; P data &amp; information preserved on behalf of Pacific Island member nations || Well logs, Maps, Magnetic tapes, Core &amp; cutting samples, Other geological and geophysical information ||  || Externally managed ||
http://www.ga.gov.au/energy/projects/pacific-islands-applied-geoscience-commission.html
http://www.sopac.org/index.php/member-countries/tonga
|-
| {{flagcountry|Vanuatu}} ||SOPAC ||  ||  || Created as SOPAC Petroleum Data Bank a cooperative effort with Geoscience Australia  || SOPAC acts as custodian and primary point for E &amp; P data &amp; information preserved on behalf of Pacific Island member nations || Well logs, Maps, Magnetic tapes, Core &amp; cutting samples, Other geological and geophysical information ||  || Externally managed ||
http://www.ga.gov.au/energy/projects/pacific-islands-applied-geoscience-commission.html
http://www.sopac.org/index.php/member-countries/vanuatu
|-
| {{flagcountry|Guyana}} ||GGMC  ||Guyana Geology and Mines Commission  ||  ||  || Promotion and preservation of technical E&amp;P information assets of the country || ||  ||  || http://www.ggmc.gov.gy
|-
| {{flagcountry|Syria}} ||SPC  || Syrian Petroleum Company  ||  ||  || || ||  ||  || http://www.spc-sy.com/en/aboutus/aboutus1_en.php
|-
| {{flagcountry|Liberia}} ||NOCAL || National Oil Company of Liberia ||  ||  ||Promotion and preservation of technical E&amp;P information assets of the country   || ||  ||  || http://www.nocal-lr.com/
|-
| {{flagcountry|Chile}} ||ENAP || National Oil Company of Chile ||  ||  ||Promotion and preservation of technical E&amp;P information assets of the country   || ||  ||  || http://www.enap.cl
|-
| {{flagcountry|Thailand}} ||PTTEP ||PTT Exploration and Production Public Company Ltd  ||  ||  ||Promotion and preservation of technical E&amp;P information assets of the country   || ||  ||  || http://www.pttep.com/
http://www.pttep.com/en/index.aspx
|-
| {{flagcountry|Venezuela}} ||PDVSA ||Petroleos de Venezuela  || Onshore &amp; Offshore ||  || Promotion and preservation of technical E&amp;P information assets of the country || ||  ||  || http://www.pdvsa.com/
|-
| {{flagcountry|Trinidad}} || || Trinidad Ministry of Energy and Energy Affairs ||  ||  ||Promotion and preservation of technical E&amp;P information assets of the country   || ||  ||  || http://www.energy.gov.tt/energy_industry.php?mid=31
http://www.petrotrin.com/Petrotrin2007/UpstreamBusiness.htm
|-
| {{flagcountry|Mozambique}} || NAPD ||  ||  || Established in 1999 under NORAD support  || To ensure that data &amp; information from petroleum activities is made available and attract more investors by promoting the petroleum activities || Well logs, Maps, Magnetic tapes, Core &amp; cutting samples, Other geological and geophysical information ||  || National Budget and INP funds || http://www.inp.gov.mz
|-
| {{flagcountry|Denmark}} || || Danish Energy Agency  ||  ||   || Online GIS service for wells and license data  || ||  ||  ||
http://www.ens.dk/EN-US/OILANDGAS/Sider/Oilandgas.aspx
|-
| {{flagcountry|Dominican Republic}} || || Directorate of Hydrocarbons  ||  ||   ||  ||  ||  ||  || http://www.dgm.gov.do/sdhidrocarburo/index.html
|-
| {{flagcountry|Equatorial Guinea}} || ||   ||  ||   ||   Exploration databank for Equatorial Guinea||  ||  || ||
http://www.equatorialoil.com
http://www.equatorialoil.com/database.html
|-
| {{flagcountry|Faroe Islands}} || Jardfeingi || Jardfeingi Faorese Earth and Energy Directorate  ||  ||   ||  Promotion of exploration and licensing rounds ||  ||  || || http://www.jardfeingi.fo
|-
| {{flagcountry|Philippines}} || PNOC || Philippine National Oil Company  ||  ||   ||  Promotion of exploration and licensing rounds ||  ||  || ||
http://www.pnoc.com.ph
http://www.pnoc-ec.com.ph/business.php?id=2
|-
| {{flagcountry|Greenland}} || GreenPetroData || MMR- Ministry of Mineral Resources  ||  ||  ||  Web and GIS system providing access to all Released Well and Geophysical data. ||  ||  || ||
https://www.greenpetrodata.gl/
http://govmin.gl/
|-
| {{flagcountry|Iceland}} || Iceland Continental Shelf Portal (ICSP)  || Orkustofnunn - National Energy Authority   ||Offshore ||   The Iceland Continental Shelf Portal (ICSP)|| Provides access to information about data pertaining to the Icelandic Continental Shelf, in particular initially to the northern Dreki Area to assist with licensing round promotion || ||  ||  ||
http://www.os.is
http://www.nea.is/oil-and-gas-exploration/
|-
| {{flagcountry|Myanmar}} || MOGE || Myanmar Oil &amp; Gas Enterprise   ||  ||   ||    ||  ||  || ||
http://www.energy.gov.mm/upstreampetroleumsubsector.htm
|-
| {{flagcountry|Yemen}} || PEPA || Petroleum Exploration and Production Authority (PEPA)
   ||  ||   ||    ||  ||  || ||
http://www.pepa.com.ye/
|-
| {{flagcountry|Tunisia}} || ETAP || Enterprise Tunisienne D’Activities Petrolieres   ||  ||   ||  Promotion and preservation of technical E&amp;P information assets of the country    ||  ||  || ||
http://www.etap.com.tn
|-
| {{flagcountry|Gabon}} || DGH || Direction Generale des Hydrocarbures (DGH)||  ||   ||  ||  ||  || ||
http://www.gabon-industriel.com/les-actions/energie/petrole
|-
| {{flagcountry|Republic of the Congo  }} || SNPC || Société Nationale des pétroles du Congo ||  ||   ||    ||  ||  || ||
|-
| {{flagcountry|Mali}} || Aurep || {{Not a typo|Autorite}} pour la Promotion de la Recherce des Petroliere au Mali
||  ||   ||   Databank service managing the geological and geophysical data relative to petroleum research. ||  ||  || ||
http://www.aurep.org
http://www.aurep.org/htmlpages/mali.html
|-
| {{flagcountry|Guatemala}} || MEM || Dirección General de Hidrocarbures||  ||   ||  Online maps and images of wells, seismic, licenses, protected areas, exploration and production ||  ||  || ||
http://www.mem.gob.gt/Portal/home.aspx
http://www.mem.gob.gt/Portal/Home.aspx?secid=25
|-
| {{flagcountry|Iran}} || NIOC || National Oil Company of Iran ||  ||   ||   ||  ||  || ||
http://www.nioc.ir
|-
| {{flagcountry|Libya}} || NOC || NOC Libya ||  ||   ||  Virtual data room in place for promotion of exploration and exploitation of hydrocarbons ||  ||  || ||
|-
| {{flagcountry|United Arab Emirates}} || ADNOC || Abu Dhabi National Oil Company ||  ||   ||  ||  ||  ||  ||
http://www.adnoc.ae
|-
| {{flagcountry|Qatar}} ||  || Qatar Petroleum ||  ||   ||  ||  ||  ||  ||
http://www.qp.com.qa
|-
| {{flagcountry|South Korea}} || KNOC || Korea National Petroleum Corporation ||  ||   ||  ||  ||  ||  ||
http://www.knoc.co.kr
|-
| {{flagcountry|Seychelles}} || SNOC || Seychelles National Oil Company ||  ||   ||  ||  ||  ||  ||
|-
| {{flagcountry|Saudi Arabia}} ||  || Saudi Aramco ||  ||   ||  ||  ||  ||  ||
http://www.saudiaramco.com
|-
| {{flagcountry|Belarus}} || ||  ||  ||   ||  ||  ||  ||  ||
http://geologiya.org/index.php?categoryid=14
http://minpriroda.by/ru/napravlenia/minsyrbaza
|-
| {{flagcountry|East Timor}} || LAFAEK || Autoridade Nacional do Petróleo ||  || || Online GIS with wells and licences ||  ||  || Norad/OfD assistance  || http://www.anp-tl.org/webs/anptlweb.nsf/pgMaps
|}

==See also==
*[[Norwegian Petroleum Directorate]]
*[[Energistics]]
*[[Professional Petroleum Data Management Association (PPDM)]]
*[[Oil and gas industry in the United Kingdom]]
*[[Petroleum exploration in Guyana]]

==Notes==
{{Reflist}}

==External links==
*[http://www.energistics.org/regulatory/national-data-repository-ndr-work-group Energistics: National Data Repository Work Group]
*[http://www.kadme.com/wp-content/uploads/KADME-Oil-and-Gas-Technology-Jan2011.pdf National Data Repositories: the case for open data in the oil and gas industry]
*[http://www.seg.org/ts Society of Exploration Geophysicists]

[[Category:Data management]]
[[Category:Open standards]]
[[Category:Hydrocarbons]]
[[Category:Geophysics organizations]]</text>
      <sha1>rhbih2gcj6413ezp1v39iedwdhiyu5s</sha1>
    </revision>
  </page>
  <page>
    <title>GB &amp; Smith</title>
    <ns>0</ns>
    <id>48206970</id>
    <revision>
      <id>751710808</id>
      <parentid>751710679</parentid>
      <timestamp>2016-11-27T11:36:28Z</timestamp>
      <contributor>
        <username>RichardWeiss</username>
        <id>193093</id>
      </contributor>
      <comment>link and remove what smells like advertising</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7921" xml:space="preserve">{{ad|date=August 2016}}
{{Orphan|date=April 2016}}
{{Use dmy dates|date=September 2016}}
{{Infobox company
| name = GB &amp; Smith
| logo = GB &amp; Smith.png
| caption =
| type = Private
| traded_as =
| successor =
| foundation = 2007
| founder =  Sebastien Goiffon and Alexandre Biegala
| defunct =
| location_city =
| location_country =
| location =
| locations =
| area_served =
| key_people =
| industry = Software
| products =
| services =
| revenue =
| operating_income =
| net_income =
| owner =
| num_employees = 70
| parent =
| divisions =
| subsid =
| homepage = {{url|gbandsmith.com}}
| footnotes =
| intl =
}}

'''GB &amp; Smith''' is an independent [[software]] editor which provides layer independent matrix-based console allowing instant visual review on any supported [[computing platform]].

== History ==

GB &amp; Smith was founded in 2007 by Sebastien Goiffon and Alexandre Biegala.&lt;ref&gt;{{cite web|title=Price Entrepreneur of the Year 2014: Winners North region|url=http://business.lesechos.fr/entrepreneurs/success-stories/prix-de-l-entrepreneur-de-l-annee-2014-les-laureats-region-nord-103724.php|publisher=[[Les Échos (newspaper)|Les Échos]]|accessdate=20 March 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=S'implanter à Londres ? Les clés du succès selon GB &amp; Smith|url=http://www.lesechos.fr/02/04/2015/lesechos.fr/0204265266931_s-implanter-a-londres---les-cles-du-succes-selon-gb---smith.htm|publisher=[[Les Échos (newspaper)|Les Échos]]|accessdate=19 March 2016}}&lt;/ref&gt;

As of 2016 company is becoming agnostic gradually offering it is security administration solutions for [[Microsoft]], [[Oracle Corporation|Oracle]], [[IBM]], [[Tableau Software|Tableau]].&lt;ref name=les&gt;{{cite web|title=GB &amp; Smith secures corporate data|url=http://www.lesechos.fr/26/12/2013/LesEchos/21592-075-ECH_gb---smith-securise-les-donnees-d-entreprise.htm|publisher=[[Les Échos (newspaper)|Les Échos]]|accessdate=20 March 2016}}&lt;/ref&gt;

Alexandre Biegala and Sebastien Goiffon invented a method around [[identity access management]] (IAM) to easily review and administer security rights of various applications and over multiple technologies through a single User interface.&lt;ref name=les /&gt;&lt;ref&gt;{{cite web|title=GB &amp; Smith Lille : + 761% de croissance en cinq ans!|url=http://www.lavoixdunord.fr/economie/gb-smith-lille-761-de-croissance-en-cinq-ans-ia0b0n1806557|publisher=[[La Voix du Nord (daily)|La Voix du Nord]]|accessdate=20 March 2016}}&lt;/ref&gt;

GbandSmith was granted a patent for a solution on security administration of rights and has become the Security Administration company and known to be a pioneer in Administration Intelligence and real Self-service security administration.&lt;ref&gt;{{cite web|title=US Patent Issued to GB &amp; Smith on Feb. 10 for "Matrix Security Management System for Managing User Accounts and Security Settings"|url=https://www.highbeam.com/doc/1P3-3585409721.html|website=Highbeam.com|accessdate=19 March 2016}}&lt;/ref&gt;

[[BusinessObjects]] co-founder Denis Payre joined GB &amp; Smith on 1 April 2016. In 1996, Denis Payre and his partner, Bernard Liautaud were ranked by ''Business Week'' among the "Best Entrepreneurs", alongside [[Steve Jobs]] and [[Steven Spielberg]].&lt;ref&gt;{{cite web|title=GB&amp;SMITH Announces Denis Payre, Co-Founder of Business Objects, to Join its Board of Directors|url=http://www.bizjournals.com/prnewswire/press_releases/2016/04/01/NE60812|website=[[The Business Journals]]|accessdate=1 April 2016}}&lt;/ref&gt;

==Solutions==

=== 365Suite Agnostic Self-Service Security Administration ===
365Suite is a set of agnostic tools solutions focused on Security administrations such as access rights management, security policy audits and related metadata. 365Suite enables centralizing security administration into a single console managing multiple applications. 365 runs on top of solutions such as Microsoft [[SharePoint]], [[Microsoft Active Directory]], SAP [[SAP BusinessObjects|BO]], [[SAP HANA|Hana]], Oracle [[Oracle Business Intelligence Suite Enterprise Edition|OBIEE,]] [[Oracle Database|ODB]], [[Tableau Software|Tableau]], etc.&lt;ref&gt;{{cite web|title=Le Comparateur assurance remporte le premier prix du Fast50 avec une croissance de + 1 562% en 4 ans|url=http://www.lavoixdunord.fr/economie/le-comparateur-assurance-remporte-le-premier-prix-du-fast50-ia0b0n3165825|publisher=[[La Voix du Nord (daily)|La Voix du Nord]]|accessdate=20 March 2016}}&lt;/ref&gt;

365 solutions consists in two solutions:
* 365View: Single security administration console to operate multiple IT solutions simultaneously (sharepoint, Oarcle BI).
* 365Eyes: Centralized Metadata repository focused on security administration with ability to operate, monitor, restore and compare metadata from multiple IT solutions.

=== 360Suite ===

360Suite consists in a suite of eight solutions focused around [[SAP SE|SAP]] BusinessObjects:  
* 360Plus: Backup, incremental backup,Promotion, including ability to restore deleted files.&lt;ref&gt;{{cite web|title=Le Français GB &amp; Smith invente le concept prometteur d'administration intelligence|url=http://www.channelnews.fr/le-francais-gb-a-smith-invente-le-concept-prometteur-dadministration-intelligence-21842|publisher=ChannelNews|accessdate=19 March 2016}}&lt;/ref&gt;
* 360View: Security administration, via a security matrix crossing Ressources and Users, Bulk updates (UNV to UNX, unbounded documents)&lt;ref&gt;{{cite web|title=GB &amp; Smith, un esprit de conquête et un esprit libre|url=http://www.lopinion.fr/2-decembre-2014/gb-smith-esprit-conquete-esprit-libre-18979|publisher=[[L'Opinion (newspaper)|L'Opinion]]|accessdate=19 March 2016}}&lt;/ref&gt;
* 360Cast: Schedule and burst dynamically reports.&lt;ref&gt;{{cite web|title=La Société Ugitech Choisit Les Solutions 360suite De Gb &amp; Smith Pour Administrer Sa Plateforme Sap Businessobjects Bi 4.0|url=http://www.decideo.fr/La-Societe-UGITECH-choisit-les-solutions-360suite-de-GB-SMITH-pour-administrer-sa-plateforme-SAP-BusinessObjects-BI-4-0_a6323.html|publisher=Decideo|accessdate=19 March 2016}}&lt;/ref&gt;
* 360Eyes: Explore and analyze BO [[metadata]] and perform impact analysis.
* 360Eyes compliance: Compliance to ensure BO compliance.
* 360Vers: Facilitate and monitor BO versioning.
* 360Bind: Automate BO Non regression tests. With ability to compare results and pixels from Webi, Deski and Crystal reports.
* 360Init: Initialize and import your BO security.

== Recognition ==

* 2013-15 [[Deloitte Fast 500#Fast 500 EMEA|Deloitte EMEA technology Fast 500]].&lt;ref&gt;{{cite web|title=Technology Fast 500 EMEA 2013 Ranking|url=http://www2.deloitte.com/content/dam/Deloitte/global/Documents/Technology-Media-Telecommunications/dttl_TMT-Event-Fast-500-2013-winners-ranking.pdf|publisher=[[Deloitte]].com|accessdate=19 March 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=2015 Technology Fast 500TM Europe, Middle East &amp; Africa (EMEA) Ranking|url=https://www2.deloitte.com/content/dam/Deloitte/global/Documents/Technology-Media-Telecommunications/gx-deloitte-tmt-emea-fast500-2015-rankings.pdf|publisher=[[Deloitte Fast 500]]|accessdate=19 March 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Technology Fast 50|url=http://www2.deloitte.com/content/dam/Deloitte/fr/Documents/technology%20fast%2050/Deloitte_Palmar%C3%A8s-Fast50_2014.pdf|publisher=[[Deloitte]].com|accessdate=19 March 2016}}&lt;/ref&gt;
* 2014 [[Ernst &amp; Young]] emerging company.&lt;ref&gt;{{cite web|title=Sébastien GOIFFON et Alexandre BIEGALA reçoivent le Prix de l'Entreprise d'Avenir de l'Année 2014 pour le Nord de France|url=http://blog.gbandsmith.com/wp-content/uploads/2014/10/cp_resultats_ceremonie_nord_de_france_2014_gbs2_0.pdf|publisher=[[Ey.com]]|accessdate=19 March 2016}}&lt;/ref&gt;

==References==
{{reflist}}

== External links ==
*{{Official website|http://www.gbandsmith.com}}

{{DEFAULTSORT:GB and Smith}}
[[Category:Computer access control]]
[[Category:Business intelligence companies]]
[[Category:Identity management]]
[[Category:Data analysis software]]
[[Category:Data management]]</text>
      <sha1>dnalpre5swuup8pz0grt1k1jukbl8hn</sha1>
    </revision>
  </page>
  <page>
    <title>NoSQL</title>
    <ns>0</ns>
    <id>23968131</id>
    <revision>
      <id>762054203</id>
      <parentid>762054101</parentid>
      <timestamp>2017-01-26T10:16:07Z</timestamp>
      <contributor>
        <ip>212.56.218.234</ip>
      </contributor>
      <comment>Undid revision 762054101 by [[Special:Contributions/212.56.218.234|212.56.218.234]] ([[User talk:212.56.218.234|talk]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="27391" xml:space="preserve">{{Redirect|Structured storage|the Microsoft technology also known as structured storage|COM Structured Storage}}
A '''NoSQL''' (originally referring to "non SQL", "non relational" or "not only SQL")&lt;ref&gt;http://nosql-database.org/ "NoSQL DEFINITION: Next Generation Databases mostly addressing some of the points: being non-relational, distributed, open-source and horizontally scalable"&lt;/ref&gt; database provides a mechanism for [[Computer data storage|storage]] and [[data retrieval|retrieval]] of data which is modeled in means other than the tabular relations used in [[relational database]]s. Such databases have existed since the late 1960s, but did not obtain the "NoSQL" moniker until a surge of popularity in the early twenty-first century,{{r|leavitt}} triggered by the needs of [[Web 2.0]] companies such as [[Facebook]], [[Google]], and [[Amazon.com]].&lt;ref&gt;{{cite conference |title=History Repeats Itself: Sensible and NonsenSQL Aspects of the NoSQL Hoopla |first=C. |last=Mohan |conference=Proc. 16th Int'l Conf. on Extending Database Technology |year=2013 |url=http://openproceedings.eu/2013/conf/edbt/Mohan13.pdf}}&lt;/ref&gt;&lt;ref&gt;http://www.eventbrite.com/e/nosql-meetup-tickets-341739151 "Dynamo clones and BigTables"&lt;/ref&gt;&lt;ref&gt;http://www.wired.com/2012/01/amazon-dynamodb/ "Amazon helped start the “NoSQL” movement."&lt;/ref&gt; NoSQL databases are increasingly used in [[big data]] and [[real-time web]] applications.&lt;ref&gt;{{cite web|url= http://db-engines.com/en/blog_post/23 |title= RDBMS dominate the database market, but NoSQL systems are catching up |publisher= DB-Engines.com |date= 21 Nov 2013 |accessdate= 24 Nov 2013 }}&lt;/ref&gt;   NoSQL systems are also sometimes called "Not only SQL" to emphasize that they may support [[SQL]]-like query languages.&lt;ref&gt;{{cite web|url=http://searchdatamanagement.techtarget.com/definition/NoSQL-Not-Only-SQL|title=NoSQL (Not Only SQL)|quote=NoSQL database, also called Not Only SQL}}&lt;/ref&gt;&lt;ref&gt;{{cite web | url = http://martinfowler.com/bliki/NosqlDefinition.html | title = NosqlDefinition | first = Martin | last = Fowler | authorlink = Martin Fowler | quote = many advocates of NoSQL say that it does not mean a "no" to SQL, rather it means Not Only SQL }}&lt;/ref&gt;

Motivations for this approach include: simplicity of design, simpler [[Horizontal scaling#Horizontal and vertical scaling|"horizontal" scaling]] to [[cluster computing|clusters]] of machines  (which is a problem for relational databases),&lt;ref name="leavitt"&gt;{{cite journal |first=Neal |last=Leavitt |title=Will NoSQL Databases Live Up to Their Promise? |journal=[[IEEE Computer]] |year=2010 |url=http://www.leavcom.com/pdf/NoSQL.pdf}}&lt;/ref&gt; and finer control over availability. The data structures used by NoSQL databases (e.g. key-value, wide column, graph, or document) are different from those used by default in relational databases, making some operations faster in NoSQL. The particular suitability of a given NoSQL database depends on the problem it must solve.  Sometimes the data structures used by NoSQL databases are also viewed as "more flexible" than relational database tables.&lt;ref&gt;http://www.allthingsdistributed.com/2012/01/amazon-dynamodb.html "Customers like SimpleDB’s table interface and its flexible data model. Not having to update their schemas when their systems evolve makes life much easier"&lt;/ref&gt;

Many NoSQL stores compromise [[consistency (database systems)|consistency]] (in the sense of the [[CAP theorem]]) in favor of availability, partition tolerance, and speed. Barriers to the greater adoption of NoSQL stores include the use of low-level query languages (instead of SQL, for instance the lack of ability to perform ad-hoc JOINs across tables), lack of standardized interfaces, and huge previous investments in existing relational databases.&lt;ref&gt;{{cite web
| url         = http://www.journalofcloudcomputing.com/content/pdf/2192-113X-2-22.pdf
| title       = Data management in cloud environments: NoSQL and NewSQL data stores
| first1 = K. | last1 = Grolinger | first2 = W. A. | last2 = Higashino | first3 = A. | last3 = Tiwari | first4 = M. A. M. | last4 = Capretz
| date = 2013
| publisher   = Aira, Springer
| accessdate  = 8 Jan 2014
}}
&lt;/ref&gt;
Most NoSQL stores lack true [[ACID]] transactions, although a few databases, such as [[MarkLogic]], [[Aerospike database|Aerospike]], FairCom [[c-treeACE]], Google [[Spanner (database)|Spanner]] (though technically a [[NewSQL]] database), Symas [[Lightning Memory-Mapped Database|LMDB]], and [[OrientDB]] have made them central to their designs. (See [[#ACID and JOIN Support|ACID and JOIN Support]].)

Instead, most NoSQL databases offer a concept of "eventual consistency" in which database changes are propagated to all nodes "eventually" (typically within milliseconds) so queries for data might not return updated data immediately or might result in reading data that is not accurate, a problem known as stale reads.&lt;ref&gt;https://aphyr.com/posts/322-call-me-maybe-mongodb-stale-reads&lt;/ref&gt;  Additionally, some NoSQL systems may exhibit lost writes and other forms of [[data loss]].&lt;ref&gt;Martin Zapletal: Large volume data analysis on the Typesafe Reactive Platform, ScalaDays 2015, [http://www.slideshare.net/MartinZapletal/zapletal-martinlargevolumedataanalytics Slides]&lt;/ref&gt; Fortunately, some NoSQL systems provide concepts such as [[write-ahead logging]] to avoid data loss.&lt;ref&gt;http://www.dummies.com/how-to/content/10-nosql-misconceptions.html "NoSQL databases lose data" section&lt;/ref&gt; For [[distributed transaction processing]] across multiple databases, data consistency is an even bigger challenge that is difficult for both NoSQL and relational databases. Even current relational databases "do not allow referential integrity constraints to span databases."&lt;ref&gt;https://iggyfernandez.wordpress.com/2013/07/28/no-to-sql-and-no-to-nosql/&lt;/ref&gt; There are few systems that maintain both [[ACID]] transactions and [[X/Open XA]] standards for distributed transaction processing.

== History ==
The term ''NoSQL'' was used by Carlo Strozzi in 1998 to name his lightweight, [[Strozzi NoSQL (RDBMS)|Strozzi NoSQL open-source relational database]] that did not expose the standard [[SQL|Structured Query Language]] (SQL) interface, but was still relational.&lt;ref name=":0"&gt;{{cite web
| url         = http://publications.lib.chalmers.se/records/fulltext/123839.pdf
| title       = Investigating storage solutions for large data: A comparison of well performing and scalable data storage solutions for real time extraction and batch insertion of data
| first       = Adam
| last        = Lith
| first2 = Jakob | last2 = Mattson
| date        = 2010
| publisher   = Department of Computer Science and Engineering, Chalmers University of Technology
| location    = Göteborg
| page        = 70
| accessdate  = 12 May 2011
| quote       = Carlo Strozzi first used the term NoSQL in 1998 as a name for his open source relational database that did not offer a SQL interface[...]
}}
&lt;/ref&gt;  His NoSQL RDBMS is distinct from the circa-2009 general concept of NoSQL databases.  Strozzi suggests that, because the current NoSQL movement "departs from the relational model altogether, it should therefore have been called more appropriately 'NoREL'",&lt;ref&gt;{{cite web|url=http://www.strozzi.it/cgi-bin/CSA/tw7/I/en_US/nosql/Home%20Page |title=NoSQL Relational Database Management System: Home Page |publisher=Strozzi.it |date=2 October 2007 |accessdate=29 March 2010}}&lt;/ref&gt; referring to 'No Relational'.

Johan Oskarsson of [[Last.fm]] reintroduced the term ''NoSQL'' in early 2009 when he organized an event to discuss "open source [[distributed database|distributed, non relational databases]]".&lt;ref&gt;{{cite web|url= http://blog.sym-link.com/2009/05/12/nosql_2009.html |title= NoSQL 2009 |publisher= Blog.sym-link.com |date= 12 May 2009 |accessdate= 29 March 2010 }}&lt;/ref&gt; The name attempted to label the emergence of an increasing number of non-relational, distributed data stores, including open source clones of Google's BigTable/MapReduce and Amazon's Dynamo. Most of the early NoSQL systems did not attempt to provide [[ACID|atomicity, consistency, isolation and durability]] guarantees, contrary to the prevailing practice among relational database systems.&lt;ref&gt;{{cite web|url= http://databases.about.com/od/specificproducts/a/acid.htm |title= The ACID Model| first = Mike | last = Chapple }}&lt;/ref&gt;

Based on 2014 revenue, the NoSQL market leaders are [[MarkLogic]], [[MongoDB]], and [[Datastax]].&lt;ref&gt;{{cite web|accessdate=2015-11-17|url=http://wikibon.com/hadoop-nosql-software-and-services-market-forecast-2013-2017/|title=Hadoop-NoSQL-rankings}}&lt;/ref&gt; Based on 2015 popularity rankings, the most popular NoSQL databases are [[MongoDB]], [[Apache Cassandra]], and [[Redis]].&lt;ref&gt;{{cite web|accessdate=2015-07-31|url=http://db-engines.com/en/ranking|title=DB-Engines Ranking}}&lt;/ref&gt;

== Types and examples of NoSQL databases ==
There have been various approaches to classify NoSQL databases, each with different categories and subcategories, some of which overlap. What follows is a basic classification by data model, with examples:
* '''[[Column (data store)|Column]]''': [[Accumulo]], [[Apache Cassandra|Cassandra]], [[Druid (open-source data store)|Druid]], [[HBase]], [[Vertica]], [[SAP HANA]]
* '''[[Document-oriented database|Document]]''': [[Apache CouchDB]], [[Clusterpoint]], [[Couchbase]], [[DocumentDB]], [[HyperDex]], [[Lotus Notes|IBM Domino]], [[MarkLogic]], [[MongoDB]], [[OrientDB]], [[Qizx]], [[RethinkDB]]
* '''[[Key-value store|Key-value]]''': [[Aerospike database|Aerospike]], [[Couchbase]], [[Dynamo (storage system)|Dynamo]], FairCom [[c-treeACE]], [[FoundationDB]], [[HyperDex]], [[MemcacheDB]], [[MUMPS]], [[Oracle NoSQL Database]], [[OrientDB]], [[Redis]], [[Riak]], [[Berkeley DB]]
* '''[[Graph database|Graph]]''': [[AllegroGraph]], ArangoDB, [[InfiniteGraph]], [[Apache Giraph]], [[MarkLogic]], [[Neo4J]], [[OrientDB]], [[Virtuoso Universal Server|Virtuoso]], [[Stardog]]
* '''[[Multi-model database|Multi-model]]''': Alchemy Database, ArangoDB, CortexDB, [[Couchbase]], [[FoundationDB]], [[MarkLogic]], [[OrientDB]]

A more detailed classification is the following, based on one from Stephen Yen:&lt;ref&gt;{{cite web|url=https://dl.dropboxusercontent.com/u/2075876/nosql-steve-yen.pdf|format=PDF|title=NoSQL is a Horseless Carriage|last=Yen|first=Stephen|publisher=NorthScale|accessdate=2014-06-26}}.&lt;/ref&gt;

{| style="text-align: left;" class="wikitable sortable"
|-
! Type !! Examples of this type
|-
| Key-Value Cache || [[Oracle Coherence|Coherence]], [[IBM WebSphere eXtreme Scale|eXtreme Scale]], [[GigaSpaces]],  GemFire, [[Hazelcast]], [[Infinispan]], JBoss Cache, [[Memcached]], Repcached, [[Terracotta, Inc.|Terracotta]], [[Velocity (memory cache)|Velocity]]
|-
| Key-Value Store || Flare, Keyspace, RAMCloud, SchemaFree, [[Hyperdex]], [[Aerospike database|Aerospike]]
|-
| Key-Value Store (Eventually-Consistent) || DovetailDB, [[Oracle NoSQL Database]], [[Dynamo (storage system)|Dynamo]], [[Riak]], Dynomite, MotionDb, [[Voldemort (distributed data store)|Voldemort]], SubRecord
|-
| Key-Value Store (Ordered) || Actord, [[FoundationDB]], Lightcloud, [[Lightning Memory-Mapped Database|LMDB]], Luxio, [[MemcacheDB]],  NMDB, Scalaris, TokyoTyrant
|-
| Data-Structures Server || [[Redis]]
|-
| Tuple Store || [[Jini|Apache River]], Coord, [[GigaSpaces]]
|-
| Object Database || DB4O, [[Objectivity/DB]], [[Perst]], Shoal, [[Zope Object Database|ZopeDB]]
|-
| Document Store || [[Clusterpoint]], [[Couchbase]], [[CouchDB]], [[DocumentDB]], [[Lotus Notes|IBM Domino]], [[MarkLogic]], [[MongoDB]], [[Qizx]], [[RethinkDB]], [[XML database|XML-databases]]
|-
| [[Wide column store|Wide Column Store]] || [[BigTable]], [[Apache Cassandra|Cassandra]], [[Druid (open-source data store)|Druid]], [[Apache HBase|HBase]], [[Hypertable]], KAI, KDI, OpenNeptune, Qbase
|}

[[Correlation database]]s are model-independent, and instead of row-based or column-based storage, use value-based storage.

=== Key-value store ===
{{main|Key-value database}}
Key-value (KV) stores use the [[associative array]] (also known as a map or dictionary) as their fundamental data model. In this model, data is represented as a collection of key-value pairs, such that each possible key appears at most once in the collection.&lt;ref&gt;{{cite web
| accessdate =1 January 2012
| publisher = Stackexchange
| location = http://dba.stackexchange.com/questions/607/what-is-a-key-value-store-database
| title = Key Value stores and the NoSQL movement
| author = Sandy
| date = 14 January 2011
| url = http://dba.stackexchange.com/a/619
| quote = Key-value stores allow the application developer to store schema-less data. This data usually consists of a string that represents the key, and the actual data that is considered the value in the "key-value" relationship. The data itself is usually some kind of primitive of the programming language (a string, an integer, or an array) or an object that is being marshaled by the programming language's bindings to the key-value store. This structure replaces the need for a fixed data model and allows proper formatting.}}&lt;/ref&gt;&lt;ref&gt;{{cite web
| accessdate =1 January 2012
| publisher = Marc Seeger
| location = http://blog.marc-seeger.de/2009/09/21/key-value-stores-a-practical-overview/
| title = Key-Value Stores: a practical overview
| first = Marc | last = Seeger
| date = 21 September 2009
| url = http://blog.marc-seeger.de/assets/papers/Ultra_Large_Sites_SS09-Seeger_Key_Value_Stores.pdf
| quote = Key-value stores provide a high-performance alternative to relational database systems with respect to storing and accessing data. This paper provides a short overview of some of the currently available key-value stores and their interface to the Ruby programming language.}}&lt;/ref&gt;

The key-value model is one of the simplest non-trivial data models, and richer data models are often implemented as an extension of it. The key-value model can be extended to a discretely ordered model that maintains keys in [[Lexicographical order|lexicographic order]]. This extension is computationally powerful, in that it can efficiently retrieve selective key ''ranges''.&lt;ref&gt;{{cite web
| accessdate =8 May 2014
| publisher = Ilya Katsov
| title = NoSQL Data Modeling Techniques 
| first = Ilya | last = Katsov
| date = 1 March 2012
| url = http://highlyscalable.wordpress.com/2012/03/01/nosql-data-modeling-techniques/}}&lt;/ref&gt;

Key-value stores can use [[consistency model]]s ranging from [[eventual consistency]] to [[serializability]]. Some databases support ordering of keys. There are various hardware implementations, and some users maintain data in memory (RAM), while others employ [[solid-state drive]]s or [[hard disk drive|rotating disks]].

Examples include [[Oracle NoSQL Database]], [[Redis]], and [[dbm]].

=== Document store ===
{{main|Document-oriented database|XML database}}
The central concept of a document store is the notion of a "document". While each document-oriented database implementation differs on the details of this definition, in general, they all assume that documents encapsulate and encode data (or information) in some standard formats or encodings. Encodings in use include XML, [[YAML]], and [[JSON]] as well as binary forms like [[BSON]].  Documents are addressed in the database via a unique ''key'' that represents that document. One of the other defining characteristics of a document-oriented database is that in addition to the key lookup performed by a key-value store, the database offers an API or query language that retrieves documents based on their contents.

Different implementations offer different ways of organizing and/or grouping documents:
* Collections
* Tags
* Non-visible metadata
* Directory hierarchies

Compared to relational databases, for example, collections could be considered analogous to tables and documents analogous to records. But they are different: every record in a table has the same sequence of fields, while documents in a collection may have fields that are completely different.

=== Graph ===
{{main|Graph database}}

This kind of database is designed for data whose relations are well represented as a [[graph (discrete mathematics)|graph]] consisting of elements interconnected with a finite number of relations between them. The type of data could be social relations, public transport links, road maps or network topologies.

; Graph databases and their query language
{| style="text-align: left;" class="wikitable sortable"
 ! Name !! Language(s) !! Notes
 |-
 | [[AllegroGraph]] || [[SPARQL]] || [[Resource Description Framework|RDF]] triple store
 |-
 | [[DEX (Graph database)|DEX/Sparksee]] || [[C++]], [[Java (programming language)|Java]], [[.NET Framework|.NET]], [[Python (programming language)|Python]] || [[Graph database]]
 |-
 | [[FlockDB]] || [[Scala (programming language)|Scala]] || [[Graph database]]
 |-
 | [[IBM DB2]] || [[SPARQL]] || [[Resource Description Framework|RDF]] triple store added in DB2 10
 |-
 | [[InfiniteGraph]] || [[Java (programming language)|Java]] || [[Graph database]]
 |-
 | [[MarkLogic]] || [[Java (programming language)|Java]], [[JavaScript]], [[SPARQL]], [[XQuery]] || Multi-model [[Document-oriented database|document database]] and [[Resource Description Framework|RDF]] triple store
 |-
 | [[Neo4j]] || [[Cypher Query Language|Cypher]] || [[Graph database]]
 |-
 | [[Ontotext|OWLIM]] || [[Java (programming language)|Java]], [[SPARQL|SPARQL 1.1]]|| [[Resource Description Framework|RDF]] triple store
 |-
 |-
 | [[Oracle Database|Oracle]] || [[SPARQL|SPARQL 1.1]] || [[Resource Description Framework|RDF]] triple store added in 11g
 |-
 | [[OrientDB]] || [[Java (programming language)|Java]], SQL || Multi-model [[Document-oriented database|document]] and [[graph database]]
 |-
 | [[sqrrl|Sqrrl Enterprise]] || [[Java (programming language)|Java]] || [[Graph database]]
 |-
 | [[Virtuoso Universal Server|OpenLink Virtuoso]] || [[C++]], [[C Sharp (programming language)|C#]], [[Java (programming language)|Java]], [[SPARQL]] || [[Middleware]] and [[database engine]] hybrid
 |-
 | [[Stardog]] || [[Java (programming language)|Java]], [[SPARQL]] || [[Graph database]]
|}

=== Object database ===
{{main|Object database}}
* [[db4o]]
* [[Gemstone (database)|GemStone/S]]
* [[InterSystems Caché]]
* [[JADE (programming language)|JADE]]
* [[ObjectDatabase++]]
* [[ObjectDB]]
* [[Objectivity/DB]]
* [[ObjectStore]]
* [[Odaba|ODABA]]
* [[Perst]]
* [[Virtuoso Universal Server|OpenLink Virtuoso]]
* [[Versant Object Database]]
* [[ZODB]]

=== Tabular ===
* [[Apache Accumulo]]
* [[BigTable]]
* [[HBase|Apache Hbase]]
* [[Hypertable]]
* [[Mnesia]]
* [[Virtuoso Universal Server|OpenLink Virtuoso]]

=== Tuple store ===
* [[Apache River]]
* [[GigaSpaces]]
* [[Tarantool]]
* [[TIBCO Software|TIBCO]] ActiveSpaces
* [[Virtuoso Universal Server|OpenLink Virtuoso]]

=== Triple/quad store (RDF) database ===
{{main|Triplestore|Named graph}}
* [[AllegroGraph]]
* [[Jena (framework)|Apache JENA]] (It is a framework, not a database)
* [[MarkLogic]]
* [[Ontotext|Ontotext-OWLIM]]
* [[Oracle NoSQL Database|Oracle NoSQL database]]
* [[Virtuoso Universal Server]]
* [[Stardog]]

=== Hosted ===
* [[Amazon DynamoDB]]
* [[Amazon SimpleDB]]
* [[Appengine|Datastore on Google Appengine]]
* [[Clusterpoint|Clusterpoint database]]
* [[Cloudant|Cloudant Data Layer (CouchDB)]]
* [[Freebase (database)|Freebase]]
* [[Microsoft Azure#Table Service|Microsoft Azure Tables]]&lt;ref&gt;http://azure.microsoft.com/en-gb/services/storage/tables/&lt;/ref&gt;
* [[DocumentDB|Microsoft Azure DocumentDB]]&lt;ref&gt;http://azure.microsoft.com/en-gb/services/documentdb/&lt;/ref&gt;
* [[Virtuoso Universal Server|OpenLink Virtuoso]]

=== Multivalue databases ===
* D3 [[Pick database]]
* [[Extensible Storage Engine]] (ESE/NT)
* [[InfinityDB]]
* [[InterSystems Caché]]
* jBASE [[Pick database]]
* [[Northgate Information Solutions]] Reality, the original Pick/MV Database
* [[OpenQM]]
* Revelation Software's [[OpenInsight]]
* [[Rocket U2]]

=== Multimodel database ===

* [[Couchbase]]
* [[FoundationDB]]
* [[MarkLogic]]
* [[OrientDB]]

== Performance ==
Ben Scofield rated different categories of NoSQL databases as follows:&lt;ref&gt;{{cite web|url=http://www.slideshare.net/bscofield/nosql-codemash-2010|title=NoSQL - Death to Relational Databases(?)|last=Scofield|first=Ben |date=2010-01-14|accessdate=2014-06-26}}&lt;/ref&gt;

{| style="text-align: left;" class="wikitable sortable"
|-
! Data Model !! Performance !! Scalability !! Flexibility !! Complexity !! Functionality
|-
| Key–Value Store ||  high || high || high || none || variable (none)
|-
| Column-Oriented Store || high || high || moderate || low || minimal
|-
| Document-Oriented Store || high || variable (high) || high || low || variable (low)
|-
| Graph Database || variable || variable || high || high || [[graph theory]]
|-
| Relational Database || variable || variable || low || moderate || [[relational algebra]]
|}

Performance and scalability comparisons are sometimes done with the [[YCSB]] benchmark.

{{see also|Comparison of structured storage software}}

== Handling relational data ==
Since most NoSQL databases lack ability for joins in queries, the [[database schema]] generally needs to be designed differently. There are three main techniques for handling relational data in a NoSQL database. (See table Join and ACID Support for NoSQL databases that support joins.)

=== Multiple queries ===
Instead of retrieving all the data with one query, it's common to do several queries to get the desired data. NoSQL queries are often faster than traditional SQL queries so the cost of having to do additional queries may be acceptable. If an excessive number of queries would be necessary, one of the other two approaches is more appropriate.

=== Caching/replication/non-normalized data ===
Instead of only storing foreign keys, it's common to store actual foreign values along with the model's data. For example, each blog comment might include the username in addition to a user id, thus providing easy access to the username without requiring another lookup. When a username changes however, this will now need to be changed in many places in the database. Thus this approach works better when reads are much more common than writes.&lt;ref name="DataModeling-Couchbase.com_December_5_2014c"&gt;{{cite web |url=http://www.couchbase.com/sites/default/files/uploads/all/whitepapers/Couchbase_Whitepaper_Transitioning_Relational_to_NoSQL.pdf |title=Making the Shift from Relational to NoSQL
 |newspaper=Couchbase.com |accessdate= December 5, 2014}}&lt;/ref&gt;

=== Nesting data ===
With document databases like MongoDB it's common to put more data in a smaller number of collections. For example, in a blogging application, one might choose to store comments within the blog post document so that with a single retrieval one gets all the comments. Thus in this approach a single document contains all the data you need for a specific task.

== ACID and JOIN Support ==

If a database is marked as supporting [[ACID]] or [[Join (SQL)|joins]], then the documentation for the database makes that claim. The degree to which the capability is fully supported in a manner similar to most SQL databases or the degree to which it meets the needs of a specific application is left up to the reader to assess.
{| class="wikitable"
|-
! Database !! ACID !! Joins
|-
| [[Aerospike (company)|Aerospike]] || {{Yes}} || {{No}}
|-
| ArangoDB || {{Yes}} || {{Yes}}
|-
| [[CouchDB]] || {{Yes}} || {{Yes}}
|-
| [[c-treeACE]] || {{Yes}} || {{Yes}}
|-
| [[HyperDex]] || {{Yes}}{{refn|name=HyperDexAcid|group=nb|HyperDex currently offers ACID support via its Warp extension, which is a commercial add-on.}} || {{Yes}}
|-
| [[InfinityDB]] || {{Yes}} || {{No}}
|-
| [[Lightning Memory-Mapped Database|LMDB]] || {{Yes}} || {{No}}
|-
| [[MarkLogic]] || {{Yes}} || {{Yes}}{{refn|name=MarkLogicJoins|group=nb|Joins do not necessarily apply to document databases, but MarkLogic can do joins using semantics.&lt;ref&gt;http://www.gennet.com/big-data/cant-joins-marklogic-just-matter-semantics/&lt;/ref&gt;}}
|-
| [[OrientDB]] || {{Yes}} || {{Yes}}

|}

{{reflist|group=nb}}

== See also ==
&lt;!-- please do not list specific implementations here --&gt;
* [[CAP theorem]]
* [[Comparison of object database management systems]]
* [[Comparison of structured storage software]]
* [[Correlation database]]
* [[Distributed cache]]
* [[Faceted search]]
* [[MultiValue]] database
* [[Multi-model database]]
* [[Triplestore]]
* [[Schema-agnostic databases]]

== References ==
{{Reflist|33em}}

== Further reading ==
*{{cite book
 | first1 = Pramod | last1 = Sadalage | first2 = Martin | last2 = Fowler | authorlink2 = Martin Fowler
 | date = 2012
 | title = NoSQL Distilled: A Brief Guide to the Emerging World of Polyglot Persistence
 | publisher = Addison-Wesley
 | isbn = 0-321-82662-0
}}
*{{cite book
 | first1 = Dan | last1 = McCreary | first2 = Ann | last2 =Kelly
 | date = 2013
 | title = Making Sense of NoSQL: A guide for managers and the rest of us
 | isbn = 9781617291074
}}
*{{cite book
 | first1 = Lena | last1 = Wiese 
 | date = 2015
 | title =  Advanced Data Management for SQL, NoSQL, Cloud and Distributed Databases 
 | publisher = DeGruyter/Oldenbourg
 | isbn = 978-3-11-044140-6
}}
* {{cite web| first = Christof | last = Strauch | date = 2012|title=NoSQL Databases|url=http://www.christof-strauch.de/nosqldbs.pdf}}
* {{cite journal| last1 = Moniruzzaman | first1 = A. B. | last2 = Hossain | first2 = S. A. | date = 2013|title=NoSQL Database: New Era of Databases for Big data Analytics - Classification, Characteristics and Comparison|arxiv=1307.0191}}
* {{cite journal| first = Kai | last = Orend | date = 2013|title=Analysis and Classification of NoSQL Databases and Evaluation of their Ability to Replace an Object-relational Persistence Layer|citeseerx = 10.1.1.184.483 }}
* {{cite web| first1 = Ganesh | last1 = Krishnan | first2 = Sarang | last2 = Kulkarni | first3 = Dharmesh Kirit | last3 = Dadbhawala | title=Method and system for versioned sharing, consolidating and reporting information|url=https://www.google.com/patents/US7383272?pg=PA1&amp;dq=ganesh+krishnan&amp;hl=en&amp;sa=X}}

== External links ==
* {{cite web|url=http://www.christof-strauch.de/nosqldbs.pdf|title=NoSQL whitepaper| first = Christoph | last = Strauch|publisher=Hochschule der Medien|location = Stuttgart}}
* {{cite web|url=http://nosql-database.org/|title=NoSQL database List| first = Stefan | last = Edlich}}
* {{cite web|year=2010|url=http://www.infoq.com/articles/graph-nosql-neo4j|title=Graph Databases, NOSQL and Neo4j| first = Peter | last = Neubauer}}
* {{cite web|year=2012|url=http://www.networkworld.com/article/2160905/tech-primers/a-vendor-independent-comparison-of-nosql-databases--cassandra--hbase--mongodb--riak.html|title=A vendor-independent comparison of NoSQL databases: Cassandra, HBase, MongoDB, Riak| first = Sergey | last = Bushik|publisher=NetworkWorld}}
* {{cite web|year=2014|url=http://www.odbms.org/category/downloads/nosql-data-stores/nosql-data-stores-articles/|title=NoSQL Data Stores – Articles, Papers, Presentations|first = Roberto V. | last = Zicari|website=odbms.org}}
{{Use dmy dates|date=February 2012}}
{{Databases}}



[[Category:NoSQL| ]]
[[Category:Data management]]
[[Category:Distributed data stores]]
[[Category:Structured storage]]</text>
      <sha1>tahrawjhozu8h7utzb512vontdy6iyt</sha1>
    </revision>
  </page>
  <page>
    <title>SQL injection</title>
    <ns>0</ns>
    <id>526999</id>
    <revision>
      <id>761518788</id>
      <parentid>761154159</parentid>
      <timestamp>2017-01-23T11:29:22Z</timestamp>
      <contributor>
        <username>Ahmedschuh</username>
        <id>22760832</id>
      </contributor>
      <comment>Provided extra resources where you can find additional detailed technical information about the many different variants of the SQL Injection vulnerability.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="40510" xml:space="preserve">{{Use mdy dates|date=February 2012}}
[[File:KD SQLIA Classification 2010.png|thumb|alt=Classification of SQL injection attack vectors in 2010|A classification of SQL injection attacking vector as of 2010.]]

'''SQL injection ''' is a [[code injection]] technique, used  to [[Attack (computing)|attack]] data-driven applications, in which nefarious [[SQL]] statements are inserted into an entry field for execution (e.g. to dump the database contents to the attacker).&lt;ref&gt;{{cite web | url = http://technet.microsoft.com/en-us/library/ms161953%28v=SQL.105%29.aspx | title = SQL Injection | accessdate = 2013-08-04 | author = Microsoft | quote = SQL injection is an attack in which malicious code is inserted into strings that are later passed to an instance of SQL Server for parsing and execution. Any procedure that constructs SQL statements should be reviewed for injection vulnerabilities because SQL Server will execute all syntactically valid queries that it receives. Even parameterized data can be manipulated by a skilled and determined attacker.}}&lt;/ref&gt; SQL injection must exploit a [[security vulnerability]] in an application's software, for example, when user input is either incorrectly filtered for [[string literal]] [[escape sequence|escape characters]] embedded in SQL statements or user input is not [[Strongly-typed programming language|strongly typed]] and unexpectedly executed. SQL injection is mostly known as an attack [[Vector (malware)|vector]] for websites but can be used to attack any type of SQL database.

SQL injection attacks allow attackers to spoof identity, tamper with existing data, cause repudiation issues such as voiding transactions or changing balances, allow the complete disclosure of all data on the system, destroy the data or make it otherwise unavailable, and become administrators of the database server.

In a 2012 study, it was observed that the average web application received 4 attack campaigns per month, and retailers received twice as many attacks as other industries.&lt;ref&gt;{{cite web | url = http://www.imperva.com/docs/HII_Web_Application_Attack_Report_Ed4.pdf | title = Imperva Web Application Attack Report | accessdate = 2013-08-04 | author = Imperva | date = July 2012 | format = PDF | quote = Retailers suffer 2x as many SQL injection attacks as other industries. / While most web applications receive 4 or more web attack campaigns per month, some websites are constantly under attack. / One observed website was under attack 176 out of 180 days, or 98% of the time.}}&lt;/ref&gt;

==History==

The first public discussions of SQL injection started appearing around 1998;&lt;ref&gt;{{cite web |title= How Was SQL Injection Discovered? The researcher once known as Rain Forrest Puppy explains how he discovered the first SQL injection more than 15 years ago. |author= Sean Michael Kerner  |date= November 25, 2013 |url= http://www.esecurityplanet.com/network-security/how-was-sql-injection-discovered.html }}&lt;/ref&gt; for example, a 1998 article in [[Phrack Magazine]].&lt;ref&gt;{{cite journal |title= NT Web Technology Vulnerabilities |author= Jeff Forristal (signing as rain.forest.puppy) |journal= [[Phrack Magazine]] |volume= 8 |issue= 54 (article 8) |date= Dec 25, 1998 |url= http://www.phrack.com/issues.html?issue=54&amp;id=8#article }}&lt;/ref&gt;

==Form==
SQL injection (SQLI) is considered one of the top 10 web application vulnerabilities of 2007 and 2010 by the [[OWASP|Open Web Application Security Project]].&lt;ref&gt;{{cite web|url=https://www.owasp.org/index.php/Category:OWASP_Top_Ten_Project |title=Category:OWASP Top Ten Project |publisher=OWASP |accessdate=2011-06-03}}&lt;/ref&gt; In 2013, SQLI was rated the number one attack on the OWASP top ten.&lt;ref&gt;{{cite web|url=https://www.owasp.org/index.php/Top_10_2013-Top_10 |title=Category:OWASP Top Ten Project |publisher=OWASP |accessdate=2013-08-13}}&lt;/ref&gt; There are four main sub-classes of SQL injection:
* Classic SQLI
* Blind or Inference SQL injection
* [[Database management system]]-specific SQLI
* Compounded SQLI

:* SQL injection + insufficient authentication&lt;ref&gt;{{cite web|url=http://www.xiom.com/whid-2007-60 |title=WHID 2007-60: The blog of a Cambridge University security team hacked |publisher=Xiom |accessdate=2011-06-03}}&lt;/ref&gt;
:* SQL injection + [[DDoS]] attacks&lt;ref&gt;{{cite web|url=http://www.xiom.com/content/whid-2009-1-gaza-conflict-cyber-war |title=WHID 2009-1: Gaza conflict cyber war |publisher=Xiom |accessdate=2011-06-03}}&lt;/ref&gt;
:* SQL injection + [[DNS hijacking]]&lt;ref&gt;[http://www.xiom.com/whid-list/DNS%20Hijacking ] {{webarchive |url=https://web.archive.org/web/20090618125914/http://www.xiom.com/whid-list/DNS%20Hijacking |date=June 18, 2009 }}&lt;/ref&gt;
:* SQL injection + [[Cross-site scripting|XSS]]&lt;ref&gt;{{cite web|url=http://www.darkreading.com/security/management/showArticle.jhtml?articleID=211201482 |title=Third Wave of Web Attacks Not the Last |publisher=Dark Reading |accessdate=2012-07-29}}&lt;/ref&gt;

The [[Storm Worm]] is one representation of Compounded SQLI.&lt;ref&gt;{{cite web|last=Danchev |first=Dancho |url=http://ddanchev.blogspot.com/2007/01/social-engineering-and-malware.html|title=Mind Streams of Information Security Knowledge: Social Engineering and Malware |publisher=Ddanchev.blogspot.com |date=2007-01-23 |accessdate=2011-06-03}}&lt;/ref&gt;

This classification represents the state of SQLI, respecting its evolution until 2010—further refinement is underway.&lt;ref&gt;{{cite web|last=Deltchev|first=Krassen|title=New Web 2.0 Attacks|url=http://www.nds.ruhr-uni-bochum.de/teaching/theses/Web20/|work=B.Sc. Thesis|publisher=Ruhr-University Bochum|accessdate=February 18, 2010}}&lt;/ref&gt;

==Technical implementations==

===Incorrectly filtered escape characters===
This form of SQL injection occurs when user input is not filtered for [[escape character]]s and is then passed into an SQL statement. This results in the potential manipulation of the statements performed on the database by the end-user of the application.

The following line of code illustrates this vulnerability:

 statement = "&lt;source lang="sql" enclose="none"&gt;SELECT * FROM users WHERE name = '&lt;/source&gt;" + userName + "&lt;source lang="sql" enclose="none"&gt;';&lt;/source&gt;"

This SQL code is designed to pull up the records of the specified username from its table of users. However, if the "userName" variable is crafted in a specific way by a malicious user, the SQL statement may do more than the code author intended. For example, setting the "userName" variable as:

&lt;pre&gt;' OR '1'='1&lt;/pre&gt;
or using comments to even block the rest of the query (there are three types of SQL comments&lt;ref&gt;{{citation |title= IBM Informix Guide to SQL: Syntax. Overview of SQL Syntax &amp;gt; How to Enter SQL Comments |publisher= IBM |url= http://publib.boulder.ibm.com/infocenter/idshelp/v10/index.jsp?topic=/com.ibm.sqls.doc/sqls36.htm }}&lt;/ref&gt;). All three lines have a space at the end:
&lt;pre&gt;' OR '1'='1' --
' OR '1'='1' ({
' OR '1'='1' /* &lt;/pre&gt;
.
renders one of the following SQL statements by the parent language:

&lt;source lang="sql"&gt;SELECT * FROM users WHERE name = '' OR '1'='1';&lt;/source&gt;
&lt;source lang="sql"&gt;SELECT * FROM users WHERE name = '' OR '1'='1' -- ';&lt;/source&gt;

If this code were to be used in an authentication procedure then this example could be used to force the selection of every data field (*) from ''all'' users rather than from one specific user name as the coder intended,  because the evaluation of '1'='1' is always true ([[short-circuit evaluation]]).

The following value of "userName" in the statement below would cause the deletion of the "users" table as well as the selection of all data from the "userinfo" table (in essence revealing the information of every user), using an [[API]] that allows multiple statements:

 a';&lt;source lang="sql" enclose="none"&gt;DROP TABLE users; SELECT * FROM userinfo WHERE 't' = 't'&lt;/source&gt;

This input renders the final SQL statement as follows and specified:

&lt;source lang="sql"&gt;SELECT * FROM users WHERE name = 'a';DROP TABLE users; SELECT * FROM userinfo WHERE 't' = 't';&lt;/source&gt;

While most SQL server implementations allow multiple statements to be executed with one call in this way, some SQL APIs such as [[PHP]]'s &lt;code&gt;mysql_query()&lt;/code&gt; function do not allow this for security reasons. This prevents attackers from injecting entirely separate queries, but doesn't stop them from modifying queries.

===Incorrect type handling===
This form of SQL injection occurs when a '''user-supplied''' field is not [[strongly typed]] or is not checked for [[data type|type]] constraints. This could take place when a numeric field is to be used in a SQL statement, but the programmer makes no checks to validate that the user supplied input is numeric. For example:
 statement := "&lt;source lang="sql" enclose="none"&gt;SELECT * FROM userinfo WHERE id = &lt;/source&gt;" + a_variable + ";"

It is clear from this statement that the author intended a_variable to be a number correlating to the "id" field. However, if it is in fact a [[String (computer science)|string]] then the [[end-user]] may manipulate the statement as they choose, thereby bypassing the need for escape characters. For example, setting a_variable to

&lt;pre&gt;1;DROP TABLE users&lt;/pre&gt;

will drop (delete) the "users" table from the database, since the SQL becomes:

&lt;source lang="sql"&gt;SELECT * FROM userinfo WHERE id=1; DROP TABLE users;&lt;/source&gt;

===Blind SQL injection===
Blind SQL Injection is used when a web application is vulnerable to an SQL injection but the results of the injection are not visible to the attacker. The page with the vulnerability may not be one that displays data but will display differently depending on the results of a logical statement injected into the legitimate SQL statement called for that page.
This type of attack has traditionally been considered time-intensive because a new statement needed to be crafted for each bit recovered, and depending on its structure, the attack may consist of many unsuccessful requests. Recent advancements have allowed each request to recover multiple bits, with no unsuccessful requests, allowing for more consistent and efficient extraction. &lt;ref&gt;{{cite web | url = http://howto.hackallthethings.com/2016/07/extracting-multiple-bits-per-request.html | title = Extracting Multiple Bits Per Request From Full-blind SQL Injection Vulnerabilities | publisher = Hack All The Things | accessdate = July 8, 2016 |archiveurl = https://web.archive.org/web/20160708190141/http://howto.hackallthethings.com/2016/07/extracting-multiple-bits-per-request.html |archivedate = July 8, 2016}}&lt;/ref&gt; There are several tools that can automate these attacks once the location of the vulnerability and the target information has been established.&lt;ref&gt;{{cite web | url = http://www.justinclarke.com/archives/2006/03/sqlbrute.html | title = Using SQLBrute to brute force data from a blind SQL injection point | publisher = Justin Clarke | accessdate = October 18, 2008 |archiveurl = http://web.archive.org/web/20080614203711/http://www.justinclarke.com/archives/2006/03/sqlbrute.html &lt;!-- Bot retrieved archive --&gt; |archivedate = June 14, 2008}}&lt;/ref&gt;

====Conditional responses====
One type of blind SQL injection forces the database to evaluate a logical statement on an ordinary application screen. As an example, a book review website uses a [[query string]] to determine which book review to display. So the [[URL]] &lt;code&gt;&lt;nowiki&gt;http://books.example.com/showReview.php?ID=5&lt;/nowiki&gt;&lt;/code&gt; would cause the server to run the query
&lt;source lang="sql"&gt;SELECT * FROM bookreviews WHERE ID = 'Value(ID)';&lt;/source&gt;
from which it would populate the review page with data from the review with [[Identifier|ID]] 5, stored in the [[Table (database)|table]] bookreviews. The query happens completely on the server; the user does not know the names of the database, table, or fields, nor does the user know the query string. The user only sees that the above URL returns a book review. A [[Hacker (computer security)|hacker]] can load the URLs &lt;code&gt;&lt;source lang="sql" enclose="none"&gt;http://books.example.com/showReview.php?ID=5 OR 1=1&lt;/source&gt;&lt;/code&gt; and &lt;code&gt;&lt;source lang="sql" enclose="none"&gt;http://books.example.com/showReview.php?ID=5 AND 1=2&lt;/source&gt;&lt;/code&gt;, which may result in queries
&lt;source lang="sql"&gt;SELECT * FROM bookreviews WHERE ID = '5' OR '1'='1';
SELECT * FROM bookreviews WHERE ID = '5' AND '1'='2';&lt;/source&gt;
respectively. If the original review loads with the "1=1" URL and a blank or error page is returned from the "1=2" URL, and the returned page has not been created to alert the user the input is invalid, or in other words, has been caught by an input test script, the site is likely vulnerable to a SQL injection attack as the query will likely have passed through successfully in both cases. The hacker may proceed with this query string designed to reveal the version number of [[MySQL]] running on the server: &lt;code&gt;&lt;source lang="mysql" enclose="none"&gt;http://books.example.com/showReview.php?ID=5 AND substring(@@version, 1, INSTR(@@version, '.') - 1)=4&lt;/source&gt;&lt;/code&gt;, which would show the book review on a server running MySQL 4 and a blank or error page otherwise. The hacker can continue to use code within query strings to glean more information from the server until another avenue of attack is discovered or his or her goals are achieved.&lt;ref&gt;{{cite web|url=http://forum.intern0t.org/web-hacking-war-games/818-blind-sql-injection.html|title=Blind SQL Injection tutorial|author=macd3v|accessdate=6 December 2012}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=TDSS botnet: full disclosure|url=http://nobunkum.ru/analytics/en-tdss-botnet|accessdate=6 December 2012|author=Andrey Rassokhin|author2=Dmitry Oleksyuk }}&lt;/ref&gt;

===Second order SQL injection===
Second order SQL injection occurs when submitted values contain malicious commands that are stored rather than executed immediately.  In some cases, the application may correctly encode an SQL statement and store it as valid SQL.  Then, another part of that application without controls to protect against SQL injection might execute that stored SQL statement.  This attack requires more knowledge of how submitted values are later used.  Automated web application security scanners would not easily detect this type of SQL injection and may need to be manually instructed where to check for evidence that it is being attempted.

==Mitigation==
An SQL injection is a well known attack and easily prevented by simple measures. After an apparent SQL injection attack on [[TalkTalk Group|Talktalk]] in 2015, the BBC reported that security experts were stunned that such a large company would be vulnerable to it.&lt;ref&gt;{{Cite web|title = Questions for TalkTalk - BBC News|url = http://www.bbc.com/news/technology-34636308|website = BBC News|accessdate = 2015-10-26|language = en}}&lt;/ref&gt;

===Parameterized statements===
{{Main article|Prepared statement}}
With most development platforms, parameterized statements that work with parameters can be used  (sometimes called placeholders or [[bind variable]]s) instead of embedding user input in the statement. A placeholder can only store a value of the given type and not an arbitrary SQL fragment. Hence the SQL injection would simply be treated as a strange (and probably invalid) parameter value.

In many cases, the SQL statement is fixed, and each parameter is a [[Scalar (computing)|scalar]], not a [[Table (database)|table]]. The user input is then assigned (bound) to a parameter.&lt;ref&gt;{{cite web|title=SQL Injection Prevention Cheat Sheet|url=https://www.owasp.org/index.php/SQL_Injection_Prevention_Cheat_Sheet|publisher=Open Web Application Security Project|accessdate=3 March 2012}}&lt;/ref&gt;

====Enforcement at the coding level====
Using [[object-relational mapping]] libraries avoids the need to write SQL code. The ORM library in effect will generate parameterized SQL statements from object-oriented code.

===Escaping===
A straightforward, though error-prone way to prevent injections is to escape characters that have a special meaning in SQL. The manual for an SQL DBMS explains which characters have a special meaning, which allows creating a comprehensive [[Blacklist (computing)|blacklist]] of characters that need translation. For instance, every occurrence of a single quote (&lt;code&gt;'&lt;/code&gt;) in a parameter must be replaced by two single quotes (&lt;code&gt;&lt;nowiki&gt;''&lt;/nowiki&gt;&lt;/code&gt;) to form a valid SQL string literal. For example, in [[PHP]] it is usual to escape parameters using the function &lt;code&gt;mysqli_real_escape_string();&lt;/code&gt; before sending the SQL query:
&lt;source lang="php"&gt;
$mysqli = new mysqli('hostname', 'db_username', 'db_password', 'db_name');
$query = sprintf("SELECT * FROM `Users` WHERE UserName='%s' AND Password='%s'",
                  $mysqli-&gt;real_escape_string($username),
                  $mysqli-&gt;real_escape_string($password));
$mysqli-&gt;query($query);
&lt;/source&gt;

This function prepends backslashes to the following characters: \x00, \n, \r, \, ', " and \x1a.
This function is normally used to make data safe before sending a query to [[MySQL]].&lt;ref&gt;{{cite web|url=http://in2.php.net/manual/en/mysqli.real-escape-string.php|title=mysqli-&gt;real_escape_string - PHP Manual|publisher=PHP.net}}&lt;/ref&gt;&lt;br /&gt; There are other functions for many database types in PHP such as pg_escape_string() for [[PostgreSQL]]. The function &lt;code&gt;addslashes(string $str)&lt;/code&gt; works for escaping characters, and is used especially for querying on databases that do not have escaping functions in PHP.  It returns a string with backslashes before characters that need to be quoted in database queries, etc. These characters are single quote ('), double quote ("), backslash (\) and NUL (the NULL byte).&lt;ref&gt;{{cite web|url=http://pl2.php.net/manual/en/function.addslashes.php|title=Addslashes - PHP Manual|publisher=PHP.net}}&lt;/ref&gt;&lt;br /&gt;
Routinely passing escaped strings to SQL is error prone because it is easy to forget to escape a given string. Creating a transparent layer to secure the input can reduce this error-proneness, if not entirely eliminate it.&lt;ref&gt;{{cite web|url=http://www.xarg.org/2010/11/transparent-query-layer-for-mysql/|title=Transparent query layer for MySQL|publisher=Robert Eisele|date=November 8, 2010}}&lt;/ref&gt;

===Pattern check===
Integer, float or boolean,string parameters can be checked if their value is valid representation for the given type. Strings that must follow some strict pattern (date, UUID, alphanumeric only, etc.) can be checked if they match this pattern.

===Database permissions===
Limiting the permissions on the database logon used by the web application to only what is needed may help reduce the effectiveness of any SQL injection attacks that exploit any bugs in the web application.

For example, on [[Microsoft SQL Server]], a database logon could be restricted from selecting on some of the system tables which would limit exploits that try to insert JavaScript into all the text columns in the database.
&lt;source lang="tsql"&gt;
deny select on sys.sysobjects to webdatabaselogon;
deny select on sys.objects to webdatabaselogon;
deny select on sys.tables to webdatabaselogon;
deny select on sys.views to webdatabaselogon;
deny select on sys.packages to webdatabaselogon;
&lt;/source&gt;

==Examples==
* In February 2002, Jeremiah Jacks discovered that Guess.com was vulnerable to an SQL injection attack, permitting anyone able to construct a properly-crafted URL to pull down 200,000+ names, credit card numbers and expiration dates in the site's customer database.&lt;ref&gt;{{cite web|url=http://www.securityfocus.com/news/346|title=Guesswork Plagues Web Hole Reporting|publisher=[[SecurityFocus]]|date=March 6, 2002}}&lt;/ref&gt;
* On November 1, 2005, a teenaged hacker used SQL injection to break into the site of a [[Taiwan]]ese information security magazine from the Tech Target group and steal customers' information.&lt;ref&gt;{{cite web|url=http://www.xiom.com/whid-2005-46|title=WHID 2005-46: Teen uses SQL injection to break to a security magazine web site|publisher=[[Web Application Security Consortium]]|date=November 1, 2005|accessdate=December 1, 2009}}&lt;/ref&gt;
* On January 13, 2006, [[Russia]]n computer criminals broke into a [[Government of Rhode Island|Rhode Island government]] website and allegedly stole credit card data from individuals who have done business online with state agencies.&lt;ref&gt;{{cite web|url=http://www.xiom.com/whid-2006-3|title=WHID 2006-3: Russian hackers broke into a RI GOV website|publisher=[[Web Application Security Consortium]]|date=January 13, 2006|accessdate=May 16, 2008}}&lt;/ref&gt;
* On March 29, 2006, a hacker discovered an SQL injection flaw in an official [[Government of India|Indian government]]'s [[Tourism in India|tourism]] site.&lt;ref&gt;{{cite web|url=http://www.xiom.com/whid-2006-27|title=WHID 2006-27: SQL Injection in incredibleindia.org|publisher=[[Web Application Security Consortium]]|date=March 29, 2006|accessdate=March 12, 2010}}&lt;/ref&gt;
* On June 29, 2007, a computer criminal defaced the [[Microsoft]] UK website using SQL injection.&lt;ref&gt;{{cite web|url=http://www.cgisecurity.net/2007/06/hacker-defaces.html|title=Hacker Defaces Microsoft U.K. Web Page|publisher=cgisecurity.net|author=Robert|date=June 29, 2007|accessdate=May 16, 2008}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://rcpmag.com/news/article.aspx?editorialsid=8762|title=Hacker Defaces Microsoft UK Web Page|publisher=Redmond Channel Partner Online|author=Keith Ward|date=June 29, 2007|accessdate=May 16, 2008}}&lt;/ref&gt; UK website ''[[The Register]]'' quoted a Microsoft [[spokesperson]] acknowledging the problem.
* On September 19, 2007 and January 26, 2009 the Turkish hacker group "m0sted" used SQL injection to exploit Microsoft's SQL Server to hack web servers belonging to [[McAlester Army Ammunition Plant]] and the [[United States Army Corps of Engineers|US Army Corps of Engineers]] respectively.&lt;ref&gt;{{cite web|url=http://www.informationweek.com/architecture/anti-us-hackers-infiltrate-army-servers/d/d-id/1079964|publisher=[[Information Week]]|title=Anti-U.S. Hackers Infiltrate Army Servers|date=May 29, 2009|accessdate=December 17, 2016}}&lt;/ref&gt;
* In January 2008, tens of thousands of PCs were infected by an automated SQL injection attack that exploited a vulnerability in application code that uses [[Microsoft SQL Server]] as the database store.&lt;ref name="chinesefarm" /&gt;
* In July 2008, [[Kaspersky Lab|Kaspersky]]'s [[Malaysia]]n site was hacked by the "m0sted" hacker group using SQL injection.
* On April 13, 2008, the [[Sex offender registries in the United States|Sexual and Violent Offender Registry]] of [[Oklahoma]] shut down its website for "[[routine maintenance]]" after being informed that 10,597 [[Social Security number]]s belonging to [[sex offender]]s had been downloaded via an SQL injection attack&lt;ref&gt;{{cite web|url=http://thedailywtf.com/Articles/Oklahoma-Leaks-Tens-of-Thousands-of-Social-Security-Numbers,-Other-Sensitive-Data.aspx|publisher=[[The Daily WTF]]|title=Oklahoma Leaks Tens of Thousands of Social Security Numbers, Other Sensitive Data|author=Alex Papadimoulis|date=April 15, 2008|accessdate=May 16, 2008}}&lt;/ref&gt;
* In May 2008, a [[server farm]] inside [[China]] used automated queries to [[Google Search|Google's search engine]] to identify [[Microsoft SQL Server|SQL server]] websites which were vulnerable to the attack of an automated SQL injection tool.&lt;ref name="chinesefarm"&gt;{{cite web | url = http://www.pcworld.com/businesscenter/article/146048/mass_sql_injection_attack_targets_chinese_web_sites.html | title = Mass SQL Injection Attack Targets Chinese Web Sites | author = Sumner Lemon, IDG News Service | publisher = [[PC World (magazine)|PCWorld]] | date = May 19, 2008 | accessdate = May 27, 2008 }}&lt;/ref&gt;&lt;ref name="attackspecifics"&gt;{{cite web | url = http://www.bloombit.com/Articles/2008/05/ASCII-Encoded-Binary-String-Automated-SQL-Injection.aspx | title = ASCII Encoded/Binary String Automated SQL Injection Attack |author=Michael Zino| date = May 1, 2008 }}&lt;/ref&gt;
* In 2008, at least April through August, a sweep of attacks began exploiting the SQL injection vulnerabilities of Microsoft's [[Internet Information Services|IIS web server]] and [[Microsoft SQL Server|SQL Server database server]]. The attack does not require guessing the name of a table or column, and corrupts all text columns in all tables in a single request.&lt;ref name="broad_inject_specifics"&gt;{{cite web | url = http://hackademix.net/2008/04/26/mass-attack-faq/ | title = Mass Attack FAQ |author=Giorgio Maone| date = April 26, 2008 }}&lt;/ref&gt;  A HTML string that references a [[malware]] [[JavaScript]] file is appended to each value. When that database value is later displayed to a website visitor, the script attempts several approaches at gaining control over a visitor's system. The number of exploited web pages is estimated at 500,000.&lt;ref name="broad_inject_numbers"&gt;{{cite web | url = http://www.computerworld.com/article/2535473/security0/huge-web-hack-attack-infects-500-000-pages.html | title = Huge Web hack attack infects 500,000 pages |author=Gregg Keizer| date = April 25, 2008 |accessdate=October 16, 2015}}&lt;/ref&gt;
* On August 17, 2009, the [[United States Department of Justice]] charged an American citizen, [[Albert Gonzalez]], and two unnamed Russians with the theft of 130 million credit card numbers using an SQL injection attack. In reportedly "the biggest case of [[identity theft]] in American history", the man stole cards from a number of corporate victims after researching their [[Payment processor|payment processing system]]s. Among the companies hit were credit card processor [[Heartland Payment Systems]], convenience store chain [[7-Eleven|7&amp;#8209;Eleven]], and supermarket chain [[Hannaford Brothers]].&lt;ref&gt;{{cite news |url=http://news.bbc.co.uk/2/hi/americas/8206305.stm |title=US man 'stole 130m card numbers' |publisher=BBC |date=August 17, 2009 |accessdate=August 17, 2009}}&lt;/ref&gt;
* In December 2009, an attacker breached a [[RockYou]] plaintext database containing the [[Encryption|unencrypted]] usernames and passwords of about 32&amp;nbsp;million users using an SQL injection attack.&lt;ref&gt;{{cite news | url=http://www.nytimes.com/external/readwriteweb/2009/12/16/16readwriteweb-rockyou-hacker-30-of-sites-store-plain-text-13200.html | title = RockYou Hacker - 30% of Sites Store Plain Text Passwords | work=New York Times | first=Jolie | last=O'Dell | date=December 16, 2009 | accessdate=May 23, 2010}}&lt;/ref&gt;
*On July 2010, a South American security researcher who goes by the [[User (computing)|handle]] "Ch&amp;nbsp;Russo" obtained sensitive user information from popular [[BitTorrent]] site [[The Pirate Bay]]. He gained access to the site's administrative control panel and exploited a SQL injection vulnerability that enabled him to collect user account information, including [[IP address]]es, [[MD5]] [[Cryptographic hash function|password hashes]] and records of which torrents individual users have uploaded.&lt;ref&gt;{{cite news |url=http://krebsonsecurity.com/2010/07/pirate-bay-hack-exposes-user-booty/ | title= The pirate bay attack | date=July 7, 2010 }}&lt;/ref&gt;
*From July 24 to 26, 2010, attackers from [[Japan]] and [[China]] used an SQL injection to gain access to customers' credit card data from Neo Beat, an [[Osaka]]-based company that runs a large online supermarket site. The attack also affected seven business partners including supermarket chains Izumiya Co, Maruetsu Inc, and Ryukyu Jusco Co. The theft of data affected a reported 12,191 customers. As of August 14, 2010 it was reported that there have been more than 300 cases of credit card information being used by third parties to purchase goods and services in China.
* On September 19 during the [[Swedish general election, 2010|2010 Swedish general election]] a voter attempted a code injection by hand writing SQL commands as part of a [[Write-in candidate|write&amp;#8209;in]] vote.&lt;ref&gt;{{cite web|url=http://alicebobandmallory.com/articles/2010/09/23/did-little-bobby-tables-migrate-to-sweden |title=Did Little Bobby Tables migrate to Sweden? |publisher=Alicebobandmallory.com |accessdate=2011-06-03}}&lt;/ref&gt;
* On November 8, 2010 the British [[Royal Navy]] website was compromised by a Romanian hacker named TinKode using SQL injection.&lt;ref&gt;[http://www.bbc.co.uk/news/technology-11711478 Royal Navy website attacked by Romanian hacker] ''BBC News'', 8-11-10, Accessed November 2010&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://news.sky.com/skynews/Home/World-News/Stuxnet-Worm-Virus-Targeted-At-Irans-Nuclear-Plant-Is-In-Hands-Of-Bad-Guys-Sky-News-Sources-Say/Article/201011415827544 |title=Super Virus A Target For Cyber Terrorists
|author=Sam Kiley |date=November 25, 2010 |accessdate=November 25, 2010}}&lt;/ref&gt;
* On February 5, 2011 [[HBGary]], a technology security firm, was broken into by [[LulzSec]] using a SQL injection in their CMS-driven website&lt;ref&gt;{{cite web|url=http://www.par-anoia.net/We_Are_Anonymous_Inside_the_Hacker_World_of_LulzSe.pdf|title=We Are Anonymous: Inside the Hacker World of LulzSec|publisher=Little, Brown and Company}}&lt;/ref&gt;
* On March 27, 2011, [http://www.mysql.com mysql.com], the official homepage for [[MySQL]], was compromised by a hacker using SQL blind injection&lt;ref&gt;{{cite web|url=http://blog.sucuri.net/2011/03/mysql-com-compromised.html|title=MySQL.com compromised|publisher=[[sucuri]]}}&lt;/ref&gt;
* On April 11, 2011, [[Barracuda Networks]] was compromised using an SQL injection flaw. [[Email address]]es and usernames of employees were among the information obtained.&lt;ref&gt;{{cite web|url=http://www.networkworld.com/news/2011/041211-hacker-breaks-into-barracuda-networks.html?hpg1=bn |title=Hacker breaks into Barracuda Networks database}}&lt;/ref&gt;
*Over a period of 4&amp;nbsp;hours on April 27, 2011, an automated SQL injection attack occurred on [[Broadband Reports]] website that was able to extract 8% of the username/password pairs: 8,000 random accounts of the 9,000 active and 90,000 old or inactive accounts.&lt;ref name="DSLReports"&gt;{{cite web|url=http://www.dslreports.com/forum/r25793356- |title=site user password intrusion info |publisher=Dslreports.com |accessdate=2011-06-03}}&lt;/ref&gt;&lt;ref name="Cnet News"&gt;{{cite news|url=http://news.cnet.com/8301-27080_3-20058471-245.html|title=DSLReports says member information stolen|publisher=Cnet News|date=2011-04-28|accessdate=2011-04-29}}&lt;/ref&gt;&lt;ref name="The Tech Herald"&gt;{{cite news|url=http://www.thetechherald.com/article.php/201117/7127/DSLReports-com-breach-exposed-more-than-100-000-accounts|title=DSLReports.com breach exposed more than 100,000 accounts|publisher=The Tech Herald|date=2011-04-29|accessdate=2011-04-29}}&lt;/ref&gt;
*On June 1, 2011, "[[hacktivist]]s" of the group [[LulzSec]] were accused of using SQLI to steal [[coupon]]s, download keys, and passwords that were stored in plaintext on [[Sony]]'s website, accessing the personal information of a million users.&lt;ref&gt;{{citation |title= LulzSec hacks Sony Pictures, reveals 1m passwords unguarded | date= June 2, 2011 |work= electronista.com |url= http://www.electronista.com/articles/11/06/02/lulz.security.hits.sony.again.in.security.message/ }}&lt;/ref&gt;&lt;ref&gt;{{citation |title= LulzSec Hacker Arrested, Group Leaks Sony Database|author=Ridge Shan | date= June 6, 2011 |work= The Epoch Times |url=http://www.theepochtimes.com/n2/technology/lulzsec-member-arrested-group-leaks-sony-database-57296.html}}&lt;/ref&gt;
* In June 2011, [[PBS]] was hacked, mostly likely through use of SQL injection; the full process used by hackers to execute SQL injections was described in this [http://blog.imperva.com/2011/05/pbs-breached-how-hackers-probably-did-it.html Imperva] blog.&lt;ref name="PBS Breached - How Hackers Probably Did It"&gt;{{cite news|url=http://blog.imperva.com/2011/05/pbs-breached-how-hackers-probably-did-it.html|title=Imperva.com: PBS Hacked - How Hackers Probably Did It|accessdate=2011-07-01}}&lt;/ref&gt;
* In May 2012, the website for ''[[Wurm Online]]'', a [[massively multiplayer online game]], was shut down from an SQL injection while the site was being updated.&lt;ref&gt;{{cite web|url=http://wurmonline.tumblr.com/post/22835329693/wurm-online-restructuring |title=Wurm Online is Restructuring |date=May 11, 2012}}&lt;/ref&gt;
* [[2012 Yahoo! Voices hack|In July 2012]] a hacker group was reported to have stolen 450,000 login credentials from [[Yahoo!]]. The logins were stored in [[plain text]] and were allegedly taken from a Yahoo [[subdomain]], [[Yahoo! Voices]]. The group breached Yahoo's security by using a "[[Set operations (SQL)#UNION operator|union]]-based SQL injection technique".&lt;ref&gt;Chenda Ngak. [http://www.cbsnews.com/8301-501465_162-57470956-501465/yahoo-reportedly-hacked-is-your-account-safe/ "Yahoo reportedly hacked: Is your account safe?"], CBS News. July 12, 2012. Retrieved July 16, 2012.&lt;/ref&gt;&lt;ref&gt;http://www.zdnet.com/450000-user-passwords-leaked-in-yahoo-breach-7000000772/&lt;/ref&gt;
* On October 1, 2012, a hacker group called "Team GhostShell" published the personal records of students, faculty, employees, and alumni from 53 universities including [[Harvard]], [[Princeton University|Princeton]], [[Stanford]], [[Cornell]], [[Johns Hopkins University|Johns Hopkins]], and the [[University of Zurich]] on [[Pastebin|pastebin.com]]. The hackers claimed that they were trying to "raise awareness towards the changes made in today’s education", bemoaning changing education laws in Europe and increases in [[College tuition in the United States|tuition in the United States]].&lt;ref&gt;{{cite news|last=Perlroth|first=Nicole|title=Hackers Breach 53 Universities and Dump Thousands of Personal Records Online|url=http://bits.blogs.nytimes.com/2012/10/03/hackers-breach-53-universities-dump-thousands-of-personal-records-online/|newspaper=New York Times|date=3 October 2012}}&lt;/ref&gt;
* In February 2013, a group of Maldivian hackers, hacked the website "UN-Maldives" using SQL Injection.
* On June 27, 2013, hacker group "[[RedHack]]" breached Istanbul Administration Site.&lt;ref&gt;{{Cite news | title=RedHack Breaches Istanbul Administration Site, Hackers Claim to Have Erased Debts | url=http://news.softpedia.com/news/RedHack-Breaches-Istanbul-Administration-Site-Hackers-Claim-to-Have-Erased-Debts-364000.shtml}}&lt;/ref&gt;  They claimed that, they’ve been able to erase people's debts to water, gas, Internet, electricity, and telephone companies. Additionally, they published admin user name and password for other citizens to log in and clear their debts early morning. They announced the news from Twitter.&lt;ref&gt;{{Cite news | title=Redhack tweet about their achievement | url=http://twitter.com/RedHack_EN/statuses/350461821456613376 }}&lt;/ref&gt;
* On November 4, 2013, hacktivist group "RaptorSwag" allegedly compromised 71 Chinese government databases using an SQL injection attack on the Chinese Chamber of International Commerce. The leaked data was posted publicly in cooperation with [[Anonymous (group)|Anonymous]].&lt;ref&gt;http://news.softpedia.com/news/Hackers-Leak-Data-Allegedly-Stolen-from-Chinese-Chamber-of-Commerce-Website-396936.shtml&lt;/ref&gt;
* On February 2, 2014, AVS TV had 40,000 accounts leaked by a hacking group called @deletesec &lt;ref&gt;http://www.maurihackers.info/2014/02/40000-avs-tv-accounts-leaked.html&lt;/ref&gt;
* On February 21, 2014, United Nations Internet Governance Forum had 3,215 account details leaked.&lt;ref&gt;http://www.batblue.com/united-nations-internet-governance-forum-breached/&lt;/ref&gt;
* On February 21, 2014, Hackers of a group called @deletesec hacked Spirol International after allegedly threatening to have the hackers arrested for reporting the security vulnerability. 70,000 user details were exposed over this conflict.&lt;ref&gt;http://news.softpedia.com/news/Details-of-70-000-Users-Leaked-by-Hackers-From-Systems-of-SPIROL-International-428669.shtml&lt;/ref&gt;
* On March 7, 2014, officials at Johns Hopkins University publicly announced that their Biomedical Engineering Servers had become victim to an SQL injection attack carried out by an Anonymous hacker named "Hooky" and aligned with hacktivist group "RaptorSwag". The hackers compromised personal details of 878 students and staff, posting a [http://pastebin.com/UG4fYnby press release] and the leaked data on the internet.&lt;ref&gt;http://articles.baltimoresun.com/2014-03-07/news/bs-md-hopkins-servers-hacked-20140306_1_engineering-students-identity-theft-server&lt;/ref&gt;
* In August 2014, [[Milwaukee]]-based computer security company Hold Security disclosed that it uncovered [[2014 Russian hacker password theft|a theft of confidential information]] from nearly 420,000 websites through SQL injections.&lt;ref&gt;Damon Poeter. [http://www.pcmag.com/article2/0,2817,2462057,00.asp 'Close-Knit' Russian Hacker Gang Hoards 1.2 Billion ID Creds], ''PC Magazine'', August 5, 2014&lt;/ref&gt; ''[[The New York Times]]'' confirmed this finding by hiring a security expert to check the claim.&lt;ref&gt;Nicole Perlroth. [http://www.nytimes.com/2014/08/06/technology/russian-gang-said-to-amass-more-than-a-billion-stolen-internet-credentials.html?_r=0 Russian Gang Amasses Over a Billion Internet Passwords], ''The New York Times'', August 5, 2014.&lt;/ref&gt;
* In October 2015, an SQL injection attack was used to steal the personal details of 156,959 customers from British telecommunications company [[TalkTalk Group|Talk Talk's]] servers, exploiting a vulnerability in a legacy web portal&lt;ref&gt;https://ico.org.uk/about-the-ico/news-and-events/news-and-blogs/2016/10/talktalk-gets-record-400-000-fine-for-failing-to-prevent-october-2015-attack/&lt;/ref&gt;

==In popular culture==
* Unauthorized login to web sites by means of SQL injection forms the basis of one of the subplots in [[J.K. Rowling]]'s novel ''[[The Casual Vacancy]]'', published in 2012.
* An ''[[xkcd]]'' cartoon involved a character "Robert'); DROP TABLE students;--" named to carry out a SQL injection. As a result of this cartoon, SQL injection is sometimes informally referred to as 'Bobby Tables'.&lt;ref&gt;{{cite web|last=Munroe|first=Randall|title=XKCD: Exploits Of A Mom|url=http://xkcd.com/327/|accessdate=26 February 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Bobby Tables: A guide to preventing SQL injection|url=http://bobby-tables.com/|accessdate=6 October 2013}}&lt;/ref&gt;
* In 2014, an individual in Poland legally renamed his business to ''&lt;nowiki&gt;Dariusz Jakubowski x'; DROP TABLE users; SELECT '1&lt;/nowiki&gt;'' in an attempt to disrupt operation of spammers’ [[Web scraping|harvesting bots]].&lt;ref&gt;{{cite web|title=Jego firma ma w nazwie SQL injection. Nie zazdrościmy tym, którzy będą go fakturowali ;)|website=Niebezpiecznik|language=pl|date=11 September 2014|url=http://niebezpiecznik.pl/post/jego-firma-ma-w-nazwie-sql-injection-nie-zazdroscimy-tym-ktorzy-beda-go-fakturowali/|accessdate=26 September 2014}}&lt;/ref&gt;
* The 2015 game [[Hacknet]] has a hacking program called SQL_MemCorrupt. It is described as injecting a table entry that causes a corruption error in a SQL database, then queries said table, causing a SQL database crash and core dump.

==See also==
{{Portal|Software Testing}}
* [[Code injection]]
* [[Cross-site scripting]]
* [[Metasploit Project]]
* [[OWASP]] Open Web Application Security Project
* [[SGML entity]]
* [[Uncontrolled format string]]
* [[w3af]]
* [[Web application security]]

==References==
{{Reflist|30em}}

==External links==
* [http://www.techyfreaks.com/2012/05/manual-sql-injection-tutorial.html Manual Sql Injection Tutorial] By The Ajay Devgan
* [http://www.websec.ca/kb/sql_injection SQL Injection Knowledge Base], by Websec.
* [http://www.sqlinjectionwiki.com/ SQL Injection Wiki]
* [http://projects.webappsec.org/SQL-Injection WASC Threat Classification - SQL Injection Entry], by the Web Application Security Consortium.
* [https://docs.google.com/leaf?id=0BykNNUTb95yzYTRjMjNjMWEtODBmNS00YzgwLTlmMGYtNWZmODI2MTNmZWYw&amp;sort=name&amp;layout=list&amp;num=50 Why SQL Injection Won't Go Away], by Stuart Thomas.
* [http://www.owasp.org/index.php/SQL_Injection_Prevention_Cheat_Sheet SQL Injection Prevention Cheat Sheet], by OWASP.
* [http://sqlmap.org/ sqlmap: automatic SQL injection and database takeover tool]
* [http://go.microsoft.com/?linkid=9707610 SDL Quick security references on SQL injection] by Bala Neerumalla.
* [http://arstechnica.com/information-technology/2016/10/how-security-flaws-work-sql-injection/ How security flaws work: SQL injection]
* [https://www.netsparker.com/blog/web-security/sql-injection-cheat-sheet/ SQL Injection Cheat Sheet] by Netsparker

[[Category:Data management]]
[[Category:Injection exploits]]
[[Category:SQL]]
[[Category:Articles with example SQL code]]
[[Category:Computer security exploits]]</text>
      <sha1>h4mo7gk9524i9sty0vt114zqopvhskg</sha1>
    </revision>
  </page>
  <page>
    <title>Secure Electronic Delivery</title>
    <ns>0</ns>
    <id>1282406</id>
    <revision>
      <id>726442037</id>
      <parentid>726441706</parentid>
      <timestamp>2016-06-22T05:53:04Z</timestamp>
      <contributor>
        <username>Aptiva07</username>
        <id>28290881</id>
      </contributor>
      <comment>Correcting wikilink</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4212" xml:space="preserve">'''Secure Electronic Delivery'''  (SED) is a service created in 2003 and provided by the  [[British Library#Document Supply Service|British Library Document Supply Service]] (BLDSS). Its purpose is to enable faster delivery of digital materials as [[Encryption|encrypted]], copyright-compliant [[Portable Document Format| PDF Document]]s, to a personal e-mail address. These documents are supplied from the British Library via its On Demand service.&lt;ref&gt;{{cite web |url=http://www.bl.uk/sed   |title=Secure Electronic Delivery  |author=  |publisher=[[British Library]] |date=  |accessdate= }}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://www.bl.uk/reshelp/atyourdesk/docsupply/help/receiving/deliveryoptions/electronic/sed/sedhelpsheetfinal.pdf  |title= Secure Electronic Delivery – Technical Helpsheet  |author=  |publisher=[[British Library]] |date=  |accessdate= }}&lt;/ref&gt; When the British Library supplies articles electronically, it sends them securely in order to ensure its usage is permitted (research purposes) and copyright law is observed.

==Methods==
As the [[publishing | publishing industry]], authors and creators become highly protective of their assets and [[intellectual property]], they impose strict rules on delivery methods to prevent [[copyright infringement]]. Nowadays, [[Digital rights management|DRM]]-enabled secure delivery appears to be the most widely used solution to address issues faced by libraries in supplying ebooks and digital materials to their users.&lt;ref&gt;{{cite news  | title=Secure E-mail Delivery Poised to Take Off  |url=https://books.google.com.mx/books?id=PWHbLjAQ57gC&amp;pg=PA38&amp;lpg=PA38&amp;dq=document+secure+delivery+technology&amp;source=bl&amp;ots=YgfE16c0Yy&amp;sig=qd0R1j9LtZI6_hJi6zqHCGOBzfQ&amp;hl=en&amp;sa=X&amp;redir_esc=y#v=onepage&amp;q=document%20secure%20delivery%20technology&amp;f=false   |date=23 August 1999 |author=Dominique Deckmyn  |newspaper=[[Computerworld]] }}&lt;/ref&gt;&lt;ref&gt;{{cite web  |title= Practical problems for libraries distributing ebooks &amp; secure electronic delivery |url=http://www.locklizard.com/libraries-secure-electronic-delivery/   |publisher=Locklizard Limited |date=  |accessdate= }}&lt;/ref&gt;  SED, one of these solutions, is using [[Adobe LiveCycle]] Digital Rights Management (LCDRM) as an encryption method to deliver documents.&lt;ref&gt;{{cite web |url=http://www.lancaster.ac.uk/library/using-the-library/interlending-and-document-supply/secure-electronic-delivery/ |title=British Library On Demand Electronic Delivery  |author=  |publisher=[[Lancaster University]] Library  |date=  |accessdate= }}&lt;/ref&gt;

==Advantages==
SED offers convenience, quality and speed as documents are delivered upon request at any location and on any device. Requested articles are scanned for high quality reproduction, opened anywhere on any machine, including mobile devices.&lt;ref&gt;{{cite web |url= http://www.brad.ac.uk/library/media/library/interlibraryloans/sed.pdf |title=SED – Secure Electronic Delivery  |author=  |publisher=[[University of Bradford]] |date=  |accessdate= }}&lt;/ref&gt;

== Restrictions==
The following are restrictions hold in a SED service implementation:
*  The digital material is accessible only for 14 days via a link sent to a personal message.
* Due to copyright reasons,&lt;ref&gt;{{cite journal |last=Eiblum |first= Paula   |last2= Ardito |first2= Stephanie    |date= September 1999 |title= Document Delivery &amp; Copyright: Librarians Take the Fifth |url= |journal=Online (magazine) |publisher= |volume=23 |issue=5 |pages=74–77  |doi= |access-date=7 May 2016}}&lt;/ref&gt;  the material can be opened only once, saved for 14 days and does not allow a copy-paste action.
* Upon display, the material must be printed from the same device and reprinted only once.
* The On Demand encryption technology works best on the default Safari browser although other browsers may accommodate it.

==See also==
* [[Digital rights management]]
* [[Digital asset management]]

==References==
{{Reflist}}

==External links==
[http://www.bl.uk/sed SED Web page]

{{DEFAULTSORT:Secure Electronic Delivery}}
[[Category:Information technology management]]
[[Category:Content management systems]]
[[Category:Document management systems]]
[[Category:Data management]]
[[Category:Secure communication]]</text>
      <sha1>4ph6o5iya7a4l08dijsyji09hi22iop</sha1>
    </revision>
  </page>
  <page>
    <title>Research data archiving</title>
    <ns>0</ns>
    <id>10022970</id>
    <revision>
      <id>751090225</id>
      <parentid>751090168</parentid>
      <timestamp>2016-11-23T09:00:18Z</timestamp>
      <contributor>
        <username>RichardWeiss</username>
        <id>193093</id>
      </contributor>
      <comment>rm EL tag as we now dont have an EL section</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="15615" xml:space="preserve">'''Research data archiving''' is  the [[Computer_data_storage#Volatility|long-term storage]] of scholarly research [[data]], including  the natural sciences, social sciences, and life sciences. The various [[academic journals]] have differing policies regarding how much of their data and methods researchers are required to store in a public archive, and what is actually archived varies widely between different disciplines. Similarly, the major grant-giving institutions have varying attitudes towards public archival of data. In general, the tradition of science has been for publications to contain sufficient information to allow fellow researchers to replicate and therefore test the research. In recent years this approach has become increasingly strained as   research in some areas depends on large datasets which cannot easily be replicated independently.

Data archiving is more important in some fields than others.  In a few fields, all of the data necessary to replicate the work is already available in the journal article.  In drug development, a great deal of data is generated and must be archived so researchers can verify that the reports the drug companies publish accurately reflect the data.

The requirement of data archiving is a recent development in the [[history of science]].  It was made possible by advances in [[information technology]] allowing large amounts of data to be stored and accessed from central locations.  For example, the [[American Geophysical Union]] (AGU) adopted their first policy on data archiving in 1993, about three years after the beginning of the [[WWW]].&lt;ref&gt;”Policy on Referencing Data in and Archiving Data for AGU Publications” [http://www.agu.org/pubs/authors/policies/data_policy.shtml]&lt;/ref&gt; This policy mandates that datasets cited in AGU papers must be archived by a recognised data center; it permits the creation of "data papers"; and it establishes AGU's role in maintaining data archives. But it makes no requirements on paper authors to archive their data.

Prior to organized data archiving, researchers wanting to evaluate or replicate a paper would have to request data and methods information from the author.  The academic community expects authors to [[Data sharing (Science)|share supplemental data]].  This process was recognized as wasteful of time and energy and obtained mixed results.  Information could become lost or corrupted over the years.  In some cases, authors simply refuse to provide the information.

The need for data archiving and due diligence is greatly increased when the research deals with health issues or public policy formation.&lt;ref&gt;"The Case for Due Diligence When Empirical Research is Used in Policy Formation" by Bruce McCullough and Ross McKitrick. [http://economics.ca/2006/papers/0685.pdf]&lt;/ref&gt;&lt;ref&gt;[http://gking.harvard.edu/replication.shtml "Data Sharing and Replication" a website by Gary King]&lt;/ref&gt;

==Selected policies by journals==

===''The American Naturalist''===
{{quote|''[[The American Naturalist]]'' requires authors to deposit the data associated with accepted papers in a public archive. For gene sequence data and phylogenetic trees, deposition in [[GenBank]] or [[TreeBASE]], respectively, is required. There are many possible archives that may suit a particular data set, including the [[Dryad (repository)|Dryad]] repository for ecological and evolutionary biology data. All accession numbers for GenBank, TreeBASE, and Dryad must be included in accepted manuscripts before they go to Production. If the data is deposited somewhere else, please provide a link. If the data is culled from published literature, please deposit the collated data in Dryad for the convenience of your readers. Any impediments to data sharing should be brought to the attention of the editors at the time of submission so that appropriate arrangements can be worked out.|JSTOR&lt;ref&gt;[http://www.jstor.org/page/journal/amernatu/forAuthor.html#data Supporting Data and Material]&lt;/ref&gt;}}

===''Journal of Heredity''===
{{quote|The primary data underlying the conclusions of an article are critical to the verifiability and transparency of the scientific enterprise, and should be preserved in usable form for decades in the future. For this reason, ''Journal of Heredity'' requires that newly reported nucleotide or amino acid sequences, and structural coordinates, be submitted to appropriate public databases (e.g., GenBank; the [[EMBL Nucleotide Sequence Database]]; DNA Database of Japan; the [[Protein Data Bank]] ; and [[Swiss-Prot]]). Accession numbers must be included in the final version of the manuscript. For other forms of data (e.g., microsatellite genotypes, linkage maps, images), the Journal endorses the principles of the Joint Data Archiving Policy (JDAP) in encouraging all authors to archive primary datasets in an appropriate public archive, such as Dryad, TreeBASE, or the Knowledge Network for Biocomplexity. Authors are encouraged to make data publicly available at time of publication or, if the technology of the archive allows, opt to embargo access to the data for a period up to a year after publication.

The American Genetic Association also recognizes the vast investment of individual researchers in generating and curating large datasets. Consequently, we recommend that this investment be respected in secondary analyses or meta-analyses in a gracious collaborative spirit.|oxfordjournals.org&lt;ref&gt;[http://www.oxfordjournals.org/our_journals/jhered/for_authors/msprep_submission.html#4.%20DATA%20ARCHIVING%20POLICY Data archiving policy]&lt;/ref&gt;}}

===''Molecular Ecology''===
{{quote|[[Molecular Ecology]] expects that data supporting the results in the paper should be archived in an appropriate public archive, such as GenBank, [[Gene Expression Omnibus]], TreeBASE, Dryad, the [[Knowledge Network for Biocomplexity]], your own institutional or funder repository, or as Supporting Information on the Molecular Ecology web site. Data are important products of the scientific enterprise, and they should be preserved and usable for decades in the future. Authors may elect to have the data publicly available at time of publication, or, if the technology of the archive allows, may opt to embargo access to the data for a period up to a year after publication. Exceptions may be granted at the discretion of the editor, especially for sensitive information such as human subject data or the location of endangered species.|Wiley&lt;ref&gt;[http://www.wiley.com/bw/submit.asp?ref=0962-1083&amp;site=1 Policy on data archiving]&lt;/ref&gt;}}

===''Nature''===
{{quote|Such material must be hosted on an accredited independent site (URL and accession numbers to be provided by the author), or sent to the ''Nature'' journal at submission, either uploaded via the journal's online submission service, or if the files are too large or in an unsuitable format for this purpose, on CD/DVD (five copies). Such material cannot solely be hosted on an author's personal or institutional web site.&lt;ref&gt;[http://www.nature.com/authors/editorial_policies/availability.html "Availability of Data and Materials: The Policy of Nature Magazine]&lt;/ref&gt;

''Nature'' requires the reviewer to determine if all of the supplementary data and methods have been archived.  The policy advises reviewers to consider several questions, including: "Should the authors be asked to provide supplementary methods or data to accompany the paper online? (Such data might include source code for modelling studies, detailed experimental protocols or mathematical derivations.)|[[Nature (journal)|Nature]]&lt;ref&gt;{{cite web|title=Guide to Publication Policies of the Nature Journals|date=March 14, 2007|url=http://www.nature.com/authors/gta.pdf}}&lt;/ref&gt;}}

===''Science''===
{{quote|''Science'' supports the efforts of databases that aggregate published data for the use of the scientific community. Therefore, before publication, large data sets (including microarray data, protein or DNA sequences, and atomic coordinates or electron microscopy maps for macromolecular structures) must be deposited in an approved database and an accession number provided for inclusion in the published paper.&lt;ref&gt;[http://www.sciencemag.org/about/authors/prep/gen_info.dtl#datadep "General Policies of Science Magazine"]&lt;/ref&gt;

"Materials and methods" – ''Science'' now requests that, in general, authors place the bulk of their description of materials and methods online as supporting material, providing only as much methods description in the print manuscript as is necessary to follow the logic of the text. (Obviously, this restriction will not apply if the paper is fundamentally a study of a new method or technique.)|[[Science (journal)|Science]]&lt;ref&gt;[http://www.sciencemag.org/about/authors/prep/prep_online.dtl ”Preparing Your Supporting Online Material”]&lt;/ref&gt;}}

===Royal Society===
{{quote|To allow others to verify and build on the work published in [[Royal Society]] journals, it is a condition of publication that authors make available the data, code and research materials supporting the results in the article.
Datasets and code should be deposited in an appropriate, recognised, publicly available repository. Where no data-specific repository exists, authors should deposit their datasets in a general repository such as [[Dryad (repository)]] or [[Figshare]].
|[[Royal Society]]&lt;ref&gt;[https://royalsociety.org/journals/ethics-policies/data-sharing-mining/ "Data sharing and mining"]&lt;/ref&gt;}}

==Policies by funding agencies==
In the United States, the [[National Science Foundation]] (NSF) has tightened requirements on data archiving.   Researchers seeking funding from NSF are now required to file a [[data management plan]] as a two-page supplement to the grant application.&lt;ref&gt;[http://news.sciencemag.org/scienceinsider/2010/05/nsf-to-ask-every-grant-applicant.html ”NSF to Ask Every Grant Applicant for Data Management Plan”]&lt;/ref&gt;

The NSF [[Datanet]] initiative has resulted in funding of the '''Data Observation Network for Earth''' ([[DataONE]]) project, which will provide scientific data archiving for ecological and environmental data produced by scientists worldwide. DataONE's stated goal is to preserve and provide access to multi-scale, multi-discipline, and multi-national data. The community of users for DataONE includes scientists, ecosystem managers, policy makers, students, educators, and the public.

==Data archives==

===Natural sciences===
The following list refers to scientific data archives.
* [[CISL Research Data Archive]]
* [[Dryad (repository)|Dryad]]
* [[ESO/ST-ECF Science Archive Facility]]
* [http://www.ncdc.noaa.gov/paleo/treering.html International Tree-Ring Data Bank]
* [http://www.icpsr.umich.edu Inter-university Consortium for Political and Social Research]
* [http://knb.ecoinformatics.org Knowledge Network for Biocomplexity]
* [[National Archive of Computerized Data on Aging]]
* National Archive of Criminal Justice Data [http://www.icpsr.umich.edu/nacjd]
* [[National Climatic Data Center]]
* [[National Geophysical Data Center]]
* [[National Snow and Ice Data Center]]
* [[National Oceanographic Data Center]]
* [http://daac.ornl.gov Oak Ridge National Laboratory Distributed Active Archive Center]
* [[PANGAEA (data library)|Pangaea - Data Publisher for Earth &amp; Environmental Science]]
* [[World Data Center]]
* [[DataONE]]

===Social sciences===
{{cleanup merge|Data archives|date=April 2016}}

'''Data archives''' are professional institutions for the acquisition, preparation, preservation, and dissemination of social and behavioral data. The term is also sometimes used about natural science institutions (e.g., [[CISL Research Data Archive]], see [[Scientific data archiving]] and Borgman, 2007, p.&amp;nbsp;18&lt;ref&gt;Borgman, Christine L. (2007).''Scholarship in the digital age: information, infrastructure and the internet''. Cambridge, MA: The MIT Press.&lt;/ref&gt;), but here seems '''data centers''' to be the most used term. Data archives in the social sciences evolved in the 1950s and has been perceived as an international movement: 

&lt;blockquote&gt;By 1964 the International Social Science Council (ISSC) had sponsored a second conference on Social Science Data Archives and had a standing Committee on Social Science Data, both of which stimulated the data archives movement. By the beginning of the twenty-first century, most developed countries and some developing countries had organized formal and well-functioning national data archives. In addition, college and university campuses often have `data libraries' that make data available to their faculty, staff, and students; most of these bear minimal archival responsibility, relying for that function on a national institution (Rockwell, 2001, p. 3227).&lt;ref&gt;Rockwell, R. C. (2001). Data Archives: International. IN: Smelser, N. J. &amp; Baltes, P. B. (eds.) ''International Encyclopedia of the Social and Behavioral Sciences'' (vol. 5, pp. 3225- 3230). Amsterdam: Elsevier&lt;/ref&gt;&lt;/blockquote&gt;

* [[Registry of Research Data Repositories | re3data.org]] is a global registry of research data repository indexing data archives from all disciplines: http://www.re3data.org
* CESSDA Members are data archives and other organisations that archive social science data and provide data for secondary use: http://www.cessda.net/about/members.html
* Consortium of European Social Science Data Archives: http://www.cessda.org/
* The Danish Data Archives: http://www.sa.dk/content/us/about_us ; specific page (only in Danish): http://www.sa.dk/dda/default.htm
* Inter-university Consortium for Political and Social Research: http://www.icpsr.umich.edu/
* The Roper Center for Public Opinion Research: http://www.ropercenter.uconn.edu
* The Social Science Data Archive: http://dataarchives.ss.ucla.edu/
* The NCAR Research Data Archive:  http://rda.ucar.edu

===Life sciences===
{{stub section|date=April 2016}}

==See also==
*[[Data archive]]

==References==
{{Reflist}}

==Notes==
* [[Registry of Research Data Repositories]] ''re3data.org'' [http://service.re3data.org/search/results?term=]
* Statistical checklist required by ''Nature'' [http://www.nature.com/nature/authors/gta/Statistical_checklist.doc]
* Policies of ''Proceedings of the National Academy of Sciences (U.S.)'' [http://www.pnas.org/misc/iforc.shtml#policies]
* The US National Committee for CODATA [http://www7.nationalacademies.org/usnc-codata/Archiving.html]
* The Role of Data and Program Code Archives in the Future of Economic Research  [http://research.stlouisfed.org/wp/2005/2005-014.pdf]
* Data sharing and replication – Gary King website [http://gking.harvard.edu/replication.shtml]
* The Case for Due Diligence When Empirical Research is Used in Policy Formation by McCullough and McKitrick [http://economics.ca/2006/papers/0685.pdf]
* Thoughts on Refereed Journal Publication by Chuck Doswell [http://www.cimms.ou.edu/~doswell/pubreviews.html]
* “How to encourage the right behaviour” An opinion piece published in ''Nature'',  March, 2002.[http://www.nature.com/nature/journal/v416/n6876/full/416001b.html]
* [[NASA Astrophysics Data System]] [http://cdsads.u-strasbg.fr/]
* [[Panton Principles]] for Open Data in Science, at Citizendium [http://en.citizendium.org/wiki/Panton_Principles]
* [[Inter-university Consortium for Political and Social Research]] [http://www.icpsr.umich.edu]


[[Category:Computer archives]]
[[Category:Data management]]
[[Category:Data publishing]]
[[Category:Digital preservation]]
[[Category:Information retrieval techniques]]
[[Category:Knowledge representation]]
[[Category:Structured storage]]</text>
      <sha1>qxb64zeibon54urxs0gfo8c4nfcwl7g</sha1>
    </revision>
  </page>
  <page>
    <title>Digital obsolescence</title>
    <ns>0</ns>
    <id>934683</id>
    <revision>
      <id>758142348</id>
      <parentid>758141932</parentid>
      <timestamp>2017-01-03T18:47:13Z</timestamp>
      <contributor>
        <username>Greenrd</username>
        <id>15476</id>
      </contributor>
      <minor />
      <comment>fixed broken (duplicate) reference</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8857" xml:space="preserve">{{Outdated|date=January 2015}}[[File:VCF 2010 Domesday tray open.jpg|thumb|300px|A Domesday Project machine with its modified [[Laserdisc]]. The Domesday Project was published in 1986.]]
'''Digital obsolescence''' is a situation where a digital resource is no longer readable because of its archaic format: the physical media, the reader (required to read the media), the hardware, or the software that runs on it is no longer available.&lt;ref name='national-archives'&gt;{{cite web | last = | first = | authorlink = | title =Managing Digital Obsolescence Risks | work = | publisher =The National Archives | date =April 2009 | url = http://www.nationalarchives.gov.uk/documents/information-management/siro-guidance-on-the-risk-of-digital-obsolescence.pdf| format = pdf| doi = | accessdate = | archiveurl =http://webarchive.nationalarchives.gov.uk/+/http://www.nationalarchives.gov.uk/documents/information-management/siro-guidance-on-the-risk-of-digital-obsolescence.pdf | archivedate = 28 Jun 2011}}&lt;/ref&gt; 

A prime example of this is the [[BBC Domesday Project]] from the 1980s, although its data was eventually recovered after a significant amount of effort. [[Cornell University]] Library’s [http://www.icpsr.umich.edu/dpm/dpm-eng/eng_index.html digital preservation tutorial] (now hosted by [[ICPSR]]) has a timeline of obsolete media formats, called the [http://www.icpsr.umich.edu/dpm/dpm-eng/oldmedia/index.html “Chamber of Horrors”], that shows how rapidly new technologies are created and cast aside.

==Introduction==
The rapid evolution and proliferation of different kinds of [[computer hardware]], modes of digital encoding, [[operating systems]] and general or specialized [[software]] ensures that digital obsolescence will become a problem in the future.&lt;ref&gt;Rothenberg, J. (1998). [http://www.clir.org/pubs/reports/rothenberg/introduction.html#longevity Avoiding Technological Quicksand: Finding a Viable Technical Foundation for Digital Preservation]&lt;/ref&gt; Many versions of word-processing programs, data-storage media, standards for encoding images and films are considered "standards" for some time, but in the end are always replaced by new versions of the software or completely new hardware. Files meant to be read or edited with a certain program (for example [[Microsoft Word]]) will be unreadable in other programs, and as operating systems and hardware move on, even old versions of programs developed by the same company become impossible to use on the new platform (for instance, older versions of [[Microsoft Works]], before Works 4.5, cannot be run under [[Windows 2000]] or later). 

Early attention was brought to the challenges of preserving [[machine-readable data]] by the work of [[Charles M Dollar]] in the 1970s, but it was only during the 1990s that libraries and archives came to appreciate the significance of the problem&lt;ref&gt;Hedstrom, M. (1995). [http://www.uky.edu/%7Ekiernan/DL/hedstrom.html Digital Preservation: A Time Bomb for Digital Libraries]&lt;/ref&gt; and has been discussed among professionals in those branches, though so far without any obvious solutions other than continual forward-migration of files and information to the latest data-storage standards. File formats should be widespread, backward compatible, often upgraded, and, ideally, open format. In 2002, the National Initiative for a Networked Cultural Heritage cited&lt;ref&gt;National Initiative for a Networked Cultural Heritage. (2002). [http://www.nyu.edu/its/humanities/ninchguide/V/ NINCH Guide to Good Practice in the Digital Representation and Management of Cultural Heritage Materials]&lt;/ref&gt; the following as “de facto” formats that are unlikely to be rendered obsolete in the near future:  uncompressed [[TIFF]]  and [[ASCII]] and [[Rich Text Format|RTF]] (for text).

In order to prevent this from happening, it is important that an institution regularly evaluate and explore its current technologies and evaluate its long term business model.&lt;ref name='national-archives'/&gt;

== Types ==
Digital objects are vulnerable to three types of obsolescence:&lt;ref&gt;{{cite web|publisher=National Archives of Australia|title=Obsolescence – a key challenge in the digital age|url=http://www.naa.gov.au/records-management/agency/preserve/e-preservation/obsolescence.aspx|accessdate=17 March 2014}}&lt;/ref&gt;
# '''Physical media''': the physical carrier of the digital file becomes obsolete; e.g. 8 inch floppy disks, which are no longer commercially available. 
# '''Hardware''': the hardware needed to access the digital file becomes obsolete; e.g. floppy disk drive, which computers are no longer manufactured with.
# '''Software''': the software needed to access the digital file becomes obsolete; e.g. [[WordStar]], a word processor popular in the 1980s which used a [[Open data|closed data]] format and is no longer readily available.

== Strategies ==
Any organization that has digital records should assess its records to identify any potential risks for file format obsolescence. The Library of Congress maintains [http://www.digitalpreservation.gov/formats/intro/intro.shtml Sustainability of Digital Formats], which includes technical details about many different format types. The UK National Archives maintains an online registry of file formats called [http://www.nationalarchives.gov.uk/PRONOM/Default.aspx PRONOM].

In its 2014 agenda, the National Digital Stewardship Alliance recommended developing File Format Action Plans: "it is important to shift from more abstract considerations about file format obsolescence to develop actionable strategies for monitoring and mining information about the heterogeneous digital files the organizations are managing."&lt;ref&gt;{{cite web|publisher=National Digital Stewardship Alliance|title=National Agenda for Digital Stewardship 2014|url=http://www.digitalpreservation.gov/ndsa/documents/2014NationalAgenda.pdf|accessdate=17 March 2014|date=2014}}&lt;/ref&gt; 

File Format Action Plans are documents internal to an organization which list the type of digital files in its holdings and assess what actions should be taken to ensure its ongoing accessibility.&lt;ref&gt;{{cite web|last=Owens|first=Trevor|title=File Format Action Plans in Theory and Practice|url=http://blogs.loc.gov/digitalpreservation/2014/01/file-format-action-plans-in-theory-and-practice/|accessdate=17 March 2014|date=6 January 2014}}&lt;/ref&gt;  Examples include the [http://fclaweb.fcla.edu/node/795 Florida Digital Archive Action Plan] and University of Michigan's [http://deepblue.lib.umich.edu/static/about/deepbluepreservation.html Deep Blue Preservation and Format Support Policy].

== Copyright issues ==
Untangling [[copyright]] issues also presented a significant challenge for projects attempting to overcome the obsolescence issues related to the BBC Domesday Project. In addition to copyright surrounding the many contributions made by the estimated 1 million people who took part in the project, there are also copyright issues that relate to the technologies employed. It is likely that the Domesday Project will not be completely free of copyright restrictions until at least 2090, unless copyright laws are revised for earlier [[Copyright term|expiration]] of software into [[Public domain software|public domain]].&lt;ref&gt;{{Cite web |url=http://www2.si.umich.edu/CAMILEON/reports/IPRreport.doc|title= The CAMiLEON Project: Legal issues arising from the work aiming to preserve elements of the interactive multimedia work entitled "The BBC Domesday Project."|first= Andrew|last= Charlesworth|date= 5 November 2002|publisher= Information Law and Technology Unit, University of Hull|location= Kingston upon Hull|format= Microsoft Word|accessdate= 23 March 2011}}&lt;/ref&gt;

==Intentional obsolescence==
In some cases, obsolete technologies are used in a deliberate attempt to avoid data intrusion in a strategy known as "[[security through obsolescence]]".&lt;ref&gt;{{cite news | url=http://www.linux.com/articles/23313 | title=Security through obsolescence | first=Robin | last=Miller | publisher=Linux.com | date=2002-06-06 | accessdate=2008-07-18}}&lt;/ref&gt;

==See also==
* [[Obsolescence]]
* [[Digital preservation]]
* [[Digital Dark Age]]
* [[CAMiLEON]]
* [[Emulation (computing)]]
* [[M-DISC]]

==References==
{{reflist|30em}}

==External links==
* [http://www.digitalpreservation.gov/formats/ The Library of Congress, Sustainability of Digital Formats]
* [[Wired Magazine]]: [http://wired-vig.wired.com/wired/archive/6.09/saved.html What death can't destroy and how to digitize it]
* [https://www.icpsr.umich.edu/icpsrweb/content/datamanagement/preservation/ Digital Preservation at ICPSR]

{{DigitalPreservation}}
{{DEFAULTSORT:Digital Obsolescence}}
[[Category:Data management]]
[[Category:Digital libraries]]
[[Category:Digital preservation]]
[[Category:Future problems]]
[[Category:Obsolescence]]
[[Category:Records management]]</text>
      <sha1>o35mx4ql30kgkhu9nv103f3ki883xq6</sha1>
    </revision>
  </page>
  <page>
    <title>Imprima iRooms</title>
    <ns>0</ns>
    <id>51364737</id>
    <revision>
      <id>761663431</id>
      <parentid>736859457</parentid>
      <timestamp>2017-01-24T03:25:05Z</timestamp>
      <contributor>
        <username>Josve05a</username>
        <id>12023796</id>
      </contributor>
      <minor />
      <comment>/* top */added [[CAT:O|orphan]] tag using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3945" xml:space="preserve">{{Orphan|date=January 2017}}

{{Infobox company
| name = Imprima iRooms
| logo = Imprima-logo-hiRes-300dpi.jpg
| logo_size = 
| logo_alt = Imprima-logo-hiRes-300dpi.jpg
| logo_caption = 
| type = [[Private company|Private]]
| industry = [[Virtual Data Room]], [[Technology]]
| founded = 1910&lt;br/&gt;2001 &lt;small&gt;(relaunched)&lt;/small&gt;
| founder = 
| hq_location = London, United Kingdom
| area_served = Worldwide
| website = {{URL|www.imprima.com}}
}}
'''Imprima iRooms''' is a [[private company]] headquartered in London. It provides [[Virtual Data Room]] services to organisations worldwide, including the likes of [[Morgan Stanley]], [[HSBC]] and others.&lt;ref name="printweek"&gt;{{cite news|last1=Francis|first1=Jo|title=MBO at Imprima print operation {{!}} PrintWeek|url=http://www.printweek.com/print-week/news/1148840/mbo-imprima-print-operation|accessdate=22 August 2016|publisher=[[PrintWeek]]}}&lt;/ref&gt; It also has offices in Paris, Frankfurt, Amsterdam and New York.&lt;ref name="growthbusiness"&gt;{{cite news|title=The 21st century virtual data room: A how-to guide|url=http://www.growthbusiness.co.uk/growing-a-business/technology-for-business/2471012/the-21st-century-virtual-data-room-a-howto-guide.thtml|accessdate=22 August 2016|publisher=Growth Business UK}}&lt;/ref&gt;

==History==
Imprima was founded over 100 years ago and during this time has served the financial sector in a variety of different capacities.&lt;ref&gt;{{cite web|title=Imprima de Bussy Limited: Private Company Information|url=http://www.bloomberg.com/research/stocks/private/snapshot.asp?privcapId=613207|publisher=[[Bloomberg Businessweek]]|accessdate=22 August 2016}}&lt;/ref&gt; By 1990, the company was a leading provider of Financial Print solutions, involved in the publication and delivery of sensitive and business-critical communications for their clients.&lt;ref&gt;{{cite web|title=Companies and products...|url=http://www.ukauthority.com/market-report/news/4614/companies-and-products|publisher=UK Authority|accessdate=25 August 2016}}&lt;/ref&gt; In doing so, Imprima amassed an impressive customer list featuring some of the world’s most reputable financial advisors, law firms and corporations.&lt;ref name="printweek" /&gt;&lt;ref name="cloudnewsdaily"&gt;{{cite web|title=Virtual Data Room Providers|url=http://cloudnewsdaily.com/virtual-data-room/|publisher=Cloud News Daily|accessdate=22 August 2016}}&lt;/ref&gt;

==iRooms==
In 2001, Imprima launched their [[Virtual Data Room]] platform, iRooms.&lt;ref name="teletrader" /&gt; iRooms is used by organisations worldwide for Projects requiring secure online file storage and collaboration. Key use cases include [[Mergers &amp; Acquisitions]] (M&amp;A) activity and [[Real estate transaction|Real estate transactions]].&lt;ref name="Francis"&gt;{{cite news|title=Rebrand for Imprima Financial Print|url=http://www.printweek.com/print-week/news/1154511/rebrand-for-imprima-financial-print|accessdate=22 August 2016|work=www.printweek.com|publisher=[[PrintWeek]]}}&lt;/ref&gt; In 2012, iRooms software was completely revamped and receives regular upgrades.&lt;ref name="cloudnewsdaily" /&gt;&lt;ref name="Francis" /&gt;

==New Ownership==
In 2014, iRoom was acquired by its current owner, OTM Participation. At that time, Imprima operated two product lines: Financial Print and iRooms (Virtual Data Rooms).&lt;ref name="growthbusiness" /&gt; In November 2014, OTM Participation took the decision to divest away the Financial Print division, whose directors carried out an MBO.&lt;ref name="Francis" /&gt;&lt;ref name="teletrader"&gt;{{cite web|title=Imprima Adds Multiple Language Interfaces To New iRooms Release|url=http://www.teletrader.com/news/details/6743031?ts=1471881604044|publisher=www.teletrader.com|accessdate=22 August 2016}}&lt;/ref&gt;

==References==
{{Reflist}}

[[Category:Data management]]
[[Category:1910 establishments in the United Kingdom]]
[[Category:Companies based in London]]
[[Category:Technology companies established in 1910]]
[[Category:Technology companies of the United Kingdom]]</text>
      <sha1>baucb0kwll3ycntum9xhq6ixu2iazv1</sha1>
    </revision>
  </page>
  <page>
    <title>Machine-Readable Documents</title>
    <ns>0</ns>
    <id>51558108</id>
    <revision>
      <id>749697700</id>
      <parentid>748852842</parentid>
      <timestamp>2016-11-15T18:51:02Z</timestamp>
      <contributor>
        <username>Owen Ambur</username>
        <id>3035628</id>
      </contributor>
      <comment>Added reference to usability versus human-readability.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10853" xml:space="preserve">'''Machine-readable documents''' are [[document]]s whose content can be readily processed by [[computer]]s.  Such documents are distinguished from [[machine-readable data]] by virtue of having sufficient structure to provide the necessary context to support the business processes for which they are created.  [[Data]] without [[context (language use)]] is meaningless and lacks the four essential [http://www.archives.gov/records-mgmt/policy/managing-web-records.html#1.0 characteristics] of trustworthy [[business record]]s specified in [[ISO 15489 Information and documentation -- Records management]]: 
* Reliability
* Authenticity
* Integrity
* [[Usability]]
The vast bulk of information is [[unstructured data]] and, from a business perspective, that means it is "immature", i.e., Level 1 (chaotic) of the [[Capability Maturity Model]].  Such immaturity fosters inefficiency, diminishes quality, and limits effectiveness.  Unstructured information is also ill-suited for [[records management]] functions, provides inadequate [[evidence]] for legal purposes, drives up the cost of [[discovery (law)]] in [[litigation]], and makes access and usage needlessly cumbersome in routine, ongoing [[business process]]es.

There are at least four aspects to machine-readability:  
* First, words or phrases should be discretely delineated (tagged) so that computer software and/or hardware logic can be applied to them as individual conceptual elements.  
* Second, the semantics of each element should be specified so that computers can help human beings achieve a common understanding of their meanings and potential usages.  
* Third, if the relationships among the individual elements are also specified, computers can automatically apply inferences to them, thereby further relieving human beings of the burden of trying to understand them, particularly for purposes of inquiry, discovery, and analysis. 
* Fourth, if the structures of the documents in which the elements occur are also specified, human understanding is further enhanced and the data becomes more reliable for legal and business-quality purposes.

As early as 1981, the U.S. [[Government Accountability Office]] (GAO) began reporting on the problem of inadequate record-keeping practices in the U.S. federal government.&lt;ref&gt;{{cite web
 | url=http://www.gao.gov/products/PLRD-81-2
 | title=FEDERAL RECORDS MANAGEMENT: A History of Neglect
 | work=gao.gov
 | date=1981-02-24
 | accessdate=2016-09-08}}
&lt;/ref&gt;  Such deficiencies are not unique to government and advances in information technology mean that most information is now "born digital" and thus potentially far more easily managed by automated means.&lt;ref&gt;{{cite web
 | url=http://www.oclc.org/content/dam/research/activities/hiddencollections/borndigital.pdf
 | title=Defining "Born Digital": An Essay by Ricky Erway, OCLC Research
 | work=oclc.org
 | date=2010-11-30
 | accessdate=2016-09-08}}
&lt;/ref&gt;  However, in testimony to Congress in 2010, GAO highlighted problems with managing electronic records, and as recently as 2015, GAO has continued to report inadequacies in the performance of Executive Branch agencies in meeting records management requirements.&lt;ref&gt;{{cite web
 | url=http://www.gao.gov/new.items/d10838t.pdf
 | title=INFORMATION MANAGEMENT: The Challenges of Managing Electronic Records, Statement of Valerie C. Melvin, Director, Information Management and Human Capital Issues 
 | work=gao.gov
 | date=2010-06-17
 | accessdate=2016-09-08}}
&lt;/ref&gt;
&lt;ref&gt;{{cite web
 | url=http://www.gao.gov/products/GAO-15-339
 | title=INFORMATION MANAGEMENT: Additional Actions Are Needed to Meet Requirements of the Managing Government Records Directive
 | work=gao.gov
 | date=2015-05-14
 | accessdate=2016-09-08}}
&lt;/ref&gt;  Moreover, more than two decades after a major and formerly highly respected auditing firm, [[Arthur Andersen]], met its demise due to a records destruction scandal, record-keeping practices became a central issue in the 2016 Presidential election.

On January 4, 2011, President Obama signed H.R. 2142, the [[Government Performance and Results Act]] (GPRA) Modernization Act of 2010 (GPRAMA), into law as P.L. 111-352. Section 10 of GPRAMA requires U.S. federal agencies to publish their strategic and performance plans and reports in searchable, machine-readable format.&lt;ref&gt;{{cite web
 | url=http://xml.fido.gov/stratml/references/PL111-532StratML.htm#SEC10
 | title=GPRAMA SEC. 10. FORMAT OF PERFORMANCE PLANS AND REPORTS.
 | work=congress.gov
 | date=2011-01-04
 | accessdate=2016-09-08}}
&lt;/ref&gt;
Additionally, in 2013, he issued [[Executive Order]] 13642, Making Open and Machine Readable the New Default for Government Information in general.&lt;ref&gt;{{cite web
 | url=http://xml.fido.gov/stratml/carmel/EOOMRDwStyle.xml
 | title=Executive Order 13642 in open, standard, machine-readable Strategy Markup Language format
 | work=whitehouse.gov
 | date=2013-05-09
 | accessdate=2016-09-08}}
&lt;/ref&gt;
On July 28, 2016, the [[Office of Management and Budget]] (OMB) followed up by including in the revised issuance of Circular A-130 direction for agencies to use [http://xml.fido.gov/stratml/carmel/iso/A130wStyle.xml#_3f7d15f0-5799-11e6-8d37-8523b3fa12e0 open, machine-readable formats] and to publish "[http://xml.fido.gov/stratml/carmel/iso/A130wStyle.xml#_3f7d449e-5799-11e6-8d37-8523b3fa12e0 public information online in a manner that promotes analysis and reuse for the widest possible range of purposes]", meaning that the information is both publicly accessible and machine-readable.

In support of such policy direction, technological advancement is enabling more efficient and effective management and use of machine-readable electronic records.  [[Document-oriented database]]s have been developed for storing, retrieving, and managing document-oriented information, also known as semi-structured data.  Extensible Markup Language ([[XML]]) is a World Wide Web Consortium ([[W3C]]) [[World Wide Web Consortium#W3C recommendation .28REC.29|Recommendation]] setting forth rules for encoding documents in a format that is both [[human-readable]] and machine-readable.  Many [[XML editor]] tools have been developed and most, if not all major information technology applications support XML to greater or lesser degrees.  The fact that XML itself is an open, standard, machine-readable format makes it relatively easy for application developers to do so.

The W3C's accompanying XML Schema ([[XSD]]) Recommendation specifies how to formally describe the elements in an XML document.  With respect to the specification of XML schemas, the [[Organization for the Advancement of Structured Information Standards]] (OASIS) is a leading [[standards-developing organization]]. [[JSON#JSON Schema|JSON Schema]] was proposed by the [[Internet Engineering Task Force]] (IETF) but was allowed to expire in 2013 and thus is less mature and a riskier alternative to XSD, the most recent version of which was approved by the W3C in 2012.

The W3C's Extensible Stylesheet Language ([[XSL]]) family of languages provides for the transformation and rendering of XML documents for human-readable presentation.  Machine-readable documents can be automatically rendered in human-readable format but documents formatted primarily for attractiveness of presentation cannot easily be processed by computers to support [[usability]] by human beings.  

The [[Portable Document Format]] (PDF) is a file format used to present documents in a manner independent of application software, hardware, and operating systems. Each PDF file encapsulates a complete description of the presentation of the document, including the text, fonts, graphics, and other information needed to display it.  [[PDF/A]] is an ISO-standardized version of the PDF specialized for use in the archiving and long-term preservation of electronic documents.  PDF/A-3 allows embedding of other file formats, including XML, into PDF/A conforming documents, thus potentially providing the best of both human- and machine-readability.  The W3C's [[XSL-FO]] (XSL Formatting Objects) markup language is commonly used to generate PDF files

[[Metadata]], data about data, can be used to organize electronic resources, provide digital identification, and support the archiving and preservation of resources.  In well-structured, machine-readable electronic records, the content can be [[Repurposing|repurposed]] as both data and metadata.  In the context of electronic record-keeping systems, the terms "management" and "metadata" are virtually synonymous.  Given proper metadata, records management functions can be automated, thereby reducing the risk of [[spoliation of evidence]] and other fraudulent manipulations of records.  Moreover, such records can be used to automate the process of [[audit]]ing data maintained in [[database]]s, thereby reducing the risk of single points of failure associated with the [[Machiavellianism#In the workplace|Machiavellian]] concept of a [[single source of truth]].

[[Blockchain (database)]] is a new technology for maintaining continuously-growing lists of records secured from tampering and revision.  A key feature is that every node in a decentralized system has a copy of the blockchain so there is no [[single point of failure]] subject to manipulation and [[fraud]].

==See also==

* [[Budapest Declaration on Machine Readable Travel Documents]]
* [[Comparison of XML editors]]
* [[Integrity]] and particularly [[Data integrity]]
* [[Linked data]]
* [[Machine-readable passport]]
* [[Open data]]
* [[Data reliability|Reliability]], particularly [[Reliability (statistics)]], [[Data reliability]], [[Reliability (computer networking)]], and [[Reliability (research methods)]]
* [[Strategy Markup Language]] (StratML)
* [[Structured document]]
* [[Tag (metadata)]]
* [[Universal Business Language]] (UBL)
* [[XBRL]] (eXtensible Business Reporting Language)

==References==
{{reflist}}

==External  links==
* [http://xml.fido.gov/stratml/carmel/M-13-13wStyle.xml#_78e85ef4-b91c-11e2-bf2b-79d279ad226c OMB M-13-13], Open Data Policy: Managing Information as an Asset, which requires agencies to use open, machine-readable, data format standards
* [http://ambur.net/CaponeConsultancyMethod.pdf Driving a Stake in the Heart of the Capone Consultancy Method of Records Management: Best Practices for Correcting Non-Records Non-Policy Nonsense], March 9, 2015
* The U.S. Code, which includes [http://uscode.house.gov/search.xhtml?searchString=machine-readable&amp;pageNumber=1&amp;itemsPerPage=100&amp;sortField=CODE_ORDER&amp;action=search&amp;q=bWFjaGluZS1yZWFkYWJsZQ%3D%3D%7C%3A%3A%3A%3A%3A%3A%3A%3Afalse%3A%7C%3A%3A%3A%3A%3A%3A%3A%3Afalse%3A%7Cfalse%7C%5B%3A%3A%3A%3A%3A%3A%3A%3Afalse%3A%5D%7C%5B%3A%5D 51 references] to the term "machine-readable" as of September 10, 2016

[[Category:Data management]]
[[Category:Records management]]</text>
      <sha1>p22a7jz3oplavgfrg5sfu88i5kwl8nq</sha1>
    </revision>
  </page>
  <page>
    <title>Big memory</title>
    <ns>0</ns>
    <id>51756257</id>
    <revision>
      <id>760003267</id>
      <parentid>741524795</parentid>
      <timestamp>2017-01-14T10:52:48Z</timestamp>
      <contributor>
        <username>DrStrauss</username>
        <id>29858946</id>
      </contributor>
      <comment>rm tag</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1923" xml:space="preserve">'''Big-memory''' is a term used to describe server workloads which need to run on machines with a large amount of RAM ([[Random-access memory]]) memory. Some example workloads are databases, in-memory caches, and graph analytics.&lt;ref&gt;{{cite web |url=http://research.cs.wisc.edu/multifacet/papers/isca13_direct_segment.pdf|title=Efficient Virtual Memory for Big Memory Servers|accessdate=2016-09-24 }}&lt;/ref&gt;
Or, more generally, [[Data Science]] and [[Big data]].

Some database systems are designed to run mostly in memory, rarely if ever retrieving data from disk or flash memory. See a [[List of in-memory databases]].

The performance of big memory systems depends on how the CPU's or CPU cores access the memory, via a conventional [[Memory controller]] or via NUMA ( [[Non-uniform memory access]] ). Performance also depends on the size and design of the [[CPU cache]].

Performance also depends on OS design. The "Huge pages" feature in Linux can improve the efficiency of [[Virtual Memory]].&lt;ref&gt;{{cite web |url=http://lwn.net/Articles/374424/ |title=Huge pages part 1 (Introduction)  |accessdate=2016-09-24 }}&lt;/ref&gt; The new "Transparent huge pages" feature in Linux can offer better performance for some big-memory workloads.&lt;ref&gt;{{cite web |url=http://lwn.net/Articles/423584/ |title=Transparent huge pages in 2.6.38 |accessdate=2016-09-24 }}&lt;/ref&gt; The "Large-Page Support" in Microsoft Windows enables server applications to establish large-page memory regions which are typically three orders of magnitude larger than the native page size.&lt;ref&gt;{{cite web |url=https://msdn.microsoft.com/en-us/library/windows/desktop/aa366720(v=vs.85).aspx|title=Large-Page Support |accessdate=2016-09-24 }}&lt;/ref&gt;

==References==
{{reflist}}


{{database-stub}}
[[Category:Big data| ]]
[[Category:Data management]]
[[Category:Distributed computing problems]]
[[Category:Technology forecasting]]
[[Category:Transaction processing]]</text>
      <sha1>3qhz8zy29c0wfpnz9xpth1cqwagnu6v</sha1>
    </revision>
  </page>
  <page>
    <title>Data</title>
    <ns>0</ns>
    <id>18985040</id>
    <revision>
      <id>762555663</id>
      <parentid>762555662</parentid>
      <timestamp>2017-01-29T14:33:47Z</timestamp>
      <contributor>
        <username>ClueBot NG</username>
        <id>13286072</id>
      </contributor>
      <minor />
      <comment>Reverting possible vandalism by [[Special:Contribs/41.48.16.44|41.48.16.44]] to version by LilyKitty. [[WP:CBFP|Report False Positive?]] Thanks, [[WP:CBNG|ClueBot NG]]. (2914878) (Bot)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11607" xml:space="preserve">{{about||data in computer science|Data (computing)|other uses}}
{{pp-move-indef}}
[[File:Data types - en.svg|thumb|right|200px|Some of the different types of data.]]
'''Data''' ({{IPAc-en|ˈ|d|eɪ|t|ə}} {{respell|DAY|tə}}, {{IPAc-en|ˈ|d|æ|t|ə}} {{respell|DA|tə}}, or {{IPAc-en|ˈ|d|ɑː|t|ə}} {{respell|DAH|tə}})&lt;ref&gt;The pronunciation {{IPAc-en|ˈ|d|eɪ|t|ə}} {{respell|DAY|tə}} is widespread throughout most varieties of English. The pronunciation {{IPAc-en|ˈ|d|æ|t|ə}} {{respell|DA|tə}} is chiefly [[Hiberno-English|Irish]] and [[American English|North American]]. The pronunciation {{IPAc-en|ˈ|d|ɑː|t|ə}} {{respell|DAH|tə}} is chiefly [[Australian English|Australian]], [[New Zealand English|New Zealand]] and [[South African English|South African]]. Each pronunciation may be realized differently depending on the dialect/language of the speaker.&lt;/ref&gt; is a [[set (mathematics)|set]] of values of [[Qualitative data|qualitative]] or [[Quantitative data|quantitative]] variables. An example of qualitative data would be an [[anthropologist]]'s handwritten notes about her interviews with people of an Indigenous tribe. Pieces of data are individual pieces of [[information]]. While the concept of data is commonly associated with [[scientific research]], data is collected by a huge range of organizations and institutions, including businesses (e.g., sales data, revenue, profits, [[stock price]]), governments (e.g., [[crime rate]]s, [[unemployment rate]]s, [[literacy]] rates) and non-governmental organizations (e.g., censuses of the number of [[homelessness|homeless people]] by non-profit organizations).

Data is [[measurement|measured]], [[data reporting|collected and reported]], and [[data analysis|analyzed]], whereupon it can be [[data visualization|visualized]] using graphs, images or other analysis tools. Data as a general [[concept]] refers to the fact that some existing [[information]] or [[knowledge]] is ''[[Knowledge representation and reasoning|represented]]'' or ''[[code]]d'' in some form suitable for better usage or [[data processing|processing]]. ''[[Raw data]]'' ("unprocessed data") is a collection of [[number]]s or [[character (computing)|characters]] before it has been "cleaned" and corrected by researchers. Raw data needs to be corrected to remove [[outlier]]s or obvious instrument or data entry errors (e.g., a thermometer reading from an outdoor Arctic location recording a tropical temperature).  Data processing commonly occurs by stages, and the "processed data" from one stage may be considered the "raw data" of the next stage. [[Field work|Field data]] is raw data that is collected in an uncontrolled "[[in situ]]" environment. [[Experimental data]] is data that is generated within the context of a scientific investigation by observation and recording. Data has been described as the new [[Petroleum|oil]] of the [[digital economy]].&lt;ref&gt;[https://www.wired.com/insights/2014/07/data-new-oil-digital-economy/ Data Is the New Oil of the Digital Economy]&lt;/ref&gt;&lt;ref&gt;[https://spotlessdata.com/blog/data-new-oil Data is the new Oil]&lt;/ref&gt;

== Etymology and terminology ==
The first English use of the word "data" is from the 1640s. Using the word "data" to mean "transmittable and storable computer information" was first done in 1946. The expression "data processing" was first used in 1954.&lt;ref name="eol"&gt;http://www.etymonline.com/index.php?term=data&lt;/ref&gt;

The [[Data (word)|Latin word ''data'']] is the plural of ''datum'', "(thing) given," neuter past participle of ''dare'' "to give".&lt;ref name="eol"/&gt;  Data may be used as a plural noun in this sense, with some writers in the 2010s using ''datum'' in the singular and ''data'' for plural. In the 2010s, though, in non-specialist, everyday writing, "data" is most commonly used in the singular, as a [[mass noun]] (like "information", "sand" or "rain").&lt;ref&gt;{{cite web|last=Hickey |first=Walt |url=http://fivethirtyeight.com/datalab/elitist-superfluous-or-popular-we-polled-americans-on-the-oxford-comma/ |title=Elitist, Superfluous, Or Popular? We Polled Americans on the Oxford Comma |publisher=FiveThirtyEight |date=2014-06-17 |accessdate=2015-05-04}}&lt;/ref&gt;

== Meaning ==
Data, [[information]], [[knowledge]] and [[wisdom]] are closely related concepts, but each has its own role in relation to the other, and each term has its own meaning. Data is collected and analyzed; data only becomes information suitable for making decisions once it has been analyzed in some fashion. &lt;ref&gt;{{cite web|title=Joint Publication 2-0, Joint Intelligence|url=http://www.dtic.mil/doctrine/new_pubs/jp2_0.pdf|work=Defense Technical Information Center (DTIC)|publisher=Department of Defense|accessdate=February 22, 2013|pages=GL-11|date=22 June 2007}}&lt;/ref&gt; [[Knowledge]] is derived from extensive amounts of experience dealing with information on a subject. For example, the height of [[Mount Everest]] is generally considered data. The height can be recorded precisely with an [[altimeter]] and entered into a database. This data may be included in a book along with other data on Mount Everest to describe the mountain in a manner useful for those who wish to make a decision about the best method to climb it. Using an understanding based on experience climbing mountains to advise persons on the way to reach Mount Everest's peak may be seen as "knowledge". Some complement the series "data", "information" and "knowledge" with "wisdom", which would mean the status of a person in possession of a certain "knowledge" who also knows under which circumstances is good to use it.

Data is the least abstract concept, information the next least, and knowledge the most abstract.&lt;ref&gt;{{cite web|author=Akash Mitra|year=2011|title=Classifying data for successful modeling|url=http://www.dwbiconcepts.com/data-warehousing/12-data-modelling/101-classifying-data-for-successful-modeling.html}}&lt;/ref&gt; Data becomes information by interpretation; e.g., the height of Mount Everest is generally considered "data", a book on Mount Everest geological characteristics may be considered "information", and a climber's guidebook containing practical information on the best way to reach Mount Everest's peak may be considered "knowledge". "Information" bears a diversity of meanings that ranges from everyday usage to technical use. Generally speaking, the concept of information is closely related to notions of constraint, communication, control, data, form, instruction, knowledge, meaning, mental stimulus, pattern, perception, and representation.&lt;!--given by nupur seth--&gt; Beynon-Davies uses the concept of a [[sign]] to differentiate between data and information; data is a series of symbols, while information occurs when the symbols are used to refer to something.&lt;ref&gt;{{cite book|author=P. Beynon-Davies|year=2002|title=Information Systems: An introduction to  informatics in organisations|publisher=[[Palgrave Macmillan]] |location=Basingstoke, UK|isbn=0-333-96390-3}}&lt;/ref&gt;&lt;ref&gt;{{cite book|author=P. Beynon-Davies|year=2009|title=Business information systems|publisher=Palgrave |location=Basingstoke, UK|isbn=978-0-230-20368-6}}&lt;/ref&gt;

Before the development of computing devices and machines, only people could collect data and impose patterns on it. Since the development of computing devices and machines, these devices can also collect data. In the 2010s, computers are widely used in many fields to collect data and sort or process it, in disciplines ranging from [[marketing]], analysis of [[social services]] usage by citizens to scientific research. These patterns in data are seen as information which can be used to enhance knowledge. These patterns may be interpreted as "[[truth]]" (though "truth" can be a subjective concept), and may be authorized as aesthetic and ethical criteria in some disciplines or cultures. Events that leave behind perceivable physical or virtual remains can be traced back through data. Marks are no longer considered data once the link between the mark and observation is broken.&lt;ref&gt;{{cite book|author=Sharon Daniel|title=The Database: An Aesthetics of Dignity}}&lt;/ref&gt;

Mechanical computing devices are classified according to the means by which they represent data. An [[analog computer]] represents a datum as a voltage, distance, position, or other physical quantity. A [[Computer|digital computer]] represents a piece of data as a sequence of symbols drawn from a fixed [[alphabet]]. The most common digital computers use a binary alphabet, that is, an alphabet of two characters, typically denoted "0" and "1". More familiar representations, such as numbers or letters, are then constructed from the binary alphabet. Some special forms of data are distinguished. A [[computer program]] is a collection of data, which can be interpreted as instructions. Most computer languages make a distinction between programs and the other data on which programs operate, but in some languages, notably [[Lisp (programming language)|Lisp]] and similar languages, programs are essentially indistinguishable from other data. It is also useful to distinguish [[metadata]], that is, a description of other data. A similar yet earlier term for metadata is "ancillary data."  The prototypical example of metadata is the library catalog, which is a description of the contents of books.

== In other fields ==
Though data is also increasingly used in other fields, it has been suggested that the highly interpretive nature of them might be at odds with the ethos of data as "given". Peter Checkland introduced the term ''capta'' (from the Latin ''capered'', “to take”) to distinguish between an immense number of possible data and a sub-set of them, to which attention is oriented.&lt;ref&gt;{{cite book | author = P. Checkland and S. Holwell | title = Information, Systems, and Information Systems: Making Sense of the Field. | year = 1998 | publisher = John Wiley &amp; Sons | location = Chichester, West Sussex | isbn = 0-471-95820-4 | pages = 86–89  }}&lt;/ref&gt; [[Johanna Drucker]] has argued that since the humanities affirm knowledge production as "situated, partial, and constitutive," using ''data'' may introduce assumptions that are counterproductive, for example that phenomena are discrete or are observer-independent.&lt;ref&gt;{{cite web
 |author=Johanna Drucker
 |year=2011
 |title=Humanities Approaches to Graphical Display
 |url=http://www.digitalhumanities.org/dhq/vol/5/1/000091/000091.html
}}&lt;/ref&gt; The term ''capta'', which emphasizes the act of observation as constitutive, is offered as an alternative to ''data'' for visual representations in the humanities.

== See also ==
{{div col|5}}
* [[Biological data]]
* [[Data acquisition]]
* [[Data analysis]]
* [[Data cable]]
* [[Dark data]]
* [[Data domain]]
* [[Data element]]
* [[Data farming]]
* [[Data governance]]
* [[Data integrity]]
* [[Data maintenance]]
* [[Data management]]
* [[Data mining]]
* [[Data modeling]]
* [[Data visualization]]
* [[Computer data processing]]
* [[Data publication]]
* [[Information privacy|Data protection]]
* [[Data remanence]]
* [[Data set]]
* [[Data warehouse]]
* [[Database]]
* [[Datasheet]]
* [[Environmental data rescue]]
* [[Fieldwork]]
* [[Metadata]]
* [[Open data]]
* [[Scientific data archiving]]
* [[Statistics]]
* [[Computer memory]]
* [[Data structure]]
* [[Raw Data]]
* [[Secondary Data]]
{{div col end}}

== References ==
{{FOLDOC}}
{{Reflist}}

== External links ==
{{Wiktionary}}
* [http://purl.org/nxg/note/singular-data Data is a singular noun] (a detailed assessment)

{{Statistics}}

[[Category:Computer data| ]]
[[Category:Data| ]]
[[Category:Data management]]</text>
      <sha1>g0jivpy60x98e6s0l0iweihoxpcsqfn</sha1>
    </revision>
  </page>
  <page>
    <title>Rasdaman</title>
    <ns>0</ns>
    <id>36377941</id>
    <revision>
      <id>751624882</id>
      <parentid>732639512</parentid>
      <timestamp>2016-11-26T21:58:53Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <comment>removed [[Category:Data analysis]]; added [[Category:Data management]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="12886" xml:space="preserve">{{Infobox software
| name = rasdaman
| logo = [[Image:Rasdaman logo.png|frame|center|x250px|alt=rasdaman logo (used with permission of copyright holder)|rasdaman logo (used with permission of copyright holder)]]
| developer = rasdaman GmbH
| latest_release_version = rasdaman v9.2.1
| latest_release_date = {{release date |2016|02|17}}
| status = Active
| operating_system = most [[Unix-like]] operating systems
| programming language = [[C++]]&lt;ref&gt;{{cite web |url=https://www.openhub.net/p/rasdaman |title=The rasdaman Open Source Project on Open Hub |work=Open Hub |publisher=Black Duck Software |accessdate=2016-08-01}}&lt;/ref&gt;
| genre = [[Array DBMS]]
| license = [[GNU General Public License|GPL v3]]/[[GNU Lesser General Public License|LGPL v3]] or [[Proprietary software|proprietary]]&lt;ref&gt;{{cite web|url=http://rasdaman.org/wiki/License |title=Rasdaman License |publisher=rasdaman.org |date= |accessdate=2016-08-01}}&lt;/ref&gt;
| website = {{URL|http://rasdaman.org}}, {{URL|http://rasdaman.com}}
}}

'''Rasdaman''' ("raster data manager") is an [[Array DBMS]], that is: a [[Database Management System]] which adds capabilities for storage and retrieval of massive multi-dimensional [[array data structure|arrays]], such as sensor, image, and statistics data. A frequently used synonym to arrays is raster data, such as in 2-D [[raster graphics]]; this actually has motivated the name ''rasdaman''. However, rasdaman has no limitation in the number of dimensions - it can serve, for example, 1-D measurement data, 2-D satellite imagery, 3-D x/y/t image time series and x/y/z exploration data, 4-D ocean and climate data, and even beyond spatio-temporal dimensions.

== History ==

In 1989, [[Peter Baumann (computer scientist)|Peter Baumann]] started a research on database support for images, then at [[Fraunhofer Society|Fraunhofer Computer Graphics Institute]]. Following an in-depth investigation on raster data formalizations in imaging, in particular the AFATL Image Algebra, he established a database model for multi-dimensional arrays, including a data model and declarative query language.&lt;ref&gt;Baumann, P.: [http://www.informatik.uni-trier.de/~ley/db/journals/vldb/vldb3.html#Baumann94 On the Management of Multidimensional Discrete Data]. VLDB Journal 4(3)1994, Special Issue on Spatial Database Systems, pp. 401 - 444&lt;/ref&gt;

At [[Technical University Munich|TU Munich]], in the EU funded basic research project ''RasDaMan'', a first prototype was established, on top of the O2 [[Object-oriented database|object-oriented DBMS]], and tested in Earth and Life science applications.&lt;ref name="cordis.europa.eu/"&gt;http://cordis.europa.eu/result/rcn/20754_en.html&lt;/ref&gt; Over further EU funded projects, this system was completed and extended to support relational DBMSs.
A dedicated research spin-off, rasdaman GmbH,&lt;ref name="Rasdaman.com"&gt;http://www.rasdaman.com&lt;/ref&gt; was established to give commercial support in addition to the research which subsequently has been continued at [[Jacobs University Bremen|Jacobs University]].&lt;ref name="Rasdaman.com/Archive"&gt;http://www.rasdaman.com/News/archive.php&lt;/ref&gt; Since then, both entities collaborate on the further development and use of the rasdaman technology.

== Concepts ==

=== Data model ===

Based on an array algebra&lt;ref&gt;Baumann, P.: [http://www.informatik.uni-trier.de/~ley/db/conf/ngits/ngits99.html#Baumann99 A Database Array Algebra for Spatio-Temporal Data and Beyond]. Proc. NGITS’99, LNCS 1649, Springer 1999, pp.76-93&lt;/ref&gt; specifically developed for database purposes, rasdaman adds a new attribute type, array, to the relational model. As this array definition is parametrized it constitutes a [[Second-order logic|second-order]] construct or [[Template (C++)|template]]); this fact is reflected by the second-order functionals in the algebra and query language.

For historical reasons, [[Table (database)|tables]] are called ''collections'', as initial design emphasized an embedding into the object-oriented database standard, [[ODMG]]. Anticipating a full integration with SQL, rasdaman collections represent a binary relation with the first attribute being an [[object identifier]] and the second being the array. This allows to establish [[Foreign key|foreign key references]] between arrays and regular [[Tuple|relational tuples]].

=== Raster Query Language ===

The rasdaman query language, rasql, embeds itself into standard SQL and its set-oriented processing.
On the new attribute type, multi-dimensional arrays, a set of extra operations is provided which all are based on a minimal set of algebraically defined core operators, an ''array constructor'' (which establishes a new array and fills it with values) and an ''array condenser'' (which, similarly to SQL aggregates, derives scalar summary information from an array). The query language is declarative (and, hence, optimizable) and safe in evaluation - that is: every query is guaranteed to return after a finite number of processing steps.

The rasql query guide&lt;ref&gt;n.n.: [http://rasdaman.org/browser/manuals_and_examples/manuals/doc-guides/ql-guide.pdf Rasdaman Query Language Guide]&lt;/ref&gt; provides details, here some examples may illustrate its use:

* "From all 4-D x/y/z/t climate simulation data cubes, a cutout which contains all in x, a y extract between 100 and 200, all available along z, and a slice at position 42 (effectively resulting in a 3-D x/y/z cube)":
&lt;source lang="sql"&gt;
select c[ *:*, 100:200, *:*, 42 ] 
from   ClimateSimulations as c 
&lt;/source&gt;

* "In all Landsat satellite images, suppress all non-green areas":
&lt;source lang="sql"&gt;
select img * (img.green &gt; 130)
from   LandsatArchive as img
&lt;/source&gt;

Note: this is a ''very'' naive phrasing of vegetation search; in practice one would use the [[NDVI]] formula, use null values for cloud masking, and several more techniques.

* "All MRI images where, in some region defined by the bit masks, intensity exceeds a threshold of 250":
&lt;source lang="sql"&gt;
select img
from   MRI as img, Masks as m
where  some_cells( img &gt; 250 and m )
&lt;/source&gt;

* "A 2-D x/y slice from all 4-D climate simulation data cubes, each one encoded in PNG format": 
&lt;source lang="sql"&gt;
select png( c[ *:*, *:*, 100, 42 ] )
from   ClimateSimulations as c 
&lt;/source&gt;

== Architecture ==

=== Storage management ===

[[Image:Sample tiling of an array for storage in rasdaman.png|frame|x110px|alt=Sample rasdaman tiling|Sample array tiling in rasdaman]]

Raster objects are maintained in a standard relational database, based on the partitioning of an raster object into ''tiles''.&lt;ref&gt;
Furtado, P., Baumann, P.: [http://www.informatik.uni-trier.de/~ley/db/conf/icde/icde99.html#FurtadoB99 Storage of Multidimensional Arrays based on Arbitrary Tiling]. Proc. ICDE'99, March 23–26, 1999, Sydney, Australia, pp. 328-336&lt;/ref&gt; Aside from a regular subdivision, any user or system generated partitioning is possible. As tiles form the unit of disk access, it is of critical importance that the tiling pattern is adjusted to the query access patterns; several tiling strategies assist in establishing a well-performing tiling. A geo index is employed to quickly determine the tiles affected by a query. Optionally, tiles are compressed using one of various choices, including lossless and lossy (wavelet) algorithms; independently from that, query results can be comressed for transfer to the client. Both tiling strategy and compression comprise database tuning parameters.

Tiles and tile index are stored as [[Binary large object|BLOBs]] in a relational database which also holds the data dictionary needed by rasdaman’s dynamic type system. Adaptors are available for several relational systems, among them open-source [[Postgresql|PostgreSQL]].
For arrays larger than disk space, hierarchical storage management (HSM) support has been developed.

=== Query processing ===

Queries are parsed, optimised, and executed in the rasdaman server. The parser receives the query string and generates the operation tree. Further, it applies algebraic optimisation rules to the query tree where applicable; of the 150 algebraic rewriting rules, 110 are actually optimising while the other 40 serve to transform the query into canonical form. Parsing and optimization together take less than a millisecond on a laptop.

Execution follows a ''tile streaming'' paradigm: whenever possible, array tiles addressed by a query are fetched sequentially, and each tile is discarded after processing. This leads to an architecture scalable to data volumes exceeding server main memory by orders of magnitude.
   
Query execution is parallelised. First, rasdaman offers inter-query parallelism: A dispatcher schedules requests into a pool of server processes on a per-transaction basis. Intra-query parallelism transparently distributes query subtrees across available cores, GPUs, or cloud nodes.

=== Client APIs ===

The primary interface to rasdaman is the query language. Embeddings into C++ and Java APIs allow invocation of queries, as well as client-side convenience functions for array handling. Arrays per se are delivered in the main memory format of the client language and processor architecture, ready for further processing. Data format codecs allow to retrieve arrays in common raster formats, such as [[Comma-separated values|CSV]], [[Portable Network Graphics|PNG]], and [[Netcdf|NetCDF]].

A Web design toolkit, raswct, is provided which allows to establish Web query frontends easily, including graphical widgets for parametrized query handling, such as sliders for thresholds in queries.

=== Geo Web Services ===

A [[Java (programming language)|Java]] servlet, ''petascope'', running as a rasdaman client offers Web service interfaces specifically for geo data access, processing and filtering. 
The following [[Open Geospatial Consortium|OGC]] standards are supported: [[Web Map Service|WMS]], [[Web Coverage Service|WCS]], [[Web Coverage Processing Service|WCPS]], and [[Web Processing Service|WPS]].

For [[Web Coverage Service|WCS]] and [[Web Coverage Processing Service|WCPS]], rasdaman is the [[reference implementation]].

== Status and license model ==

Today, rasdaman is a fully-fledged implementation offering select / insert / update / delete array query functionality. It is being used in both research and commercial installations.

In a collaboration of the original code owner, rasdaman GmbH&lt;ref name="Rasdaman.com"/&gt; and [[Jacobs University]], a code split was performed in 2008 - 2009 resulting in ''rasdaman community'',&lt;ref&gt;http://www.rasdaman.org&lt;/ref&gt; an [[open source]] branch, and ''rasdaman enterprise'', the commercial branch. Since then, ''rasdaman community'' is being maintained by Jacobs University whereas ''rasdaman enterprise'' remains proprietary to rasdaman GmbH.
The difference between both variants mainly consists of performance boosters (such as specific optimization techniques) intended to support particularly large databases, user numbers, and complex queries; Details are available on the ''rasdaman community'' website.&lt;ref&gt;[http://rasdaman.eecs.jacobs-university.de/trac/rasdaman/wiki/License rasdaman license model]&lt;/ref&gt;

The ''rasdaman community'' license releases the server in [[GPL]] and all client parts in [[LGPL]], thereby allowing to use the system in any kind of license environment.

== Impact and Use ==

Being the first Array DBMS shipped (first prototype available in 1996), rasdaman has shaped this recent database research domain. Concepts of the data and query model (declarativeness, sometimes choice of operators) find themselves in more recent approaches.

In 2008, the [[Open Geospatial Consortium]] released the [[Web Coverage Processing Service]] standard which defines a raster query language based on the concept of a [[Coverage data|coverage]]. Operator semantics&lt;ref&gt;Baumann, P.: [http://www.springerlink.com/openurl.asp?genre=article&amp;id=doi:10.1007/s10707-009-0087-2 The OGC Web Coverage Processing Service (WCPS) Standard]. Geoinformatica, 14(4)2010, pp. 447-479&lt;/ref&gt; is influenced by the rasdaman array algebra.

EarthLook&lt;ref&gt;http://standards.rasdaman.org/&lt;/ref&gt; is a showcase for [[Open Geospatial Consortium|OGC]] [[Coverage data|coverage]] standards in action, offering 1-D through 4-D use cases of raster data access and ad-hoc processing. EarthLook is built on rasdaman.

A sample large project in which rasdaman is being used for large-scale services in all [[Earth sciences]] is EarthServer:&lt;ref&gt;http://www.earthserver.eu&lt;/ref&gt; six services with a volume of at least 100 Terabyte each are being set up for integrated data / metadata retrieval and distributed query processing.

==References==
{{Reflist}}

{{DEFAULTSORT:rasdaman}}
[[Category:Free database management systems]]
[[Category:Proprietary database management systems]]
[[Category:NoSQL]]
[[Category:Data management]]
[[Category:Query languages]]</text>
      <sha1>229cr7hjoihp68hzim7docklms7kav0</sha1>
    </revision>
  </page>
  <page>
    <title>5 Ways of Conceptualizing Data</title>
    <ns>0</ns>
    <id>52060631</id>
    <revision>
      <id>758411216</id>
      <parentid>752813742</parentid>
      <timestamp>2017-01-05T07:31:05Z</timestamp>
      <contributor>
        <username>Melcous</username>
        <id>20472590</id>
      </contributor>
      <minor />
      <comment>fix spelling, publically --&gt; publicly</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9513" xml:space="preserve">{{multiple issues|
{{essay-like|date=December 2016}}
{{notability|date=December 2016}}
{{original research|date=December 2016}}
}}
{{Orphan|date=December 2016}}

[[Data]] can be viewed as a measurement of numbers, and characters that are set in a way to understand a certain subject. However, there are many different ways to view data; such as conceptualizing data. These are five ways of conceptualizing data. They all have positive and negative points to each technique. Although they are different, they all bring up questions and concerns with data collection and what happens with the information afterward. Another concern is what is the goal with the data that has been collected depending on the category. The five ways of conceptualizing data are technically, ethically, politically and economically, spatially/ temporal, and philosophically. Typically viewed by critical data scholars, they have all of these ways of viewing data because it is important to see the different ways that data can be viewed and to see if there may be any bias. Not only is it important to see if there is any bias, however, it is also important to understand what all the data will mean in the bigger picture. The way that this is normally done is by understanding raw data, then placing them into categories that will help with the better understanding and creating new knowledge.

==Technically==
Technically viewing data concerns the knowledge about the quality of data, if it is reliable, if it is authentic, if it is valid. It is also about knowing how the data is structured, shared, processed, and analyzed.&lt;ref&gt;(Kitchin, p.12)&lt;/ref&gt; There are views about the concerns around data such as the representativeness, how it is uncertain, the reliability of it, the chances of any errors, the likelihood of any bias, and around the measuring of the research design and the execution of it.&lt;ref&gt;(Kitchin, p.13)&lt;/ref&gt; There are also questions around if this form of scientific technique is going to bring the data that is wanted and needed.&lt;ref&gt;(Kitchin, p.13)&lt;/ref&gt; Other reliability concerns go with this technical view about data such as Quixotic reliability, Diachronic reliability, and Synchronic reliability. Quixotic reliability concern is where there is one observation method which produces unvarying measurements.&lt;ref&gt;(Kitchin, p.13-14)&lt;/ref&gt; Diachronic reliability is the stability of an observation through time. Lastly, Synchronic reliability is the similarity of observations within the same time period.&lt;ref&gt;(Kitchin,p.14)&lt;/ref&gt; With it being technology, there are many different ways that errors could arise, such as, missing data, mistakes, misunderstaning's, bias’, and uncertainty.&lt;ref&gt;(Kitchin, p.14)&lt;/ref&gt;

==Ethically==
The [[ethics|ethical]] view of data is more about the idea of why the data is generated, and what use the data is going to be placed in. There are concerns around how the data will be shared, protected, traded, and to how they are employed”.&lt;ref&gt;(Kitchin, p.15)&lt;/ref&gt; This also deals with the issue of sensitivity. Some data is low when it comes to sensitivity, such as the traffic. However, some are a lot higher, such as speaking to survivors of crime.&lt;ref&gt;(Kitchin, p. 15)&lt;/ref&gt; With the sensitivity scale, there comes privacy issues, how someone may be treated, and the issue of human rights.&lt;ref&gt;(Kitchin, p. 15)&lt;/ref&gt; It is helpful to know that some companies have a data protection act and have privacy laws.&lt;ref&gt;(Kitchin, p.15)&lt;/ref&gt; Other components that add to the category of Ethics are the question of equality, fairness, justice, honesty, respect, entitlements, rights, and care of the information that is provided and towards those that give the information.&lt;ref&gt;(Kitchin, p.14)&lt;/ref&gt; The honesty, respect and the care of the information can also be misinformed to the subject that is giving the data willingly. Causing ethical concerns for how long the information will be kept, or what the information will be used for. This is an ethical concern in the exchange of the subject and the researcher.&lt;ref&gt;(Jacob)&lt;/ref&gt;

==Politically and economically==
[[Politics|politically]] and [[economic]]ally viewed data is seen to how the data could be viewed or theorized as public goods, intellectual property, political capital, and how they are traded and how they are regulated.&lt;ref&gt;(Kitchin, p.16)&lt;/ref&gt; Economically there are many decisions when funding data researching, as well as investing in data researching. Data could be used to manage goals and raise the profits and values to those that invest in it.&lt;ref&gt;(Kitchin, p.15)&lt;/ref&gt; Such as the multi-billion-dollar data marketplace, where many companies are trading and using that data to help themselves make a profit. It is positively effecting due to the production of knowledge.&lt;ref&gt;(Kitchin, p. 15)&lt;/ref&gt; The more that the company knows about what the people want, and how to market to them, the more that they may profit and gain off of the data, due to them giving what the people want. However, there still is the political side to this. Although the data can make a profit and is economically great, there is also the competition which want to influence opinions and make the data terrain greater.&lt;ref&gt;(Kitchin, p.16)&lt;/ref&gt; It is also political because the difference between publicly good data, which is shared with anyone that can have access to it, is much different than business data. This is because business data is wanting to keep the data that they have found and use it to their advantage, such as the “production of knowledge.” &lt;ref&gt;(Kitchin, p.16)&lt;/ref&gt; The publicly good data is free to anyone that wants to view it, which would not be helpful in any way to any business strategies or marketing.&lt;ref&gt;(Kitchin, p. 16)&lt;/ref&gt;

==Spatially/temporal==
Spatial and [[temporal]] views data around technical, ethical, political, and economic [[Regime]] with the production of the data.&lt;ref&gt;(Kitchin, p.17)&lt;/ref&gt; The way that the terms spatial and temporal can be viewed is around how the data is developed and changed across time and space. Although, depending on the time and where this data is being collected, the process, the analysis, the storage of some information, yet not of others will be different, just due to a time frame and area will be different than others because of the different history that has happened and the different geographical locations. As noted the process of taking in data changes over time, however, they are never sudden changes. These changes happen slowly over time due to different laws that come in place around how data is handled or protected, the different forms of organizing, the improvements around administration, if any new technology has formed, when the methods of data sorting have changed, along with the methods of sorting the data, the geographical statistics that vary and the new techniques of statistics.&lt;ref&gt;(Kitchin, p.17)&lt;/ref&gt; Not only does the geographical location change how the assemblage of data is taken is, but it can also be different depending on the person due to how they manage the data, or how they produce it.&lt;ref&gt;(Kitchin, p.17)&lt;/ref&gt;

Looking over data temporally can bring forth either questions or patterns depending on what the data is about. An example of this is looking at graphs that have time in them. They present inclines and declines in a pattern about the data over time.&lt;ref&gt;(Whitney)&lt;/ref&gt; Spatial data, on the other hand, looks more towards the geographical sense in the data. The information that is gathered could be about the location, the size or the shape of a particular object. A system that uses spatial data is [[GIS]] (Geographical Information System) &lt;ref&gt;(Rouse)&lt;/ref&gt;

==Philosophical==
[[Philosophy]] brings forth views around the areas of epistemology and ontology. In this view of data, there is no interpretations, opinions, importance, or relevance of the data that has been found and processed. The data is simply measured for what it is. Which brings forth to how it is viewed. The data that is viewed philosophically is also viewed in an objective way which means that the data is fixed in some way to prove a specific point. Although the data may be truthful, how the data was provided and how it is placed makes the difference. The data is also viewed in a realist view such as how things truly are. No information is changed, everything is the way that it is and is seen for that.&lt;ref&gt;(Kitchin, p.17-19)&lt;/ref&gt; This view also brings up issues around property rights.&lt;ref&gt;(Liu, p.61)&lt;/ref&gt; Who would own what and who can have the right to take things.

==References==
{{Reflist|20em}}

===Works cited===
{{refbegin}}
* {{cite book |last=Kitchin |first=Rob |year=2014 |title=The data revolution: Big data, open data, data infrastructures &amp; their consequences |place=London |publisher=Sage |chapter=Conceptualising data |pp=1–26 |chapter-url=http://www.uk.sagepub.com/upm-data/63923_Kitchin_CH1.pdf |format=pdf}}
* Metcalf, Jacob, Emily F. Keller, and danah boyd. 2016. “Perspectives on Big Data, Ethics, and Society.” Council for Big Data, Ethics, and Society.
* Rouse, Margaret. (2013). "What Is Spatial Data? - Definition from WhatIs.com." ''SearchSQLServer''. TechTarget.
* Liu, Hong. (2016). "Philosophical Reflections on Data. " Philosophical Reflections on Data. Science Direct.
* Whitney, Hunter. (2014). "It's About Time." It's About Time: Visualizing Temporal Data to Reveal Patterns and Stories | UX Magazine. UX Magazine.
{{refend}}



[[Category:Data management]]</text>
      <sha1>a2dcdx408ri5n5l3tpmy8lc2y6rotgz</sha1>
    </revision>
  </page>
  <page>
    <title>Business intelligence</title>
    <ns>0</ns>
    <id>168387</id>
    <revision>
      <id>762759162</id>
      <parentid>761909642</parentid>
      <timestamp>2017-01-30T16:00:04Z</timestamp>
      <contributor>
        <username>John of Reading</username>
        <id>11308236</id>
      </contributor>
      <minor />
      <comment>Typo fixing, replaced: critized → criticized using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="44183" xml:space="preserve">{{Use dmy dates|date=March 2012}}

{{Business administration}}

'''Business Intelligence''' ('''BI''') are the set of strategies, processes, [[application software|applications]], [[data]], products, technologies and technical architectures which are used to support the collection, analysis, presentation and dissemination of business information.&lt;ref&gt;Dedić N. &amp; Stanier C. (2016). Measuring the Success of Changes to Existing Business Intelligence Solutions to Improve Business Intelligence Reporting. Lecture Notes in Business Information Processing. Springer International Publishing. Volume 268, pp. 225-236.&lt;/ref&gt; BI technologies provide historical, current and predictive views of business operations. Common functions of business intelligence technologies are [[Business reporting|reporting]], [[online analytical processing]], [[analytics]], [[data mining]], [[process mining]], [[complex event processing]], [[business performance management]], [[benchmarking]], [[text mining]], [[Predictive Analysis|predictive analytics]] and [[Prescriptive Analytics|prescriptive analytics]] and are capable of handling large amounts of structured and sometimes unstructured data to help identify, develop and otherwise create new strategic business opportunities. The goal is to allow for the easy interpretation of these [[big data]]. Identifying new opportunities and implementing an effective strategy based on insights can provide businesses with a competitive market advantage and long-term stability.&lt;ref&gt;({{cite book |title= Business Intelligence Success Factors: Tools for Aligning Your Business in the Global Economy |last= Rud|first= Olivia |year= 2009|publisher= Wiley &amp; Sons|location= Hoboken, N.J|isbn= 978-0-470-39240-9 |page= |pages= |url= |accessdate=}})&lt;/ref&gt;

Business intelligence can be used to support a wide range of business decisions ranging from operational to strategic. Basic operating decisions include product positioning or pricing. Strategic business decisions include priorities, goals and directions at the broadest level. In all cases, BI is most effective when it combines data derived from the market in which a company operates (external data) with data from company sources internal to the business such as financial and operations data (internal data). When combined, external and internal data can provide a more complete picture which, in effect, creates an "intelligence" that cannot be derived by any singular set of data.&lt;ref&gt;{{cite book| last1= Coker| first1= Frank| title= Pulse: Understanding the Vital Signs of Your Business| publisher= Ambient Light Publishing
| publication-date= 2014| pages= 41–42| isbn= 978-0-9893086-0-1}}&lt;/ref&gt; Amongst myriad uses, business intelligence tools empower organisations to gain insight into new markets, assess demand and suitability of products and services for different market segments and gauge the impact of marketing efforts.&lt;ref name=":0"&gt;Chugh, R &amp; Grandhi, S 2013, ‘Why Business Intelligence? Significance of Business Intelligence tools and integrating BI governance with corporate governance’, International Journal of E-Entrepreneurship and Innovation, vol. 4, no.2, pp. 1-14. https://www.researchgate.net/publication/273861123_Why_Business_Intelligence_Significance_of_Business_Intelligence_Tools_and_Integrating_BI_Governance_with_Corporate_Governance&lt;/ref&gt;

==Components==
Business intelligence is made up of an increasing number of components including:
* Multidimensional aggregation and allocation
* [[Denormalization]], tagging and standardization
* Realtime reporting with analytical alert
* A method of interfacing with [[unstructured data]] sources
* Group consolidation, budgeting and [[rolling forecast]]s
* [[Statistical inference]] and probabilistic simulation
* [[Key performance indicator]]s optimization
* Version control and process management
* Open item management

==History==
The earliest known use of the term "Business Intelligence" is in Richard Millar Devens’ in the ‘Cyclopædia of Commercial and Business Anecdotes’ from 1865. Devens used the term to describe how the banker, Sir Henry Furnese, gained profit by receiving and acting upon information about his environment, prior to his competitors. “''Throughout Holland, Flanders, France, and Germany, he maintained a complete and perfect train of business intelligence. The news of the many battles fought was thus received first by him, and the [[Siege of Namur (1695)|fall of Namur]] added to his profits, owing to his early receipt of the news''.” (Devens, (1865), p.&amp;nbsp;210).  The ability to collect and react accordingly based on the information retrieved, an ability that Furnese excelled in, is today still at the very heart of BI.&lt;ref name="Miller Devens"&gt;{{cite book|last=Miller Devens|first=Richard|title=Cyclopaedia of Commercial and Business Anecdotes; Comprising Interesting Reminiscences and Facts, Remarkable Traits and Humors of Merchants, Traders, Bankers Etc. in All Ages and Countries|url=https://books.google.dk/books?id=9MspAAAAYAAJ&amp;pg=PA210&amp;dq=%22business+intelligence%22&amp;hl=en&amp;ei=a5EPTdaRIsOWnAeVyYHQDg&amp;sa=X&amp;oi=book_result&amp;ct=result&amp;redir_esc=y#v=onepage&amp;q=%22business%20intelligence%22&amp;f=false|publisher=D. Appleton and company|accessdate=15 February 2014|page=210}}&lt;/ref&gt;

In a 1958 article, [[IBM]] researcher [[Hans Peter Luhn]] used the term business intelligence. He employed the Webster's dictionary definition of intelligence: "the ability to apprehend the interrelationships of presented facts in such a way as to guide action towards a desired goal."&lt;ref&gt;
{{cite journal|url= http://www.research.ibm.com/journal/rd/024/ibmrd0204H.pdf|doi=10.1147/rd.24.0314|title= A Business Intelligence System|author=H P Luhn |authorlink= Hans Peter Luhn |year= 1958 |journal= IBM Journal|volume= 2|issue= 4|pages= 314}}
&lt;/ref&gt;

Business intelligence as it is understood today is said to have evolved from the [[decision support system]]s (DSS) that began in the 1960s and developed throughout the mid-1980s. DSS originated in the computer-aided models created to assist with [[decision making]] and planning. From DSS, [[data warehouse]]s, [[Executive Information System]]s, [[Online analytical processing|OLAP]] and business intelligence came into focus beginning in the late 80s.

In 1989, Howard Dresner (later a [[Gartner]] analyst) proposed "business intelligence" as an umbrella term to describe "concepts and methods to improve business decision making by using fact-based support systems."&lt;ref name=power&gt;{{cite web |url= http://dssresources.com/history/dsshistory.html
|title= A Brief History of Decision Support Systems, version 4.0 |accessdate=10 July 2008
|author= D. J. Power |date= 10 March 2007|publisher= DSSResources.COM }}
&lt;/ref&gt; It was not until the late 1990s that this usage was widespread.&lt;ref&gt;{{cite web |url=http://dssresources.com/history/dsshistory.html |title=A Brief History of Decision Support Systems |last=Power |first=D. J. |accessdate=1 November 2010 }}&lt;/ref&gt;

Critics see BI as evolved from mere [[business reporting]] together with the advent of increasingly powerful and easy-to-use [[data analysis]] tools. In this respect it has also been criticized as a marketing buzzword in the context of the "[[big data]]" surge.&lt;ref&gt;{{cite web|title=Decoding big data buzzwords|year=2015|quote=BI refers to the approaches, tools, mechanisms that organizations can use to keep a finger on the pulse of their businesses. Also referred by unsexy versions -- “dashboarding”, “MIS” or “reporting.”|publisher=cio.com|url=http://www.cio.com/article/2919082/big-data/what-are-they-talking-about-decoding-big-data-buzzwords.html}}&lt;/ref&gt;

==Data warehousing==
Often BI applications use data gathered from a [[data warehouse]] (DW) or from a [[data mart]], and the concepts of BI and DW sometimes combine as "'''BI/DW'''"&lt;ref&gt;
{{cite book
| last1                 = Golden
| first1                = Bernard
| title                 = Amazon Web Services For Dummies
| url                   = https://books.google.com/books?id=xSVwAAAAQBAJ
| series                = For dummies
| publisher             = John Wiley &amp; Sons
| publication-date      = 2013
| page                  = 234
| isbn                  = 9781118652268
| accessdate            = 2014-07-06
| quote                 = [...] traditional business intelligence or data warehousing tools (the terms are used so interchangeably that they're often referred to as BI/DW) are extremely expensive [...]
}}
&lt;/ref&gt;
or as "'''BIDW'''". A data warehouse contains a copy of analytical data that facilitates decision support. However, not all data warehouses serve for business intelligence, nor do all business intelligence applications require a data warehouse.

To distinguish between the concepts of business intelligence and data warehouses, [[Forrester Research]] defines business intelligence in one of two ways:

# Using a broad definition: "Business Intelligence is a set of methodologies, processes, architectures, and technologies that transform raw data into meaningful and useful information used to enable more effective strategic, tactical, and operational insights and decision-making."&lt;ref&gt;{{cite web |url=http://www.forrester.com/rb/Research/topic_overview_business_intelligence/q/id/39218/t/2 |title=Topic Overview: Business Intelligence |last=Evelson |first=Boris |date=21 November 2008}}&lt;/ref&gt; Under this definition, business intelligence also includes technologies such as data integration, data quality, data warehousing, master-data management, text- and content-analytics, and many others that the market sometimes lumps into the "[[Information Management]]" segment. Therefore, Forrester refers to ''data preparation'' and ''data usage'' as two separate but closely linked segments of the business-intelligence architectural stack.
# Forrester defines the narrower business-intelligence market as, "...referring to just the top layers of the BI architectural stack such as reporting, analytics and [[Dashboards (management information systems)|dashboards]]."&lt;ref&gt;{{cite web
|url=http://blogs.forrester.com/boris_evelson/10-04-29-want_know_what_forresters_lead_data_analysts_are_thinking_about_bi_and_data_domain
|title=Want to know what Forrester's lead data analysts are thinking about BI and the data domain? |last=Evelson |first=Boris |date=29 April 2010}}&lt;/ref&gt;

==Comparison with competitive intelligence==
Though the term business intelligence is sometimes a synonym for [[competitive intelligence]] (because they both support [[decision making]]), BI uses technologies, processes, and applications to analyze mostly internal, structured data and business processes while competitive intelligence gathers, analyzes and disseminates information with a topical focus on company competitors. If understood broadly, business intelligence can include the subset of competitive intelligence.&lt;ref&gt;{{cite web |url=http://blogs.forrester.com/james_kobielus/10-04-30-what%E2%80%99s_not_bi_oh_don%E2%80%99t_get_me_startedoops_too_latehere_goes |title=What’s Not BI? Oh, Don’t Get Me Started....Oops Too Late...Here Goes.... |last=Kobielus |first=James |date=30 April 2010 |quote=“Business” intelligence is a non-domain-specific catchall for all the types of analytic data that can be delivered to users in reports, dashboards, and the like. When you specify the subject domain for this intelligence, then you can refer to “competitive intelligence,” “market intelligence,” “social intelligence,” “financial intelligence,” “HR intelligence,” “supply chain intelligence,” and the like.}}&lt;/ref&gt;

==Comparison with business analytics==
Business intelligence and [[business analytics]] are sometimes used interchangeably, but there are alternate definitions.&lt;ref&gt;{{cite web|url=http://timoelliott.com/blog/2011/03/business-analytics-vs-business-intelligence.html |title=Business Analytics vs Business Intelligence? |publisher=timoelliott.com |date=2011-03-09 |accessdate=2014-06-15}}&lt;/ref&gt;  One definition contrasts the two, stating that the term business intelligence refers to collecting business data to find information primarily through asking questions, reporting, and online analytical processes. Business analytics, on the other hand, uses statistical and quantitative tools for explanatory and [[predictive modelling]].&lt;ref&gt;{{cite web|url=http://www.businessanalytics.com/difference-between-business-analytics-and-business-intelligence/ |title=Difference between Business Analytics and Business Intelligence |publisher=businessanalytics.com |date=2013-03-15 |accessdate=2014-06-15}}&lt;/ref&gt;

In an alternate definition, [[Thomas H. Davenport|Thomas Davenport]], professor of information technology and management at [[Babson College]] argues that business intelligence should be divided into [[Information retrieval|querying]], [[Business reporting|reporting]], [[Online analytical processing]] (OLAP), an "alerts" tool, and business analytics. In this definition, business analytics is the subset of BI focusing on statistics, prediction, and optimization, rather than the reporting functionality.&lt;ref&gt;{{Cite interview |url=http://www.informationweek.com/news/software/bi/222200096 |title=Analytics at Work: Q&amp;A with Tom Davenport |last=Henschen |first=Doug |date=4 January 2010}}&lt;/ref&gt;

==Applications in an enterprise==
Business intelligence can be applied to the following business purposes, in order to drive business value.{{Citation needed|date=October 2010}}
# [[Measurement]]&amp;nbsp;– program that creates a hierarchy of [[performance metrics]] (see also [[Metrics Reference Model]]) and [[benchmarking]] that informs business leaders about progress towards business goals ([[business process management]]).
# [[Analytics]]&amp;nbsp;– program that builds quantitative processes for a business to arrive at optimal decisions and to perform business knowledge discovery. Frequently involves: [[data mining]], [[process mining]], [[statistical analysis]], [[predictive analytics]], [[predictive modeling]], [[business process modeling]], [[data lineage]], [[complex event processing]] and [[Prescriptive Analytics|prescriptive analytics]].
# [[Business reporting|Reporting]]/[[enterprise reporting]]&amp;nbsp;– program that builds infrastructure for strategic reporting to serve the strategic management of a business, not operational reporting. Frequently involves [[data visualization]], [[executive information system]] and [[OLAP]].
# [[Collaboration]]/[[collaboration platform]]&amp;nbsp;– program that gets different areas (both inside and outside the business) to work together through [[data sharing]] and [[electronic data interchange]].
# [[Knowledge management]]&amp;nbsp;– program to make the company data-driven through strategies and practices to identify, create, represent, distribute, and enable adoption of insights and experiences that are true business knowledge. Knowledge management leads to [[learning management]] and [[regulatory compliance]].

In addition to the above, business intelligence can provide a pro-active approach, such as alert functionality that immediately notifies the end-user if certain conditions are met. For example, if some business metric exceeds a pre-defined threshold, the metric will be highlighted in standard reports, and the business analyst may be alerted via e-mail or another monitoring service. This end-to-end process requires data governance, which should be handled by the expert.{{Citation needed|date=January 2012}}

==Prioritization of projects==
It can be difficult to provide a positive business case for business intelligence initiatives, and often the projects must be prioritized through strategic initiatives. BI projects can attain higher prioritization within the organization if managers consider the following:
* As described by Kimball&lt;ref&gt;Kimball et al., 2008: 29&lt;/ref&gt; the BI manager must determine the tangible benefits such as eliminated cost of producing legacy reports.
* Data access for the entire organization must be enforced.&lt;ref&gt;{{cite web|url= http://content.dell.com/us/en/enterprise/d/large-business/ready-business-intelligence.aspx|title= Are You Ready for the New Business Intelligence?|publisher=Dell.com | accessdate=19 June 2012}}&lt;/ref&gt; In this way even a small benefit, such as a few minutes saved, makes a difference when multiplied by the number of employees in the entire organization.
* As described by Ross, Weil &amp; Roberson for Enterprise Architecture,&lt;ref&gt;[[Jeanne W. Ross]], [[Peter Weill]], [[David C. Robertson]] (2006) ''Enterprise Architecture As Strategy'', p. 117 ISBN 1-59139-839-8.&lt;/ref&gt; managers should also consider letting the BI project be driven by other business initiatives with excellent business cases. To support this approach, the organization must have enterprise architects who can identify suitable business projects.
* Using a structured and quantitative methodology to create defensible prioritization in line with the actual needs of the organization, such as a weighted decision matrix.&lt;ref&gt;{{cite web|last=Krapohl|first=Donald|title=A Structured Methodology for Group Decision Making|url=http://www.augmentedintel.com/wordpress/index.php/a-structured-methodology-for-group-decision-making/|publisher=AugmentedIntel|accessdate=22 April 2013}}&lt;/ref&gt;

==Success factors of implementation==
According to Kimball et al., there are three critical areas that organizations should assess before getting ready to do a BI project:&lt;ref&gt;Kimball et al. 2008: p. 298&lt;/ref&gt;

# The level of commitment and sponsorship of the project from senior management.
# The level of business need for creating a BI implementation.
# The amount and quality of business data available.

===Business sponsorship===

The commitment and [[:wikt:sponsor|sponsor]]ship of senior management is according to Kimball ''et al.'', the most important criteria for assessment.&lt;ref&gt;Kimball et al., 2008: 16&lt;/ref&gt; This is because having strong management backing helps overcome shortcomings elsewhere in the project. However, as Kimball ''et al.'' state: “even the most elegantly designed DW/BI system cannot overcome a lack of business [management] sponsorship”.&lt;ref&gt;Kimball et al., 2008: 18&lt;/ref&gt;

It is important that personnel who participate in the project have a vision and an idea of the benefits and drawbacks of implementing a BI system. The best business sponsor should have organizational clout and should be well connected within the organization. It is ideal that the business sponsor is demanding but also able to be realistic and supportive if the implementation runs into delays or drawbacks. The management sponsor also needs to be able to assume accountability and to take responsibility for failures and setbacks on the project. Support from multiple members of the management ensures the project does not fail if one person leaves the steering group. However, having many managers work together on the project can also mean that there are several different interests that attempt to pull the project in different directions, such as if different departments want to put more emphasis on their usage. This issue can be countered by an early and specific analysis of the business areas that benefit the most from the implementation. All stakeholders in the project should participate in this analysis in order for them to feel invested in the project and to find common ground.

Another management problem that may be encountered before the start of an implementation is an overly aggressive business sponsor. Problems of [[scope creep]] occur when the sponsor requests data sets that were not specified in the original planning phase.

===Business needs===

Because of the close relationship with senior management, another critical thing that must be assessed before the project begins is whether or not there is a business need and whether there is a clear business benefit by doing the implementation.&lt;ref name="Kimball et al., 2008: 17"&gt;Kimball et al., 2008: 17&lt;/ref&gt;
The needs and benefits of the implementation are sometimes driven by competition and the need to gain an advantage in the market. Another reason for a business-driven approach to implementation of BI is the acquisition of other organizations that enlarge the original organization it can sometimes be beneficial to implement DW or BI in order to create more oversight.

Companies that implement BI are often large, multinational organizations with diverse subsidiaries.&lt;ref&gt;{{cite web|title=How Companies Are Implementing Business Intelligence Competency Centers |url=http://www.computerworld.com/pdfs/SAS_Intel_BICC.pdf |publisher=Computer World |deadurl=yes |accessdate=1 April 2014 |archiveurl=https://web.archive.org/web/20130528054421/http://www.computerworld.com/pdfs/SAS_Intel_BICC.pdf |archivedate=28 May 2013 }}&lt;/ref&gt; A well-designed BI solution provides a consolidated view of key business data not available anywhere else in the organization, giving management visibility and control over measures that otherwise would not exist.

===Amount and quality of available data===

Without proper data, or with too little quality data, any BI implementation fails; it does not matter how good the management sponsorship or business-driven motivation is. Before implementation it is a good idea to do [[data profiling]]. This analysis identifies the “content, consistency and structure [..]”&lt;ref name="Kimball et al., 2008: 17"/&gt; of the data. This should be done as early as possible in the process and if the analysis shows that data is lacking, put the project on hold temporarily while the IT department figures out how to properly collect data.

When planning for business data and business intelligence requirements, it is always advisable to consider specific scenarios that apply to a particular organization, and then select the business intelligence features best suited for the scenario.

Often, scenarios revolve around distinct business processes, each built on one or more data sources. These sources are used by features that present that data as information to knowledge workers, who subsequently act on that information. The business needs of the organization for each business process adopted correspond to the essential steps of business intelligence. These essential steps of business intelligence include but are not limited to:
#Go through business data sources in order to collect needed data
#Convert business data to information and present appropriately
#Query and analyze data
#Act on the collected data
The '''quality aspect''' in business intelligence should cover all the process from the source data to the final reporting. At each step, the '''quality gates''' are different:
# Source Data:
#* Data Standardization: make data comparable (same unit, same pattern...)
#* [[Master data management|Master Data Management:]] unique referential
# [[Operational data store|Operational Data Store (ODS)]]:
#* [[Data cleansing|Data Cleansing:]] detect &amp; correct inaccurate data
#* Data Profiling: check inappropriate value, null/empty
# [[Data warehouse]]:
#* Completeness: check that all expected data are loaded
#* [[Referential integrity]]: unique and existing referential over all sources
#* Consistency between sources: check consolidated data vs sources
# Reporting:
#* Uniqueness of indicators: only one share dictionary of indicators
#* Formula accuracy: local reporting formula should be avoided or checked

==User aspect==

Some considerations must be made in order to successfully integrate the usage of business intelligence systems in a company. Ultimately the BI system must be accepted and utilized by the users in order for it to add value to the organization.&lt;ref name = kimball&gt;Kimball&lt;/ref&gt;&lt;ref name = swain&gt;Swain Scheps ''Business Intelligence for Dummies'', 2008, ISBN 978-0-470-12723-0&lt;/ref&gt; If the [[usability]] of the system is poor, the users may become frustrated and spend a considerable amount of time figuring out how to use the system or may not be able to really use the system. If the system does not add value to the users´ mission, they simply don't use it.&lt;ref name = swain /&gt;

To increase user acceptance of a BI system, it can be advisable to consult business users at an early stage of the DW/BI lifecycle, for example at the requirements gathering phase.&lt;ref name = kimball /&gt; This can provide an insight into the [[business process]] and what the users need from the BI system. There are several methods for gathering this information, such as questionnaires and interview sessions.

When gathering the requirements from the business users, the local IT department should also be consulted in order to determine to which degree it is possible to fulfill the business's needs based on the available data.&lt;ref name = kimball /&gt;

Taking a user-centered approach throughout the design and development stage may further increase the chance of rapid user adoption of the BI system.&lt;ref name = swain /&gt;

Besides focusing on the user experience offered by the BI applications, it may also possibly motivate the users to utilize the system by adding an element of competition. Kimball&lt;ref name = kimball /&gt; suggests implementing a function on the Business Intelligence portal website where reports on system usage can be found. By doing so, managers can see how well their departments are doing and compare themselves to others and this may spur them to encourage their staff to utilize the BI system even more.

In a 2007 article, H. J. Watson gives an example of how the competitive element can act as an incentive.&lt;ref name = watson&gt;{{cite journal|title=The Current State of Business Intelligence|year=2007|doi=10.1109/MC.2007.331|last1=Watson|first1=Hugh J.|last2=Wixom|first2=Barbara H.|journal=Computer|volume=40|issue=9|pages=96}}&lt;/ref&gt; Watson describes how a large call centre implemented performance dashboards for all call agents, with monthly incentive bonuses tied to performance metrics. Also, agents could compare their performance to other team members. The implementation of this type of performance measurement and competition significantly improved agent performance.

BI chances of success can be improved by involving senior management to help make BI a part of the [[organizational culture]], and by providing the users with necessary tools, training, and support.&lt;ref name = watson /&gt; Training encourages more people to use the BI application.&lt;ref name = kimball /&gt;

Providing user support is necessary to maintain the BI system and resolve user problems.&lt;ref name = swain /&gt; User support can be incorporated in many ways, for example by creating a website. The website should contain great content and tools for finding the necessary information. Furthermore, helpdesk support can be used. The help desk can be manned by power users or the DW/BI project team.&lt;ref name = kimball /&gt;

==BI Portals==
A '''Business Intelligence portal''' (BI portal) is the primary access interface for [[Data warehouse|Data Warehouse]] (DW) and Business Intelligence (BI) applications. The BI portal is the user's first impression of the DW/BI system. It is typically a browser application, from which the user has access to all the individual services of the DW/BI system, reports and other analytical functionality.
The BI portal must be implemented in such a way that it is easy for the users of the DW/BI application to call on the functionality of the application.&lt;ref name="Ralph"&gt;''The Data Warehouse Lifecycle Toolkit (2nd ed.). Ralph Kimball (2008).''&lt;/ref&gt;

The BI portal's main functionality is to provide a navigation system of the DW/BI application. This means that the portal has to be implemented in a way that the user has access to all the functions of the DW/BI application.

The most common way to design the portal is to custom fit it to the business processes of the organization for which the DW/BI application is designed, in that way the portal can best fit the needs and requirements of its users.&lt;ref name="Wiley"&gt;''Microsoft Data Warehouse Toolkit. Wiley Publishing. (2006)''&lt;/ref&gt;

The BI portal needs to be easy to use and understand, and if possible have a look and feel similar to other applications or web content of the organization the DW/BI application is designed for ([[consistency]]).

The following is a list of desirable features for [[web portal]]s in general and BI portals in particular:

;Usable: User should easily find what they need in the BI tool.
;Content Rich: The portal is not just a report printing tool, it should contain more functionality such as advice, help, support information and documentation.
;Clean: The portal should be designed so it is easily understandable and not over-complex as to confuse the users
;Current: The portal should be updated regularly.
;Interactive: The portal should be implemented in a way that makes it easy for the user to use its functionality and encourage them to use the portal. Scalability and customization give the user the means to fit the portal to each user.
;Value Oriented: It is important that the user has the feeling that the DW/BI application is a valuable resource that is worth working on.

==Marketplace==
There are a number of business intelligence vendors, often categorized into the remaining independent "pure-play" vendors and consolidated "megavendors" that have entered the market through a recent trend&lt;ref&gt;{{cite news|url=http://www.zdnet.com/gartner-releases-2013-bi-magic-quadrant-7000011264/ |title=Gartner releases 2013 BI Magic Quadrant |publisher=ZDNet |author=Andrew Brust| date= 2013-02-14|accessdate=21 August 2013}}&lt;/ref&gt; of acquisitions in the BI industry.&lt;ref&gt;{{cite web |url=http://www.bi-verdict.com/fileadmin/FreeAnalyses/consolidations.htm |title=Consolidations in the BI industry |date=7 March 2008 |last=Pendse |first=Nigel |work=The OLAP Report}}&lt;/ref&gt; The business intelligence market is gradually growing. In 2012 business intelligence services brought in $13.1 billion in revenue.&lt;ref&gt;{{cite web|title=Why Business Intelligence Is Key For Competitive Advantage|url=https://cisonline.bu.edu/news-resources/why-business-intelligence-is-key-for-competitive-advantage/|website=Boston University|accessdate=23 October 2014}}&lt;/ref&gt;

Some companies adopting BI software decide to pick and choose from different product offerings (best-of-breed) rather than purchase one comprehensive integrated solution (full-service).&lt;ref&gt;{{cite web |url=http://www.b-eye-network.com/view/2608 |title=Three Trends in Business Intelligence Technology |last=Imhoff |first=Claudia |date=4 April 2006}}&lt;/ref&gt;

===Industry-specific===
Specific considerations for business intelligence systems have to be taken in some sectors such as [[Bank regulation|governmental banking regulations]] or healthcare.&lt;ref&gt;{{cite journal |vauthors=Mettler T, Vimarlund V |title=Understanding business intelligence in the context of healthcare |journal=Health Informatics Journal |volume=15 |issue=3 |pages=254–264 |year=2009 |doi=10.1177/1460458209337446 }}&lt;/ref&gt; The information collected by banking institutions and analyzed with BI software must be protected from some groups or individuals, while being fully available to other groups or individuals. Therefore, BI solutions must be sensitive to those needs and be flexible enough to adapt to new regulations and changes to existing law.{{citation needed|date=May 2016}}

==Semi-structured or unstructured data==
Businesses create a huge amount of valuable information in the form of e-mails, memos, notes from call-centers, news, user groups, chats, reports, web-pages, presentations, image-files, video-files, and marketing material and news. According to Merrill Lynch, more than 85% of all business information exists in these forms. These information types are called either ''[[Semi-structured data|semi-structured]]'' or ''[[Unstructured data|unstructured]]'' data. However, organizations often only use these documents once.&lt;ref name = rao&gt;{{cite journal|doi=10.1109/MITP.2003.1254966 |url=http://www.ramanarao.com/papers/rao-itpro-2003-11.pdf|title=From unstructured data to actionable intelligence|year=2003|last1=Rao|first1=R.|journal=IT Professional|volume=5|issue=6|pages=29}}&lt;/ref&gt;

The managements of semi-structured data is recognized as a major unsolved problem in the information technology industry.&lt;ref name = blumberg&gt;{{cite journal|url=http://soquelgroup.com/Articles/dmreview_0203_problem.pdf|author1=Blumberg, R.  |author2=S. Atre  |lastauthoramp=yes |title=The Problem with Unstructured Data|journal=DM Review |year=2003|pages=42–46}}&lt;/ref&gt; According to projections from Gartner (2003), white collar workers spend anywhere from 30 to 40 percent of their time searching, finding and assessing unstructured data. BI uses both structured and unstructured data, but the former is easy to search, and the latter contains a large quantity of the information needed for analysis and decision making.&lt;ref name = blumberg /&gt;&lt;ref name = negash&gt;{{cite journal|url=http://site.xavier.edu/sena/info600/businessintelligence.pdf|author=Negash, S |title=Business Intelligence|journal= Communications of the Association of Information Systems|volume=13|year= 2004|pages=177–195}}&lt;/ref&gt; Because of the difficulty of properly searching, finding and assessing unstructured or semi-structured data, organizations may not draw upon these vast reservoirs of information, which could influence a particular decision, task or project. This can ultimately lead to poorly informed decision making.&lt;ref name = rao /&gt;

Therefore, when designing a business intelligence/DW-solution, the specific problems associated with semi-structured and unstructured data must be accommodated for as well as those for the structured data.&lt;ref name = negash /&gt;

===Unstructured data vs. semi-structured data===
Unstructured and semi-structured data have different meanings depending on their context. In the context of relational database systems, unstructured data cannot be stored in predictably ordered [[columns]] and [[Row (database)|rows]]. One type of unstructured data is typically stored in a [[BLOB]] (binary large object), a catch-all data type available in most [[relational database]] management systems. Unstructured data may also refer to irregularly or randomly repeated column patterns that vary from row to row within each file or document.{{citation needed|date=May 2016}}

Many of these data types, however, like e-mails, word processing text files, PPTs, image-files, and video-files conform to a standard that offers the possibility of metadata. Metadata can include information such as author and time of creation, and this can be stored in a relational database.
Therefore, it may be more accurate to talk about this as semi-structured documents or data,&lt;ref name = blumberg /&gt; but no specific consensus seems to have been reached.

Unstructured data can also simply be the knowledge that business users have about future business trends. Business forecasting naturally aligns with the BI system because business users think of their business in aggregate terms. Capturing the business knowledge that may only exist in the minds of business users provides some of the most important data points for a complete BI solution.

===Problems with semi-structured or unstructured data===
There are several challenges to developing BI with semi-structured data. According to Inmon &amp; Nesavich,&lt;ref name = inmon&gt;Inmon, B. &amp; A. Nesavich, "Unstructured Textual Data in the Organization" from "Managing Unstructured data in the organization", Prentice Hall 2008, pp. 1–13&lt;/ref&gt; some of those are:

# Physically accessing unstructured textual data&amp;nbsp;– unstructured data is stored in a huge variety of formats.
# [[Terminology]]&amp;nbsp;– Among researchers and analysts, there is a need to develop a standardized terminology.
# Volume of data&amp;nbsp;– As stated earlier, up to 85% of all data exists as semi-structured data. Couple that with the need for word-to-word and semantic analysis.
# Searchability of unstructured textual data&amp;nbsp;– A simple search on some data, e.g. apple, results in links where there is a reference to that precise search term. (Inmon &amp; Nesavich, 2008)&lt;ref name = inmon /&gt; gives an example: “a search is made on the term felony. In a simple search, the term felony is used, and everywhere there is a reference to felony, a hit to an unstructured document is made. But a simple search is crude. It does not find references to crime, arson, murder, embezzlement, vehicular homicide, and such, even though these crimes are types of felonies.”

===The use of metadata===
To solve problems with searchability and assessment of data, it is necessary to know something about the content. This can be done by adding context through the use of [[metadata]].&lt;ref name = rao /&gt; Many systems already capture some metadata (e.g. filename, author, size, etc.), but more useful would be metadata about the actual content&amp;nbsp;– e.g. summaries, topics, people or companies mentioned. Two technologies designed for generating metadata about content are automatic categorization and [[information extraction]].

==2009 predictions==
A 2009 paper predicted&lt;ref&gt;[http://www.gartner.com/it/page.jsp?id=856714 Gartner Reveals Five Business Intelligence Predictions for 2009 and Beyond]. gartner.com. 15 January 2009&lt;/ref&gt; these developments in the business intelligence market:
* Because of lack of information, processes, and tools, through 2012, more than 35 percent of the top 5,000 global companies regularly fail to make insightful decisions about significant changes in their business and markets.
* By 2012, business units will control at least 40 percent of the total budget for business intelligence.
* By 2012, one-third of analytic applications applied to business processes will be delivered through [[Granularity|coarse-grained]] application [[mashup (web application hybrid)|mashups]].
* BI has a huge scope in Entrepreneurship however majority of new entrepreneurs ignore its potential.&lt;ref&gt;[http://brighterkashmir.com/role-of-business-intelligence-in-entrepreneurship/ huge scope in Entrepreneurship]&lt;/ref&gt;

A 2009 ''Information Management'' special report predicted the top BI trends: "[[green computing]], [[social networking service]]s, [[data visualization]], [[Mobile business intelligence|mobile BI]], [[predictive analytics]], [[composite application]]s, [[cloud computing]] and [[Multi-touch|multitouch]]".&lt;ref&gt;{{cite web |url=http://www.information-management.com/specialreports/2009_148/business_intelligence_data_vizualization_social_networking_analytics-10015628-1.html |title=10 Red Hot BI Trends |last=Campbell |first=Don |date=23 June 2009 |work=Information Management}}&lt;/ref&gt; Research undertaken in 2014 indicated that employees are more likely to have access to, and more likely to engage with, cloud-based BI tools than traditional tools.&lt;ref&gt;{{cite web |url=http://www.aberdeen.com/Aberdeen-Library/8906/RR-analytics-cloud-saas-bi.aspx |title=Cloud Analytics in 2014: Infusing the Workforce with Insight |last=Lock|first=Michael|date=27 March 2014 }}&lt;/ref&gt;

Other business intelligence trends include the following:

* Third party SOA-BI products increasingly address [[Extract, transform, load|ETL]] issues of volume and throughput.
* Companies embrace in-memory processing, 64-bit processing, and pre-packaged analytic BI applications.
* Operational applications have callable BI components, with improvements in response time, scaling, and concurrency.
* Near or real time BI analytics is a baseline expectation.
* Open source BI software replaces vendor offerings.

Other lines of research include the combined study of business intelligence and uncertain data.&lt;ref&gt;{{Cite journal
 | last=Rodriguez | first=Carlos
 | last2=Daniel | first2=Florian
 | last3=Casati | first3=Fabio
 | last4=Cappiello | first4=Cinzia
 | year=2010
 | title=Toward Uncertain Business Intelligence: The Case of Key Indicators |doi=10.1109/MIC.2010.59
 | journal=IEEE Internet Computing
 | volume=14
 | issue=4
 | pages=32 }}&lt;/ref&gt;&lt;ref&gt;{{citation |author=Rodriguez, C. |author2=Daniel, F. |author3=Casati, F. |author4=Cappiello, C. |last-author-amp=yes |url=http://mitiq.mit.edu/ICIQ/Documents/IQ%20Conference%202009/Papers/3-C.pdf |title=Computing Uncertain Key Indicators from Uncertain Data  |pages=106–120 | conference = ICIQ'09 | year = 2009}}&lt;/ref&gt; In this context, the data used is not assumed to be precise, accurate and complete. Instead, data is considered uncertain and therefore this uncertainty is propagated to the results produced by BI.

According to a study by the Aberdeen Group, there has been increasing interest in [[Software-as-a-Service]] (SaaS) business intelligence over the past years, with twice as many organizations using this deployment approach as one year ago&amp;nbsp;– 15% in 2009 compared to 7% in 2008.&lt;ref&gt;{{cite web|last1=Julian|first1=Taylor|title=Business intelligence implementation according to customer's needs|url=http://apro-software.com/services/software-development/business-intelligence|publisher=APRO Software|accessdate=16 May 2016|date=10 January 2010}}&lt;/ref&gt;

An article by InfoWorld’s Chris Kanaracus points out similar growth data from research firm IDC, which predicts the SaaS BI market will grow 22 percent each year through 2013 thanks to increased product sophistication, strained IT budgets, and other factors.&lt;ref&gt;[http://infoworld.com/d/cloud-computing/saas-bi-growth-will-soar-in-2010-511 SaaS BI growth will soar in 2010 | Cloud Computing]. InfoWorld (2010-02-01). Retrieved 17 January 2012.&lt;/ref&gt;

An analysis of top 100 Business Intelligence and Analytics scores and ranks the firms based on several open variables&lt;ref&gt;{{cite web|url=http://www.appsbi.com/top-100-analytics-companies-ranked-and-scored-by-mattermark|title=Top 100 analytics companies ranked and scored by Mattermark -  Business Intelligence - Dashboards - Big Data|publisher=}}&lt;/ref&gt;

==See also==
{{colbegin|3}}
* [[Accounting intelligence]]
* [[Analytic applications]]
* [[Artificial intelligence marketing]]
* [[Business Intelligence 2.0]]
* [[Business process discovery]]
* [[Business process management]]
* [[Business activity monitoring]]
* [[Business service management]]
* [[Comparison of OLAP Servers]]
* [[Customer dynamics]]
* [[Data Presentation Architecture]]
* [[Data visualization]]
* [[Decision engineering]]
* [[Enterprise planning systems]]
* [[Infonomics]]
* [[Intelligent document|Document intelligence]]
* [[Integrated business planning]]
* [[Location intelligence]]
* [[Media intelligence]]
* [[Meteorological intelligence]]
* [[Mobile business intelligence]]
* [[Multiway Data Analysis]]
* [[Operational intelligence]]
* [[Business Information Systems]]
* [[Business intelligence tools]]
* [[Process mining]]
* [[Real-time business intelligence]]
* [[Runtime intelligence]]
* [[Sales intelligence]]
* [[Test and learn]]

{{colend}}

==References==
{{Reflist|30em}}

==Bibliography==
*Ralph Kimball ''et al.'' "The Data warehouse Lifecycle Toolkit" (2nd ed.) Wiley ISBN 0-470-47957-4
*Peter Rausch, Alaa Sheta, Aladdin Ayesh : ''Business Intelligence and Performance Management: Theory, Systems, and Industrial Applications'', Springer Verlag U.K., 2013, ISBN 978-1-4471-4865-4.

==External links==
* [http://online.sju.edu/resource/engineering-technology/key-role-hadoop-plays-in-business-intelligence "The Key Role Hadoop Plays in Business Intelligence and Data Warehousing" - St. Joseph's University]
* {{cite journal
|url=http://cacm.acm.org/magazines/2011/8/114953-an-overview-of-business-intelligence-technology/fulltext
|title=An Overview Of Business Intelligence Technology
|date=August 2011 | accessdate=26 October 2011
|first1=Surajit |last1=Chaudhuri |first2=Umeshwar |last2=Dayal |first3=Vivek |last3=Narasayya
|journal=Communications of the ACM
|volume =54 |issue= 8 |pages=88–98
|doi=10.1145/1978542.1978562 }}

{{Data warehouse}}

{{DEFAULTSORT:Business Intelligence}}
[[Category:Business intelligence| ]]
[[Category:Financial data analysis]]
[[Category:Data management]]
[[Category:Financial technology]]
[[Category:Information management]]</text>
      <sha1>netsuhhdi6gjifd9gofxt3sr8csfy6s</sha1>
    </revision>
  </page>
  <page>
    <title>Data recovery</title>
    <ns>0</ns>
    <id>2160183</id>
    <revision>
      <id>762889004</id>
      <parentid>762887218</parentid>
      <timestamp>2017-01-31T07:17:15Z</timestamp>
      <contributor>
        <username>Harshitgpt89</username>
        <id>29513190</id>
      </contributor>
      <comment>/* Logical damage */  added other forms of logical damages</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="17631" xml:space="preserve">{{Use dmy dates|date=June 2016}}
{{Multiple issues|
{{Refimprove|date=February 2012}}
{{Manual|date=April 2016}}
}}

In [[computing]], '''data recovery''' is a process of salvaging (retrieving) inaccessible, lost, corrupted, damaged or formatted data from [[secondary storage]], [[removable media]] or [[Computer file|files]], when the data stored in them cannot be accessed in a normal way. The data is most often salvaged from storage media such as internal or external [[hard disk drive]]s (HDDs), [[solid-state drive]]s (SSDs), [[USB flash drive]]s, [[Magnetic tape data storage|magnetic tapes]], [[CD]]s, [[DVD]]s, [[RAID]] subsystems, and other [[electronic devices]]. Recovery may be required due to physical damage to the storage devices or logical damage to the [[file system]] that prevents it from being [[Mount (computing)|mounted]] by the host [[operating system]] (OS).

The most common data recovery scenario involves an operating system failure, malfunction of a storage device, logical failure of storage devices, accidental damage or deletion, etc. (typically, on a single-drive, single-[[disk partition|partition]], single-OS system), in which case the ultimate goal is simply to copy all important files from the damaged media to another new drive. This can be easily accomplished using a [[Live CD]], many of which provide a means to [[Mount (computing)|mount]] the system drive and backup drives or removable media, and to move the files from the system drive to the backup media with a [[file manager]] or [[optical disc authoring software]]. Such cases can often be mitigated by [[disk partition]]ing and consistently storing valuable data files (or copies of them) on a different partition from the replaceable OS system files.

Another scenario involves a drive-level failure, such as a compromised [[file system]] or drive partition, or a [[hard disk drive failure]]. In any of these cases, the data is not easily read from the media devices. Depending on the situation, solutions involve repairing the logical file system, partition table or [[master boot record]],or updating the firmware or drive recovery techniques ranging from software-based recovery of corrupted data, hardware- and software-based recovery of damaged service areas (also known as the hard disk drive's "firmware"), to hardware replacement on a physically damaged drive which involves changes the parts of the damaged drive to make the data in a readable form and can be copied to a new drive. If a drive recovery is necessary, the drive itself has typically failed permanently, and the focus is rather on a one-time recovery, salvaging whatever data can be read.

In a third scenario, files have been accidentally "[[file deletion|deleted]]" from a storage medium by the users. Typically, the contents of deleted files are not removed immediately from the physical drive; instead, references to them in the directory structure are removed, and thereafter space they deleted data occupy is made available for later data overwriting. In the mind of [[end user]]s, deleted files cannot be discoverable through a standard file manager, but the deleted data still technically exists on the physical drive. In the meantime, the original file contents remain, often in a number of disconnected [[File system fragmentation|fragments]], and may be recoverable if not overwritten by other data files.

The term "data recovery" is also used in the context of [[Computer forensics|forensic]] applications or [[espionage]], where data which have been [[Encryption|encrypted]] or hidden, rather than damaged, are recovered. Sometimes data present in the computer gets encrypted or hidden due to reasons like virus attack which can only be recovered by some computer forensic experts. 

==Physical damage==
{{See also|Data recovery hardware}}

A wide variety of failures can cause physical damage to storage media, which may result from human errors and natural disasters. [[CD-ROM]]s can have their metallic substrate or dye layer scratched off; hard disks can suffer any of several mechanical failures, such as [[head crash]]es and failed motors; [[tape drive|tapes]] can simply break. Physical damage always causes at least some data loss, and in many cases the logical structures of the file system are damaged as well. Any logical damage must be dealt with before files can be salvaged from the failed media.

Most physical damage cannot be repaired by end users. For example, opening a hard disk drive in a normal environment can allow airborne dust to settle on the platter and become caught between the platter and the [[read/write head]], causing new head crashes that further damage the platter and thus compromise the recovery process. Furthermore, end users generally do not have the hardware or technical expertise required to make these repairs. Consequently, data recovery companies are often employed to salvage important data with the more reputable ones using [[Cleanroom#Cleanroom classifications|class 100]] dust- and static-free [[cleanroom]]s.&lt;ref&gt;{{cite web|last=Vasconcelos|first=Pedro|title=DIY data recovery could mean "bye-bye"|url=http://blog.ontrackdatarecovery.co.uk/data-recovery-realities/diy-data-recovery-could-mean-bye-bye/|work=The Ontrack Data Recovery Blog|publisher=Kroll Ontrack UK|accessdate=23 May 2013}}&lt;/ref&gt;

===Recovery techniques===
Recovering data from physically damaged hardware can involve multiple techniques. Some damage can be repaired by replacing parts in the hard disk. This alone may make the disk usable, but there may still be logical damage. A specialized disk-imaging procedure is used to recover every readable bit from the surface. Once this image is acquired and saved on a reliable medium, the image can be safely analyzed for logical damage and will possibly allow much of the original file system to be reconstructed.

==== {{Anchor|SERVICE-AREA}}Hardware repair ====
[[File:HD with toasty PCB.jpg|thumb|right|250px|Media that has suffered a catastrophic electronic failure requires data recovery in order to salvage its contents.]]

A common misconception is that a damaged [[printed circuit board]] (PCB) may be simply replaced during recovery procedures by an identical PCB from a healthy drive. While this may work in rare circumstances on hard disk drives manufactured before 2003, it will not work on newer drives.  Electronics boards of modern drives usually contain drive-specific [[adaptation data]] required for accessing their system areas, so the related componentry needs to be either reprogrammed (if possible) or unsoldered and transferred between two electronics boards.&lt;ref&gt;{{cite web
 | url = http://www.donordrives.com/pcb-replacement-guide
 | title = Hard Drive Circuit Board Replacement Guide or How To Swap HDD PCB
 | accessdate = 27 May 2015
 | website = donordrives.com
}}&lt;/ref&gt;&lt;ref&gt;{{cite web
 | url = http://www.pcb4you.com/pages/firmware-adaptation-service-rom-swap
 | archiveurl = https://web.archive.org/web/20130329021847/http://www.pcb4you.com/pages/firmware-adaptation-service-rom-swap
 | title = Firmware Adaptation Service - ROM Swap
 | accessdate = 27 May 2015 | archivedate = 29 March 2013
 | website = pcb4you.com
}}&lt;/ref&gt;

Each hard disk drive has what is called a ''system area'' or ''service area''; this portion of the drive, which is not directly accessible to the [[end user]], usually contains drive's [[firmware]] and adaptive data that helps the drive operate within normal parameters.&lt;ref&gt;{{cite web
 | url = http://www.recover.co.il/SA-cover/SA-cover.pdf
 | title = Hiding Data in Hard Drive's Service Areas
 | date = 14 February 2013 | accessdate = 23 January 2015
 | author = Ariel Berkman | website = recover.co.il
 | format = PDF
}}&lt;/ref&gt;  One function of the system area is to log defective sectors within the drive; essentially telling the drive where it can and cannot write data.

The sector lists are also stored on various chips attached to the PCB, and they are unique to each hard disk drive. If the data on the PCB do not match what is stored on the platter, then the drive will not calibrate properly.&lt;ref&gt;[https://web.archive.org/web/20130416232748/http://datarecoveryreport.com/#Swapping_PCB_Logic_Board#Swapping_PCB_Logic_Board Swapping Data Recovery Report]&lt;/ref&gt; In most cases the drive heads will click because they are unable to find the data matching what is stored on the PCB.

==Logical damage==
{{See also|List of data recovery software}}
[[Image:Data loss of image file.JPG|thumb|Result of a failed data recovery from a hard disk drive.]]

The term "logical damage" refers to situations in which the error is not a problem in the hardware and requires software-level solutions.

===Corrupt partitions and file systems, media errors===
In some cases, data on a hard disk drive can be unreadable due to damage to the [[partition table]] or [[file system]], or to (intermittent) media errors. In the majority of these cases, at least a portion of the original data can be recovered by repairing the damaged partition table or file system using specialized data recovery software such as [[Testdisk]]; software like [[dd rescue]] can image media despite intermittent errors, and image raw data when there is partition table or file system damage. This type of data recovery can be performed by people without expertise in drive hardware, as it requires no special physical equipment or access to platters.

Sometimes data can be recovered using relatively simple methods and tools;&lt;ref&gt;[http://www.recover-computerdata.com/ Data Recovery Software]&lt;/ref&gt; more serious cases can require expert intervention, particularly if parts of files are irrecoverable. [[File carving|Data carving]] is the recovery of parts of damaged files using knowledge of their structure.

===Overwritten data===
{{See also|Data erasure}}

After data has been physically overwritten on a hard disk drive, it is generally assumed that the previous data are no longer possible to recover. In 1996, [[Peter Gutmann (computer scientist)|Peter Gutmann]], a computer scientist, presented a paper that suggested overwritten data could be recovered through the use of [[magnetic force microscope]].&lt;ref&gt;[http://www.cs.auckland.ac.nz/~pgut001/pubs/secure_del.html ''Secure Deletion of Data from Magnetic and Solid-State Memory''], Peter Gutmann, Department of Computer Science, University of Auckland&lt;/ref&gt; In 2001, he presented another paper on a similar topic.&lt;ref&gt;[http://www.cypherpunks.to/~peter/usenix01.pdf ''Data Remanence in Semiconductor Devices''], Peter Gutmann, IBM T.J. Watson Research Center&lt;/ref&gt;  To guard against this type of data recovery, Gutmann and Colin Plumb designed a method of irreversibly scrubbing data, known as the [[Gutmann method]] and used by several disk-scrubbing software packages.

Substantial criticism has followed, primarily dealing with the lack of any concrete examples of significant amounts of overwritten data being recovered.&lt;ref&gt;{{cite web | last = Feenberg | first = Daniel | title = Can Intelligence Agencies Read Overwritten Data? A response to Gutmann. | publisher = National Bureau of Economic Research | date = 14 May 2004 | url = http://www.nber.org/sys-admin/overwritten-data-guttman.html | accessdate = 21 May 2008}}&lt;/ref&gt; Although Gutmann's theory may be correct, there is no practical evidence that overwritten data can be recovered, while research has shown to support that overwritten data cannot be recovered.{{specify|date=June 2013}}&lt;ref&gt;{{cite web|url=https://www.anti-forensics.com/disk-wiping-one-pass-is-enough/ |title=Disk Wiping –  One Pass is Enough |date=17 March 2009 |website=anti-forensics.com |deadurl=yes |archiveurl=https://web.archive.org/web/20120902011743/http://www.anti-forensics.com:80/disk-wiping-one-pass-is-enough |archivedate=2 September 2012 |df=dmy }}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://www.anti-forensics.com/disk-wiping-one-pass-is-enough-part-2-this-time-with-screenshots/ |title=Disk Wiping –  One Pass is Enough –  Part 2 (this time with screenshots) |date=18 March 2009 |website=anti-forensics.com |deadurl=yes |archiveurl=https://web.archive.org/web/20121127130830/https://www.anti-forensics.com/disk-wiping-one-pass-is-enough-part-2-this-time-with-screenshots/ |archivedate=27 November 2012 |df=dmy }}&lt;/ref&gt;&lt;ref&gt;{{cite web
 | url = http://blogs.sans.org/computer-forensics/2009/01/15/overwriting-hard-drive-data/
 | title = Overwriting Hard Drive Data
 | date = 15 January 2009
 | first = Dr. Craig | last = Wright
}}&lt;/ref&gt;

[[Solid-state drive]]s (SSD) overwrite data differently from [[hard disk drive]]s (HDD) which makes at least some of their data easier to recover. Most SSDs use [[flash memory]] to store data in pages and blocks, referenced by logical block addresses (LBA) which are managed by the flash translation layer (FTL). When the FTL modifies a sector it writes the new data to another location and updates the map so the new data appear at the target LBA. This leaves the pre-modification data in place, with possibly many generations, and recoverable by data recovery software.


===Lost, Deleted &amp; Formatted Data ===

Sometimes, data present in the physical drives (Internal/External Hard disk, Pen Drive, etc) gets lost, deleted and formatted due to circumstances like virus attack, accidental deletion or accidental use of SHIFT+DELETE. In these cases, data recovery software are used to recover/restore the data files. 

===Logical Bad Sector ===

In the list of logical failures of Hard disk, Logical bad sector is the most common in which data files can't be retrieved from particular sector of the media drives. To resolve this, software are incorporated to correct the logical sectors of media drive and is this is not enough, then there is need for replacement of hardware parts to make the logical bad sectors to be OK.

==Remote data recovery==
Recovery experts do not always need to have physical access to the damaged hardware.  When the lost data can be recovered by software techniques, they can often perform the recovery using remote access software over the Internet, LAN or other connection to the physical location of the damaged media.  The process is essentially no different from what the end user could perform by themselves.&lt;ref&gt;{{Cite web|url = http://datarecovery-overinternet.datarecoverydigest.com/|title = Data Recovery Over the Internet|date = 17 December 2012|accessdate = 29 April 2015|website = Data Recovery Digest|publisher = |last = Barton|first = Andre}}&lt;/ref&gt;

Remote recovery requires a stable connection with an adequate bandwidth. However, it is not applicable where access to the hardware is required, as in cases of physical damage.

==Four phases of data recovery==
Usually, there are four phases when it comes to successful data recovery, though that can vary depending on the type of data corruption and recovery required.&lt;ref&gt;{{cite web
 | url = http://www.dolphindatalab.com/the-four-phases-of-data-recovery/
 | title = [Infographic] Four Phases Of Data Recovery
 | date = 28 December 2012 | accessdate = 23 March 2015
 | author = Stanley Morgan | website = dolphindatalab.com
}}&lt;/ref&gt;

; Phase 1: Repair the hard disk drive
: Repair the hard disk drive so it is running in some form, or at least in a state suitable for reading the data from it. For example, if heads are bad they need to be changed; if the PCB is faulty then it needs to be fixed or replaced; if the spindle motor is bad the platters and heads should be moved to a new drive.

; Phase 2: Image the drive to a new drive or a disk image file
: When a hard disk drive fails, the importance of getting the data off the drive is the top priority. The longer a faulty drive is used, the more likely further data loss is to occur. Creating an image of the drive will ensure that there is a secondary copy of the data on another device, on which it is safe to perform testing and recovery procedures without harming the source.

; Phase 3: Logical recovery of files, partition, MBR and filesystem structures
: After the drive has been cloned to a new drive, it is suitable to attempt the retrieval of lost data. If the drive has failed logically, there are a number of reasons for that. Using the clone it may be possible to repair the partition table or [[master boot record]] (MBR) in order to read the file system's data structure and retrieve stored data.

; Phase 4: Repair damaged files that were retrieved
: Data damage can be caused when, for example, a file is written to a sector on the drive that has been damaged. This is the most common cause in a failing drive, meaning that data needs to be reconstructed to become readable. Corrupted documents can be recovered by several software methods or by manually reconstructing the document using a hex editor.

== See also ==
{{Portal|Computer security|Computing}}

{{Div col||20em}}
* [[Backup]]
* [[Cleanroom]]
* [[Comparison of file systems]]
* [[Computer forensics]]
* [[Continuous data protection]]
* [[Data archaeology]]
* [[Data loss]]
* [[Error detection and correction]]
* [[File carving]]
* [[Hidden file and hidden directory]]
* [[Undeletion]]
{{Div col end}}

== References ==
{{Reflist|30em}}

==Further reading==
* Tanenbaum, A. &amp; Woodhull, A. S. (1997). ''Operating Systems: Design And Implementation,'' 2nd ed. New York: Prentice Hall.
* {{dmoz|Computers/Hardware/Storage/Data_Recovery/|Data recovery}}


{{Data erasure}}

{{DEFAULTSORT:Data recovery}}
[[Category:Data recovery| ]]
[[Category:Computer data]]
[[Category:Data management]]
[[Category:Transaction processing]]
[[Category:Hard disk software|*]]
[[Category:Backup|Recovery]]</text>
      <sha1>qcw1v405k7sgjwvxeip6mm2b5l6r9tn</sha1>
    </revision>
  </page>
  <page>
    <title>Draft:Cloudiway</title>
    <ns>118</ns>
    <id>52991127</id>
    <revision>
      <id>762064649</id>
      <parentid>762057798</parentid>
      <timestamp>2017-01-26T12:16:39Z</timestamp>
      <contributor>
        <username>Melcous</username>
        <id>20472590</id>
      </contributor>
      <comment>/* External links */ See [[WP:ELNO]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4071" xml:space="preserve">{{New unreviewed article
| source = ArticleWizard
| date = January 2017
}}

{{Infobox company
 | name = Cloudiway
| logo = 
| type = [[Privately held company]]
| founder = Emmanuel Dreux
| area_served = Worldwide
| industry = [[Information technology]]&lt;br/&gt;[[Computer software]]&lt;br/&gt;[[Cloud computing]]&lt;br&gt;[[Email management]]
| products = Coexistence, mail migration, file migration, group migration, site migration, on-premises to cloud migration, mail archive migration, data migration
| foundation = 2010
| location_city = 
| location_country = 
| location = Annecy, France
| locations = 
| homepage = [http://www.cloudiway.com/ www.cloudiway.com]
}}

'''Cloudiway''' is an international [[Software as a service|SaaS]] company founded in 2010 in Annecy, France. The company builds cloud-based tools such as enterprise coexistence (free/busy calendar sharing, mail routing and automatic address list synchronisation), data migration to the cloud, and identity access management tools.

Cloudiway are currently the only company to offer enterprise coexistence between G Suite, Office 365 and Microsoft Exchange.

== Product history ==
After releasing CloudAnywhere, a locally-installed identity access management tool, Cloudiway began working on mail migration tools to enable migration between remote systems such as [[Gmail|G Mail]], [[Office 365]], [[Internet Message Access Protocol|IMAP]] and [[Microsoft Exchange Server|Exchange]]. Soon after, file migration and site migration utilities were launched, followed by enterprise coexistence in 2016&lt;ref&gt;{{Cite web|url=http://inpublic.globenewswire.com/releaseDetails.faces?rId=1999484|title=GlobeNewswire: Cloudiway is launching its solution for Coexistence between Google apps and Office 365/Exchange|website=inpublic.globenewswire.com|access-date=2017-01-26}}&lt;/ref&gt;. Mail archive migration and group migration utilities were launched shortly after, also in 2016. 

With a stable set of utilities under its belt, Cloudiway introduced a wider variety of mail migration sources and destinations, including [[Lotus Notes Mail|Lotus Notes mail]], [[Zimbra]] and Amazon WorkMail. File migration, which had been launched with [[Google Drive]], [[OneDrive]] and [[SharePoint]], was expanded in early 2017 to include file systems (such as Windows servers), Azure [[Binary large object|blob]] storage and [[Amazon S3]] storage.

== Company history ==
Cloudiway is headed by Emmanuel Dreux, a [[Microsoft Most Valuable Professional]] (MVP), who worked at IBM/Lotus and [[Microsoft]] before working on CloudAnywhere — Cloudiway's first product. The company moved to new headquarters in 2016&lt;ref&gt;{{Cite news|url=http://www.finanznachrichten.de/nachrichten-2016-06/37631705-cloudiway-cloudiway-continues-to-expand-its-global-expansion-and-announces-new-location-in-miami-florida-399.htm|title=CLOUDIWAY: Cloudiway continues to expand its global expansion and announces New location in Miami, Florida|newspaper=FinanzNachrichten.de|language=de|access-date=2017-01-26}}&lt;/ref&gt; to accommodate growing staff numbers. 

Cloudiway are a [https://partnercenter.microsoft.com/en-us/pcv/solution-providers/cloudiway_4298856859/861247_1 Microsoft Partner].  

The company offers consulting services in English and French, with offices in Florida, USA and Annecy, France. Through their consulting services and partnerships, the company has provided cloud migration solutions to a broad range of businesses, including education, the public sector, small private businesses and global enterprises. 


== References ==
{{Reflist}}

== External links ==
* [http://www.cloudiway.com Official Cloudiway website]
* [https://partnercenter.microsoft.com/en-us/pcv/solution-providers/cloudiway_4298856859/861247_1 Microsoft Partner Microsoft Partner page]
* [http://www.distributique.com/actualites/lire-cloudiway-%C2%A0sur-le-cloud-le-marche-francais-n-etait-pas-pret%C2%A0-25414.html Article in French about Cloudiway]


[[Category:Data management]]
[[Category:Cloud computing]]
[[Category:As a service]]
[[Category:Software companies of France]]</text>
      <sha1>hk2g411n9pc776lxmh77srpiiwgyc4d</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Information architecture</title>
    <ns>14</ns>
    <id>52912241</id>
    <revision>
      <id>762078286</id>
      <parentid>762078257</parentid>
      <timestamp>2017-01-26T14:32:26Z</timestamp>
      <contributor>
        <username>Swpb</username>
        <id>1921264</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="436" xml:space="preserve">{{cat main}}

Concepts, methodologies and topics related to the practice and theory of information architecture.

[[Category:Data management]]
[[Category:Enterprise architecture]]
[[Category:Information architects]]
[[Category:Information governance]]
[[Category:Information science]]
[[Category:Information technology management]]
[[Category:Information technology]]
[[Category:Records management]]
[[Category:Technical communication]]</text>
      <sha1>poidxuv0ublit5ehuakhbxuc2ekdtox</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Audio storage</title>
    <ns>14</ns>
    <id>754789</id>
    <revision>
      <id>547353504</id>
      <parentid>531605223</parentid>
      <timestamp>2013-03-28T00:08:39Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor />
      <comment>[[User:Addbot|Bot:]] Migrating 17 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q8275551]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="229" xml:space="preserve">{{Commons category|Audio storage media}}
{{Cat main|Sound recording and reproduction}}

[[Category:Storage media]]
[[Category:Electronic documents]]
[[Category:Sound recording technology]]
[[Category:Sound production technology]]</text>
      <sha1>9gvkbkbt5f86x1559ielb35u2d9rhjv</sha1>
    </revision>
  </page>
  <page>
    <title>Document retrieval</title>
    <ns>0</ns>
    <id>731640</id>
    <revision>
      <id>722724171</id>
      <parentid>701537816</parentid>
      <timestamp>2016-05-29T20:18:34Z</timestamp>
      <contributor>
        <username>Dcirovic</username>
        <id>11795905</id>
      </contributor>
      <minor />
      <comment>clean up using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5819" xml:space="preserve">'''Document retrieval''' is defined as the matching of some stated user query against a set of [[free-text]] records. These records could be any type of mainly [[natural language|unstructured text]], such as [[newspaper article]]s, real estate records or paragraphs in a manual. User queries can range from multi-sentence full descriptions of an information need to a few words.

Document retrieval is sometimes referred to as, or as a branch of, '''text retrieval'''. Text retrieval is a branch of [[information retrieval]] where the information is stored primarily in the form of [[natural language|text]]. Text databases became decentralized thanks to the [[personal computer]] and the [[CD-ROM]]. Text retrieval is a critical area of study today, since it is the fundamental basis of all [[internet]] [[search engine]]s.

==Description==
Document retrieval systems find information to given criteria by matching text records (''documents'') against user queries, as opposed to [[expert system]]s that answer questions by [[Inference|inferring]] over a logical [[knowledge base|knowledge database]]. A document retrieval system consists of a database of documents, a [[classification algorithm]] to build a full text index, and a user interface to access the database.

A document retrieval system has two main tasks:
# Find relevant documents to user queries
# Evaluate the matching results and sort them according to relevance, using algorithms such as [[PageRank]].

Internet [[search engines]] are classical applications of document retrieval. The vast majority of retrieval systems currently in use range from simple Boolean systems through to systems using [[statistical]] or [[natural language processing]] techniques.

==Variations==
There are two main classes of indexing schemata for document retrieval systems: ''form based'' (or ''word based''), and ''content based'' indexing. The document classification scheme (or [[Search engine indexing|indexing algorithm]]) in use determines the nature of the document retrieval system.

===Form based===
Form based document retrieval addresses the exact syntactic properties of a text, comparable to substring matching in string searches. The text is generally unstructured and not necessarily in a natural language, the system could for example be used to process large sets of chemical representations in molecular biology. A [[suffix tree]] algorithm is an example for form based indexing.

===Content based===
The content based approach exploits semantic connections between documents and parts thereof, and semantic connections between queries and documents. Most content based document retrieval systems use an [[inverted index]] algorithm.

A ''signature file'' is a technique that creates a ''quick and dirty'' filter, for example a [[Bloom filter]], that will keep all the documents that match to the query and ''hopefully'' a few ones that do not. The way this is done is by creating for each file a signature, typically a hash coded version. One method is superimposed coding. A post-processing step is done to discard the false alarms. Since in most cases this structure is inferior to [[inverted file]]s in terms of speed, size and functionality, it is not used widely. However, with proper parameters it can beat the inverted files in certain environments.

==Example: PubMed==
The [[PubMed]]&lt;ref&gt;{{cite journal |vauthors=Kim W, Aronson AR, Wilbur WJ |title=Automatic MeSH term assignment and quality assessment |journal=Proc AMIA Symp |pages=319–23 |year=2001 |pmid=11825203 |pmc=2243528 }}
&lt;/ref&gt; form interface features the "related articles" search which works through a comparison of words from the documents' title, abstract, and [[Medical Subject Headings|MeSH]] terms using a word-weighted algorithm.&lt;ref&gt;{{cite web|url=https://www.ncbi.nlm.nih.gov/books/NBK3827/#pubmedhelp.Computation_of_Related_Citati|title=Computation of Related Citations}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|journal=BMC Bioinformatics|date=Oct 30, 2007|volume=8|pages=423|pmid=17971238|title=PubMed related articles: a probabilistic topic-based model for content similarity|author=Lin J1, Wilbur WJ|doi=10.1186/1471-2105-8-423|pmc=2212667}}&lt;/ref&gt;

== See also ==

* [[Compound term processing]]
* [[Document classification]]
* [[Enterprise search]]
* [[Full text search]]
* [[Information retrieval]]
* [[Latent semantic indexing]]
* [[Search engine]]

== References ==

&lt;references/&gt;

==Further reading==
* {{cite journal|first1=Christos|last1=Faloutsos|first2=Stavros|last2=Christodoulakis|title=Signature files: An access method for documents and its analytical performance evaluation|journal=ACM Transactions on Information Systems|volume=2|issue=4|year=1984|pages=267–288|doi=10.1145/2275.357411}}
* {{cite journal|author1=Justin Zobel |author2=Alistair Moffat |author3=Kotagiri Ramamohanarao |title=Inverted files versus signature files for text indexing|journal=ACM Transactions on Database Systems|volume=23|issue=4|year=1998|pages= 453–490|url=http://www.cs.columbia.edu/~gravano/Qual/Papers/19%20-%20Inverted%20files%20versus%20signature%20files%20for%20text%20indexing.pdf|doi=10.1145/296854.277632}}
* {{cite journal|author1=Ben Carterette |author2=Fazli Can |title=Comparing inverted files and signature files for searching a large lexicon|journal=Information Processing and Management|volume= 41|issue=3|year=2005|pages= 613–633|url=http://www.users.miamioh.edu/canf/papers/ipm04b.pdf|doi=10.1016/j.ipm.2003.12.003}}

== External links ==
* [http://cir.dcs.uni-pannon.hu/cikkek/FINAL_DOMINICH.pdf Formal Foundation of Information Retrieval], Buckinghamshire Chilterns University College

[[Category:Information retrieval genres]]
[[Category:Electronic documents]]
[[Category:Substring indices]]
[[Category:Search engine software]]

[[zh:文本信息检索]]</text>
      <sha1>iu7olurw00fk6emu4f3rcvs4xa7ym58</sha1>
    </revision>
  </page>
  <page>
    <title>Electronic Document Professional</title>
    <ns>0</ns>
    <id>17463457</id>
    <revision>
      <id>697529947</id>
      <parentid>693388930</parentid>
      <timestamp>2015-12-31T02:38:53Z</timestamp>
      <contributor>
        <username>Mccalpin</username>
        <id>468039</id>
      </contributor>
      <minor />
      <comment>/* History */ fixed the mangled edit from Dec 2, 2015</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7533" xml:space="preserve">The '''EDP''' ('''Electronic Document Professional''') is a professional designation awarded by [[Xplor International]] for participants in the [[electronic document]] industry who have satisfied a number of criteria.

The EDP is not a measure of specific knowledge, but awardees normally have a broad knowledge of the electronic document industry along with specific knowledge in one or more areas. Thus, the EDP differs from certifications such as the [[CDIA+]] from CompTIA in which the awardee has passed a formal exam. Rather, the EDP is more closely related to the older MIT and LIT designations from [[AIIM]]. Generally, the EDP does show that the awardee has been in the industry for at least 5 years, and has participated in at least 3 major projects showing competence in a number of 'bodies of knowledge'.

The EDP program is regulated by the EDP Commission of [[Xplor International]], which is a body of senior professionals in the electronic document industry who set the standards and judge the qualifications of the applicants.

== History ==

The first EDP 'class' (as the annual group of awardees is called) was in 1990, when 12 industry professionals were given the award. In 1991, the class was much smaller (only 6), but in 1992 and in all the years following, the number of awardees has generally been double digits, with as many as 25 at one time. The Dutch Chapter of Xplor International has particularly stressed the EDP designation as an essential part of being a professional in the electronic document industry, with the result that in some years, more than ten Dutch members alone were named EDPs, and there are more EDPs per capita in The Netherlands today than in any other country.

In 2009, Xplor relaunched its certification program, so that there is now a three-level certification process to help employers benchmark their staff.&lt;ref&gt;http://www.printingnews.com/web/online/Industry-News/Xplor-Expands-EDP-Certification/1$10502&lt;/ref&gt;

The EDP and the Master-EDP awards are presented once a year at the annual international conference of Xplor. While the first EDPs were awarded at the 1990 conference in Nashville, Tennessee (USA), the first 'class' of Master-EDPs was awarded at the association's annual event in St. Petersburg, Florida (USA) in March, 2010. At that event, the following industry professionals received the association's highest certification:&lt;ref name="m-edp"&gt;[http://www.xplor.org/EDP/MEDPlist.cfm]&lt;/ref&gt;
* William Broddy, M-EDP
* Ernie Crawford, M-EDP
* Scott Draeger, M-EDP
* Oscar Dubbeldam, M-EDP
* William J. "Bill" McCalpin, M-EDP, CDIA, MIT, LIT
* Walter Riddock, M-EDP, CMDSM
* Donald Scrima, M-EDP

At the association's next annual event in April, 2011, two more industry professionals will be named M-EDPs:&lt;ref name="m-edp"/&gt;
* Pat McGrew, M-EDP
* Carrie Murphy, M-EDP

On the other hand, the new EDA designation is awarded at the point that the individual's application is accepted and verified, throughout the year. Currently (December 2010), there are 48 industry professionals who have received this designation,&lt;ref&gt;http://www.xplor.org/EDP/EDA.cfm?viewpage=EDAlist&lt;/ref&gt; nearly all of whom received the designation as a result of attending courses certified by Xplor and taught by [http://www.acadami.org ''acadami''], two of whose principals are M-EDPs.

== Levels of certification ==

=== Electronic Document Associate (EDA) ===
The EDA designation recognizes electronic document sales, development and support specialists who have shown significantly more knowledge of the industry than someone in another discipline.
It requires candidates to be in the industry for 2+ years and have successfully completed 5 days of Xplor Continuing Education Unit (CEU) certified courses, or the equivalent.&lt;ref&gt;http://www.xplor.org/EDP/EDA.cfm&lt;/ref&gt;

=== Electronic Document Professional (EDP) ===
EDPs have clearly shown enough working knowledge of the process to make significant decisions regarding technology or process deployment. For example, management should trust them to lead projects, or support teams.
To become certified as an EDP, a candidate must be in the industry for 5+ years, have successfully completed 10 days of Xplor CEU training (or the equivalent), and have shown their working knowledge and experience through 3 work examples.&lt;ref&gt;http://www.xplor.org/EDP/EDP.cfm&lt;/ref&gt;

=== Master Electronic Document Professional (M-EDP) ===
M-EDPs are the recognized experts on specific technologies, processes, or management skills. For example, an M-EDP may have co-developed a composition or print stream transform system. Another might be the expert on print costing, or statement design. By earning their M-EDP, they are clearly recognized as one of the ‘go to’ people in the industry.

To earn the M-EDP, a candidate must have been in the industry for at least 10 years, have been an EDP for at least 5 years, and be able to prove their area of expertise through published material.&lt;ref&gt;http://www.xplor.org/EDP/MEDP.cfm&lt;/ref&gt;

== Designees ==
In all, there are more than 500 industry professionals with the EDP designation, including:
* [http://www.xplorcanada.org/media/eastern/Broddy-Bio-2005.pdf William Broddy, M-EDP] - formerly the Vice Chair of the EDP Commission
* [http://www.mccalpin.com William J. 'Bill' McCalpin, M-EDP] - author of ''The Document Dilemma'' and former General Manager of Xplor International 
* [http://www.acadami.org/about.html Dr. Michael Turton, EDP] - recognized expert in the design of transaction documents and the use of color in transaction documents
* Scott Kelly, EDP - current Xplor International board member
* [http://www.crawfordtech.com/ManagementTeam.htm Ernie Crawford, M-EDP] - President of Crawford Technologies
* [http://www.nautilussolutions.com Stephen Poe, EDP] - Principal at Nautilus Solutions
* [http://www.gmc.net Scott Draeger, M-EDP] - Vice President of Product at GMC Software
* [http://welcome.hp.com/country/us/en/prodserv/software/eda/products/hpexstream.html Loic Avenel, EDP] - EMEA Product Marketing Manager at HP Exstream
* [http://welcome.hp.com/country/us/en/prodserv/software/eda/products/hpexstream.html Olivier DOILLON, EDP] - EMEA South Presales Manager at HP Exstream
* Roberta McKee-Jackson, EDP - Principal at RSM Consulting and current Vice-Chair of EDP Commission
* Skip Henk, EDP - Current President/CEO of Xplor International
* [http://welcome.hp.com/country/us/en/prodserv/software/eda/products/hpexstream.html Kent Lewis, EDP] - Product Manager, HP Exstream
* [http://welcome.hp.com/country/us/en/prodserv/software/eda/products/hpexstream.html Matt Riley, EDP] - Solution Support Manager, HP Exstream
* Donald A. Scrima, M-EDP - Current Chair of EDP Commission (2009-20120) and Principal at AFP Education &amp; Consulting
* [http://www.deberichtenfabriek.nl/wie-wij-zijn/dick-paul-en-rene-joor René M. Joor, EDP] - Current member of the EDP Commission, Partner at De Berichtenfabriek BV, the Netherlands 
* Kenneth H. Pugh, EDP - Senior Output Analyst at Aon Corporation
* Kathy Rixham, EDP - Print System Administrator at RR Donnelley
* Kees van de Graaf, EDP - Consultant at PostNL
* James Shand, EDP - Xplor International member of +30-years, Current President Xplor UK &amp; Ireland and member of the Xplor International Board of Directors

==References==
{{reflist}}

==External links==
* [http://www.xplor.org/ Xplor International Web Page]
* [http://www.xplor.org/edp/index.cfm Direct Link to Xplor International Certification Program]

[[Category:Electronic documents]]</text>
      <sha1>foqirqtg6cfxxy5kirqecdvibl4moob</sha1>
    </revision>
  </page>
  <page>
    <title>SAFE-BioPharma Association</title>
    <ns>0</ns>
    <id>2321249</id>
    <revision>
      <id>715659724</id>
      <parentid>715574250</parentid>
      <timestamp>2016-04-17T06:03:33Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor />
      <comment>Removed invisible unicode characters + other fixes, removed: ‎ using [[Project:AWB|AWB]] (12002)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5445" xml:space="preserve">{{primary sources|date=February 2010}}
'''SAFE-BioPharma Association''' is the non-profit association that created and manages the SAFE-BioPharma [[digital identity]] and [[digital signature]] standard for the global [[pharmaceutical]], [[biotech]] and [[healthcare]] industries. SAFE stands for "Signatures &amp; Authentication For Everyone" (but originally stood for "Secure Access For Everyone"&lt;ref&gt;{{cite web|title=CSI page on SAFE describing original goal and name|url=http://www.csirochester.com/safe.htm}}&lt;/ref&gt;).  It was originally created as an initiative of the [[Pharmaceutical Research and Manufacturers of America]] (PhRMA) association to encourage the use of a common digital identity and digital signature standard for the pharmaceutical industry, but is now an independent non-profit association offering such standards services to the government and the entire healthcare industry.

The SAFE-BioPharma industry standard is used to establish and manage digital identities and to issue and apply digital signatures. It mitigates legal, regulatory and business risk associated with business-to-business and business-to-regulator electronic transactions. It also facilitates interoperability by providing a secure, enforceable, and regulatory-compliant way to verify identities of parties involved in electronic transactions.

SAFE-BioPharma’s vision is to be a catalyst in transforming the biopharmaceutical and healthcare communities to a fully electronic business environment by 2012.

== Certificate authority ==
The SAFE-BioPharma digital identity and signature standard operates one of the nation’s leading [[public key infrastructure]] (PKI) certificate authority bridges. These PKI certificate bridges establish an infrastructure for the trusted exchange of confidential information and the reliable authentication of identities over the Internet. By providing a highly secure way to validate, trust, and manage identities of unknown participants in an Internet transaction, the SAFE-BioPharma Bridge [[certificate authority|Certificate Authority]] (SBCA) is essential to helping achieve the speed, efficiency and cost-savings inherent in use of the Internet for business transactions. This technology also improves interoperability across many different systems.

== Cross-certification ==
The SAFE-BioPharma Bridge Certificate Authority is cross-certified with the [http://www.cio.gov/fpkia/ Federal Bridge Public Key Infrastructure Architecture] (FPKIA), facilitating the ability of SAFE-BioPharma member companies that meet certain security, technical and operational criteria to leverage the identity credentials of any and all bridge members in the exchange of sensitive and confidential information. In essence, it allows officials in [[Health and Human Services]], the [[Food and Drug Administration]], [[United States Department of Defense|Department of Defense]], and other government agencies to trust the origins of electronic documents received from corporate managers, physicians, clinical researchers, etc. who are credentialed to digitally sign documents with SAFE-BioPharma digital signatures. Additionally, the identities are trusted for authentication access control for sites requiring strong authentication.

== Regulatory acceptance ==
SAFE-BioPharma has worked closely with the US Food and Drug Administration (FDA), the [[European Medicines Agency]] (EMA) and other global healthcare and regulatory agencies to ensure the digital signatures generated using SAFE-BioPharma certificates meet regulatory requirements and are accepted by these agencies when used on documents that are part of electronic submissions. It thus allows voluminous paper documents used for regulatory compliance to be digitally signed and submitted in electronic form. This improves accuracy, reduces costs, enables electronic search and retrieval and saves energy and natural resources.

== Member organizations ==
SAFE-BioPharma Association members include [[Abbott Laboratories]] (NYSE: ABT), [[Amgen]] (NASDAQ: AMGN), [[AstraZeneca]] (NYSE: AZN), [[Bristol-Myers Squibb]] (NYSE:BMY), [[GlaxoSmithKline]] (NYSE: GSK), [[Johnson &amp; Johnson]] (NYSE: JNJ), [[Eli Lilly and Company|Eli Lilly]] (NYSE: LLY), [[Merck &amp; Co.|Merck]] (NYSE:MRK), [[National Notary Association]], [[Pfizer]] (NYSE: PFE), Premier Inc.,  and [[Sanofi-Aventis]] (NYSE:SNY).

SAFE-BioPharma is a trademark of the SAFE-BioPharma Association. Any use of this trademark requires approval from the SAFE-BioPharma Association.

==See also==
* [[Electronic lab notebook]]
* [[Public key infrastructure|PKI]]
* [[Title 21 CFR Part 11]]
* [[Digital signature]]
* [[Electronic Signatures in Global and National Commerce Act]] (ESIGN, USA)
* [[European Medicines Agency]] (EMEA)
* [[Food and Drug Administration]] (FDA)
* [[Pharmaceutical company]]
* [[Japan Pharmaceutical Manufacturers Association]] (JPMA)
* [[Digital signature]]
* [[Data management]]

==External links==
* [http://www.safe-biopharma.org SAFE-BioPharma Organization Website]

==References==

{{Reflist}}

{{DEFAULTSORT:Safe-Biopharma Association}}
[[Category:Public-key cryptography]]
[[Category:Cryptography companies]]
[[Category:Electronic documents]]
[[Category:Non-profit organizations based in New Jersey]]
[[Category:Pharmaceutical industry trade groups]]
[[Category:International medical and health organizations]]
[[Category:Medical and health organizations based in the United States]]</text>
      <sha1>2r1ccl8ppeloj0t0xw0zya8ochoo0gu</sha1>
    </revision>
  </page>
  <page>
    <title>Digital object identifier</title>
    <ns>0</ns>
    <id>422994</id>
    <revision>
      <id>763040353</id>
      <parentid>763023012</parentid>
      <timestamp>2017-02-01T01:00:32Z</timestamp>
      <contributor>
        <ip>73.253.110.94</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="27416" xml:space="preserve">{{Selfref|For the use of digital object identifiers on Wikipedia, see [[Wikipedia:Digital Object Identifier]].}}
{{Use dmy dates|date=February 2011}}
{{Infobox identifier
| name          = Digital object identifier
| image         = DOI logo.svg
| image_size    = 130px
| image_caption = 
| image_alt     = 
| image_border  = no
| full_name     = 
| acronym       = DOI
| number        = 
| start_date    = {{Start date|2000}}
| organisation  = International DOI Foundation
| digits        = 
| check_digit   = 
| example       = 
| website       = {{URL|doi.org}}
}}
In computing, a '''Digital Object Identifier''' or '''DOI''' is a [[persistent identifier]] or [[handle (computing)|handle]] used to uniquely identify objects. An implementation of the [[Handle System]]&lt;ref&gt;{{cite web|url= http://handle.net/|title=The Handle System}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://www.doi.org/factsheets.html |title=Factsheets}}&lt;/ref&gt; and standardized by the [[ISO]], DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they can be used to identify other objects, such as commercial videos.

DOI means "digital identifier of an object" rather than "identifier of a digital object".&lt;ref name = "iso"&gt;{{cite web | url = https://www.iso.org/obp/ui/#iso:std:iso:26324:ed-1:v1:en | title = ISO 26324:2012(en), Information and documentation — Digital object identifier system | publisher = [[ISO]] | date = | accessdate = 2016-04-20 | quote = DOI is an acronym for 'digital object identifier', meaning a 'digital identifier of an object' rather than an 'identifier of a digital object'.}} "Introduction", paragraph 2.&lt;/ref&gt; Thus ''DOI'' stands for "digital object-identifier" rather than "digital-object identifier".

[[Metadata]] about the object is stored in association with the DOI name. It may include a location, such as a [[URL]], indicating where the object can be found. The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI provides more stable linking than simply using its URL, because if its URL changes, the publisher only needs to update the metadata for the DOI to link to the new URL.&lt;ref&gt;{{cite book|author= Witten, Ian H.|author2= David Bainbridge|author3= David M. Nichols|last-author-amp= yes |date= 2010|title= How to Build a Digital Library|edition= 2nd|location= Amsterdam; Boston|publisher= Morgan Kaufmann|pages= 352–253|isbn= 978-0-12-374857-7}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal|first1= Marc|last1= Langston|first2= James|last2= Tyler|title= Linking to journal articles in an online teaching environment: The persistent link, DOI, and OpenURL|journal= The Internet and Higher Education|volume= 7|issue= 1|date= 2004|pages= 51–58|doi= 10.1016/j.iheduc.2003.11.004}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal |url=http://www.bloomberg.com/bw/stories/2001-07-22/online-extra-how-the-digital-object-identifier-works |title= How the 'Digital Object Identifier' works |date= 23 July 2001 |work= BusinessWeek |accessdate= 20 April 2010 |quote= Assuming the publishers do their job of maintaining the databases, these centralized references, unlike current Web links, should never become outdated or broken. |publisher= [[BusinessWeek]]}}&lt;/ref&gt;

A DOI name differs from standard identifier registries such as the [[ISBN]] and [[ISRC]]. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable.

The DOI system began in 2000 and is managed by the International DOI Foundation.&lt;ref&gt;{{citation|last= Paskin|first= Norman|chapter= Digital Object Identifier (DOI®) System|title= Encyclopedia of Library and Information Sciences|date= 2010|publisher= Taylor and Francis|pages= 1586–1592|edition= 3rd}}&lt;/ref&gt; Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.&lt;ref name="dd"&gt;{{Cite journal|title= Digital Object Identifiers: Promise and problems for scholarly publishing|first1= Lloyd A.|last1= Davidson|first2= Kimberly|last2= Douglas|date= December 1998|journal= Journal of Electronic Publishing|volume= 4|issue= 2|doi= 10.3998/3336451.0004.203}}&lt;/ref&gt; The DOI system is implemented through a federation of registration agencies coordinated by the International DOI Foundation,&lt;ref&gt;{{cite web|url= https://doi.org/ |title= Welcome to the DOI System |publisher= Doi.org |date= 28 June 2010 |accessdate= 7 August 2010}}&lt;/ref&gt; which developed and controls the system. The DOI system has been developed and implemented in a range of publishing applications since 2000; by late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations.&lt;ref&gt;{{Cite web|url= https://doi.org/news/DOINewsApr11.html#1 |title= DOI® News, April 2011: 1. DOI System exceeds 50 million assigned identifiers |publisher= Doi.org |date= 20 April 2011 |accessdate= 3 July 2011}}&lt;/ref&gt; By April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.

==Nomenclature==
A DOI name takes the form of a [[character string]] divided into two parts, a prefix and a suffix, separated by a slash. The prefix identifies the registrant of the name, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal [[Unicode]] characters are allowed in these strings, which are interpreted in a [[case-insensitive]] manner. The prefix usually takes the form &lt;code&gt;10.NNNN&lt;/code&gt;, where &lt;code&gt;NNNN&lt;/code&gt; is a series of at least 4 numbers greater than or equal to &lt;code&gt;1000&lt;/code&gt;, whose limit depends only on the total number of registrants.&lt;ref name="CrossRefDOI"&gt;{{cite web |url=http://www.crossref.org/01company/15doi_info.html |access-date=10 June 2016 |title=doi info &amp; guidelines |website=CrossRef.org |publisher=Publishers International Linking Association, Inc. |date=2013 |quote=All DOI prefixes begin with "10" to distinguish the DOI from other implementations of the Handle System followed by a four-digit number or string (the prefix can be longer if necessary).}}&lt;/ref&gt;&lt;ref name="DOIKeyFacts"&gt;{{cite web |url=https://doi.org/factsheets/DOIKeyFacts.html |access-date=10 June 2016 |title=Factsheet—Key Facts on Digital Object Identifier System |website=doi.org |publisher=International DOI Foundation |date=June 6, 2016 |quote=Over 18,000 DOI name prefixes within the DOI System}}&lt;/ref&gt; The prefix may be further subdivided with periods, like &lt;code&gt;10.NNNN.N&lt;/code&gt;.&lt;ref name="2.2.2"&gt;{{cite web |url=https://doi.org/doi_handbook/2_Numbering.html#2.2.2 |access-date=10 June 2016 |title=DOI Handbook—2 Numbering |website=doi.org |publisher=International DOI Foundation |date=February 1, 2016 |quote=The registrant code may be further divided into sub-elements for administrative convenience if desired. Each sub-element of the registrant code shall be preceded by a full stop.}}&lt;/ref&gt;

For example, in the DOI name &lt;code&gt;10.1000/182&lt;/code&gt;, the prefix is &lt;code&gt;10.1000&lt;/code&gt; and the suffix is &lt;code&gt;182&lt;/code&gt;. The "10." part of the prefix identifies the DOI registry,{{efn-ua|Other registries are identified by other strings at the start of the prefix. Handle names that begin with "100." are also in use, as for example in the following citation: {{cite journal|hdl=100.2/ADA013939 |url=http://handle.dtic.mil/100.2/ADA013939 |title=Development of a Transmission Error Model and an Error Control Model l |journal=&lt;!-- --&gt; |volume=&lt;!-- --&gt; | date=May 1975 |last1=Hammond |first1=Joseph L., Jr. |last2=Brown |first2=James E. |last3=Liu |first3=Shyan-Shiang S. |bibcode=1975STIN...7615344H|publisher=Rome Air Development Center|series=Technical Report RADC-TR-75-138}}}} and the characters &lt;code&gt;1000&lt;/code&gt; in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. &lt;code&gt;182&lt;/code&gt; is the suffix, or item ID, identifying a single object (in this case, the latest version of the ''DOI Handbook'').

DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, [[performance]]s, and abstract works&lt;ref name=doifaq2&gt;{{Cite journal |url=https://doi.org/faq.html#1 |title=Frequently asked questions about the DOI system: 2. What can be identified by a DOI name? | accessdate = 23 April 2010 | date = 17 February 2010|origyear=update of earlier version |publisher=International DOI Foundation}}
&lt;/ref&gt; such as licenses, parties to a transaction, etc.

The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the [[indecs Content Model]].

===Display===
The official ''DOI Handbook'' explicitly states that DOIs should display on screens and in print in the format "doi:10.1000/182".&lt;ref name="C4WDefault-2811140"&gt;{{cite web |url=https://doi.org/doi_handbook/2_Numbering.html#2.6.1 |title=DOI Handbook – Numbering |date=13 February 2014 |accessdate=30 June 2014 |work=doi.org |author1=&lt;!--Staff writer(s); no by-line.--&gt; |archiveurl=https://web.archive.org/web/20140630181440/http://www.doi.org/doi_handbook/2_Numbering.html |archivedate=30 June 2014 |deadurl=no |at=Section 2.6.1 Screen and print presentation}}&lt;/ref&gt; Contrary to the ''DOI Handbook'', [[CrossRef]], a major DOI registration agency, recommends displaying a URL (for example, &lt;code&gt;&lt;nowiki&gt;https://doi.org/10.1000/182&lt;/nowiki&gt;&lt;/code&gt;) instead of the officially specified format (for example, &lt;code&gt;[https://doi.org/10.1000/182 doi:10.1000/182]&lt;/code&gt;)&lt;ref&gt;{{Cite web| title=DOI Display Guidelines|url=http://www.crossref.org/02publishers/doi_display_guidelines.html}}&lt;/ref&gt;&lt;ref&gt;{{Cite web| title=New Crossref DOI display guidelines are on the way|url=http://blog.crossref.org/2016/09/new-crossref-doi-display-guidelines.html}}&lt;/ref&gt; This URL provides the location of an [[HTTP proxy]] server which will redirect web accesses to the correct online location of the linked item.&lt;ref name="dd"/&gt;&lt;ref&gt;{{Cite journal | first=Andy |last=Powell |title=Resolving DOI Based URNs Using Squid: An Experimental System at UKOLN |journal = D-Lib Magazine|date=June 1998|url=http://www.dlib.org/dlib/june98/06powell.html| issn=1082-9873}}&lt;/ref&gt; This recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL – the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.

==Applications==
Major applications of the DOI system currently include:
* [[Scientific literature|scholarly materials]] (journal articles, books, ebooks, etc.) through [[CrossRef]], a consortium of around 3,000 publishers;
* research datasets through [[DataCite]], a consortium of leading research libraries, technical information providers, and scientific data centers;
* [[European Union]] official publications through the [[Publications Office (European Union)|EU publications office]];
* Permanent global identifiers for commercial video content through the Entertainment ID Registry, commonly known as [[EIDR]].

In the [[Organisation for Economic Co-operation and Development]]'s publication service [[OECD iLibrary]], each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.&lt;ref&gt;{{cite journal|doi=10.1787/603233448430 |title=We Need Publishing Standards for Datasets and Data Tables |date=2009|journal=Research Information |last=Green|first=T.}}&lt;/ref&gt;

A multilingual European DOI registration agency activity, [http://www.mEDRA.org ''m''EDRA], Traditional Chinese content thru [http://doi.airiti.com/ Airiti Inc.] and a Chinese registration agency, [http://www.wanfangdata.com/ Wanfang Data], are active in non-English language markets. Expansion to other sectors is planned by the International DOI Foundation.{{Citation needed|date=May 2010}}

==Features and benefits==
The DOI system was designed to provide a form of [[Persistent identifier|persistent identification]], in which each DOI name permanently and unambiguously identifies the object to which it is associated. And, it associates [[metadata]] with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the [[Handle System]] and the [[indecs Content Model]] with a social infrastructure.

The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the [[Uniform Resource Identifier|URI]] specification. The DOI name resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on [[open architecture]]s, incorporates [[Computational trust|trust mechanisms]], and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.&lt;ref&gt;{{cite web|url=http://arstechnica.com/science/2010/03/dois-and-their-discontents-1/|title=DOIs and their discontents|last=Timmer|first=John|date=6 March 2010|work=[[Ars Technica]]|accessdate=5 March 2013}}&lt;/ref&gt; DOI name resolution may be used with [[OpenURL]] to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.&lt;ref&gt;{{Cite journal|first1=Susanne|last1=DeRisi|first2=Rebecca|last2=Kennison|first3=Nick|last3=Twyman|title=Editorial: The what and whys of DOIs|journal=[[PLoS Biology]]|volume=1|issue=2|page=e57|date=2003|doi=10.1371/journal.pbio.0000057|pmid=14624257|pmc=261894}} {{open access}}&lt;/ref&gt; However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.&lt;ref&gt;{{Cite book|contribution=Open access to scientific and technical information: the state of the art|first=Jack|last=Franklin|title=Open access to scientific and technical information: state of the art and future trends|editor1-first=Herbert|editor1-last=Grüttemeier|editor2-first=Barry|editor2-last=Mahon|publisher=IOS Press|date=2003|page=74|url=https://books.google.com/?id=2X3gW1lUvN4C&amp;pg=PA74#v=onepage&amp;q|isbn=978-1-58603-377-4}}&lt;/ref&gt;

The [[indecs Content Model]] is used within the DOI system to associate metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.

The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as [[GS1]] and [[ISBN]].

==Comparison with other identifier schemes==
A DOI name differs from commonly used Internet pointers to material, such as the [[Uniform Resource Locator]] (URL), in that it identifies an object itself as a [[First class (computing)|first-class entity]], rather than the specific place where the object is located at a certain time. It implements the [[Uniform Resource Identifier]] ([[Uniform Resource Name]]) concept and adds to it a data model and social infrastructure.&lt;ref&gt;{{cite web|url=https://doi.org/factsheets/DOIIdentifierSpecs.html |title=DOI System and Internet Identifier Specifications |publisher=Doi.org |date=18 May 2010 |accessdate=7 August 2010}}&lt;/ref&gt;

A DOI name also differs from standard identifier registries such as the [[International Standard Book Number|ISBN]], [[International Standard Recording Code|ISRC]], etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.&lt;ref&gt;{{cite web|url=https://doi.org/factsheets/DOIIdentifiers.html |title=DOI System and standard identifier registries |publisher=Doi.org |accessdate=7 August 2010}}&lt;/ref&gt;

The DOI system offers persistent, [[Semantic interoperability|semantically-interoperable]] resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn't mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include [[Persistent Uniform Resource Locator]] (PURL), URLs, [[Globally Unique Identifier]]s (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., [[Archival Resource Key|ARK]]).

A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name.

==Resolution==
DOI name resolution is provided through the [[Handle System]], developed by [[Corporation for National Research Initiatives]], and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its &lt;code&gt;&lt;type&gt;&lt;/code&gt; field, which defines the syntax and semantics of its data.

To resolve a DOI name, it may be input to a DOI resolver (e.g. [https://doi.org/ doi.org]) or may be represented as an HTTP string by preceding the DOI name by the string &lt;code&gt;&lt;nowiki&gt;https://doi.org/&lt;/nowiki&gt;&lt;/code&gt; (preferred)&lt;ref&gt;{{cite web|author1=International DOI Foundation|title=Resolution|url=https://doi.org/doi_handbook/3_Resolution.html#3.7.3|website=DOI Handbook|accessdate=19 March 2015|date=2014-08-07}}&lt;/ref&gt; or &lt;code&gt;&lt;nowiki&gt;https://dx.doi.org/&lt;/nowiki&gt;&lt;/code&gt;. For example, the DOI name &lt;code&gt;10.1000/182&lt;/code&gt; can be resolved at the address "&lt;nowiki&gt;https://doi.org/10.1000/182&lt;/nowiki&gt;". Web pages or other hypertext documents can include hypertext links in this form. Some browsers allow the direct resolution of a DOI (or other handles) with an add-on, e.g., [http://www.handle.net/hs-tools/extensions/firefox_hdlclient.html CNRI Handle Extension for Firefox]. The CNRI Handle Extension for Firefox enables the browser to access handle or DOI URIs like hdl:4263537/4000 or doi:10.1000/1 using the native Handle System protocol. It will even replace references to web-to-handle proxy servers with native resolution.

Alternative DOI resolvers include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/ and http://doai.io. The last is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.&lt;ref&gt;{{cite web|url=http://doai.io/|title=DOAI|publisher=CAPSH (Committee for the Accessibility of Publications in Sciences and Humanities)|accessdate=6 August 2016}}&lt;/ref&gt;&lt;ref&gt;{{Cite web| last = Schonfeld| first = Roger C.| title = Co-opting 'Official' Channels through Infrastructures for Openness |work = The Scholarly Kitchen| accessdate = 2016-10-17| date = 2016-03-03| url = https://scholarlykitchen.sspnet.org/2016/03/03/coopting-official-channels/}}&lt;/ref&gt;

==Organizational structure==
The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.&lt;ref&gt;{{cite book|url=https://doi.org/doi_handbook/7_IDF.html#7.5 |title=DOI Handbook |chapter=Chapter 7: The International DOI Foundation |publisher=Doi.org |accessdate=8 July 2015}}&lt;/ref&gt; It safeguards all [[intellectual property|intellectual property rights]] relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.

The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.

Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation.

Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a [[not-for-profit]] cost recovery basis.

==Standardization==
The DOI system is an international standard developed by the [[International Organization for Standardization]] in its technical committee on identification and description, TC46/SC9.&lt;ref&gt;{{cite web|url=http://www.iso.org/iso/pressrelease.htm?refid=Ref1561 |title=Digital object identifier (DOI) becomes an ISO standard |publisher=iso.org |date=10 May 2012 |accessdate=10 May 2012}}&lt;/ref&gt; The Draft International Standard ISO/DIS 26324, Information and documentation – Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,&lt;ref&gt;{{cite web|url=https://doi.org/about_the_doi.html#TC46 |title=about_the_doi.html DOI Standards and Specifications |publisher=Doi.org |date=28 June 2010 |accessdate=7 August 2010}}&lt;/ref&gt; which was approved by 100% of those voting in a ballot closing on 15 November 2010.&lt;ref&gt;{{cite web|url=https://doi.org/about_the_doi.html#TC46 |title=Overviews &amp; Standards – Standards and Specifications: 1. ISO TC46/SC9 Standards |publisher=Doi.org |date=18 November 2010 |accessdate=3 July 2011}}&lt;/ref&gt; The final standard was published on 23 April 2012.&lt;ref&gt;{{cite web|url=http://www.iso.org/iso/catalogue_detail?csnumber=43506 |title=ISO 26324:2012 |publisher=iso.org |date=23 April 2012 |accessdate=10 May 2012}}&lt;/ref&gt;

DOI is a registered URI under the [[info URI scheme]] specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.&lt;ref&gt;{{cite web|url=http://info-uri.info/registry/docs/misc/faq.html#which_namespaces |title=About "info" URIs – Frequently Asked Questions |publisher=Info-uri.info |accessdate=7 August 2010}}&lt;/ref&gt;

The DOI syntax is a [[NISO]] standard, first standardised in 2000, ANSI/NISO Z39.84{{hyphen}}2005 Syntax for the Digital Object Identifier.&lt;ref&gt;{{cite web|url=http://www.techstreet.com/standards/niso-z39-84-2005-r2010?product_id=1262088 |title=ANSI/NISO Z39.84{{hyphen}}2000 Syntax for the Digital Object Identifier |publisher=Techstreet.com |accessdate=7 August 2010}}&lt;/ref&gt;

==See also==
{{columns-list|3|
* [[Bibcode]]
* [[Digital identity]]
* [[Metadata standards]]
* [[Object identifier]]
* [[ORCID]]
* [[PubMed#PubMed identifier|PMID]]
* [[Publisher Item Identifier]] (PII)
* [[Permalink]]
* [[Scientific literature]]
* [[Universally Unique Identifier]] (UUID)
}}

==Notes==
{{notelist-ua}}

==References==
{{Reflist|colwidth=30em}}

==External links==
{{Wikidata property|P356}}
* {{official website|https://doi.org/}}
* [http://shortdoi.org Short DOI] – DOI Foundation service for converting long DOIs to shorter equivalents
* [https://doi.org/factsheets/DOIIdentifierSpecs.html Factsheet: DOI System and Internet Identifier Specifications]
* [http://search.crossref.org/ CrossRef DOI lookup]

{{Audiovisual works|state=uncollapsed}}
{{ISO standards}}
{{Authority control}}

{{DEFAULTSORT:Digital Object Identifier}}
[[Category:Academic publishing]]
[[Category:Electronic documents]]
[[Category:Identifiers]]
[[Category:Index (publishing)]]</text>
      <sha1>0z6p5zss5o63hfzd6lsol85zyx3a2r2</sha1>
    </revision>
  </page>
  <page>
    <title>Novell Vibe</title>
    <ns>0</ns>
    <id>25023858</id>
    <revision>
      <id>742795326</id>
      <parentid>706957544</parentid>
      <timestamp>2016-10-05T21:01:01Z</timestamp>
      <contributor>
        <username>Reinvolve</username>
        <id>22870028</id>
      </contributor>
      <minor />
      <comment>caps</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5504" xml:space="preserve">{{multiple issues |{{Notability|Products|date=April 2012}}
{{refimprove|date=April 2012}}
{{advert|date=November 2012}}
}}

{{Infobox software
|name                       = Novell Vibe
|logo                       = 
|screenshot                 = 
|caption                    = 
|collapsible                = 
|author                     = 
|developer                  = [[Novell]]
|released                   = {{Start date|2008|06|25}} &lt;!-- {{Start date|YYYY|MM|DD}} --&gt;
|discontinued               = 
|latest release version     = 
|latest release date        = &lt;!-- {{Start date and age|YYYY|MM|DD}} --&gt;
|latest preview version     = 
|latest preview date        = &lt;!-- {{Start date and age|YYYY|MM|DD}} --&gt;
|frequently updated         = 
|programming language       = 
|operating system           = 
|platform                   =
|size                       = 
|language                   = 
|status                     = 
|genre                      = [[Web application]]
|license                    = Proprietary
|website                    = [http://www.novell.com/products/vibe/  Novell Vibe]
}}

'''Novell Vibe''' is a web-based team collaboration platform developed by [[Novell]], and was initially released by Novell in June 2008 under the name of Novell Teaming. Novell Vibe is a collaboration platform that can serve as a knowledge repository, [[document management system]], project collaboration hub, process automation machine, corporate [[intranet]] or [[extranet]]. Users can upload, manage, comment on, and edit content in a secure manner. Supported content includes documents, calendars, discussion forums, wikis, blogs, tasks, and more.

Document management functionality allows for document versions, approvals, and document life cycle tracking. Users can download and modify pre-built custom web pages and workflows free of charge from the Vibe Resource Library.&lt;ref&gt;{{cite web|url=http://www.novell.com/products/vibe/resource-library/ |title=Novell Vibe Resource Library |publisher=Novell.com |date= |accessdate=2012-04-12}}&lt;/ref&gt;

==History==
Novell Vibe is the result of a merging of two products in November 2010: Novell Teaming and Novell Pulse.&lt;ref&gt;http://www.novell.com/communities/node/12257/great-vibes-paths-merging-novell-teaming-and-novell-pulse&lt;/ref&gt;

Novell Teaming began as SiteScape Forum. When Novell Acquired SiteScape in 2008, the name was changed to Novell Teaming.{{citation needed|date=August 2012}}

Created in 2009, Novell Pulse was a communication tool based on the [[Google Wave Federation Protocol]].&lt;ref&gt;http://www.scala-lang.org/node/6618&lt;/ref&gt;

===Server===
Any combination of Linux and Windows servers can run the Vibe application. Furthermore, MySQL, MS SQL, and Oracle databases are supported.

===End-User operating systems===
Windows, Linux, Mac, iOS or Android

===Browsers===
Firefox, Internet Explorer, Safari, or Chrome

===Mobile===
Any mobile device that has a browser can access the Vibe site. Native iOS &lt;ref&gt;https://itunes.apple.com/us/app/novell-vibe/id476653054?mt=8&lt;/ref&gt; and Android &lt;ref&gt;https://play.google.com/store/apps/details?id=com.novell.vibe.android&lt;/ref&gt; apps are available for free download in the app stores.

===Microsoft Office integration===
Word, PowerPoint, and Excel (versions 2013,2010 and 2007) on the Windows operating system are supported.

==Interoperability==
Vibe can be used in conjunction with various other software products, such as [[Novell Access Manager]], [[Novell GroupWise]], [[Skype]], and [[YouTube]]. Novell Vibe integrates with an LDAP directory for authentication.

==Extendability==
Vibe administrators can extend the Vibe software by creating [[software extensions]], remote applications, or [[JAR (file format)]] files that enhance the power and usefulness of the Vibe software to create a custom experience for users.

Software extensions enable third-party developers to create abilities which extend an application. Vibe administrators or Vibe developers can create custom extensions (add-ons) to enhance Vibe. For example, you might have an extension that enables Flash video support in Vibe.

===Remote applications===
A remote application is a program that runs on a remote server and delivers data for use on the Novell Vibe site (such as data from a remote database). For example, Vibe administrators or Vibe developers could set up a remote application for Twitter that displays all of a user's Twitter entries in Vibe.

Unlike creating an extension for Vibe, creating a remote application does not modify the Vibe software.

== Open-source solutions ==
Not all of these projects implement all of the features Novell Vibe has to offer as well as Vibe is missing some features these products have:

* [[Kablink|Kablink Vibe]] - an open source version of Novell Vibe
* [[Redmine]]/[[ChiliProject]]
* [[trac]]
* [[Feng Office Community Edition]]
* [[Open Workbench]]
* [[OpenProj]]

''see also'': [[:Category:Free project management software|Wikipedia category for free project management software]]

==References==
{{Reflist}}

==External links==
* [http://www.novell.com/products/vibe/resource-library/  Novell Vibe product page]
* [http://kabtim.ru//  Kablink-Vibe]
{{Novell}}

[[Category:Novell software]]
[[Category:Proprietary wiki software]]
[[Category:Electronic documents]]
[[Category:Instant messaging]]
[[Category:Online chat]]
[[Category:Social information processing]]
[[Category:Groupware]]
[[Category:Internet Protocol based network software]]
[[Category:Blog software]]</text>
      <sha1>feld695rwhwtr8chf1lh2bptmf231v4</sha1>
    </revision>
  </page>
  <page>
    <title>DataCite</title>
    <ns>0</ns>
    <id>26964279</id>
    <revision>
      <id>758768395</id>
      <parentid>753456648</parentid>
      <timestamp>2017-01-07T13:41:27Z</timestamp>
      <contributor>
        <username>Egon Willighagen</username>
        <id>17845450</id>
      </contributor>
      <comment>/* Background */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7006" xml:space="preserve">[[File:DataCite logo.png|thumb|DataCite's logo.]]

'''DataCite''' is an international [[not-for-profit]] organization which aims to improve [[data citation]] in order to:
*establish easier access to research data on the Internet
*increase acceptance of research data as legitimate, citable contributions to the scholarly record
*support data archiving that will permit results to be verified and re-purposed for future study.&lt;ref&gt;{{cite web|publisher=DataCite|title=What is DataCite?|url=http://www.datacite.org/whatisdatacite|accessdate=17 March 2014}}&lt;/ref&gt; 

==Background==
In August 2009 a paper was published laying out an approach for a global registration agency for research data.&lt;ref&gt;{{cite web|title=Approach for a joint global registration agency for research data|doi=10.3233/ISU-2009-0595}}&lt;!--| accessdate=2011-05-23--&gt;&lt;/ref&gt; DataCite was subsequently founded in London on 1 December 2009&lt;ref&gt;{{cite journal|last1=Neumann|first1=Janna|last2=Brase|first2=Jan|title=DataCite and DOI names for research data|journal=Journal of Computer-Aided Molecular Design|date=20 July 2014|volume=28|issue=10|pages=1035–1041|doi=10.1007/s10822-014-9776-5}}&lt;/ref&gt; by organisations from 6 countries: the [[British Library]]; the Technical Information Center of Denmark (DTIC); the [[TU Delft]] Library from the Netherlands; the National Research Council’s [[Canada Institute for Scientific and Technical Information]] (NRC-CISTI); the [[California Digital Library]] (University of California Curation Center);&lt;ref&gt;{{cite web|url=http://webarchives.cdlib.org/sw1st7gf68/http://www.universityofcalifornia.edu/news/article/23055 |title=University of California becomes founding member of Datacite |accessdate=2014-05-05 }}{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}&lt;/ref&gt; [[Purdue University]] (USA);&lt;ref&gt;{{cite web |url= http://blogs.lib.purdue.edu/news/2010/02/16/purdue-libraries-a-founding-member-of-international-cooperative-to-advance-research/ | title=Purdue Libraries becomes founding member of Datacite | accessdate=2010-04-15}}&lt;/ref&gt; and the German National Library of Science and Technology (TIB).&lt;ref&gt;{{cite web|url=http://www.tib-hannover.de/en/the-tib/news/news/id/133/ | title=TIB becomes founding member of Datacite | accessdate=2010-04-15}}&lt;/ref&gt;

After the founding of DataCite, leading research libraries and information centres converged for the first official members’ convention in Paris on 5 February 2010. The inclusion of five further members was approved in the office of the International Council for Science (ICSU): Australian National Data Service (ANDS);&lt;ref&gt;{{cite web|url=http://ands.org.au/guides/doi.html | title=ANDS joins Datacite | accessdate=2010-04-15}}&lt;/ref&gt; Deutsche Zentralbibliothek für Medizin (ZB MED); GESIS - Leibniz-Institut für Sozialwissenschaften; French Institute for Scientific and Technical Information (INIST);&lt;ref&gt;{{cite web|url=http://www.inist.fr/spip.php?article66 |title=Inist join Datacite consortium |accessdate=2010-04-15 |deadurl=yes |archiveurl=https://web.archive.org/web/20100309010702/http://www.inist.fr:80/spip.php?article66 |archivedate=2010-03-09 |df= }}&lt;/ref&gt; and Eidgenössische Technische Hochschule (ETH) Zürich.

== Technical ==

The primary means of establishing easier access to research data is by DataCite members assigning persistent identifiers, such as [[digital object identifier]]s (DOIs), to data sets. Although currently leveraging the well-established DOI infrastructure, DataCite takes an open approach to identifiers, and considers other systems and services that help forward its objectives.&lt;ref&gt;{{cite web|publisher=DataCite |title=What do we do? |url=http://www.datacite.org/whatdowedo |accessdate=17 March 2014 |deadurl=yes |archiveurl=https://web.archive.org/web/20140317195125/http://www.datacite.org/whatdowedo |archivedate=17 March 2014 |df= }}&lt;/ref&gt; 

DataCite's recommended format for a data citation is: 
*Creator (PublicationYear): Title. Publisher. Identifier
OR
*Creator (PublicationYear): Title. Version. Publisher. ResourceType. Identifier

DataCite recommends that DOI names are displayed as linkable, permanent URLs.&lt;ref&gt;{{cite web|publisher=DataCite|title=Why cite data?|url=http://www.datacite.org/whycitedata|accessdate=17 March 2014}}&lt;/ref&gt;

Third-party tools allow the migration of content to and from other services such as ODIN, for [[ORCID]]&lt;ref name="ODIN"&gt;{{cite web |url=http://odin-project.eu/2013/05/13/new-orcid-integrated-data-citation-tool/|title=New ORCID-integrated data citation tool |last=Thorisson |first=Gudmundur |date=2013-05-13 |publisher=ODIN Project |accessdate=7 May 2014}}&lt;/ref&gt;

==Members==
* Australia:
** [[Australian National Data Service]] - ANDS
* Canada:
** [[National Research Council (Canada)|National Research Council Canada]] - NRC-CNRC
* China:
** [[Beijing Genomics Institute]] - BGI 
* Denmark:
** Technical Information Center of Denmark (DTU Library)
* Estonia:
** [[University of Tartu]] (/UT)
* France:
** [[Institut de l'information scientifique et technique]] - INIST-CNRS
* Germany:
** [[German National Library of Economics]] - ZBW
** [[German National Library of Medicine]] - ZB MED
** [[German National Library of Science and Technology]] - TIB
** Leibniz Institute for the Social Sciences - GESIS
** Göttingen State and University Library - [[Göttingen State and University Library|SUB]]
** Gesellschaft für wissenschaftliche Datenverarbeitung mbH Göttingen - GWDG 
* Hungary:
** Library and Information Centre, Hungarian Academy of Sciences - MTA KIK
* International:
** [[World Data System|ICSU World Data System]] - ICSU-WDS 
* Italy:
** [[Conference of Italian University Rectors]] - CRUI
* Japan:
** Japan Link Center - JaLC
* Netherlands:
** [[TU Delft]] Library
* Norway:
** [[BIBSYS]]
* Republic of Korea:
** [[Korea Institute of Science and Technology Information]] - KISTI 
* South Africa:
** South African Environmental Observation Network - SAEON
* Sweden:
** Swedish National Data Service - SND
* Switzerland:
** [[CERN]] - European Organization for Nuclear Research
** [[Swiss Federal Institute of Technology Zurich]] - ETH
* Thailand:
** National Research Council of Thailand - NRCT
* United Kingdom:
** [[The British Library]] - BL
** [[Digital Curation Centre]] 
* United States:
** [[California Digital Library]] - CDL
** [[OSTI|Office of Scientific and Technical Information, US Department of Energy]] - OSTI
** [[Purdue University|Purdue University Libraries]] - PUL
** [[Inter-university Consortium for Political and Social Research]] - ICPSR 
** [[Harvard University Library]] 
** [[Institute of Electrical and Electronics Engineers]] - IEEE 

==References==
&lt;references /&gt;

== External links ==
* [http://www.datacite.org/ Official '''DataCite''' website]

[[Category:Academic publishing]]
[[Category:Data publishing]]
[[Category:Electronic documents]]
[[Category:Identifiers]]
[[Category:Index (publishing)]]
[[Category:Non-profit organizations]]
[[Category:Non-profit technology]]</text>
      <sha1>13dvg4pw8smr4skoh1vpqc59ur1je6z</sha1>
    </revision>
  </page>
  <page>
    <title>Apache Wave</title>
    <ns>0</ns>
    <id>22992426</id>
    <revision>
      <id>759216965</id>
      <parentid>755996669</parentid>
      <timestamp>2017-01-09T22:05:30Z</timestamp>
      <contributor>
        <username>Sheridan</username>
        <id>33042</id>
      </contributor>
      <comment>/* End of development of original Google Wave under Google in 2010 */ took out superfluous words of title</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="27456" xml:space="preserve">{{Infobox software
|name                       = Apache Wave
|logo                       = Apache Wave logo.png
|screenshot                 = Google Wave.png
|caption                    = Google Wave, the previous incarnation of Apache Wave
|collapsible                = 
|author                     = [[Google]]
|developer                  = [[Apache Software Foundation]], Google
|released                   = {{start date|2009|5|27}}
|latest release version     =
|latest release date        = &lt;!-- {{Start date and age|YYYY|MM|DD}} --&gt;
|latest preview version     =
|latest preview date        = &lt;!-- {{Start date and age|YYYY|MM|DD}} --&gt;
|frequently updated         =
|programming language       = [[Java (programming language)|Java]]&lt;ref&gt;{{cite web |url=http://www.lextrait.com/Vincent/implementations.html |title=The Programming Languages Beacon, v10.0 |first=Vincent |last =Lextrait |date=January 2010 |accessdate=14 March 2010}}&lt;/ref&gt;
|operating system           =
|platform                   = [[Web application]]
|size                       =
|language                   =
|status                     =
|genre                      = [[Collaborative real-time editor]]
|license                    = [[Apache License]]
|website                    = {{URL|incubator.apache.org/wave/}}
|repo                       = {{URL|https://git-wip-us.apache.org/repos/asf/incubator-wave.git}}
}}
'''Apache Wave''' is a software framework for [[Collaborative real-time editor|real-time collaborative editing]] online. [[Google]] originally developed it as '''Google Wave'''.&lt;ref&gt;{{cite web|url=http://wave.google.com/about.html
|title=Google Wave Overview|author=Google Inc.
|quote=[A] new web application for real-time communication and collaboration.
|year=2009|accessdate=May 2010| archiveurl= https://web.archive.org/web/20100427183005/http://wave.google.com/about.html| archivedate= 27 April 2010 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;
It was announced at the [[Google I/O]] conference on May 27, 2009.&lt;ref&gt;[[TechCrunch]] (May 28, 2009):
[http://www.techcrunch.com/2009/05/28/google-wave-drips-with-ambition-can-it-fulfill-googles-grand-web-vision/ Google Wave Drips With Ambition.  A New Communication Platform For A New Web.]&lt;/ref&gt;&lt;ref name="iokeynote"&gt;{{cite web
|url=https://www.youtube.com/watch?v=v_UyVmITiYQ
|title=I/O Conference Google Wave Keynote|author=Google Inc.}}&lt;/ref&gt;

Wave is a [[web application|web-based]] [[computing platform]] and [[communications protocol]] designed to merge key features of [[Media (communication)|communications media]] such as [[email]], [[instant messaging]], [[wiki]]s, and [[Social networking service|social networking]].&lt;ref name="aboutgw"&gt;{{cite web
|url=http://wave.google.com/help/wave/about.html|title=About Google Wave|author=Google Inc.}}&lt;/ref&gt; Communications using the system can be [[Synchronization|synchronous]] or [[Asynchronous communication#Electronically mediated communication|asynchronous]]. Software extensions provide contextual [[spell checker|spelling and grammar checking]], [[machine translation|automated language translation]]&lt;ref name="iokeynote" /&gt; and other features.&lt;ref name="gwdevblog"&gt;{{cite web|url=http://googlewavedev.blogspot.com/2009/05/introducing-google-wave-apis-what-can.html|title=Google Wave Developer Blog|publisher=Google}}&lt;/ref&gt;

Initially released only to developers, a preview release of Google Wave was extended to 100,000 users in September 2009, each allowed to invite additional users. Google accepted most requests submitted starting November 29, 2009, soon after the September extended release of the technical preview. On May 19, 2010, it was released to the general public.&lt;ref&gt;Shankland, Stephen. (2010-05-19) [http://news.cnet.com/8301-30685_3-20005394-264.html Google Wave: Now open to the public | Deep Tech – CNET News]. News.cnet.com. Retrieved on 2010-12-14.&lt;/ref&gt;

On August 4, 2010, Google announced the suspension of stand-alone Wave development and the intent of maintaining the web site at least for the remainder of the year,&lt;ref&gt;[http://googleblog.blogspot.com/2010/08/update-on-google-wave.html Official Google Blog: Update on Google Wave]. Googleblog.blogspot.com (2010-04-08). Retrieved on 2010-12-14.&lt;/ref&gt; and on November 22, 2011, announced that existing Waves would become read-only in January 2012 and all Waves would be deleted in April 2012.&lt;ref&gt;{{cite web|url=http://googleblog.blogspot.com/2011/11/more-spring-cleaning-out-of-season.html |title=Official Blog: More spring cleaning out of season |publisher=Googleblog.blogspot.com |date=2011-11-22 |accessdate=2013-06-15}}&lt;/ref&gt; Development was handed over to the [[Apache Software Foundation]] which started to develop a server-based product called '''Wave in a Box'''.&lt;ref&gt;Meyer, David. (2010-09-03) [http://www.zdnet.co.uk/news/application-development/2010/09/03/google-puts-open-source-wave-in-a-box-40089999/ Google puts open-source Wave in a 'box' | Application Development | ZDNet UK]. Zdnet.co.uk. Retrieved on 2010-12-14.&lt;/ref&gt;&lt;ref&gt;[http://www.idg.se/2.1085/1.355483/google-wave-inte-ute-ur-leken Google Wave inte ute ur leken]. IDG.se. Retrieved on 2010-12-14.&lt;/ref&gt;&lt;ref&gt;Murphy, David. (1970-01-01) [http://www.pcmag.com/article2/0,2817,2368730,00.asp Google Spins Wave Into 'Wave in a Box' for Third-Party Use | News &amp; Opinion]. PCMag.com. Retrieved on 2010-12-14.&lt;/ref&gt;

==History==
[[File:Googlewave.svg|thumb|upright|The original logo while owned by Google]]

===Origin of name===
The science fiction television series ''[[Firefly (TV series)|Firefly]]'' provided the inspiration for the project's name.&lt;ref name=itnewsau&gt;{{cite news
|first=Nate
|last=Cochrane
|url=http://www.itnews.com.au/News/104396,opinion-googles-wave-drowns-the-bling-in-microsofts-bing.aspx
|title=Opinion: Google's wave drowns the bling in Microsoft's Bing
|agency=IT News Australia
|date=2009-05-29
|accessdate=2009-06-03
| archiveurl= https://web.archive.org/web/20090603041903/http://www.itnews.com.au/News/104396,opinion-googles-wave-drowns-the-bling-in-microsofts-bing.aspx| archivedate= 3 June 2009 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt; In the series, a ''wave'' is an electronic communication, often consisting of a [[video call]] or video message.&lt;ref name=itnewsau/&gt;  During the developer preview, a number of references were made to the series, such as [[Lars Rasmussen (Software Developer)|Lars Rasmussen]] replying to a message with "shiny", a word used in the series to mean ''cool'' or ''good'', and the crash message of Wave being a popular quotation from the series: "Curse your sudden but inevitable betrayal!"&lt;ref name="iokeynote" /&gt;&lt;ref&gt;Originally said by [[List of characters in the Firefly universe#Hoban Washburne|Wash]] at 6:36, in ''[[Serenity (Firefly episode)|Serenity]]''; [[Firefly (TV series)|Firefly]]: The Complete Series (Blu-ray), 2008, 20th Century Fox.&lt;/ref&gt; Another common error message, "Everything's shiny, Cap'n. Not to fret!" is a quote from [[Kaylee Frye]] in the 2005 motion-picture ''Firefly'' continuation, ''[[Serenity (film)|Serenity]]'', and it is matched with a sign declaring that "This wave is experiencing some turbulence and might explode. If you don't want to explode..." which is another reference to the opening of the film.

During an event in [[Amsterdam]], [[Netherlands]],&lt;ref name="nextweb"&gt;{{cite news | first = Ralf | last = Rottmann | url = http://thenextweb.com/appetite/2009/10/30/breaking-google-wave-opened-federation-today-host/ | title = Google Wave to be opened for federation today! | agency = The Next Web | date = October 30, 2009}}&lt;/ref&gt; it became apparent that the 60-strong team that was currently working on Wave in [[Sydney, Australia]] use  [[Joss Whedon]]-related references to describe, among others, the sandbox version of Wave called ''[[Dollhouse (TV series)|Dollhouse]]'' after the TV-series by ''Firefly'' producer Joss Whedon, which was aired on Fox in the U.S. The development of external extensions is codenamed "Serenity", after the spaceship used in ''Firefly'' and ''Serenity''.

===Open source===
Google released most of the source code as [[open source software]],&lt;ref name="iokeynote"/&gt; allowing the public to develop its features through extensions.&lt;ref name="iokeynote" /&gt;  Google allowed third parties to build their own Wave services (be it private or commercial) because it wanted the [[Google Wave Federation Protocol|Wave protocol]] to replace the [[e-mail]] protocol.&lt;ref name="iokeynote"/&gt;&lt;ref name="gwarchitecture" /&gt;&lt;ref name="wpcsmodel" /&gt; Initially, Google was the only Wave service provider, but it was hoped that other service providers would launch their own Wave services, possibly designing their own unique web-based clients as is common with many email service providers.  The possibility also existed for native Wave clients to be made, as demonstrated with their [[command-line interface|CLI]]-based console client.&lt;ref name="osreleasenext"&gt;{{cite web|url=https://groups.google.com/group/wave-protocol/browse_thread/thread/618ff4e9ef477e80?pli=1|title=Google Wave Federation Protocol and Open Source Updates|publisher=Google}}&lt;/ref&gt;

Google released initial open-source components of Wave:&lt;ref name="osrelease1"&gt;{{cite web|url=http://googlewavedev.blogspot.com/2009/07/google-wave-federation-protocol-and.html|title=Google Wave Federation Protocol and Open Source Updates|publisher=Google}}&lt;/ref&gt;
# the [[operational transformation]] (OT) code,
# the underlying wave model, and
# a basic client/server prototype that uses the wave protocol

In addition, Google provided some detail about later phases of the open-source release:&lt;ref name="osreleasenext" /&gt;
# wave model code that is a simplified version of Google's production code and is tied to the OT code; this code will evolve into the shared code base that Google will use and expects that others will too
# a testing and verification suite for people who want to do their own implementation (for example, for porting the code to other languages)

===Reception===
{{Wikinews|Google to discontinue social networking application Google Wave}}
During the initial launch of Google Wave, invitations were widely sought by users and were sold on auction sites.&lt;ref&gt;[http://mashable.com/2009/09/30/google-wave-invite/ Google Wave Invite Selling for $70 on eBay]&lt;/ref&gt;&lt;!--  However, people were confused as to how to use it.&lt;ref&gt;[http://www.csmonitor.com/Innovation/Horizons/2009/1124/so-youve-got-google-wave-now-what Christian Science Monitor: So you've got Google wave, now what?]&lt;/ref&gt; Google Wave was called an "over-hyped disappointment for the first generation of users"&lt;ref&gt;[http://www.linux-mag.com/id/7653 Linux Mag 2009 Review]&lt;/ref&gt; with "dismal usability"&lt;ref&gt;[http://themilwaukeeseo.com/2009/12/14/the-google-wave-failure/ Google Wave Failure on Milwaukee SEO]&lt;/ref&gt; that "humans may not be able to comprehend."&lt;ref name="EngagetWave"&gt;[http://www.engadget.com/2009/10/27/google-wave-to-have-its-own-app-store/ Google Wave to get its own App Store (Engadget)]&lt;/ref&gt; --&gt;
Those who received invitations and decided to test Google Wave could not communicate with their contacts on their regular email accounts. The initial spread of Wave was very restricted.

===End of development of ''Google Wave''===
Google Wave initially received positive press coverage for its design&lt;ref&gt;[http://news.bbc.co.uk/1/hi/technology/8282687.stm B.B.C. report introducing Google Wave in September 2009]&lt;/ref&gt; and potential uses.&lt;ref name="EngagetWave"&gt;[http://www.engadget.com/2009/10/27/google-wave-to-have-its-own-app-store/ Google Wave to get its own App Store (Engadget)]&lt;/ref&gt;&lt;ref&gt;[http://asia.cnet.com/blogs/tokyo-shift/post.htm?id=63015591 CNET Predictions for 2010]&lt;/ref&gt; On August 4, 2010, Google announced Wave would no longer be developed as a stand-alone product due to a lack of interest.&lt;ref name="ZDNet on GW's death"&gt;[http://www.zdnet.com/blog/google/how-will-google-wave-be-reincarnated/2344 ZDNet on GW's death]&lt;/ref&gt; Google's statement surprised many in the industry and user community.

Google later clarified the Wave service would be available until [[Google Docs]] was capable of accessing saved waves.&lt;ref&gt;{{cite web|url=http://www.google.com/support/wave/bin/answer.py?answer=1083134 |title=Status of Google Wave - Google Help |publisher=Google.com |date= |accessdate=2013-06-15}}&lt;/ref&gt;

Response to the news of the end of development came from Wave users in the form of a website.&lt;ref&gt;[http://www.webpronews.com/topnews/2010/08/09/save-google-wave-site-forms '"Save Google Wave" Site Forms']&lt;/ref&gt; Since their announcement in early August, the website has recorded over 49,000 supporter registrations urging Google Wave's continuation.&lt;ref&gt;[http://www.savegooglewave.com Save Google Wave!]. Retrieved on 2011-05-14.&lt;/ref&gt;

In retrospect, the lack of success of Google Wave was attributed among other things to its complicated user interface resulting in a product that merged features of email, instant messengers and wikis but ultimately failed to do anything significantly better than the existing solutions.&lt;ref&gt;[http://arstechnica.com/software/news/2010/08/google-wave-why-we-didnt-use-it.ars Google Wave: why we didn't use it], [[Ars Technica]]&lt;/ref&gt;

Chris Dawson of online technology magazine [[Zdnet]] discussed inconsistencies in the reasoning of Google in deciding to end support for Wave,&lt;ref name="ZDNet on GW's death"/&gt; mentioning its "deep involvement" in developing social media networks, to which many of Wave's capabilities are ideally suited. Perhaps Google Wave was ended to clear the stage for their new social network [[Google+]] that tried to compete with Facebook but has not gained the reach as a similar comprehensive social networking site.&lt;ref&gt;[http://www.utalkmarketing.com/pages/Article.aspx?ArticleID=21768&amp;Title=Can_Google+_really_challenge_Facebook_and_be_an_asset_to_brands "Can Google+ really challenge Facebook and be an asset to brands?" utalkmarketing.com]&lt;/ref&gt;

===Apache Wave===
Google Wave was accepted by the [[Apache Software Foundation]]'s Incubator program under the project name Apache Wave. The Google Wave Developer blog was updated with news of the change on December 6, 2010.&lt;ref&gt;North, Alex. (2010-12-06) [http://googlewavedev.blogspot.com/2010/12/introducing-apache-wave.html Google Wave Developer Blog: Introducing Apache Wave]. Googlewavedev.blogspot.com. Retrieved on 2010-12-14.&lt;/ref&gt; A Wave Proposal page with details on the project's goals was created on the Apache Foundation's Incubator Wiki.&lt;ref&gt;[http://wiki.apache.org/incubator/WaveProposal WaveProposal – Incubator Wiki]. Wiki.apache.org (2010-11-24). Retrieved on 2010-12-14.&lt;/ref&gt;

====Wave in a Box====
[[File:Wave in a Box logo.png|thumb|75px|The logo for Wave in a Box]]
Wave in a Box is the current server implementation of Apache Wave.  Currently, there are not any demo servers available.&lt;ref name=demo_servers&gt;{{cite web|title=Wave in a Box demo servers|url=http://incubator.apache.org/wave/demo-servers.html|publisher=Apache Software Foundation|accessdate=10 October 2012}}&lt;/ref&gt;

==Features==
Google Wave was a new [[Internet]] [[communications]] platform. It was written in [[Java (programming language)|Java]] using [[OpenJDK]] and its web interface used the [[Google Web Toolkit]]. Google Wave works like previous messaging systems such as [[email]] and [[Usenet]], but instead of sending a message along with its entire thread of previous messages, or requiring all responses to be stored in each user's inbox for context, message documents (referred to as ''waves'') that contain complete threads of multimedia messages (blips) are perpetually stored on a central server. Waves are shared with collaborators who can be added or removed from the wave at any point during a wave's existence.

Waves, described by Google as "''equal parts conversation and document''", are hosted [[XML]] documents that allow seamless and low latency concurrent modifications.&lt;ref name="ot"&gt;[http://www.waveprotocol.org/whitepapers/operational-transform Google Wave Operational Transformation – Google Wave Federation Protocol]. Waveprotocol.org. Retrieved on 2010-12-14.&lt;/ref&gt; Any participant of a wave can reply anywhere within the message, edit any part of the wave, and add participants at any point in the process. Each edit/reply is a blip and users can reply to individual blips within waves. Recipients are notified of changes/replies in all waves in which they are active and, upon opening a wave, may review those changes in chronological order. In addition, waves are live. All replies/edits are visible in real-time, letter-by-letter, as they are typed by the other collaborators. Multiple participants may edit a single wave simultaneously in Google Wave. Thus, waves can function not only as e-mails and [[threaded discussion|threaded conversations]] but also as an [[instant messaging]] service when many participants are online at the same time. A wave may repeatedly shift roles between e-mail and instant messaging depending on the number of users editing it concurrently. The ability to show messages as they are typed can be disabled, similar to conventional instant messaging.&lt;ref name="aboutgw"/&gt;

The ability to modify a wave at any location lets users create collaborative documents, [[collaborative editing|edited]] in a manner akin to [[wiki]]s. Waves can easily link to other waves. In many respects, it is a more advanced forum.&lt;ref&gt;[http://variableghz.com/2009/10/google-wave-review/ Google Wave Review]. VariableGHz (2009-10-13). Retrieved on 2010-12-14.&lt;/ref&gt; It can be read and known to exist by only one person, or by two or more and can also be public, available for reading ''and'' writing to everyone on the Wave.

The history of each wave is stored within it. Collaborators may use a playback feature to observe the order in which it was edited, blips that were added, and who was responsible for what in the wave.&lt;ref name="aboutgw"/&gt;&lt;ref name="gwdevblog"/&gt; The history may also be searched by a user to view and/or modify specific changes, such as specific kinds of changes or messages from a single user.&lt;ref name="iokeynote" /&gt;

==Extension programming interface==
{{anchor|Google Wave extensions}}
Google Wave is extensible through an [[application programming interface]] (API). It provides extensions in the form of ''Gadgets'' and ''Robots'', and is embeddable by dropping interactive windows into a given wave on external sites, such as [[blog]] sites.&lt;ref name="iokeynote" /&gt;&lt;ref name="codepage" /&gt;

The last version of robots API is 2.0.&lt;ref name="robotsapiv2"&gt;{{cite web|url=http://googlewavedev.blogspot.com/2010/03/introducing-robots-api-v2-rise-of.html|title=Introducing Robots API v2: The Rise of Active Robots|publisher=Google}}&lt;/ref&gt;

Google Wave also supports extension installers, which bundle back-end elements (robots and gadgets) and front-end user interface elements into an integrated package. Users may install extensions directly within the Wave client using an extension installer.

===Extensions===
Google Wave extensions are [[Plug-in (computing)|add-ins]] that may be installed on Google Wave to enhance its functionality. They may be [[Internet bot]]s (robots) to automate common tasks, or gadgets to extend or change user interaction features, e.g., posting blips on [[microblog]] feeds or providing RSVP recording mechanisms.&lt;ref name="iokeynote" /&gt;&lt;ref name="aboutgw" /&gt;&lt;ref name="codepage"&gt;{{cite web|url=https://code.google.com/apis/wave/|title=Google Wave API – Google Code|publisher=Google}}&lt;/ref&gt;

Over 150 Google Wave extensions have been developed either in the form of Gadgets or Robots.&lt;ref&gt;[http://wave-samples-gallery.appspot.com/ Google Wave Samples Gallery]. Wave-samples-gallery.appspot.com. Retrieved on 2010-12-14.&lt;/ref&gt;

====Robots====
A robot is an automated participant on a wave. It can read the contents of a wave in which it participates, modify its contents, add or remove participants, and create new blips or new waves. Robots perform actions in response to events. For example, a robot might publish the contents of a wave to a public [[blog]] site and update the wave with user comments.

Robots may be added as participants to the Wave itself. In theory, a robot can be added anywhere a human participant can be involved.

====Gadgets====
Gadget extensions are applications that run within the wave, and to which all participants have access. Robots and Gadgets can be used together, but they generally serve different purposes. A gadget is an application users could participate with, many of which are built on Google’s [[OpenSocial]] platform. A good comparison would be iGoogle gadgets or Facebook applications.

The gadget is triggered based on the user action. They can be best described as applications installed on a mobile phone. For example, a wave might include a [[sudoku]] gadget that lets the wave participants compete to see who can solve the puzzle first.

Gadgets may be added to individual waves and all the participants share and interact with the gadget.

==Federation protocol==
{{Main article|Google Wave Federation Protocol}}
Google Wave provides [[Federation (information technology)|federation]] using an extension of [[XMPP|Extensible Messaging and Presence Protocol]] (XMPP), the [[open standard|open]] [[Google Wave Federation Protocol|Wave Federation Protocol]]. Being an open protocol, anyone can use it to build a custom Wave system and become a wave provider.&lt;ref&gt;{{cite web|url=http://www.waveprotocol.org/|title=Google Wave Federation Protocol|publisher=Google}}&lt;/ref&gt;  The use of an open protocol is intended to parallel the openness and ease of adoption of the [[e-mail]] protocol and, like e-mail, allow communication regardless of provider. Google hoped that waves would replace e-mail as the dominant form of Internet communication.&lt;ref name="iokeynote"/&gt;&lt;ref name="gwarchitecture"&gt;[http://www.waveprotocol.org/whitepapers/google-wave-architecture Google Wave Federation Architecture – Google Wave Federation Protocol]. Waveprotocol.org. Retrieved on 2010-12-14.&lt;/ref&gt;&lt;ref name="wpcsmodel"&gt;[http://www.waveprotocol.org/whitepapers/internal-client-server-protocol Google Wave Client-Server Protocol – Google Wave Federation Protocol]. Waveprotocol.org. Retrieved on 2010-12-14.&lt;/ref&gt;  In this way, Google intended to be only one of many wave providers&lt;ref name="iokeynote"/&gt;&lt;ref name="gwarchitecture" /&gt;&lt;ref name="wpcsmodel" /&gt;  and to also be used as a supplement to [[e-mail]], [[instant messaging]], [[FTP]], etc.

A key feature of the protocol is that waves are stored on the service provider's servers instead of being sent between users. Waves are federated; copies of waves and wavelets are distributed by the wave provider of the originating user to the providers of all other participants in a particular wave or wavelet so all participants have immediate access to up-to-date content. The originating wave server is responsible for hosting, processing, and concurrency control of waves.&lt;ref name="gwarchitecture" /&gt;&lt;ref name="wpcsmodel" /&gt;  The protocol allows private reply wavelets within parent waves, where other participants have no access or knowledge of them.&lt;ref name="gwarchitecture" /&gt;&lt;ref name="wpcsmodel" /&gt;

Security for the communications is provided via [[Transport Layer Security]] authentication, and encrypted connections and waves/wavelets are identified uniquely by a service provider's [[domain name]] and ID strings. User-data is not federated, that is, not shared with other wave providers.

===Adoption of Wave Protocol and Wave Federation Protocol===
Besides Apache Wave itself, there are other open-source variants of servers and clients with different percentage of Wave Federation and Wave Protocol support. Wave has been adopted for corporate applications by Novell for [[Novell Pulse]],&lt;ref&gt;[http://www.novell.com/products/pulse/ Novell Vibe cloud service]. Novell.com. Retrieved on 2010-12-14.&lt;/ref&gt; or by [[SAP AG|SAP]] for Cloudave,&lt;ref&gt;Elliott, Timo. (2009-10-19) [http://www.cloudave.com/link/sap-gravity-prototype-business-collaboration-using-google-wave SAP's Gravity Prototype: Business Collaboration Using Google Wave]. Cloudave.com. Retrieved on 2010-12-14.&lt;/ref&gt; and community projects such as PyOfWave or [[Kune (software)|Kune]].

====Compatible third-party servers====
The following servers are compatible with the Google Wave protocol:
* '''[[Kune (software)|Kune]]'''&lt;ref&gt;{{cite web|title=Kune Homepage|url=http://kune.ourproject.org|accessdate=22 April 2012}}&lt;/ref&gt; is a free/open source platform for social networking, collaborative work and web publishing, focusing on work groups and organizations rather than in individuals. It provides lists, tasks, documents, galleries, etc., while using waves underneath. It focuses on [[free culture movement|free culture]] and [[social movements]] needs.
* '''[[Novell Vibe]]''', formerly known as Novell Pulse&lt;ref&gt;[http://www.novell.com/promo/vibe.html Novell Vibe]. Novell.com (2009-12-31). Retrieved on 2010-12-14.&lt;/ref&gt;
* '''Rizzoma'''&lt;ref&gt;{{cite web|title=Rizzoma Homepage|url=http://rizzoma.com|accessdate=9 May 2012}}&lt;/ref&gt; is a platform for collaborative work in real time. It allows communication within a certain context permitting a chat to instantly become a document where topics of a discussion organized into branches of mind-map diagram and minor details are collapsed to avoid distraction. The user is able to sign in using a Google or Facebook account and choose whether your topics are private or public.
* '''[[SAP StreamWork]]''' is a collaboration decision making service.&lt;ref&gt;Williams, Alex. (2010-05-17) [http://www.readwriteweb.com/cloud/2010/05/sap-streamworks-integrates-wit.php SAP StreamWork Integrates With Google Wave – ReadWriteCloud]. Readwriteweb.com. Retrieved on 2010-12-14.&lt;/ref&gt;&lt;ref&gt;[http://www.sapstreamwork.com/how-it-works/ How It Works | SAP® StreamWork™]. Sapstreamwork.com. Retrieved on 2010-12-14.&lt;/ref&gt;

==See also==
{{Portal|Software}}
* [[Microsoft Sharepoint Workspace]]
* [[Real-time text]]
* [[Opera Unite]]

==References==
{{Reflist|colwidth=30em}}

==External links==
{{Commons category|Google Wave}}
* [http://incubator.apache.org/wave/ Apache Wave]
* [http://incubator.apache.org/wave/demo-servers.html Wave in a Box]
* [http://wave.google.com/ Google Wave]
* [https://code.google.com/apis/wave/ Google Wave API]
* [http://googlewavedev.blogspot.com/ Google Wave Developer Blog]
* [https://www.youtube.com/watch?v=v_UyVmITiYQ Full Video of the Developer Preview at Google IO on ]
* [https://www.youtube.com/watch?v=p6pgxLaDdQw Google Wave overview video]
* [http://www.waveprotocol.org/ Google Wave Federation Protocol]

{{Google Inc.}}
{{Apache}}

[[Category:Discontinued Google services|Wave]]
[[Category:Web applications]]
[[Category:Computing platforms]]
[[Category:Electronic documents]]
[[Category:Instant messaging]]
[[Category:Online chat]]
[[Category:Social information processing]]
[[Category:Groupware]]
[[Category:Wikis]]
[[Category:Internet protocols]]
[[Category:Internet Protocol based network software]]
[[Category:Self-organization]]
[[Category:Blogging]]
[[Category:Collaborative real-time editors]]
[[Category:2009 software]]
[[Category:2010 disestablishments]]
[[Category:Discontinued software]]
[[Category:Discontinued Google software]]
[[Category:Software using the Apache license]]
[[Category:Social networking services]]
[[Category:Apache Software Foundation|Wave]]</text>
      <sha1>1jmppoa6x7mlpfo6ztelmzjwcz5fi5g</sha1>
    </revision>
  </page>
  <page>
    <title>Lettrs</title>
    <ns>0</ns>
    <id>39130775</id>
    <revision>
      <id>760064908</id>
      <parentid>746352852</parentid>
      <timestamp>2017-01-14T19:37:54Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* History */HTTP&amp;rarr;HTTPS for [[Yahoo!]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11408" xml:space="preserve">{{lowercase title}}
{{Infobox software
| name         = lettrs
| logo         = [[File:Lettrs logo Square.jpg|200px]]
| released     = {{Start date|2012}}
| developer   = [[Drew Bartkiewicz]] (CEO)
| status             = Active
| operating system   = [[IOS|Apple iOS]], [[Android (operating system)|Android]]
| genre              = [[Social Networking]]
| website  =  {{URL|http://www.lettrs.com}}
}}

'''lettrs''' is a global [[mobile app|mobile application]] and [[social network]] that allows users to compose and send mobile messages privately or publicly.&lt;ref name=crunch&gt;{{cite web |url=https://www.crunchbase.com/organization/lett-rs |title= lettrs |author=&lt;!--Staff writer(s); no by-line.--&gt; |website=CrunchBase.com |publisher=AOL Inc |accessdate=26 January 2015}}&lt;/ref&gt;&lt;ref name=bweek&gt;{{cite web |url=http://www.businessweek.com/articles/2014-01-30/youve-sent-mail-a-letter-writing-app-forces-users-to-slow-down |title=You've Sent Mail: A Letter-Writing App Forces Users to Slow Down |last1=Leonard |first1=Devin |date=30 January 2014 |website=BusinessWeek.com |publisher=Bloomberg LP |accessdate=26 January 2015}}&lt;/ref&gt;&lt;ref name=think&gt;{{cite web |url=http://customerthink.com/lettrs_launches_platform_to_organize_and_deliver_the_world_s_letters_in_the_cloud/ |title=lettrs Launches Platform to Organize and Deliver the World’s Letters in the Cloud |author=&lt;!--Staff writer(s); no by-line.--&gt; |date=21 December 2012 |website=CustomerThink.com |publisher=Customer Think Corporation |accessdate=26 January 2015}}&lt;/ref&gt; The lettrs app converts mobile voice, data and pictures to digital personal and public messages via its text and mobile delivery inventions.&lt;ref name=mash1&gt;{{cite web |url=http://mashable.com/2013/04/23/lettrs-mobile-app/ |title=Lettrs App Lets You Send Snail Mail From Your iPhone |last1=Petronzio |first1=Matt |date=23 April 2013 |website=Mashable.com |publisher=Mashable Inc |accessdate=26 January 2015}}&lt;/ref&gt;

lettrs is headquartered in [[New York City]] and [[Drew Bartkiewicz]] is the company’s CEO and co-founder.&lt;ref name=crunch&gt;&lt;/ref&gt; In 2015, [[Mark Jung]] was named the company [[Chairman]].&lt;ref name=trutower2&gt;{{cite web |url=http://www.trutower.com/2015/05/06/lettrs-chat-app-chairman-woman-note-23902/ |title=lettrs Messaging App Announces New Chairman and Launch of "Woman Of Note" Collection |last1=Nay |first1=Josh Robert |date=6 May 2015 |website=trutower.com |publisher=TruTower |access-date=3 July 2015}}&lt;/ref&gt;&lt;ref name=techco&gt;{{cite web |url=http://tech.co/lettrs-women-of-note-2015-05 |title=lettrs: Bringing Hand Written Notes to the Digital World |last1=Schmidt |first1=Will |date=6 May 2015 |website=tech.co |publisher=TechCo |access-date=3 July 2015}}&lt;/ref&gt; lettrs has a global user base in 174 companies,&lt;ref name=mobile&gt;{{cite web |url=http://www.adweek.com/socialtimes/messaging-app-lettrs-launches-socialstamps-advertisers-charities/554471?red=im |title=Messaging App Lettrs Launches SocialStamps for Advertisers, Charities |last1=Shaul |first1=Brandy |date=9 December 2014 |website=Social Times |publisher=Prometheus Global Media |accessdate=26 January 2015}}&lt;/ref&gt; over 1 million downloads and has been featured in several media outlets, including [[The Wall Street Journal]], [[CBS]] and [[NPR]].&lt;ref name=crunch/&gt;&lt;ref name=postal&gt;{{cite web |url=http://postalvision2020.com/postalvision-2020-3-0/speakers-3-0-conference/drew-bartkiewicz/ |title=Drew Bartkiewicz |author=&lt;!--Staff writer(s); no by-line.--&gt; |website=PostalVision2020.com |publisher=Ursa Major Associates, LLC |accessdate=26 January 2015}}&lt;/ref&gt;

==History==
lettrs was established in 2012 by technology entrepreneur [[Drew Bartkiewicz]].&lt;ref name=bweek/&gt;&lt;ref name=think/&gt;&lt;ref name=postal/&gt; Bartkiewicz came up with the idea for the company in 2008&lt;ref name=bweek/&gt; after being inspired by his grandmother’s letter writing&lt;ref name=yahoo&gt;{{cite web |url=https://news.yahoo.com/lettrs-brings-snail-mail-back-future-230223676.html |title=Lettrs Brings Snail Mail Back to The Future |last1=Van Paris |first1=Calin |date=19 June 2012 |website=Yahoo News |publisher=Mashable Inc |accessdate=26 January 2015}}&lt;/ref&gt;&lt;ref name=mash2&gt;{{cite web |url=http://mashable.com/2012/06/19/lettrs/ |title=Lettrs Brings Snail Mail Back to The Future |last1=Van Paris |first1=Calin |date=19 June 2012 |website=Nashable |publisher=Mashable Inc |accessdate=26 January 2015}}&lt;/ref&gt; and his own experiences during his service in the military.&lt;ref name=bweek/&gt;&lt;ref name=think/&gt;&lt;ref name=parcel&gt;{{cite web |url=http://postandparcel.info/54661/in-depth/innovation-in-depth/postalvision-sets-sights-on-congress-and-younger-americans/ |title=PostalVision sets sights on Congress and younger Americans |author=&lt;!--Staff writer(s); no by-line.--&gt; |date=27 March 2013 |website=PostandParcel.info |publisher=Post &amp; Parcel |accessdate=26 January 2015}}&lt;/ref&gt; lettrs was officially established the summer of 2012 with the help of his wife, Araceli Bartkiewicz, and children,&lt;ref name=postal/&gt; though it was not launched as a global platform from its [[Software release life cycle#Beta|beta]] phase until December 2012.&lt;ref name=think/&gt;&lt;ref name=nextweb&gt;{{cite web |url=http://thenextweb.com/apps/2013/04/23/word-up-lettrs-launches-on-ios/ |title=TNW Pick of the Day: Lettrs turns your iPhone into a personal writing desk, transcriber and post office |last1=Sawers |first1=Paul |date=23 April 2013 |website=The Next Web |publisher=The Next Web, Inc |accessdate=26 January 2015}}&lt;/ref&gt;

Bartkiewicz introduced the lettrs mobile application at the PostalVision 2020/3.0 conference in [[Washington, D.C.]] in 2013.&lt;ref name=postal/&gt;&lt;ref name=mash1/&gt;&lt;ref name=nextweb/&gt; The Android version was released in July 2014,&lt;ref name=android&gt;{{cite web |url=http://www.androidcentral.com/lettrs-app-comes-android-more-personal-messages |title=The lettrs app comes to Android for more personal messages |last1=Callaham |first1=John |date=14 June 2014 |website=AndroidCentral.com |publisher=Mobile Nations |accessdate=26 January 2015}}&lt;/ref&gt; followed by a re-release of the iOS app in October.&lt;ref name=apple&gt;{{cite web |url=http://www.prweb.com/releases/2014/10/prweb12261924.htm |title=lettrs Raises $1.5M, Releases First Native iPad and Popular New iPhone App |author=&lt;!--Staff writer(s); no by-line.--&gt; |date=21 October 2014 |website=PRWeb.com |publisher=Vocus PRW Holdings, LLC |accessdate=26 January 2015}}&lt;/ref&gt;

==Features==
lettrs provides a [[mobile phone|mobile]] platform for customers to create and deliver mobile letters in 80 translated languages with a selection of writing themes, proprietary “SocialStamps” and styles.&lt;ref name=think/&gt;&lt;ref name=mnn&gt;{{cite web |url=http://www.mnn.com/lifestyle/responsible-living/blogs/miss-sending-letters-try-lettrs |title=Miss sending letters? Try lettrs! |last1=Vartan |first1=Starre |date=8 February 2013 |website=Mother Nature Network |publisher=MNN Holding Company, LLC |accessdate=26 January 2015}}&lt;/ref&gt; It facilitates both private messaging and public posting of signed, translated and networked mobile-to-mobile letters.&lt;ref name=crunch/&gt;

The signature service of lettrs is the translation of letter messages in real time complete with original user signatures and selectable SocialStamps. The lettrs mobile network is able to translate an original digital letter in up to 80 languages. Users may also share open letters and the lettrs stamps across other major social networks.&lt;ref name=bweek/&gt;&lt;ref name=mash1/&gt;&lt;ref name=yahoo/&gt;&lt;ref name=parcel/&gt;

In December 2014 the company introduced a feature named SocialStamps that allows users to add a customized stamp to a letter. At the feature’s launch, the company offered 47 different stamps with plans to issue new stamps monthly. As part of the release the lettrs 2014 Persons of Note stamps on the lettrs network featured [[Michelle Phan]], [[Narendra Modi]], [[Bob Woodruff]] of [[ABC News]] and [[Stanley A. McChrystal]].&lt;ref name=mobile/&gt;&lt;ref name=stamps&gt;{{cite web |url=http://venturebeat.com/2014/12/09/lettrs-com-calls-postage-stamps-into-social-duty-for-its-old-style-letters/ |title=Lettrs calls postage stamps into social duty for its old-style letters |last1=Levine |first1=Barry |date=9 December 2014 |website=VentureBeat.com |publisher=VentureBeat |accessdate=26 January 2015}}&lt;/ref&gt;&lt;ref name=marketer&gt;{{cite web |url=http://www.mobilemarketer.com/cms/news/social-networks/19316.html |title=United Way of New York City leverages lettrs' SocialStamps for fundraising |last1=Samuely |first1=Alex |date=9 December 2014 |website=MobileMarketer.com |publisher=Napean LLC |accessdate=26 January 2015}}&lt;/ref&gt;

Users can share letters and the SocialStamps via [[Facebook]] and [[Twitter]].&lt;ref name=think/&gt;&lt;ref name=nextweb/&gt; lettrs also integrates with [[Google+]] and [[Instagram]] so that users may broaden the distribution of their letters beyond the mobile app.&lt;ref name=bweek/&gt; Users can also pen open public letters or petitions for supporting causes, persons, or brands.&lt;ref name=think/&gt;&lt;ref name=nextweb/&gt;

lettrs conducted its first Hollywood movie integration in April 2015 with Relativity Media. The company released stamps featuring images from the movie ''[[Desert Dancer]]''.&lt;ref name=trutower/&gt; In May 2015, lettrs released the "Women of Note" stamp collection. It featured 12 notable women including [[Michelle Obama]], [[Queen Rania of Jordan]], [[Shakira]], [[Michelle Bachelet]], [[Laura Bush]], [[Sonia Gandhi]], [[Ellen DeGeneres]] and [[Angelina Jolie]].&lt;ref name=trutower2/&gt;&lt;ref name=techco/&gt;

==Recognition and partnerships==
In 2014, Google selected lettrs as one of the Best Android Apps of the year.&lt;ref name=time&gt;{{cite web |url=http://time.com/3611709/best-android-google-play-apps-2014/ |title=Google Says These Are 2014's Best Android Apps |last1=Luckerson |first1=Victor |date=1 December 2014 |website=Time.com |publisher=Time Inc |accessdate=1 February 2015}}&lt;/ref&gt;&lt;ref name=techco/&gt;

lettrs has worked with the [[United Service Organizations|USO]], [[Aspen Institute]], and the [[United Way]].&lt;ref name=trutower2/&gt; In 2014, the company published the first digitally sourced book of letters, ''Poetguese''. The book contains a foreword by author [[Paulo Coelho]] with all proceeds donated to charity.&lt;ref name=broadway&gt;{{cite web |url=http://www.broadwayworld.com/bwwbooks/article/Lettrs-Announces-POETGUESE-20141008 |title=lettrs Announces Poetguese |author=&lt;!--Staff writer(s); no by-line.--&gt; |date=8 October 2014 |website=broadwayworld.com |publisher=Wisdom Digital Media |access-date=3 July 2015}}&lt;/ref&gt;

lettrs established lettrs Foundation, an organization that partners with schools and non-profits to improve literacy through social networking, including partnerships with the United Way and Aspen Institute.&lt;ref name=trutower&gt;{{cite web |url=http://www.trutower.com/2014/07/14/lettrs-letter-writing-messaging-app-launch-on-android/ |title=lettrs Platform Launches on Android, Bringing Handwritten Letters Back to the Mainstream |last1=Nay |first1=Josh Robert |date=14 July 2014 |website=TruTower.com |publisher=TruTower |accessdate=1 February 2015}}&lt;/ref&gt;

==References==
{{reflist}}

==External links==
*{{Official site|http://about.lettrs.com}}
*[http://www.lettrsfoundation.org lettrs Foundation]

[[Category:Mobile social software]]
[[Category:Postal system]]
[[Category:Postal services]]
[[Category:Internet terminology]]
[[Category:Electronic documents]]</text>
      <sha1>tqjpucdsrh20pi62bos80bk46yzuu3v</sha1>
    </revision>
  </page>
  <page>
    <title>IMail</title>
    <ns>0</ns>
    <id>30943549</id>
    <revision>
      <id>761300173</id>
      <parentid>680171791</parentid>
      <timestamp>2017-01-22T05:04:21Z</timestamp>
      <contributor>
        <username>L2d4y3</username>
        <id>13015377</id>
      </contributor>
      <comment>Fix broken link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5829" xml:space="preserve">{{Multiple issues|
{{refimprove|date=February 2011}}
{{notability|date=February 2011}}
{{more footnotes|date=February 2011}}
{{COI|date=February 2011}}
}}

{{lowercase title}}

'''Invisible mail''', also referred to as '''iMail''', '''i-mail''' or '''Bote mail''', is a method of exchanging [[Digital data|digital]] messages from an author to one or more recipients in a secure and untraceable way. It is an open protocol and its java implementation (I2P-Bote) is free and open source software, licensed under the GPLv3.&lt;ref&gt;http://stats.i2p/cgi-bin/viewmtn/revision/file/cf46b537180b1a5b5740a1e2e85fc049ccc512ef/license.txt&lt;/ref&gt;

As with [[email]], one can send and receive iMails. However, normal [[email]]s are visible to an [[ISP]] and to the administrators of the mail servers providing the service. Https, or secure, connections still allow the server admin to view the content of an email and its related IP number. In invisible mails both the mail's content, and the identities (of the sender as well as the receiver) remain unknown to a third party observer or attacker. Furthermore, all iMails are automatically and transparently [[end-to-end principle|end-to-end]] encrypted.

At present, iMail cannot be sent to regular email accounts. iMail addresses are called iMail destinations. They are much longer than the average email addresses and do not carry the "@" sign nor a domain. They already include the encryption key, so using an iMail destination is not harder than using standard email with [[GNU Privacy Guard|gpg]] encryption. The destination is two in one: the "address" as well as the public key. In contrast to gpg- or pgp-encrypted emails, I2P-Bote also encrypts the mail headers.

I2P-Bote also works as an anonymous or pseudonymous remailer. iMails are sent via the [[I2P]] network, a secure and pseudonymous p2p overlay network on the internet and sender and receiver need not be online at the same time ([[store-and-forward]] model). The entire system is serverless and fully distributed. iMail [[Peer-to-peer|peer]]s accept, forward, store and deliver messages. Neither the users nor their computers are required to be online simultaneously; they need connect only briefly for as long as it takes to send or receive messages.

An iMail message consists of three components, the message ''envelope'', the message ''header'', and the message ''body''. The message header contains control information, including, minimally one or more recipient addresses. Usually descriptive information is also added, such as a subject header field and a message submission date/time stamp.

iMails can carry international typesets and have small  multi-media content attachments, a process standardized in [[Request for Comments|RFC]] 2045 through 2049. Collectively, these RFCs have come to be called [[Multipurpose Internet Mail Extensions]] (MIME).

==Features==
* ''secure messages'': All iMail messages are automatically end-to-end encrypted from the sender to the receiver.
* ''message authentication'': All iMail messages that are not sent without any information on the originator are automatically signed and the message's integrity and authenticity is checked by the receiver.
* ''anonymous messages'': iMails can also be sent without any information about the originator.
&lt;ref&gt;http://stats.i2p/cgi-bin/viewmtn/revision/file/cf46b537180b1a5b5740a1e2e85fc049ccc512ef/doc/techdoc.txt&lt;/ref&gt;

===Attachment size limitations===
{{Main|Email attachment}}
iMail messages may have one or more attachments. Attachments serve the purpose of delivering binary or text files of unspecified size. In principle there is no technical intrinsic restriction in the I2P-Bote protocol limiting the size or number of attachments. In practice, however, the slow speeds, overheads and data volume due to redundancy limit the viable size of files or the size of an entire message.

===Email spoofing===
{{Main|Email spoofing}}
[[Email spoofing]] occurs when the header information of an email is altered to make the message appear to come from a known or trusted source. In the case of iMails, this is countered by [[cryptography|cryptographically]] signing each iMail with its originator's key.

===Tracking of sent mails===
The I2P-Bote mail service provides no mechanisms for tracking a transmitted message, but a means to verify that it has been delivered, which however does not necessarily mean it has been read.

===Drawbacks===
iMails can only be received or sent via the web interface, there is no implementation of POP3 or SMTP for iMail yet. Furthermore, there are no bridges that allow for sending from I2P-Bote to a standard internet email account or vice versa.

==See also==

===Related services===
* [[Email]]
* [[I2P]]
* [[Data security]]
* [[Email encryption]]
* [[Email client]], [[Comparison of email clients]]
* [[Email hosting service]]
* [[Internet mail standard]]s
* [[Mail transfer agent]]
* [[Mail user agent]]
* [[Unicode and email]]
* [[Webmail]]
* [[Anonymous remailer]]
* [[Disposable email address]]
* [[Email encryption]]
* [[Email tracking]]
* [[Electronic mailing list]]
* [[Mailing list archive]]

===Protocols===
* [[IMAP]]
* [[POP3]]
* [[SMTP]]
* [[UUCP]]
* [[X400]]

==References==
{{reflist}}
* http://i2pbote.i2p/src.zip  (source code)
* http://i2pbote.i2p/history.txt (history.txt)
* http://awxcnx.de/handbuch_55.htm (German Privacy Foundation)
* http://www.unitethecows.com/other-p2p-clients/48940-i2pbote-0-1-2-released.html

==External links==
{{Wiktionary|iMail|email|outbox}}
* http://i2pbote.i2p (I2P-internal)
* http://www.i2p2.de
&lt;!-- please see http://en.wikipedia.org/wiki/WP:EL before adding links --&gt;

{{Computer-mediated communication}}
{{Email clients}}

[[Category:Email]]
[[Category:Internet terminology]]
[[Category:American inventions]]
[[Category:Electronic documents]]</text>
      <sha1>mf3dmkrtipsjb1ggxpp95az365zbhb0</sha1>
    </revision>
  </page>
  <page>
    <title>Teleadministration</title>
    <ns>0</ns>
    <id>47533003</id>
    <revision>
      <id>740039241</id>
      <parentid>721262395</parentid>
      <timestamp>2016-09-18T18:01:14Z</timestamp>
      <contributor>
        <username>Danielem1976</username>
        <id>26012251</id>
      </contributor>
      <comment>Removed template: now there are links.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="22505" xml:space="preserve">'''Teleadministration''' is based on the concept that documents in electronic format have legal value. Administrative informatics is not new, but for many years it was merely Information Technology applied to legal documents, that is, the reproduction of paper-based legal documents into electronic file systems. Instead, Teleadministration turns this approach into its head. It is based on research conducted in 1978, the year when, at a conference promoted by the [[Court of Cassation (Italy)|Court of Cassation]], [[:it:Giovanni Duni|Giovanni Duni]] launched the then-futuristic idea that an electronic document could have legal value.&lt;ref&gt;1. Duni, G., L'utilizzabilità delle tecniche elettroniche nell'emanazione degli atti e nei procedimenti amministrativi. Spunto per una teoria dell'atto amministrativo emanato nella forma elettronica, in "Rivista amm. della Repubblica italiana", 1978, pag.407 ss.&lt;/ref&gt; 1978 was also the year in which the first research on digital signatures ([[RSA (cryptosystem)|RSA]])&lt;ref&gt;2. Rivest, Shamir e [[Leonard Adleman|Adleman]], A method for obtaining digital signature and public key cryptosystems, in Communications of the ACM, vol. 21, febbraio 1978, 120-126. This research referred to the asymmetric encryption technology (Diffie and Hellman, New directions in Cryptography, in IEEE Transaction on Information Theory, November 1976, 644 ss. Diffie and Hellman’s research was disseminated in Italy by  Gardner, “Un nuovo tipo di cifrario che richiederebbe milioni di anni per essere decifrato”, in Le Scienze, December 1977, 126 ss.), who added the regulation for issuing the keys and the public certification process associated to them.&lt;/ref&gt; was published in the United States, yet it would take more than twenty-five years for jurists and mathematicians to start working together.&lt;ref&gt;3. The first application of the research by Rivest, Shamir and Adleman was the 1995 Utah Code, § from 46-3-101 to 46-3-504 (Enacted by l. 1995, ch. 61). The Utah code was analysed in the brilliant dissertation written by Francesca Flora, Evoluzione della informatica nel sistema di governo degli Stati Uniti d’America (Cagliari, dept. Of Politiacl Science, November 1996). For application at the federal level one had to wait until 1998: US Senate, S. 1594, Digital Signature and Electronic Authentication Law (SEAL) of 1998. — US House of Representatives, H.R. 3472, Digital Signature and Electronic Authentication Law (SEAL) of 1998.&lt;/ref&gt;

For many years, and even before 1978, IT helped [[Public Administration]] but kept a “safe distance”, assuming that the ‘sacred nature’ of the Law demanded the use of pen and paper. Information Technology merely managed and filed copies of legal documents: it was known as “parallel IT”,&lt;ref&gt;4. Duni, G., Amministrazione digitale, Voce della Enciclopedia del diritto, Annali, I, Milano 2007, p. 13-49.&lt;/ref&gt; since it was an accessory to the activity with formal value, the one based on pen and paper.

Thus, the logical, legal and material premise of Teleadministration is the conferment of legal value to IT documents.

== Origins and terminology  ==
In Italy, the linguistic expression &lt;ref&gt;5. [http://www.treccani.it/enciclopedia/teleamministrazione_(Lessico-del-XXI-Secolo) Teleamministrazione, Lessico del XXI secolo], Treccani&lt;/ref&gt; teleamministrazione was first used in 1991 at the Roman ‘La Sapienza’ university, during a conference organised by the Court of Cassation,&lt;ref&gt;6. Duni, G., Il progetto nazionale di teleamministrazione pubblica, in “L’informatica giuridica e il Ced della Corte di Cassazione”, proceedings of the conference held at the Univ. of Rome “La Sapienza”, 27-29 Nov. 1991, Milan 1992, p. 87 ss.&lt;/ref&gt; in which it was said that: «the new system of administrative information technology is called “teleadministration” because all the work of the [[Public Administration]] will be carried out through devices, that could also be computers, linked to the central server through a network.» Teleadministration was indeed considered a type of teleworking.&lt;ref&gt;7. Applicazioni della multimedialità nella P.A.: teleamministrazione e telelavoro”, in Funzione Pubblica, special issue “I convegni di FORUM P.A. ’96”, volume I, p. 105.&lt;/ref&gt;

With Teleadministration, amministrative procedures become electronic administrative procedures and, more specifically, those that are initiated by a party realize the electronic One Stop Shop.

== The fundamentals of teleadministration ==
In the decades from 1970 to 1990, the [[Supreme Court of Cassation (Italy)|Court of Cassation]] was at the core of research on the relationship between IT and Law, organising international conferences every five years on the topic. The 1993 international conference featured the fundamentals of teleadministration, providing the details of the administrative systems behind the One Stop Shop concept:&lt;ref&gt;8. Duni, G., presentation at the 5th International Congress at the Court of Cassation on “IT and Legal Activity” Rome, 3–7 May 1993, I.P.Z.S. - Libreria dello Stato, 1994, II, p. 381 ss.&lt;/ref&gt;
# A citizen presents his/her claim to one administration, which then manages the entire procedure.
# A single “administrative file” is created, no matter how many different administrations may be involved.
# For both internal and external stages in an administrative procedure, a "warning signal" is sent telematically to the relevant office where the next stage is due, the employee in that office then becomes responsible for that phase of the procedure.
# Any information concerning records already held by public administration is accessed telematically, without involving the citizen. 
# The (electronic) signature identifies the identity of the operator through sophisticated techniques.
# The original of an administrative act is electronic and is therefore always available telematically to any administration that may need it. 
# The presence of an increasing amount of on-line data will necessitate greater use of automatic data processing in decision-making.
# Saving data on multiple memory locations will guarantee the safekeeping of the acts. 
# Statistical data will be available in real time and under multiple profiles, with great benefits for top level decision-making.
# Private citizens can obtain paper copies of the electronic acts.
It should be noted that the 5th fundamental mentions electronic rather than digital signatures.  This is because, in the jurists’ domain, digital signatures were not yet known. The generic reference to the electronic signature is however valid, and its general nature is actually suitable for the rules contained six years later in the Directive 1999/93/EC.
As we will see, the Directive refers in particular to all procedures initiated by private citizens, but the system remains valid for all procedures initiated by the administration offices as well.

== Acknowledgement of the principles in current law ==
The legally accepted form of acts and documents evolved according to the following stages:
# Acts only exist in paper form
# Acts in electronic format are a possible option
# Electronic format is compulsory, safe for a few exceptions
In Italy, Phase 2 was launched by Art 15, para, 2,  Law N. 59 of 15 March 1997, (the so-called ‘Bassanini 1’ law: it established the legal value of electronic documents, while regulations would establish the authentication criteria). The EC intervened later with its Directive 1999/93/EC of the [[European Parliament]] and the [[Council of the European Union|Council]] of 13 December 1999 (Transposed by Law Decree N.10 of 23 January 2002), which imposes an obligation on Member States to give legal value to documents with digital signatures (not directly named as such, but all their features are described in the directive). It also establishes that electronic documents should not be rejected a priori, hence opening to a range of different solutions to establish the authorship of a document (the so-called ‘weak signatures’).

The 1993 Directive was revoked and absorbed (for reasons of legal certainty and clarity) by Regulation 910/2014 of the European Parliament and Council of 23 July 2014 (also known as eIDAS regulation) in the OJEU 28 August 2014, which did not renege on the principle of also accepting the so-called ‘weak signatures’.
In Italy, The move to Phase 3 was established by Art.40 of Legislative sl. Decree N.82 of 7 March 2007, Code of Digital Administration (CAD), entitled “Creation of electronic documents”, which states: “Public Administrations make the original copy of their documents with electronic means, according to the provisions of the present Code and the technical specifications of Article 71”. Exceptions are extremely rare: Comma 3 states: by means of appropriate regulations…. , proposed by the delegated Ministers for Public Functions, Innovation and Technology and the Minister for Cultural Heritage and Activities, the categories of administrative documents that can be created on paper in original are identified, having regard to the special historical and archive value they will have by nature” (think, for example, of the resignation of a President of the Republic).

Unfortunately, national administrations are ignoring this provision, and today it is only private companies that are no longer allowed paper-based communication with public administrations (Art. N. 5 bis of the CAD and D.P.C.M. 22 July 2011); compulsory electronic invoicing was added on 31 March 2015 by Law N. 44 of 24 December 2007, Art. N. 1, para 209-214 implemented by Ministerial  Decree N.55 of 3 April 2013, further clarified by Ministerial circular N.1 of 9 March 2015.
The modernisation of procedures was also touched by Presidential Decree N. 447 of 20 October 1998 (creation of the One Stop Shop, but only for production activities, and paper based), while interest for a telematic procedure only began with Legislative Decree N. 82 of 7 March 2005, CAD, which is not as relevant in its first version but was later modified by several interventions, particularly Legislative Decree N. 335 of 10 December 2013.

European sources are also essential. The EC, and later the European Union, have undertaken a wide range of actions on e-government: one of the most important was the launch of the IDABC programme (and financing) for Interoperable Delivery of European eGovernment Services to public Administrations, Business and Citizens, via Decision 2004/387/EC of the European Parliament and Council of 21 April 2004. However, the ultimate acknowledgement of the principles of teleadministration, with the telematic One Stop Shop, is contained in the Directive 2006/123/EC of the European Parliament and Council of 12 December 2006, on the Internal market for services, which provides for Member States to set up an electronic One Stop Shop in the wide field of administrative procedures.

== Teleadministration and the ‘star’ procedure ==
This article is not meant to argue the great effect that teleadministration has on the efficiency of administrative activity, as we assume that the reader is fully aware that, once paper based documents are abandoned, the real-time flow of documents greatly improves time management and responsibility of the single offices/ operators, while direct online access improves transparency. Rather, this paragraph wants to emphasize how teleadministration promotes maximum usage of the “star procedure”, known and researched in Germany as “Sternverfahren”. This procedure, an alternative to the sequential procedure, which has by nature longer head times, in the paper-based world would require making several copies of the administrative file (which can be extremely voluminous) for each office and each administration that needs to express an opinion or issue an authorisation. With the One Stop Shop, the administration initiating the process is charged with this task. Electronic files clearly provide evident benefits for these procedures, since all involved administrations can directly and simultaneously access the file, view the part they need to evaluate and add their opinion or authorisation directly, using a star-shaped scheme.

== Assessing the actual acceptance of teleadministration in current law and real life ==
As a scientific proposition, teleadministration sketched the system of telematic administrative procedures well ahead of the law, and particularly the electronic One Stop Shop concept. Both concepts are based on the dematerialisation of documents and on telematic administrative work.

The concept of documents’ dematerialisation,&lt;ref&gt;9. According to some commentators “dematerialization” is not the appropriate term for documents that are created in electronic form, but rather for those that are created in paper form and are later converted in digital format. Though conceptually correct, this observation ignores the fact that the expression now is generally understood to mean any “form that does without the material presence of paper, from the creation of the document”&lt;/ref&gt; in existence since 1978 as a scientific notion,&lt;ref&gt;10. See note. 1.&lt;/ref&gt; was first embraced in Italy (Law N. 59 of 15 March 1997, Art. 15, para 2) and later by the E.C. in Directive 1999/93/EC.

Once the principle that an electronic document can have legal value was accepted, it was possible to deal with its management within a telematic procedure.
As mentioned, configuring this procedure within the rules of teleadministration is today accepted in both European and Italian laws. European laws also provide quite a detailed description of the electronic One Stop Shop, with rules that fit nicely within the scientific rules of teleadministration; their main limitation is that they were specifically designed for the free circulation of services within Europe, and hence for the procedures these require. It is the above-mentioned 1996/123/EC Directive, whose Art.6 establishes the One Stop Shop and Art. 8 provides that it should be managed “remotely and electronically”, leaving further details to the Commission. And indeed the Commission, with its Decision of 16 October 2009, provided a number of measures to facilitate the use of electronic procedures through the “One Stop Shop” under Directive 2006/123/EC. These sources are clear and they apply to a wide-ranging sector: the problem is that any sector or procedure that is not related to the supply of services within the Union is not regulated, and Member States are therefore able to carry on with old-fashioned paper-based procedures.

In light of this limitation, a group of illustrious European Law academics, coordinated by Giovanni Duni, has drafted the most effective text for a Directive providing a universal system of telematic administrative procedure.&lt;ref&gt;11. The outcome of this collective research effort is the text for a draft Directive, that can be found on the CNR ITTIG journal  Informatica e diritto, Vol. XXI, 2012, N. 2, pp. 113-129: The telematic procedures in the European Union. Introducing a draft Directive, as well as  on line, with an Italian and an English language version of the draft on the site [http://www.teleamministrazione.it www.teleamministrazione.it]&lt;/ref&gt;

Italian sources are based on the Code of Digital Administration, the above-mentioned Law Decree N.82 of 7 March 2005 in its current version, following several modifications, which (if correctly interpreted and implemented) should make it compulsory for all public administrations to use teleadministration, thus making the telematic administrative procedure the default procedural method. Art. 14, the key provision, establishes that the proceeding administration creates an electronic file, to which all involved administrations can and should have access, and feed it with the acts of their competence. Private citizens can also access it under Law 241/90
Thus the electronic file is the technical and organizational specification of the telematic administrative procedure, as it is clear that its creation is an operative stage of the procedure and not simply a new filing system for the archives.

Art. 10 of the CAD appears at first to be at odds with this interpretation, since it establishes that the One Stop Shop for productive activities provides its services electronically but leaves doubts about the possibility that the back office activities could be still paper-based. However, if Art. 10 and Art. 41 are interpreted together, the only possible conclusion is that the former is a clarification of front office activities, but all the administrative activity is based on the general rule of electronic files, and therefore on the teleadministration and the One Stop Shop. 
Compared to European and Italian law, reality is somewhat behind. Eight years after Directive 1996/123/CE, there would be grounds for an infraction procedure against Italy. But since Italy enjoys the company of several other non-compliant Member States, they are all ‘safe’ for the time being.

Though the letter of the CAD may be not be respected, it seems very unlikely that this may determine the invalidity or nullity of the acts for violation of Art. 41(electronic file) or Art. 40 (statutory requirement of digital signature), because in front of a claim of this nature, the administrative judge would apply Artt. 21 septies and 21 octies of Law 241/90. The claimant should demonstrate that the use of the electronic format and electronic file wold have led to a different outcome.

== Legal sources ==

=== US Law ===
1995 Utah Code, paras 46-3-101 to 46-3-504 (Enacted by Law 1995, Ch. 61). US Senate, S. 1594, Digital Signature and Electronic Authentication Law (SEAL) of 1998. — US House of Representatives, H.R. 3472, Digital Signature and Electronic Authentication Law (SEAL) of 1998

=== EU Law ===
*European Parliament and Council Directive 1999/93/EC of 13 December 1999, revoked and absorbed by European Parliament and Council Regulation 910/2014 of 23 July 2014, (known as the eIDAS regulation) in OJEU 28 August 2014. 
*European Parliament and Council Decision 2004/387/EC of 21 April 2004 on the interoperable delivery of European eGovernment services
*European Parliament and Council Directive 2006/123/EC of 12 December 2006, on internal market services, providing for Member States to establish the electronic One Stop Shop in this vast field of administrative procedures. 
*Decision of 16 October 2009, establishing measures to facilitate electronic procedures through the «One Stop Shop» under Directive 2006/123/CE 
The telematic procedures in the European Union. Introducing a draft Directive, research coordinated by Duni, G., in CNR ITTIG Informatica e diritto, Vol. XXI, 2012, n. 2, pp.&amp;nbsp;113–129 and in www.teleamministrazione.it.

=== Italian Law ===
*l. 15 March 1997, N. 59, art. 15, para, 2. 
*D. lg. 23 January 2002, N. 10. 
*D.P.R. 20 October 1998, N. 447 
*D. legils. 7 March 2005, N. 82, codice dell'amministrazione digitale (CAD) 
*D.P.C.M. 22 July 2011 
*D. Legisl 10 December 2013, N. 335

== Bibliography ==
*Contaldo, A., La teleamministrazione con reti transnazionali europee come strumento per l'integrazione delle Pubbliche Amministrazioni dei paesi dell'Unione Europea, in Riv. trim. diritto amministrativo, I, 2004, p.&amp;nbsp;95 and later 
*Diffie and Hellman, New directions in Cryptography, in IEEE Transaction on Information Theory, November 1976, 644 ss 
*Duni, G., L'utilizzabilità delle tecniche elettroniche nell'emanazione degli atti e nei procedimenti amministrativi. Spunto per una teoria dell'atto amministrativo emanato nella forma elettronica, in "Rivista amm. della Repubblica italiana", 1978, pag.407 ss. — Il progetto nazionale di teleamministrazione pubblica, in “L’informatica giuridica e il Ced della Corte di Cassazione”, proceedings of the conference held at Univ. of Rome “La Sapienza”, 27-29 Nov. 1991, Milan 1992, p.&amp;nbsp;87 ss. — La teleamministrazione: una “scommessa” per il futuro del Paese, presentation at the 5th International Congress at the Court of Cassation on “IT and Legal Activity” Rome, 3–7 May 1993, I.P.Z.S. - Libreria dello Stato, 1994, II, p.&amp;nbsp;381 ss. — Amministrazione digitale, Item under the Enciclopedia del diritto, Annali, I, Milan 2007, p.&amp;nbsp;13-49 — L’amministrazione digitale. Il diritto amministrativo nell’evoluzione telematica, Giuffrè 2008. 
*Flora, F., Evoluzione della informatica nel sistema di governo degli Stati Uniti d’America. Dissertation, Cagliari, Dept. Of Political Science, November 1996. 
*Gagliotti, A., Teleamministrazione e concorsi pubblici, in Giustizia amministrativa n. 3/2003, http://www.giustamm.it/ago1/articoli/gaglioti_teleamministrazione.htm#_ednref5 
*Gardner, Un nuovo tipo di cifrario che richiederebbe milioni di anni per essere decifrato, in Le Scienze, December 1977, 126 ss. 
*Masucci, Informatica pubblica, in Dizionario di diritto pubblico directed by S. Cassese, IV, Milan, 2006, 3115 ss.; 
*Notarmuzi, Il codice dell’amministrazione digitale, in Astrid Rassegna, www.astrid-online.it, 2006, n. 12; Id., Il procedimento amministrativo informatico, ivi, n. 16; 
*Osnaghi, Firme elettroniche e documento informatico: il codice richiede ulteriori integrazioni, ivi, n. 10; 
*Rabbito. c., L'informatica al servizio della pubblica amministrazione. Dai principi della teleamministrazione ai piani di e-government, Gedit, 2007. 
*Rivest, Shamir e Adleman, A method for obtaining digital signature and public key cryptosystems, in Communications of the ACM, vol. 21, February 1978, 120-126 
*The telematic procedures in the European Union. Introducing a draft Directive, ricerca coordinata da Duni, g., in CNR ITTIG Informatica e diritto, Vol. XXI, 2012, N. 2, pp.&amp;nbsp;113–129 and in www.teleamministrazione.it. 
*Applicazioni della multimedialità nella P.A.: teleamministrazione e telelavoro”, in Funzione Pubblica, special issue “I convegni di FORUM P.A. ’96”, volume I, p.&amp;nbsp;105.

==See also==
*[[Digital era governance]]
*[[Electronic document]]
*[[Electronic paper]]
*[[Paperless office]]
*[[Bureaucrat]]
*[[E-Government Act of 2002]]
*[[E-government]]
*[[Public administration]]

==References==
&lt;references/&gt;

[[Category:E-government]]
[[Category:Administrative law]]
[[Category:Public-key cryptography]]
[[Category:Electronic documents]]
[[Category:Public administration]]</text>
      <sha1>tv4f5t2ywyfitaj28s63ynesq320mpw</sha1>
    </revision>
  </page>
  <page>
    <title>TheSwizzle.com</title>
    <ns>0</ns>
    <id>37690869</id>
    <revision>
      <id>755847861</id>
      <parentid>755847529</parentid>
      <timestamp>2016-12-20T14:48:00Z</timestamp>
      <contributor>
        <username>DberkowitzNY</username>
        <id>6452429</id>
      </contributor>
      <comment>noted site/company is inactive and got acquired</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3302" xml:space="preserve">{{Multiple issues|
{{COI|date=August 2013}}
{{refimprove|date=August 2013}}
}}
{{Infobox dot-com company
| name   = TheSwizzle
| logo         = 
[[File:TheSwizzle.com Logo.jpg|150x50px|TheSwizzle.com Logo]]
| company_type   = [[Privately held company|Private]]
| location_city    = [[New York City]],&lt;ref&gt;{{cite web|url=http://www.theswizzle.com |title=TheSwizzle |accessdate=2012-11-19}}&lt;/ref&gt; [[New York (state)|New York]]
| location_country = USA
| foundation = 2010
| founder                   = [[Scott Kurnit]]
| registration              = Optional
| current_status            = Inactive
| industry       = [[advertising]], [[online advertising]], [[email]]
| homepage       = [http://www.theswizzle.com/ www.theswizzle.com]
}}
'''TheSwizzle''' was a [[webmail]] tool that worked with existing email and enabled consumers to manage email subscriptions, primarily from commercial vendors.&lt;ref&gt;{{cite web |accessdate=November 19, 2012 |url=http://mashable.com/2012/10/03/swizzle-emails |title=The Swizzle Cleans Your Inbox By Combining Promo E-mails Into a Daily Digest |publisher=Mashable |date=October 25, 2012 |author=Veena Bissram }}&lt;/ref&gt;&lt;ref&gt;{{cite web |accessdate=November 19, 2012 |url=http://revision3.com/tzdaily/swizzle-junk-email |title=Clean Junk Mail From Your Inbox! |publisher=revision3 |date=October 25, 2012 |author=Veronica Belmont }}&lt;/ref&gt;&lt;ref&gt;{{cite web |accessdate=November 19, 2012 |url=http://www.pcmag.com/article2/0,2817,2411068,00.asp |title=Swizzle |publisher=pcmag |date=October 17, 2012 |author=Samara Lynn }}&lt;/ref&gt; It was acquired by Mailstrom of 410 Labs in September 2014, and TheSwizzle.com subsequently shut down.&lt;ref&gt;{{Cite news|url=http://technical.ly/baltimore/2014/09/11/mailstrom-the-410-labs-email-helper-lands-ex-competitors-users-swizzle/|title=Mailstrom, the 410 Labs email helper, lands ex-competitor's users - Technical.ly Baltimore|date=2014-09-11|newspaper=Technical.ly Baltimore|language=en-US|access-date=2016-12-20}}&lt;/ref&gt;

==Features==
The product claims several features, including cleaning up users' inboxes by helping to unsubscribe from unwanted emails while at the same time, allowing receipt as well as searching among those commercially oriented emails an individual still wants to receive. By packaging these messages into a digest format, users can consolidate their email box.

==History==
The Swizzle is a product of Keep Holdings, a consumer and brand engagement conglomerate of business units including Keep.com, [[AdKeeper]] and TheSwizzle.com.
The company was founded in March, 2010,&lt;ref&gt;{{cite web |accessdate=January 17, 2010 |url=http://investing.businessweek.com/research/stocks/private/snapshot.asp?privcapId=114604562 |title=AdKeeper, Inc. Snapshot |publisher=Bloomberg Businessweek |author=Staff }}&lt;/ref&gt; by [[Scott Kurnit]], who serves as Chairman and CEO. Kurnit is best known as the founder of [[About.com]], which grew to a public market value of $1.7 billion, and was sold to Primedia for $724 million, in 2001. About.com is now owned by [[IAC (company)|IAC]].

==See also==
* [[Scott Kurnit]]
* [[AdKeeper]]

==References==
{{Reflist}}

==External links==
* {{Official website|http://www.theswizzle.com/}}

{{DEFAULTSORT:Swizzle}}
[[Category:Electronic documents]]
[[Category:Internet properties established in 2010]]</text>
      <sha1>lmsooa3nr8r3g0bgopdoywqns2r8jj3</sha1>
    </revision>
  </page>
  <page>
    <title>Email</title>
    <ns>0</ns>
    <id>9738</id>
    <revision>
      <id>763013937</id>
      <parentid>763013632</parentid>
      <timestamp>2017-01-31T22:14:06Z</timestamp>
      <contributor>
        <username>Sterilized by cupcake</username>
        <id>30215987</id>
      </contributor>
      <minor />
      <comment>/* Host-based mail systems */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="81455" xml:space="preserve">{{about|the communications medium|the former manufacturing conglomerate|Email Limited}}
{{redirect|Inbox|the Google product|Inbox by Gmail}}
&lt;!--Before adding {{lowercase title}} again, please see talk page.--&gt;
[[File:2016-03-22-trojita-home.png|thumb|right|400px|This screenshot shows the "Inbox" page of an email system, where users can see new emails and take actions, such as reading, deleting, saving, or responding to these messages]]
[[File:(at).svg|thumb|100px|The [[at sign]], a part of every SMTP [[email address]]&lt;ref&gt;{{cite web|url=https://tools.ietf.org/html/rfc5321#section-2.3.11|title=RFC 5321 – Simple Mail Transfer Protocol|accessdate=19 January 2015|work=Network Working Group }}&lt;/ref&gt;]]
'''Electronic mail''', or '''email''', is a method of exchanging digital messages between people using digital devices such as computers, tablets and mobile phones. Email first entered substantial use in the 1960s and by the mid-1970s had taken the form now recognized as email. Email operates across [[computer network]]s, which in the 2010s is primarily the [[Internet]]. Some early email systems required the author and the recipient to both be [[Online and offline|online]] at the same time, in common with [[instant messaging]]. Today's email systems are based on a [[store-and-forward]] model. Email [[Server (computing)|servers]] accept, forward, deliver, and store messages. Neither the users nor their computers are required to be online simultaneously; they need to connect only briefly, typically to a [[Message transfer agent|mail server]] or a [[webmail]] interface, for as long as it takes to send or receive messages.

Originally an [[ASCII]] text-only communications medium, Internet email was extended by [[Multipurpose Internet Mail Extensions]] (MIME) to carry text in other character sets and multimedia content attachments. [[International email]], with internationalized email addresses using [[UTF-8]], has been standardized, but as of 2016 it has not been widely adopted.{{citation needed|date=March 2016}}

The history of modern Internet email services reaches back to the early [[ARPANET]], with standards for encoding email messages published as early as 1973 (RFC 561). An email message sent in the early 1970s looks very similar to a basic email sent today. Email played an important part in creating the Internet,&lt;ref&gt;{{Harv|Partridge|2008}}&lt;/ref&gt; and the conversion from ARPANET to the Internet in the early 1980s produced the core of the current services.

==Terminology==
Historically, the term ''electronic mail'' was used generically for any electronic document transmission. For example, several writers in the early 1970s used the term to describe [[fax]] document transmission.&lt;ref&gt;Ron Brown, Fax invades the mail market, [https://books.google.com/books?id=Ry64sjvOmLkC&amp;pg=PA218 New Scientist], Vol. 56, No. 817 (Oct., 26, 1972), pages 218–221.&lt;/ref&gt;&lt;ref&gt;Herbert P. Luckett, What's News: Electronic-mail delivery gets started, [https://books.google.com/books?id=cKSqa8u3EIoC&amp;pg=PA85 Popular Science], Vol. 202, No. 3 (March 1973); page 85&lt;/ref&gt; As a result, it is difficult to find the first citation for the use of the term with the more specific meaning it has today.

Electronic mail has been most commonly called ''email'' or ''e-mail'' since around 1993,&lt;ref&gt;{{cite book|url=https://books.google.com/ngrams/graph?content=electronic+mail%2Ce-mail&amp;year_start=1980&amp;year_end=1995&amp;corpus=15&amp;smoothing=0&amp;share= |title=Google Ngram Viewer |publisher=Books.google.com |accessdate=2013-04-21}}&lt;/ref&gt; but variations of the [[spelling]] have been used:

* ''email'' is the most common form used online, and is required by [[IETF]] [[Request for Comments|Requests for Comments]] (RFC) and working groups&lt;ref&gt;{{cite web|url=https://www.rfc-editor.org/rfc-style-guide/terms-online.txt|publisher=IETF|title=RFC Editor Terms List}} This is suggested by the [https://www.rfc-editor.org/rfc-style-guide/rfc-style-manual-08.txt RFC Document Style Guide]&lt;/ref&gt; and increasingly by [[style guide]]s.&lt;ref&gt;{{cite web|url=http://styleguide.yahoo.com/word-list/e |title=Yahoo style guide |publisher=Styleguide.yahoo.com |accessdate=2014-01-09}}&lt;/ref&gt;&lt;ref name="aces2011"&gt;[http://www.huffingtonpost.com/2011/03/18/ap-removes-hyphen-from-em_n_837833.html "AP Removes Hyphen From ‘Email’ In Style Guide"], 18 March 2011, huffingtonpost.com&lt;/ref&gt; This spelling also appears in most dictionaries.&lt;ref name="AskOxford Language Query team"&gt;{{cite web | url=http://www.askoxford.com/asktheexperts/faq/aboutspelling/email | title=What is the correct way to spell 'e' words such as 'email', 'ecommerce', 'egovernment'? | publisher=[[Oxford University Press]] | work=FAQ | accessdate=4 September 2009 | author=AskOxford Language Query team | archiveurl=https://web.archive.org/web/20080701194047/http://www.askoxford.com/asktheexperts/faq/aboutspelling/email?view=uk | quote=We recommend email, as this is now by far the most common form | archivedate=July 1, 2008}}&lt;/ref&gt;&lt;ref name="Reference.com"&gt;{{cite web|url=http://dictionary.reference.com/browse/email |title=Reference.com |publisher=Dictionary.reference.com |accessdate=2014-01-09}}&lt;/ref&gt;&lt;ref name="ReferenceA"&gt;Random House Unabridged Dictionary, 2006&lt;/ref&gt;&lt;ref name="ReferenceB"&gt;The American Heritage Dictionary of the English Language, Fourth Edition&lt;/ref&gt;&lt;ref name="Princeton University WordNet 3.0"&gt;Princeton University WordNet 3.0&lt;/ref&gt;&lt;ref name="ReferenceC"&gt;The American Heritage Science Dictionary, 2002&lt;/ref&gt;&lt;ref name="Merriam-Webster Dictionary"&gt;{{cite web|title=Merriam-Webster Dictionary|url=http://www.merriam-webster.com/dictionary/email|publisher=Merriam-Webster|accessdate=9 May 2014}}&lt;/ref&gt;
* ''e-mail'' is the format that sometimes appears in edited, published American English and British English writing as reflected in the [[Corpus of Contemporary American English]] data,&lt;ref&gt;{{cite web|url=http://english.stackexchange.com/questions/1925/email-or-e-mail|title= "Email" or "e-mail"|work=English Language &amp; Usage – Stack Exchange|date=August 25, 2010|accessdate=September 26, 2010}}&lt;/ref&gt; but is falling out of favor in style guides.&lt;ref name="aces2011" /&gt;&lt;ref name="ap"&gt;{{cite web|title=AP changes e-mail to email|url=http://www.aces2011.org/sessions/18/the-ap-stylebook-editors-visit-aces-2011/|work=15th National Conference of the American Copy Editors Society (2011, Phoenix)|publisher=ACES|accessdate=23 March 2011|author=Gerri Berendzen|authorlink=AP Stylebook editors share big changes|author2=Daniel Hunt}}&lt;/ref&gt;
* ''mail'' was the form used in the original protocol standard, ''RFC&amp;nbsp;524''.&lt;ref name=rfc524&gt;{{cite web|url=http://www.faqs.org/rfcs/rfc524.html |title=RFC 524 (rfc524) – A Proposed Mail Protocol |publisher=Faqs.org |date=1973-06-13 |accessdate=2016-11-18}}&lt;/ref&gt; The service is referred to as ''mail'', and a single piece of electronic mail is called a ''message''.&lt;ref name="11above"&gt;{{cite web|url=http://www.faqs.org/rfcs/rfc1939.html |title=RFC 1939 (rfc1939) – Post Office Protocol – Version 3 |publisher=Faqs.org |accessdate=2014-01-09}}&lt;/ref&gt;&lt;ref name="12above"&gt;{{cite web|url=http://www.faqs.org/rfcs/rfc3501.html |title=RFC 3501 (rfc3501) – Internet Message Access Protocol – version 4rev1 |publisher=Faqs.org |accessdate=2014-01-09}}&lt;/ref&gt;
* ''EMail'' is a traditional form that has been used in RFCs for the "Author's Address"&lt;ref name="11above"/&gt;&lt;ref name="12above"/&gt; and is expressly required "for historical reasons".&lt;ref&gt;{{cite web|url=https://www.rfc-editor.org/rfc-style-guide/terms-online.txt |title='&amp;#39;"RFC Style Guide"'&amp;#39;, Table of decisions on consistent usage in RFC |accessdate=2014-01-09}}&lt;/ref&gt;
* ''E-mail'' is sometimes used, capitalizing the initial ''E'' as in similar abbreviations like ''[[E-piano]]'', ''[[E-guitar]]'', ''[[A-bomb]]'', and ''[[H-bomb]]''.&lt;ref&gt;{{cite web|url=http://alt-usage-english.org/excerpts/fxhowdoy.html |title=Excerpt from the FAQ list of the Usenet newsgroup alt.usage.english |publisher=Alt-usage-english.org |accessdate=2014-01-09}}&lt;/ref&gt;

== {{anchor|history}} Origin ==
The [[AUTODIN]] network, first operational in 1962, provided a message service between 1,350 terminals, handling 30 million messages per month, with an average message length of approximately 3,000 characters. Autodin was supported by 18 large computerized switches, and was connected to the [[United States General Services Administration]] Advanced Record System, which provided similar services to roughly 2,500 terminals.&lt;ref name="NAS USPS"&gt;USPS Support Panel, Louis T Rader, Chair, Chapter IV: Systems, [https://books.google.com/books?id=5TQrAAAAYAAJ&amp;pg=PA27 Electronic Message Systems for the U.S. Postal Service], National Academy of Sciences, Washington, D.C., 1976; pages 27–35.&lt;/ref&gt; By 1968, AUTODIN linked more than 300 sites in several countries.

===Host-based mail systems===
With the introduction of [[Massachusetts Institute of Technology|MIT]]'s [[Compatible Time-Sharing System]] (CTSS) in 1961,&lt;ref&gt;"CTSS, Compatible Time-Sharing System" (September 4, 2006), [[University of South Alabama]], [http://www.cis.usouthal.edu/faculty/daigle/project1/ctss.htm USA-CTSS].&lt;/ref&gt; multiple users could log in to a central system&lt;ref&gt;an [[IBM 7094]]&lt;/ref&gt; from remote dial-up terminals, and store and share files on the central disk.&lt;ref&gt;[[Tom Van Vleck]], "The IBM 7094 and CTSS" (September 10, 2004), ''Multicians.org''
 ([[Multics]]), web: [http://www.multicians.org/thvv/7094.html Multicians-7094].&lt;/ref&gt; Informal methods of using this to pass messages were developed and expanded:
* 1965 – [[Massachusetts Institute of Technology|MIT]]'s [[Compatible Time-Sharing System|CTSS]] MAIL.&lt;ref name="thvv"&gt;{{cite web|url=http://www.multicians.org/thvv/mail-history.html|title=The History of Electronic Mail|author=Tom Van Vleck}}&lt;/ref&gt;

Developers of other early systems developed similar email applications:
&lt;!-- Please do not delete references without first reading them. I've added a page number. --&gt;
* 1962 – [[IBM Administrative Terminal System|1440/1460 Administrative Terminal System]]&lt;ref&gt;{{cite book
 |     author = IBM
 |      title = 1440/1460 Administrative Terminal System (1440-CX-07X and 1460-CX-08X) Application Description
 |    section =
 | sectionurl =
 |    version = Second Edition
 |  publisher = IBM
 |       date =
 |        url = http://bitsavers.org/pdf/ibm/144x/H20-0129-1_1440_admTermSys.pdf
 |         id = H20-0129-1
 | accessdate =
 |      quote =
 |       page = 10
 |      pages =
 |        ref =
 |mode=cs2
 }}&lt;/ref&gt;
* 1968 – [[IBM Administrative Terminal System|ATS/360]]&lt;ref&gt;{{cite book
 |     author = IBM
 |      title = System/36O Administrative Terminal System DOS (ATS/DOS) Program Description Manual
 |    section =
 | sectionurl =
 |    version =
 |  publisher = IBM
 |       date =
 |        url =
 |         id = H20-0508
 | accessdate =
 |      quote =
 |       page =
 |      pages =
 |        ref =
 |mode=cs2
 }}&lt;/ref&gt;&lt;ref&gt;{{cite book
 |     author = IBM
 |      title = System/360 Administrative Terminal System-OS (ATS/OS) Application Description Manual
 |    section =
 | sectionurl =
 |    version =
 |  publisher = IBM
 |       date =
 |        url =
 |         id = H20-0297
 | accessdate =
 |      quote =
 |       page =
 |      pages =
 |        ref =
 |mode=cs2
 }}&lt;/ref&gt;
* 1971 –  ''[[SNDMSG]]'', a local inter-user mail program incorporating the experimental file transfer program, ''CPYNET'', allowed the first [[Computer network|networked]] electronic mail&lt;ref name="firstnetworkemail"&gt;{{cite web|author=Ray Tomlinson |url=http://openmap.bbn.com/~tomlinso/ray/firstemailframe.html |title=The First Network Email |publisher=Openmap.bbn.com |accessdate=2014-01-09}}&lt;/ref&gt;
* 1972 – [[Unix]] [[mail (Unix)|mail]] program&lt;ref&gt;{{cite web|url=http://minnie.tuhs.org/cgi-bin/utree.pl?file=V3/man/man1/mail.1 |title=Version 3 Unix mail(1) manual page from 10/25/1972 |publisher=Minnie.tuhs.org |accessdate=2014-01-09}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://minnie.tuhs.org/cgi-bin/utree.pl?file=V6/usr/man/man1/mail.1 |title=Version 6 Unix mail(1) manual page from 2/21/1975 |publisher=Minnie.tuhs.org |accessdate=2014-01-09}}&lt;/ref&gt;
* 1972 – APL Mailbox by [[Lawrence M. Breed|Larry Breed]]&lt;ref&gt;[http://www.jsoftware.com/papers/APLQA.htm APL Quotations and Anecdotes], including [[Leslie H. Goldsmith|Leslie Goldsmith]]'s story of the Mailbox&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.actewagl.com.au/Education/communications/Internet/historyOfTheInternet/InternetOnItsInfancy.aspx|title=Home &gt; Communications &gt; The Internet &gt; History of the internet &gt; Internet in its infancy|archive-url=https://web.archive.org/web/20110227151622/http://www.actewagl.com.au/Education/communications/Internet/historyOfTheInternet/InternetOnItsInfancy.aspx| archive-date=2011-02-27 |work=actewagl.com.au |accessdate=2016-11-03}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=https://www.youtube.com/watch?v=mjgkhK-nXmk | title=The STSC Story: It's About Time |date=c. 1979 |editor=[http://aprogramminglanguage.com Catherine Lathwell] |publisher=[[Scientific Time Sharing Corporation]] |at=7:08 |accessdate=2017-01-06 |}} Promotional video for Scientific Time Sharing Corporation, which features President [[Jimmy Carter]]'s press secretary [[Jody Powell]] explaining how the company's "APL Mailbox" enabled the 1976 Carter presidential campaign to easily move information around the country to coordinate the campaign. &lt;/ref&gt;
* 1974 – The [[PLATO (computer system)|PLATO]] IV Notes on-line [[message board]] system was generalized to offer 'personal notes' in August 1974.&lt;ref name="NAS USPS" /&gt;&lt;ref name=wooley&gt;David Wooley, [http://www.thinkofit.com/plato/dwplato.htm#pnotes PLATO: The Emergence of an Online Community], 1994.&lt;/ref&gt;
* 1978 – ''Mail'' client written by Kurt Shoens for Unix and distributed with the Second Berkeley Software Distribution included support for aliases and distribution lists, forwarding, formatting messages, and accessing different mailboxes.&lt;ref name="shoens"&gt;The Mail Reference Manual, Kurt Shoens, University of California, Berkeley, 1979.&lt;/ref&gt; It used the Unix ''mail'' client to send messages between system users. The concept was extended to communicate remotely over the Berkeley Network.&lt;ref name="berknet"&gt;An Introduction to the Berkeley Network, Eric Schmidt, University of California, Berkeley, 1979.&lt;/ref&gt;
* 1979 – ''EMAIL'', an application written by [[Shiva Ayyadurai]]. He has been associated with controversial, self-made claims that he had "invented" email due to its presence of certain functionality. These claims have been disputed by various parties.&lt;ref&gt;{{cite web|url=http://www.bizjournals.com/boston/news/2016/05/10/cambridge-man-who-claims-he-invented-email-sues.html|title=Cambridge man who claims he invented email sues Gawker for $35M - Boston Business Journal|last=Harris|first=David L.|date=May 10, 2016|website=Boston Business Journal|access-date=2016-05-16}}&lt;/ref&gt;&lt;ref&gt;[https://assets.documentcloud.org/documents/2829697/Gov-Uscourts-Mad-180248-1-0.pdf ''Shiva Ayyadurai v. Gawker Media, et. al''., Complaint] (D. Mass, filed May 10, 2016)&lt;/ref&gt;&lt;ref name=DavidCrockerWaPo&gt;{{cite news|last=Crocker|first=David|title=A history of e-mail: Collaboration, innovation and the birth of a system|url=http://www.washingtonpost.com/national/on-innovations/a-history-of-e-mail-collaboration-innovation-and-the-birth-of-a-system/2012/03/19/gIQAOeFEPS_story.html|accessdate=10 June 2012|newspaper=Washington Post|date=20 March 2012}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://blogs.smithsonianmag.com/aroundthemall/2012/02/a-piece-of-email-history-comes-to-the-american-history-museum/|title=A Piece of Email History Comes to the American History Museum|date=22 February 2012|accessdate=11 June 2012|first=Joseph|last=Stromberg|publisher=[[Smithsonian Institution]]}}&lt;/ref&gt;&lt;ref name=SmithsonianStatement&gt;{{Cite press release|url=http://americanhistory.si.edu/press/releases/statement-national-museum-american-history-collection-materials-va-shiva-ayyudurai|title=Statement from the National Museum of American History: Collection of Materials from V.A. Shiva Ayyadurai|date=23 February 2012|accessdate=19 February 2013|publisher=National Museum of American History}}&lt;/ref&gt;
* 1979 – [[MH Message Handling System]] developed at RAND provided several tools for managing electronic mail on Unix.&lt;ref name="borden-mh"&gt;A Mail Handling System, Bruce Borden, The Rand Corporation, 1979.&lt;/ref&gt;
* 1981 – [[IBM OfficeVision|PROFS]] by IBM&lt;ref&gt;[http://www.ibm.com/ibm100/us/en/icons/networkbus/ ''"...PROFS changed the way organizations communicated, collaborated and approached work when it was introduced by IBM's Data Processing Division in 1981..."''], IBM.com&lt;/ref&gt;&lt;ref&gt;[https://fas.org/spp/starwars/offdocs/reagan/chron.txt ''"1982 – The National Security Council (NSC) staff at the White House acquires a prototype electronic mail system, from IBM, called the Professional Office System (PROFs)...."''], fas.org&lt;/ref&gt;
* 1982 – [[ALL-IN-1]]&lt;ref&gt;{{cite web|url=https://research.microsoft.com/en-us/um/people/gbell/Digital/timeline/1982.htm |title=Gordon Bell's timeline of Digital Equipment Corporation |publisher=Research.microsoft.com |date=1998-01-30 |accessdate=2014-01-09}}&lt;/ref&gt; by [[Digital Equipment Corporation]]
* 1982 – HP Mail (later HP DeskManager) by [[Hewlett-Packard]]&lt;ref&gt;{{cite web|url=http://www.hpmuseum.net/divisions.php?did=10|title=HP Computer Museum}}&lt;/ref&gt;

These original messaging systems had widely different features and ran on systems that were incompatible with each other. Most of them only allowed communication between users logged into the same host or "mainframe", although there might be hundreds or thousands of users within an organization.

===LAN email systems===
In the early 1980s, networked [[personal computer]]s on [[LAN]]s became increasingly important. Server-based systems similar to the earlier mainframe systems were developed. Again, these systems initially allowed communication only between users logged into the same server infrastructure. Examples include:
* [[cc:Mail]]
* [[Lantastic]]
* [[WordPerfect Office]]
* [[Microsoft Mail]]
* [[Banyan VINES]]
* [[Lotus Notes]]
Eventually these systems too could link different organizations as long as they ran the same email system and proprietary protocol.&lt;ref&gt;with various vendors supplying gateway software to link these incompatible systems&lt;/ref&gt;

===Email networks===
To facilitate electronic mail exchange between remote sites and with other organizations, telecommunication links, such as dialup modems or leased lines, provided means to transport email globally, creating local and global networks. This was challenging for a number of reasons, including the widely [[Non-Internet email address|different email address formats]] in use. 
* In 1971 the first [[ARPANET]] email was sent,&lt;ref&gt;{{cite web|title=The First Network Email|url=http://openmap.bbn.com/~tomlinso/ray/firstemailframe.html|author=Ray Tomlinson}}&lt;/ref&gt; and through RFC 561, RFC 680, RFC 724, and finally 1977's RFC 733, became a standardized working system.
* PLATO IV was networked to individual terminals over leased data lines prior to the implementation of personal notes in 1974.&lt;ref name=wooley/&gt;
* Unix mail was networked by 1978's [[uucp]],&lt;ref&gt;{{cite web|url=http://cm.bell-labs.com/7thEdMan/vol2/uucp.bun |title=Version 7 Unix manual: "UUCP Implementation Description" by D. A. Nowitz, and "A Dial-Up Network of UNIX Systems" by D. A. Nowitz and M. E. Lesk |accessdate=2014-01-09}}&lt;/ref&gt; which was also used for [[USENET]] newsgroup postings, with similar headers.
* BerkNet, the Berkeley Network, was written by [[Eric Schmidt]] in 1978 and included first in the Second Berkeley Software Distribution. It provided support for sending and receiving messages over serial communication links. The Unix mail tool was extended to send messages using BerkNet.&lt;ref name="berknet"/&gt; 
* The [[delivermail]] tool, written by [[Eric Allman]] in 1979 and 1980 (and shipped in 4BSD), provided support for routing mail over dissimilar networks, including Arpanet, UUCP, and BerkNet. (It also provided support for mail user aliases.)&lt;ref name="joy-4bsd"&gt;Setting up the Fourth Berkeley Software Tape, William N. Joy, Ozalp Babaoglu, Keith Sklower, University of California, Berkeley, 1980.&lt;/ref&gt;
* The mail client included in 4BSD (1980) was extended to provide interoperability between a variety of mail systems.&lt;ref name="shoens-mail"&gt;Mail(1), UNIX Programmer's Manual, 4BSD, University of California, Berkeley, 1980.&lt;/ref&gt;
* [[BITNET]] (1981) provided electronic mail services for educational institutions. It was based on the IBM VNET email system.&lt;ref&gt;[http://www.livinginternet.com/u/ui_bitnet.htm "BITNET History"], livinginternet.com&lt;/ref&gt;
* 1983 – [[MCI Mail]] Operated by MCI Communications Corporation.  This was the first commercial public email service to use the internet. MCI Mail also allowed subscribers to send regular postal mail (overnight) to non-subscribers.&lt;ref&gt;"[[MCI Mail]]", MCI Mail&lt;/ref&gt;
* In 1984, IBM PCs running DOS could link with [[FidoNet]] for email and shared bulletin board posting.

===Email address internationalization===
Globally countries started adopting [[Internationalized domain name|IDN]] registrations for supporting country specific scripts (non-English) for domain names. In 2010  Egypt, the Russian Federation, Saudi Arabia, and the United Arab Emirates started offering IDN registrations. The government of India also registered [[Bhārat Gaṇarājya|.bharat]]&lt;ref&gt;{{Cite web|url=https://registry.in/Internationalized_Domain_Names_IDNs|title=Internationalized Domain Names (IDNs) {{!}} Registry.In|website=registry.in|access-date=2016-10-17}}&lt;/ref&gt; in 8 languages/scripts in 2014. 
In 2016 Data Xgen Technologies was credited as World's first email platform offering EAI in India and [[Russia]].&lt;ref&gt;http://economictimes.indiatimes.com/tech/internet/datamail-worlds-first-free-linguistic-email-service-supports-eight-india-languages/articleshow/54923001.cms&lt;/ref&gt;&lt;ref&gt;http://digitalconqurer.com/gadgets/made-india-datamail-empowers-russia-email-address-russian-language/&lt;/ref&gt;

===Attempts at interoperability===
{{Refimprove section|date=August 2010}}
Early interoperability among independent systems included:
* [[ARPANET]], a forerunner of the Internet, defined protocols for dissimilar computers to exchange email.
* [[uucp]] implementations for Unix systems, and later for other operating systems, that only had dial-up communications available.
* [[CSNET]], which initially used the UUCP protocols via dial-up to provide networking and mail-relay services for non-ARPANET hosts.
* Action Technologies developed the [[Message Handling System]] (MHS) protocol (later bought by [[Novell]],&lt;ref&gt;[https://books.google.com/books?id=vxcEAAAAMBAJ&amp;pg=PA64 "Delivering the Enterprise Message], 19 Sep 1994, Daniel Blum, Network World&lt;/ref&gt;&lt;ref&gt;[http://www.networkworld.com/archive/1994/94-03-07hot_.html ''"...offers improved performance, greater reliability and much more flexibility in everything from communications hardware to scheduling..."''], 03/07/94, Mark Gibbs,
Network World&lt;/ref&gt;&lt;ref&gt;{{cite web | url = http://support.microsoft.com/kb/118859 | title = MHS: Correct Addressing format to DaVinci Email via MHS | work = Microsoft Support Knowledge Base | accessdate = 2007-01-15 }}&lt;/ref&gt; which abandoned it after purchasing the non-MHS WordPerfect Office&amp;mdash;renamed [[Novell GroupWise|Groupwise]]).
* [[HP OpenMail]] was known for its ability to interconnect several other APIs and protocols, including MAPI, cc:Mail, SMTP/MIME, and X.400.
* Soft-Switch released its eponymous email gateway product in 1984, acquired by [[Lotus Software]] ten years later.&lt;ref&gt;https://www.linkedin.com/in/nickshelness&lt;/ref&gt;
* The [[Coloured Book protocols]] ran on [[United Kingdom|UK]] academic networks until 1992.
* [[X.400]] in the 1980s and early 1990s was promoted by major vendors, and mandated for government use under [[GOSIP]], but abandoned by all but a few in favor of [[Internet]] [[Simple Mail Transfer Protocol|SMTP]] by the mid-1990s.

===From SNDMSG to MSG===
In the early 1970s, [[Ray Tomlinson]] updated an existing utility called [[SNDMSG]] so that it could copy messages (as files) over the network. [[Lawrence Roberts (scientist)|Lawrence Roberts]], the project manager for the ARPANET development, took the idea of READMAIL, which dumped all "recent" messages onto the user's terminal, and wrote a programme for [[TOPS-20#TENEX|TENEX]] in [[Text Editor and Corrector|TECO]] macros called ''RD'', which permitted access to individual messages.&lt;ref name="livinginternet1"&gt;{{cite web|url=http://www.livinginternet.com/e/ei.htm |title=Email History |publisher=Livinginternet.com |date=1996-05-13 |accessdate=2014-01-09}}&lt;/ref&gt; Barry Wessler then updated RD and called it ''NRD''.&lt;ref&gt;* {{Cite journal|last=Partridge|first=Craig|title=The Technical Development of Internet Email|journal=IEEE Annals of the History of Computing|volume=30|issue=2|publisher=IEEE Computer Society|location=Berlin|date=April–June 2008|url=http://www.ir.bbn.com/~craig/email.pdf|format=PDF | doi =  10.1109/mahc.2008.32|pages=3–29}}&lt;/ref&gt;

Marty Yonke rewrote NRD to include reading, access to SNDMSG for sending, and a help system, and called the utility ''WRD'', which was later known as ''BANANARD''. John Vittal then updated this version to include three important commands: ''Move'' (combined save/delete command), ''Answer'' (determined to whom a reply should be sent) and ''Forward'' (sent an email to a person who was not already a recipient). The system was called ''MSG''. With inclusion of these features, MSG is considered to be the first integrated modern email programme, from which many other applications have descended.&lt;ref name="livinginternet1"/&gt;

===ARPANET mail===
Experimental email transfers between separate computer systems began shortly after the creation of the [[ARPANET]] in 1969.&lt;ref name="thvv" /&gt; [[Ray Tomlinson]] is generally credited as having sent the first email across a network, initiating the use of the "[[At sign|@]]" sign to separate the names of the user and the user's machine in 1971, when he sent a message from one [[Digital Equipment Corporation]] [[DEC-10]] computer to another DEC-10. The two machines were placed next to each other.&lt;ref name="firstnetworkemail"/&gt;&lt;ref&gt;Wave New World,Time Magazine, October 19, 2009, p.48&lt;/ref&gt; Tomlinson's work was quickly adopted across the ARPANET, which significantly increased the popularity of email. Tomlinson is internationally known as the inventor of modern email.&lt;ref&gt;{{cite web|url=http://www.npr.org/2016/03/06/469428062/ray-tomlinson-inventor-of-modern-email-has-died|title=Ray Tomlinson, Inventor Of Modern Email, Dies|date=6 March 2016|work=NPR.org}}&lt;/ref&gt;

Initially addresses were of the form, ''username@hostname''&lt;ref&gt;RFC 805, 8 February 1982, Computer Mail Meeting Notes&lt;/ref&gt; but were extended to "username@host.domain" with the development of the [[Domain Name System]] (DNS).

As the influence of the ARPANET spread across academic communities, [[Gateway (telecommunications)|gateways]] were developed to pass mail to and from other networks such as [[CSNET]], [[JANET NRS|JANET]], [[BITNET]], [[X.400]], and [[FidoNet]]. This often involved addresses such as:
:hubhost!middlehost!edgehost!user@uucpgateway.somedomain.example.com
which routes mail to a user with a "[[UUCP#Mail routing|bang path]]" address at a UUCP host.

==Operation==
The diagram to the right shows a typical sequence of events&lt;ref&gt;{{cite video|title=How E-mail Works|medium=internet video|publisher=howstuffworks.com|year=2008|url=http://www.webcastr.com/videos/informational/how-email-works.html}}&lt;/ref&gt; that takes place when sender [[Placeholder names in cryptography|Alice]] transmits a message using a [[E-mail client|mail user agent]] (MUA) addressed to the [[email address]] of the recipient.
&lt;span style="float:right"&gt;[[File:email.svg|400px|Email operation]]&lt;/span&gt;
# The MUA formats the message in email format and uses the submission protocol, a profile of the [[Simple Mail Transfer Protocol]] (SMTP), to send the message to the local [[mail submission agent]] (MSA), in this case ''smtp.a.org''.
# The MSA determines the destination address provided in the SMTP protocol (not from the message header), in this case ''bob@b.org''. The part before the @ sign is the ''local part'' of the address, often the [[username]] of the recipient, and the part after the @ sign is a [[domain name]]. The MSA resolves a domain name to determine the [[fully qualified domain name]] of the [[Message transfer agent|mail server]] in the [[Domain Name System]] (DNS).
# The [[DNS server]] for the domain ''b.org'' (''ns.b.org'') responds with any [[MX record]]s listing the mail exchange servers for that domain, in this case ''mx.b.org'', a [[message transfer agent]] (MTA) server run by the recipient's ISP.&lt;ref&gt;[https://dnsdb.cit.cornell.edu/explain_mx.html "MX Record Explanation"], it.cornell.edu&lt;/ref&gt;
# smtp.a.org sends the message to mx.b.org using SMTP. This server may need to forward the message to other MTAs before the message reaches the final [[message delivery agent]] (MDA).
# The MDA delivers it to the [[Email Mailbox|mailbox]] of user ''bob''.
# Bob's MUA picks up the message using either the [[Post Office Protocol]] (POP3) or the [[Internet Message Access Protocol]] (IMAP).

In addition to this example, alternatives and complications exist in the email system:
* Alice or Bob may use a client connected to a corporate email system, such as [[IBM]] [[Lotus Notes]] or [[Microsoft]] [[Microsoft Exchange Server|Exchange]]. These systems often have their own internal email format and their clients typically communicate with the email server using a vendor-specific, proprietary protocol. The server sends or receives email via the Internet through the product's Internet mail gateway which also does any necessary reformatting. If Alice and Bob work for the same company, the entire transaction may happen completely within a single corporate email system.
* Alice may not have a MUA on her computer but instead may connect to a [[webmail]] service.
* Alice's computer may run its own MTA, so avoiding the transfer at step 1.
* Bob may pick up his email in many ways, for example logging into mx.b.org and reading it directly, or by using a webmail service.
* Domains usually have several mail exchange servers so that they can continue to accept mail even if the primary is not available.

Many MTAs used to accept messages for any recipient on the Internet and do their best to deliver them. Such MTAs are called ''[[open mail relay]]s''. This was very important in the early days of the Internet when network connections were unreliable.{{citation needed|date=July 2015}}  However, this mechanism proved to be exploitable by originators of [[email spam|unsolicited bulk email]] and as a consequence open mail relays have become rare,&lt;ref name="IMCR-016"&gt;{{cite web|url=http://www.imc.org/ube-relay.html |title=Allowing Relaying in SMTP: A Series of Surveys |accessdate=2008-04-13 |last=Hoffman |first=Paul |date=2002-08-20 |work=IMC Reports |publisher=[[Internet Mail Consortium]] |archiveurl=https://web.archive.org/web/20070118121843/http://www.imc.org/ube-relay.html |archivedate=2007-01-18 }}&lt;/ref&gt; and many MTAs do not accept messages from open mail relays.

==Message format {{anchor|Internet Message Format}}==
The Internet email message format is now defined by RFC 5322, with multimedia content attachments being defined in RFC 2045 through RFC 2049, collectively called ''[[Multipurpose Internet Mail Extensions]]'' or ''MIME''. RFC 5322 replaced the earlier RFC 2822 in 2008, and in turn RFC 2822 in 2001 replaced RFC 822 – which had been the standard for Internet email for nearly 20 years. Published in 1982, RFC 822 was based on the earlier RFC 733 for the [[ARPANET]].&lt;ref&gt;{{cite web|first=Ken|last=Simpson|title=An update to the email standards|date=October 3, 2008|publisher=MailChannels Blog Entry|url= http://blog.mailchannels.com/2008/10/update-to-email-standards.html}}&lt;/ref&gt;

Internet email messages consist of two major sections, the message header and the message body.  The header is structured into [[Field (computer science)|fields]] such as From, To, CC, Subject, Date, and other information about the email. In the process of transporting email messages between systems, SMTP communicates delivery parameters and information using message header fields. The body contains the message, as unstructured text, sometimes containing a [[signature block]] at the end. The header is separated from the body by a blank line.

===Message header===
&lt;!-- This section is linked from [[Bracket]] --&gt;
Each message has exactly one [[Header (computing)|header]], which is structured into [[Field (computer science)|fields]]. Each field has a name and a value. RFC 5322 specifies the precise syntax.

Informally, each line of text in the header that begins with a [[Printable characters|printable character]] begins a separate field. The field name starts in the first character of the line and ends before the separator character ":". The separator is then followed by the field value (the "body" of the field). The value is continued onto subsequent lines if those lines have a space or tab as their first character. Field names and values are restricted to 7-bit [[ASCII]] characters. Non-ASCII values may be represented using MIME [[MIME#Encoded-Word|encoded words]].

====Header fields====
Email header fields can be multi-line, and each line should be at most 78 characters long and in no event more than 998 characters long.&lt;ref&gt;{{cite web|url=https://tools.ietf.org/html/rfc5322|title=RFC 5322, Internet Message Format|author=P. Resnick, Ed.|date=October 2008|publisher=IETF}}&lt;/ref&gt; Header fields defined by RFC 5322 can only contain [[US-ASCII]] characters; for encoding characters in other sets, a syntax specified in RFC 2047 can be used.&lt;ref&gt;{{cite web|last=Moore|first=K|title=MIME (Multipurpose Internet Mail Extensions) Part Three: Message Header Extensions for Non-ASCII Text|url=https://tools.ietf.org/html/rfc2047|publisher=[[Internet Engineering Task Force|IETF]]|accessdate=2012-01-21| date=November 1996 }}&lt;/ref&gt; Recently the IETF EAI working group has defined some standards track extensions,&lt;ref&gt;{{cite web|url=https://tools.ietf.org/html/rfc6532|title=RFC 6532, Internationalized Email Headers|author=A Yang, Ed.|date=February 2012|publisher=IETF|ISSN=2070-1721}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://tools.ietf.org/html/rfc6531|title=RFC 6531, SMTP Extension for Internationalized Email Addresses|author=J. Yao, Ed., W. Mao, Ed.|date=February 2012|publisher=IETF|ISSN=2070-1721}}&lt;/ref&gt; replacing previous experimental extensions, to allow [[UTF-8]] encoded [[Unicode]] characters to be used within the header. In particular, this allows email addresses to use non-ASCII characters. Such addresses are supported by Google and Microsoft products, and promoted by some governments.&lt;ref name="economictimes.indiatimes.com"&gt;{{Cite news|url=http://economictimes.indiatimes.com/tech/internet/now-get-your-email-address-in-hindi/articleshow/53830034.cms|title=Now, get your email address in Hindi - The Economic Times|newspaper=The Economic Times|access-date=2016-10-17}}&lt;/ref&gt;

The message header must include at least the following fields:&lt;ref&gt;{{cite web|url=https://tools.ietf.org/html/rfc5322#section-3.6 |title=RFC 5322, 3.6. Field Definitions |publisher=Tools.ietf.org |date=October 2008|accessdate=2014-01-09}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://tools.ietf.org/html/rfc5322#section-3.6.4 |title=RFC 5322, 3.6.4. Identification Fields |publisher=Tools.ietf.org |date=October 2008|accessdate=2014-01-09}}&lt;/ref&gt;
* ''From'': The [[email address]], and optionally the name of the author(s). In many email clients not changeable except through changing account settings.
* ''Date'': The local time and date when the message was written. Like the ''From:'' field, many email clients fill this in automatically when sending. The recipient's client may then display the time in the format and time zone local to him/her.

RFC 3864 describes registration procedures for message header fields at the [[Internet Assigned Numbers Authority|IANA]]; it provides for [http://www.iana.org/assignments/message-headers/perm-headers.html permanent] and [http://www.iana.org/assignments/message-headers/prov-headers.html provisional] field names, including also fields defined for MIME, netnews, and HTTP, and referencing relevant RFCs. Common header fields for email include:&lt;ref&gt;{{cite web|url=https://tools.ietf.org/html/rfc5064 |title=RFC 5064 |publisher=Tools.ietf.org|date=December 2007 |accessdate=2014-01-09}}&lt;/ref&gt;

* ''To'': The email address(es), and optionally name(s) of the message's recipient(s). Indicates primary recipients (multiple allowed), for secondary recipients see Cc: and Bcc: below.
* ''Subject'': A brief summary of the topic of the message. [[E-mail subject abbreviations|Certain abbreviations]] are commonly used in the subject, including [[E-mail subject abbreviations|"RE:" and "FW:"]].
* ''Cc'': [[Carbon copy]]; Many email clients will mark email in one's inbox differently depending on whether they are in the To: or Cc: list. (''Bcc'': [[Blind carbon copy]]; addresses are usually only specified during SMTP delivery, and not usually listed in the message header.)
* [[Content-Type]]: Information about how the message is to be displayed, usually a [[MIME]] type.
* ''Precedence'': commonly with values "bulk", "junk", or "list"; used to indicate that automated "vacation" or "out of office" responses should not be returned for this mail, e.g. to prevent vacation notices from being sent to all other subscribers of a mailing list. [[Sendmail]] uses this field to affect prioritization of queued email, with "Precedence: special-delivery" messages delivered sooner. With modern high-bandwidth networks, delivery priority is less of an issue than it once was. [[Microsoft Exchange Server|Microsoft Exchange]] respects a fine-grained automatic response suppression mechanism, the ''X-Auto-Response-Suppress'' field.&lt;ref&gt;Microsoft, Auto Response Suppress, 2010, [http://msdn.microsoft.com/en-us/library/ee219609(v=EXCHG.80).aspx microsoft reference], 2010 Sep 22&lt;/ref&gt;
* ''Message-ID'': Also an automatically generated field; used to prevent multiple delivery and for reference in In-Reply-To: (see below).
* ''In-Reply-To'': [[Message-ID]] of the message that this is a reply to. Used to link related messages together. This field only applies for reply messages.
* ''References'': [[Message-ID]] of the message that this is a reply to, and the message-id of the message the previous reply was a reply to, etc.
* ''Reply-To'': Address that should be used to reply to the message.
* ''Sender'': Address of the actual sender acting on behalf of the author listed in the From: field (secretary, list manager, etc.).
* ''Archived-At'': A direct link to the archived form of an individual email message.

Note that the ''To:'' field is not necessarily related to the addresses to which the message is delivered. The actual delivery list is supplied separately to the transport protocol, [[Simple Mail Transfer Protocol|SMTP]], which may or may not originally have been extracted from the header content. The "To:" field is similar to the addressing at the top of a conventional letter which is delivered according to the address on the outer envelope. In the same way, the "From:" field does not have to be the real sender of the email message. Some mail servers apply [[email authentication]] systems to messages being relayed. Data pertaining to server's activity is also part of the header, as defined below.

SMTP defines the ''trace information'' of a message, which is also saved in the header using the following two fields:&lt;ref&gt;{{cite IETF|title=Simple Mail Transfer Protocol|rfc=5321|author=[[John Klensin]]|sectionname=Trace Information|section=4.4| date=October 2008 |publisher=[[Internet Engineering Task Force|IETF]]}}&lt;/ref&gt;
* ''Received'': when an SMTP server accepts a message it inserts this trace record at the top of the header (last to first).
* ''Return-Path'': when the delivery SMTP server makes the ''final delivery'' of a message, it inserts this field at the top of the header.

Other fields that are added on top of the header by the receiving server may be called ''trace fields'', in a broader sense.&lt;ref&gt;{{cite web|url=http://www.ietf.org/mail-archive/web/apps-discuss/current/msg04115.html|title=Trace headers|author=John Levine|date=14 January 2012|work=email message|publisher=[[Internet Engineering Task Force|IETF]]|accessdate=16 January 2012|quote=there are many more trace fields than those two}}&lt;/ref&gt;
* ''Authentication-Results'': when a server carries out authentication checks, it can save the results in this field for consumption by downstream agents.&lt;ref&gt;This extensible field is defined by RFC 7001, that also defines an [[Internet Assigned Numbers Authority|IANA]] registry of [http://www.iana.org/assignments/email-auth/ Email Authentication Parameters].&lt;/ref&gt;
* ''Received-SPF'': stores results of [[Sender Policy Framework|SPF]] checks in more detail than Authentication-Results.&lt;ref&gt;RFC 7208.&lt;/ref&gt;
* ''Auto-Submitted'': is used to mark automatically generated messages.&lt;ref&gt;Defined in RFC 3834, and updated by RFC 5436.&lt;/ref&gt;
* ''VBR-Info'': claims [[Vouch by Reference|VBR]] whitelisting&lt;ref&gt;RFC 5518.&lt;/ref&gt;

===Message body===

====Content encoding====
Email was originally designed for 7-bit [[ASCII]].&lt;ref&gt;{{cite book|title=TCP/IP Network Administration|year=2002|isbn=978-0-596-00297-8|author=Craig Hunt|publisher=[[O'Reilly Media]]|page=70}}&lt;/ref&gt; Most email software is [[8-bit clean]] but must assume it will communicate with 7-bit servers and mail readers. The [[MIME]] standard introduced character set specifiers and two content transfer encodings to enable transmission of non-ASCII data: [[quoted printable]] for mostly 7-bit content with a few characters outside that range and [[base64]] for arbitrary binary data. The [[8BITMIME]] and [[BINARY]] extensions were introduced to allow transmission of mail without the need for these encodings, but many [[mail transport agent]]s still do not support them fully. In some countries, several encoding schemes coexist; as the result, by default, the message in a non-Latin alphabet language appears in non-readable form (the only exception is coincidence, when the sender and receiver use the same encoding scheme). Therefore, for international [[character set]]s, [[Unicode]] is growing in popularity.{{citation needed|date=September 2014}}

====Plain text and HTML====
Most modern graphic [[email client]]s allow the use of either [[plain text]] or [[HTML#HTML email|HTML]] for the message body at the option of the user. [[HTML email]] messages often include an automatically generated plain text copy as well, for compatibility reasons. Advantages of HTML include the ability to include in-line links and images, set apart previous messages in [[block quote]]s, wrap naturally on any display, use emphasis such as [[underline]]s and [[italics]], and change [[font]] styles. Disadvantages include the increased size of the email, privacy concerns about [[web bug]]s, abuse of HTML email as a vector for [[phishing]] attacks and the spread of [[malware|malicious software]].&lt;ref&gt;{{cite web|title=Email policies that prevent viruses|url=http://advosys.ca/papers/mail-policies.html}}&lt;/ref&gt;

Some web-based [[mailing list]]s recommend that all posts be made in plain-text, with 72 or 80 [[characters per line]]&lt;ref&gt;{{cite web|url=http://helpdesk.rootsweb.com/listadmins/plaintext.html |title=When posting to a RootsWeb mailing list... |publisher=Helpdesk.rootsweb.com |accessdate=2014-01-09}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.openbsd.org/mail.html |title=...Plain text, 72 characters per line... |publisher=Openbsd.org |accessdate=2014-01-09}}&lt;/ref&gt; for all the above reasons, but also because they have a significant number of readers using [[List of email clients#Text-based|text-based email clients]] such as [[Mutt (email client)|Mutt]]. Some [[Microsoft]] [[email client]]s allow rich formatting using their proprietary [[Rich Text Format]] (RTF), but this should be avoided unless the recipient is guaranteed to have a compatible [[email client]].&lt;ref&gt;{{cite web|url=http://support.microsoft.com/kb/138053 |title=How to Prevent the Winmail.dat File from Being Sent to Internet Users |publisher=Support.microsoft.com |date=2010-07-02 |accessdate=2014-01-09}}&lt;/ref&gt;

==Servers and client applications==
[[File:Mozilla Thunderbird 3.1.png|thumb|right|300px|The interface of an email client, [[Mozilla Thunderbird|Thunderbird]].]]
&lt;!-- This section is linked from [[Catch-all (Mail)]]. See [[WP:MOS#Section management]] --&gt;
Messages are exchanged between hosts using the [[Simple Mail Transfer Protocol]] with software programs called [[mail transfer agent]]s (MTAs); and delivered to a mail store by programs called [[mail delivery agent]]s (MDAs, also sometimes called local delivery agents, LDAs). Accepting a message obliges an MTA to deliver it,&lt;ref&gt;In practice, some accepted messages may nowadays not be delivered to the recipient's InBox, but instead to a Spam or Junk folder which, especially in a corporate environment, may be inaccessible to the recipient&lt;/ref&gt; and when a message cannot be delivered, that MTA must send a [[bounce message]] back to the sender, indicating the problem.

Users can retrieve their messages from servers using standard protocols such as [[Post Office Protocol|POP]] or [[IMAP]], or, as is more likely in a large [[corporation|corporate]] environment, with a [[Proprietary software|proprietary]] protocol specific to [[Novell Groupwise]], [[Lotus Notes]] or [[Microsoft Exchange Server]]s.  Programs used by users for retrieving, reading, and managing email are called [[mail user agent]]s (MUAs).

Mail can be stored on the [[client (computing)|client]], on the [[Server (computing)|server]] side, or in both places. Standard formats for mailboxes include [[Maildir]] and [[mbox]]. Several prominent email clients use their own proprietary format and require conversion software to transfer email between them. Server-side storage is often in a proprietary format but since access is through a standard protocol such as [[IMAP]], moving email from one server to another can be done with any [[Mail user agent|MUA]] supporting the protocol.

Many current email users do not run MTA, MDA or MUA programs themselves, but use a web-based email platform, such as Gmail, Hotmail, or Yahoo! Mail, that performs the same tasks.&lt;ref&gt;http://dir.yahoo.com/business_and_economy/business_to_business/communications_and_networking/internet_and_world_wide_web/email_providers/free_email/&lt;/ref&gt; Such [[webmail]] interfaces allow users to access their mail with any standard [[web browser]], from any computer, rather than relying on an email client.

===Filename extensions===
Upon reception of email messages, [[email client]] applications save messages in operating system files in the file system. Some clients save individual messages as separate files, while others use various database formats, often proprietary, for collective storage. A historical standard of storage is the ''[[mbox]]'' format. The specific format used is often indicated by special [[filename extension]]s:
;&lt;tt&gt;eml&lt;/tt&gt;
:Used by many email clients including [[Novell GroupWise]], [[Microsoft Outlook Express]], [[Lotus notes]], [[Windows Mail]], [[Mozilla Thunderbird]], and Postbox. The files are [[plain text]] in [[MIME]] format, containing the email header as well as the message contents and attachments in one or more of several formats.
;&lt;tt&gt;emlx&lt;/tt&gt;
:Used by [[Apple Mail]].
;&lt;tt&gt;msg&lt;/tt&gt;
:Used by [[Microsoft Outlook|Microsoft Office Outlook]] and [[OfficeLogic|OfficeLogic Groupware]].
;&lt;tt&gt;mbx&lt;/tt&gt;
:Used by [[Opera Mail]], [[KMail]], and [[Apple Mail]] based on the [[mbox]] format.

Some applications (like [[Apple Mail]]) leave attachments encoded in messages for searching while also saving separate copies of the attachments. Others separate attachments from messages and save them in a specific directory.

===URI scheme mailto===
{{main article|mailto}}
The [[URI scheme]], as registered with the [[Internet Assigned Numbers Authority|IANA]], defines the &lt;tt&gt;mailto:&lt;/tt&gt; scheme for SMTP email addresses. Though its use is not strictly defined, URLs of this form are intended to be used to open the new message window of the user's mail client when the URL is activated, with the address as defined by the URL in the ''To:'' field.&lt;ref&gt;RFC 2368 section 3 : by Paul Hoffman in 1998 discusses operation of the "mailto" URL.&lt;/ref&gt;

==Types==

===Web-based email===
{{main article|Webmail}}
Many email providers have a web-based email client (e.g. [[AOL Mail]], [[Gmail]], [[Outlook.com]], [[Hotmail]] and [[Yahoo! Mail]]). This allows users to log in to the email account by using any compatible [[web browser]] to send and receive their email. Mail is typically not downloaded to the client, so can't be read without a current Internet connection.

===POP3 email services===
The [[Post Office Protocol]] 3 (POP3) is a mail access protocol used by a client application to read messages from the mail server. Received messages are often deleted from the [[server (computing)|server]]. POP supports simple download-and-delete requirements for access to remote mailboxes (termed maildrop in the POP RFC's).&lt;ref name="Windows to Linux"&gt;{{cite book | last = Allen | first = David | title = Windows to Linux | publisher = Prentice Hall | year = 2004 | location = | page =192 | url = https://books.google.com/books?id=UD0h_GqgbHgC&amp;printsec=frontcover&amp;dq=network%2B+guide+to+networks&amp;hl=en&amp;src=bmrr&amp;ei=hMnATfmmA8j00gGMsOC2Cg&amp;sa=X&amp;oi=book_result&amp;ct=result&amp;resnum=1&amp;ved=0CE8Q6AEwAA#v=onepage&amp;q&amp;f=false}}&lt;/ref&gt;

===IMAP email servers===
The [[Internet Message Access Protocol]] (IMAP) provides features to manage a mailbox from multiple devices. Small portable devices like [[smartphone]]s are increasingly used to check email while travelling, and to make brief replies, larger devices with better keyboard access being used to reply at greater length. IMAP shows the headers of messages, the sender and the subject and the device needs to request to download specific messages. Usually mail is left in folders in the mail server.

===MAPI email servers===
[[MAPI|Messaging Application Programming Interface]] (MAPI) is used by [[Microsoft Outlook]] to communicate to [[Microsoft Exchange Server]] - and to a range of other email server products such as [[Axigen|Axigen Mail Server]], [[Kerio Connect]], [[Scalix]], [[Zimbra]], [[HP OpenMail]], [[IBM Lotus Notes]], [[Zarafa (software)|Zarafa]], and [[Bynari]] where vendors have added MAPI support to allow their products to be accessed directly via Outlook.

==Uses==
{{Refimprove section|date=November 2007}}

===Business and organizational use===
Email has been widely accepted by business, governments and non-governmental organizations in the developed world, and it is one of the key parts of an 'e-revolution' in workplace communication (with the other key plank being widespread adoption of highspeed [[Internet]]). A sponsored 2010 study on workplace communication found 83% of U.S. knowledge workers felt email was critical to their success and productivity at work.&lt;ref name=om&gt;By Om Malik, GigaOm. "[http://gigaom.com/collaboration/is-email-a-curse-or-a-boon/ Is Email a Curse or a Boon?]" September 22, 2010. Retrieved October 11, 2010.&lt;/ref&gt;

It has some key benefits to business and other organizations, including:
; Facilitating logistics
: Much of the business world relies on communications between people who are not physically in the same building, area, or even country; setting up and attending an in-person meeting, [[telephone call]], or [[conference call]] can be inconvenient, time-consuming, and costly. Email provides a method of exchanging information between two or more people with no set-up costs and that is generally far less expensive than a physical meeting or phone call.
; Helping with synchronisation
: With [[Real-time computing|real time]] communication by meetings or phone calls, participants must work on the same schedule, and each participant must spend the same amount of time in the meeting or call. Email allows [[wikt:asynchrony|asynchrony]]: each participant may control their schedule independently.
; Reducing cost
: Sending an email is much less expensive than sending postal mail, or [[long distance telephone call]]s, [[telex]] or [[telegrams]].
; Increasing speed
: Much faster than most of the alternatives.
; Creating a "written" record
: Unlike a telephone or in-person conversation, email by its nature creates a detailed written record of the communication, the identity of the sender(s) and recipient(s) and the date and time the message was sent. In the event of a contract or legal dispute, saved emails can be used to prove that an individual was advised of certain issues, as each email has the date and time recorded on it.

====Email marketing====
Email marketing via "[[opt-in email|opt-in]]" is often successfully used to send special sales offerings and new product information.&lt;ref name=brett&gt;{{cite journal | last1 = Martin | first1 = Brett A. S. | last2 = Van Durme | first2 = Joel | last3 = Raulas | first3 = Mika | last4 = Merisavo | first4 = Marko | year = 2003 | title = E-mail Marketing: Exploratory Insights from Finland | url = http://www.basmartin.com/wp-content/uploads/2010/08/Martin-et-al-2003.pdf | format = PDF | journal = Journal of Advertising Research | volume = 43 | issue = 3| pages = 293–300 | doi=10.1017/s0021849903030265}}&lt;/ref&gt; Depending on the recipient's culture,&lt;ref&gt;{{cite web|url=http://www.computerworld.com/article/2467778/endpoint-security/spam-culture--part-1--china.html|title=Spam culture, part 1: China|first=Amir|last=Lev|publisher=}}&lt;/ref&gt; email sent without permission&amp;mdash;such as an "opt-in"&amp;mdash;is likely to be viewed as unwelcome "[[email spam]]".

===Personal use===

====Desktop====
Many users access their personal email from friends and family members using a [[desktop computer]] in their house or apartment.

====Mobile====
Email has become widely used on [[smartphone]]s and [[Wi-Fi]]-enabled [[laptop]]s and [[tablet computer]]s. Mobile "apps" for email increase accessibility to the medium for users who are out of their home. While in the earliest years of email, users could only access email on desktop computers, in the 2010s, it is possible for users to check their email when they are away from home, whether they are across town or across the world. Alerts can also be sent to the smartphone or other device to notify them immediately of new messages. This has given email the ability to be used for more frequent communication between users and allowed them to check their email and write messages throughout the day. Today, there are an estimated 1.4 billion email users worldwide and 50 billion non-spam emails that are sent daily.

Individuals often check email on smartphones for both personal and work-related messages. It was found that US adults check their email more than they browse the web or check their [[Facebook]] accounts, making email the most popular activity for users to do on their smartphones. 78% of the respondents in the study revealed that they check their email on their phone.&lt;ref&gt;{{cite web|url=http://marketingland.com/smartphone-activities-study-email-web-facebook-37954|title=Email Is Top Activity On Smartphones, Ahead Of Web Browsing &amp; Facebook [Study]|date=28 March 2013|publisher=}}&lt;/ref&gt; It was also found that 30% of consumers use only their smartphone to check their email, and 91% were likely to check their email at least once per day on their smartphone. However, the percentage of consumers using email on smartphone ranges and differs dramatically across different countries. For example, in comparison to 75% of those consumers in the US who used it, only 17% in India did.&lt;ref&gt;{{cite web|url=http://www.emailmonday.com/mobile-email-usage-statistics|title=The ultimate mobile email statistics overview|publisher=}}&lt;/ref&gt;

==Issues==
{{Refimprove section|date=October 2016}}

===Attachment size limitation===
{{Main article|Email attachment}}
Email messages may have one or more attachments, which are additional files that are appended to the email. Typical attachments include [[Microsoft Word]] documents, [[pdf]] documents and scanned images of paper documents. In principle there is no technical restriction on the size or number of attachments, but in practice email clients, [[server (computing)|server]]s and Internet service providers implement various limitations on the size of files, or complete email - typically to 25MB or less.&lt;ref&gt;[http://exchangepedia.com/2007/09/exchange-server-2007-setting-message-size-limits.html ''"Setting Message Size Limits in Exchange 2010 and Exchange 2007"''].&lt;/ref&gt;&lt;ref&gt;[http://www.geek.com/articles/news/google-updates-file-size-limits-for-gmail-and-youtube-20090629/#ixzz0oIzFY0Q8 ''"Google updates file size limits for Gmail and YouTube"'', geek.com].&lt;/ref&gt;&lt;ref&gt;[http://mail.google.com/support/bin/answer.py?answer=8770&amp;topic=1517 ''"Maximum attachment size"'', mail.google,com].&lt;/ref&gt; Furthermore, due to technical reasons, attachment sizes as seen by these transport systems can differ to what the user sees,&lt;ref&gt;{{cite web|url=http://technet.microsoft.com/en-us/magazine/2009.01.exchangeqa.aspx?pr=blog|title=Exchange 2007: Attachment Size Increase,...|date=2010-03-25|publisher=TechNet Magazine, Microsoft.com US}}&lt;/ref&gt; which can be confusing to senders when trying to assess whether they can safely send a file by email. Where larger files need to be shared, [[file hosting service]]s of various sorts are available; and generally suggested.&lt;ref&gt;[https://support.office.com/en-us/article/Send-large-files-to-other-people-7005da19-607a-47d5-b2c5-8f3982c6cc83 "Send large files to other people"], Microsoft.com&lt;/ref&gt;&lt;ref&gt;[http://www.makeuseof.com/tag/8-ways-to-email-large-attachments/ "8 ways to email large attachments"], Chris Hoffman, December 21, 2012, makeuseof.com&lt;/ref&gt; Some large files, such as digital photos, color presentations and video or music files are too large for some email systems.

===Information overload===
The ubiquity of email for knowledge workers and "white collar" employees has led to concerns that recipients face an "[[information overload]]" in dealing with increasing volumes of email.&lt;ref&gt;{{cite web|last=Radicati|first=Sara|title=Email Statistics Report, 2010|url=http://www.radicati.com/wp/wp-content/uploads/2010/04/Email-Statistics-Report-2010-2014-Executive-Summary2.pdf}}&lt;/ref&gt;&lt;ref&gt;{{cite news|last=Gross|first=Doug|title=Happy Information Overload Day!|url=http://articles.cnn.com/2010-10-20/tech/information.overload.day_1_mails-marsha-egan-rss?_s=PM:TECH|work=CNN|date=July 26, 2011}}&lt;/ref&gt; This can lead to increased stress, decreased satisfaction with work, and some observers even argue it could have a significant negative economic effect,&lt;ref&gt;{{cite news|url=http://www.nytimes.com/2008/04/20/technology/20digi.html?_r=2&amp;oref=slogin&amp;oref=slogin|title=Struggling to Evade the E-Mail Tsunami|date=2008-04-20|publisher=The New York Times|first=Randall|last=Stross|accessdate=May 1, 2010}}&lt;/ref&gt; as efforts to read the many emails could reduce [[productivity]].

===Spam===
{{Main article|Email spam}}
Email "spam" is the term used to describe unsolicited bulk email. The low cost of sending such email meant that by 2003 up to 30% of total email traffic was already spam.&lt;ref&gt;[http://visionedgemarketing.com/growth-of-spam-email-2/ "Growth of Spam Email"]&lt;/ref&gt;&lt;ref name="R"&gt;Rich Kawanagh. The top ten email spam list of 2005. ITVibe news, 2006, January 02, [http://itvibe.com/news/3837/ ITvibe.com]&lt;/ref&gt;&lt;ref&gt;How Microsoft is losing the war on spam [http://dir.salon.com/story/tech/feature/2005/01/19/microsoft_spam/index.html Salon.com]&lt;/ref&gt; and was threatening the usefulness of email as a practical tool. The US [[CAN-SPAM Act of 2003]] and similar laws elsewhere&lt;ref&gt;Spam Bill 2003 ([http://www.aph.gov.au/library/pubs/bd/2003-04/04bd045.pdf PDF])&lt;/ref&gt; had some impact, and a number of effective [[anti-spam techniques (email)|anti-spam techniques]] now largely mitigate the impact of spam by filtering or rejecting it for most users,&lt;ref&gt;[http://www.wired.com/2015/07/google-says-ai-catches-99-9-percent-gmail-spam/ "Google Says Its AI Catches 99.9 Percent of Gmail Spam"], Cade Metz, July 09 2015, wired.com&lt;/ref&gt; but the volume sent is still very high&amp;mdash;and increasingly consists not of advertisements for products, but malicious content or links.&lt;ref name="securelist"&gt;[https://securelist.com/analysis/quarterly-spam-reports/74682/spam-and-phishing-in-q1-2016/ "Spam and phishing in Q1 2016"],  May 12, 2016, securelist.com&lt;/ref&gt;

===Malware===
A range of malicious email types exist. These range from [[List of email scams|various types of email scams]], including [[Social engineering (security)|"social engineering"]] scams such as [[advance-fee scam]] "Nigerian letters", to [[phishing]], [[email bomb]]ardment and [[Computer worm|email worms]].

===Email spoofing===
{{Main article|Email spoofing}}
[[Email spoofing]] occurs when the email message header is designed to make the message appear to come from a known or trusted source. [[Email spam]] and [[phishing]] methods typically use spoofing to mislead the recipient about the true message origin. Email spoofing may be done as a prank, or as part of a criminal effort to defraud an individual or organization. An example of a potentially fraudulent email spoofing is if an individual creates an email which appears to be an invoice from a major company, and then sends it to one or more recipients. In some cases, these fraudulent emails incorporate the logo of the purported organization and even the email address may appear legitimate.

===Email bombing===
{{main article|Email bomb}}

[[Email bomb]]ing is the intentional sending of large volumes of messages to a target address. The overloading of the target email address can render it unusable and can even cause the mail server to crash.

===Privacy concerns===
{{Main article|Internet privacy}}

Today it can be important to distinguish between Internet and internal email systems. Internet email may travel and be stored on networks and computers without the sender's or the recipient's control. During the transit time it is possible that third parties read or even modify the content. Internal mail systems, in which the information never leaves the organizational network, may be more secure, although [[information technology]] personnel and others whose function may involve monitoring or managing may be accessing the email of other employees.

Email privacy, without some security precautions, can be compromised because:
* email messages are generally not encrypted.
* email messages have to go through intermediate computers before reaching their destination, meaning it is relatively easy for others to intercept and read messages.
* many Internet Service Providers (ISP) store copies of email messages on their mail servers before they are delivered. The backups of these can remain for up to several months on their server, despite deletion from the mailbox.
* the "Received:"-fields and other information in the email can often identify the sender, preventing anonymous communication.

There are [[cryptography]] applications that can serve as a remedy to one or more of the above. For example, [[Virtual Private Network]]s or the [[Tor (anonymity network)|Tor anonymity network]] can be used to encrypt traffic from the user machine to a safer network while [[GNU Privacy Guard|GPG]], [[Pretty Good Privacy|PGP]], SMEmail,&lt;ref&gt;[http://www.arxiv.org/pdf/1002.3176 SMEmail – A New Protocol for the Secure E-mail in Mobile Environments], Proceedings of the Australian Telecommunications Networks and Applications Conference (ATNAC'08), pp. 39–44, Adelaide, Australia, Dec. 2008.&lt;/ref&gt; or [[S/MIME]] can be used for [[end-to-end principle|end-to-end]] message encryption, and SMTP STARTTLS or SMTP over [[Transport Layer Security]]/Secure Sockets Layer can be used to encrypt communications for a single mail hop between the SMTP client and the SMTP server.

Additionally, many [[mail user agent]]s do not protect logins and passwords, making them easy to intercept by an attacker. Encrypted authentication schemes such as [[Simple Authentication and Security Layer|SASL]] prevent this. Finally, attached files share many of the same hazards as those found in [[Peer-to-peer|peer-to-peer filesharing]]. Attached files may contain [[Trojan horse (computing)|trojans]] or [[Computer virus|viruses]].

===Flaming===
[[Flaming (Internet)|Flaming]] occurs when a person sends a message (or many messages) with angry or antagonistic content. The term is derived from the use of the word "incendiary" to describe particularly heated email discussions. The ease and impersonality of email communications mean that the [[social norms]] that encourage civility in person or via telephone do not exist and civility may be forgotten.&lt;ref&gt;{{cite journal|author1=S. Kiesler |author2=D. Zubrow |author3=A.M. Moses |author4=V. Geller |title=Affect in computer-mediated communication: an experiment in synchronous terminal-to-terminal discussion|journal=Human-Computer Interaction|volume=1|pages=77–104|year=1985|doi=10.1207/s15327051hci0101_3}}&lt;/ref&gt;

===Email bankruptcy===
{{main article|Email bankruptcy}}
Also known as "email fatigue", email bankruptcy is when a user ignores a large number of email messages after falling behind in reading and answering them. The reason for falling behind is often due to information overload and a general sense there is so much information that it is not possible to read it all. As a solution, people occasionally send a "boilerplate" message explaining that their email inbox is full, and that they are in the process of clearing out all the messages. [[Harvard University]] law professor [[Lawrence Lessig]] is credited with coining this term, but he may only have popularized it.&lt;ref&gt;{{cite news|title=All We Are Saying.|url=http://www.nytimes.com/2007/12/23/weekinreview/23buzzwords.html?ref=weekinreview|publisher=The New York Times|date=December 23, 2007|accessdate=2007-12-24|first=Grant|last=Barrett}}&lt;/ref&gt;

===Tracking of sent mail===
The original SMTP mail service provides limited mechanisms for tracking a transmitted message, and none for verifying that it has been delivered or read. It requires that each mail server must either deliver it onward or return a failure notice (bounce message), but both software bugs and system failures can cause messages to be lost. To remedy this, the [[Internet Engineering Task Force|IETF]] introduced [[Delivery Status Notification]]s (delivery receipts) and [[Return receipt#Email|Message Disposition Notifications]] (return receipts); however, these are not universally deployed in production. (A complete Message Tracking mechanism was also defined, but it never gained traction; see RFCs 3885&lt;ref&gt;RFC 3885, ''SMTP Service Extension for Message Tracking''&lt;/ref&gt; through 3888.&lt;ref&gt;RFC 3888, ''Message Tracking Model and Requirements''&lt;/ref&gt;)

Many ISPs now deliberately disable non-delivery reports (NDRs) and delivery receipts due to the activities of spammers:
* Delivery Reports can be used to verify whether an address exists and if so, this indicates to a spammer that it is available to be spammed.
* If the spammer uses a forged sender email address ([[email spoofing]]), then the innocent email address that was used can be flooded with NDRs from the many invalid email addresses the spammer may have attempted to mail. These NDRs then constitute spam from the ISP to the innocent user.

In the absence of standard methods, a range of system based around the use of [[web bug]]s have been developed. However, these are often seen as underhand or raising privacy concerns,&lt;ref&gt;{{cite news|url=http://query.nytimes.com/gst/fullpage.html?res=940CE0D9143AF931A15752C1A9669C8B63&amp;sec=&amp;spon=&amp;pagewanted=print|title=Software That Tracks E-Mail Is Raising Privacy Concerns|author=Amy Harmon|publisher=The New York Times|date=2000-11-22|accessdate=2012-01-13}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://email.about.com/od/emailbehindthescenes/a/html_return_rcp.htm |title=About.com |publisher=Email.about.com |date=2013-12-19 |accessdate=2014-01-09}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.webdevelopersnotes.com/tips/yahoo/notification-when-yahoo-email-is-opened.php |title=Webdevelopersnotes.com |publisher=Webdevelopersnotes.com |accessdate=2014-01-09}}&lt;/ref&gt; and only work with email clients that support rendering of HTML. Many mail clients now default to not showing "web content".&lt;ref&gt;[http://www.slipstick.com/outlook/email/microsoft-outlook-web-bugs-blocked-html-images "Outlook: Web Bugs &amp; Blocked HTML Images"], slipstick.com&lt;/ref&gt; [[Webmail]] providers can also disrupt web bugs by pre-caching images.&lt;ref&gt;[http://arstechnica.com/information-technology/2013/12/gmail-blows-up-e-mail-marketing-by-caching-all-images-on-google-servers/ "Gmail blows up e-mail marketing..."], Ron Amadeo, Dec 13 2013, Ars Technica&lt;/ref&gt;

==U.S. government==
The U.S. state and federal governments have been involved in electronic messaging and the development of email in several different ways. Starting in 1977, the U.S. Postal Service (USPS) recognized that electronic messaging and electronic transactions posed a significant threat to First Class mail volumes and revenue. The USPS explored an electronic messaging initiative in 1977 and later disbanded it. Twenty years later, in 1997, when email volume overtook postal mail volume, the USPS was again urged to embrace email, and the USPS declined to provide email as a service.&lt;ref&gt;{{cite web|url=http://www.fastcompany.com/1780716/can-technology-save-us-postal-service|title=Can Technology Save The U.S. Postal Service?|work=Fast Company}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://tech.mit.edu/V131/N60/emaillab.html|title=Can an MIT professor save the USPS? - The Tech|work=mit.edu}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.fedtechmagazine.com/article/2013/01/why-united-states-postal-service-taking-cues-silicon-valley|title=Why the USPS Is Taking Cues from Silicon Valley|work=FedTech}}&lt;/ref&gt; The USPS initiated an experimental email service known as [[E-COM]]. E-COM provided a method for the simple exchange of text messages. In 2011, shortly after the USPS reported its state of financial bankruptcy, the USPS Office of Inspector General (OIG) began exploring the possibilities of generating revenue through email servicing.&lt;ref&gt;{{cite web|url=http://cmsw.mit.edu/usps-can-save-itself/|title=Shiva Ayyadurai: USPS can save itself|work=MIT Comparative Media Studies/Writing}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://bostinno.streetwise.co/2012/01/13/could-email-save-snail-mail-or-is-the-internet-too-reliant-on-the-usps/|title=Could Email Save Snail Mail, Or Is The Internet Too Reliant on the USPS?|date=6 March 2012|work=BostInno}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.bostonglobe.com/lifestyle/2012/03/02/dear-usps/V4GJ8w9UCcfV4v0WiVjvmK/story.html|title=‘Dear USPS . . .’|work=BostonGlobe.com}}&lt;/ref&gt; Electronic messages were transmitted to a post office, printed out, and delivered as hard copy. To take advantage of the service, an individual had to transmit at least 200 messages. The delivery time of the messages was the same as First Class mail and cost 26 cents. Both the [[Postal Regulatory Commission]] and the [[Federal Communications Commission]] opposed E-COM. The FCC concluded that E-COM constituted common carriage under its jurisdiction and the USPS would have to file a [[tariff]].&lt;ref&gt;In re Request for declaratory ruling and investigation by Graphnet Systems, Inc., concerning the proposed E-COM service, FCC Docket No. 79-6 (September 4, 1979)&lt;/ref&gt; Three years after initiating the service, USPS canceled E-COM and attempted to sell it off.&lt;ref&gt;Hardy, Ian R; [https://archive.org/web/*/http:/www.ifla.org/documents/internet/hari1.txt The Evolution of ARPANET Email]; 1996-05-13; History Thesis Paper; University of California at Berkeley&lt;/ref&gt;&lt;ref&gt;James Bovard, The Law Dinosaur: The US Postal Service, CATO Policy Analysis (February 1985)&lt;/ref&gt;&lt;ref name="JayAkkad"&gt;{{cite web|url=http://www.cs.ucsb.edu/~almeroth/classes/F04.176A/homework1_good_papers/jay-akkad.html |title=Jay Akkad, The History of Email |publisher=Cs.ucsb.edu |accessdate=2014-01-09}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.gao.gov/archive/2000/gg00188.pdf |title=US Postal Service: Postal Activities and Laws Related to Electronic Commerce, GAO-00-188 |format=PDF |accessdate=2014-01-09}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://govinfo.library.unt.edu/ota/Ota_4/DATA/1982/8214.PDF |title=Implications of Electronic Mail and Message Systems for the U.S. Postal Service , Office of Technology Assessment, Congress of the United States, August 1982 |format=PDF |accessdate=2014-01-09}}&lt;/ref&gt;

The early ARPANET dealt with multiple email clients that had various, and at times incompatible, formats. For example, in the [[Multics]], the "@" sign meant "kill line" and anything before the "@" sign was ignored, so Multics users had to use a command-line option to specify the destination system.&lt;ref name="thvv"/&gt; The [[United States Department of Defense|Department of Defense]] [[DARPA]] desired to have uniformity and interoperability for email and therefore funded efforts to drive towards unified inter-operable standards. This led to David Crocker, John Vittal, Kenneth Pogran, and [[Austin Henderson]] publishing RFC 733, "Standard for the Format of ARPA Network Text Message" (November 21, 1977), a subset of which provided a stable base for common use on the ARPANET, but which was not fully effective, and in 1979, a meeting was held at BBN to resolve incompatibility issues. [[Jon Postel]] recounted the meeting in RFC 808, "Summary of Computer Mail Services Meeting Held at BBN on 10 January 1979" (March 1, 1982), which includes an appendix listing the varying email systems at the time. This, in turn, led to the release of David Crocker's RFC 822, "Standard for the Format of ARPA Internet Text Messages" (August 13, 1982).&lt;ref&gt;{{cite web|url=http://www.livinginternet.com/e/ei.htm |title=Email History, How Email was Invented, Living Internet |publisher=Livinginternet.com |date=1996-05-13 |accessdate=2014-01-09}}&lt;/ref&gt; RFC 822 is a small adaptation of RFC 733's details, notably enhancing the [[host (network)|host]] portion, to use [[Domain Name]]s, that were being developed at the same time.

The [[National Science Foundation]] took over operations of the ARPANET and Internet from the Department of Defense, and initiated [[NSFNet]], a new [[backbone network|backbone]] for the network. A part of the NSFNet AUP forbade commercial traffic.&lt;ref&gt;{{cite web|author=Robert Cannon |url=http://www.cybertelecom.org/notes/internet_history80s.htm |title=Internet History |publisher=Cybertelecom |accessdate=2014-01-09}}&lt;/ref&gt; In 1988, [[Vint Cerf]] arranged for an interconnection of [[MCI Mail]] with NSFNET on an experimental basis. The following year [[Compuserve]] email interconnected with NSFNET. Within a few years the commercial traffic restriction was removed from NSFNETs AUP, and NSFNET was privatised. In the late 1990s, the [[Federal Trade Commission]] grew concerned with fraud transpiring in email, and initiated a series of procedures on spam, fraud, and phishing.&lt;ref&gt;[http://www.cybertelecom.org/spam/Spamref.htm Cybertelecom : SPAM Reference] {{webarchive |url=https://web.archive.org/web/20140919090804/http://www.cybertelecom.org/spam/Spamref.htm |date=September 19, 2014 }}&lt;/ref&gt; In 2004, FTC jurisdiction over spam was codified into law in the form of the [[Can Spam Act|CAN SPAM Act.]]&lt;ref&gt;{{cite web|author=Robert Cannon |url=http://www.cybertelecom.org/spam/canspam.htm |title=Can Spam Act |publisher=Cybertelecom |accessdate=2014-01-09}}&lt;/ref&gt; Several other U.S. federal agencies have also exercised jurisdiction including the [[United States Department of Justice|Department of Justice]] and the [[United States Secret Service|Secret Service]]. NASA has provided email capabilities to astronauts aboard the Space Shuttle and International Space Station since 1991 when a [[Macintosh Portable]] was used aboard [[Space Shuttle]] mission [[STS-43]] to send the first email via [[AppleLink]].&lt;ref&gt;{{cite web|last=Cowing |first=Keith |url=http://www.spaceref.com/news/viewnews.html?id=213 |title=2001: A Space Laptop &amp;#124; SpaceRef – Your Space Reference |publisher=Spaceref.com |date=2000-09-18 |accessdate=2014-01-09}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.macobserver.com/columns/thisweek/2004/20040831.shtml |title=The Mac Observer – This Week in Apple History – August 22–31: "Welcome, IBM. Seriously," Too Late to License |publisher=Macobserver.com |date=2004-10-31 |accessdate=2014-01-09}}&lt;/ref&gt;&lt;ref&gt;{{cite book|last=Linzmayer|first=Owen W.|title=Apple confidential 2.0 : the definitive history of the world's most colorful company|year=2004|publisher=No Starch Press|location=San Francisco, Calif.|isbn=1-59327-010-0|edition=[Rev. 2. ed.].}}&lt;/ref&gt; Today astronauts aboard the International Space Station have email capabilities via the [[WI-FI|wireless networking]] throughout the station and are connected to the ground at 10 [[Mbit/s]] Earth to station and 3 Mbit/s station to Earth, comparable to home [[DSL]] connection speeds.&lt;ref name=issit&gt;{{cite news|title=First Tweet from Space|url=http://bits.blogs.nytimes.com/2010/01/22/first-tweet-from-space/|newspaper=The New York Times|first=Nick|last=Bilton|date=January 22, 2010}}&lt;/ref&gt;

==See also==
{{colbegin||22em}}
* [[Anonymous remailer]]
* [[Anti-spam techniques]]
* [[biff]]
* [[Bounce message]]
* [[Comparison of email clients]]
* [[Dark Mail Alliance]]
* [[Disposable email address]]
* [[E-card]]
* [[Electronic mailing list]]
* [[Email art]]
* [[Email authentication]]
* [[Email digest]]
* [[Email encryption]]
* [[Email hosting service]]
* [[Email storm]]
* [[Email tracking]]
* [[HTML email]]
* [[Information overload]]
* [[Internet fax]]
* [[Internet mail standard]]s
* [[List of email subject abbreviations]]
* [[MCI Mail]]
* [[Netiquette]]
* [[Posting style]]
* [[Privacy-enhanced Electronic Mail]]
* [[Push email]]
* [[RSS]]
* [[Telegraphy]]
* [[Unicode and email]]
* [[Usenet quoting]]
* [[Webmail]], [[Comparison of webmail providers]]
* [[X-Originating-IP]]
* [[X.400]]
* [[Yerkish]]
{{colend}}

==References==
{{Reflist|2}}

==Further reading==
{{refbegin}}
* Cemil Betanov, ''Introduction to X.400'', Artech House, ISBN 0-89006-597-7.
* Marsha Egan, "[http://www.inboxdetox.com Inbox Detox and The Habit of Email Excellence]", Acanthus Publishing ISBN 978-0-9815589-8-1
* Lawrence Hughes, ''Internet e-mail Protocols, Standards and Implementation'', Artech House Publishers, ISBN 0-89006-939-5.
* Kevin Johnson, ''Internet Email Protocols: A Developer's Guide'', Addison-Wesley Professional, ISBN 0-201-43288-9.
* Pete Loshin, ''Essential Email Standards: RFCs and Protocols Made Practical'', John Wiley &amp; Sons, ISBN 0-471-34597-0.
* {{Cite journal|last=Partridge|first=Craig|title=The Technical Development of Internet Email|journal=IEEE Annals of the History of Computing|volume=30|issue=2|publisher=IEEE Computer Society|location=Berlin|date=April–June 2008|url=http://www.ir.bbn.com/~craig/papers/email.pdf|format=PDF|issn=1934-1547|ref=harv|postscript={{inconsistent citations}} | doi =  10.1109/mahc.2008.32|pages=3–29}}
* Sara Radicati, ''Electronic Mail: An Introduction to the X.400 Message Handling Standards'', Mcgraw-Hill, ISBN 0-07-051104-7.
* John Rhoton, ''Programmer's Guide to Internet Mail: SMTP, POP, IMAP, and LDAP'', Elsevier, ISBN 1-55558-212-5.
* John Rhoton, ''X.400 and SMTP: Battle of the E-mail Protocols'', Elsevier, ISBN 1-55558-165-X.
* David Wood, ''Programming Internet Mail'', O'Reilly, ISBN 1-56592-479-7.
{{refend}}

==External links==
{{Wiktionary|email|outbox}}
* [http://www.iana.org/assignments/message-headers/perm-headers.html IANA's list of standard header fields]
* [http://emailhistory.org/ The History of Email] is Dave Crocker's attempt at capturing the sequence of 'significant' occurrences in the evolution of email; a collaborative effort that also cites this page.
* [http://www.multicians.org/thvv/mail-history.html The History of Electronic Mail] is a personal memoir by the implementer of an early email system
* [http://www.circleid.com/posts/20140903_a_look_at_the_origins_of_network_email/ A Look at the Origins of Network Email] is a short, yet vivid recap of the key historical facts
* [https://www.fbi.gov/news/stories/2015/august/business-e-mail-compromise Business E-Mail Compromise - An Emerging Global Threat], [[FBI]]
&lt;!-- please see http://en.wikipedia.org/wiki/WP:EL before adding links --&gt;
{{Computer-mediated communication}}
{{E-mail clients}}

{{Authority control}}

{{DEFAULTSORT:Email}}
[[Category:Email| ]]
[[Category:Internet terminology]]
[[Category:Electronic documents]]
[[Category:History of the Internet]]
[[Category:1971 introductions]]</text>
      <sha1>onsiytwc6yry2e1kntvpayw6jnutlbp</sha1>
    </revision>
  </page>
  <page>
    <title>Social Sciences Citation Index</title>
    <ns>0</ns>
    <id>6853403</id>
    <revision>
      <id>761250910</id>
      <parentid>760300945</parentid>
      <timestamp>2017-01-21T22:19:39Z</timestamp>
      <contributor>
        <username>Riceissa</username>
        <id>20698937</id>
      </contributor>
      <comment>/* External links */ link was broken</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3149" xml:space="preserve">{{ infobox bibliographic database
| title = Science Citation Index
| image = 
| caption = 
| producer = [[Thomson Reuters]]
| country = United States
| history = 
| languages = 
| providers = 
| cost = 
| disciplines = Social sciences
| depth = Index &amp; citation indexing 
| formats = 
| temporal = 
| geospatial = 
| number = 
| updates = 
| p_title = 
| p_dates = 
| ISSN = 
| web = http://thomsonreuters.com/en/products-services/scholarly-scientific-research/scholarly-search-and-discovery/social-sciences-citation-index.html
| titles = http://ip-science.thomsonreuters.com/cgi-bin/jrnlst/jlresults.cgi?PC=SS
}}
The '''Social Sciences Citation Index''' ('''SSCI''') is a commercial [[citation index]] product of [[Thomson Reuters]]' Healthcare &amp; Science division. It was developed by the [[Institute for Scientific Information]] from the [[Science Citation Index]].

==Overview==
The SSCI citation database covers some 3,000 of the world's leading [[academic journals]] in the [[social sciences]] across more than 50 [[academic discipline|disciplines]].&lt;ref&gt;{{cite web |title=Social Sciences Citation Index |url=http://scientific.thomson.com/products/ssci/ |accessdate=2008-06-11}}&lt;/ref&gt; It is made available online through the [[Web of Science]] service for a fee. The database records which articles are cited by other articles.

==Criticism==
[[Philip Altbach]] has criticised the Social Sciences Citation Index of favouring English-language journals generally and American journals specifically, while greatly underrepresenting journals in non-English languages.&lt;ref&gt;{{cite book |last= Altbach|first=Philip |authorlink= Philip Altbach |year=2005 |article=Academic Challenges: The American Professoriate in Comparative Perspective |title=The Professoriate: Profile of a Profession |url= |location=Dortrecht |publisher=Springer |pages=147–165 |isbn= |author-link= }}&lt;/ref&gt;

In 2004, economists [[Daniel B. Klein]] and Eric Chiang conducted a survey of the Social Sciences Citation Index and identified a bias against free market oriented research. In addition to an ideological bias, Klein and Chiang also identified several methodological deficiencies that encouraged the over-counting of citations, and they argue that the Social Sciences Citation Index does a poor job reflecting the relevance and accuracy of articles.&lt;ref&gt;Daniel Klein and Eric Chiang. [http://econjwatch.org/articles/the-social-science-citation-index-a-black-box-with-an-ideological-bias The Social Science Citation Index: A Black Box—with an Ideological Bias?] ''Econ Journal Watch'', Volume 1, Number 1, April 2004, pp 134–165.&lt;/ref&gt;

==See also==
* [[Arts and Humanities Citation Index]]
* [[Science Citation Index]]

==References==
{{reflist}}

==External links==
*{{Official website|http://ip-science.thomsonreuters.com/mjl/publist_ssci.pdf}}
*[http://thomsonreuters.com/products_services/science/science_products/a-z/social_sciences_citation_index Introduction to SSCI]

{{Thomson Reuters}}

[[Category:Thomson Reuters]]
[[Category:Social sciences literature]]
[[Category:Citation indices]]
[[Category:Social science journals| ]]

{{database-stub}}
{{sci-stub}}</text>
      <sha1>6iwpse9ifa7aj6wnq3cu6pjopk5l23j</sha1>
    </revision>
  </page>
  <page>
    <title>Materials Science Citation Index</title>
    <ns>0</ns>
    <id>27789063</id>
    <revision>
      <id>678206130</id>
      <parentid>670536809</parentid>
      <timestamp>2015-08-28T00:13:54Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor />
      <comment>Robot - Moving category Bibliographic databases to [[:Category:Bibliographic databases and indexes]] per [[WP:CFD|CFD]] at [[Wikipedia:Categories for discussion/Log/2015 July 4]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3035" xml:space="preserve">{{Third-party|date=February 2013}}
'''The Materials Science Citation Index''' is a [[citation index]], established in 1992, by [[Thomson ISI]] ([[Thomson Reuters]]). Its overall focus is [[citation|cited reference]] searching of the notable and significant [[science journal|journal literature]] in [[materials science]]. The database makes accessible the various [[physical properties|properties]], behaviors, and materials in the materials science discipline. This then encompasses [[applied physics]], [[ceramic engineering|ceramics]], [[Advanced composite materials (science &amp; engineering)|composite materials]], [[metals]] and [[metallurgy]], [[polymer engineering]], [[semiconductors]], [[thin films]], [[biomaterial]]s, [[Dentistry|dental technology]], as well as [[optics]]. The [[database]] indexes relevant materials science information from over 6,000 [[scientific journal]]s that are part of the ISI database which is [[multidisciplinary]]. Author abstracts are searchable, which links articles sharing one or more [[bibliographic]] references. The database also allows a researcher to use an appropriate (or related to research) article as a base to search forward in time to discover more recently published articles that cite it.&lt;ref name=msci-est&gt;Pemberton, Julia K. "''Two new databases from ISI''." CD-ROM Professional 5.4 (1992): 107+. General OneFile. Web. 20 June 2010.&lt;/ref&gt;

''Materials Science Citation Index'' lists 625 high impact journals, and is accessible via the [[Science Citation Index Expanded]] collection of databases.&lt;ref name=msci-jnlList&gt;[http://science.thomsonreuters.com/cgi-bin/jrnlst/jlresults.cgi?PC=MS Materials Science Citation Index journal list]. Thomson Reuters. July 2010.&lt;/ref&gt;

==Editions==
Coverage of Materials science is accomplished with the following editions:&lt;ref name=MS-indexes&gt;[http://science.thomsonreuters.com/mjl/scope/scope_scie/ Scope Notes]. Science Citation Index, Science Citation Index Expanded. Thomson Reuters. 2010.&lt;/ref&gt;&lt;ref&gt;[http://science.thomsonreuters.com/cgi-bin/jrnlst/jlsubcatg.cgi?PC=D Subject categories]. Science Citation Index Expanded. Thomson Reuters. 2010&lt;/ref&gt;
*Materials Science, Ceramics
*Materials Science, Characterization &amp; Testing
*Materials Science, Biomaterials
*Materials Science, Coatings &amp; Films
*Materials Science, Composites
*Materials Science, Paper &amp; Wood
*Materials Science, Multidisciplinary
*Materials Science, Textiles

==See also==
* [[Science Citation Index]]
* [[Academic publishing]]
* [[List of academic databases and search engines]]
* [[Social Sciences Citation Index]], which covers over 1500 journals, beginning with 1956
* [[Arts and Humanities Citation Index]], which covers over 1000 journals, beginning with 1975
* [[Impact factor]]
* [[VINITI Database RAS]]

==References==
{{Reflist}}

{{Thomson Reuters}}

[[Category:Thomson Reuters]]
[[Category:Bibliographic databases and indexes]]
[[Category:Online databases]]
[[Category:Citation indices]]
[[Category:Materials science journals| ]]


{{science-journal-stub}}</text>
      <sha1>8dr53zeh2xbbzty7olo7igq5ii90kbx</sha1>
    </revision>
  </page>
  <page>
    <title>Russian Science Citation Index</title>
    <ns>0</ns>
    <id>35108736</id>
    <revision>
      <id>670537328</id>
      <parentid>564703231</parentid>
      <timestamp>2015-07-08T16:30:29Z</timestamp>
      <contributor>
        <username>Fgnievinski</username>
        <id>6727347</id>
      </contributor>
      <comment>+[[Category:Russian-language journals]]; +[[Category:Science and technology in Russia]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2476" xml:space="preserve">{{primary sources|date=March 2012}}
'''Russian Science Citation Index''' is a [[bibliographic database]] of [[scientific publication]]s in Russian. It accumulates more than 2 million publications of Russian authors, as well as information about citing these publications from more than 2000 Russian journals. The Russian Science Citation Index has been developed since 2009 by the Scientific Electronic Library. The information-analytical system Science Index is a search engine of this database; it offers a wide range of services for authors, research institutions and scientific publishers. It is designed not only for operational search for relevant bibliographic information, but is also as a powerful tool to assess the impact and effectiveness of research organizations, scientists, and the level of scientific journals, etc.

== Purpose ==
From 3000 Russian scientific journals only about 150 are presented in foreign databases (i.e. not more than 5%). Those are mainly translated journals. So far, the vast majority of Russian scientific publications remain "invisible" and not available online.  Russian Science Citation Index makes it real to objectively compare Russian journals with  the best international journals and brings them closer to researchers all over the world.

== Functionality ==
In Russia, this database is one of the main sources of information for evaluating the effectiveness of organizations involved in research. It allows to appraise: 
* Scientific capacity and effectiveness of research, and
* Publication activity
through the following indicators:
* The number of publications (including foreign scientific and technical journals, and local publications from the list of [[Higher Attestation Commission]]) of researchers from a particular scientific organization, divided by the number of researchers,
* The number of publications (registered in the Russian Science Citation Index) of researchers from a particular scientific organization, divided by the number of researchers, and
* Citation of researchers (registered in the Russian Science Citation Index) from a particular scientific organization, divided by the number of researchers.

== See also ==
*[[List of academic databases and search engines]]
*[[Science Citation Index]]
*[[Scopus]]

==External links==
* [http://elibrary.ru/ Scientific Electronic Library]


[[Category:Citation indices]]
[[Category:Russian-language journals| ]]
[[Category:Science and technology in Russia]]</text>
      <sha1>b31c48gt29qy8vxs9hrrhchxc9l72jx</sha1>
    </revision>
  </page>
  <page>
    <title>Conference Proceedings Citation Index</title>
    <ns>0</ns>
    <id>47141591</id>
    <revision>
      <id>670682707</id>
      <parentid>669755873</parentid>
      <timestamp>2015-07-09T14:36:31Z</timestamp>
      <contributor>
        <username>Derek R Bullamore</username>
        <id>698799</id>
      </contributor>
      <comment>Filling in 1 references using [[WP:REFLINKS|Reflinks]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1357" xml:space="preserve">{{Incomplete|date=July 2015}}
{{infobox bibliographic database
| title = Conference Proceedings Citation Index
| image = 
| caption = 
| producer = [[Thomson Reuters]]
| country = United States
| history = 
| languages = 
| providers = 
| cost = 
| disciplines = 
| depth = 
| formats = 
| temporal = 
| geospatial = 
| number = 
| updates = 
| p_title = 
| p_dates = 
| ISSN = 
| web = http://wokinfo.com/products_tools/multidisciplinary/webofscience/cpci/
| titles = 
}}
The '''Conference Proceedings Citation Index''' ('''CPCI''') is a [[citation index]] produced by [[Thomson Reuters]] covering [[conference proceedings]].&lt;ref&gt;{{cite web|url=http://wokinfo.com/media/pdf/proceedingswhtpaper.pdf |format=PDF |title=White Paper : Conference Proceddings and Their Impact on Global Research |publisher=Wokinfo.com |accessdate=2015-07-09}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://wokinfo.com/products_tools/multidisciplinary/webofscience/cpci/cpciessay/|title=CPCI Essay - IP &amp; Science - Thomson Reuters|author=Thomson Reuters|publisher=Wokinfo.com|accessdate=2015-07-09}}&lt;/ref&gt;

==References==
{{Reflist}}

==External links==
*{{official website|http://wokinfo.com/products_tools/multidisciplinary/webofscience/cpci/}}

{{Thomson Reuters}}

[[Category:Citation indices]]
[[Category:Online databases]]
[[Category:Thomson Reuters]]
[[Category:Conference proceedings]]</text>
      <sha1>lt2ut3s2escbshe7x36xpyong89bo2i</sha1>
    </revision>
  </page>
  <page>
    <title>Scientific Information Database</title>
    <ns>0</ns>
    <id>40601299</id>
    <revision>
      <id>697042515</id>
      <parentid>675299087</parentid>
      <timestamp>2015-12-27T21:12:28Z</timestamp>
      <contributor>
        <username>Rathfelder</username>
        <id>398607</id>
      </contributor>
      <comment>removed [[Category:Iran]]; added [[Category:Science and technology in Iran]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1332" xml:space="preserve">{{Refimprove|date=February 2014}}
{{Notability|Web|date=December 2013}}
{{Infobox company
| name             = Scientific Information Database, Iran
| type             = [Journal Citation Database]
| location         = Tehran, Iran
| homepage         = [http://www.sid.ir]
| footnotes        = پایگاہ اطلاعات علمی و پژوھشی
}}

'''Scientific Information Database''' (or '''SID''') is the Iranian database for the calculation of Persian and English articles citation. It is the like the [[Institute for Scientific Information]] '''ISI''', a local citation counting manager.&lt;ref&gt;{{cite web|url=http://sid.ir |title=Scientific Information Database |publisher=Sid.ir |date= |accessdate=2014-02-03}}&lt;/ref&gt;

==Categories==
This database, does not include just the journal citation reports, it has different categories:
* English Journals, English Journals Database of Iran
* Persian Journals, بانک نشریات فارسی ایران
* Research Projects, طرح ھای پژوھشی
* English Scientific Community
* Persian Scientific Community, بانک مجامع علمی فارسی ایران 
* Science Centers, بانک مراکز علمی ایران

==References==
{{Reflist}}

==External links==
* [http://sid.ir Homepage of SID]

[[Category:Science and technology in Iran]]
[[Category:Citation indices]]</text>
      <sha1>mxs6coorgdapb2osn63cjfm8x62lubp</sha1>
    </revision>
  </page>
  <page>
    <title>Emerging Sources Citation Index</title>
    <ns>0</ns>
    <id>49394017</id>
    <revision>
      <id>747003768</id>
      <parentid>747002085</parentid>
      <timestamp>2016-10-30T22:02:14Z</timestamp>
      <contributor>
        <username>Rbleibenusw</username>
        <id>24206652</id>
      </contributor>
      <comment>Clarivate</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2028" xml:space="preserve">{{incomplete|date=January 2014}}
{{ infobox bibliographic database
| title = Emerging Sources Citation Index
| image = 
| caption = 
| producer = [[Thomson Reuters]]
| country = United States
| history = 2015-present
| languages = 
| providers = 
| cost = 
| disciplines = Multidisciplinary
| depth = 
| formats = 
| temporal = 
| geospatial = Worldwide
| number =
| updates = 
| p_title = 
| p_dates = 
| ISSN = 
| web = http://wokinfo.com/products_tools/multidisciplinary/esci/
| titles = 
}}
The '''Emerging Sources Citation Index''' is a [[citation index]] produced since 2015 by [[Thomson Reuters]], and now by [[Clarivate Analytics]]. It is accessible through the ''[[Web of Science]]''.&lt;ref name=AtoZ&gt;{{cite web |last=ISI Web of Knowledge platform |title =Available databases A to Z |publisher=Thomson Reuters |year=2015 |url=http://ip-science.thomsonreuters.com/mjl/}}&lt;/ref&gt; The index includes [[academic journal]]s "of regional importance and in emerging scientific fields".&lt;ref&gt;{{cite web |title=ESCI Fact Sheet |url=http://wokinfo.com/media/pdf/ESCI_Fact_Sheet.pdf?utm_source=false&amp;utm_medium=false&amp;utm_campaign=false |publisher=Thomson Reuters |accessdate=28 April 2016 |format=PDF}}&lt;/ref&gt; [[Jeffrey Beall]] has stated that, among the databases produced by Thomson Reuters, the Emerging Sources Citation Index is the easiest one to get into and as a result it contains many [[Predatory open access publishing|predatory journals]].&lt;ref name=Beall&gt;{{cite web |last1=Beall |first1=Jeffrey |authorlink1=Jeffrey Beall |title=The TR Master Journal List is not a Journal Whitelist |url=https://scholarlyoa.com/2016/04/28/the-tr-master-journal-list-is-not-a-journal-whitelist/ |website=Scholarly Open Access |publisher=WordPress.com |accessdate=28 April 2016 |date=28 April 2016}}&lt;/ref&gt;

==References==
{{Reflist}}

==External links==
*{{Official website|http://wokinfo.com/products_tools/multidisciplinary/esci/}}
{{Thomson Reuters}}

[[Category:Citation indices]]
[[Category:Online databases]]
[[Category:Thomson Reuters]]</text>
      <sha1>n9b53bm6h8q67j0ch5qlo9nyiw1viyi</sha1>
    </revision>
  </page>
  <page>
    <title>Thomson Directories</title>
    <ns>0</ns>
    <id>1322148</id>
    <revision>
      <id>651198200</id>
      <parentid>575654596</parentid>
      <timestamp>2015-03-13T14:53:09Z</timestamp>
      <contributor>
        <ip>86.4.89.109</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2039" xml:space="preserve">{{Advert|date=July 2009}}
{{Refimprove|date=July 2009}}
'''Thomson Directories''', more commonly referred to as Thomson Local, is a local business directory company based in [[Farnborough, Hampshire|Farnborough]], [[Hampshire]], [[England]], and offers business listings both in print and online following the launch of ThomsonLocal.com in 2003.

174 regional editions of the Thomson Local are produced and delivered free of charge to residential and commercial addresses throughout the UK.
 
The Chief Executive Officer is currently Elio Shiavo.&lt;ref&gt;CEO http://www.answers.com/topic/thomson-directories-1&lt;/ref&gt; 

The company was purchased by [[US West]], a telecommunications company in the United States, in 1994.&lt;ref&gt;{{cite news|url=http://www.independent.co.uk/news/business/us-west-pays-70m-pounds-for-thomson-directories-american-telephone-company-continues-to-develop-multimedia-in-uk-1437180.html|title=US West pays 70m pounds for Thomson Directories: American telephone company continues to develop multimedia in UK |last=Fagan|first=Mary|date=20 May 1994|work=The Independent|accessdate=2009-08-15}}&lt;/ref&gt; In 1999, the company was sold by [[3i]] to [[TDL Infomedia]], a subsidiary of [[Apax Partners]].&lt;ref&gt;{{cite news|url=http://www.independent.co.uk/news/business/3i-sells-thomson-guides-for-pounds-220m-1109799.html|title=  3i sells Thomson guides for pounds 220m |last=Baker|first=Lucy|date=31 July 1999|work=The Independent|accessdate=2009-08-15}}&lt;/ref&gt;

The company was placed in [[Administration (law)]] in August 2013, and acquired by Corporate Media Partners.&lt;ref&gt;{{cite news|url=http://www.bbc.co.uk/news/uk-england-hampshire-23710958|title= Thomson Local directory firm goes into administration|work=BBC News|accessdate=2013-09-11}}&lt;/ref&gt;

==References==
{{Reflist}}

==External links==
*[http://www.thomsondirectories.com/ Official Thomson Directory Site]

[[Category:Companies of the United Kingdom]]
[[Category:Directories]]
[[Category:Apax Partners companies]]
[[Category:3i Group companies]]


{{UK-company-stub}}</text>
      <sha1>rf9m9kjtoft36gers4t7szlqk8hlfki</sha1>
    </revision>
  </page>
  <page>
    <title>Subcontractors Register</title>
    <ns>0</ns>
    <id>6401934</id>
    <revision>
      <id>442014646</id>
      <parentid>429883439</parentid>
      <timestamp>2011-07-29T09:52:19Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor />
      <comment>[[WP:CHECKWIKI]] error fixes (category with space) + [[WP:GENFIXES|general fixes]] using [[Project:AWB|AWB]] (7796)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="931" xml:space="preserve">[[Image:Subcontractors Register.jpg|thumb|1937 edition of the ''Subcontractors Register'']] 

The '''Subcontractors Register for the Allied Building Trades''' was a directory of [[subcontractors]] for the [[New York City]] area, listing companies by their trade. It was published by the "Society of the Allied Building Trades, Inc." and was published by Joseph O'Malley (1893–1985) who was later joined by his nephew, [[Walter Francis O'Malley]], as editor. The 1942 version calls itself: "A Classified List for the Allied Building Trades of Sub-Contractors, Material Dealers &amp; Manufacturers, General Contractors &amp; Builders, Architects - Engineers, Real Estate Management Firms".

==External links==
*[http://www.findagrave.com/cgi-bin/fg.cgi?page=gr&amp;GSln=o'malley&amp;GSmid=46580804&amp;GRid=7768640&amp; Findagrave: Joseph O'Malley]

[[Category:O'Malley family]]
[[Category:Directories]]
[[Category:History of New York City]]


{{US-stub}}</text>
      <sha1>bvnf9h2fx093jwuypir7trb4wm7usrv</sha1>
    </revision>
  </page>
  <page>
    <title>High Weirdness by Mail</title>
    <ns>0</ns>
    <id>3350777</id>
    <revision>
      <id>718909524</id>
      <parentid>718909467</parentid>
      <timestamp>2016-05-06T11:05:20Z</timestamp>
      <contributor>
        <username>Randy Kryn</username>
        <id>4796325</id>
      </contributor>
      <minor />
      <comment>fix of last edit</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1676" xml:space="preserve">{{italic title}}
'''''High Weirdness By Mail – A Directory of the Fringe: Crackpots, Kooks &amp; True Visionaries''''', by [[Ivan Stang]] (ISBN 0-671-64260-X) is a 1988 book dedicated to an examination of "weird culture" by actually putting the reader in touch with it by mail.

The book is divided into sections&amp;mdash;"Weird Science," "UFO Contactees," "Drug Stuff," and others, and each section contains a variety of mini-articles describing organizations. Each organization article concludes with a mailing address (and in some cases, phone numbers).

Several years after the book's publication, Stang reported on the [[newsgroup]] [[alt.slack]] that his inclusion of entries for [[white supremacist]] groups in the book caused his name to be mentioned by those groups as a possible target for retaliation.  (The book's commentaries on various [[hate group]]s were less than flattering.)  Stang reported this incident to the [[FBI]], but did not receive any actual harassment or threats from the groups in question.

The [[Association for Consciousness Exploration]] produced a follow-up lecture by Rev. Stang on cassette entitled ''High Weirdness by Mail'', recorded live at the 1993 WinterStar Symposium.

== Controversy ==

[[Bob Black]] claims that his review of ''High Weirdness By Mail'' was the cause of his being sent a small 'prank' mail bomb. [http://www.inspiracy.com/black/bomb.html]

== External links ==
* [http://www.subgenius.com Home Page of the Church of the SubGenius]
* [http://subgenius.com/hwbw.htm The Return of ''High Weirdness by Mail'']

[[Category:1988 books]]
[[Category:Church of the SubGenius]]
[[Category:Directories]]

{{Nonfiction-book-stub}}</text>
      <sha1>bow55i4w320nxxlivlabaac0xmx8vkw</sha1>
    </revision>
  </page>
  <page>
    <title>Vsya Rossiya</title>
    <ns>0</ns>
    <id>11017381</id>
    <revision>
      <id>736631813</id>
      <parentid>710446145</parentid>
      <timestamp>2016-08-28T20:33:19Z</timestamp>
      <contributor>
        <username>Iridescent</username>
        <id>937705</id>
      </contributor>
      <minor />
      <comment>/* top */[[WP:AWB/T|Typo fixing]], [[WP:AWB/T|typo(s) fixed]]: between 500 to → between 500 and using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2485" xml:space="preserve">{{italic title}}
'''''Vsya Rossiya''''' (literally translated "''All Russia''" or "''The whole Russia''") was the title of a series of directories of the [[Russian Empire]] published by [[Aleksei Sergeevich Suvorin]] on a yearly basis from 1895 to 1923 and was continued under the name '''''Ves SSSR''''' (Literally translated ''All of the USSR'' or ''The whole USSR'') from 1924 to 1931. Each volume was anywhere between 500 and 1500 pages long. The directories contained detailed lists of government offices, public services and medium and large businesses present in major cities across Russia including [[Kiev]], [[Minsk]], . These directories are often used by [[genealogists]] today to trace family members who were living in pre-revolutionary Russia and the early [[Soviet Union|Soviet]] period when [[vital records]] are missing or prove difficult to find. [[Historians]] use them to research the [[social histories]] of late 19th century and early 20th century Russia.

==Contents==

The following information can be found in most editions:
*a surname index of over 100,000 names and thousands of companies
*a directory of prominent landowners
*Lists members of the Imperial House of Russia and government officials 
*statistical information about the Russian Empire
*Population figures
*information and guidelines about trade and industry in Russia
*Lists of joint-stock companies
*Sub-sections detailing a directory of each district of each province, listing administrative officials, merchants, industrial and commercial manufacturers
*Original advertising

== Availability ==

Many original directories in the series (or [[microfiche]] copies thereof) can be found in libraries across the U.S., Europe (including the [[Baltic countries]], Finland the United Kingdom and Germany) however most only have an incomplete collection.

==Other city directories in Russia ==
Suvorin also published city directories for [[Saint Petersburg]] under the title ''[[Ves Petersburg]]'' (Literally translated ''All Petersburg'' or ''The Whole Saint Petersburg'') for the years 1894 to 1940 and for [[Moscow]] under the title ''[[Vsia Moskva]]'' (Literally translated ''All Moscow'' or ''The Whole Moscow'') for the years 1875 to 1936.

== External links ==

*[http://surname.litera-ru.ru/ A Russian website offering a search engine in Cyrillic for some city directories.]

[[Category:Directories]]
[[Category:Russian non-fiction books]]
[[Category:Russian Empire]]
[[Category:1895 books]]</text>
      <sha1>ph9z12kz4eu340k0kmdphwlrmu1osxf</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Personal information managers</title>
    <ns>14</ns>
    <id>1444957</id>
    <revision>
      <id>548049631</id>
      <parentid>530413802</parentid>
      <timestamp>2013-03-31T22:09:17Z</timestamp>
      <contributor>
        <username>EmausBot</username>
        <id>11292982</id>
      </contributor>
      <minor />
      <comment>Bot: Migrating 11 langlinks, now provided by Wikidata on [[d:Q9073404]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="171" xml:space="preserve">{{Cat main|personal information manager}}

[[Category:Directories]]
[[Category:Planning]]
[[Category:Personal life|Information managers]]
[[Category:Application software]]</text>
      <sha1>r5petieioqas869hwq5t5gql4uuk2pq</sha1>
    </revision>
  </page>
  <page>
    <title>Yellowikis</title>
    <ns>0</ns>
    <id>1430812</id>
    <revision>
      <id>755476314</id>
      <parentid>749201096</parentid>
      <timestamp>2016-12-18T07:11:57Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* Legal issues */clean up; http&amp;rarr;https for [[The Guardian]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5987" xml:space="preserve">{{Infobox Website
| name = Yellowikis
| favicon =
| logo = [[Image:Yelwiki.png]]
| screenshot =
| url = [http://yellowikis.wikia.com/wiki/Main_Page yellowikis.wikia.com]
| commercial =
| alexa =
| type = [[MediaWiki]]
| registration = Optional
| owner = [[Wikia]]
| author =
| launch date = 2005
| current status = Inactive
| revenue =
}}
'''Yellowikis''' was a [[MediaWiki]] [[website]] collecting basic information about businesses. This information included basic contact details such as company name, address, websites, and telephone numbers, as well as internal Yellowiki [[wikilink]]s to competitors. Yellowikis was launched in January 2005. {{As of|2011|3}}, the Yellowikis main page had been translated into more than 25 different languages.{{citation needed|date=April 2015}}

Some users also entered a number of codes including a two letter country code as well as an [[International Standard Industrial Classification]], [[North American Industry Classification System|North American Industry Classification]] or US [[Standard Industrial Classification]]. Some users are also adding [[geocode]]s and [[Skype]] ids.

==Legal issues==
A commercial business listing company, [[Yell Limited]], requested that the founders of Yellowikis, Paul Youlten and Rosa Blaus, amend their site, claiming that Yellowikis was "passing itself off" as being associated with Yell.com and that people would confuse the two organisations.&lt;ref&gt;{{cite news
 | title =Legal threat to wiki listing site
 | work =BBC News
 | date = 12 July 2006
 | url =http://news.bbc.co.uk/1/hi/technology/5169674.stm
 | accessdate =2006-07-12 }}&lt;/ref&gt;&lt;ref&gt;{{cite news|title=Teenager faces action over listings website|author=Bobbie Johnson|date=2006-08-02|work=[[The Guardian]]|url=https://www.theguardian.com/uk_news/story/0,,1835233,00.html|publisher=Guardian News and Media Ltd | location=London}}&lt;/ref&gt;&lt;ref&gt;{{cite news|title=Yell threatens to sue wiki rival|author=Jane Hoskyn|work=vnunet.com|date=2006-07-14|url=http://www.vnunet.com/vnunet/news/2160380/yell-threatens-sue-wiki-rival|publisher=VNU Business Publications Ltd|archiveurl=https://web.archive.org/web/20070930195439/http://www.vnunet.com/vnunet/news/2160380/yell-threatens-sue-wiki-rival|archivedate=2007-09-30}}&lt;/ref&gt; This might be considered to be anti-competitive behaviour/anti-competitive in the eyes of certain commentators, however, such claim is unlikely to carry water from a legal perspective. Yell's claim is given considerable weight by the slogan on Yellowiki's front page that they are "Yellow Pages for the 21st Century" although in their public protestations, Yellowikis claim that they are not trying to create association between themselves and Yellow Pages.&lt;ref&gt;{{cite web
 | last =The Yellowikis Community
 | title =Response to Yell
 | work =
 | publisher =Yellowikis
 | year =2006
 | url =http://www.yellowikis.org/wiki/index.php/Response_to_Yell
 | accessdate =2006-07-14 }}&lt;/ref&gt;

[[Yellow Pages]] is a registered [[trade mark]] in many countries including the UK. In some territories, however, the mark has lost its distinctiveness as a source of origin of goods and services.

From 9 to 14 October 2006, the domain address redirected to the new [http://www.owikis.org.uk/ Owikis] website, which stated "The trademark dispute between Yell Limited and Paul Youlten concerning the Yellowikis website has been satisfactorily resolved".

On 15 October 2006, the Yellowikis website reappeared, with the explanation that [[United Kingdom]] users would have to use Owikis, with the word ''Yell'' from the domain name and the color yellow from the logo; international users could continue to use Yellowikis. {{As of|2008|5}}, the Owikis site is not yet available.

As of at least May 2014, the [http://yellowikis.wikia.com/wiki/Main_Page Wikia page] is dead.&lt;ref&gt;{{cite web|url=http://yellowikis.wikia.com/wiki/Main_Page |title=Web Archive |accessdate=2014-05-28 |deadurl=yes |archiveurl=https://web.archive.org/web/20140109015130/http://yellowikis.wikia.com/wiki/Main_Page |archivedate=January 9, 2014 }}&lt;/ref&gt;

==References==
{{Reflist}}

==Further reading==
* {{cite news|url=http://www.researchbuzz.org/2005/06/business_information_in_wiki_f.shtml|title=Business Information in Wiki Format|date=2005-06-22|publisher=ResearchBuzz}}
* {{cite news|url=http://competia.com/competia_w/site/fiche/1954|title=Yellowikis|date=2005-07-26|publisher=Competia}}
* {{cite web|url=http://alina_stefanescu.typepad.com/totalitarianism_today/2005/05/a_wiki_worth_wa.html|title=A wiki worth watching|work=totalitarianism today|accessdate=2005-10-07}}
* {{cite web|url=http://www.stabani.com/index.php?s=yellowikis|title=Why I think Yellowikis is a good idea |work=site spotlight|accessdate=2006-01-16|author=S.Tabani}}
* [[n:Emily Chang|Emily Chang]] {{cite web|url=http://www.emilychang.com/go/ehub/|title=Emily Chang's eHub|work=eHub|accessdate=2005-10-09}} 
* {{cite web|author=Richard MacManus|url=http://blogs.zdnet.com/web2explorer/?p=58|title=Yellowikis - A Case Study of a Web 2.0 Business, Part 1|date=2005-10-15}}
* {{cite web|author=Richard MacManus|url=http://blogs.zdnet.com/web2explorer/?p=59|title=Yellowikis: a Web 2.0 Case Study, Part 2 - Industry Disruption and The Competition|date=2005-10-16}}
* {{cite web|author=Richard MacManus|url=http://blogs.zdnet.com/web2explorer/?p=62|title=Yellowikis: Demonstrating Web 2.0 principles|date=2005-10-17}}
* {{cite news|url=http://news.independent.co.uk/media/article1096343.ece|title=New Media: Who are the real winners now we've all gone Wiki-crazy?|date=2006-06-26|publisher=[[The Independent]] | location=London | first=Kathy | last=Marks}}
* [[n:Yell threatens to shut down Yellowikis|Yell threatens to shut down Yellowikis]] from [[Wikinews]], 2006-07-05

==External links==
* [http://yellowikis.wikia.com/wiki/Main_Page Yellowikis] on [[Wikia]]

[[Category:Directories]]
[[Category:MediaWiki websites]]
[[Category:Internet properties established in 2005]]
[[Category:Yellow pages]]</text>
      <sha1>pan7bs6i0qeoc850nipblbja5ztr7fl</sha1>
    </revision>
  </page>
  <page>
    <title>Annuario Pontificio</title>
    <ns>0</ns>
    <id>2940988</id>
    <revision>
      <id>757928967</id>
      <parentid>741305588</parentid>
      <timestamp>2017-01-02T14:58:13Z</timestamp>
      <contributor>
        <username>Neddyseagoon</username>
        <id>883252</id>
      </contributor>
      <comment>/* External links */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6736" xml:space="preserve">{{italic title}}
{{Refimprove|date=April 2010}}
{{Infobox book
| name = Annuario Pontificio 
| title_orig =
| translator =
| image = Annuario Pontificio 2008 (MK).jpg
| caption =
| author = [[Libreria Editrice Vaticana]], Secretary of State
| illustrator =
| cover_artist =
| country = [[Vatican City|Vatican]]
| language = Italian
| series =
| subject =
| genre = [[Annual publication]], [[Reference]]
| publisher = [[Holy See]]
| pub_date = December 2, 2014
| media_type = Printed Book
| pages = 
| isbn = 9788820997472
| isbn_note = &lt;ref&gt;[http://www.libreriaeditricevaticana.va/content/libreriaeditricevaticana/it/news-ed-eventi/annuario-pontificio-2016.html 2016]&lt;/ref&gt;
| oclc= 
| preceded_by = Annuario Pontificio 2015
| followed_by = Annuario Pontificio 2016
}}
The '''''Annuario Pontificio''''' ([[Italian language|Italian]] for ''Pontifical Yearbook'') is the annual directory of the [[Holy See]]. It [[List of popes|lists all the popes]] to date and all officials of the Holy See's [[dicastery|departments]]. It also gives complete lists, with contact information, of the [[Cardinal (Catholicism)|cardinals]] and [[Catholic Church|Catholic]] bishops throughout the world, the [[diocese]]s (with statistics about each), the departments of the [[Roman Curia]], the Holy See's [[diplomatic mission]]s abroad, the [[embassy|embassies]] accredited to the Holy See, the headquarters of [[religious institute]]s (again with statistics on each), certain academic institutions, and other similar information. The index includes, along with all the names in the body of the book, those of all priests who have been granted the title of "[[Monsignor]]".
As the title suggests, the red-covered yearbook, compiled by the Central Statistics Office of the Church and published by [[Libreria Editrice Vaticana]], is mostly in Italian.

The 2015 edition has more than 2,400 pages and costs {{€|78}}.&lt;ref&gt;{{cite web |url=http://www.vaticanum.com/en/annuario-pontificio-2015-book-2|access-date=April 5, 2016|title=Annuario Pontificio 2015|publisher=Città del Vaticano}}&lt;/ref&gt; According to the ''Pontifical Yearbook of 2010'', the number of Catholics in the world increased from 1,147 million to 1,166 million between 2007 and 2008, a growth of 1.7 percent.&lt;ref&gt;{{cite web| url=http://www.zenit.org/article-28425?l=english |title=Number of Catholics Increases Worldwide: 2010 "Annuario" Shows Growth in Asia and Africa |publisher=Zenit |date=February 21, 2010 |accessdate=April 11, 2010}}&lt;/ref&gt; By the ''Yearbook of 2016'' it was 1,272,281,000 at the end of 2014.

==History==
A [[yearbook]] of the Catholic Church was published, with some interruptions, from 1716 to 1859 by the Cracas printing firm in Rome, under the title (in Italian) ''Information for the Year ...'' From 1851, a department of the Holy See began producing a different publication called (in Italian) ''Hierarchy of the Holy Catholic Apostolic Church Worldwide and in Every Rite, with historical notes'', which took the title ''Annuario Pontificio'' in 1860, but ceased publication in 1870. This was the first yearbook published by the Holy See itself, but its compilation was entrusted to the newspaper ''[[Giornale di Roma]]''. The publishers "Fratelli Monaldi" (Monaldi Brothers) began in 1872 to produce their own yearbook entitled (in Italian) ''The Catholic Hierarchy and the Papal Household for the Year ... with an appendix of other information concerning the Holy See''.

The [[Holy See Press Office|Vatican Press]] took this over in 1885, thus making it a semi-official publication.  It bore the indication "official publication" from 1899 to 1904, but this ceased when, giving the word "official" a more restricted sense, the ''Acta Sanctae Sedis'', forerunner of the ''[[Acta Apostolicae Sedis]]'', was declared the only "official" publication of the Holy See. In 1912, it resumed the title ''Annuario Pontificio''. From 1912 to 1924, it included not only lists of names, but also brief illustrative notes on departments of the Roman Curia and on certain posts within the [[papal court]], a practice to which it returned in 1940.

For some years, beginning in 1898, the ''Maison de [[la Bonne Presse]]'' publishing house of [[Paris]] produced a similar yearbook in [[French language|French]] called ''Annuaire Pontifical Catholique'', not compiled by the Holy See. This contained much additional information, such as detailed historical articles on the [[Swiss Guards]] and the [[Apostolic Palace|Papal Palace]] at the [[Vatican City|Vatican]].

== Statistical data ==
According to the ''Annuario Pontificio 2012'' the statistical data given in the yearbook regarding [[archdiocese]]s and [[diocese]]s are furnished by the diocesan curias concerned and reflect the diocesan situation on 31 December of the year prior to the date on the yearbook, unless there is another indication.  The data recorded are shown in the following order next to these abbreviations:
* Su – area in square kilometers of the diocesan territory
* pp – population of the diocese
* ct – number of Catholics
* pr – parishes and quasi-parishes
* ch – churches or mission stations
* sd – secular priests resident in the diocese
* dn – diocesan priests ordained during the year
* sr – religious priests resident in the diocese
* rn – religious priests ordained during the year
* dp – permanent deacons
* sm – seminarians taking courses of philosophy and theology
* rm – members of men's religious institutes
* rf – members of women's religious institutes
* ie – educational institutes
* ib – charitable institutes
* ba – baptisms

== See also ==
{{Portal|Catholicism}}
* [[Catholic Church by country]]
* [[History of the papacy]]
* [[Oldest popes]]
* [[Vatican Publishing House]]

==References==
{{reflist}}

==Bibliography==
* Secretary of State, ''Annuario Pontificio 2010.'' Vatican City: [[Vatican Publishing House]]. ISBN 978-88-209-8355-0
* Secretary of State, ''Annuario Pontificio 2009.'' Vatican City: [[Vatican Publishing House]]. ISBN 978-88-209-8191-4
* Secretary of State, ''Annuario Pontificio 2008.'' Vatican City: [[Vatican Publishing House]]. ISBN 978-88-209-8021-4
* Secretary of State, ''Annuario Pontificio 2007.'' Vatican City: [[Vatican Publishing House]]. ISBN 978-88-209-7908-9
* Secretary of State, ''Annuario Pontificio 2006.'' Vatican City: [[Vatican Publishing House]]. ISBN 978-88-209-7806-8
* Secretary of State, ''Annuario Pontificio 2005.'' Vatican City: [[Vatican Publishing House]]. ISBN 978-88-209-7678-1

==External links==
* [http://www.catholic-hierarchy.org/ CatholicHierarchy.org]
* [http://www.gcatholic.org/ GCatholic.org]

[[Category:Documents of the Catholic Church]]
[[Category:Directories]]
[[Category:Holy See]]</text>
      <sha1>9jugi6bcz39ih394jgzyhsiwuixf1r3</sha1>
    </revision>
  </page>
  <page>
    <title>Bulmer (directories)</title>
    <ns>0</ns>
    <id>5853521</id>
    <revision>
      <id>558920617</id>
      <parentid>557540683</parentid>
      <timestamp>2013-06-08T15:30:47Z</timestamp>
      <contributor>
        <username>Magioladitis</username>
        <id>1862829</id>
      </contributor>
      <minor />
      <comment>clean up / checkwiki error #52 (category not at the end) using [[Project:AWB|AWB]] (9241)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1748" xml:space="preserve">'''Bulmer''' was a [[Victorian era|Victorian]] [[historian]], [[surveying|surveyor]], [[cartographer]] and compiler of [[Yellow Pages|directories]]. His directories provided a history and geography of a particular area. The directories listed and described all parishes; listed trades and professions and provided a helpful street index with the names of residents, together with other local information. Data CDs of Bulmer Directories are available from publishers in the UK.&lt;ref&gt;[[S&amp;N Genealogy Supplies|S&amp;N Publishing]]&lt;/ref&gt;

==List of directories==
* Bulmer's History, Topography and Directory of East Cumberland, 1883
* Bulmer's History, Topography and Directory of West Cumberland, 1884.
* Bulmer's History, Directory and Topography of Westmorland, 1885
* Bulmer's History, Topography and Directory of Northumberland (Hexham Division), 1886
* Bulmer's History, Topography and Directory of Northumberland (Tyneside, Wansbeck and Berwick Divisions), 1887
* Bulmer's History, and Directory of Newcastle upon Tyne, 1887
* Bulmer's History, and Directory of North Yorkshire, 1890 (two Volumes)
* Bulmer's History, Topography and Directory of East Yorkshire and Hull, 1892
* Bulmer's Directory of Cumberland, 1901
* T. Bulmer: History, Topography and Directory of Westmorland, 1906
* T Bulmer: History, Topography and Directory of Furness and Cartmel, 1911
* Bulmer's History, Topography and Directory of Furness, Cartmel and Egremont division of Cumbria, 1911
* T Bulmer: History, Topography and Directory of Lancaster and district, 1912
* J Bulmer: History, Topography and Directory of Lancaster and district, 1913

== References ==

{{reflist}}

{{DEFAULTSORT:Bulmer (Directories)}}
[[Category:Directories]]


{{UK-hist-stub}}
{{Ref-book-stub}}</text>
      <sha1>slkz3r559cqzz40wmh98zekzwlstw9a</sha1>
    </revision>
  </page>
  <page>
    <title>Pamyatnaya Knizhka</title>
    <ns>0</ns>
    <id>26512894</id>
    <revision>
      <id>545872612</id>
      <parentid>539606602</parentid>
      <timestamp>2013-03-21T03:54:13Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor />
      <comment>[[User:Addbot|Bot:]] Migrating 1 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q4342965]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2801" xml:space="preserve">{{unreferenced|date=April 2010}}
'''Pamyatnaya knizhka''' Memorial Book Памятная книжка is the title of official reference books of regions and towns in [[Russian empire]].

== History ==
The books were annually published by local authorities in 89 [[gubernia]]s and regions of [[Russian Empire]] starting from the mid-1830s till 1917. They provide information on population and businesses in the course of over 60 years. Over two thousand books have been found.

== Composition of Pamyatnaya knizhka ==
The books had some peculiarities in some [[gubernia]]s and not always comprised 4 main sections:
* [[address-calendar]] (index of all local official institutions and their staff),
* administrative reference book (information on administrative units in a [[gubernia]], post offices, roads, industrial and commercial enterprises, hospitals and chemists’, educational institutions, museums and libraries, book stores and print shops, periodicals, list of towns, major landowners etc.),
* statistic data (statistic tables on population, farming, education, incomes, fires etc.);
* historical background.

== Research project ==
The [[Russian National Library]] is carrying out a research project devoted to Pamyatnaya knizhka. The project is supervised by Mrs. Nadezhda Balatskaya Надежда Михайловна Балацкая in the department Bibliografia and krayevedeniye of the [[Russian National Library]].
The project comprises official memorial books Pamyatnay knizhkas of all gubernias and regions of [[Russia]], including areas that are no longer within the [[Russian Federation]].
The main result of the project will be the publication of 15 volume bibliographic index book titled “Памятные книжки губерний и областей Российской империи: Указатель содержания”. Some of materials in work are available at [http://givc.ru national computer center]. Рабочие материалы проекта представляют огромный интерес для людей, занимающихся генеалогией. That is an important source for genealogic researchers.

==External links==
* [http://www.nlr.ru/pro/inv/mem_buks.htm the site of the project of the Russia’s National Library «Памятные книжки губерний и областей Российской империи»]
* [http://chigirin.narod.ru/book.html#spravochniki some memorial books of Russian Empire in pdf at Russian national computer center]
* [http://russian-family.ru/index.php?option=com_jdownloads&amp;task=viewcategory&amp;catid=23&amp;Itemid=27 Memorial books of some gubernias]
{{use dmy dates|date=December 2010}}

[[Category:Russian Empire]]
[[Category:Directories]]
[[Category:Genealogy publications]]</text>
      <sha1>afn382ycllxeeqjazi68evc31mmu4oy</sha1>
    </revision>
  </page>
  <page>
    <title>Dalilmasr</title>
    <ns>0</ns>
    <id>27707778</id>
    <revision>
      <id>601840646</id>
      <parentid>597578763</parentid>
      <timestamp>2014-03-29T17:54:01Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor />
      <comment>[[WP:CHECKWIKI]] error fixes + other fixes using [[Project:AWB|AWB]] (10067)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1576" xml:space="preserve">{{orphan|date=August 2010}}
'''Dalilmasr''' is an online [[Egyptian language|Egyptian]] directory, which provides information about any service, product, and/or tool available in the Egyptian market. The website includes information about companies, shops, showrooms, and service providers in the Egyptian Region.

In the five main languages it serves more than two billion people.
&lt;br&gt;'''Logo Identity:'''The logo is in two palms of the hand applauding (conducting the valued promised service) in black and red color along with white background (Egyptian Flag Colors), the other right and left acute blue triangles meaning the Nile River welfare.
&lt;ref&gt;[[Al-Ahram|Al-Ahram newspaper]] in large two columns, says Dalilmasr is the first Egyptian search engine focused on contents rather only information and care about all Egyptian cities &amp; suburbs. Issued on June 15, 2010 - page 22.&lt;/ref&gt;
&lt;ref&gt;[[Akhbar El Yom]] newspaper considered Dalilmasr as best Egyptian directory, appendix 3-page 6 on June 12, 2010&lt;/ref&gt;
&lt;ref&gt;[http://www.minia.edu.eg/doHtml.aspx?page=useful-sites.html Minia University's] directory proposed Dalilmasr as one of the recommended Egyptian websites.
&lt;/ref&gt;
&lt;ref&gt;Wikiwak indexed Dalilmasr since website domain was dalil-masr.com&lt;/ref&gt;
&lt;ref&gt;Google rank it on the top for best Egyptian directory keyword [http://www.google.com.eg/search?hl=en&amp;q=best+egyptian+directory&amp;aq=0&amp;aqi=g3&amp;aql=&amp;oq=best+egyptian+d&amp;gs_rfai= Search Result]&lt;/ref&gt;

== References ==
{{Reflist}}

== External links ==
* [http://www.dalilmasr.com Dalimasr website]

[[Category:Directories]]</text>
      <sha1>ki2lmbzrze6vq6t4gqlfrftzifikg7w</sha1>
    </revision>
  </page>
  <page>
    <title>Mobile directory</title>
    <ns>0</ns>
    <id>27409630</id>
    <revision>
      <id>605743830</id>
      <parentid>605743412</parentid>
      <timestamp>2014-04-25T12:57:30Z</timestamp>
      <contributor>
        <username>Gilliam</username>
        <id>506179</id>
      </contributor>
      <minor />
      <comment>Reverted edits by [[Special:Contribs/117.222.194.201|117.222.194.201]] ([[User talk:117.222.194.201|talk]]) to last version by Yobot</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="530" xml:space="preserve">{{multiple issues|
{{Unreferenced|date=October 2010}}
{{orphan|date=October 2010}}
}}

A '''mobile directory''' is a collection of subscriber details of a [[mobile phone]] [[Mobile phone companies|operators]]. Generally, the mobile telephony operators do not publish a mobile [[directory (databases)|directory]]. Some third party [[websites]] offer mobile directory facility through [[Reverse telephone directory|reverse search]].

[[Category:Telephone numbers]]
[[Category:Directories]]


{{telephonenumber-stub}}
{{mobile-stub}}</text>
      <sha1>65ljng79q2pcgj44aiyfbtnijj93j79</sha1>
    </revision>
  </page>
  <page>
    <title>Pigot's Directory</title>
    <ns>0</ns>
    <id>20147149</id>
    <revision>
      <id>743989939</id>
      <parentid>712825473</parentid>
      <timestamp>2016-10-12T12:33:18Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* List of Pigot’s Trade Directories by date */http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3659" xml:space="preserve">[[File:PigotDirectory1839Kent.jpg|thumb|An example page from Pigot's 1839 directory of Kent, Surrey and Sussex. This page has information on Bromley and Canterbury]]
'''Pigot's Directory''' was a major British [[Trade directory|directory]] started in 1814 by [[James Pigot]].&lt;ref&gt;{{cite web | url=http://www.hertfordshire-genealogy.co.uk/data/directories/directories-pigot.htm | title=Trade directories in Hertfordshire}}&lt;/ref&gt;

Pigot's Directories covered England, Scotland, and Wales in the period before official Civil Registration began and are a valuable source of information regarding all major professions, nobility, gentry, clergy, trades and occupations including taverns and public houses and much more are listed. There are even timetables of the coaches and carriers that served a town.

Parishes are listed for each area with useful information including the number of inhabitants, a geographical description and the main trades and industries of the area or town.

{{TOC right}}

==List of Pigot’s Trade Directories by date==
* {{cite book |url=https://books.google.com/books?id=4llGAAAAYAAJ |title= Commercial Directory for 1818-19-20 |location=Manchester |publisher=James Pigot |year=1818 }}
* {{Citation |publication-place = London |publisher = J. Pigot &amp; Co. |url =https://books.google.com/books?id=zBEHAAAAQAAJ |title = Pigot &amp; Co.'s metropolitan guide &amp; book of reference to every street, court, lane, passage, alley and public building, in the cities of London &amp; Westminster, the borough of Southwark, and their respective suburbs |publication-date = 1824 }}
* {{cite book |url= https://books.google.com/books?id=hdMHAAAAQAAJ |title= Pigot &amp; Co.'s National Commercial Directory for 1828-9 |location=London |publisher=James Pigot }}
*{{Citation |url = http://openlibrary.org/books/ia:pigotcosnational1837dire/Pigot_and_Co.'s_national_commercial_directory_for_the_whole_of_Scotland_and_of_the_Isle_of_Man_..._t |title = Pigot and Co.'s National Commercial Directory for the Whole of Scotland and of the Isle of Man, ... Manchester, Liverpool, Leeds, Hull, Birmingham, Sheffiled, Carlisle, and Newcastle-upon-Tyne |publication-date = 1837 |location = London |publisher =J. Pigot &amp; Co. }}

==List of Pigot’s Trade Directories by geographic coverage==
*[[Bedfordshire]] 1839
*[[Cambridgeshire]] 1839
*[[Cambridgeshire]] 1830
*[[Derbyshire]] 1835
*[[Durham, England|Durham]] 1834
*[[Essex]] 1839
*[[Herefordshire]] 1835
*[[Hertfordshire]] 1839
*[[Huntingdonshire]] 1830
*[[Huntingdonshire]] 1839
*[[Kent]] 1839
*[[Leicestershire]] 1835
*[[Lincolnshire]] 1835
*[[London]] 1839
*[[Middlesex]] 1839
*[[Monmouthshire (historic)|Monmouthshire]] 1835
*[[Norfolk]] 1839
*[[North Wales]] 1835
*[[Northumberland]] 1828
*[[Northumberland]] 1834
*[[Nottinghamshire]] 1835
*[[Rutlandshire]] 1835
*[[Shropshire]] 1835
*[[South Wales]] 1835
*[[Staffordshire]] 1835
*[[Suffolk]] 1830
*[[Suffolk]] 1839
*[[Surrey]] 1839
*[[Sussex]] 1839
*[[Sussex]] 1840
*[[Warwickshire]] 1835
*[[Worcestershire]] 1835

==References==
{{reflist}}

==External links==
* {{citation |title=Historical Directories - England &amp; Wales |publisher=[[University of Leicester]] |location=UK |url=http://specialcollections.le.ac.uk/cdm/landingpage/collection/p16445coll4}}. Includes digitized Pigot's &amp; Slater's directories for England &amp; Wales, various dates
* {{citation |title=Historical Directories - Scotland |publisher=[[National Library of Scotland]] |location=UK |url=http://www.nls.uk/family-history/directories/post-office/index.cfm?place=Scotland }}. Includes digitized Pigot's &amp; Slater's directories for Scotland, various dates

[[Category:Directories]]


{{ref-book-stub}}</text>
      <sha1>5h7qfuctqsjn228pou7vjz0175py7l5</sha1>
    </revision>
  </page>
  <page>
    <title>Who Owns Whom</title>
    <ns>0</ns>
    <id>24167579</id>
    <revision>
      <id>748145834</id>
      <parentid>722514951</parentid>
      <timestamp>2016-11-06T16:18:38Z</timestamp>
      <contributor>
        <username>W w smith</username>
        <id>29497539</id>
      </contributor>
      <minor />
      <comment>fixed broken link (404) for external reference "gapbooks"</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2736" xml:space="preserve">'''''Who Owns Whom''''' is a set of annual directories published by GAP Books in association with [[Dun &amp; Bradstreet]] (D&amp;B).  They provide the relationship between companies worldwide showing who is the ultimate [[parent company]] and who are their [[subsidiaries]].  Details include parent name, address and telephone number, country of incorporation and [[SIC code]] for each ultimate parent company along with the names of the subsidiaries, where they are based, and who owns whom for each subsidiary.  The set of directories are broken down into seven geographic regions: UK &amp; Ireland; West Europe; North Europe; South, Central &amp; East Europe; North &amp; South America; Australasia, Asia, Africa &amp; Middle East.

''Who Owns Whom'' was first published in 1958 by D&amp;B, who still own the rights to the data.  Within the set of ten directories are listed approximately 2,500,000 [[corporate groups]], ranging from companies with hundreds of members across all continents to single-country groups with only a handful of members.

==Criteria for entry==
In order to maintain full coverage, all entries in ''Who Owns Whom'' are published entirely [[free of charge]].  Every effort is made by D&amp;B, who supply the data, to include all corporate groups within the coverage area.  The corporate group may be a vast [[multinational corporation|multinational]] with a range of subsidiaries spanning many different countries and industries, or it may consist of two companies – a parent and a subsidiary.  There is no size criterion for entry.  All corporate groups, which come to the attention of D&amp;B, are included regardless of their [[revenue|sales turnover]] or number of employees.  Companies and organisations are only included if ownership is greater than 50% by other companies or organisations.

==Coverage==
All types of industries are covered, including [[agriculture]], [[forestry]] and [[fishing]], [[mining]], [[construction]], [[manufacturing]], [[transportation]], [[communication]] and [[public utilities]], [[wholesale]], [[retail]], [[finance]] and [[insurance]], [[public services|services]] and [[public administration]].  Both public and private companies are covered, along with many companies owned by official bodies such as governments, nationalised industries and state holding companies which have subsidiaries, but are not themselves independently registered.

==External links==
* http://library.dialog.com/bluesheets/html/bl0522.html
* http://www.gapbooks.com/shop/dandb-who-owns-whom/
* http://www.loc.gov/rr/business/duns/duns30.html

[[Category:Directories]]
[[Category:Yearbooks]]
[[Category:Corporate subsidiaries]]
[[Category:Books about multinational companies]]


{{globalization-book-stub}}
{{Ref-book-stub}}</text>
      <sha1>5hf7q2c4idjaa7kp1y3o3ips4aapoye</sha1>
    </revision>
  </page>
  <page>
    <title>Major Information Technology Companies of the World</title>
    <ns>0</ns>
    <id>33232338</id>
    <revision>
      <id>604710562</id>
      <parentid>593209219</parentid>
      <timestamp>2014-04-18T08:48:56Z</timestamp>
      <contributor>
        <username>John of Reading</username>
        <id>11308236</id>
      </contributor>
      <minor />
      <comment>Typo/[[WP:AWB/GF|general]] fixing, replaced: published published → published using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="859" xml:space="preserve">The '''Major Information Technology Companies of the World''' is a directory of [[information technology]] companies published by [[Graham &amp; Whiteside]] annually since 1997. The directory contains over 8000 companies.&lt;ref&gt;{{cite web|url=http://www.gale.cengage.com/servlet/BrowseSeriesServlet?region=9&amp;imprint=000&amp;cf=ps&amp;titleCode=MITCW|title=Major Information Technology Companies of the World|publisher=Cengage|accessdate=27 September 2011}}&lt;/ref&gt; The directory is also available online as part of the Gale Directory Library.&lt;ref&gt;{{cite web|url=http://www.gale.cengage.com/pdf/facts/GML25909_MITCOW_Major_GDL.pdf|title=Major Information Technology Companies of the World|publisher=Gale|accessdate=27 September 2011|year=2009}}&lt;/ref&gt;

==See also==
*[[Corporate Technology Directory]]

==References==
{{reflist}}

[[Category:Directories]]


{{technology-stub}}</text>
      <sha1>jmwahp1oteo1dxevpk1q61w0xm0y8q3</sha1>
    </revision>
  </page>
  <page>
    <title>Royal Almanac</title>
    <ns>0</ns>
    <id>31485471</id>
    <revision>
      <id>729621945</id>
      <parentid>625425900</parentid>
      <timestamp>2016-07-13T13:27:35Z</timestamp>
      <contributor>
        <username>LouisAlain</username>
        <id>14909828</id>
      </contributor>
      <comment>link to new article</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="34262" xml:space="preserve">{{unreferenced|date=May 2011}}

The '''Royal Almanac''' is a French administrative directory founded in 1683 by the [[bookseller]] Laurent d'Houry, which appeared under this title from 1700 to 1792, and under other titles until 1919.

He presented each year in the official [[order of precedence]], the list of members of the [[royal family]] of [[France]], the princes of blood, and the main body of the kingdom, great crown officers, senior clerics, abbots of large abbeys (with income of each abbey), marshals of France, colonels and general officers, ambassadors and consuls of France, presidents of the main courts, state councilors, bankers, etc..

Despite the fact that he could present indigestible because of the many lists that he was composed, he enjoyed a wide circulation with a readership consisting primarily of financial, political and all persons who had an interest in knowing the administrative organization of France.

Although his edition is due to the initiative of a private publisher, included in the lists of the Almanac was a royal official and abuse were therefore punished. Thus, a [[Poitou|Poitevin]], Pierre Joly, was interned in the [[Bastille]] at the end of the eighteenth century to have usurped the banking profession by being registered as such in the Almanach Royal.

His edition was in regular format in-8 o editor with a binder leather adorned with a sprinkling of [[fleur de lys]] gold.

==History==
===Founded at the request of King===
Laurent d'Houry imagines a calendar or [[Almanac]] 1683. The first edition contained only a few pages with a calendar and omens for the coming year. The last edition in this form, in 1699, already shows some lists that foreshadow the upcoming Royal Almanac. Thus we find lists of counselors of state with their ordinary homes, the commissioners of the Board, auditors general and stewards of finances, the Chancellor, archbishops and bishops of France, universities, and the list of major exhibitions, sessions of the courts of [[Paris]] and the log of the Palace, and finally addresses the messengers and items indicating the day of departure.

In 1699, [[Louis XIV]] asked him what the author describes in detail his work. Here as his widow explains these beginnings :

:"Louis XIV, who wanted this glorious memory Almanac, made him ask the author, who had the honor to present to Her Majesty's what induced him to give the title of Royal Almanac, &amp; to make it his principal occupation of this work. "

The same year [[Louis Tribouillet]], chaplain of the king and canon of [[Meaux]], publishes its State of France. This book describes in detail the functioning of the Court of the King, all his ministers, the treatments they receive, the various expenses of the state, clergy, etc..

The first edition of the Almanac Royal appears in 1700, at the same time as another book, Calendar of the Court of John Colombat, one of the printers of the King. Parisians have a choice between three books with similar content: the Almanach Royal Houry, Calendar of the Court of Colombat and the State of France Tribouillet. At that time, the yard around Louis XIV is highly stratified and since the expansion of the [[Palace of Versailles]] in 1684, it continues to grow. In this context, recognition of peers is a valuable asset "if someone has just placed a new post is an overflow of praise in its favor during the floods and the Chapel (...) but c is that while envy and jealousy talk like adulation.. One can understand the need to maintain directories so that everyone can follow the evolution of all these people. The multiplicity of these publications so says the king's will want to "officially" referencing his [[courtier]]s to charges created to keep beside him at [[Palace of Versailles|Versailles]], and maintain the jealousies of each other.

Even if the king gave his approval, publishing such a book is not without risk. In December 1708, Laurent d'Houry is being prosecuted for having established a [[printing press]] in his house and forced to sell its equipment two months later. Then, in February 1716, he was imprisoned in the Bastille on complaint of the [[Earl of Stair]], the British Ambassador, "to be disrespectful in his almanac, the King George by not naming him not as king of England, or rather from Great Britain, and mention the king as son of Jacques II Stuart, exiled to St. Germain.

===Affirmation of a monopoly===
The Almanac and the Royal Court Calendar coexist peacefully for ten years and a lot of money to their authors, but from 1710, Laurent d'Houry integrates more and more topics like the book Colombat Biblio. The abscess broke out in 1717 when Houry Almanac releases its Abstract that will follow the format of the Calendar of the Court and simultaneously filing a [[lawsuit]] against its competitor. A Judgement of Solomon is made: if it is allowed to Houry now to continue the publication of its abstract and to counterfeit the Court Calendar, Colombat is obliged to freeze its calendar format and forbidden 'expand content. This stops any changes in the calendar of the Court, leaving the way open to the Almanac.

Upon the death of Laurent Houry in 1725, his family is [[destitute]]. Revenues from sales of the Almanac are not sufficient to cover expenses of printing and bookselling. In these circumstances, his widow, Elizabeth Dubois, took over the business. Their son Charles-Maurice, who had hitherto been a mere [[proofreader]] of the Almanac, is trying to evict her mother and she is suing cons. It prepares the edition of 1726 but a ruling forbade him to publish it in his name alone. The ruling of 11 December 1726 forbids even disturb the affairs of his mother and to participate directly or indirectly to the development of the Almanac. That is why Charles-Maurice is mentioned as editor of the Almanac on the edition of 1726.

In 1731, she filed a new lawsuit against Colombat which increased its schedule despite the prohibition of 1717. Unsuccessful, she resumed the publication of the Abstract and Colombat complaint in turn, she then accepts to abandon the publication of the Abstract "if returned to Colombat format from 1718. "The disputes have become extinct with the death of the parties.

The privilege of the Almanac is about to expire, Charles-Maurice d'Houry tries one last time to seize it, but a royal letter of 27 March 1744 confirmed definitively André-François Le Breton as sole heir.

==A family of hegemony 131 years==
===Directorate of André Le Breton===
In 1728, the widow of his grandson, Houry combines son André-François Le Breton, who was 18 years old and an orphan under the [[guardianship]] of Charles-Maurice d'Houry. Andrew Francis had inherited, according to the will of the estate of Lawrence Houry, half of the rights of the Almanac, and his widow, the rest.

Under his leadership, the Almanac takes a new breath and adds new sections, which sometimes does not go without punishment. For example, in 1768, he has trouble with [[Voltaire]], who sent him a letter incendiary:

:"I say as much to Le Breton, the Almanach royal printer: I'll pay him Almanac point that sold me this year. He had the rudeness to say that Mr. President... Mr. advisor..., remains in the cul-de-sac to Menard, in the cul-de-sac of the White Coats in the ass -de-sac de l'Orangerie. (...) How can you say that president remains in a serious ass?"

In 1773, Le Breton moved his print shop in a wing of the former Hotel d'Alegre, at 13 rue de Hautefeuille, he acquired William Louis Joly de Fleury and was previously occupied by Ambassador Portugal.

In the late eighteenth century, the weather is bad and bad [[wheat]] harvests. The price of this staple increases disproportionately. Recently, a rumor that the government would have the [[monopoly]] wheat, thus perpetuating the high cost of food. This rumor became official when accidentally in his edition of 1774, Le Breton added a "Treasurer's grain account of the King" in the person of Sr. Mirlavaud. The edition of the Almanac had yet been proofread and approved by the Chancellor, but was still sentenced to close his shop for 3 months and publish a revised edition, without the line in question,.

In 1777, Le Breton was again accused of inserting information deemed subversive. It has, according to its critics, cited "the Floral &amp; Pranks Vergès &amp; Vaucresson, among the prosecutors and attorneys general of the [[Parliament of Paris]], who had been involved in a reform of parliament made by Maupeou against the venality of Parliament desired by [[Louis XV]], but annulled by [[Louis XVI]]. In rebuke, Le Breton was sentenced to " carton "section on the Almanacs that had not yet been sold, and replacement cost, the Almanac issue of those who so request it."

He died on 4 October 1779 and his cousin, Charles-Laurent Houry, son of Charles-Maurice d'Houry, took over the business.

===French Revolution===
The privilege granted to the family of Houry for the Almanac has been threatened in 1789 when [[Camille Desmoulins]], in his speech at the Lantern to the Parisians, says it will cease in favor of Baldwin, another Parisian publisher. This threat has not been brought into effect since the Almanac remained in Houry. Looking at the publications of the time, we can nevertheless see that Baldwin got the impressions of the [[National Assembly (French Revolution)|National Assembly]] and other organs of state.

===Last generation Houry===
Following the death of Joan Nera, widow of Laurent-Charles Houry, the Almanac is echoed by Jean-François-Noël Debure, husband of Anne-Charlotte d'Houry, their daughter. Debure is from a prominent and wealthy family of Parisian booksellers, especially combined with the Didot family. It is a printer since 1784 with the title's printer [[Duke of Orleans]].

Debure takes time printing of the family of Houry, but his other business is in trouble and he is forced to file for [[bankruptcy]]. To keep the property inherited from his family, Anne-Charlotte d'Houry hires a separation procedure. In November 1791, the bankruptcy is declared and it is opposed to the [[creditor]]s to preserve his legacy, and this opposition is futile and a ruling allows creditors to seize his furniture, but that does not appear to have been necessary because a subsequent decision allows him to recover property that creditors have not taken her husband.

Francois-Jean-Noel Debure dies 1802 in [[Loiret]]. However, it is focused died from 1795 through various sources. Maybe it was just left without leaving an address.

Stephen Lawrence Testu worked as a clerk in the family home Debure since 1788, and had gradually won the confidence of the household. Because of the absence of Mr. Debure, Anne-Charlotte is alone with his two son. Despite their age difference, he is 20 years younger than she, she married Testu July 1795. Testu few highlights its knowledge in the profession to convince him to transfer the management of printing. It accepts in 1797 and offered him priority over the rights of the Almanac in exchange for a perpetual [[Annuity (European financial arrangements)|annuity]] of 800 francs, then she completely abandons the Almanac. This influx of money seems to turn the head Testu who play games and learn to enjoy the easy life, neglects the direction of its establishment and constantly running out of money, he contracted many loans that gradually ruining his business. Relations were strained with his wife because he left the marital home in September 1801 and the only ties that bind the couple are now linked to multiple trials they s'intentent.

In 1810, Testu secretly sold the rights of the Almanac in which he partnered with Guyot. Anne-Charlotte d'Houry opposes this sale she saw as a usurpation, but loses the case in 1812. She gets in return a pension of 1 200 francs Testu does not pay. Indeed, a decree of 1820 declares the debtor more than 90 000 francs... In 1814, due to the large sums invested by Guyot in the case, an order confirming the owner of the Almanac, a copy of this order is also printed at the end of the following books. Testu still gets Guyot repayment of its debts and an annuity of 2,400 francs.

Guyot dismisses Testu business in 1820 and continues even to pay his annuity. The latter, again running out of money turned in 1823 against his wife, calls it reaches the marital home and she pays all household expenses, or alternatively, that she pays rent of 6000 francs. Judges déboutent Testu the marital home, since he himself had deserted 22 years earlier and has no housing to offer his wife even though she already lives in a very beautiful but still require his wife, yet very rich, to pay him a pension of 1,800 francs by invoking the solidarity between spouses.

The [[hegemony]] of the family of Houry on the Almanac established in 1683 has finally ceased in 1814 when, by order, the company is transferred to the association Guyot-Testu. Anne-Charlotte d'Houry died 22 July 1828 aged about 83 years.

===Judgement of publishing===
In 1867, edition of the Almanac is transferred to the widow Berger-Levrault, who had already published the Yearbook of the French empire diplomatic, and military Yearbook of the French empire, both published as the Almanac according to documents provided by the administration.

The edition of the Almanac stops definitively in 1919 after four years of interruption due to war, the latter number includes the years 1915 to 1919. Not found in the literature the reasons that prompted the shutdown of publication, but it can be assumed that the combination of very large volume of the book (more than 1650 pages in 1900) and the hard times that the [[Economics]] and French policy at that time was to make the management of such a volume of information extremely complex and unprofitable for the publisher. It is also possible that the new government formed after elections in 1919 no longer supported the development of the Almanac.

==Changing content==
===The topics in the Almanac===
The Almanac or Calendar, as he was called in its early editions, was just a simple calendar which were associated topics on astronomical events, the days of fairs, the newspaper of the Palace, the residence of messengers The departure of the mails, the price of currencies and the list of collectors' offices. After his presentation to the king in 1699, many items are constantly being added including the clergy, the royal family of France, then the families of other sovereign nations, officers, ambassadors, etc..

In 1705, Houry added to the list of knights of the Holy Spirit and peer and marshal of France. In 1707, it is the state of the clergy and, in 1712, the birth of kings, princes and princesses of Europe. After the death of Louis XIV, the Duke of Orleans, became the Regent, is added to the list of members of the royal family of the members of the [[House of Orleans]]. Later, he put more of his own, the full house of the Queen and princes.

It is not possible to describe all items contained in an almanac as there, so the contents of 1780 covers ten pages:

The Almanac also stands abreast of scientific advances. In the middle of the eighteenth century, improving the accuracy of clocks and many wealthy fans begin to observe and study the stars. It is indispensable to know precisely the difference between true solar time of [[sundial]]s, and mean solar time clocks, especially since the advent of clocks seconds. This is the equation of the pendulum, also called the equation of time, the table is added shortly before 1750.

With the [[French Revolution]], the Almanac exchange of title and its content is modified to match the new institutions.

The abolition of all distinctions requires overhaul the topics, timing of the vulgar era is replaced by the [[French Republican Calendar|Republican calendar]], the place reserved for kings and princes of Europe is replaced by a note on the friendly powers of France, the administrative organs of royal power are replaced by new ministries.

The content changes again with the reforms of the Consulate and the Empire, the Restoration, the [[Hundred Days]], the [[July Monarchy]], the [[French Second Republic|Second Republic]] followed the [[Second French Empire|Second Empire]] and the [[French Third Republic|Third Republic]] who sees the end of the edition of Almanac. In each case, the bindings are supplied with the times.

As the number of entries is increasing, the number of pages follows the same trend: they numbered one hundred in 1699, nearly five-hundred in 1760, and seven-hundred just before the French Revolution. The course of a thousand pages is taken in 1840 to over 1000-6-cents in 1900. On average, about thirty names are listed per page, the total number of people or places listed annually in the tens of thousands, but no table patronymic does quickly find a particular name.

All changes in the Almanac makes it a very useful book for historians that may follow, year after year, ministries and other administrative bodies, movements of people in these offices, and retail organization public services to a resident of Paris (such as places of mailboxes, timetables and fares for ticks and royal messengers ).

===Chronology of the 237 years of publishing Almanacs===
After the death of Laurent Houry, his descendants continued his work until 1814. The edition was continued until 1919. It would be tedious to describe in words the evolution of the Almanac of the 237 years that have elapsed since the first edition by Laurent d'Houry in 1683, hence the choice of this table layout.

Throughout its existence, the Almanac has crossed 11 schemes political editor changed the title 14 times and 9 times.

==Publication==
===Collecting information===
Since its inception in 1700, following a royal demand, the Almanac invented by Laurent d'Houry aims to be an official handbook.

Until the French Revolution, contributors are cordially invited to provide information to the bookseller, as pointed note of the printer in the first pages of the Almanac. In 1771, for example, we read in the Journal History of the Revolution that the Bar Association in the person of a certain Gerbier, asserted that "there would be no change in the order of the table, and that it would be printed in the Almanac as Royal was last year, leaving out only the dead."

With the French Revolution, the order was given to government to provide all information to the publisher. In 1802, Testu gets even exclusivity.

Later, the collection of information for the Almanac is even part of the operating budget of the ministries and can be seen in order of December 31, 1844 signed by [[Louis-Philippe I]] "on the organization of the Ministry Administration Navy "Article 6 of which list the items in the budget" the formation of the Royal Almanac.

====Typography====
The print quality improves significantly when Laurent d'Houry became printer. It multiplies the bands and tail-lights to decorate and titles for sections. The Almanac is still very poor prints because the image is not its goal. Only that the reader can find are patterns explaining the oppositions of the planets and eclipses are present every year, and the map of [[departments of France]] editions of 1791 and 1792.

Despite the short time to prepare the book, the printer treats the presentation and uses in the case of many variations in size and shape of characters for easy reading of long lists, special characters to emphasize certain lines, compositions tables or columns and clusters in braces.

That Le Breton, grand son of Laurent Houry, who brings more to the book. It increases greatly and restructures the Almanac, and also improves its presentation in order to preserve readability. Many notes are added to guide the reader and help in understanding the operation of certain administrative bodies.

The Almanacs modern nineteenth-century advantage of technological advances. Cartoon characters are modernized and the use of fonts to graphics customizable multiplies, sometimes to excess: we can count at least 7 in 11 fonts fonts differ on the cover page of the National Almanac 1850 printed by Guyot et Scribe!

Announcements, the ancestors of [[advertising]], are introduced by the publisher Berger-Levrault in the late nineteenth century.

====Well-to-shoot====
The deadline for submitting this information to the editor is set to "first ten days of October (or November). The last-minute changes are incorporated in an erratum end of the book. When they are too large, they may even delay the release. In late December, an event is sent to the administration for approving the content. This approval is required before the sale.

It leaves only two months to integrate the information of the year in the text of the previous edition and call all of the pages before submitting the book for the right to shoot. The editing step, at least for the test in 1706, has not been done with great care as can be seen by very many shells and mistakes which have crept into the [[table of contents]] presented in thumbnail to the right.

Once the administrative agreement obtained, it is inserted end of the book, the Almanac is stapled or bound and is then distributed to customers at the end of the year.

===Printing===
Early Almanacs were not printed by Laurent d'Houry. The Almanac of 1706 and is printed by Jacques Vincent, installed Huchette street, at the sign of the Angel. November 15, 1712, Laurent Houry became printer and immediately began printing his work. Then all the almanacs will be printed by their publishers.

====Draw====
There is no source that explains the draw of the Almanac. The only figures available are the annual rents generated by sales.

In 1782, Mercier said a pension of more than 40 000 francs. Diderot, at the same time, puts the figure at 30,000 pounds. For a price of sale of 5 to 6 pounds, the draw must necessarily be greater than about 7500 almanacs.

Around 1820, during the trials that have brought the widow and Debure Testu, income of the Almanac was estimated between 25 and 30 000 francs. In 1834, another almanac, the Almanac of France, said that its cost is 35 cents for a sale price of 50 cents. Booksellers then purchase the item at prices of 38 cents, to resell a suggested retail price of 50 cents. The publisher earns so 3 cents per pound sold, the bookseller earns 12 (minus shipping costs, dependent). If one considers - arbitrarily - a four Almanac is sold directly into the library Testu (priced at 10 francs 50) the remainder being passed through intermediaries, we can prorate that to generate an annual income of 30 000 francs Testu must sell approximately 25 000.

In the absence of more precise information, we can only estimate at about 15,000, the number of copies sold per year between the late eighteenth and early nineteenth century.

====Binding====
The almanac is sold either stapled or bound by the printer. The [[paperback]] version allows the purchaser to connect his book as he wishes, and so it is possible to find books with bindings very ornate, with lace, weapons of families, many colors brightened or gilding Biblio 24, etc..

The bound version provided by the printer is usually presented in a bound in calf or Morocco,{{clarify|date=August 2012}} full, and lilies in the boxes back. With the revolution, the lilies are replaced with Phrygian caps in roundels Library 25.

====Distribution====
The Almanac is normally available from the bookseller, but it can also be found in the province in other bookstores that serve as intermediaries, for example in 1816, at Pesche, bookseller at Le Mans Ref 17, or by correspondence through the [[Sorbonne]], as did Voltaire Ref 18.

===Readership===
The Almanac has a very great interest because of the number of subjects it addresses the organization of the French administration. In 1785, Mairobert wrote that "the Almanach Royal is in the hands of everyone and is among the Princes, the King's office, the foreign ministers would cater Ref 19. Louis-Sebastien Mercier in a pamphlet, the Tableau de Paris that stands in 1782 Ref 20 explains that "Those who are thrown into the paths of ambition, study Almanac Royal with serious attention," "more a beautiful royal consult the almanac to see if her lover is a lieutenant or sergeant,... ", that" everyone is buying the almanac to find out exactly where they stand. "And finally" even Fontenelle said, that it was the book which contained the greatest truths."

Adages use the almanac as a reference. According to Jean-François de La Harpe is "the only book to read to get rich is the Royal Almanac Ref 21, [[Jean-Joseph Regnault-Warin]] uses the phrase" having the memory of a Royal Almanac Ref 22 " or the Memoirs of the Academy of hawkers Ref 23 explains that "it is enough to read the Almanac for education."

In the eyes of [[justice]], the book can be used as a basis for comparison: during a police investigation in 1824 Ref 24, a defendant defends himself by explaining that the volume of documents he was accused of having carried "could be equal to that of a royal Almanac almanac or a related trade."

Whether to have a certain level of resources to purchase this item, the customer extended beyond the financial and political world.

==Competition==
The Royal Almanac is competing at its inception with the Almanac of the Court of Colombat who can not make it evolve since 1717. In fact, bibliographers consider that the Royal Almanac is one of the "oldest and most helpful Ref 25". If it essentially describes the royal court and the Parisian institutions, other major cities also have their almanacs, such as the city of Lyon equally voluminous Ref 26. The Almanac is however considered a reference book. In 1780, a notice of a bookseller named Desnos inserted at the end of the Gazette of 27 offers for courts Ref 8 pounds to "the statesman, letters, and generally all persons attached to the service of the King (... ) Almanach Royal, Calendar of the Court, said Colombat, Mignone Strennas-Note 22, Ref 28, the State Military Note 23, the four connected units, with shelves &amp; stylus to write, which makes the closure ": the Royal Almanac ranks first in the collection.

===The Court Calendar===
Since 1717, the Calendar of the Court can not change, its sections are limited to an ephemeris of the celestial motions (30 years) increased by astronomical tables with sky conditions, and timing of the court to the family and royal house, lists of boards, departments and secretaries of state finances, births and deaths of kings, queens, princes and princesses of Europe, the knights of various orders, the archbishops and bishops of the kingdom and Cardinals of the Sacred College.

It is primarily sought for its ephemeris of the celestial motions and astronomical tables of events

===The Almanac of Commerce===
The Almanac of Business, published by Sébastien Bottin in the eighteenth century contains, besides the addresses of shops in Paris, many useful statistics financiers. It is supplementary to the Almanach royal, which concerns only the French administration.

===The State of France===
Some have criticized the Almanach Royal of being a [[plagiarism]] of the State of France, another administrative directory, the first publication seems to have been made in 1619 and is still published in the middle of the eighteenth century Ref 30. However, the edition of 1736 of the State of France said it was a "periodical whose audience has applied for renewal from time to time, and had been published until 1699, 1702, 1708, 1712, 1718 and 1727 Ref 31. The latest editions of 1727 and 1736 five volumes contain over 500 pages each. Offices are described down to the smallest detail Note 25, the state of France is a companion volume of the Almanach royal use by those who wish to deepen their knowledge on the functioning of the French administration.

==Examples of information contained in the Almanac==
Further details concerning the organization of the administration of the French state, and persons who occupied positions, many other topics are discussed in the Almanacs, for example in the eighteenth century:

===The cost of construction in Paris===
This section is only found in the Almanacs of the early eighteenth century, and stops just before 1726.

There are prices for masonry, carpentry and joinery, roofing, locksmithing, painting and glazing that are usually in Paris, for example:

:"Walls of circular pits, with layers of stone studded with low excess moilon quilted 18 inch thick, 22 pounds fquare fathom, and more in proportion to the depth of the wells, or other difficulties that may encounter."

With these data, the historian is able to quantify fully the construction of a building in Paris at that time.

===The official ceremonies===
The Almanac explains in great detail some official ceremonies:
* Opening Ceremony of the Annual Courthouse
:"The Entry of Parliament is the day after the S. Martin, 12 November, which day Presidents in red dresses holding their furs &amp; Mortar Note 26, &amp; Gowns Gentlemen Consultants red, after attending the solemn Mass that are usually said by a Bishop in the grand hall of the Palace, receive oaths of Lawyers &amp; Counsel. The first president made this day a speech to thank those who celebrated the Mass, which responds to him by another harangue Note 27."
* Procession of the University, whose description takes three pages of the Almanac Note 28
:"The Rector of the University at the end of its Rector, who regularly is only three months, indicating a general procession which assists the whole body. It is a ceremony that deserves to be seen. We will mark the place here What the doctors take the four faculties Note 29 that comprise the university, all the graduates of these faculties, with the Religious Orders Note 30. Procession from the Church of Religious Trinitarians, otherwise known as Maturin. (...) The procession is closed by the booksellers, papermakers, bookbinders, Parcherminiers, illuminators, writers swear by the University."

The detailed description of the ceremonies to stop mid-eighteenth century to make room for a still more comprehensive directory. A reference is then made towards the end of the book "guides for all kinds of ceremonies to be observed in the receipt of any office or employment whether in dress or in the Sword."

===Transportation===
Transportation of persons is ensured by the coaches, carriages, wagons and other carriages. Found in the Almanac schedules and rates of major roads.

In 1715, a passenger wishing to travel from Paris to [[Caen]] will go rue Saint-Denis on Monday at six o'clock in the morning. He has previously "sent his clothes the night before early." Fifteen years later, the starting time is advanced to 5 am in summer and in 1750, the departure is 5 hours throughout the year. In 1780, two flights are scheduled Tuesdays and Fridays at 23:30 and the journey takes two days. A van, slower, except Sunday at noon and made the trip in four and a half days in summer and five days in winter. In 1790, transportation is now provided by the General Department of stagecoaches and mail royal France. Three coaches liaise on Tuesday, Thursday and Sunday and the van on Sunday. The departure is now Notre-Dame-des-Victoires.

Rates are rarely reported but in 1725 and 1761 is 18 pounds per person tournaments. He is 21 pounds in 1770 to reach 42 pounds in 1790 (fortunately for the traveler, it is stated that the "sleeping bag weighing 10 pounds is" free").

===Company guards of the King Pumps===
In 1716, the king appoints François Perrier Dumouriez as Director General of public pump to remedy fire, without the audience is obliged to pay anything. In 1722, he founded the Compagnie des Gardes Pumps du Roy, under the direction of the same. This company later became the Brigade of firefighters in Paris Note 32.

The Almanac of 1719 lists these pumps and their wardens and deputy wardens. We then learned that a brigade is made up of four guards and four sub-custodians who are responsible for maintaining the material deposited in each district. What became three years later the Society of King's Guards Pumps were not at that time that 41 people, 17 pumps distributed in groups of 8 men and 4 or 3 pumps in the City Hall, the convent of the Grands Augustins The Carmelite convent in the Place Maubert, Convent of Mercy, and the Fathers of Little Place des Victoires, in addition to a pump at the Director General of the pumps, Rue Mazarine. Except Dumouriez guards pumps are not professional fire but shoemakers, carpenters, locksmiths, etc..

==Considerations bibliophiles==
===Availability===
Almanacs are found regularly in auctions and in the antiquarian booksellers. Given their importance documentary and the fact that there are beautiful copies, these books are particularly sought after by historians, writers, bibliophiles and enthusiasts.

Volumes in the first round of the seventeenth century often exceed several thousand euros Ref 32, the other is generally negotiated between a few tens and five hundred euros, sometimes more, depending on their rarity, condition and quality bookbinding Note 33. Just over half are however available for free download on Gallica or Google Books:

===Notes handwritten readers===
Almanacs contain some handwritten notes left by their readers. The value of the book can then be influenced upward or downward depending on the quality and content of these notes, and especially the person who wrote them - when you can identify it. They are usually found on page intentionally left blank for the ephemeris. Some of these notes can provide very interesting information, such as notes written on the page in August 1715 a copy of the BNF. It relates the circumstances of the death of Louis XIV, who was suffering from gangrene Note 34:

:"We thought the death dez Roy Lundy 25. He marched
:better a day or two quoyque hopeless. It
:died after having suffered much and with great
:Patience on Sunday September 1, r t is 8 pm Morning
:M r le Duc d'Orleans went to Parl t and was declared
:Regent on 2. September e"

==References==
{{Reflist}}

[[Category:Historiography of France]]
[[Category:18th-century books]]
[[Category:Directories]]</text>
      <sha1>kvw919kfshuntua849gdkt623w30s77</sha1>
    </revision>
  </page>
  <page>
    <title>List of yellow pages</title>
    <ns>0</ns>
    <id>12767772</id>
    <revision>
      <id>761527664</id>
      <parentid>753653344</parentid>
      <timestamp>2017-01-23T12:56:52Z</timestamp>
      <contributor>
        <username>John of Reading</username>
        <id>11308236</id>
      </contributor>
      <minor />
      <comment>Typo fixing, replaced: On April 2007 → In April 2007, .. → ., is been → has been using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="42152" xml:space="preserve">{{multiple issues|
{{Advert|date=May 2010}}
{{Refimprove|date=May 2010}}
}}

[[Yellow pages]] [[telephone directories]] of businesses:

{{compact ToC|side=yes|top=yes|num=yes}}

==A==
* '''Afghanistan''': In [[Afghanistan]], the Canadian INGO [[Peace Dividend Trust]] launched a free online directory with over 2700 verified and registered Afghan enterprises in late 2006.
* '''Africa''': In [[Africa]], a business directory is YelloPagesAfrica published by ''Yellopages Development Company Limited''. It is an online business directory. It ia an interactive online business directory with a mission to integrate Africa businesses. It covers the entire Africa continent and operates on a do-it-yourself basis.
* '''Albania''': In [[Albania]], the directory is called Flete te Verdha - Albanian Yellow Pages which is a registered trademark belonging to Maxidisk SH.P.K Group and Fleteteverdha sh.p.k from Tirana.
* '''Algeria''': In [[Algeria]], the Yellow Pages business directory is published in French as '''Les Pages Jaunes'''. It is also available online in English and in French.
* '''Anguilla''': In [[Anguilla]], the directory is published by [[Global Directories Limited]] and titled ''Anguilla Yellow Pages''. Print copies are distributed free to each telephone subscriber and is also available online at www.anguillayp.com.
* '''Armenia''': In [[Armenia]], "Spyur" Information Center introduces "Armenia Yellow Pages". Directory is printed in English, [[Armenian language|Armenian]] and [[Russian language|Russian]](17500 copies a year). Other directory Armenian Business Pages was launched in 2015 by Comfy LLC and represents only digital version of yellow pages of Armenia. The directory is in the process of Electronic Armenia® trademark registration.&lt;ref&gt;http://www.pages.am&lt;/ref&gt;
* '''Aruba''': In [[Aruba]], the official telephone directory of Setar is published by [[Global Directories Limited]] and titled ''Aruba Yellow Pages''. 85.000 Print copies are distributed free to households and companies and is also available online
* '''Austria''': The "Yellow Pages" and [[Yellow Pages#Internet yellow pages|IYP]] services are provided by: HEROLD Business Data GmbH, a [[European Directories]] group company.
* '''Australia''': In Australia, the most comprehensive business directory is the Yellow Pages published by [[Telstra#Directories and advertising (Sensis)|Sensis]]. The directory is also available online, on mobile and via smartphone app.

==B==
* '''Bangladesh''': In [[Bangladesh]], the business directory is published by '''''Ad Yellowpages''''' '''''Pages''''' and titled '''''Ad Yellowpages Yellow Pages'''''. AdYP, a sister conern of Ad Yellowpages.com is an new concept brought forward by the founders of the site. The site provide a variety of information about local places and businesses in Bangladesh.&lt;ref&gt;https://localyaar.com&lt;/ref&gt;
* '''Bahrain''': In [[Bahrain]], the business directory is published by '''''Primedia International BSC (c)''''' and titled '''''Bahrain Yellow Pages'''''. Primedia International signifies a fundamental move away from the traditional business directories to new print &amp; online media.
* '''Barbados''': In [[Barbados]], the directory is published by [[Global Directories Limited]] and titled ''Barbados Yellow Pages''. Print copies are distributed free to each telephone subscriber and is also available online at www.barbadosyp.com and on mobile devices at yp2go.bb.
* '''Belarus''': In [[Belarus]], the directory is titled ''Business-Belarus'' ([[Russian language|Russian]]), it is also available online. There is an alternative directory, called ''Belarus XXI vek'' (Belarus 21st century), which is analogue to Yellow Pages; it is also available online.
* '''Belgium''': In Belgium, the directory is titled ''Pages d'Or'' (golden pages) (French) or ''Gouden Gids'' (golden guide) ([[Dutch (language)|Dutch]]), and is distributed free to each telephone subscriber, it is also available online.
* '''Bonaire''': In [[Bonaire]], the directory is published by [[Global Directories Limited]] and titled ''Bonaire Yellow Pages''. Print copies are distributed free to each telephone subscriber.
* '''Bolivia''': In [[Bolivia]], yellowpages exist online under the URL ''Yellow Pages.com.bo''.
* '''Bosnia''': In [[Bosnia and Herzegovina|Bosnia]], Yellow Pages exist online under YellowPages.ba.
* '''Brazil''': In [[Brazil]], the directory is titled ''Páginas Amarelas'' and is distributed free to each telephone subscriber. Available online by DYK Internet S/A.
* '''British Virgin Islands''': In the [[British Virgin Islands]], the directory is published by [[Global Directories Limited]] and titled ''British Virgin Islands Yellow Pages''. Print copies are distributed free to each telephone subscriber and is also available online at www.britishvirginislandsyp.com.

==C==
* '''Cambodia''': In [[Cambodia]], the official Yellow Pages directory is called [[Cambodia Yellow Pages]] and published under contract to local Ministry of Posts and Telecommunications by [[CAMYP Co., Ltd]].
* '''Canada''': In Canada, the company [[Yellow Pages Group]] owns the trademarks ''Yellow Pages'' and ''Pages Jaunes''. It produces and distributes directories in both English and French. Yellow Pages Group is the market leader in print and online commercial directories and one of the largest media companies in Canada, producing the official directories of [[Bell Canada]], [[Telus]], [[Aliant]], [[Manitoba Telecom Services|MTS]], and others. [[Saskatchewan]]'s [[SaskTel]], through subsidiary [[DirectWest]], is believed to be the last major [[incumbent local exchange carrier]] to publish its own directories. Competitive local directory publishers, such as PhoneGuide or DirectWest's operations in Manitoba and Alberta, usually include commercial directories on yellow paper.
* '''Cayman Islands''': In the [[Cayman Islands]], the directory is published by [[Global Directories Limited]] and titled ''Cayman Islands Yellow Pages''. Print copies are distributed free to each telephone subscriber and is also available online and on mobile devices.
* '''Chile''':
* '''China''': In China, the modern yellow pages industry was started in the late 1990s with the formation of two international joint ventures between US yellow pages publishers and China’s telecom operators, namely: a joint venture started in Shenzhen between [[RHDonnelley]] and [[China Unicom]] (later including Hong Kong’s PCCW and InfoSpace); and a joint venture between [[China Telecom Shanghai]] and what later came to be known as the yellow pages operations of [[Verizon]] {{Citation needed|date=May 2010}}. Later, another mainly state-owned telecom operator, [[China Netcom]] began to produce, either directly or on a sub-contracted basis, yellow pages in selected cities around the country. By early 2005, there were a number of independent local and international yellow pages operators in numerous cities including [[Yilong Huangbaoshu]], based in Hangzhou, Zhejiang Province with operations in Hangzhou and Ningbo {{Citation needed|date=May 2010}}. However, there is no nationwide Yellow pages in any format and only some international-trade related businesses including INBIT (USA), CHINAPAGES.COM and ALIBABA.COM (Chinese) are running some kind of national online databases based on business lists not from telephone companies. [[China Yellow Pages]] is also a common-place for finding manufacturers and exporters from China.
* '''Colombia''': In [[Colombia]], the standard yellow and [[White Pages]] are published and distributed every year free of charge by [[Publicar]], a Colombian subsidiary company of [[Carvajal Group|Carvajal]], which also publishes and distributes yellow and white pages in other Latin American countries.
* '''Croatia''': In [[Croatia]], the directory is called ''Žute stranice'' (yellow pages), published by [[MTI Telefonski imenik/Zute stranice]]. Another directory is ''CroPages Business Directory/Poslovni Adresar'', published by [[Masmedia]].
* '''Cuba''': In [[Cuba]], the equivalent online directory is titled [[Paginas Amarillas]], with information on the whole of Cuba.
* '''Cyprus''': In [[Cyprus]], the Yellow Pages is edited by ID Yellowpages Ltd [[Cyprus Yellow Pages Directory]].
* '''Cyprus (North)'''; In [[Turkish Republic of Northern Cyprus]] [[CYPYP North Cyprus Yellow Pages]]
* '''Czech Republic''': In the [[Czech Republic]] and [[Slovakia]], the directory is titled ''Zlaté stránky'' (golden pages), published by [[Mediatel]], Prague (a [[European Directories]] group company) and is distributed free to each telephone subscriber, usually in exchange for its previous version.

==D==
* '''Denmark''': In Denmark, a full online directory including most phone numbers is provided by ''De Gule Sider'' (a brand of Eniro, a Nordic search engine and directories company), with paper versions of yellow and white pages distributed to subscribers throughout the country; it was formerly a part of [[TDC Forlag]], a subsidiary of the national telecoms operator.
* '''Dominica''': In [[Dominica]], the directory is published by [[Global Directories Limited]] and titled ''Dominica Yellow Pages''. Print copies are distributed free to each telephone subscriber and is also available online at www.dominicayp.com.
* '''Dominican Republic''': In [[Dominican Republic]], published by [[Caribe Media]]. Publishing of printed and / or digital directories in the Dominican Republic.

==E==
* '''Egypt''': [[Egypt Yellow Pages Ltd]] is the official publisher of Yellow Pages branded products in Egypt. Egypt Yellow Pages Ltd, founded in 1991, is the owner of the Yellow Pages trademark in Egypt.
* '''Europe''': For whole Europe, the [[European Yellow Pages]] apply. The European Yellow Pages is an effort of providing harmonized data to different language environments through keeping the character of having localized search capabilities on a regional level. Harmonizing data in this context means providing information to global users mainly in English and to local users in their native language.
* '''Europe''': For [[Europe]] the directory [[Yellobook.eu]] is providing information about many branches and companies all around 33 major European countries.

==F==
* '''Finland''': In [[Finland]], the directories are called ''Keltaiset sivut'', Eniro.fi and [[Teloos.fi]]
* '''France''': In France, Yellow Pages are referred to as ''Pages Jaunes''. They are distributed free by Pagesjaunes'''.fr''', a company affiliated with [[Orange S.A.|France Télécom]]. Pagesjaunes'''.com''', the .com version of ''Pages Jaunes'', was the issue of a major court case at [[World Intellectual Property Organization|WIPO]]; the original registrant, an individual from Los Angeles, won against France Télécom. This court decision defended by the Parisian Lawyer, Andre Bertrand, was path-setting for the whole European Yellow Pages industry, as it decided that the phrase "Yellow Pages" cannot be considered the property of a single company. Previously, many former state monopoly telecom companies outside the US had tried to ban competition by claiming the term "yellow pages", or the translation of "yellow pages" into the vernacular, as their exclusive trademark. [[Vivendi|Vivendi Universal]] moved to enter the French Yellow Pages market in 2001 with scoot.fr, but the attempt was a killed by a reorganisation of the struggling company. Another French editor of Yellow Pages is [[Bottin]]. More competition is expected in November 2005 from the liberalisation of "12", the former unique "[[4-1-1]]" number of [[Renseignements Telephoniques]], French for Directory Inquiry. In November 2006 [[Orange S.A.|France Télécom]] sold its majority share in pagesjaunes.fr to Mediannuaire. In August 2007 pagesjaunes'''.com''' finally became active, giving France two different ''Pages Jaunes''; thus creating agitation at pagesjaunes'''.fr''', which reshaped their site and started a massive advertisement campaign all over France.

==G==
* '''Georgia''': In Georgia, the directory is called ყვითელი ფურცლები and published by "Yellow Pages Tbilisi" Ltd.
* '''Germany''': In Germany, a directory titled ''Die Gelben Seiten'' is distributed free to each subscriber, by the [[Deutsche Telekom]], owner of [[T-Mobile]]. Other Yellow pages are edited by ''Go Yellow.de'', ''Klicktel.de'' and [[Gelbex.de]]. In 2006 a lawsuit with the [[Deutsches Patentamt]] denied the validity of the German Trademark "Gelbe Seiten" which in fact is the German translation of the universal expression "Yellow Pages". Klaus Harisch, an Internet Pioneer from Munich and founder of Go Yellow.de had spent over 7 Million Euros on Lawyer Fees to fight for the cancellation of the German "Gelbe Seiten" trademark. Deutsche Telekom had also registered "Yellow Pages" as a German trademark which they lost at the same time. On a European Level Deutsche Telekom had failed to register "Gelbe Seiten Deutschland" or "Yellow Pages Germany" as a Euro Trademark with [[OMPI]].
* '''Gibraltar''': A combined White and Yellow Pages directory, along with an [[Yellow Pages#Internet yellow pages|IYP]] service, are provided by: gibyellow.gi, a [[European Directories]] group company.
* '''Greece''': In Greece, Yellow Pages are called ''"Chrysos Odigos"'' that can be translated as "The Golden Guide".
* '''Grenada''': In [[Grenada]], the directory is published by [[Global Directories Limited]] and titled ''Grenada Yellow Pages''. Print copies are distributed free to each telephone subscriber and is also available online at www.grenadayp.com.
* '''Guyana''': In [[Guyana]], the directory service is provided by "[[GT&amp;T]]" in printed format and in online services, there are quite a few, some of them are "[[YellowPagesGuyana]]", "[[YellowGuyana]]" and "[[GT&amp;T]]'s" own online yellowpages directory--"[[yellowpages.com.gy]]".

==H==
* '''Hong Kong''': In [[Hong Kong]], the phone directory is titled ''Hong Kong Yellow Pages'', published by [[PCCW|PCCW Media Limited]].
* '''Hungary''': In Hungary, the directory is called ''Arany Oldalak'' (gold pages); are published and distributed by [[MTT Magyar Telefonkönyvkiadó]] Kft, Budaörs.

==I==
* '''India''': [[India]] is a very large country in terms of population, business activities and economy. There are multiple Yellow Pages being published by private sector companies. Some of them focus the whole nation and some are regional.
* '''Indonesia''': In [[Indonesia]], the telecommunication company [[Telkom (Indonesia)|Telkom]] with [[PT. Infomedia Nusantara]] (one of its subsidiaries), regularly publishes phone books. The company provides directory, call centre, and content services since 1984. The phone books consist of white pages and yellow pages, which are published in hard and soft copies.
* '''Iran''': In the Islamic Republic of [[Iran]], the directory is called ''The first book'' or in [[Persian language|Persian]] ''Keta:b e Avval''. This directory divides into different sections such as Directory of Businesses, jobs and maps and city guides. There is an official YellowPages in Iran owned and published by Iranian Yellow Page company. It has been developed in Persian and English languages, and contains different categories and locations of Iran. There is also an unofficial company that runs ''The Iran Yellow Pages''. This directory is published by Moballeghan Publishing and Advertising Company (1986) with the cooperation of The Trade Promotion Organization of Iran. By 2010 a new updated comprehensive directory called ''"The First Portal"'', or ''"First Eurasia E-commerce"'' or in [[Persian language|Persian]] ''"تجارت الكترونيك اول"'' comes to the [[Iran]] high potential markets.
* '''Iraq''': In Iraq, the directory is called ''Yellow Pages'' or in Arabic (Al Safahat al Safraa). This directory divides into different sections such as directory of businesses, jobs and maps and city guides and contains thousands of businesses in many categories. The directory is published by Alam Al-Rooya Publishing and Advertising Company.
* '''Ireland''': In the [[Republic of Ireland]], the directory is called ''Golden Pages'' and is published by FCR Media. Ireland's free Yellow pages is called BusinessPages.i.e.
* '''Israel''': In Israel, the yellow pages Hebrew edition is called ''Dapei Zahav'' (Golden Pages) and the English edition is ''Golden Pages''. The print directories come out in separate issues based on Israel's different telephone area codes, published by Golden Pages Publications Ltd. Five million copies of the yellow pages are distributed annually.
* '''Italy''': in Italy, the directory is titled ''Pagine Gialle'' (Yellow Pages). The printed versions come out in separate issues for [[province]] as [[White pages]]. Some years ago, an alternative directory, called ''Pagine utili'' (Useful Page) was proposed.

==J==
* '''Jamaica''': In [[Jamaica]], the directory is published by [[Global Directories Limited]] and titled ''Jamaica Yellow Pages''. Print copies are distributed free to each telephone subscriber and is also available online at www.jamaicayp.com and to mobile subscribers at yp2go.com.jm.
* '''Japan''': In Japan, the Yellow Pages directory, are known as [[:ja:タウンページ|Town Page]], and published by [[Nippon Telegraph and Telephone|NTT]].
* '''Jordan''': In [[Jordan]], the directory is titled '' Yellow Pages - Jordan'', Yellow Pages Jordan is operated since 2001 by PAGESJAUNES LIBAN, a subsidiary of the European PagesJaunes Group - France in 1997.

==K==
* '''Kazakhstan:''' In [[Kazakhstan]], the directory is ''Yellow Pages of Kazakhstan'', published by [[Yellow Pages Kazakhstan]] Management Group.
* '''South Korea:''' In [[South Korea]], the directory is published and distributed by many publishers:
** ''BiG Yellow Pages. Korean National Directory'', by [[Yellow Pages Korea]];
** ''Korea Yellow Pages'', by [[Korea Yellow Pages]];
** ''Korea English Yellow Pages'', by [[Korea Telecom Directory]].
* '''Kosovo:''' In [[Kosovo]], [[Faqe te Verdha]] is a trademark belonging to [[KOSOFT]], [[Pristina]].
* '''Kyrgyzstan:''' In [[Kyrgyzstan]], Yellowpages can be found under the URL "yellowpages.kg".

==L==
* '''Lebanon:''' In [[Lebanon]], the Yellow Pages business directory is published in [[Arabic language|Arabic]] and French by PAGESJAUNES LIBAN.

==M==
* '''Macau:''' In [[Macau]], the phone directory is titled ''Macau Yellow Pages/Páginas Amarelas'', publ. by Directel [[Macau Listas Telefonicas]] Lda.
* '''Madagascar:''' In [[Madagascar]] yellow pages can be found via the site Madagascar Yellow Pages.
* '''Malaysia:''' In [[Malaysia]], there are 4 large directories Malaysia Yellow Pages, Malaysia Super Pages, Malaysia Business Directory and BCZ.com
* '''Maldives:''' In [[Maldives]], the commercial phone directory is called Yell.
* '''Mali:''' In [[Mali]], the equivalent online directory is titled [[Malipages.com]].
* '''Malta:''' In [[Malta]], the Yellow Pages Directory is published by [[Ark Publishing Group]]. It has been publishing the Yellow Pages since 1997 and each year distributes 200,000 directories free of charge to the general public.
* '''Mauritius:''' In [[Mauritius]], the Yellow Pages Directory is published by [[Teleservices Ltd]] and is known as MT yellow pages 
* '''Mexico''': In Mexico, there are several commercial phone directories. The incumbent is called Seccion Amarilla.com.mx (Yellow Section) is published by Anuncios en Directorios, S.A. de C.V., a subsidiary of Telmex, the local Telco. Others are Paginas Amarillas.com.mx (Yellow Pages) published by Phonebook of the World, Paginas Amarillas.com published by Publicar, Mexico Data Online.com published by the Mexico Business Directory and Paginas Utiles.com.mx published by Ideas Intercativas, S.A.
* '''Moldova:''' In [[Moldova]] yellow pages can be found via the site [[www.yp.md]].
* '''Mongolia:''' In [[Mongolia]], the directory is called ''Mongolia Yellow Pages'' (yellow pages) and can be found via [[www.yp.mn]].
* '''Montserrat''': In [[Montserrat]], the directory is published by [[Global Directories Limited]] and titled ''Montserrat Yellow Pages''. Print copies are distributed free to each telephone subscriber and is also available online.
* '''Morocco:''' In [[Morocco]], the directory is called ''Pages Jaunes'' (yellow pages).
* '''Myanmar:''' In [[Myanmar]], the directory is called ''Yangon Directory'' (ရန်ကုန်လမ်းညွန်).

==N==
* '''Netherlands:''' In [[Netherlands]], the directory is called ''Gouden Gids'' (literally "Golden Guide"), and within the district concerned, it is distributed free to each telephone subscriber, by De Telefoongids BV (a [[European Directories]] group company).
* '''New Zealand:''' In New Zealand, the yellow.co.nz directory is printed in 18 regional editions by Yellow Pages Group (YPG). YPG also publishes 18 regional editions of '[[White Pages]]' (combined government, residential and business listings), and a 'Local Directory' for some urban areas and sub-regions.
* '''Nigeria:''' In [[Nigeria]], Yellow Pages companies are privately owned. [[NigerianYellowPages.com]] is the official trademark owner of the walking finger logo with six (6) edition of its yellow pages in different formats. YellowPages.net.ng claimed to be the first Yellow Pages Directory in the world to emulate social media network concept.  
** '''Nigerian Yellow Pages''': Content of [[Nigerian Yellow Pages]] or commonly known as NigerianYellowPages.com is available in six formats (editions): ''CD-ROM directory''; ''MS Windows Desktop directory''; ''Internet directory''; ''Mobile Phone Internet directory''  ''Mobile Phone SMS text directory'' and ''Print directory''. There is also a mobile edition, which is accessible on mobile phones and other mobile devices such as PDA. There is a dedicated classifieds section in their yellow pages for jobs, properties, homes, rentals and announcements. It has its own toolbar, the [[Nigerian Yellow Pages Toolbar]].
** '''Africaonline business directory''': The other Yellowpage business directory in Nigeria is the Africaonline business directory, [[yellopages.com]]. This is an interactive online business directory that enables businesses to upload their profiles and place their adverts. Yellopages.com includes Nigerian content and serves to integrate Nigerian businesses.
* '''Norway:''' In Norway, the directory is called ''Gule Sider'' (Yellow Pages). The two second largest directories are [[Opplysningen 1881]] and [[Nettkatalogen]]. [[Gul.no]], [[180.no]], [[Avanti Media AS]], [[Bedriftssøket AS]], [[Gul Index]] and [[Finnfirma.no]] are som of the other directories in growth. The searchengine Sesam.no provides a business directory branded [[Sesam Katalog]].

==P==
* '''Pakistan:'''
** '''Jamal's Yellow Pages of Pakistan:''' is a B2B [[Trade Directory]] published by US Publishers (Pvt) Ltd. since 1983. The directory is published in printed form (3 volumes per set), CDROM version and online.
** '''Time's Trade Directory of Pakistan:''' Time Publishers (Pvt) Ltd. published "Time's Trade Directory of Pakistan - National Yellow Pages" since 2002. B2B Version also launch similarly as Time's e-Directory. The online version also provide comprehensive information about Pakistan Businesses to the web user worldwide.
** '''PhoneBook.com.pk:''' The [[Pakistan Telecommunication Company]] maintains a yellow pages and white pages directory. JS Enterprises Private Limited is publishing this directory, which is also associated with [[Jang Group of Newspapers]] and the GEO Television Network.
** '''Ebizpk.com: '''Online Green &amp; Yellow Pages of Pakistan. Launched in January 2010. Initially Listing 10 companies of each sector.
** '''Dmoz Pakistan:''' Database of [[Pakistani]] companies, [[Government]] departments and business organizations in categorized format.
** '''[http://ypages.pk Ypages.pk]:'''  Launched in June 2012, Online Yellow Pages of [[Pakistan]] provides all local business contact details. Ypages associated with ALM Advertising Agency.
** '''[yellowpagespk.com]: '''Marshall Online Yellow Pages  in Islamabad Pakistan &amp; Online Business Directory in Islamabad Rawalpindi Lahore Karachi Pakistan.
* '''Palestine:''' [[Palestine Yellow Pages Ltd]] is the official publisher of Yellow Pages branded products in Palestine. [[Palestine Yellow Pages]] is the exclusive owner of the Yellow Pages, Walking Fingers &amp; Design, and YellowPages.com.ps trademarks in Palestine. [[Palestine Yellow Pages]] is part of the [[Al Wahda-Express Group of Companies]]. Founded in 1986, [[Al Wahda-Express Group of Companies]] employs nearly 1000 employees publishing print, online and mobile Yellow Pages directories throughout 5 countries including Palestine.
* '''Panama:''' In [[Panama]], [[Yellow Pages Panama]]
* '''Peru:''' In United States, [[Peruvian Yellow Pages]], since 1993, the printed edition, is the first and oldest publication for Peruvians living in the USA. Now with the online version covering coast to coast the American territory. The online version of the Peruvian yellow pages is available at [[Peruvian Yellow Pages]].
* '''Philippines:''' In [[Philippines]], Directories Philippines Corporation (DPC), regularly publishes phone books of more than a dozen telecom companies in the country.
* '''Poland:''' In [[Poland]], it is called ''żółte strony'' and is distributed by [[Polskie Książki Telefoniczne]] (a [[European Directories]] group company) as a part of their phone books. The second largest directory, published by [[Eniro]], is called "Panorama Firm" (panorama of companies). [[YellowPages.pl]]. It is the biggest online directory in Poland. Polish Yellow Pages has existed on the market since 1998. Yellow Pages enables them to search companies and products and services, it is a business platform, which helps to promote a company and to establish trade relations. In April 2007 started [[Zumi.pl]] - first local search web which connects maps and information about companies in Poland. Several historical directories from Poland are available online as scans, and can be searched via [[kalter.org]].
* '''Portugal:''' In [[Portugal]], the ''Páginas Amarelas'' are controlled by [[Portugal Telecom]] and the website is [[pai.pt]]. The printed version is distributed for free to all land line users. There is also available a residential listing called Páginas Brancas.

==Q==
* '''Qatar''': In [[Qatar]], the official Yellow Pages directory is called Qatar Yellow Pages and published by ''''Primedia Qatar W.L.L'.''' The Qatar yellowpages features comprehensive business listings for industrial and commercial establishments across the region markets. This Directory is one of the most economical media for business to showcase their products and services. The user has a choice to reference print or source online or mobile wap.

==R==
* '''Romania:''' In [[Romania]], the directory is called 'Pagini Aurii' (Golden Pages) [[paginiaurii.ro]].
* '''Russia:''' In Russia, KONTAKT EAST HOLDINGS (KEH.ST) established in 2006, is a Swedish holding company that owns Russian Company OOO ''Желтые страницы'' ("JOLTI STRANITSI") (Russian translation of Yellow Pages). OOO "JOLTI STRANITSI" is the result of the successful merger in 2007 of YPI YELLOW PAGES, established in 1993, a leading publisher of Yellow Pages directories in the St. Petersburg and Perm markets and Eniro RUS-M, a publisher of leading Yellow Pages directories in Moscow, Samara and 7 other Russian cities in the Urals and Volga region.

Other directories in Russia include:

:*''Адрес Mосква'' (Moscow address), by ZAO [[Verlag Euro Address]];
:*''Большая Телефонная Книга Москвы'' (Big Phone Book of Moscow), by [[Extra M Media]];
:*''Вся Деловая Москва'' (all business Moscow), by [[Biznes-Karta Business Information Agency]];
:*''Московский Бизнес - Moscow Business Telephone Guide'' by [[Московский Бизнес - Moscow Business Telephone Guide]].

==S==
* '''Saint Kitts and Nevis''': In [[St Kitts &amp; Nevis]], the directory is published by [[Global Directories Limited]] and titled ''St Kitts and Nevis Yellow Pages''. Print copies are distributed free to each telephone subscriber and is also available online at www.stkittsandnevisyp.com.
* '''Saint Lucia''': In [[Saint Lucia]], the directory is published by [[Global Directories Limited]] and titled ''St Lucia Yellow Pages''. Print copies are distributed free to each telephone subscriber.
* '''Saint Vincent''': In [[Saint Vincent and the Grenadines]], the directory is published by [[Global Directories Limited]] and titled ''St Vincent Yellow Pages''. Print copies are distributed free to each telephone subscriber and is also available online at www.stvincentyp.com.
* '''Saudi Arabia''': In [[Saudi Arabia]], the directory is Saudianyellowpages.com' 'Saudiarabyellowpages.com'',. Established in 2001, is the LARGEST yellow pages of Saudi Arabia. Yellow Pages Saudi Arabia.
* '''Saudi Arabia''': Daleeli.com is an online business directory in [[Saudi Arabia]] to locate addresses, Phone numbers, maps, websites &amp; locations of Business Places and offices in Saudi Arabia.
* '''Serbia:''' In [[Serbia]], the directory is called '''Zute Strane - Yellow Pages''' (Serbian Business Directory) which is a registered trademark belonging to Yellow Pages Co. from Belgrade.
* '''Sierra Leone:''' In [[Sierra Leone]], the online yellow pages directory, [[LeoneDirect]] powered by [[Denza, LLC.]] provides contact information for local companies.
* '''Singapore:''' In [[Singapore]], it is known as "Yellow Pages" and is registered as a [[Public company]] under the name [[Yellow Pages Singapore|Yellow Pages (Singapore) Limited (Reg. no.:200304719G)]]. It is [[Public company|listed]] on the Singapore [[Singapore Exchange|SGX]] mainboard on 9 Dec 2004. It includes the Singapore Phone Book, the Chinese Yellow Pages and the Yellow Pages Buying and Commercial/Industrial Guides and advertisement sales. Yellow Pages Singapore also publishes and distributes niche directories and guides.
* '''Slovakia:''' In [[Slovakia]], it is called "Zlaté stránky" (which means Golden Pages), published by [[Mediatel]] (a [[European Directories]] group company), Bratislava and is distributed free to each telephone subscriber, usually in exchange for its previous version. The online version is available at [[zlatestranky.sk]].
* '''Slovenia:''' In [[Slovenia]], the directory is called ''[[Rumene strani]]'' (Yellow Pages) which is a registered trademark belonging to Inter Marketing.
* '''South Africa:''' In [[South Africa]], the directory is called 'the Yellow Pages' which is distributed by Trudon [[yellowpages.co.za]], a subsidiary of World Directories which also publishes books in Belgium, Ireland, the Netherlands, Portugal and Romania. There are 19 regional editions covering the nine provinces. Each of the four metropolitan areas has a separate white and yellow pages book. The remaining 15 areas have both sections in one book. They also have a mobile version [[pocketbook.co.za]]
* '''Spain:''' In Spain, it is called ''Páginas Amarillas'', it was distributed by [[Telefónica|Telefónica Publicidad e Información]] S.A. Yellowpages - now known as Yell Publicidad - can also be found via the Internet Address [[www.paginasamarillas.es]]. Since July 2006 the company is owned by Yell Group from the UK. A competitor is [[www.qdq.com]], a directory edited by Pages Jaunes Group from France. Another competitor is [[citiservi]], a different yellow pages service where Professionals search for Customers requesting services. Also there is an English-speaking expat directory of businesses along the south east of Spain called [[www.littleyellowpages.com]]. This site is aimed mainly at English speaking expatriates living in Spain.
* '''Sri Lanka:''' In [[Sri Lanka]], the official 'Yellow Pages' publisher is produced by [[Sri Lanka Telecom]]. However, competing publishers also use the term 'Rainbow Pages' though not the walking fingers logo.
* '''Sweden:''' In Sweden, it is called ''Gula Sidorna'', distributed by [[Eniro]]. yellowpages.se is a portal to different Yellowpages from Sweden. Gulex.se is an alternative Swedish directory, distributed by the Norwegian company Advista. Lokaldelen i Sverige AB (a [[European Directories]] group company) provide over 250 local directories in Sweden. Also hitta.se, an Online business directory by Norwegian company [[Schibsted]].
* '''Switzerland:''' In Switzerland the brand local.ch produces and distributes directories in several forms (printed, online and on mobile) including yellow and white pages - online available in [[German language|German]], French, [[Italian language|Italian]] and English.
* '''Syria:''' In [[Syria]], the directory is called الصفحات الصفراء (Yellow Pages).

==T==
* '''Thailand:''' In [[Thailand]] it is called ''Samood Nar Leung'' and also called ''Thailand YellowPages''. The company [[Teleinfo Media Public Company Limited]] produce and distribute Yellow pages nationwide. Thailand YellowPages is generated in several forms e.g. paper, Call Center no. 1188. Thailand YellowPages is produced both in Thai and English.
* '''Tunisia:''' In [[Tunisia]], it is called "الصفحات الصفراء" (Pages Jaunes) and it is owned by "Les Editions Techniques Spécialisées", a Tunisian private company. The online version, available at [[pagesjaunes.com.tn]] for free was one of the first online directories in Arabic.
* '''Turkey:''' Yellow Medya Istanbul based Yellow Medya.
* '''Turkish Republic of Northern Cyprus''' In [Turkish Republic of Northern Cyprus]. Known as CYPYP it is found at [[cypyp.com]]
* '''Turks and Caicos Islands''': In the [[Turks and Caicos Islands]] there are two telephone directories. One is published by Olympia Publishing Company, a Turks &amp; Caicos Islands company, and carries listings from the two major telecommunications companies on the Island and the other is published by a subsidiary of Global Directories Limited, a Caymanian-based company, which carries the listings from one of the two major telecommunications companies on the Islands. Both publications are titled the ''Turks and Caicos Islands Yellow Pages'' and refer to themselves as "Local" but the Olympia Publishing Company directory is the larger and more definitive and it is the only local directory publisher.

==U==
* '''Ukraine''': In [[Ukraine]], the free business directory is titled ''PromUA'' ([[Russian language|Russian]]), it is available online at [[prom.ua]]. Other directories are: ukrindustrial.com, ukrbiznes.com, [[ukrpartner.com]].
* '''United Arab Emirates:''' Dubai-based Local Search UAE is an Online Business Directory UAE where all businesses across Abu Dhabi, Al Ain, Dubai, Fujairah, Sharjah, Ras Al Khaimah &amp; Umm Al Quwain are listed and can be searched.
* '''United Arab Emirates:''' Dubai-based [[Express Print (Publishers) L.L.C.]] is the official publisher of '''Etisalat Yellow Pages''' branded products in the UAE. Express Print (Publishers) L.L.C. publishes the Yellow Pages in both print and electronic formats. Etisalat Yellow Pages print edition consists of 3 regional directories for the areas of Abu Dhabi, Dubai and the Northern Emirates. Directories are published annually and distributed towards the end of the first quarter of each year. Express Print (Publishers) L.L.C. also publishes the Etisalat Yellow Pages on 2 electronic platforms -Online &amp; Mobile.
* '''United Arab Emirates:''' As of late 2016, Dubai-based [[ZOSER MEA]] is the official publisher of [[du Yellow Pages]] branded products in the UAE. ZOSER MEA publishes the Yellow Pages in both print and electronic formats. Directories are published annually and distributed in the month of January each year.
* '''United Arab Emirates:''' In [[United Arab Emirates]], the directory is titled ''Yellow Page Gulf UAE'',. Established in January 2011, is the LARGEST yellow pages of GULF. Yellow Pages Gulf.
* '''United Kingdom:''' The first Yellow Pages directory in the UK was produced by the [[Kingston upon Hull|Hull]] Corporation's telephone department (now [[Kingston Communications]]) in 1954. This was distributed with the alphabetical phone directory rather than as a stand-alone publication. The company now produces [[Hull Colour Pages|The Hull Colour Pages]].

:With the encouragement of [[The Thomson Corporation]], at the time an advertising sales agent for the nationalised [[General Post Office (United Kingdom)|General Post Office]]'s [[telephone directory]], a business telephone number directory named the Yellow Pages was first produced in 1966 by the GPO for the [[Brighton]] area, and was rolled out nationwide in 1973. The Thomson Corporation formed Thomson Yellow Pages in 1966 to publish and to distribute the directory to telephone subscribers for the GPO, and later for [[Royal Mail|The Post Office]].

:Thomson Yellow Pages was sold by The Thomson Corporation in 1980, at the same time as Post Office Telecommunications became the (then) state-owned [[British Telecom]] (BT). The Yellow Pages directory continued to be distributed to all telephone subscribers by BT. At the same time, The Thomson Corporation formed Thomson Directories Ltd, and began to publish the [[Thomson Local]] directory, which would remain the Yellow Pages' main, and often sole, competitor in the UK for more than the next two decades, and would be the competitive driving force behind such changes to Yellow Pages as the adoption (in 1999) of colour printing and "white knock out" listings.

:In 1984 the year that BT was privatised, the department producing the directory became a stand alone subsidiary of BT, named Yellow Pages. In the mid-1990s the Yellow Pages business was re-branded as [[Yell Group|Yell]], although the directory itself continued to be known as the Yellow Pages.

:Yell was bought by venture capitalists in 2001, and in 2003 was floated on the Stock Exchange. After the one year "no competition" clause expired BT too went into competition with the Yellow Pages, re-entering the market by adding similar content to their existing directory, "The Phone Book", adding a classified section to the traditional alphabetical domestic and business listings.

:Yellow Pages, [[Thomson Local]] and BT's [[The Phone Book]] display advertising and can be booked directly with advertising sales representatives.

:Nowadays the KC Yellow Pages is referred to as [[Hull Colour Pages]], and is separate from the White Pages. Yell now also publishes an East Yorkshire edition of Yellow Pages in competition.

[[Image:Bsyps.png|right|Bell System Yellow Pages Logo]]
* '''United States:''' In the past, AT&amp;T, Verizon, and Qwest, the three largest phone companies in the U.S., dominated the U.S. yellow pages industry; however, the term "yellow pages" and the ''Walking Fingers'' logo was heavily marketed by AT&amp;T pre [[Bell System Divestiture|divestiture]]. However, AT&amp;T never filed a trademark registration application for the current and most recognized version of the ''Walking Fingers'' logo, so it is in the public domain. AT&amp;T allowed the "independent yellow pages" industry to use the logo freely.&lt;ref&gt;[http://www.ll.georgetown.edu/federal/judicial/fed/opinions/9_opinions/91-1461.html Bellsouth v. Datanational]&lt;/ref&gt; The "independents" are unrelated to the incumbent phone company and are either pure advertising operations with no phone infrastructure or telephone companies who provide local telephone service elsewhere. Such independents include operators who typically focus on industry or business segments, or local market directories.

:Yellow pages publishers or their agents sell the right to place advertisements within the same category, next to the basic listings.

:For example, [[AT&amp;T]] is the dominant local telephone service provider in [[California]], but since Bell Atlantic and [[GTE]] merged to become [[Verizon]], it now provides service in many pockets such as [[West Los Angeles (region)|West Los Angeles]]. [[Los Angeles]] telephone users can select from telephone directories published by AT&amp;T, Verizon (published by [[Idearc Media|Idearc]]), Yellow Book USA, PDC Pages (Phone Directories Company) [[PDC Pages.com]] and other independent publishing companies. [[R. H. Donnelley]] is also in local markets across country with Dex Printed Directories and [[DexKnows.com]]. In Northern California, Valley Yellow Pages [[MyYP.com]] is a large regional independent publisher. Additionally, in the smaller markets, many yellow pages publishers are beginning to offer directories catering to specific niche business or industry segments, such as automotive, manufacturing, environmental/green products, imports, exports, and the like. One such example is the [[Export Yellow Pages]] (a yellow page directory published in partnership with the US Department of Commerce that focuses on U.S. exporters) and vertical directories offered by Yellow Pages Nationwide, Inc. Media an Online Digital Yellow Pages company, Consolidation and M&amp;A activity in the directory publishing market continues to remain very high in the U.S. and there is an increasing move toward internet based directories as internet usage for search increases and concerns over the possible negative environmental effects of the books becomes more evident.
: [[Yellowpages.com]] LLC is a subsidiary of AT&amp;T.

* '''Uzbekistan:''' In [[Uzbekistan]], the directory is called ''Yellow Pages of Uzbekistan'', published by Yellow Pages Ltd.

==V==
* '''Vietnam:''' In [[Vietnam]], the official title "Telephone Directory &amp; Yellow Pages'' in English and ''Nien Giam Dien Thoai Nhung Trang Vang va Nhung Trang Trang "in Vietnamese are produced and distributed nationwide by [http://yp.vn/ VietNam Yellow Pages Media JSC].

Vietnam Business Yellow Pages in Vietnamese and English is directory of Vietnam Business.

==Notes and references==
{{reflist}}

==See also==
* [[Telecommunications service]]
* [[Yellow pages]]

[[Category:Directories|*]]
[[Category:Yellow pages|*]]</text>
      <sha1>9p25zplpzn0jsna6shut4sbnyny4q5t</sha1>
    </revision>
  </page>
  <page>
    <title>Almanach de Bruxelles (defunct)</title>
    <ns>0</ns>
    <id>36297187</id>
    <revision>
      <id>735155121</id>
      <parentid>693696961</parentid>
      <timestamp>2016-08-18T22:42:34Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor />
      <comment>Robot - Moving category Defunct publications of France to [[:Category:Defunct periodicals of France]] per [[WP:CFD|CFD]] at [[Wikipedia:Categories for discussion/Log/2016 July 21]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1108" xml:space="preserve">The '''''Almanach de Bruxelles''''' is a now defunct [[France|French]] social register that listed [[royal family|royal]] and [[nobility|noble]] [[dynasties]] of [[Europe]]. It was established in 1918 during the [[Second World War]] to compete against the prominent German [[Almanach de Gotha]].&lt;ref&gt;March 17, 1918. [http://query.nytimes.com/mem/archive-free/pdf?res=9E0DEED6143AEF33A25754C1A9659C946996D6CF Almanach de Gotha has a french rival] at ''[[New York Times]]''&lt;/ref&gt;

==See also==
* ''[[Almanach de Gotha]]''

==Sources==
{{reflist}}

==External links==
*[http://www.worldcat.org/title/almanach-de-bruxelles-annuaire-genealogique-historique-heraldique-des-maisons-souverains-princieres-et-ducales/oclc/06083750 ''Almanach de Bruxelles'' (1918-] at [[WorldCat]]

[[Category:Genealogy publications]]
[[Category:Directories]]
[[Category:Biographical dictionaries]]
[[Category:Defunct periodicals of France]]
[[Category:European nobility]]
[[Category:French royalty]]
[[Category:1918 establishments in France]]
[[Category:Publications established in 1918]]
 

{{royal-bio-book-stub}}
{{bio-dict-stub}}</text>
      <sha1>ageth95tli046g77qrfhlj8p6wgytom</sha1>
    </revision>
  </page>
  <page>
    <title>Association of Directory Publishers</title>
    <ns>0</ns>
    <id>39023454</id>
    <revision>
      <id>743554364</id>
      <parentid>741346072</parentid>
      <timestamp>2016-10-10T05:00:48Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* History */http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5312" xml:space="preserve">The '''Association of Directory Publishers''' (ADP), is an international [[trade association]] founded in 1898 and is headquartered in [[Traverse City, Michigan]].&lt;ref name=associations2012&gt;{{cite book |title= Encyclopedia of Associations |issn=0071-0202 |volume= 1 |edition=51st |year= 2012 |page= 318 |via=[[Boston Public Library]] Reference &amp; Reader's Advisory Department }}&lt;!--|accessdate=April 8, 2013 --&gt;&lt;/ref&gt; 

==About==
ADP is the oldest international trade association serving the Yellow Pages industry.  The Association represents the various interests of its membership which includes publishers of print, online and mobile directories, Certified Marketing Representatives (CMRs), advertising agencies and suppliers to the Yellow Pages and local search industry.

ADP represents the $35 billion Yellow Pages industry known as the original "local search engine" that brings buyers to sellers at the exact moment they are ready to buy.

The Association helps its members expand their businesses by offering them services and tools targeted to assisting them in achieving their clients' advertising objectives. ADP offers a wide variety of research, marketing and sales materials created with information from leading organization that are developed specifically to help members increase their company's bottom line.

ADP is a unique Association because of the governance structure of one company, one vote.  Every publisher from the smallest to largest has an equal opportunity to determine the leadership and direction of the Association.  ADP represents member companies of all sizes and from numerous countries.

==History==
The group formed in 1898 as the '''Association of American Directory Publishers,''' headquartered in New York. It aimed "to improve the [[Reference work|directory]] business."&lt;ref&gt;{{citation |url=https://books.google.com/books?id=gt7UAAAAMAAJ |year=1908 |work=Boyd's Directory of Harrisburg |title=(Advertisement for the Association of American Directory Publishers)}}&lt;/ref&gt; It changed its name to the '''Association of North American Directory Publishers''' in 1919.&lt;ref&gt;{{citation |title=Printers' Ink |location=NY |date=September 11, 1919 }}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://www.worldcat.org/identities/lccn-no2011-132108 |title=Association of North American Directory Publishers |work=WorldCat |publisher=[[OCLC]] |accessdate=April 5, 2013 }}&lt;/ref&gt; It has held annual meetings starting in 1899 and has published the ''Directory Bulletin''.&lt;ref&gt;{{citation |title=Directory Bulletin |volume =1 |year=1901 |location=Milwaukee |publisher=Association of American Directory Publishers |url=https://books.google.com/books?id=2zTZAAAAMAAJ }}&lt;/ref&gt; Officers have included George W. Overton and [[Ralph Lane Polk]].&lt;ref name=members1921 /&gt; Among the members in the 1920s:&lt;ref name=members1921&gt;{{citation |chapter=Members of Association of North American Directory Publishers |year=1921 |url=https://books.google.com/books?id=qG4UAAAAYAAJ&amp;pg=PA480 |title=Manchester Directory |publisher=Sampson &amp; Murdock Co. }}&lt;/ref&gt;

{{Col-begin}}
{{Col-1-of-3}}
* Action Pages
* Atkinson Erie Directory Company
* Atlanta City Directory Company
* W.H. Boyd Company
* Burch Directory Company
* Caron Directory Company
* Chicago Directory Company
* J.W. Clement Company
* Cleveland Directory Company
* Connelly Directory Company
* Fitzgerald Directory Company
* Gate City Directory Company
* Hartford Printing Company
* Henderson Directories Ltd.
* Hill Directory Company
{{Col-2-of-3}}
* C.E. Howe Company
* Kimball Directory Company
* Leshnick Directory Company
* Los Angeles Directory Company
* John Lovell &amp; Son Ltd.
* McCoy Directory Company
* H.A. Manning Company
* Maritime Directory Company
* Henry M. Meek Publishing Company
* Might Directories Ltd.
* Minneapolis Directory Company
* Piedmont Directory Company
* [[R.L. Polk &amp; Company]]
{{Col-3-of-3}}
* Polk-Gould Directory Company
* Polk-Husted Directory Company
* Polk-McAvoy Directory Company
* Polk's Southern Directory Company
* Portland Directory Company
* Price &amp; Lee Company
* W.L. Richmond
* Roberts Bros Company
* Sampson &amp; Murdock Company
* Soards Directory Company 
* Utica Directory Publishing Company
* Williams Directory Company&lt;ref&gt;{{Citation |publisher = Williams Directory Co. |publication-place = Cincinnati, Ohio |author = A.V. Williams |url = http://hdl.handle.net/2027/nyp.33433082423645 |title = The development and growth of city directories |publication-date = 1913 }}&lt;/ref&gt;
* John F. Worley Directory Company
* Wright Directory Company
{{Col-end}}

In 1992 the group renamed itself the "Association of Directory Publishers."&lt;ref name=associations2012 /&gt;

==References==
{{Reflist|30em}}

==Further reading==
* {{citation |title=Pacific Bell fends off feisty competitors seeking confidential Yellow Pages data |work=San Francisco Business Times |date=March 1, 1991 }}

==External links==
* [http://www.adp.org/ Official website]
* {{cite web |url=http://www.adp.org/committees |title=Our Industry: Timeline |publisher=Association of Directory Publishers }}

[[Category:Organizations established in 1898]]
[[Category:1898 establishments in the United States]]
[[Category:Professional associations based in the United States]]
[[Category:Publishing organizations]]
[[Category:Directories]]
[[Category:Yellow pages]]</text>
      <sha1>kg7zfb5vw7asx2xrih55e9qx74q5wby</sha1>
    </revision>
  </page>
  <page>
    <title>Polk's Directory</title>
    <ns>0</ns>
    <id>39021356</id>
    <redirect title="R.L. Polk &amp; Company" />
    <revision>
      <id>564060306</id>
      <parentid>548870957</parentid>
      <timestamp>2013-07-13T04:16:38Z</timestamp>
      <contributor>
        <username>AvocatoBot</username>
        <id>14893258</id>
      </contributor>
      <minor />
      <comment>Robot: Fixing double redirect to [[R.L. Polk &amp; Company]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="164" xml:space="preserve">#REDIRECT [[R.L. Polk &amp; Company]]

[[Category:Histories of cities in the United States]]
[[Category:Publications established in the 1870s]]
[[Category:Directories]]</text>
      <sha1>8vy9c7etmdq3c25rtpxl3m3k9d0g16l</sha1>
    </revision>
  </page>
  <page>
    <title>White's Directories</title>
    <ns>0</ns>
    <id>40453381</id>
    <revision>
      <id>747276051</id>
      <parentid>735301350</parentid>
      <timestamp>2016-11-01T13:29:08Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* top */http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6082" xml:space="preserve">{{italic title}}
'''''White's Directories''''' were a series of directory publications issued by William White of [[Sheffield]], England, beginning in the 1820s.&lt;ref&gt;{{citation |chapter=White's Directories (advert) |quote=Established 1822 |url= https://books.google.com/books?id=dMwUAAAAQAAJ&amp;pg=PA83 |title=White's general and commercial directory of Hull |year=1882}}&lt;/ref&gt;&lt;ref&gt;{{Citation |url = http://openlibrary.org/books/ia:pigotcosnational1837dire/Pigot_and_Co.'s_national_commercial_directory_for_the_whole_of_Scotland_and_of_the_Isle_of_Man_..._t |title = Pigot and Co.'s National Commercial Directory for the Whole of Scotland and of the Isle of Man, ... Manchester, Liverpool, Leeds, Hull, Birmingham, Sheffiled, Carlisle, and Newcastle-upon-Tyne |publication-date = 1837 |location = London |publisher =[[James Pigot|J. Pigot &amp; Co.]] }}&lt;/ref&gt; White began his career in publishing by working for [[Edward Baines (1774–1848)|Edward Baines]].&lt;ref&gt;{{cite journal |title=Locational Behaviour of Urban Retailing during the Nineteenth Century: The Example of Kingston upon Hull |author= M. T. Wild and G. Shaw |journal=Transactions of the Institute of British Geographers |number= 61 |year=1974 |jstor=621602 }}&lt;/ref&gt;{{refn|group=nb|By the 1850s Sheffield had two professional directory publishers: William White (34 Collegiate Crescent, Broomhall Park) and Francis White (Broomhall Terrace, 104 Ecclesial New Road)&lt;ref&gt;{{cite book |title=Post office directory of Sheffield |year=1854 |publisher=Kelley and Co. |url=https://books.google.com/books?id=bO4NAAAAQAAJ }}&lt;/ref&gt;&lt;ref&gt;{{Citation |publisher = W. Satchell |publication-place = London |title = Book of British Topography: a Classified Catalogue of the Topographical Works in the Library of the British Museum Relating to Great Britain and Ireland |author = John Parker Anderson |publication-date = 1881 |chapter=Yorkshire: Sheffield |chapterurl=https://archive.org/stream/bookofbritishtop00andeuoft#page/327/mode/1up }}&lt;/ref&gt;}}

==Notes==
{{reflist|group=nb}}

==References==
{{reflist}}

==Further reading==

=== 1820s-1830s ===
* {{cite book |title=History, directory, and gazetteer, of the counties of Durham and Northumberland, and the towns and counties of Newcastle-upon-Tyne and Berwick-upon-Tweed |location=Newcastle |publisher= Printed for W. White &amp; Co. by E. Baines and Son |year= 1827–1828 |url= http://catalog.hathitrust.org/Record/009725890 }}
* {{cite book |title=Directory of the Borough of Leeds, the City of York, and the Clothing District of Yorkshire |location=Leeds |publisher= Printed for Wm. Parson &amp; Wm. White by Edward Baines and Son |year= 1830 |url=http://catalog.hathitrust.org/Record/007973427 }}
* {{Citation |publisher = Printed for the author by R. Leader |title = History, Gazetteer, and Directory of Norfolk, and the City and County of the City of Norwich |url = http://openlibrary.org/books/OL20613547M/History_Gazetteer_and_Directory_of_Norfolk_and_the_City_and_County_of_the_City_of_Norwich_... |author = William White |publication-date = 1836 |oclc = 25166377 }}
** [https://archive.org/stream/historygazettee01whitgoog#page/n3/mode/2up 1845 ed.]
** [https://archive.org/stream/historygazetteer00whit#page/n5/mode/2up 1864 ed.]
* {{Citation |publisher = W. White |publication-place = Sheffield |author =William White |title = History, Gazetteer, and Directory, of the West-Riding of Yorkshire, with the City of York and Port of Hull |publication-date = 1837 |url=https://archive.org/stream/historygazetteer01whit#page/n5/mode/2up }}
** {{cite book |title=History, gazetteer and directory of the East and North Ridings of Yorkshire |author=William White |location= Sheffield |publisher= Robert Leader for the author |year= 1840 |url=http://catalog.hathitrust.org/Record/011724851 }}

=== 1840s ===
* {{cite book |title=History, gazetteer, and directory of Suffolk, and the towns near its borders |location=Sheffield |publisher= Printed for the author by R. Leader and sold by W. White |year=1844 |url= http://catalog.hathitrust.org/Record/000194916 }}
** [http://catalog.hathitrust.org/Record/011595374 1874 ed.]
* {{Citation |url = http://openlibrary.org/books/ia:generaldirectory00whit/General_directory_of_the_town_and_borough_of_Sheffield_with_Rotherham_Chesterfield_and_all_the_paris |title = General directory of the town and borough of Sheffield |publication-date = 1845 |publisher = William White |location=Sheffield }}
* {{Citation |publisher = Printed for the author, by R. Leader |publication-place = Sheffield |author = William White |url = http://openlibrary.org/books/OL14012344M/History_gazetteer_and_directory_of_Leicestershire_and_the_small_county_of_Rutland |title = History, gazetteer, and directory of Leicestershire, and the small county of Rutland |publication-date = 1846 }}

=== 1870s ===
* {{cite book |title=History, gazetteer and directory of Lincolnshire, and the city and diocese of Lincoln |location=Sheffield |publisher= W. White |year= 1872 |url=http://catalog.hathitrust.org/Record/008912723 }}
* {{Citation |publisher = W. White |publication-place = Sheffield |title = History, gazetteer and directory of the county of Hampshire, including the Isle of Wight |url = http://catalog.hathitrust.org/Record/009009769 |publication-date = 1878 }}
* {{Citation |publisher = William White |publication-place = Sheffield |title = History, Gazetteer and Directory of the County of Devon including the City of Exeter |url = http://openlibrary.org/books/OL14012345M/History_gazetteer_and_directory_of_the_County_of_Devon_including_the_City_of_Exeter_and_comprising_a |publication-date = 1878 |edition=2nd }}

==External links==
* {{citation |title=Historical Directories |publisher=[[University of Leicester]] |location=UK |url=http://www.historicaldirectories.org/hd/findbykeyword.asp }}. Includes digitized White's directories, various dates
* {{citation |work=WorldCat |url=http://www.worldcat.org/wcidentities/lccn-n50-38455 |title=William White of Sheffield }}

[[Category:Directories]]
[[Category:Publications established in the 1820s]]


{{ref-book-stub}}</text>
      <sha1>3zol0sgxwlgrfut7z6o907lhw0rsg4m</sha1>
    </revision>
  </page>
  <page>
    <title>Writers' &amp; Artists' Yearbook</title>
    <ns>0</ns>
    <id>37966541</id>
    <revision>
      <id>722552187</id>
      <parentid>632750182</parentid>
      <timestamp>2016-05-28T22:00:24Z</timestamp>
      <contributor>
        <username>Bellerophon5685</username>
        <id>1258165</id>
      </contributor>
      <comment>/* External links */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6412" xml:space="preserve">[[File:Writers' &amp; Artists' Yearbook cover 2003.jpg|thumb|150px|2003 edition of ''Writers' &amp; Artists' Yearbook'']]
{{Italic title}}'''''Writers' &amp; Artists' Yearbook''''' is an annual directory for writers, designers, illustrators and photographers. It is published in the UK each July, with a separate version for children's writers and artists published in August. The yearbook contains some 4,500 named industry contacts updated for each edition and includes articles about getting work published.&lt;ref Name=BBC&gt;[http://news.bbc.co.uk/dna/place-lancashire/plain/A16932017 "The Writers' and Artists' Yearbook", BBC]&lt;/ref&gt;&lt;ref name="Irish Times"&gt;{{cite news|title=Essential Reading for Writers|newspaper=Irish Times|date=13 September 2003}}&lt;/ref&gt; In 2007, an associated website, known as Writers&amp;Artists, was launched.&lt;ref name="website launch"&gt;{{cite web|title=New website with free resources for writers and artists|url=http://www.publishers.org.uk/index.php?option=com_content&amp;view=article&amp;id=554:new-website-with-free-resources-for-writers-and-artists&amp;catid=80:general-news&amp;Itemid=1617|publisher=Publishers Association|accessdate=2 March 2014}}&lt;/ref&gt;

== History ==

First published in 1906, by [[A &amp; C Black|Adam &amp; Charles Black]], the original ''Writers’ &amp; Artists’ Yearbook'' was an 80-page booklet, costing one [[shilling]]. It gave details of seven literary agents and 89 publishers.&lt;ref Name=BBC/&gt; It has been published on an annual basis since, expanding over time to include information for illustrators and photographers.&lt;ref Name=BBC/&gt; A &amp; C Black became part of [[Bloomsbury Publishing]] in 2000, and other titles in its reference division include ''[[Who's Who (UK)|Who's Who]]'', ''[[Wisden Cricketers' Almanack|Wisden]]'' and ''[[Black's Medical Dictionary]]''.&lt;ref name="A &amp; C Black"&gt;{{cite news|last=Neill|first=Graeme|title=Coleman to leave A &amp; C Black for Magi|url=http://www.thebookseller.com/news/coleman-leave-c-black-magi.html|accessdate=2 March 2014|newspaper=The Bookseller|date=2 February 2011}}&lt;/ref&gt;
Articles offering advice first appeared in the 1914 yearbook.&lt;ref Name=BBC/&gt; Forewords have been written by, among others, [[William Boyd (writer)|William Boyd]] and [[Kate Mosse]].&lt;ref name=A&amp;U&gt;{{cite web|title=Writers' and Artists' Yearbook 2013|url=https://www.allenandunwin.com/default.aspx?page=305&amp;book=9781408157497|publisher=Allen &amp; Unwin|accessdate=2 March 2014}}&lt;/ref&gt;&lt;ref name=Bibliography&gt;{{cite web|last=Mosse|first=Kate|title=Complete Bibliography|url=http://www.katemosse.co.uk/index.php/kates-books/|publisher=Kate Mosse|accessdate=2 March 2014}}&lt;/ref&gt; Following the success of ''[[Fifty Shades of Grey]]'', a new section on writing erotic fiction – by an anonymous author – appeared in the 2014 edition.&lt;ref name="Fifty Shades"&gt;{{cite news|last=Wyatt|first=Daisy|title=Fifty Shades of Grey inspires new chapter on erotic fiction in Bloomsbury Writers' and Artists' Yearbook|url=http://www.independent.co.uk/arts-entertainment/books/news/fifty-shades-of-grey-inspires-new-chapter-on-erotic-fiction-in-bloomsbury-writers-and-artists-yearbook-8685703.html|accessdate=2 March 2014|newspaper=The Independent|date=3 July 2013}}&lt;/ref&gt;

=== Website and competitions ===

In 2007, ''Writers' &amp; Artists' Yearbook'' launched an associated website. Initially this was only accessible to anyone purchasing the print edition.&lt;ref name="website launch"/&gt; In 2009, the website was relaunched and now includes blogs from guest authors and a social networking feature that enables authors and artists to add a public profile.&lt;ref name=Bookseller&gt;{{cite news|last=Gallagher|first=Victoria|title=Writers and Artists Yearbook launches social networking|url=http://www.thebookseller.com/news/writers-and-artists-yearbook-launches-social-networking.html|accessdate=2 March 2014|date=7 August 2009}}&lt;/ref&gt; From 2013, the website featured a section focusing on [[self-publishing]], also hosting a conference on the subject in November of that year in association with [[National Novel Writing Month]].&lt;ref name=self-publish&gt;{{cite news|title=Self-published writers get online resource|url=http://www.thebookseller.com/news/self-published-writers-get-online-resource.html|accessdate=2 March 2014|newspaper=The Bookseller|date=27 September 2013}}&lt;/ref&gt;
''Writers' &amp; Artists' Yearbook'' runs an annual short story competition and has also collaborated with Bloomsbury to run a competition for aspiring crime writers.&lt;ref&gt;{{cite web|title=Writers’ &amp; Artists’ Yearbook 2014 Short Story Competition|url=http://www.commonwealthwriters.org/writers-and-artists-short-story-competition-2014/|publisher=Commonwealth Writers|accessdate=2 March 2014}}&lt;/ref&gt;&lt;ref name="Book Trust"&gt;{{cite web|title=Prizes|url=http://www.booktrust.org.uk/books/adults/short-stories/prizes/|publisher=Book Trust|accessdate=2 March 2014}}&lt;/ref&gt;&lt;ref name="crime competition"&gt;{{cite news|last=Williams|first=Charlotte|title=Bloomsbury launches crime story competition|url=http://www.thebookseller.com/news/bloomsbury-launches-crime-story-competition.html|accessdate=2 March 2014|newspaper=The Bookseller|date=1 March 2012}}&lt;/ref&gt;

== Sections and listings ==

The yearbook is divided into the following sections:&lt;ref Name=BBC/&gt;
* Newspapers and magazines – regional, national and overseas, [[Print syndication|syndicates]] and [[News agency|news agencies]]
* Books – regional, national and overseas, audio publishers, [[Book packaging|book packagers]] and [[Book sales club|book clubs]]
* Poetry organisations
* Television, film and radio broadcasters
* Theatre – producers
* [[Literary agent]]s 
* Art and illustration – agents, commercial studios and card and stationery publishers 
* Societies, prizes and festivals – associations and clubs, prizes and awards and [[literary festival]]s
* Digital and self-publishing
* Resources for writers – courses, libraries and writers' retreats
* Copyright and libel information
* Finance for writers and artists.

== See also ==

* ''[[Writer's Digest]]''
* ''[[Novel &amp; Short Story Writer's Market]]''

== References ==

{{reflist|2}}

== External links ==
*[http://www.writersandartists.co.uk/ Writers&amp;Artists website]

{{DEFAULTSORT:Writers' and Artists' Yearbook}}
[[Category:Directories]]
[[Category:1906 establishments in the United Kingdom]]
[[Category:Handbooks and manuals]]
[[Category:Yearbooks]]
[[Category:Publishing]]
[[Category:A &amp; C Black books]]</text>
      <sha1>66eb8g7xnl2dur54nwfzzp2ogw2ei7m</sha1>
    </revision>
  </page>
  <page>
    <title>Address book</title>
    <ns>0</ns>
    <id>442661</id>
    <revision>
      <id>757793505</id>
      <parentid>747704276</parentid>
      <timestamp>2017-01-01T20:00:37Z</timestamp>
      <contributor>
        <username>SporkBot</username>
        <id>12406635</id>
      </contributor>
      <minor />
      <comment>Replace template per [[Wikipedia:Templates for discussion/Log/2016 July 2|TFD outcome]]; no change in content</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4629" xml:space="preserve">{{redirect|Address Book|the Apple Inc. software|Address Book (application)}}
{{unreferenced|date=February 2012}}
[[File:Address book 1.jpg|thumb|A blank page in a typical paper address book]]
[[File:Jack L. Warner's address book - National Museum of American History - DSC06088.JPG|thumb|[[Jack L. Warner]]'s address book on display at the [[National Museum of American History]]]]
An '''address book''' or a '''name and address book''' ('''NAB''') is a [[book]] or a [[database]] used for storing entries called '''contacts'''. Each contact entry usually consists of a few standard [[Field (computer science)|fields]] (for example: first name, last name, company name, [[address (geography)|address]], [[telephone]] number, [[e-mail]] address, [[fax]] number, [[mobile phone]] number). Most such systems store the details in alphabetical order of people's names, although in [[paper]]-based address books entries can easily end up out of order as the owner inserts details of more individuals or as people move. Many address books use small [[ring binder]]s that allow adding, removing and shuffling of pages to make room.

== Little black book ==
A related term that has entered the popular [[lexicon]] is '''little black book''' (or simply '''black book'''). Such books are used as [[courtship|dating]] guides, listing people who the owner has dated in the past or hopes to in the future, and details of their various relationships. More explicit variations are guides for [[sexual partner]]s. It is unclear how prevalent this is in practice or when it originated, though such books have been mentioned in many pieces of [[popular culture]]. For example, the 1953 film version of ''[[Kiss Me, Kate]]'' features a musical scene in which [[Howard Keel]]'s character laments the loss of the social life he enjoyed before marriage, naming numerous female romantic encounters while perusing a miniature black book. More recently, the mid-2000s [[Guinness Brewmasters]] advertising campaign features the "little black book" as an invention of one of the brewmasters.

== Software address book ==
[[File:X-office-address-book.svg|thumb|A digital address book icon]]
Address books can also appear as [[software]] designed for this purpose, such as the [[Address Book (application)|"Address Book"]] application included with [[Apple Inc.]]'s [[Mac OS X]]. Simple address books have been incorporated into [[e-mail]] software for many years, though more advanced versions have emerged in the 1990s and beyond; and also in [[mobile phone]]s.

A [[personal information manager]] (PIM) integrates an address book, [[calendar]], task list, and sometimes other features.

Entries can be imported and exported from the software in order to transfer them between programs or computers. The common file formats for these operations are:
* [[LDAP Data Interchange Format|LDIF]] (*.ldif, *.ldi)
* Tab delimited (*.tab, *.txt)
* [[Comma-separated values|Comma-separated]] (*.csv)
* [[vCard]] (*.vcf)

Individual entries are frequently transferred as [[vCard]]s (*.vcf), which are roughly comparable to physical [[business card]]s. And some software applications like [[Lotus Notes]] and Open Contacts can handle a vCard file containing multiple vCard records.

== Online address book ==
An online address book typically enables users to create their own web page (or profile page) which is then indexed by search engines like Google and Yahoo. This in turn enables users to be found by other people via a search of their name and then contacted via their web page containing their personal information. Ability to find people registered with online address books via search engine searches usually varies according to the commonness of the name and the amount of results for the name. Typically users of such systems can synchronize their contact details with other users that they know to ensure that their contact information is kept up to date.

== Network address book ==
Currently, most people have many different address books: their email accounts, their mobile phone, and the "friends lists" on their social networks. A network address book allows them to organize and manage all of their address books through a single interface and share their contacts across their different address books and social networks.

== See also ==
{{colbegin|3}}
* [[Calendaring software]]
* [[Contact list]]
* [[Mobile social address book]]
* [[Personal information manager]]
* [[Rolodex]]
* [[Suvorin directories]]
* [[Telephone directory]]
* [[Windows Address Book]]
{{colend}}

{{Authority control}}
[[Category:Office equipment]]
[[Category:Directories]]</text>
      <sha1>r56tjvaj2v05f0nomvkxevw50605pit</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Library cataloging and classification</title>
    <ns>14</ns>
    <id>7117782</id>
    <revision>
      <id>604572664</id>
      <parentid>548841861</parentid>
      <timestamp>2014-04-17T09:39:00Z</timestamp>
      <contributor>
        <username>Glenn</username>
        <id>9232</id>
      </contributor>
      <comment>+[[Category:Directories]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="269" xml:space="preserve">{{Commons cat|Library cataloging and classification}}

{{Cat main|Library catalog|Library classification|Inventory (library)}}

[[Category:Library science|Cataloging and classification]]
[[Category:Classification systems]]
[[Category:Metadata]]
[[Category:Directories]]</text>
      <sha1>isgvmea7ts839xupwzemczbrqb31l7c</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Web directories</title>
    <ns>14</ns>
    <id>826434</id>
    <revision>
      <id>747862427</id>
      <parentid>609345481</parentid>
      <timestamp>2016-11-04T20:36:29Z</timestamp>
      <contributor>
        <username>The Transhumanist</username>
        <id>1754504</id>
      </contributor>
      <comment>add link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="402" xml:space="preserve">{{Cat main|Web directory|List of web directories}}
A [[web directory]] is a [[directory (databases)|directory]] on the [[World Wide Web]] that specializes in [[hyperlink|linking]] to other [[web site]]s and categorizing those links. This category includes online web directories.


[[Category:Websites|Directories]]
[[Category:Indexes]]
[[Category:Directories]]
[[Category:Online services|Directories]]</text>
      <sha1>s3paw1xheciv1ox2nqbwwk5rj0upcbs</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Lists</title>
    <ns>14</ns>
    <id>691070</id>
    <revision>
      <id>746120283</id>
      <parentid>712484104</parentid>
      <timestamp>2016-10-25T10:54:13Z</timestamp>
      <contributor>
        <username>Fayenatic london</username>
        <id>1639942</id>
      </contributor>
      <comment>removed [[Category:Contents]]; added [[Category:Wikipedia navigation]] using [[WP:HC|HotCat]] - see [[Wikipedia:Categories_for_discussion/Log/2016_September_23#Category:Contents]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="151" xml:space="preserve">{{Commons category|Information lists}}
{{Category see also|Timelines}}
{{Category diffuse}}

[[Category:Wikipedia navigation]]
[[Category:Directories]]</text>
      <sha1>lnc5pgjqj2xba5ikihq7opuoaqbmh77</sha1>
    </revision>
  </page>
  <page>
    <title>Index Herbariorum</title>
    <ns>0</ns>
    <id>44490466</id>
    <revision>
      <id>733795450</id>
      <parentid>733273595</parentid>
      <timestamp>2016-08-10T04:53:52Z</timestamp>
      <contributor>
        <username>Rpyle731</username>
        <id>46515</id>
      </contributor>
      <minor />
      <comment>stub sort</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1748" xml:space="preserve">{{Italic title}}
The '''Index Herbariorum''' provides a global directory of [[herbaria]] and their associated staff. This searchable online index allows scientists rapid access to data related to 3,400 locations where a total of 350&amp;nbsp;million botanical [[Biological specimen|specimens]] are permanently housed (singular, [[herbarium]]; plural, herbaria). The Index Herbariorum has its own staff and website. Overtime, six editions of the Index were published from 1952 to 1974. The Index became available on-line in 1997.&lt;ref name=IH&gt;{{cite web|url=http://sciweb.nybg.org/science2/IndexHerbariorum.asp|title=Index Herbariorum|publisher=sciweb.nybg.org|accessdate=2014-11-23}}&lt;/ref&gt;

The index was originally published by the [[International Association for Plant Taxonomy]], which sponsored the first six editions (1952–1974); subsequently the [[New York Botanical Garden]] took over the responsibility for the index. The Index provides the supporting institution's name (often a university, botanical garden, or not-for-profit organization) its city and state, each herbarium's acronym, along with contact information for staff members along with their research specialties and the important holdings of each herbarium's collection.

==Editors==
*6th edition (1974)  was co-edited by [[Patricia Holmgren]], Director of the  New York Botanical Garden, and
*7th printed edition ed. by  Patricia Holmgren. 
*8th printed editions, ed. by  Patricia Holmgren.
*Online edition, prepared by Noel Holmgren of the New York Botanical Garden
*2006+, ed. by Barbara M. Thiers, Director of the New York Botanical Garden  Herbarium &lt;ref name=IH /&gt;
&lt;ref name=IH /&gt;

==References==
{{Reflist}}

[[Category:Directories]]
[[Category:Herbaria]]


{{botany-stub}}</text>
      <sha1>9ea1tojfo5gtq1kr36j10jmmljdtmum</sha1>
    </revision>
  </page>
  <page>
    <title>Sands Directory</title>
    <ns>0</ns>
    <id>45359850</id>
    <revision>
      <id>762235890</id>
      <parentid>689255928</parentid>
      <timestamp>2017-01-27T14:52:59Z</timestamp>
      <contributor>
        <username>Wittylama</username>
        <id>492056</id>
      </contributor>
      <comment>/* See also */ adding searchable version link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9775" xml:space="preserve">{{Use Australian English|date=March 2015}}
{{Use dmy dates|date=March 2015}}

[[File:Sands Directory 1899 (book).JPG|thumb|1899 edition of Sands Directory ([[National Library of Australia]])]]
The '''Sands Directories''', also published as the '''Sands and Kenny Directory''' and the '''Sands and McDougall Directory''' were annual publications in Australia.

They listed household, business, society, and Government contacts&lt;ref name=":0" /&gt; in [[Melbourne]], [[Adelaide]] and [[Sydney]] including some rural areas of Victoria and New South Wales from the 1850s.&lt;ref name=Bibliog&gt;{{cite book|last1=Eslick, Christine; Joy Hughes, and R. Ian Jack|title=Bibliography of New South Wales local history: an annotated bibliography of secondary works published before 1982 and New South Wales directories 1828 -1950|date=1987|publisher=New South Wales University Press|location=Kensington, NSW|url=http://library.sl.nsw.gov.au/record=b1187352~S2|isbn=0-86840-154-4|pages=372; 398}}&lt;/ref&gt; [[City directory|City directories]] are an important resource for historical research, allowing individual addresses and occupations to be linked to specific streets and suburbs.&lt;ref&gt;{{cite book|last1=Williams|first1=A.V.|title=The development and growth of city directories|date=1913|publisher=Williams directory co.|location=Cincinnati, Ohio|url=http://babel.hathitrust.org/cgi/pt?id=nyp.33433082423645;view=1up;seq=5|accessdate=5 March 2015}}&lt;/ref&gt;

==Publisher==
[[File:Sands Directory 1899 (cover).JPG|thumb|1899 edition of Sands Directory (cover)]]
[[John Sands (printer)|John Sands]] (1818-1873) was an engraver, printer and stationer.  Born in England he moved to [[Sydney, Australia|Sydney]] in 1837.&lt;ref name="ADB Sands"&gt;{{cite book|last1=Walsh|first1=G.P.|title='Sands, John (1818–1873)', Australian Dictionary of Biography|date=1976|publisher=National Centre of Biography, Australian National University|url=http://adb.anu.edu.au/biography/sands-john-4536|accessdate=5 March 2015}}&lt;/ref&gt;  Sands formed several business partnerships, in 1851 with his brother-in-law Thomas Kenny, and in 1860 with Dugald McDougall with the business being known as [[John Sands (company)|Sands, Kenny &amp; Co.]]&lt;ref name="ADB Sands"/&gt; Directory titles changed as the publisher changed partners, and at different points the Sands Directories were also published as the 'Sands and Kenny' or 'Sands and McDougall Directories'.&lt;ref name=Kingston /&gt;

==Sands, Kenny &amp; Co's commercial and general Melbourne directory==
The first Melbourne Directory was published by Sands and Kenny in 1857.&lt;ref name=Kingston&gt;{{cite web|title=Sands and McDougall Melbourne Directories|url=http://www.kingston.vic.gov.au/library/Library-Services/Family-History-Resources/Sands-and-McDougall-Melbourne-Directories|website=Kingston Libraries|publisher=Kingston Libraries|accessdate=10 February 2015|ref=Kingston}}&lt;/ref&gt; By 1858 the second edition of the directory was distributed to public libraries in the major seaports of Great Britain, Ireland, the United States of America, and Canada.&lt;ref&gt;{{cite news |url=http://nla.gov.au/nla.news-article154855152 |title=Publications Received |newspaper=[[The Age]] |location=Melbourne |date=1 February 1858 |accessdate=5 March 2015 |page=6 }}&lt;/ref&gt;  From 1862 to 1974 the Melbourne directories were published as the Sands and McDougall Melbourne Directory.&lt;ref name=Kingston /&gt;&lt;ref&gt;{{cite news|last1=Stephens|first1=Andrew|title=Sands &amp; McDougall directory exhibition brings old Melbourne back to life.|url=http://www.smh.com.au/entertainment/sands--mcdougall-directory-exhibition-brings-old-melbourne-back-to-life-20140811-102quc.html|accessdate=5 March 2015|work=Sydney Morning Herald|date=15 August 2014}}&lt;/ref&gt;

The 1860 Melbourne directory was 400 pages long and contained over 10,000 entries.&lt;ref name=":0"&gt;{{cite news |url=http://nla.gov.au/nla.news-article154880275 |title=Sands and Kenny's Melbourne Directory|newspaper=[[The Age]]|location=Melbourne |date=24 January 1860 |accessdate=5 March 2015 |page=5 }}&lt;/ref&gt;

==Sands Sydney, Suburban and Country Commercial Directory==
[[File:Sands Directory 1899 (spine).JPG|thumb|1899 edition of Sands Directory (spine)]]
The ''Sands Sydney, Suburban and Country Commercial Directory'', first published in 1858,&lt;ref name="FMP"&gt;{{cite web|title=New South Wales, Sydney Directory 1847-1913|url=http://www.findmypast.com.au/articles/world-records/full-list-of-australia-and-new-zealand-records/newspapers-directories-and-social-history/new-south-wales-sydney-directory-1847-1913|website=Find My Past|accessdate=5 March 2015}}&lt;/ref&gt; included a variety of information including street addresses and businesses, farms and country towns, stock numbers (e.g. horses, cattle and sheep on each station) as well as information about public watering places including dams, tanks and wells.&lt;ref&gt;{{cite news |url=http://nla.gov.au/nla.news-article118819151 |title=Sands' Directory |newspaper=[[The Evening News (Sydney)|The Evening News]] |location=Sydney |date=15 January 1923 |accessdate=5 March 2015 |page=9 }}&lt;/ref&gt;  With the primary function of post office directory it provides lists of householders, businesses, public institutions and officials.&lt;ref&gt;{{cite web|title=Family history and genealogy|url=http://www.sl.nsw.gov.au/about/collections/documenting/social/famhist.html|website=State Library of NSW|accessdate=5 March 2015}}&lt;/ref&gt;

The Sydney editions of the directory, covering the state of New South Wales, were published each year from 1858–59 to 1932–33.&lt;ref name=Sydney&gt;{{cite web|title=Sands Sydney, Suburban and Country Commercial Directory|url=http://www.cityofsydney.nsw.gov.au/learn/search-our-collections/sands-directory|website=The City of Sydney|publisher=The City of Sydney|accessdate=10 February 2015}}&lt;/ref&gt;  There were four years when the directory did not appear during this time, they were 1872, 1874, 1878 and 1881.&lt;ref name= CGHG&gt;{{Citation | author1=Cridland, Marilyn | author2=Central Coast Family History Group (N.S.W.) | title=A guide to the Sands Directory | publication-date=1997 | publisher=Central Coast Family History Group Inc | page =1|url=http://trove.nla.gov.au/work/35275389 | accessdate=5 March 2015 }}&lt;/ref&gt;  The directory is arranged by municipalities in which properties were located, listing the primary householder street by street.&lt;ref&gt;{{cite web|title=Sands Sydney Directory Guide|url=http://www.waverley.nsw.gov.au/__data/assets/pdf_file/0009/28557/Sands_Sydney_Directory_guide_for_LS_website_revised.pdf|website=Waverley Council|accessdate=5 March 2015}}&lt;/ref&gt; As a consequence, the household and business information in the directories is used for research into Sydney history,&lt;ref&gt;{{cite web|title=Sands Directory – Researching your house's history|url=http://insidehistorymagazine.blogspot.com.au/2011/11/sands-directory-researching-your-houses.html|website=Inside History magazine|accessdate=5 March 2015}}&lt;/ref&gt; with particular application for genealogical research.&lt;ref name= CGHG/&gt;&lt;ref name="Sands digital edition"&gt;{{cite web|title=Sands' Directory [digital edition]|url=http://www.cityofsydney.nsw.gov.au/learn/search-our-collections/sands-directory|website=City of Sydney|accessdate=5 March 2015}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last1=Royal Australian Historical Society|title=Sands Directories are now online!|url=http://www.rahs.org.au/sands-directories-are-now-online/|accessdate=5 March 2015}}&lt;/ref&gt;

By 1909 the Sydney directory contained over 1700 pages.&lt;ref name=SMH_Trove&gt;{{cite news |url=http://nla.gov.au/nla.news-article15027528 |title=Sands' Directory 1909. |newspaper=[[The Sydney Morning Herald]] |location=NSW |date=9 January 1909 |accessdate=11 February 2015 |page=11}}&lt;/ref&gt; The full title of the 1913 edition of the directory of Sydney is ''Sands Sydney, Suburban and Country Directory for 1913 comprising, amongst other information, street, alphabetical, trade and professional, country towns, country alphabetical, pastoral, educational, governmental, parliamentary, law and miscellaneous lists''.&lt;ref name="FMP"/&gt;

==Sands &amp; McDougall's South Australian directory==
Sands and McDougall arrived in [[Adelaide, South Australia|Adelaide]] in 1883.&lt;ref name="SLSA"&gt;{{cite web|title=South Australian directories|url=http://guides.slsa.sa.gov.au/directories|website=State Library of South Australia|accessdate=5 March 2015}}&lt;/ref&gt;  They took over the directory previously published by Josiah Boothby, publishing their first South Australian directory in January 1884.&lt;ref name="SLSA"/&gt;&lt;ref&gt;{{Citation | author1=Sands &amp; McDougall Limited | title=Sands &amp; McDougall's South Australian directory : with which is incorporated Boothby's South Australian directory | publication-date=1884 | publisher=Printed and published by Sands &amp; McDougall | url=http://trove.nla.gov.au/work/21481893 | accessdate=5 March 2015 }}&lt;/ref&gt;&lt;ref&gt;{{cite news |url=http://nla.gov.au/nla.news-article166349067 |title=South Australian Directory |newspaper=[[The Southern Cross (South Australia)|The Southern Cross]] |location=Adelaide |date=27 March 1896 |accessdate=5 March 2015 |page=4}}&lt;/ref&gt; The Sands &amp; McDougall's Directory of South Australia was published from 1884 to 1974.&lt;ref name=Trove_SA&gt;{{cite web | author1=Sands &amp; McDougall Limited | title=Sands &amp; McDougall's directory of South Australia | publication-date=1884 | publisher=Sands &amp; McDougall | url=http://trove.nla.gov.au/work/21481863 | accessdate=11 February 2015 }}&lt;/ref&gt;

==See also==
[[Western Australia Post Office Directory|Wise Directories]]

==External links==
* [http://www.cityofsydney.nsw.gov.au/learn/search-our-collections/sands-directory digitised Sydney sands directory], at the City of Sydney archives

==References==
{{reflist|2}}

[[Category:Gazetteers]]
[[Category:Directories]]
[[Category:Australian directories]]</text>
      <sha1>qk7sli30aa37lg7l1b86hic59z4ipqe</sha1>
    </revision>
  </page>
  <page>
    <title>Whitepages (company)</title>
    <ns>0</ns>
    <id>25901032</id>
    <revision>
      <id>763025316</id>
      <parentid>762822514</parentid>
      <timestamp>2017-01-31T23:21:53Z</timestamp>
      <contributor>
        <username>Lizhpowell</username>
        <id>29411754</id>
      </contributor>
      <comment>link Alex Algard</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="24442" xml:space="preserve">{{pp-protect|small=yes}}
{{good article}}
{{Infobox dot-com company
| name             = Whitepages
| logo     = [[File:White-Pages-Logo.png|175px]]
| caption          =
| type             = Private 
| industry         = 
| foundation       = 1997
| founder          = [[Alex Algard]]
| defunct          = &lt;!-- {{End date|YYYY|MM|DD}} --&gt;
| location_city    = Seattle, Washington, US
| location_country = US
| locations        = &lt;!-- Number of locations, stores, offices, etc. --&gt;
| area_served      = Worldwide
| key_people       =  Rob Eleveld (CEO)&lt;ref name="newCEO"/&gt;
| products         = People search, contact data, mobile apps
| production       = 
| services         = 
| revenue          = $70 million (2015)&lt;ref name="recentsource"/&gt;
| operating_income = 
| net_income       = 
| aum              = &lt;!-- Only used with financial services companies --&gt;
| assets           = 
| equity           = 
| owner            = 
| num_employees    = 120 (2016)&lt;ref&gt;{{citation|publisher=Whitepages|title=Careers|url=http://whitepagesinc.com/about/careers.html|accessdate=August 19, 2013}}&lt;/ref&gt;
| parent           = 
| divisions        = 
| subsid           = 
| website = {{URL|http://www.whitepages.com}}
| footnotes        = 
| intl             =
| bodystyle        =
| website_type          = Directory
| current_status        = Active
}}'''Whitepages''' is a provider of online directory services, fraud screening and identity verification for businesses, public record background checks, and other products, based on its database of contact information for people and businesses. It has the largest database available of contact information on US residents.&lt;ref name="VB"/&gt;

Whitepages was founded in 1997 as a hobby for then-[[Stanford]] student [[Alex Algard]]. It was incorporated in 2000 and received $45 million in funding in 2005. Investors were later bought-out by Algard in 2013. From 2008 to 2013, Whitepages released several mobile apps, a re-design in 2009, the ability for consumers to control their contact information, and other features. From 2010 to 2016, the company shifted away from advertising revenue and began focusing more on selling business services and subscription products.

==History==
The idea for Whitepages was conceived by Alex Algard, while studying at [[Stanford]] in 1996. Algard was searching for a friend's contact information and the phone company gave him the wrong number.&lt;ref name="ppp"/&gt; He thought of an online email directory as an easier to way to find people.&lt;ref name="seven"/&gt;&lt;ref name="two"&gt;{{cite news|title=WhitePages.com has number for fast growth|url=http://community.seattletimes.nwsource.com/archive/?date=20031013&amp;slug=btinterface13|newspaper=The Seattle Times|accessdate=August 7, 2013|date=October 13, 2003}}&lt;/ref&gt; Algard bought the Whitepages.com domain for nine hundred dollars,&lt;ref name="four"&gt;{{cite news|first=Nicholas|last=Carlson|date=January 24, 2007|url=http://www.internetnews.com/xSP/article.php/3655611|publisher=InternetNews|title=WhitePages.com: Reach out and search someone|accessdate=December 2, 2013}}&lt;/ref&gt;&lt;ref name="recentsource"/&gt; which he says was all of his savings at the time.&lt;ref name="seven"/&gt; He continued operating the website as a hobby while working as an investment banker for [[Goldman Sachs]].&lt;ref name="dakfhukajehf"/&gt; He expanded the database of contact information using data licensed from American Business Information (now a part of Infogroup).&lt;ref name="recentsource"/&gt; Eventually WhitePages was producing more ad-revenue than Algard was earning at Goldman Sachs.&lt;ref name="recentsource"/&gt; In 1998, Algard left his job to focus on the website; he incorporated Whitepages in 2000.&lt;ref name="dakfhukajehf"&gt;{{citation|publisher=Private Equity Growth Capital Council|url=http://www.pegcc.org/wordpress/wp-content/uploads/pec_cs_whitepages_020309a.pdf|title=WhitePages.com: From hobby to number one people search destination|accessdate=August 6, 2013}}&lt;/ref&gt;

The site grew and attracted more advertisers. The company brokered deals with Yellowpages and Superpages, whereby Whitepages earned revenue for sending them referral traffic. By 2005, $15 million in annual revenues was coming from these contracts.&lt;ref name="recentsource"/&gt; In 2003, Algard stepped down as CEO to focus on CarDomain.com, which he had also founded&lt;ref name="ppp"&gt;{{cite news|first=Brad|last=Broberg|title=Founder returns to WhitePages.com|publisher=Puget Sound Business Journal|date=September 30, 2007|url=http://www.bizjournals.com/seattle/stories/2007/10/01/focus10.html|accessdate=August 7, 2013}}&lt;/ref&gt; and Max Bardon took his place as CEO temporarily.&lt;ref name="recentsource"/&gt; In 2005, Technology Crossover Ventures and Providence Equity Partners invested $45 million in the company.&lt;ref name="recentsource"/&gt;&lt;ref name="one"/&gt; That same year, MSN adopted Whitepages' directory data for its "Look it up" feature.&lt;ref&gt;{{cite news|title=MSN Replaces InfoSpace with WhitePages.com|url=http://www.mediapost.com/publications/article/28828/#axzz2bIuB3tM1|first=Shankar|last=Gupta|date=April 5, 2005|accessdate=August 7, 2013|publisher=MediaPost}}&lt;/ref&gt; Algard returned to the company in 2007.&lt;ref name="ppp"/&gt; By the end of that year, the Whitepages database had grown to 180 million records&lt;ref&gt;{{cite news|title=WhitePages.com coverage expands from 40 to 80 percent|url=http://seattletimes.com/html/businesstechnology/2004062675_btbriefs10.html|newspaper=The Seattle Times|date=December 10, 2007|accessdate=August 7, 2013}}&lt;/ref&gt; and the company was listed as one of [[Deloitte]]'s 500 fastest growing technology companies in North America three times.&lt;ref name="seven"/&gt;&lt;ref&gt;{{cite news|title=WhitePages hires new CTO|first=Rebecca|last=Collins|url=http://www.bizjournals.com/seattle/blog/techflash/2010/11/whitepages-taps-new-cto.html|publisher=Puget Sound Business Journal|date=November 17, 2010|accessdate=August 8, 2013}}&lt;/ref&gt; By 2008 the company had $66 million in annual revenues.&lt;ref name="recentsource"/&gt;

In 2008, Whitepages said it would start working on options for users to control their information on the site.&lt;ref&gt;{{cite news|first=Steven|last=Vaughan-Nichols|newspaper=Computerworld|url=http://www.computerworld.com.au/article/216557/whitepages_com_grapples_privacy_web_2_0_world/?|title=WhitePages.com grapples with privacy in Web 2.0 world|date=May 19, 2008|accessdate=August 7, 2013}}&lt;/ref&gt; That same year, it acquired [[Voice over Internet Protocol|VoIP]] developer [[Snapvine]]&lt;ref name="one"&gt;{{cite news|first=Angel|last=Gonzalez|url=http://seattletimes.com/html/businesstechnology/2004458452_whitepages05.html|newspaper=The Seattle Times|title=WhitePages.com to buy Snapvine|accessdate=August 7, 2013|date=June 5, 2008}}&lt;/ref&gt; in order to add features where users could be called through the website without giving out their phone number.&lt;ref&gt;{{cite news|title=WhitePages.com to buy Snapvine for around $20 million|first=Michael|last=Arrington|date=June 4, 2008|accessdate=August 7, 2013|url=http://techcrunch.com/2008/06/04/whitepagescom-to-buy-snapvine-for-around-20-million/|publisher=TechCrunch}}&lt;/ref&gt; It also introduced an [[api]],  which gave third-party developers access to Whitepages' data.&lt;ref&gt;{{cite news|first=Mike|last=Gunderloy|date=March 31, 2008|url=http://gigaom.com/2008/03/31/open-phone-data-whitepages/|accessdate=August 7, 2013|title=Open Phone Data from WhitePages.com|publisher=Giga Om}}&lt;/ref&gt; Whitepages released an iOS app that August, followed by the Whitepages Caller ID app for Android devices  in February 2009&lt;ref&gt;{{cite news|publisher=VentureBeat|first=MG|last=Siegler|date=February 27, 2009|accessdate=August 7, 2013|url=http://venturebeat.com/2009/02/27/caller-id-a-paid-android-app-to-better-screen-my-phone-calls/|title=Caller ID: A paid Android app to better screen my phone calls}}&lt;/ref&gt; and for Blackberry that May.&lt;ref name="plp"&gt;{{cite news|publisher=VentureBeat|title=The background-check scams: Is WhitePages really better than Intelius?|url=http://venturebeat.com/2009/05/07/the-background-check-scams-is-whitepages-really-better-than-intelius/|first=Matt|last=Marshall|date=May 7, 2009|accessdate=August 7, 2013}}&lt;/ref&gt; 

The app displays information on callers, such as their latest social media posts, local weather at the caller's location and the identity of the caller.&lt;ref name="eightlyy"&gt;{{cite news|first=Austin|last=Carr|newspaper=Fast Company|url=http://www.fastcompany.com/3000252/whitepages-launches-caller-id-social-mobile-age|title=WhitePages Launches Caller ID for the Social, Mobile Age|date=August 7, 2012|accessdate=August 7, 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite news|newspaper=Time Magazine|first=Doug|last=Aamoth|url=http://techland.time.com/2012/12/04/top-10-tech-lists/slide/current-caller-id-android/|date=December 4, 2012|accessdate=August 7, 2013|title=Current Caller ID (Android)}}&lt;/ref&gt;&lt;ref name="twenty"&gt;{{cite news|title=WhitePages' new Current Caller ID App is the future of smartphone calling|url=http://venturebeat.com/2012/08/08/whitepages-current-caller-id-android/|date=August 8, 2012|first=Devindra|last=Hardawar|accessdate=August 7, 2013|publisher=VentureBeat}}&lt;/ref&gt; It originally had the ability to display information on callers, such as their latest social media posts, local weather at the caller's location and the identity of the caller.&lt;ref name="eightlyy"&gt;{{cite news|first=Austin|last=Carr|newspaper=Fast Company|url=http://www.fastcompany.com/3000252/whitepages-launches-caller-id-social-mobile-age|title=WhitePages Launches Caller ID for the Social, Mobile Age|date=August 7, 2012|accessdate=August 7, 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite news|newspaper=Time Magazine|first=Doug|last=Aamoth|url=http://techland.time.com/2012/12/04/top-10-tech-lists/slide/current-caller-id-android/|date=December 4, 2012|accessdate=August 7, 2013|title=Current Caller ID (Android)}}&lt;/ref&gt;&lt;ref name="twenty"&gt;{{cite news|title=WhitePages' new Current Caller ID App is the future of smartphone calling|url=http://venturebeat.com/2012/08/08/whitepages-current-caller-id-android/|date=August 8, 2012|first=Devindra|last=Hardawar|accessdate=August 7, 2013|publisher=VentureBeat}}&lt;/ref&gt; The ability for consumers to add themselves to the directory was added in the summer of 2009 and being able to edit existing entries was added that October.&lt;ref&gt;{{cite news|title=WhitePages Now Lets you control your own listings|first=Erick|last=Schonfeld|date=October 14, 2009|accessdate=August 8, 2013|url=http://techcrunch.com/2009/10/14/whitepages-now-lets-you-control-your-own-listings/|publisher=TechCrunch}}&lt;/ref&gt;

Whitepages.com underwent a re-design in 2009.&lt;ref name="three"&gt;{{cite news|title=WhitePages launches $2.5 million overhaul|first=Brier|last=Dudley|url=http://seattletimes.com/html/technologybrierdudleysblog/2009467080_whitepagescom_launches_25_mill.html|date=July 14, 2009|accessdate=August 7, 2013|newspaper=The Seattle Times}}&lt;/ref&gt; According to VentureBeat reporter Matt Marshall, the redesign made the advertising "cleaner" and made it more obvious when someone was going to a third-party website like US Search.&lt;ref name="VB"&gt;{{cite news|date=July 14, 2009|first=Matt|last=Marshall|url=http://venturebeat.com/2009/07/14/whitepages-now-the-largest-database-of-american-people-cleans-up-act/|publisher=VentureBeat|title=WhitePages, now the largest database of American people, cleans up act|accessdate=August 7, 2013}}&lt;/ref&gt; Marshall had previously criticized Whitepages, because website users that clicked on US Search ads and purchased data from US Search were sent through perpetual advertisements for other services that made it difficult to access the information they paid for.&lt;ref name="VB"/&gt;&lt;ref&gt;{{cite news|title=The background-check scams: Is WhitePages really better than Intelius?|url=http://venturebeat.com/2009/05/07/the-background-check-scams-is-whitepages-really-better-than-intelius/|date=May 7, 2009|first=Matt|last=Marshall|accessdate=August 7, 2013}}&lt;/ref&gt; A local business lookup feature called "Store Finder" was added in June 2010.&lt;ref&gt;{{cite news|title=WhitePages upgrades business search, adds "store finder"|url=http://seattletimes.com/html/technologybrierdudleysblog/2012197459_whitepages_upgrades_business_s.html|first=Brier|last=Dudley|newspaper=The Seattle Times|date=June 24, 2010}}&lt;/ref&gt; The following month, Whitepages.com launched a deal site, Dealpop.com,&lt;ref&gt;{{cite news|title=Local shops join forces with coupon websites to sweeten sales|first=Melissa|last=Allison|author2=Amy Martinez |url=http://seattletimes.com/html/retailreport/2012259556_retailreport02.html|newspaper=The Seattle Times|date=July 1, 2010|accessdate=August 6, 2013}}&lt;/ref&gt; which differed from [[Groupon]] by offering short-term deals on nationally available products.&lt;ref&gt;{{cite news|first=Amy|last=Martinez|date=October 20, 2010|accessdate=August 7, 2013|newspaper=The Seattle Times|url=http://seattletimes.com/html/businesstechnology/2013209878_dealpopweb21.html|title=WhitePages' DealPop to try national approach as it takes on Groupon, other coupon websites}}&lt;/ref&gt; Dealpop was sold to [[Martin Tobias#Tippr.com|Tippr]] the following year.&lt;ref&gt;{{cite news|title=Tippr Grabs Sales &amp; Tech Talent in DealPop Acquisition, Continuing Daily Deals Dogfight for Third Place|url=http://www.xconomy.com/seattle/2011/06/01/tippr-grabs-sales-tech-talent-in-dealpop-acquisition-continuing-daily-deals-dogfight-for-third-place/|newspaper=Xconomy|date=July 1, 2011|accessdate=August 7, 2013|first=Curt|last=Wooodward}}&lt;/ref&gt;

In 2010, Superpages and Yellowpages cut back spending with Whitepages from $33 million to $7 million, causing a substantial decline in revenues and a tense relationship with investors. Algard spent $50 million in cash the company had on-hand and $30 million from a bank loan, to buyout the investors in 2013. He also used his personal house, savings account and personal belongings as collateral for the loan.&lt;ref name="recentsource"/&gt; Algard began shifting the company's business model to reduce its reliance on advertising and instead focus on business users and paid subscriptions.&lt;ref name="recentsource"/&gt;&lt;ref name="Carlson 2013"&gt;{{cite web | last=Carlson | first=Nicholas | title=With Buyback, 16-Year-Old Startup WhitePages Is Doing Something Very Rare With $80 Million | website=Business Insider | date=October 21, 2013 | url=http://www.businessinsider.com/whitepages-stock-buyback-2013-10 | accessdate=August 18, 2016}}&lt;/ref&gt; 

Whitepages released the Localicious app in July 2011. The app was released on Android first, because Whitepages was frustrated with Apple's approval process for iPhone apps.&lt;ref name="agiu"&gt;{{cite news|title=WhitePages goes Android first with latest app|url=http://news.cnet.com/8301-1023_3-20079150-93/whitepages-goes-android-first-with-latest-app/|date=July 13, 2011|first=Ina|last=Fried|accessdate=August 7, 2013|publisher=All Things Digital}}&lt;/ref&gt; Whitepages PRO was also introduced that same year.&lt;ref name="cardnotpresent"&gt;{{cite news|url=http://pro.whitepages.com/sites/pro.whitepages.com/files/Marketing_Documents/CardNotPresent%20Article%2010.24.12.pdf|publisher=CNP Report|first=D.J.|last=Murphy|date=October 24, 2012|accessdate=September 24, 2013|title=WhitePages PRO Taps Phone Data and More to Identify CNP Fraud}}&lt;/ref&gt; An updated Android app called  Current Caller ID was released in August 2012.&lt;ref name="eightlyy"/&gt; Within a year of its release, 5 billion calls and texts had been transmitted using the app. It was updated in July 2013 with new features, such as the ability to customize the layout of caller information for each caller and the ability to "Like" Facebook posts from within the app.&lt;ref name="fgy"&gt;{{cite news|title=WhitePages' Current Caller ID app powers more than 5B calls &amp; texts, adds new customization features|url=http://venturebeat.com/2013/07/25/whitepages-current-caller-id-app-powers-more-than-5b-calls-texts-adds-new-customization-features/|publisher=VentureBeat|first=Devindra|last=Hardawar|date=July 25, 2013|accessdate=August 7, 2013}}&lt;/ref&gt; In June 2013, Whitepages acquired Mr. Number, an Android app for blocking unwanted callers.&lt;ref&gt;{{cite news|title=WhitePages Scoops up Mr. Number, an Android App for Blocking Unwanted Calls|date=June 1, 2013|first=Ina|last=Fried|url=http://allthingsd.com/20130601/whitepages-scoops-up-mr-number-an-android-app-for-blocking-unwanted-calls/|newspaper=The Wall Street Journal|accessdate=August 7, 2013}}&lt;/ref&gt;

In August 2013 Whitepages purchased all the interests in the company owned by investors for $80 million.&lt;ref name="dafhybniub"&gt;{{cite news|title=With Buyback, 16-Year-Old Startup WhitePages Is Doing Something Very Rare With $80 Million|first=Nicholas|last=Carlson|date=October 21, 2013|url=http://www.businessinsider.com/whitepages-stock-buyback-2013-10#ixzz2qRETXgXX|publisher=Business Insider|accessdate=October 30, 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite news|title=Nextcast: WhitePages CEO Alex Algard on the distraction of outside investors and keeping your startup zeal|first=Jeff|last=Dickey|date=April 5, 2014|accessdate=May 2, 2014|url=http://www.geekwire.com/2014/nextcast-whitepages-ceo-alex-algard-distraction-outside-investors-keep-startup-zeal/|publisher=Geekwire}}&lt;/ref&gt; In 2015, WhitePages acquired San Francisco-based NumberCorp to improve the database of phone numbers used for scams in the Caller ID app.&lt;ref name="Perez 2015"&gt;{{cite web | last=Perez | first=Sarah | title=Whitepages Acquires NumberCop To Improve Its Scam-Detecting Caller ID App | website=TechCrunch | date=June 10, 2015 | url=http://social.techcrunch.com/2015/06/10/whitepages-acquires-numbercop-to-improve-its-scam-detecting-caller-id-app/ | accessdate=August 12, 2016}}&lt;/ref&gt; In April 2016, Whitepages spun-off its caller ID business into a separate company called Hiya&lt;ref name="Lunden 2016"&gt;{{cite web | last=Lunden | first=Ingrid | title=Whitepages spins out its caller-ID business as Hiya to take on TrueCaller | website=TechCrunch | date=April 27, 2016 | url=http://social.techcrunch.com/2016/04/27/whitepages-hiya/ | accessdate=July 8, 2016}}&lt;/ref&gt; with a staff of 40 in Seattle.&lt;ref name="Flynn 2016"&gt;{{cite web | last=Flynn | first=Kerry | title=Meet Hiya: Whitepages Spins Off Caller ID Business With Mission To Fight Robocalls, Spam Texts Worldwide | website=International Business Times | date=April 27, 2016 | url=http://www.ibtimes.com/meet-hiya-whitepages-spins-caller-id-business-mission-fight-robocalls-spam-texts-2360298 | accessdate=July 8, 2016}}&lt;/ref&gt; In September 2016, Alex Algard stepped down as CEO of WhitePages, in order to focus on the mobile spam-blocking spin-off Hiya. He appointed Rob Eleveld as the new WhitePages CEO.&lt;ref name="newCEO"&gt;{{cite web | title=Whitepages Founder Alex Algard Gives Up CEO Slot To Focus On Caller ID Startup Hiya | newspaper=Forbes | date=September 16, 2016 | url=http://www.forbes.com/sites/amyfeldman/2016/09/16/whitepages-founder-alex-algard-gives-up-ceo-slot-there-to-focus-on-caller-id-spinoff-hiya/#2db4ba761803 | accessdate=September 20, 2016}}&lt;/ref&gt;

==Services==
Whitepages has the largest database of contact information on Americans.&lt;ref name="VB"/&gt; As of 2008, it had data on about 90 percent of the US adult population,&lt;ref&gt;{{cite news|publisher=IntoMobile|first=Dusan|last=Belic|date=May 8, 2012|accessdate=September 24, 2013|url=http://www.intomobile.com/2012/05/08/whitepages-ios-app-gets-nearby-search-capability/|title=WhitePages' iOS app gets nearby search capability}}&lt;/ref&gt; including 200 million records on people and 15 million business listings.&lt;ref name="seven"&gt;{{cite news|title=A Directory of Success: WhitePages CEO Alex Algard|date=February 2, 2011|newspaper=Examiner|first=Paul|last=Kim}}&lt;/ref&gt; Whitepages' data is collected from property deeds,&lt;ref name="five"/&gt; telecom companies, and public records.&lt;ref name="ll"&gt;{{cite news|title=WhitePages IDs Growth in the Explosion of Personal Data|date=August 20, 2012|first=Curt|last=Woodward|accessdate=August 7, 2013|url=http://www.xconomy.com/seattle/2012/08/20/whitepages/}}&lt;/ref&gt; Privacy is a common concern regarding Whitepages' publishing of personal contact information.&lt;ref name="StairReynolds2008"&gt;{{cite book|author1=Ralph M. Stair|author2=George Reynolds|author3=George Walter Reynolds|title=Fundamentals of Information Systems|url=https://books.google.com/books?id=J85RP4YmBTYC&amp;pg=PA253|accessdate=7 August 2013|date=December 2008|publisher=Cengage Learning|isbn=978-1-4239-2581-1|pages=253–}}&lt;/ref&gt; The Whitepages.com website has features that allow users to remove themselves from the directory or correct and update information.&lt;ref name="five"&gt;{{cite news|title=Connecticut may let residents remove directory information|url=http://www.scmagazine.com/connecticut-may-let-residents-remove-directory-data/article/100267/#|date=December 28, 2007|first=Dan|last=Kaplan|newspaper=SC Magazine}}&lt;/ref&gt;&lt;ref name="StairReynolds2008"/&gt; WhitePages.com has about 50 million unique visitors per month&lt;ref&gt;{{cite news|publisher=VentureBeat|title=WhitePages acquires Mr. Number, the phone-spam Android app with 7M downloads, to reduce phone spam|url=http://www.reuters.com/article/2013/05/31/idUS27174982720130531|first=John|last=Koetsier|date=May 31, 2013|accessdate=December 2, 2013}}&lt;/ref&gt; and performs two billion searches per month.&lt;ref name="cardnotpresent"/&gt;

WhitePages started developing features for business users around 2010.&lt;ref name="recentsource"/&gt; WhitePages Pro is used for things like verifying the identity of a sales lead, find fake form data in online forms and to check form data from consumers making a purchase against common indicators of fraud, like shipping to a mailbox at an unoccupied building.&lt;ref name="recentsource"&gt;{{cite news| first=Amy|last=Feldman |title=Alex Algard Risked Everything To Turn His Struggling Firm, Whitepages, Into A Growing Tech Company | newspaper=Forbes | date=August 23, 2016 | url=http://www.forbes.com/sites/amyfeldman/2016/08/03/alex-algard-risked-everything-to-turn-his-struggling-firm-whitepages-into-a-growing-tech-company/#165b97ae73d0 | accessdate=August 10, 2016}}&lt;/ref&gt;&lt;ref name="cardnotpresent"/&gt;&lt;ref name="Whitepages Pro"&gt;{{cite web | title=Whitepages Pro – Mobile Identity Data for Businesses | website=Whitepages Pro | url=http://pro.whitepages.com/ | accessdate=August 15, 2016}}&lt;/ref&gt; In 2016, advertising on WhitePages.com was turned off in favor of selling monthly subscriptions that give users unlimited background checks and other records.&lt;ref name="recentsource"/&gt;

As of 2013 Whitepages provides its data and related services through seven web properties, ten mobile apps&lt;ref&gt;{{citation|url=http://whitepagesinc.com/about/|publisher=WhitePages|title=About Us|accessdate=December 2, 2013}}&lt;/ref&gt; and  through multiple web properties, including 411.com and Switchboard.com.&lt;ref name="SuiElwood2012"&gt;{{cite book|author1=Daniel Zhi Sui|author2=Sarah Elwood|author3=Michael F. Goodchild|title=Crowdsourcing Geographic Knowledge: Volunteered Geographic Information (VGI) in Theory and Practice|url=https://books.google.com/books?id=SSbHUpSk2MsC&amp;pg=PA267|accessdate=7 August 2013|date=10 August 2012|publisher=Springer|isbn=978-94-007-4587-2|pages=267–}}&lt;/ref&gt; The Hiya app (previously known as WhitePages Caller ID) checks incoming calls against a database of phone numbers known for spam or scam calls and helps users report scams to the Federal Trade Commission.&lt;ref name="Stern 2016"&gt;{{cite web | last=Stern | first=Joanna | title=How to Stop Robocalls … or at Least Fight Back | website=WSJ | date=June 28, 2016 | url=http://www.wsj.com/articles/how-to-stop-robocalls-or-at-least-fight-back-1467138771 | accessdate=July 8, 2016}}&lt;/ref&gt;&lt;ref name="Lerman 2016"&gt;{{cite web | last=Lerman | first=Rachel | title=Whitepages spins out mobile caller-ID startup Hiya | website=The Seattle Times | date=April 27, 2016 | url=http://www.seattletimes.com/business/technology/whitepages-spins-out-mobile-caller-id-startup-ceo-takes-on-dual-roles/ | accessdate=July 8, 2016}}&lt;/ref&gt; Hiya mobile app replaces the Android user interface for making and receiving phone calls.&lt;ref name="fgy"/&gt;

==References==
{{reflist|2}}

==External links==
*[http://www.whitepages.com/ Official website]

{{DEFAULTSORT:Whitepages.Com}}
[[Category:Directories]]
[[Category:Internet properties established in 1997]]
[[Category:Privately held companies based in Washington (state)]]
[[Category:Companies based in Seattle, Washington]]
[[Category:Online person databases]]</text>
      <sha1>i7gkruewobsz0fvf3csi3ixei4q0nhb</sha1>
    </revision>
  </page>
  <page>
    <title>Web search query</title>
    <ns>0</ns>
    <id>11525372</id>
    <revision>
      <id>724066361</id>
      <parentid>720529338</parentid>
      <timestamp>2016-06-06T23:00:26Z</timestamp>
      <contributor>
        <username>Dcirovic</username>
        <id>11795905</id>
      </contributor>
      <minor />
      <comment>refs using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10948" xml:space="preserve">A '''web search query''' is a query that a user enters into a [[web search engine]] to satisfy his or her [[information needs]]. Web search queries are distinctive in that they are often plain text or [[hypertext]] with optional search-directives (such as "and"/"or" with "-" to exclude). They vary greatly from standard [[query language]]s, which are governed by strict syntax rules as [[command language]]s with keyword or positional [[Parameter (computer science)|parameters]].

== Types ==
There are three broad categories that cover most web search queries: informational, navigational, and transactional.&lt;ref&gt;Broder, A. (2002). A taxonomy of Web search. SIGIR Forum, 36(2), 3–10.&lt;/ref&gt; These are also called "do, know, go."&lt;ref&gt;{{cite web|last=Gibbons|first=Kevin|title=Do, Know, Go: How to Create Content at Each Stage of the Buying Cycle|url=http://searchenginewatch.com/article/2235624/Do-Know-Go-How-to-Create-Content-at-Each-Stage-of-the-Buying-Cycle|publisher=Search Engine Watch|accessdate=24 May 2014}}&lt;/ref&gt; Although this model of searching was not theoretically derived, the classification has been  empirically validated with actual search engine queries.&lt;ref&gt;Jansen, B. J., Booth, D., and Spink, A. (2008) [https://faculty.ist.psu.edu/jjansen/academic/pubs/jansen_user_intent.pdf Determining the informational, navigational, and transactional intent of Web queries], Information Processing &amp; Management. 44(3), 1251-1266.&lt;/ref&gt;

* '''Informational queries''' – Queries that cover a broad topic (e.g., ''colorado'' or ''trucks'') for which there may be thousands of relevant results.
* '''Navigational queries''' – Queries that seek a single website or web page of a single entity (e.g., ''youtube'' or ''delta air lines'').
* '''Transactional queries''' – Queries that reflect the intent of the user to perform a particular action, like purchasing a car or downloading a screen saver.

Search engines often support a fourth type of query that is used far less frequently:

* '''Connectivity queries''' – Queries that report on the connectivity of the indexed [[web graph]] (e.g., Which links point to this [[Uniform Resource Locator|URL]]?, and How many pages are indexed from this [[domain name]]?).&lt;ref&gt;{{cite web|last=Moore|first=Ross|title=Connectivity servers|url=http://nlp.stanford.edu/IR-book/html/htmledition/connectivity-servers-1.html|publisher=Cambridge University Press|accessdate=24 May 2014}}&lt;/ref&gt;

== Characteristics ==

Most commercial web search engines do not disclose their search logs, so information about what users are searching for on the Web is difficult to come by.&lt;ref&gt;Dawn Kawamoto and Elinor Mills (2006), [http://news.cnet.com/AOL-apologizes-for-release-of-user-search-data/2100-1030_3-6102793.html AOL apologizes for release of user search data]&lt;/ref&gt; Nevertheless, research studies appeared in 1998.&lt;ref&gt;Jansen, B. J., Spink, A., Bateman, J., and Saracevic, T. 1998. [https://faculty.ist.psu.edu/jjansen/academic/jansen_sigir_forum.pdf Real life information retrieval: A study of user queries on the web]. SIGIR Forum, 32(1), 5 -17.&lt;/ref&gt;&lt;ref&gt;Silverstein, C., Henzinger, M., Marais, H., &amp; Moricz, M. (1999). Analysis of a very large Web search engine query log. SIGIR Forum,
33(1), 6–12.&lt;/ref&gt; Later, a study in 2001&lt;ref&gt;{{cite journal|author1=Amanda Spink |author2=Dietmar Wolfram |author3=Major B. J. Jansen |author4=Tefko Saracevic | year = 2001 | title = [https://faculty.ist.psu.edu/jjansen/academic/jansen_public_queries.pdf Searching the web: The public and their queries] | journal = Journal of the American Society for Information Science and Technology | volume = 52 | issue = 3 | pages = 226–234 | doi = 10.1002/1097-4571(2000)9999:9999&lt;::AID-ASI1591&gt;3.3.CO;2-I }}&lt;/ref&gt; analyzed the queries from the [[Excite]] search engine showed some interesting characteristics of web search:

* The average length of a search query was 2.4 terms. 
* About half of the users entered a single query while a little less than a third of users entered three or more unique queries. 
* Close to half of the users examined only the first one or two pages of results (10 results per page).
* Less than 5% of users used advanced search features (e.g., [[boolean operators]] like AND, OR, and NOT).
* The top four most frequently used terms were, '' (empty search), and, of, ''and'' sex.

A study of the same Excite query logs revealed that 19% of the queries contained a geographic term (e.g., place names, zip codes, geographic features, etc.).&lt;ref&gt;{{cite conference |author1=Mark Sanderson  |author2=Janet Kohler  |lastauthoramp=yes | year = 2004 | title = Analyzing geographic queries | booktitle = Proceedings of the Workshop on Geographic Information (SIGIR '04) | url =http://supremacyseo.com/analyzing-geographic-queries }}&lt;/ref&gt; Studies also show that, in addition to short queries (i.e., queries with few terms), there are also predictable patterns to how users change their queries.&lt;ref&gt;Jansen, B. J., Booth, D. L., &amp; Spink, A. (2009). [[Patterns of query modification during Web searchinhttps://faculty.ist.psu.edu/jjansen/academic/pubs/jansen_patterns_query_reformulation.pdf|Patterns of query modification during Web searchin]]g. Journal of the American Society for Information Science and Technology. 60(3), 557-570. 60(7), 1358-1371.&lt;/ref&gt;

A 2005 study of Yahoo's query logs revealed 33% of the queries from the same user were repeat queries and that 87% of the time the user would click on the same result.&lt;ref&gt;{{cite conference |author1=Jaime Teevan |author2=Eytan Adar |author3=Rosie Jones |author4=Michael Potts | year = 2005 | title = History repeats itself: Repeat Queries in Yahoo's query logs | booktitle = Proceedings of the 29th Annual ACM Conference on Research and Development in Information Retrieval (SIGIR '06) | pages = 703–704 | url =http://www.csail.mit.edu/~teevan/work/publications/posters/sigir06.pdf | doi=10.1145/1148170.1148326 }}&lt;/ref&gt; This suggests that many users use repeat queries to revisit or re-find information. This analysis is confirmed by a Bing search engine blog post telling about 30% queries are navigational queries &lt;ref&gt;http://www.bing.com/community/site_blogs/b/search/archive/2011/02/10/making-search-yours.aspx&lt;/ref&gt;

In addition, much research has shown that query term frequency distributions conform to the [[power law]], or ''long tail'' distribution curves. That is, a small portion of the terms observed in a large query log (e.g. &gt; 100 million queries) are used most often, while the remaining terms are used less often individually.&lt;ref name="baezayates1"&gt;{{cite journal | author = Ricardo Baeza-Yates | year = 2005 | title = Applications of Web Query Mining | booktitle = Lecture Notes in Computer Science | pages = 7–22 | volume = 3408 | publisher = Springer Berlin / Heidelberg | url = http://www.springerlink.com/content/kpphaktugag5mbv0/ | ISBN = 978-3-540-25295-5}}&lt;/ref&gt; This example of the [[Pareto principle]] (or ''80–20 rule'') allows search engines to employ [[optimization techniques]] such as index or [[Partition (database)|database partitioning]], [[web cache|caching]] and pre-fetching. In addition, studies have been conducted on discovering linguistically-oriented attributes that can recognize if a web query is navigational, informational or transactional.&lt;ref&gt;{{cite journal | author = Alejandro Figueroa | year = 2015 | title = Exploring effective features for recognizing the user intent behind web queries | booktitle = Computers in Industry | pages = 162–169 | volume = 68 | publisher = Elsevier | url = https://www.researchgate.net/publication/271911317_Exploring_effective_features_for_recognizing_the_user_intent_behind_web_queries}}&lt;/ref&gt;

But in a recent study in 2011 it was found that the average length of queries has grown steadily over time and average length of non-English languages queries had increased more than English queries.&lt;ref&gt;{{cite journal |author1=Mona Taghavi |author2=Ahmed Patel |author3=Nikita Schmidt |author4=Christopher Wills |author5=Yiqi Tew | year = 2011 | title = An analysis of web proxy logs with query distribution pattern approach for search engines | booktitle = Journal of Computer Standards &amp; Interfaces | pages = 162–170 | volume = 34 | issue = 1 |publisher = Elsevier  | url = http://www.sciencedirect.com/science/article/pii/S0920548911000808 | doi=10.1016/j.csi.2011.07.001}}&lt;/ref&gt; Google has implemented the [[Google Hummingbird|hummingbird]] update in August 2013 to handle longer search queries since more searches are conversational (i.e. "where is the nearest coffee shop?").&lt;ref&gt;{{cite web|last=Sullivan|first=Danny|title=FAQ: All About The New Google "Hummingbird" Algorithm|url=http://searchengineland.com/google-hummingbird-172816|publisher=Search Engine Land|accessdate=24 May 2014}}&lt;/ref&gt; 
For longer queries, [[Natural language processing]] helps, since parse trees of queries can be matched with that of answers and their snippets.&lt;ref&gt;{{vcite journal |author=Galitsky B|title=Machine learning of syntactic parse trees for search and classification of text|journal=Engineering Applications of Artificial Intelligence |volume=26 |issue=3 |date=2013 |pages=153-172|doi=10.1016/j.engappai.2012.09.017}}&lt;/ref&gt; For multi-sentence queries where keywords statistics and [[Tf–idf]] is not very helpful, [[Parse thicket]] technique comes into play to structurally represent complex questions and answers.&lt;ref&gt;{{vcite journal |author=Galitsky B, Ilvovsky D, Kuznetsov SO, Strok F|title=Finding Maximal Common Sub-parse Thickets
for Multi-sentence Search |journal=Lecture Notes In Artificial Intelligence |volume = 8323 |date=2013 |http://www.aclweb.org/anthology/R13-1037
}}&lt;/ref&gt;

== Structured queries ==
With search engines that support Boolean operators and parentheses, a technique traditionally used by librarians can be applied. A user who is looking for documents that cover several topics or ''facets'' may want to describe each of them by a [[logical disjunction|disjunction]] of characteristic words, such as &lt;code&gt;vehicles OR cars OR automobiles&lt;/code&gt;. A ''faceted query'' is a [[logical conjunction|conjunction]] of such facets; e.g. a query such as &lt;code&gt;(electronic OR computerized OR DRE) AND (voting OR elections OR election OR balloting OR electoral)&lt;/code&gt; is likely to find documents about electronic voting even if they omit one of the words "electronic" and "voting", or even both.&lt;ref&gt;{{Cite web
|url=http://eprints.eemcs.utwente.nl/6918/01/TR-CTIT-06-57.pdf
|title=Exploiting Query Structure and Document Structure to Improve Document Retrieval Effectiveness
|author1=Vojkan Mihajlović |author2=Djoerd Hiemstra |author3=Henk Ernst Blok |author4=Peter M.G. Apers |postscript=&lt;!--None--&gt;}}&lt;/ref&gt;

== See also ==
* [[Information retrieval]]
* [[Web search engine]]
* [[Web query classification]]
* [[Taxonomy for search engines]]

== References ==
{{reflist|2}}

{{Internet search}}

[[Category:Internet search]]</text>
      <sha1>ar5jnedoolddh2swp3jprfwspmyfonr</sha1>
    </revision>
  </page>
  <page>
    <title>Hyper Search</title>
    <ns>0</ns>
    <id>11853249</id>
    <revision>
      <id>558370252</id>
      <parentid>555925186</parentid>
      <timestamp>2013-06-05T00:33:28Z</timestamp>
      <contributor>
        <username>EmausBot</username>
        <id>11292982</id>
      </contributor>
      <minor />
      <comment>Bot: Migrating 2 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:Q3787879]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="765" xml:space="preserve">'''Hyper Search''' has been the first{{cn|date=May 2013}} published technique to introduce [[link analysis]] for search engines. It was created by Italian researcher [[Massimo Marchiori]].

==Bibliography==

* [[Massimo Marchiori]], [http://www.w3.org/People/Massimo/papers/WWW6/ "The Quest for Correct Information on the Web: Hyper Search Engines"], ''Proceedings of the Sixth International World Wide Web Conference (WWW6)'', 1997.
* [[Sergey Brin]] and [[Lawrence Page]], [http://www-db.stanford.edu/~backrub/google.html "The anatomy of a large-scale hypertextual Web search engine"], ''Proceedings of the Seventh International World Wide Web Conference (WWW7)'', 1998. 

== See also ==
* [[PageRank]]
* [[Spamdexing]]

[[Category:Internet search]]

{{web-stub}}</text>
      <sha1>3fv2giwphv2iel7kfxk452j1k5g591s</sha1>
    </revision>
  </page>
  <page>
    <title>Real-time web</title>
    <ns>0</ns>
    <id>23231423</id>
    <revision>
      <id>754600281</id>
      <parentid>754406310</parentid>
      <timestamp>2016-12-13T14:46:59Z</timestamp>
      <contributor>
        <ip>2001:B07:6456:7053:4951:B96E:54D1:2317</ip>
      </contributor>
      <comment>/* True-realtime web (an "alternate" model) */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6204" xml:space="preserve">{{multiple issues|
{{more footnotes|date=November 2010}}
{{refimprove|date=November 2010}}
{{essay|date=April 2015}}
{{Buzzword|date=July 2011}}xxdxxx
}}

The '''real-time web''' is a network web using technologies and practices that enable users to receive information as soon as it is published by its authors, rather than requiring that they or their software check a source periodically for updates.

==Difference from real-time computing==
The real-time web is fundamentally different from [[real-time computing]] since there is no knowing when, or if, a response will be received. The information types transmitted this way are often short messages, status updates, news alerts, or links to longer documents. The content is often "soft" in that it is based on the [[social web]]—people's opinions, attitudes, thoughts, and interests—as opposed to hard news or facts.

==(Old) True-realtime web (an "alternate" model)==
From another point of view, the real-time web consists in making the client interface (or the web side; or the web layer) of a web application, to communicate continuously with the corresponding real-time server, during every user connection. As a fast pic of the client/server model, imagine each client object (each web module of the web [[GUI]] of an application) having its object class alive as a sub process (of its user session) in the server environment. In this scenario, the web is considered as the human entrance (interface) to the real-time environment: at each connected web URL, or Internet real-time zone, corresponds a different "front-end" web application. The real-time server acts as a [[logic network operating system]] for the programmable array of applications; handles the array of connected users for each application; attends for connections from real-world appliances and second level real-time servers. Applications behaviours and the intercommunication procedures between online services or applications, online users, and connected devices or appliances, are settled in the corresponding source code of each real-time service written in the real-time-interpreted programming language of the centric server.

As opposite to previous scenario, real-time web is exactly soft [[real-time computing]]: the round trip of a data ping-pong signal from the real-time server to the client must take about 1s (max) to be considered real-time and not to be annoying for humans (or users) during their connections.{{Citation needed|date=April 2016}} About the dispute between social web and real-time web, we can say real-time web is social by default and it is not true the contrary (WEB-r comes before Web 2.0). The WEB-r model is called [[true-realtime web]] to highlight the differences with the defective (de facto) model of real-time web generally perceived. From the industry point of view, this model of (general) real-time Internet can also be defined as [[electronic web]], that comes with the intrinsic meaning of not being limited to the web side of the Net, and with the direct reference to its server/rest-of-the-world perspective as a mechanism of a single clock.

==History==
Examples of real-time web are Facebook's newsfeed, and Twitter, implemented in social networking, search, and news sites. Benefits are said to include increased user engagement ("flow") and decreased server loads. In December 2009 real-time search facilities were added to [[Google Search]].&lt;ref&gt;{{cite web|url=http://googleblog.blogspot.com/2009/12/relevance-meets-real-time-web.html|title=Relevance meets the real-time web}}&lt;/ref&gt;

The absolutely first realtime web implementation worldwide have been the WIMS true-realtime server and its web apps in 2001-2011 (WIMS = Web Interactive Management System); based on the WEB-r model of above; built in Java (serverside) and Adobe Flash (clientside). The true-realtime web model was born in 2000 at mc2labs.net by an Italian independent researcher.

==Real-time search==
A problem created by the rapid pace and huge volume of information created by real-time web technologies and practices is finding relevant information. One approach, known as '''real-time search''', is the concept of searching for and finding information online as it is produced. Advancements in web search technology coupled with growing use of [[social media]] enable online activities to be queried as they occur. A traditional [[web search]] [[Web crawler|crawls]] and [[Index (search engine)|indexes]] web pages periodically, returning results based on relevance to the search query. [[Google Real-Time Search]] was available in [[Google Search]] until July 2011.

==See also==
*[[Comet (programming)|Comet]]
*[[Collaborative real-time editor]]
*[[Firebase]]
*[[Internet of Things|Internet of Things (IoT)]]
*[[Meteor (web framework)|Meteor]]
*[[Microblogging]]
*[[Node.js]]
*[[Prospective search]]
*[[PubNub]]
*[[Push technology|Push Technology]]
*[[Scoopler]]
*[[Vert.x]]
*[https://www.syncano.io Syncano]

==References==
&lt;references /&gt;

==External links==
*{{Cite news|url=https://www.theguardian.com/business/2009/may/19/google-twitter-partnership|title=Google 'falling behind Twitter'|last=Wray|first=Richard|date=19 May 2009|work=The Guardian|accessdate=17 June 2009}}
*{{Cite news|url=http://www.nytimes.com/2009/06/14/business/14digi.html|title=Hey, Just a Minute (or Why Google Isn't Twitter)|last=Stross|first=Randall|date=13 June 2009 |work=New York Times|accessdate=17 June 2009}}
*{{Cite news|url=http://online.wsj.com/article/BT-CO-20090615-712397.html |title=Internet Giants Look For Edge In Real-Time Search |last=Morrison |first=Scott |date=15 June 2009 |work=Wall Street Journal |accessdate=17 June 2009 |deadurl=yes |archiveurl=https://web.archive.org/web/20090616204058/http://online.wsj.com/article/BT-CO-20090615-712397.html |archivedate=16 June 2009 }} 
*{{Cite news|url=http://www.readwriteweb.com/archives/explaining_the_real-time_web_in_100_words_or_less.php|title=Explaining the Real-Time Web in 100 Words or Less|last=Kirkpatrick|first=Marshall|date=22 September 2009|work=ReadWriteWeb}}

{{Use dmy dates|date=October 2010}}

{{DEFAULTSORT:Real-Time Web}}
[[Category:Internet search]]
[[Category:Real-time web| ]]</text>
      <sha1>iqbtglqgxrmmg8sqz09qz1n2ordvyiv</sha1>
    </revision>
  </page>
  <page>
    <title>Instant indexing</title>
    <ns>0</ns>
    <id>6111052</id>
    <revision>
      <id>752691331</id>
      <parentid>752690705</parentid>
      <timestamp>2016-12-02T19:06:28Z</timestamp>
      <contributor>
        <username>Kuru</username>
        <id>764407</id>
      </contributor>
      <comment>rmv spam link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3251" xml:space="preserve">{{Orphan|date=February 2009}}

'''Instant indexing''' is a feature offered by [[Internet]] [[search engine]]s that enables users to submit content for immediate inclusion into the [[search engine indexing|index]].

==Delayed inclusion==
Certain search engine services may require an extended period of time for inclusion, which is seen as a delay and a frustration by [[website]] administrators who wish to have their websites appear in [[search engine results page|search engine results]].&lt;ref&gt;{{cite web|url=https://www.youtube.com/watch?v=HBDC35Vgj34|title=How to index your domains|accessdate=2015-12-24}}&lt;/ref&gt;

Delayed inclusion may due to the size of the index that the service must maintain or due to corporate, political or social policies{{Citation needed|date=February 2007}}. Some services, such as [[Ask.com]] only index content collected by a [[web crawling|crawler program]] which does not allow for manual adding of content to index.&lt;ref&gt;{{cite web|url=https://www.smartz.com/web-marketing/search-engine-optimization/submit-site-to-search-engines/|title=How to Submit Your Site to Search Engines|accessdate=2015-12-24}}&lt;/ref&gt;

==Criticisms==
A criticism of instant indexing is that certain services filter results manually or via algorithms that prevent instant inclusion to avoid inclusion of content that violates the service's policies.{{Citation needed|date=February 2007}}

Instant indexing impacts the timeliness of the content included in the index. Given the manner in which many [[web crawling|crawlers]] operate in the case of Internet search engines, websites are only visited if a some other website links to them. Unlinked web sites are never visited (see [[invisible web]]) by the crawler because it cannot reach the website during its traversal. It is assumed that unlinked websites are less authoritative and less popular, and therefore of less quality. Over time, if a website is popular or authoritative, it is assumed that other websites will eventually link to it. If a search engine service provides instant indexing, it bypasses this quality control mechanism by not requiring incoming links. This infers that the search engine's service produces lower quality results.

Select search services that offer such a service typically also offer [[paid inclusion]], also referred to as [[pay per click|inorganic search]]. This may reduce the quality of search results.

==External links==
* {{cite web | url = http://www.web-cite.com/search_marketing/000078.html | title = Don't Blink: Instant Indexing? | publisher = Web-Cite Exposure | date = 2003-03-26 | accessdate = 2006-09-23 |archiveurl = http://web.archive.org/web/20060427184004/http://www.web-cite.com/search_marketing/000078.html &lt;!-- Bot retrieved archive --&gt; |archivedate = 2006-04-27}}
* {{cite web | url = http://www.earthstation9.com/index.html?us_searc.htm | title = The Wonderful World of Search Engines and Web Directories — A Search Engine Guide | author = Stan Daniloski | publisher = Earth Station 9 | date = 2004-09-17 | accessdate = 2006-09-23}}

== See also ==
* [[Search engine]]
* [[Search engine indexing]]
* [[Web crawling]]

== References ==
{{Reflist}}

[[Category:Internet terminology]]
[[Category:Internet search]]


{{website-stub}}</text>
      <sha1>ewd9tyywi119s3j74xqawnx6qspy2no</sha1>
    </revision>
  </page>
  <page>
    <title>ZyLAB Technologies</title>
    <ns>0</ns>
    <id>2744940</id>
    <revision>
      <id>738080704</id>
      <parentid>730924932</parentid>
      <timestamp>2016-09-06T20:07:44Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor />
      <comment>[[User:Green Cardamom/WaybackMedic 2|WaybackMedic 2]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="12984" xml:space="preserve">{{advert|date=April 2012}}
{{Infobox company |
  name   = ZyLAB |
  logo   = &lt;!--  Commented out because image was deleted: [[Image:zylab logo.jpg|center]] --&gt; |
  slogan = "eDiscovery &amp; Information Risk Management" |
  type   = Private |
  foundation     = 1983 |
  location       = [[McLean, Virginia]]&lt;br&gt;[[Amsterdam]] |
  key_people     = [[Pieter Varkevisser]], president &amp; CEO&lt;br&gt;[[Dr. Johannes C. Scholtes]], chairman &amp; chief strategy officer | Mary Mack, Enterprise Technology Counsel
  num_employees  = 140 |
  industry       = [[Software]], eDiscovery and Information Risk Management, Records Management, Email Archiving, SharePoint Archiving |
  products       = ZyLAB Information Management Platform and various bundles for eDiscovery, email &amp; SharePoint archiving, text-analytics, visualization, contract management, and workflow. |

  homepage       = [http://www.zylab.com/ www.zylab.com]
}}

'''ZyLAB''' is a developer of software for [[Electronic discovery|e-discovery]], information risk management, email management, records, contract, and document management, knowledge management, and workflow. The company is headquartered in [[McLean, Virginia]] and in [[Amsterdam]], [[Netherlands]]. ZyLAB’s most important products are ZyLAB eDiscovery &amp; Production System, the ZyLAB Information Management Platform and bundles that build systems for deployments.

== History ==
In 1983 ZyLAB was the first company providing a [[Full text search|full-text]] search program for electronic files stored in the file system of [[IBM PC compatible|IBM-compatible PCs]]. The program was called ZyINDEX. The first version of ZyINDEX was written in [[Pascal (programming language)|Pascal]] and worked on [[MS-DOS]]. Subsequent programs were written in [[C (programming language)|C]], [[C++]] and [[C Sharp (programming language)|C#]] and work on a variety of Microsoft operating systems.

In 1991, ZyLAB integrated ZyINDEX with an optical character recognition ([[Optical character recognition|OCR]]) program, Calera Wordscan, which was a spin-off from [[Raymond Kurzweil]]’s first OCR implementation. This integration was called ZyIMAGE. ZyIMAGE was the first PC program to include a [[Fuzzy string searching|fuzzy string search]] algorithm to overcome scanning and OCR errors.

In 1998, the company developed support to full-text search email, including attachments.

In 2000, ZyLAB embraced the new [[XML]] standard and created a full content management and records management system based on the XML standard and build a full solution for e-discovery, historical archives, records management, document management, email archiving, contract management, and professional back-office solutions.

In 2003, the company invested in expanding the ZyIMAGE product suite with advanced [[text analytics]], [[text mining]], [[data visualization]], [[computational linguistics]], and [[Machine translation|automatic translation]].

2005: ZyIMAGE Information Access Platform was released, an integrated solution to address information access problems.

Platforms for ZyIMAGE e-Discovery and legal production, historical archiving, compliance, back-office records management and [[COMINT#COMINT|COMINT]] were launched in 2007.

2010: ZyLAB Information Management Platform was released, an integrated solution to address e-Discovery and information management problems.

==Customers==
Initial customers of ZyINDEX were organizations such as the [[FBI]] and other law enforcement agencies to investigate electronic data from seized PCs, the [[United States Navy|U.S. Navy]] for on-board manuals, and law firms around the world for [[Electronic discovery|e-Discovery]]. Over the years, ZyLAB received grants from the European Union (DG13).

Other well-known ZyLAB customers were [[O. J. Simpson murder case|O.J. Simpson's defense team]], war crime tribunals such as the [[trial of Slobodan Milošević]], the [[Special Court for Sierra Leone]], the [[Extraordinary Chambers in the Courts of Cambodia|UN-AKRT-ECCC Cambodia Khmer Rouge trials]] and the [[International Criminal Tribunal for Rwanda|Rwanda tribunal]]. In 2007, the U.S. [[Executive Office of the President of the United States|Executive Office of the President]] selected ZyLAB for email archiving, basically for its open XML structures, which is endorsed by organizations such as the [[National Archives and Records Administration]]. ZyLAB’s software was used for many other high-profile investigations such as the [[Oklahoma City bombing]].

Public websites also use the ZyLAB Webserver.

[[Gartner]] positioned ZyLAB in the "Leaders" quadrant in its 2007, 2008 and 2009 Magic Quadrant for Information Access Solutions, gave it a strong positive rating in its 2007, 2008 and 2009 e-Discovery Marketscope and a Positive Rating in its 2007 and 2008 Records Management MarketScope.

ZyLAB’s chief strategy officer, Dr. Johannes C. Scholtes, is professor in [[text mining]] at [[Maastricht University|the University of Maastricht]] faculty of Humanities and Sciences and director in the board of AIIM.

==System overview and compatibility==
According to the company’s website it delivers systems for deployments, product bundles and the core components is the ZyLAB Information Management platform include:

Systems:
*ZyLAB eDiscovery and Production
*ZyLAB Compliance and Litigation readiness
*ZyLAB Law Enforcement and Investigations
*ZyLAB Communications Intelligence
*ZyLAB Digital Print and Media Archiving
*ZyLAB Enterprise Information Management

Bundles:
*E-Mail Archiving Bundle
*Microsoft SharePoint Bundle
*Analytics Bundle
*eDiscovery EDRM Processing bundle
*DoD and Sox Compliant RMA Bundle
*TIFF Archiving and Production Bundle
*WebPublishing Bundle
*Commercial Publishing Bundle
*Business Process Automation Bundle
*Development and Integrators Bundle
*Scanning Bundle
*Digital Copier Bundle
*Professional Text Mining
*Machine translation

===Supported configurations===
*'''Server OS''': Windows 2003, Windows 2008
*'''Databases''': XML, MS SQL Server 2005, MS SQL Server 2008, Oracle 10g, Oracle 11g, mySQL
*'''Web Servers''': IIS
*'''Client OS''': Windows XP, Windows Vista, Windows 7
*'''Clustering''': Support for Active/Passive Failover.
*'''Authentication''': Active Directory, LDAP, XML, NTFS, IBM Tripoli.
*'''Virtualization''': VMware Infrastructure, VMware Workstation, VMware Server, VMware Fusion.

===Languages supported===
*'''Unicode'''. Support for documents in all languages.
*'''Internationalization'''. ZyLAB offers translated products for English, German, French, Dutch, Spanish, Italian, Danish, Swedish, Norwegian, Finnish, Portuguese, Arabic and [[Persian language|Persian]]. In addition to these languages, over 400 languages are supported by ZyLAB's recognition and full-text indexing technology, including all Western-European, Eastern European, Baltic, African, Asian and South American languages. ZyLAB's technical ability for broad language and character recognition enhances the accuracy of stored information searches and helps diminish the costs incurred by incorrect searches or text correction.

==Zy-IMAGE-nation Annual Conference==
The annual Zy-IMAGE-nation Conference is sponsored by ZyLAB. During this conference, seminars and interactive sessions from leading professionals about the advanced technologies and procedural enhancements that are driving new levels of operational efficiency in private and public sectors. The focus of the conference is on technologies that provide integrated capabilities for managing the accumulated knowledge of an organization, especially records and e-mail, as well as other business-critical processes. Related topics to be covered include best practices for e-discovery preparation and implementation, records management, email archiving, and knowledge management.

==See also==
* [[Electronic discovery|e-Discovery]]
* [[Optical character recognition|Optical Character Recognition (OCR)]]
* [[Document Imaging]]
* [[E-mail archiving|E-mail Archiving]]
* [[Knowledge Management]]
* [[Document management system|Document Management (System)]]
* [[Enterprise content management|Enterprise Content Management]]
* [[Records management|Records Management]]
* [[Contract management|Contract Management]]
* [[Workflow]]
* [[Text mining|Text Mining]]
* [[Text analytics|Text Analytics]]
* [[Machine translation|Automatic Machine Translation]]
* [[Data visualization|Data Visualization]]

==References==
{{Reflist}}
*[http://www.pcmag.com/encyclopedia_term/0,,t=zyindex&amp;i=55248,00.asp Definition of ZyINDEX] in [[PC Magazine|''PCMAG.com'']]'s encyclopedia
*[http://www.pcmag.com/encyclopedia_term/0,2542,t=ZyIMAGE&amp;i=55247,00.asp Definition of ZyIMAGE] in [[PC Magazine|''PCMAG.com'']]'s encyclopedia
*[https://web.archive.org/web/20090108050805/http://www.informationweek.com/777/knowledge.htm Review] of ZyImage 3.0 in ''[[InformationWeek]]''
*[http://www.accessmylibrary.com/coms2/summary_0286-9201794_ITM Mac version of ZyINDEX made its debut on Comdex]
*[http://query.nytimes.com/gst/fullpage.html?res=940DE6DA1730F93AA35751C0A96E948260 Review] of ZyINDEX in the ''[[New York Times]]''
*[https://web.archive.org/web/20090224230803/http://www.computerwoche.de/heftarchiv/1988/26/1155611/ Review] of ZyINDEX on ''Computerwoche.de'' (article in German)
*[https://web.archive.org/web/20110717125042/http://www.computerwoche.de/index.cfm?pid=2123&amp;pk=1096333 Review] of ZyIMAGE's webserver on ''Computerwoche.de'' (article in German)
*[http://nl.newsbank.com/nl-search/we/Archives?p_product=MH&amp;s_site=miami&amp;p_multi=MH&amp;p_theme=realcities&amp;p_action=search&amp;p_maxdocs=200&amp;p_topdoc=1&amp;p_text_direct-0=0EB367D56736E685&amp;p_field_direct-0=document_id&amp;p_perpage=10&amp;p_sort=YMD_date: Review] of ZyINDEX in the ''[[Miami Herald]]''
*[http://www.usdoj.gov/oig/special/0203/chapter3.htm ZyINDEX used in the Investigation of the Belated Production of Documents in the Oklahoma City Bombing Case]
*[http://www.fcw.com/print/6_31/news/70014-1.html Review]{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }} of ZyIMAGE on ''Federal Computer Week (FCW.com)''
*Zylab retrieval engine optimized for CD-ROM; Zylab, Progressive Technologies merge," Seybold Report on Desktop Publishing. vol. 8, No. 10, Jun. 6, 1994, p. 40.
*Knibbe, "ZyImage 2 boosts, OCR, batch duties," InfoWorld, vol. 15, Issue 51, Dec. 20, 1993, p.&amp;nbsp;20.
*Knibbe, "ZyImage 3.0 will facilitate distribution on CD-ROMs; Boasts integration with WordScan OCR software," InfoWorld, vol. 16, No. 38, Sep. 19, 1994, p.&amp;nbsp;22.
*Marshall, "Text retrieval alternatives: 10 more ways to pinpoint important information," Infoworld, vol. 14, No. 12, Mar. 23, 1992, pp.&amp;nbsp;88–89.
*Marshall, "ZyImage adds scanning access to ZyIndex," InfoWorld, vol. 16, No. 15, Apr. 11, 1994, pp.&amp;nbsp;73, 76, and 77.
*Marshall, "ZyImage is ZyIndex plus a scan interface integrated," InfoWorld. vol. 15, Issue 10, Mar. 8, 1993, p.&amp;nbsp;100.
*Marshall et al., "ZyIndex for Windows, Version 5.0," InfoWorld, v. 15, n. 21, May 1993, pp.&amp;nbsp;127, 129, 133 and 137.
*Simon, "ZyImage: A Winning Combination of OCR And Text Indexing," PC Magazine. vol. 12, No. 6, Mar. 30, 1993, p.&amp;nbsp;56.
*Rooney, "Text-retrieval veterans prepare Windows attack," PC Week, v. 9, n. 24, Jun. 1992, p.&amp;nbsp;46.
*Rooney, "ZyLab partners with Calera: firms roll out document-image system," PC Week, vol. 10, No. 3, Jan. 25, 1993, p.&amp;nbsp;22.
*Torgan, "ZyImage: Document Imaging and Retrieval System," PC Magazine. vol. 12, No. 3, Feb. 9, 1993, p.&amp;nbsp;62.

===Gartner reports===
*Introduction to Investigative Case Management Products (18 April 2007)
*Hype Cycle for Legal and Regulatory Information Governance, 2007 (16 July 2007)
*MarketScope for Contract Management, 2007 (16 July 2007)
*Choosing an E-Discovery Solution in 2007 and 2008 (18 July 2007)
*Magic Quadrant for Information Access Technology, 2007 (5 September 2007)
*Magic Quadrant for Information Access Technology, 2008
*Magic Quadrant for Information Access Technology, 2009
*The Expanding Enterprise E-Discovery Marketplace (12 November 2007)
*MarketScope for E-Discovery and Litigation Support Vendors, 2007 (14 December 2007)
*MarketScope for E-Discovery Product Vendors, 2008
*MarketScope for E-Discovery Product Vendors, 2009
*MarketScope for Records Management (20 May 2008)
*Hype Cycle for Content Management, 2008 (8 July 2008)
*Using the Electronic Discovery Reference Model to Identify, Collect and Preserve Digital Evidence (11 July 2008)
*Using the Electronic Discovery Reference Model to Process, Review and Analyze Digital Evidence (11 July 2008)
*Hype Cycle for Governance, Risk and Compliance Technologies, 2009 (17 July 2009)

==External links==
*[http://www.zylab.com/ ZyLAB official website]
*[http://www.edrm.net/ The Electronic Discovery Reference Model (EDRM)]
*[http://www.aiim.org/ AIIM]

[[Category:Companies established in 1983]]
[[Category:Software companies of the United States]]
[[Category:Information retrieval organizations]]</text>
      <sha1>i3tm9qkkrzbida5kexd39lhnhywfo8s</sha1>
    </revision>
  </page>
  <page>
    <title>European Conference on Information Retrieval</title>
    <ns>0</ns>
    <id>10328235</id>
    <revision>
      <id>696281305</id>
      <parentid>673371570</parentid>
      <timestamp>2015-12-22T03:03:09Z</timestamp>
      <contributor>
        <username>Jfoley-cs</username>
        <id>20336486</id>
      </contributor>
      <comment>Update to consolidate past/present and update to current list of locations.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3234" xml:space="preserve">The '''European Conference on Information Retrieval''' (ECIR) is the main 
European research conference for the presentation of new results in the field of [[information retrieval]] (IR).
It is organized by the [[Information Retrieval Specialist Group]] of the [[British Computer Society]] (BCS-IRSG).
      
The event started its life as the ''Annual Colloquium on Information Retrieval Research'' in 1978 and was 
held in the UK each year until 1998 when it was hosted in Grenoble, France. Since then the venue has
alternated between the United Kingdom and continental Europe. To mark the metamorphosis
from a small informal colloquium to a major event in the IR research calendar, the 
BCS-IRSG later renamed the event to ''European Conference on Information Retrieval''. In recent years,
ECIR has continued to grow and has become the major European forum for the discussion
of research in the field of Information Retrieval.

Some of the topics dealt with include:
* IR models, techniques, and algorithms
* IR applications
* IR system architectures
* Test and evaluation methods for IR
* [[Natural Language Processing]] for IR
* Distributed IR
* Multimedia and cross-media IR

==Time and Location==

Traditionally, the ECIR is held in Spring, near the Easter weekend. A list of locations and planned venues are presented below.

* [[Padova, Italy]], 2016 [http://ecir2016.dei.unipd.it/]
* [[Vienna, Austria]], 2015 [http://www.ecir2015.org/]
* [[Amsterdam, Netherlands]], 2014 [http://ecir2014.org/]
* [[Moscow, Russia]], 2013 [http://ecir2013.org/]
* [[Barcelona, Spain]], 2012 [http://ecir2012.upf.edu/]
* [[Dublin, Ireland]], 2011 [http://www.ecir2011.dcu.ie/]
* [[Milton Keynes]], 2010 [http://kmi.open.ac.uk/events/ecir2010/]
* [[Toulouse]], 2009 [http://ecir09.irit.fr/]
* [[Glasgow]], 2008 [http://ecir2008.dcs.gla.ac.uk/]
* [[Rome]], 2007 [http://ecir2007.fub.it/]
* [[London]], 2006 [http://ecir2006.soi.city.ac.uk/]
* [[Santiago de Compostela|Santiago]], 2005 [http://www-gsi.dec.usc.es/ecir05/]
* [[Sunderland, Tyne and Wear|Sunderland]], 2004 [http://ecir04.sunderland.ac.uk/]
* [[Pisa]], 2003 [http://ecir03.isti.cnr.it/]
* [[Glasgow]], 2002 [http://irsg.bcs.org/past_ecir.php]*
* [[Darmstadt]], 2001* (organized by GMD)
* [[Cambridge]], 2000* (organized by Microsoft Research)
* [[Glasgow]], 1999*
* [[Grenoble]], 1998*
* [[Aberdeen, Scotland|Aberdeen]], 1997*
* [[Manchester]], 1996*
* [[Crewe]], 1995* (organized by Manchester Metropolitan University)
* [[Drymen]], Scotland, 1994* (organized by Strathclyde University)
* [[Glasgow]], 1993* (organized by Strathclyde University)
* [[Lancaster, Lancashire|Lancaster]], 1992*
* [[Lancaster, Lancashire|Lancaster]], 1991*
* [[Huddersfield]], 1990*
* [[Huddersfield]], 1989*
* [[Huddersfield]], 1988*
* [[Glasgow]], 1987*
* [[Glasgow]], 1986*
* [[Bradford]], 1985*
* [[Bradford]], 1984*
* [[Sheffield]], 1983*
* [[Sheffield]], 1982*
* [[Birmingham]], 1981*
* [[Leeds]], 1980*
* [[Leeds]], 1979*

&lt;br /&gt; *as the Annual Colloquium on Information Retrieval Research

==External links==
* [http://irsg.bcs.org/ecir.php Official page at the website of the British Computer Society]

[[Category:Information retrieval organizations]]
[[Category:Computer science conferences]]</text>
      <sha1>riqu9nbt26tklccsku5oewp3cs1l4qn</sha1>
    </revision>
  </page>
  <page>
    <title>Coveo</title>
    <ns>0</ns>
    <id>16001013</id>
    <revision>
      <id>747841320</id>
      <parentid>701612031</parentid>
      <timestamp>2016-11-04T17:59:10Z</timestamp>
      <contributor>
        <ip>38.104.140.214</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3058" xml:space="preserve">{{Infobox company
| name = Coveo Solutions Inc.
| logo = [[Image:Coveo logo.png|120px]]
| type = Private
| slogan = 
| foundation =  2005
| location_city = [[Quebec City]], [[Canada]]
| key_people = Louis Têtu, Chairman and CEO &lt;br /&gt;Laurent Simoneau, President and CTO
| num_employees =200+
| industry = [[Enterprise search]]
| products = Coveo Search &amp; Relevance Platform,&lt;br /&gt;Coveo for Sitecore,&lt;br /&gt;Coveo for Salesforce
| homepage = http://www.coveo.com
}}

'''Coveo''' is a provider of [[enterprise search]] and website search technologies, with integrated plug-ins for [[Salesforce.com]], Sitecore CEP, and [[Microsoft Outlook]] and [[SharePoint]].  APIs also allow for custom integration with other applications.

==History==
Coveo Solutions Inc. was founded in 2005 as a spin-off of [[Copernic|Copernic Technologies Inc.]] Laurent Simoneau, Coveo's president and chief executive officer was formerly Copernic's chief operating officer. About 30 employees moved into the new company, with offices at that time in [[Quebec City]] and [[Montreal]] in Canada and in [[Palo Alto]], Calif.&lt;ref&gt;http://www.eweek.com/c/a/Enterprise-Applications/Copernic-Ready-to-Take-On-Google-In-Enterprise-Search-Product/&lt;/ref&gt;

==Products==
'''Coveo Search &amp; Relevance Platform'''

Coveo Search &amp; Relevance Platform is a modular enterprise search technology that can index information stored in diverse repositories throughout the company, perform text analytics and metadata enrichment on the indexed content, and make the content findable through search-driven interfaces.

'''Coveo for Sitecore'''

Coveo for Sitecore is an integrated website search product to be used in conjunction with Sitecore’s Customer Experience Platform.  The product enables the unified indexing of multiple repositories, contextual search, and search management via the Sitecore console.

'''Coveo for Salesforce'''

Coveo for Salesforce is an integrated CRM search product to be used in conjunction with Salesforce.com Service Cloud and Communities Editions.  The product enables the unified indexing of multiple repositories, contextual search, and search management via the Salesforce console.

==Customers==
Coveo claims its clients include more than 700 implementations including AmerisourceBergen, CA, California Water Service Co., Deloitte, ESPN, Haley &amp; Aldrich, GEICO, Lockheed Martin, P&amp;G, PRTM, PricewaterhouseCoopers, Rabobank, SNC-Lavalin, Spencer Stuart, Theodoor Gilissen, and the U.S. Navy.&lt;ref&gt;{{cite web|url=http://www.coveo.com/en/~/media/Files/about-us/Coveo-Corporate-Fact-Sheet-Q109.ashx |title=Coveo corporate fact sheet |date= |accessdate=2011-02-27}}&lt;/ref&gt; These companies were also mentioned while not confirmed by a citation: HP, PwC, Netezza Corporation, NATO, NASA, AC Nielsen, among many others.{{Citation needed|date=February 2010}}

==References==
{{reflist}}

==External links==
* [http://www.coveo.com/ Coveo.com]

[[Category:Companies based in Quebec City]]
[[Category:Information retrieval organizations]]
[[Category:BlackBerry development software]]</text>
      <sha1>gia8fu1hkzihglmcfe8koywrlqm7vvn</sha1>
    </revision>
  </page>
  <page>
    <title>Concept Searching Limited</title>
    <ns>0</ns>
    <id>17770654</id>
    <revision>
      <id>730991874</id>
      <parentid>682544518</parentid>
      <timestamp>2016-07-22T05:43:12Z</timestamp>
      <contributor>
        <username>TAnthony</username>
        <id>1808194</id>
      </contributor>
      <comment>/* top */USA is deprecated, per [[MOS:NOTUSA]], and correct [[MOS:OVERLINK|overlinking]] of common places using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3364" xml:space="preserve">{{Infobox company |
  name   = Concept Searching Limited |
  logo = [[Image:conceptSearching.jpg]] |
  slogan = "Retrieval Just Got Smarter" |
  type   =  [[Privately held company|Private]] |
  foundation     = 2002|
  location       = UK, United States |
  area_served    = Global |
  industry       = [[Information retrieval]] |
  products       = conceptSearch&lt;br/&gt;conceptClassifier&lt;br/&gt;conceptClassifier for SharePoint&lt;br/&gt;conceptClassifier for SharePoint Online&lt;br/&gt;Taxonomy Manager&lt;br/&gt;Taxonomy Workflow |
  homepage       = [http://www.conceptsearching.com/ www.conceptsearching.com]
}}

'''Concept Searching Limited''' is a [[software company]] which specializes in [[information retrieval]] software. It has products for [[Enterprise search]], Taxonomy Management and  [[Statistical classification]].

==History==
Concept Searching was founded in 2002 in the UK and now has offices in the USA and South Africa. In August 2003 the company introduced the idea of using [[Compound term processing]].&lt;ref&gt;[http://direct.bl.uk/bld/PlaceOrder.do?UIN=138451913&amp;ETOC=RN Lateral thinking in information retrieval] ''Information Management and Technology.'' 2003. vol 36; part 4, pp 169-173&lt;/ref&gt;&lt;ref&gt;[http://www.conceptsearching.com/Web/UserFiles/File/Concept%20Searching%20Lateral%20Thinking.pdf] Lateral Thinking in Information Retrieval&lt;/ref&gt;

Compound term processing allows statistical information retrieval applications to perform matching using multi-word concepts. This can improve the quality of search results and also allows unstructured information to be automatically classified with semantic metadata.&lt;ref&gt;[http://airforcemedicine.afms.mil/711hswom/InterSymp2008/AFMS%20-%20InterSymp%202008.html] US Air Force Medical Service presentation at InterSymp-2008&lt;/ref&gt;

The company's products run on the Microsoft [[.NET Framework|.NET]] platform. The products integrate with Microsoft [[SharePoint]] and many other platforms.&lt;ref&gt;[http://pinpoint.microsoft.com/en-US/partners/Concept-Searching-Inc-4297066101] Microsoft Partner Profile&lt;/ref&gt;

Concept Searching has developed the '''Smart Content Framework''', which is a toolset that provides an enterprise framework to mitigate risk, automate processes, manage information, protect privacy, and address compliance issues. The Smart Content Framework is used by many large organizations including 23,000 users at the [[NASA]] Safety Center &lt;ref&gt;[http://www.aiim.org/About/News/CS-NASA-Safety] NASA Safety Center using Smart Content Framework&lt;/ref&gt;

== Awards ==
* 100 Companies that Matter in Knowledge Management 2009/2010/2011/2012/2013/2014/2015 &lt;ref&gt;{{cite web |url=http://www.kmworld.com/Articles/Editorial/Features/KMWorld-100-COMPANIES-That-Matter-in-Knowledge-Management-102189.aspx |title=KMWorld Magazine}}&lt;/ref&gt;
* KMWorld Trend-Setting Products of 2009/2010/2011/2012/2013/2014/2015 &lt;ref&gt;{{cite web |url=http://www.kmworld.com/Articles/Editorial/Features/KMWorld-Trend-Setting-Products-of-2015-105783.aspx |title=Trend-Setting Products}}&lt;/ref&gt;

==See also==
* [[Compound term processing]]
* [[Enterprise search]]
* [[Full text search]]
* [[Information retrieval]]
* [[Concept Search]]

==References==
{{Reflist}}

==External links==
*[http://www.conceptsearching.com/ Company Website]

[[Category:Information retrieval organizations]]
[[Category:Privately held companies of the United Kingdom]]</text>
      <sha1>2i28kjk6e5spyzo3hdu0xfd651m4t0h</sha1>
    </revision>
  </page>
  <page>
    <title>Conference and Labs of the Evaluation Forum</title>
    <ns>0</ns>
    <id>27511028</id>
    <revision>
      <id>752813387</id>
      <parentid>748808507</parentid>
      <timestamp>2016-12-03T13:46:20Z</timestamp>
      <contributor>
        <username>Varepsilon i</username>
        <id>24584483</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2436" xml:space="preserve">The '''Conference and Labs of the Evaluation Forum''' (formerly '''Cross-Language Evaluation Forum'''), or '''CLEF''', is an organization promoting research in multilingual [[information access]] (currently focusing on [[European Commissioner for Multilingualism|European languages]]). Its specific functions are to maintain an underlying framework for testing [[information retrieval]] systems and to create [[digital library|repositories]] of data for researchers to use in developing  comparable [[Technical standard|standards]].&lt;ref name="Peters"&gt;{{cite conference | first1 = Carol | last1 = Peters| first2 = Martin | last2 = Braschler | first3 = Khalid | last3 = Choukri | first4 = Julio | last4 = Gonzalo | first5 = Michael | last5 = Kluck | title = The Future of Evaluation for Cross-Language Information Retrieval Systems | conference = Second Workshop of the Cross-Language Evaluation Forum, CLEF 2001 | citeseerx = 10.1.1.109.7647 }}&lt;/ref&gt;
The organization holds a forum meeting   every September in Europe. Prior to each forum, participants receive a set of challenge tasks. The tasks  are designed to test various aspects of information retrieval systems and encourage their development. Groups of researchers propose and organize campaigns to satisfy those tasks. The results are used as [[benchmark (computing)|benchmarks]] for the state of the art  in the specific areas.,&lt;ref&gt;{{cite journal | url = http://www.springerlink.com/content/l7v0354471u53385/ | title = Special Issue on CLEF | journal = Information Retrieval | volume = 7 | issue = 1–2 | year = 2004 }}&lt;/ref&gt;&lt;ref&gt;Fredric C. Gey, Noriko Kando, and Carol Peters "Cross-Language Information Retrieval: the way ahead" in ''Information Processing &amp; Management''
vol. 41, no. 3,  p.415-431 May 2005, {{doi|10.1016/j.ipm.2004.06.006}}&lt;/ref&gt;

For example, the 2010 medical retrieval task focuses on retrieval of computed tomography,  MRI, and radiographic images.&lt;ref name="ImageCLEFmed"&gt;{{cite web | last = Mueller| first = Henning| authorlink = | coauthors = | title = Medical Retrieval Task| work = | publisher =ImageCLEF - Cross-language image retrieval evaluations | date = 20 May 2010| url =http://www.imageclef.org/2010/medical | format = | doi = | accessdate = 27 May 2010 }}&lt;/ref&gt;

==References==
{{reflist}}

== External links ==
* [http://www.clef-initiative.eu/ CLEF homepage]

[[Category:Information retrieval organizations]]


{{Compu-conference-stub}}</text>
      <sha1>ponqlkrm70l24e2xwncg80qoqirbsdp</sha1>
    </revision>
  </page>
  <page>
    <title>Information Retrieval Specialist Group</title>
    <ns>0</ns>
    <id>10218640</id>
    <revision>
      <id>667009502</id>
      <parentid>666857473</parentid>
      <timestamp>2015-06-15T06:23:15Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor />
      <comment>/* External links */Removed invisible unicode characters + other fixes, removed: ‎ using [[Project:AWB|AWB]] (11140)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1466" xml:space="preserve">{{Unreferenced|date=January 2010}}

The '''Information Retrieval Specialist Group''' ('''IRSG''') or '''BCS-IRSG''' is a Specialist Group of the [[British Computer Society]] concerned with supporting communication between researchers and practitioners, promoting the use of [[Information Retrieval]] (IR) methods in industry and raising public awareness. There is a newsletter called ''The Informer'', an annual European Conference (ECIR), and continual organisation and sponsorship of conferences, workshops and seminars. The current chair is Dr. Andy MacFarlane.{{Citation needed|date=January 2010}}

==European Conference on Information Retrieval==
Organising [[European Conference on Information Retrieval|ECIR]] is one of the major activities of the Information Retrieval Specialist Group. The conference began in 1979 and has grown to become one of the major Information Retrieval conferences alongside [[Special Interest Group on Information Retrieval|SIGIR]] receiving hundreds of paper and poster submissions every year from around the world.{{Citation needed|date=January 2010}} ECIR was initially established by the IRSG under the name "Annual Colloquium on Information Retrieval Research", and held in the UK until 1997. It was renamed ECIR in 2003 to better reflect its status as an international conference.

== External links ==
* [http://irsg.bcs.org/ IRSG website]

[[Category:Information retrieval organizations]]
[[Category:BCS Specialist Groups]]</text>
      <sha1>089oclkbv3h674f2hsenmea3841d5ef</sha1>
    </revision>
  </page>
  <page>
    <title>Special Interest Group on Information Retrieval</title>
    <ns>0</ns>
    <id>14109784</id>
    <revision>
      <id>747323389</id>
      <parentid>733168913</parentid>
      <timestamp>2016-11-01T18:57:33Z</timestamp>
      <contributor>
        <ip>50.53.1.33</ip>
      </contributor>
      <comment>/* Conferences */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3418" xml:space="preserve">{{Infobox organization
|name           = ACM Special Interest Group on Information Retrieval
|image          = sig-information-retrieval-logo.png
|size           = 140px
|alt            = ACM SIGIR
|parent_organization = [[Association for Computing Machinery]]
|website        = {{URL|sigir.org}}
}}

'''SIGIR''' is the [[Association for Computing Machinery]]'s '''Special Interest Group on Information Retrieval'''. The scope of the group's specialty is the theory and application of computers to the acquisition, organization, storage, [[Information retrieval|retrieval]] and distribution of information; emphasis is placed on working with non-numeric information, ranging from natural language to highly structured data bases.

== Conferences ==
The annual international SIGIR conference, which began in 1978, is considered the most important in the field of information retrieval. SIGIR also sponsors the annual [[Joint Conference on Digital Libraries]] (JCDL) in association with [[ACM SIGWEB|SIGWEB]], the [[Conference on Information and Knowledge Management]] (CIKM), and the [[International Conference on Web Search and Data Mining]] (WSDM) in association with [[SIGKDD]], [[SIGMOD]], and [[ACM SIGWEB|SIGWEB]].

=== SIGIR conference locations ===
{| class="wikitable" border="1"
|-
! Number
! Year
! Location
|-
| 22
| 1999
| [[Berkeley, California]]
|-
| 23
| 2000
| [[Athens]]
|-
| 24
| 2001
| [[New Orleans]]
|-
| 25
| 2002
| [[Tampere]]
|-
| 26
| 2003
| [[Toronto]]
|-
| 27
| 2004
| [[Sheffield]]
|-
| 28
| 2005
| [[Salvador, Bahia]]
|-
| 29
| 2006
| [[Seattle]]
|-
| 30
| 2007
| [[Amsterdam]]
|-
| 31
| 2008
| [[Singapore]]
|-
| 32
| 2009
| [[Boston]]
|-
| 33
| 2010
| [[Geneva]]
|-
| 34
| 2011
| [[Beijing]]
|-
| 35
| 2012
| [[Portland, Oregon]]
|-
| 36
| 2013
| [[Dublin]]
|-
| 37
| 2014
| [[Gold Coast, Queensland]]
|-
| 38
| 2015
| [[Santiago]]
|-
| 39
| 2016
| [[Pisa]]
|-
| 40
| 2017
| [[Tokyo]]
|-
| 41
| 2018
| [[Ann Arbor]]
|}

== Awards ==
The group gives out several awards to contributions to the field of information retrieval. The most important award is the [[Gerard Salton Award]] (named after the computer scientist [[Gerard Salton]]), which is awarded every three years to an individual who has made "significant, sustained and continuing contributions to research in information retrieval". Additionally, SIGIR presents a Best Paper Award &lt;ref&gt;{{cite web | url=http://sigir.org/awards/awards.html#bestpaper | title=SIGIR Conference Best Paper Awards | accessdate=2012-08-29 }}&lt;/ref&gt; to recognize the highest quality paper at each conference. "Test of time" Award &lt;ref&gt;{{cite web | url=http://sigir.org/awards/test-of-time-awards/ | title=SIGIR Conference Test of Time Awards | accessdate=2015-12-29 }}&lt;/ref&gt; is a recent award that is given to a paper that  has had "long-lasting influence, including impact on a subarea of information retrieval research, across subareas of information retrieval research, and outside of the information retrieval research community". This award is selected from a set of full papers presented at the main SIGIR conference 10-12 years before.

==See also==
* [[Conference on Information and Knowledge Management]]

==References==

{{Reflist}}
==External links==
* {{official website|http://www.sigir.org/}}

{{Authority control}}

[[Category:Association for Computing Machinery Special Interest Groups]]
[[Category:Information retrieval organizations]]</text>
      <sha1>q0swjfk64i6o4j0sxgtfl53cfzj75wj</sha1>
    </revision>
  </page>
  <page>
    <title>Gerard Salton Award</title>
    <ns>0</ns>
    <id>1981660</id>
    <revision>
      <id>677373770</id>
      <parentid>675414620</parentid>
      <timestamp>2015-08-22T20:49:52Z</timestamp>
      <contributor>
        <username>Hiemstra</username>
        <id>5757030</id>
      </contributor>
      <minor />
      <comment>/* Chronological honorees and lectures */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2993" xml:space="preserve">The '''Gerard Salton Award''' is presented by the [[Association for Computing Machinery]] (ACM) [[Special Interest Group on Information Retrieval]] (SIGIR) every three years to an individual who has made "significant, sustained and continuing contributions to research in [[information retrieval]]". SIGIR also co-sponsors (with [[SIGWEB]]) the [[Vannevar Bush Award]], for the best paper at the [[Joint Conference on Digital Libraries]].

==Chronological honorees and lectures==
* 1983 - [[Gerard Salton]], [[Cornell University]] : "About the future of automatic information retrieval."
* 1988 - [[Karen Spärck Jones]], [[University of Cambridge]] : "A look back and a look forward."
* 1991 - [[Cyril Cleverdon]], [[Cranfield Institute of Technology]] : "The significance of the Cranfield tests on index languages."
* 1994 - William S. Cooper, [[University of California, Berkeley]] : "The formalism of probability theory in IR: a foundation or an encumbrance?"
* 1997 - [[Tefko Saracevic]], [[Rutgers University]] : "Users lost (summary): reflections on the past, future, and limits of information science." 
* 2000 - [[Stephen Robertson (computer scientist)|Stephen E. Robertson]], [[City University, London|City University London]] : "On theoretical argument in information retrieval."&lt;BR&gt;'''For ...''' ''"Thirty years of significant, sustained and continuing contributions to research in information retrieval. Of special importance are the theoretical and empirical contributions to the development, refinement, and evaluation of probabilistic models of information retrieval."''
* 2003 - [[W. Bruce Croft]], [[University of Massachusetts Amherst]] : "Information retrieval and computer science: an evolving relationship."&lt;BR&gt;'''For ...''' ''"More than twenty years of significant, sustained and continuing contributions to research in information retrieval. His contributions to the theoretical development and practical use of [[Bayesian inference]] networks and [[language modelling]] for retrieval, and to their evaluation through extensive experiment and application, are particularly important. The Center for Intelligent Information Retrieval which he founded illustrates the strong synergies between fundamental research and its application to a wide range of practical information management problems."''
* 2006 - [[C. J. van Rijsbergen]], [[University of Glasgow]] : 	"Quantum haystacks."
* 2009 - [[Susan Dumais]], [[Microsoft Research]] : "An Interdisciplinary Perspective on Information Retrieval."
* 2012 - [[Norbert Fuhr]], [[University of Duisburg-Essen]]: "Information Retrieval as Engineering Science."
* 2015 - [[Nicholas J. Belkin]], [[Rutgers University]]: “People, Interacting with Information”

==External links==
* [http://www.acm.org/sigir/ ACM SIGIR homepage]
* [http://www.sigir.org/awards/awards.html ACM SIGIR awards]

[[Category:Association for Computing Machinery]]
[[Category:Computer science awards]]
[[Category:Information retrieval organizations]]</text>
      <sha1>7p103c98c2iwzcil5sre072cnj75pqi</sha1>
    </revision>
  </page>
  <page>
    <title>Ness Computing</title>
    <ns>0</ns>
    <id>32567205</id>
    <revision>
      <id>762161003</id>
      <parentid>716722231</parentid>
      <timestamp>2017-01-27T01:24:41Z</timestamp>
      <contributor>
        <ip>2601:647:4D03:3CA7:351F:EDB6:72D7:FA03</ip>
      </contributor>
      <comment>clean up text and remove some marketing-y phrases</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1862" xml:space="preserve">{{Notability|Companies|date=July 2011}}

'''Ness Computing''' was a personal search company. It was acquired by OpenTable in March 2014 and was shut down later that year.&lt;ref&gt;{{cite web|last=Lunden|first=Ingrid|title=OpenTable Buys Ness For $17.3M|url=http://techcrunch.com/2014/02/06/opentable-ness/|work=TechCrunch|accessdate=26 March 2014}}&lt;/ref&gt; 

It was founded in October 2009 by Corey Reese,&lt;ref&gt;http://www.linkedin.com/in/coreyreese&lt;/ref&gt; Paul Twohey,&lt;ref&gt;http://www.linkedin.com/in/twohey&lt;/ref&gt; Nikhil Raghavan,&lt;ref&gt;http://www.linkedin.com/in/nikhilraghavan&lt;/ref&gt; and Steven Schlansker.&lt;ref&gt;http://www.linkedin.com/in/stevenschlansker&lt;/ref&gt; The company was headquartered in Los Altos, California.

Ness aimed to help people make decisions about dining, nightlife, entertainment, shopping, music, travel and more. The company referred to its technology as the "Likeness Engine", a combination of a [[recommendation engine]] that used [[machine learning]] to look at data from diverse sources and a traditional [[search engine]] that served up results based on these signals. 

The free Ness Dining App (for iPhone) was referred to as the [[Netflix]] &lt;ref&gt;http://eater.com/archives/2011/08/26/ness-iphone-app-recommends-restaurants-using-likeness-score.php&lt;/ref&gt; or [[Pandora Radio|Pandora]] &lt;ref&gt;http://gigaom.com/2011/08/25/ness-restaurant-app/&lt;/ref&gt; for restaurants. Based on a user's ratings and preferences, the service delivered recommendations for a particular time, location, price range, and cuisine preference. Users could view the menu for a place via SinglePlatform,&lt;ref&gt;http://www.singleplatform.com/&lt;/ref&gt; browse [[Instagram]] photos tagged at the restaurant, and make reservations in the app via [[OpenTable]].

==References==
{{Reflist}}

[[Category:Information retrieval organizations]]
[[Category:Software companies based in California]]</text>
      <sha1>2pwqy0zraygr7snf2btr953mb6emq85</sha1>
    </revision>
  </page>
  <page>
    <title>Datanet</title>
    <ns>0</ns>
    <id>13555870</id>
    <revision>
      <id>666920057</id>
      <parentid>666920031</parentid>
      <timestamp>2015-06-14T16:12:22Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>Category:Information retrieval organizations</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5892" xml:space="preserve">{{Use mdy dates|date=September 2011}}
'''DataNet''', or '''Sustainable Digital Data Preservation and Access Network Partner''' was a research program of the U.S. [[National Science Foundation]] Office of Cyberinfrastructure.  The office announced a request for proposals with this title on September 28, 2007.&lt;ref name="datanetprogram"&gt;{{cite web
|url=http://www.nsf.gov/funding/pgm_summ.jsp?pims_id=503141
|publisher=National Science Foundation
|title=Sustainable Digital Data Preservation and Access Network Partners (DataNet) Program Summary
|date=September 28, 2007
|accessdate=October 3, 2007
}}&lt;/ref&gt;  The lead paragraph of its synopsis describes the program as:

&lt;blockquote&gt;Science and engineering research and education are increasingly digital and increasingly data-intensive.  Digital data are not only the output of research but provide input to new hypotheses, enabling new scientific insights and driving innovation. Therein lies one of the major challenges of this scientific generation: how to develop the new methods, management structures and technologies to manage the diversity, size, and complexity of current and future data sets and data streams.  This solicitation addresses that challenge by creating a set of exemplar national and global data research infrastructure organizations (dubbed DataNet Partners) that provide unique opportunities to communities of researchers to advance science and/or engineering research and learning.&lt;/blockquote&gt;

The introduction in the solicitation&lt;ref name="datanetsolicitation"&gt;{{cite web
|url=http://www.nsf.gov/publications/pub_summ.jsp?ods_key=nsf07601
|publisher=National Science Foundation
|title=Sustainable Digital Data Preservation and Access Network Partners Program Announcements &amp; Information
|date=September 28, 2007
|accessdate=October 3, 2007
}}&lt;/ref&gt; goes on to say:

&lt;blockquote&gt;Chapter 3 (Data, Data Analysis, and Visualization) of [http://www.nsf.gov/pubs/2007/nsf0728/index.jsp NSF’s Cyberinfrastructure Vision for 21st century Discovery] presents a vision in which “science and engineering digital data are routinely deposited in well-documented form, are regularly and easily consulted and analyzed by specialists and non-specialists alike, are openly accessible while suitably protected, and are reliably preserved.” The goal of this solicitation is to catalyze the development of a system of science and engineering data collections that is open, extensible and evolvable.&lt;/blockquote&gt;

The initial plan called for a $100 million initiative: five awards of $20&amp;nbsp;million each over five years with the possibility of continuing funding.  Awards were given in two rounds. In the first round, for which  full proposals were due on March 21, 2008, two DataNet proposals were awarded. [[DataONE]],&lt;ref&gt;{{cite web|author=William Michener |url=https://www.dataone.org |title=DataONE: Observation Network for Earth |publisher=www.dataone.org | accessdate=2013-01-19|display-authors=etal}}&lt;/ref&gt; led by William Michener at the [[University of New Mexico]] covers ecology, evolutionary, and earth science. The Data Conservancy,&lt;ref&gt;{{cite web|author=Sayeed Choudhury |url=https://dataconservancy.org |title=Data Conservancy |publisher=dataconservancy.org | accessdate=2013-01-19|display-authors=etal}}&lt;/ref&gt; led by Sayeed Choudhury of [[Johns Hopkins University]], focuses on astronomy, earth science, life sciences, and social science. 

For the second round, preliminary proposals were due on October 6, 2008 and full proposals on February 16, 2009. Awards from the second round were greatly delayed, and funding was reduced substantially from $20 million per project to $8 million.&lt;ref&gt;{{cite web|author=National Science Foundation |url=http://www.nsf.gov/awardsearch/simpleSearchResult?queryText=%22datanet+full+proposal%3A%22 |title=NSF DataNet Awards |publisher=www.nsf.gov | accessdate=2013-01-19}}&lt;/ref&gt; Funding for three second round projects began in Fall 2011. SEAD: Sustainable Environment through Actionable Data,&lt;ref&gt;{{cite web|author=[[Margaret Hedstrom]] |url=http://sead-data.net/ |title=SEAD Sustainable Environment - Actionable Data |publisher=sead-data.net | accessdate=2013-01-19|display-authors=etal}}&lt;/ref&gt; led by [[Margaret Hedstrom]] of the [[University of Michigan]], seeks to provide data curation software and services for the "long tail" of small- and medium-scale data producers in the domain of sustainability science. The DataNet Federation Consortium,&lt;ref&gt;{{cite web|author=[[Reagan Moore]] |url=http://datafed.org/ |title=DataNet Federation Consortium |publisher=datafed.org | accessdate=2013-01-19|display-authors=etal}}&lt;/ref&gt; led by Reagan Moore of the [[University of North Carolina]], uses the integrated Rule-Oriented Data System (iRODS) to provide data grid infrastructure for science and engineering. ''Terra Populus'',&lt;ref&gt;{{cite web|author=[[Steven Ruggles]] |url=http://www.terrapop.org/ |title=Terra Populus: Integrated Data on Population and the Environment |publisher=terrapop.org | accessdate=2013-01-19|display-authors=etal}}&lt;/ref&gt; led by [[Steven Ruggles]] of the [[University of Minnesota]] focuses on tools for data integration across the domains of social science and environmental data, allowing interoperability of the three major data formats used in these domains: microdata, areal data, and raster data.

==References==
{{reflist|30em}}

==External links==
* [http://www.dataone.org DataONE]
* [http://dataconservancy.org/ Data Conservancy]
* [http://sead-data.net/ SEAD Sustainable Environment - Actionable Data]
* [http://datafed.org/ DataNet Federation Consortium]
* [http://www.terrapop.org/ Terra Populus: Integrated Data on Population and the Environment] 
 

[[Category:National Science Foundation]]
[[Category:Science and technology in the United States]]
[[Category:Information retrieval organizations]]
[[Category:Digital library projects]]</text>
      <sha1>t2bp6o8ah9knr94665fl7wyl00jpf09</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Alphabet Inc.</title>
    <ns>14</ns>
    <id>47562417</id>
    <revision>
      <id>742942805</id>
      <parentid>676714196</parentid>
      <timestamp>2016-10-06T19:59:55Z</timestamp>
      <contributor>
        <username>Look2See1</username>
        <id>11406674</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1323" xml:space="preserve">{{Commons category|Alphabet Inc.}}
{{Portal|Alphabet|Google}}
*'''[[Alphabet Inc.]]''' — an {{C|Multinational companies headquartered in the United States|American multinational}} {{C|Conglomerate companies of the United States|conglomerate company}} based in {{C|Mountain View, California|Mountain View}}, {{C|San Francisco Bay Area}}, {{C|California}}.
:::::*It is the parent corporation of {{C|Google}}; and other [[information technology]], investment, life sciences, and research companies.


{{clr}}
::{{Cat main|Alphabet Inc.}}

{{Alphabet Inc.|state=collapsed}}
{{Google Inc.}}

[[Category:Conglomerate companies of the United States]]
[[Category:Holding companies of the United States]]
[[Category:Multinational companies headquartered in the United States]]
[[Category:Technology companies of the United States]]
[[Category:Technology companies based in the San Francisco Bay Area]]
[[Category:Companies based in Mountain View, California]]
[[Category:Wikipedia categories named after conglomerate companies of the United States]]
[[Category:Wikipedia categories named after information technology companies of the United States]]











[[Category:Software companies based in the San Francisco Bay Area]]
[[Category:Information retrieval organizations]]
[[Category:Internet companies of the United States]]</text>
      <sha1>gvt4pncmua2caxiai9p7qh7p802g54q</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Waymo</title>
    <ns>14</ns>
    <id>52604208</id>
    <revision>
      <id>755169306</id>
      <timestamp>2016-12-16T16:58:58Z</timestamp>
      <contributor>
        <username>Zubairudalhatu</username>
        <id>6242228</id>
      </contributor>
      <comment>page created</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="331" xml:space="preserve">{{Commons category|Google}}
{{Cat main|Waymo}}

[[Category:Technology companies based in the San Francisco Bay Area]]
[[Category:Companies based in Mountain View, California]]
[[Category:Web portals]]
[[Category:Information retrieval organizations]]
[[Category:Alphabet Inc.]]
[[Category:Wikipedia categories named after websites]]</text>
      <sha1>1h3g2c57tshzw4d0t8scq18pzfnh9j0</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Search engine software</title>
    <ns>14</ns>
    <id>6521632</id>
    <revision>
      <id>666714595</id>
      <parentid>666705385</parentid>
      <timestamp>2015-06-13T03:50:19Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>-Category:Data search engines (redundant)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="138" xml:space="preserve">[[Category:Information retrieval systems]]
[[Category:Utility software by type]]
[[Category:Marketing software]]
[[Category:Web software]]</text>
      <sha1>7wag14eowzs4ou0ffskogmc97z143ne</sha1>
    </revision>
  </page>
  <page>
    <title>IBM Omnifind</title>
    <ns>0</ns>
    <id>13762814</id>
    <revision>
      <id>738672037</id>
      <parentid>706436098</parentid>
      <timestamp>2016-09-10T11:33:28Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor />
      <comment>[[User:Green Cardamom/WaybackMedic 2|WaybackMedic 2]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2380" xml:space="preserve">'''IBM OmniFind''' was an [[enterprise search]] platform from [[IBM]].
It did come in several packages adapted to different business needs, including OmniFind Enterprise Edition, OmniFind Enterprise Starter Edition, and OmniFind Discovery Edition.&lt;ref&gt;[http://www-01.ibm.com/software/ecm/omnifind/library.html IBM - OmniFind - Library]&lt;/ref&gt; IBM OmniFind as a standalone product was withdrawn in April 2011&lt;ref&gt;[http://www-01.ibm.com/common/ssi/cgi-bin/ssialias?subtype=ca&amp;infotype=an&amp;appname=iSource&amp;supplier=897&amp;letternum=ENUS911-075 IBM US Announcement Letter]&lt;/ref&gt; and is now part of [[IBM Watson Content Analytics with Enterprise Search]].&lt;ref&gt;[http://www-01.ibm.com/common/ssi/cgi-bin/ssialias?infotype=AN&amp;subtype=CA&amp;htmlfid=897/ENUS211-133 IBM US Announcement Letter]&lt;/ref&gt;

'''IBM OmniFind Yahoo! Edition''' was a free-of-charge version that could handle up to 500,000 documents in its index and was intended for small businesses. IBM OmniFind Yahoo! Edition was simple to install, provided a user friendly front end for administration, and incorporated technology from the open source [[Lucene]] project. IBM withdrew this product from marketing effective September 22, 2010 and withdrew support effective June 30, 2011.&lt;ref&gt;[http://www-01.ibm.com/common/ssi/cgi-bin/ssialias?subtype=ca&amp;infotype=an&amp;appname=iSource&amp;supplier=897&amp;letternum=ENUS910-115 IBM US Announcement Letter]&lt;/ref&gt;

'''IBM OmniFind Personal E-mail Search''' was a research product launched in 2007 for doing [[semantic search]] over personal emails by extracting and organizing concepts and relationships (such as phone numbers and addresses). The project appears to have been silently abounded sometimes around 2010.

== See also ==
* [[Languageware]]
* [[UIMA]]
* [[Comparison of enterprise search software]]
* [[List of enterprise search vendors]]

==External links==
* [http://www.ibm.com/software/data/enterprise-search/ IBM OmniFind]
* [http://omnifind.ibm.yahoo.com/ IBM OmniFind Yahoo! Edition] {{Dead link|date=May 2012}}
* [https://web.archive.org/web/20071030125647/http://www.alphaworks.ibm.com/tech/emailsearch IBM OmniFind Personal E-mail Search] 
* [http://www.opentestsearch.com/search-engines/ibm-omnifind-yahoo-edition-review/ Online demo and review of IBM OmniFind Yahoo! Edition]

==Notes==
{{reflist}}

[[Category:IBM software|OmniFind]]
[[Category:Information retrieval systems]]</text>
      <sha1>bd4rbvriv0xecc5unj62vxil1nowjbm</sha1>
    </revision>
  </page>
  <page>
    <title>Locate (Unix)</title>
    <ns>0</ns>
    <id>3522125</id>
    <revision>
      <id>731966698</id>
      <parentid>731966661</parentid>
      <timestamp>2016-07-28T18:03:48Z</timestamp>
      <contributor>
        <ip>128.40.9.123</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3283" xml:space="preserve">{{lowercase}}
'''&lt;code&gt;locate&lt;/code&gt;''' is a [[Unix]] utility which serves to find [[computer file|file]]s on [[filesystem]]s. It searches through a prebuilt [[database]] of files generated by the &lt;code&gt;updatedb&lt;/code&gt; command or by a [[Daemon (computing)|daemon]] and compressed using [[incremental encoding]]. It operates significantly faster than &lt;code&gt;[[find]]&lt;/code&gt;, but requires regular updating of the database. This sacrifices overall efficiency (because of the regular interrogation of filesystems even when no user needs information) and absolute accuracy (since the database does not update in [[Real-time computing|real time]]) for significant speed improvements, particularly on very large filesystems.

&lt;code&gt;locate&lt;/code&gt; was first created in 1982.&lt;ref&gt;{{cite magazine|last=Woods|first=James A.|date=1983-01-15|title=Finding Files Fast|url=https://archive.org/stream/login-feb83/login_feb83_issue#page/n9/mode/2up|magazine=[[;login:]]|volume=8|issue=1|pages=8–10|publisher=[[Usenix]]|access-date=2016-03-27}}&lt;/ref&gt;  The BSD and [[GNU Findutils]] versions derive from the original implementation.&lt;ref&gt;{{cite web|url=https://www.gnu.org/software/findutils/manual/html_node/find_html/Introduction.html#Introduction|title=Finding Files|date=2012-11-17|website=[[GNU]]|publisher=[[Free Software Foundation]]|access-date=2016-03-27|quote=GNU locate and its associated utilities were originally written by James Woods, with enhancements by David MacKenzie.}}&lt;/ref&gt;  Their primary database is world-readable, so the index is built as an unprivileged user.

&lt;code&gt;mlocate&lt;/code&gt; (Merging Locate) and the earlier &lt;code&gt;slocate&lt;/code&gt; (Secure Locate) use a restricted-access database, only showing filenames accessible to the user.&lt;ref&gt;{{cite web|url=http://carolina.mff.cuni.cz/~trmac/blog/mlocate/|archive-url=https://web.archive.org/web/20060411074142/http://carolina.mff.cuni.cz/~trmac/blog/mlocate/|archive-date=2006-04-11|title=mlocate|date=2005|author=Miloslav Trmač|access-date=2016-03-27|quote=...faster and does not trash the system caches as much...attempts to be compatible to GNU locate, when it does not conflict with slocate compatibility.|dead-url=yes}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.geekreview.org/slocate/|archive-url=https://web.archive.org/web/20050507092723/http://www.geekreview.org/slocate/|archive-date=2005-05-07|title=Secure Locate|date=1999|author=Kevin Lindsay|access-date=2016-03-27|quote=...will also check file permissions and ownership so that users will not see files they do not have access to.|dead-url=yes}}&lt;/ref&gt;

== References ==
{{reflist}}

== External links ==
* [https://www.gnu.org/software/findutils/findutils.html GNU Findutils]
* [https://fedorahosted.org/mlocate/ mlocate]
* {{man|1|locate|FreeBSD}}
* {{man|1|locate|OpenBSD}}

Variants:
* [http://rlocate.sourceforge.net/ rlocate] - Variant using kernel module and daemon for continuous updates.
* [http://www.kde-apps.org/content/show.php/KwickFind+(Locate+GUI+Frontend)?content=54817 KwickFind] - KDE GUI frontend for locate
* [http://www.locate32.net/ Locate32 for Windows] - GPL'ed graphical Windows variant

{{unix commands}}

[[Category:GNU Project software]]
[[Category:Unix file system-related software]]
[[Category:Information retrieval systems]]


{{Unix-stub}}</text>
      <sha1>74kxxbl2zv8k5e9k3buttpilfouie4o</sha1>
    </revision>
  </page>
  <page>
    <title>Contextual Query Language</title>
    <ns>0</ns>
    <id>9672320</id>
    <revision>
      <id>666712419</id>
      <parentid>599561646</parentid>
      <timestamp>2015-06-13T03:21:05Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>move to Category:Information retrieval systems</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2682" xml:space="preserve">'''Contextual Query Language''' (CQL), previously known as '''Common Query Language''',&lt;ref&gt;[http://www.loc.gov/standards/sru/cql/spec.html CQL: the Contextual Query Language: Specifications] SRU: Search/Retrieval via URL, Standards, Library of Congress&lt;/ref&gt; is a [[formal language]] for representing queries to [[information retrieval]] systems such as [[search engine]]s, [[bibliography|bibliographic catalogs]] and [[museum]] collection information. Based on the [[semantics]] of [[Z39.50]], its design objective is that queries be human readable and writable, and that the language be intuitive while maintaining the expressiveness of more complex [[query language]]s. It is being developed and maintained by the Z39.50 Maintenance Agency, part of the [[Library of Congress]].

== Examples of query syntax ==

Simple queries:

&lt;blockquote&gt;&lt;tt&gt;dinosaur&lt;br/&gt;
"complete dinosaur"&lt;br/&gt;
title = "complete dinosaur"&lt;br/&gt;
title exact "the complete dinosaur"&lt;/tt&gt;&lt;/blockquote&gt;

Queries using [[Boolean logic]]:

&lt;blockquote&gt;&lt;tt&gt;dinosaur or bird&lt;br/&gt;
Palomar assignment and "ice age"&lt;br/&gt;
dinosaur not reptile&lt;br/&gt;
dinosaur and bird or dinobird&lt;br/&gt;
(bird or dinosaur) and (feathers or scales)&lt;br/&gt;
"feathered dinosaur" and (yixian or jehol)&lt;/tt&gt;&lt;/blockquote&gt;

Queries accessing [[index (publishing)|publication indexes]]:

&lt;blockquote&gt;&lt;tt&gt;publicationYear &lt; 1980&lt;br/&gt;
lengthOfFemur &gt; 2.4&lt;br/&gt;
bioMass &gt;= 100&lt;/tt&gt;&lt;/blockquote&gt;

Queries based on the proximity of words to each other in a document:

&lt;blockquote&gt;&lt;tt&gt;ribs prox/distance&lt;=5 chevrons&lt;br/&gt;
ribs prox/unit=sentence chevrons&lt;br/&gt;
ribs prox/distance&gt;0/unit=paragraph chevrons&lt;/tt&gt;&lt;/blockquote&gt;

Queries across multiple [[Dimension (data warehouse)|dimensions]]:

&lt;blockquote&gt;&lt;tt&gt;date within "2002 2005"&lt;br/&gt;
dateRange encloses 2003&lt;/tt&gt;&lt;/blockquote&gt;

Queries based on [[Relevance (information retrieval)|relevance]]:

&lt;blockquote&gt;&lt;tt&gt;subject any/relevant "fish frog"&lt;br/&gt;
subject any/rel.lr "fish frog"&lt;/tt&gt;&lt;/blockquote&gt;

The latter example specifies using a specific [[algorithm]] for [[logistic regression]].&lt;ref&gt;[http://srw.cheshire3.org/contextSets/rel/ Relevance Ranking Context Set version 1.1]&lt;/ref&gt;

== References ==
{{Reflist}}

== External links ==
* [http://www.loc.gov/standards/sru/cql/ CQL home page]
* [http://www.loc.gov/z3950/agency/ Z39.50 Maintenance Agency]
* [http://zing.z3950.org/cql/intro.html A Gentle Introduction to CQL]

{{Query languages}}

{{USGovernment|sourceURL=http://www.loc.gov/standards/sru/cql/}}
{{LOC-stub}}

[[Category:Information retrieval systems]]
[[Category:Library science]]
[[Category:Library of Congress]]
[[Category:Query languages]]
[[Category:Knowledge representation languages]]</text>
      <sha1>0m9p20h4ri08w133ag6qbrq8hiz9kp1</sha1>
    </revision>
  </page>
  <page>
    <title>Wolfram Alpha</title>
    <ns>0</ns>
    <id>21903944</id>
    <revision>
      <id>762129243</id>
      <parentid>760402531</parentid>
      <timestamp>2017-01-26T20:56:46Z</timestamp>
      <contributor>
        <username>Pleasantville</username>
        <id>3058640</id>
      </contributor>
      <comment>/* Licensing partners */ refine verb tense</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="20588" xml:space="preserve">{{Use mdy dates|date=October 2014}}
{{Infobox website
| name = Wolfram Alpha
| logo =  Wolfram Alpha December 2016.svg
| caption = Wolfram Alpha is based on the computational platform [[Mathematica]], written by British scientist [[Stephen Wolfram]] in 1988.
| url = {{URL|http://www.wolframalpha.com/}}.
| slogan             = Making the world’s knowledge computable.&lt;ref&gt;[http://www.wolframalpha.com/about.html Wolfram Alpha About page]&lt;/ref&gt;
| commercial = Yes
| type =  [[Answer engine]]
| registration = Optional
| owner = Wolfram Alpha LLC
| author = [[Wolfram Research]]
| alexa  = {{Decrease}} 1,932 ({{as of|2015|26|31|alt=March 2015}})&lt;ref name="alexa"&gt;{{cite web|url= http://www.alexa.com/siteinfo/wolframalpha.com |title= Wolframalpha.com Site Info | publisher= [[Alexa Internet]] |accessdate= 2015-03-31}}&lt;/ref&gt;
| num_employees ≈ 200 (as of 2012)
| programming_language = [[Wolfram Language]]
| launch date = {{start date and age|2009|5|18}}&lt;ref name="launch date"&gt;{{cite web |author=The Wolfram&amp;#124;Alpha Launch Team |url=http://blog.wolframalpha.com/2009/05/08/so-much-for-a-quiet-launch/ |title=So Much for A Quiet Launch |work=Wolfram&amp;#124;Alpha Blog |publisher=Wolfram Alpha |date=May 8, 2009 |accessdate=2013-02-09}}&lt;/ref&gt; (official launch)&lt;br&gt;{{start date|2009|5|15}}&lt;ref name="updated launch detail"&gt;{{cite web |author=The Wolfram&amp;#124;Alpha Launch Team |url=http://blog.wolframalpha.com/2009/05/12/going-live-and-webcasting-it/ |work=Wolfram&amp;#124;Alpha Blog |title=Going Live—and Webcasting It |publisher=Wolfram Alpha |date=May 12, 2009 |accessdate=2013-02-09}}&lt;/ref&gt; (public launch)
| current status = Active
}}

'''Wolfram Alpha''' (also styled '''WolframAlpha''' and '''Wolfram|Alpha''') is a computational knowledge engine&lt;ref name=Guardiandatasource&gt;{{cite news |title=Where does Wolfram Alpha get its information? |author=Bobbie Johnson |publisher=The Guardian |date=May 21, 2009 |accessdate=2013-03-08 |url=https://www.theguardian.com/technology/2009/may/21/1 }}&lt;/ref&gt; or [[answer engine]] developed by [[Wolfram Research]], which was founded by [[Stephen Wolfram]]. It is an online service that answers factual queries directly by computing the answer from externally sourced "curated data",&lt;ref&gt;{{Cite web|title = About Wolfram{{!}}Alpha: Making the World's Knowledge Computable|url = http://www.wolframalpha.com/about.html|website=wolframalpha.com|accessdate = 2015-11-25}}&lt;/ref&gt; rather than providing a list of documents or web pages that might contain the answer as a [[search engine]] might.&lt;ref&gt;{{cite news |url=https://www.theguardian.com/technology/2009/mar/09/search-engine-google |title=British search engine 'could rival Google' |last=Johnson |first=Bobbie |date=March 9, 2009 |work=The Guardian |publisher=Guardian News and Media |location=UK |accessdate=2013-02-09}}&lt;/ref&gt;

Wolfram Alpha, which was released on May 18, 2009, is based on Wolfram's earlier flagship product [[Wolfram Mathematica]], a computational platform or toolkit that encompasses computer algebra, symbolic and numerical computation, visualization, and statistics capabilities.&lt;ref name="launch date" /&gt; Additional data is gathered from both academic and commercial websites such as the CIA's ''[[The World Factbook]]'', the United States Geological Survey, a Cornell University Library publication called ''All About Birds'', ''Chambers Biographical Dictionary'', [[Dow Jones]], the ''Catalogue of Life'',&lt;ref name=Guardiandatasource /&gt; [[CrunchBase]],&lt;ref name=techcrunch&gt;{{cite news |last=Dillet |first=Romain |title=Wolfram Alpha Makes CrunchBase Data Computable Just In Time For Disrupt SF |url=http://techcrunch.com/2012/09/07/wolfram-alpha-makes-crunchbase-data-computable-just-in-time-for-disrupt/ |publisher=TechCrunch |date=September 7, 2012 |accessdate=2013-02-09}}&lt;/ref&gt; [[Best Buy]],&lt;ref&gt;{{cite news |last=Golson |first=Jordan |title=Wolfram Delivers Siri-Enabled Shopping Results From Best Buy |url=http://www.macrumors.com/2011/12/16/wolfram-delivers-siri-enabled-shopping-results-from-best-buy/ |publisher=MacRumors |date=December 16, 2011 |accessdate=2013-02-09}}&lt;/ref&gt; the [[Federal Aviation Administration|FAA]]&lt;ref&gt;{{cite news |last=Barylick |first=Chris |title=Wolfram Alpha search engine now tracks flight paths, trajectory information |url=http://www.engadget.com/2011/11/19/wolfram-alpha-search-engine-now-tracks-flight-paths-trajectory/ |publisher=Engadget |date=November 19, 2011 |accessdate=2013-02-09}}&lt;/ref&gt; and optionally a user's Facebook account.

== Overview ==
Users submit queries and computation requests via a text field.  Wolfram Alpha then computes answers and relevant visualizations from a [[knowledge base]] of [[Data curation|curated]], [[structured data]] that come from other sites and books. The site "use[s] a portfolio of automated and manual methods, including statistics, visualization, source cross-checking, and expert review."&lt;ref&gt;{{cite web |title=Data in Wolfram&amp;#124;Alpha |url=http://www.wolframalpha.com/faqs5.html |website=Wolfram Alpha |accessdate=4 August 2015}}&lt;/ref&gt; The curated data makes Alpha different from [[semantic search]] engines, which index a large number of answers and then try to match the question to one.

Wolfram Alpha can only provide robust query results based on computational facts, not queries on the social sciences, cultural studies or even many questions about history where responses require more subtlety and complexity. It is able to respond to particularly-phrased [[natural language understanding|natural language]] fact-based questions such as "Where was [[Mary Robinson]] born?" or more complex questions such as "How old was [[Queen Elizabeth II]] in 1974?" It displays its "Input interpretation" of such a question, using standardized phrases such as "age | of Queen Elizabeth II (royalty) | in 1974", the answer of which is "Age at start of 1974: 47 years", and a biography link. Wolfram Alpha does not answer queries which require a narrative response such as "What is the difference between the Julian and the Gregorian calendars?" but will answer factual or computational questions such as "June 1 in Julian calendar".

Mathematical symbolism can be parsed by the engine, which typically responds with more than the numerical results. For example, "lim(x-&gt;0) (sin x)/x" yields the correct [[limit (functions)|limit]]ing value of 1, as well as a plot, up to 235 terms ({{as of|2013|lc=y}}) of the [[Taylor series]], and (for registered users) a possible derivation using [[L'Hôpital's rule]]. It is also able to perform calculations on data using more than one source. For example, "What is the [[List of countries by GDP (nominal) per capita|fifty-second smallest]] country by [[GDP per capita]]?" yields [[Nicaragua]], $1160 per year.

== Technology ==
Wolfram Alpha is written in 15 million lines of [[Wolfram Language]] code&lt;ref&gt;{{cite web |author=WolframResearch |url=https://www.youtube.com/watch?v=56ISaies6Ws#t=927s |title=Stephen Wolfram: The Background and Vision of Mathematica |publisher=Youtube.com |date=October 10, 2011 |accessdate=2013-02-09}}&lt;/ref&gt; and runs on more than 10,000 CPUs.&lt;ref&gt;{{cite news |first=Frederic |last=Lardinois |url=http://readwrite.com/2009/04/25/wolframalpha_our_first_impressions |title=Wolfram&amp;#124;Alpha: Our First Impressions |date=April 25, 2009 |publisher=ReadWriteWeb |accessdate=2013-02-09}}&lt;/ref&gt;&lt;ref&gt;{{cite news |first=Stephen |last=Wolfram |url=http://blog.wolframalpha.com/2009/05/15/wolframalpha-is-launching-made-possible-by-mathematica/ |title=Wolfram&amp;#124;Alpha Is Launching: Made Possible by ''Mathematica'' |work=WolframAlpha Blog |publisher=Wolfram Alpha |date=May 15, 2009 |accessdate=2013-02-09}}&lt;/ref&gt; The database currently includes hundreds of datasets, such as "All Current and Historical Weather." The datasets have been accumulated over several years.&lt;ref&gt;{{cite web |title=Taking a first bite out of Wolfram Alpha | first=Jane Fae | last=Ozimek |work=The Register |date=May 18, 2009 |url=http://www.theregister.co.uk/2009/05/18/wolfram_alpha/ |accessdate=2013-02-09}}&lt;/ref&gt; The curated (as distinct from auto-generated) datasets are checked for quality either by a scientist or other expert in a relevant field, or someone acting in a clerical capacity who simply verifies that the datasets are "acceptable".&lt;ref name=semanticabyss&gt;{{cite web |title=The Semantic Abyss - Plumbing the Semantic Web: Exploring the depths of the semantic gap between the Semantic Web and real world users and consumers |url=http://semanticabyss.blogspot.ca/2009/03/what-is-curated-data.html |year=2009 |author=Jack Krupansky}}&lt;/ref&gt;{{unreliable source?|date=September 2015}}

One example of a live dataset that Wolfram Alpha can use is the profile of a [[Facebook]] user, through inputting the "facebook report" query. If the user authorizes Facebook to share his or her account details with the Wolfram site, Alpha can generate a "personal analytics" report containing the age distribution of friends, the frequency of words used in status updates and other detailed information.&lt;ref name=techland&gt;{{cite news |first=Thomas E. |last=Weber |url=http://techland.time.com/2012/09/05/wolfram-alphas-facebook-analytics-tool-digs-deep-into-your-social-life/ |title=Wolfram Alpha's Facebook Analytics Tool Digs Deep into Your Social Life |work=Tech |publisher=Time Magazine |date=September 5, 2012 |accessdate=2013-02-09}}&lt;/ref&gt; Within two weeks of launching the Facebook analytics service, 400,000 users had used it.&lt;ref&gt;{{cite news |last=R. |first=A. |title=Visualising Facebook Who am I? |url=http://www.economist.com/blogs/graphicdetail/2012/09/visualising-facebook |publisher=The Economist |work=Graphic detail |date=September 21, 2012 |accessdate=2013-02-09}}&lt;/ref&gt; Downloadable query results are behind a pay wall but summaries are accessible to free accounts.&lt;ref name=publiclibraries&gt;{{cite web |url=http://publiclibrariesonline.org/2013/03/a-wolf-or-a-ram-what-is-wolfram-alpha/ |title=A Wolf or a Ram? What is Wolfram Alpha? |author=Joanna Nelson |date=March 4, 2013 |publisher=Public Libraries Online }}&lt;/ref&gt;

== Licensing partners ==
Wolfram Alpha has been used to power some searches in the [[Microsoft]] [[Bing (search engine)|Bing]] and [[DuckDuckGo]] search engines.&lt;ref&gt;{{cite news |first=Tom |last=Krazit |url=http://news.cnet.com/8301-30684_3-10315117-265.html |title=Bing strikes licensing deal with Wolfram Alpha |publisher=CNET |date=August 21, 2009 |accessdate=2013-02-09}}&lt;/ref&gt;&lt;ref&gt;{{cite web |author=The Wolfram&amp;#124;Alpha Team |date=April 18, 2011 |url=http://blog.wolframalpha.com/2011/04/18/wolframalpha-and-duckduckgo-partner-on-api-binding-and-search-integration/ |title=Wolfram&amp;#124;Alpha and DuckDuckGo Partner on API Binding and Search Integration |work=Wolfram&amp;#124;Alpha Blog |publisher=Wolfram Alpha |accessdate=2013-02-09}}&lt;/ref&gt; For factual [[question answering]], it is also queried by Apple's [[Siri (software)|Siri]], Samsung's [[S Voice]], as well as Dexetra's [[speech recognition]] software for the [[Android (operating system)|Android]] platform, Iris, and the voice control software on [[BlackBerry 10]].&lt;ref&gt;{{cite web|url=http://www.berryreview.com/2013/10/21/blackberry-teams-up-with-wolfram-alpha-for-blackberry-10-voice-control/|title=BlackBerry Teams Up with Wolfram Alpha For BlackBerry 10 Voice Control|work=BerryReview}}&lt;/ref&gt;

== History ==
Launch preparations began on May 15, 2009 at 7&amp;nbsp;pm [[Central Daylight Time (North America)#Central Daylight Time|CDT]] and were broadcast live on [[Justin.tv]]. The plan was to publicly launch the service a few hours later, with expected issues due to extreme load. The service was officially launched on May 18, 2009.&lt;ref name="BBC"&gt;{{cite news |url=http://news.bbc.co.uk/1/hi/technology/8052798.stm |title=Wolfram 'search engine' goes live |publisher=BBC News |date=May 18, 2009 |accessdate=2013-02-09}}&lt;/ref&gt;

Wolfram Alpha has received mixed reviews.&lt;ref name="spivack"&gt;{{cite web |first=Nova |last=Spivack |title=Wolfram Alpha is Coming – and It Could be as Important as Google |date=March 7, 2009 |url=http://www.novaspivack.com/uncategorized/wolfram-alpha-is-coming-and-it-could-be-as-important-as-google |accessdate=2013-02-09 |publisher=Nova Spivack – Minding the Planet}}&lt;/ref&gt;&lt;ref&gt;{{cite news |first=Ryan |last=Singel |title=Wolfram&amp;#124;Alpha Fails the Cool Test |date=May 18, 2009 |url=http://www.wired.com/epicenter/2009/05/wolframalpha-fails-the-cool-test/ |publisher=Wired |accessdate=2013-02-09}}&lt;/ref&gt; Wolfram Alpha advocates point to its potential, some even stating that how it determines results is more important than current usefulness.&lt;ref name="spivack"/&gt;

On December 3, 2009, an [[iPhone]] app was introduced. Some users&lt;ref name="ios-price"&gt;{{cite web |first=MG |last=Siegler |url=http://techcrunch.com/2009/12/03/wolfram-alpha-iphone-app/ |title=Nice Try, Wolfram Alpha. Still Not Paying $50 For Your App. |publisher=TechCrunch |date=December 3, 2009 |accessdate=2013-02-09}}&lt;/ref&gt; considered the initial $50 price of the [[iOS]] app unnecessarily high, since the same features could be freely accessed by using a web browser instead. They also complained about the simultaneous removal of the mobile formatting option for the site.&lt;ref name="mobile-format"&gt;{{cite news |url=http://www.tuaw.com/2009/12/03/wolframalpha-iphone-formatted-web-page-no-longer-available/ |first=TJ |last=Luoma |title=WolframAlpha iPhone-formatted web page no longer available |publisher=TUAW |date=December 3, 2009 |accessdate=2013-02-09}}&lt;/ref&gt; Wolfram responded by lowering the price to $2, offering a refund to existing customers&lt;ref name="refund"&gt;{{cite web|last=Broida |first=Rick |url=http://reviews.cnet.com/8301-19512_7-10471978-233.html |title=Get Wolfram Alpha app for $1.99-and a refund if you paid more |publisher=CNET |date=April 1, 2010 |accessdate=2012-02-28}}&lt;/ref&gt; and re-instating the mobile site.

On October 6, 2010 an Android version of the app was released&lt;ref&gt;{{cite news |url=http://techcrunch.com/2010/10/06/wolframalphas-android-app-now-available/ |title=Wolfram Alpha's Android app now available |first=Leena |last=Rao |publisher=TechCrunch |date=October 6, 2010 |accessdate=2013-02-09}}&lt;/ref&gt; and it is now available for Kindle Fire and Nook. (The Nook version is not available outside the US). A further 71 apps are available which use the Wolfram Alpha engine for specialized tasks.&lt;ref&gt;{{cite web |url=http://products.wolframalpha.com/mobile/ |title=Wolfram&amp;#124;Alpha: Mobile &amp; Tablet Apps |year=2013 |accessdate=2013-02-09 |publisher=Wolfram Alpha}}&lt;/ref&gt;

== Wolfram Alpha Pro ==
On February 8, 2012, Wolfram Alpha Pro was released,&lt;ref name="WAProAnnounce"&gt;{{cite news |first=Stephen |last=Wolfram |url=http://blog.wolframalpha.com/2012/02/08/announcing-wolframalpha-pro/ |title=Announcing Wolfram&amp;#124;Alpha Pro |date=February 8, 2012 |work=Wolfram&amp;#124;Alpha Blog |publisher=Wolfram Alpha |accessdate=2013-02-09}}&lt;/ref&gt; offering users additional features for a monthly subscription fee. A key feature is the ability to upload many common file types and data—including raw tabular data, images, audio, XML, and dozens of specialized scientific, medical, and mathematical formats—for automatic analysis. Other features include an extended keyboard, interactivity with [[Computable Document Format|CDF]], data downloads, in-depth step by step solution, the ability to customize and save graphical and tabular results&lt;ref name="Hachman"&gt;{{cite news |last=Hachman |first=Mark |title=Data Geeks, Meet Wolfram Alpha Pro |publisher=[[PC Magazine]] |date=February 7, 2012 |url=http://www.pcmag.com/article2/0,2817,2399911,00.asp |accessdate=2012-02-15}}&lt;/ref&gt; and extra computation time.&lt;ref name="WAProAnnounce" /&gt;

Along with new premium features, Wolfram Alpha Pro has led to some changes in the free version of the site:
* An increase in advertisements on the free site.
* Text and PDF export options now require the user to set up a free account&lt;ref name="WAProAnnounce" /&gt; even though they existed before the introduction of Wolfram Alpha accounts.&lt;ref&gt;{{cite web|url=http://hplusmagazine.com/2009/06/24/users-guide-wolframalpha/|title=A User's Guide to Wolfram Alpha|first=Surfdaddy|last=Orca|publisher=H+ Magazine|date=2009-06-24|accessdate=2013-04-24}}&lt;/ref&gt;
* The option to request extra time for a long calculation used to be free&lt;ref name="extra-time-before"&gt;{{cite web|url=http://web.mst.edu/~jkmq53/school/Fall_2011/English_160/files/Marlowe_Usability_Test.docx|title=Wolfram Alpha Usability Test Survey|first=James|last=Marlowe|year=2011|accessdate=2013-04-24}}&lt;/ref&gt; but is now only available to subscribers.&lt;ref name="WAProAnnounce" /&gt;
* Step-by-Step limited to 3 for free users (previously uncapped)(no longer available).&lt;ref name="StepByStep"&gt;{{cite web|url=http://blog.wolframalpha.com/2009/12/01/step-by-step-math/|title=Step-by-Step Math}}&lt;/ref&gt;

== Copyright claims ==
''[[InfoWorld]]'' published an article&lt;ref name="copyright"&gt;{{cite web |last=McAllister |first=Neil |url=http://www.infoworld.com/d/developer-world/how-wolfram-alpha-could-change-software-248 |title=How Wolfram Alpha could change software |publisher=InfoWorld |date=July 29, 2009 |accessdate=2012-02-28}}&lt;/ref&gt; warning readers of the potential implications of giving an automated website proprietary rights to the data it generates. [[Free software movement|Free software]] advocate [[Richard Stallman]] also opposes the idea of recognizing the site as a copyright holder and suspects that Wolfram would not be able to make this case under existing copyright law.&lt;ref name="fsf"&gt;{{cite mailing list |url=http://lists.essential.org/pipermail/a2k/2009-August/004865.html |title=How Wolfram Alpha's Copyright Claims Could Change Software |date=August 4, 2009 |accessdate=2012-02-17 |mailinglist=[http://lists.essential.org/mailman/listinfo/a2k Access 2 Knowledge] |archiveurl=https://web.archive.org/web/20130428041345/http://lists.essential.org/pipermail/a2k/2009-August/004865.html |archivedate=April 28, 2013 |last=Stallman |first=Richard |authorlink=Richard Stallman}}&lt;/ref&gt;

== See also ==
* [[Commonsense knowledge problem]]
* [[Artificial general intelligence|Strong AI]]
* [[Watson (computer)]]

== References ==
{{Reflist|colwidth=30em}}

== Further reading ==
* [http://www.businessweek.com/the_thread/techbeat/archives/2009/03/wolfram_alpha_a.html Wolfram Alpha: A New Way To Search?], Stephen Wildstrom, ''BusinessWeek'', March 9, 2009.
* [http://www.informationweek.com/news/internet/search/showArticle.jhtml?articleID=215801388&amp;subSection=News Stephen Wolfram's Answer To Google: If Wolfram/Alpha works as advertised, it will be able to do something Google can't: provide answers that don't already exist in indexed documents.] by Thomas Claburn, ''InformationWeek'', March 10, 2009.
* [http://bits.blogs.nytimes.com/2009/03/09/better-search-doesnt-mean-beating-google/ Better Search Doesn’t Mean Beating Google] by Saul Hansell, ''The New York Times'', March 9, 2009.
* [http://www.pcworld.com/article/160904/wolfram_alpha_will_take_your_questions_any_questions.html Wolfram Alpha will Take Your Questions – Any Questions], Ian Paul, ''PC World'', March 9, 2009.
* [http://www.hplusmagazine.com/articles/ai/wolframalpha-searching-truth Wolfram Alpha: Searching for Truth: Stephen Wolfram talks with Rudy Rucker about his Upcoming Release] by [[Rudy Rucker]], ''H+ Magazine''.
*  [http://www.boston.com/business/technology/articles/2009/05/05/a_hungry_little_number_cruncher/ "A hungry little number cruncher: Wolfram Alpha search tool mines databases to yield math-based replies"] by [[Hiawatha Bray]], ''[[The Boston Globe]]'', May 5, 2009
* [http://newsbreaks.infotoday.com/NewsBreaks/Wolfram-Alpha-Semantic-Search-Is-Born-53892.asp "Wolfram Alpha: Semantic Search is Born" by [[Woody Evans]], May 21, 2009.]

== External links ==
* {{official website}}

{{Wolfram Research|state=uncollapsed}}
{{computable knowledge}}
{{Intelligent personal assistant software}}

[[Category:Agent-based software]]
[[Category:Computer algebra systems]]
[[Category:Educational math software]]
[[Category:Educational websites]]
[[Category:Information retrieval systems]]
[[Category:Intelligent software assistants]]
[[Category:Internet properties established in 2009]]
[[Category:Mathematics education]]
[[Category:Natural language processing software]]
[[Category:Open educational resources]]
[[Category:Physics education]]
[[Category:Semantic Web]]
[[Category:Software calculators]]
[[Category:Web analytics]]
[[Category:Websites which mirror Wikipedia]]
[[Category:Wolfram Research]]</text>
      <sha1>4rgcusbfax4wrgvkta098rd434utjua</sha1>
    </revision>
  </page>
  <page>
    <title>Quandl</title>
    <ns>0</ns>
    <id>39810775</id>
    <revision>
      <id>750645137</id>
      <parentid>746255330</parentid>
      <timestamp>2016-11-21T01:03:25Z</timestamp>
      <contributor>
        <ip>162.216.161.56</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6399" xml:space="preserve">{{Infobox dot-com company
|name               = Quandl, Inc.
|logo               = [[File:Quandl-logo.png|100px]]
|company_type       = [[Private company|Private]]
|founder            = {{Plainlist|
* Tammer Kamel
* Abraham Thomas
}}
|location           = [[Toronto]], [[Canada]]
|area_served        = Worldwide
|key_people         = {{Plainlist|
* Tammer Kamel &lt;small&gt;(CEO)&lt;/small&gt;
* Abraham Thomas &lt;small&gt;(CDO)&lt;/small&gt;
}}
|industry           = [[Internet]]
|products           = Quandl Data Marketplace
|services           = Data [[subscriptions]]
|num_employees      = 16
|url                = {{URL|quandl.com}}
|programming_language = [[Ruby (programming language)|Ruby]] and [[Java (programming language)|Java]]
|website_type       = [[E-commerce]]
|language           = English
|launch_date        = {{start date and age|2013|01|01|df=yes}}
}}

'''Quandl''' ({{IPAc-en|ˈ|k|w|ɑː|n|d|əl}}) is a Toronto-based platform for financial, economic, and alternative data, serving investment professionals. Quandl sources data from over 500 publishers.&lt;ref&gt;{{Cite web|url=https://www.producthunt.com/tech/quandl|title=Quandl - Product Hunt|website=Product Hunt|language=en-US|access-date=2016-09-01}}&lt;/ref&gt;  All Quandl's data are accessible via an [[API]].&lt;ref&gt;{{cite web |url= http://www.econometricsbysimulation.com/2013/05/quandl-package-5000000-free-datasets-at.html |title= Quandl Package - 5,000,000 free datasets at the tip of your fingers! |date= 5 May 2013 |publisher=EconBS}}&lt;/ref&gt; API access is possible through packages for multiple programming languages including [[R (programming language)|R]], [[Python (programming language)|Python]], [[Matlab]], [[Maple (software)]] and [[Stata]].&lt;ref&gt;{{cite web |url= http://blogs.computerworld.com/business-intelligenceanalytics/21881/quandl-wikipedia-data |title= Quandl: Wikipedia for data  |date= 8 March 2013 |publisher=Computer World |last = Machlis |first = Sharon }}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=https://www.quandl.com/tools/full-list |title=Full list of tools supported on Quandl}}&lt;/ref&gt;

An Excel add-in allows access to data, including stock price information.
 
Quandl's sources include the [[United Nations|UN]], [[Worldbank]], [[CLS Group]], Zacks, and several hundred more.&lt;ref&gt;{{cite web |url= http://gigaom.com/2013/05/31/its-a-beautiful-thing-when-free-data-meets-free-analytics/ |title= It's a beautiful thing when free data meets free analytics |date= 31 May 2013 |publisher=Gigaom |last = Harris |first = Derrick }}&lt;/ref&gt;&lt;ref&gt;{{cite web |url= http://www.quandl.com/resources/data-sources |title= Quandl Data Sources}}&lt;/ref&gt;

== History ==
Quandl was founded in 2012 by Tammer Kamel and Abraham Thomas.&lt;ref&gt;{{Cite web|url=https://www.quandl.com/about|title=Quandl Financial and Economic Data|website=www.quandl.com|access-date=2016-09-01}}&lt;/ref&gt; In March 2013, Quandl raised $1.5m,&lt;ref&gt;{{Cite web|url=https://www.crunchbase.com/organization/quandl|title=Quandl {{!}} CrunchBase|website=www.crunchbase.com|access-date=2016-09-01}}&lt;/ref&gt; in seed funding followed by a $5.4m Series A from [[August Capital]] in 2014.&lt;ref&gt;{{Cite web|url=http://blogs.wsj.com/venturecapital/2014/11/13/quandl-raises-5-4-million-for-its-financial-data-marketplace/|title=Quandl Raises $5.4 Million for Its Financial-Data Marketplace|last=Gage|first=Deborah|access-date=2016-09-01}}&lt;/ref&gt;

Since its launch, Quandl has been discussed as a disruptive force in the anachronistic financial data sector.&lt;ref&gt;{{Cite web|url=http://mattturck.com/2014/03/19/can-the-bloomberg-terminal-be-toppled/|title=Can the Bloomberg Terminal be "Toppled"?|date=2014-03-19|website=Matt Turck|access-date=2016-09-01}}&lt;/ref&gt; With over 100,000 users&lt;ref&gt;{{Cite web|url=https://www.integrity-research.com/new-data-provider-quandl/|title=New Data Provider Quandl Moves Toward Alternative Data • Integrity Research|language=en-US|access-date=2016-09-01}}&lt;/ref&gt; Quandl is positioning itself as a possible replacement for both Bloomberg and Reuters terminals.&lt;ref&gt;{{Cite web|url=http://www.huffingtonpost.com/irene-aldridge/blindsided-by-innovation-_b_9025960.html|title=Blindsided by innovation like Bloomberg? Don't become a statistic.|last=AbleMarkets.com|first=Irene Aldridge Quantitative portfolio manager; MD at|last2=speaker|date=2016-01-22|website=The Huffington Post|access-date=2016-09-01|last3=author|last4=Trading'|first4='High-Frequency}}&lt;/ref&gt; Quandl is an alternative for people who are unable to afford the expensive licensing fees of Bloomberg and Reuters.&lt;ref&gt;{{Cite web|url=https://openforum.hbs.org/challenge/understand-digital-transformation-of-business/data/quandl-a-marketplace-for-financial-data|title=Quandl: A Marketplace for Financial Data|access-date=2016-09-01}}&lt;/ref&gt;

== Products ==
Quandl's main focus, and area of expertise, is in the realm of alternative data.&lt;ref name=":0"&gt;{{Cite web|url=http://www.waterstechnology.com/inside-market-data/news/2462823/quandl-embarks-on-quest-for-alternative-data|title=Quandl Embarks on Quest for Alternative Data|access-date=2016-09-01}}&lt;/ref&gt; Quandl sells alternative datasets, defined as "any data that is not typically made available to Wall Street firms by traditional sources".&lt;ref name=":0" /&gt; Quandl "sources, evaluates and productizes undiscovered data" and then sells it to financial institutions, who use it to enhance their trading strategies.&lt;ref&gt;{{Cite web|url=https://www.quandl.com/institutions|title=Quandl Financial and Economic Data|website=www.quandl.com|access-date=2016-09-01}}&lt;/ref&gt;

Quandl also offers market data through its marketplace. Some data sets are free while others require a subscription. Different datasets have different prices. They have hundreds of databases and providers ranging from stock price history to global fundamentals to commodities data to Asian market data.&lt;ref&gt;{{Cite web|url=http://www.quandl.com/vendors|title=Quandl Financial and Economic Data|website=www.quandl.com|access-date=2016-09-01}}&lt;/ref&gt;

Quandl has an exclusive relationship with CLS Group in London, and is the only source of commercial [[Foreign exchange market|FX]] volume data&lt;ref&gt;{{Cite web|url=http://www.waterstechnology.com/inside-market-data/news/2465222/quandl-adds-cls-fx-trade-volume-data-to-online-platform|title=Quandl Adds CLS FX Trade, Volume Data to Online Platform|access-date=2016-09-01}}&lt;/ref&gt;

== References ==
{{Reflist|30em}}

[[Category:Information retrieval systems]]</text>
      <sha1>17mfcufnje4fdsd2maxul4i60b6k86j</sha1>
    </revision>
  </page>
  <page>
    <title>TREX search engine</title>
    <ns>0</ns>
    <id>13179109</id>
    <revision>
      <id>733031596</id>
      <parentid>730352803</parentid>
      <timestamp>2016-08-04T22:21:15Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <minor />
      <comment>/* top */ template ref</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3263" xml:space="preserve">'''TREX''' is a search engine in the [[NetWeaver|SAP NetWeaver]] integrated technology platform produced by [[SAP AG]] using [[columnar storage]].&lt;ref&gt;{{cite journal|url=http://db.csail.mit.edu/pubs/abadi-column-stores.pdf|doi=10.1561/1900000024|title=The Design and Implementation of Modern Column-Oriented Database Systems|author1=Daniel Abadi|author2=Peter Boncz|author3=Stavros Harizopoulos|author4=Stratos Idreos|author5=Samuel Madden|journal=Foundations and Trends in Databases|volume=5|issue=3|year=2012|pages=197–280}}&lt;/ref&gt; The TREX engine is a standalone component that can be used in a range of system environments but is used primarily as an integral part of such SAP products as Enterprise Portal, Knowledge Warehouse, and '''Business Intelligence (BI, formerly [[SAP Business Information Warehouse]]).''' In SAP NetWeaver BI, the TREX engine powers the BI Accelerator, which is a plug-in appliance for enhancing the performance of [[online analytical processing]]. The name "TREX" stands for '''Text Retrieval and information EXtraction''', but it is not a registered trade mark of SAP and is not used in marketing collateral.

==Search functions==

TREX supports various kinds of text search, including exact search, boolean search, wildcard search, linguistic search (grammatical variants are normalized for the index search) and fuzzy search (input strings that differ by a few letters from an index term are normalized for the index search). Result sets are ranked using term frequency-inverse document frequency ([[tf-idf]]) weighting, and results can include snippets with the search terms highlighted.

TREX supports text mining and classification using a [[vector space model]]. Groups of documents can be classified using query based classification, example based classification, or a combination of these plus keyword management.

TREX supports structured data search not only for document metadata but also for mass business data and data in SAP [[Business Objects]]. Indexes for structured data are implemented compactly using [[data compression]] and the data can be aggregated in linear time, to enable large volumes of data to be processed entirely in memory.

Recent developments include:
* A join engine to join structured data from different fields in business objects
* A fast update capability to write a delta index beside a main index and to merge them offline while a second delta index takes updates
* A [[data mining]] feature pack for advanced mathematical analysis

==History==

The first code for the engine was written in 1998 and TREX became an SAP component in 2000. The SAP NetWeaver BI Accelerator was first rolled out in 2005. As of Q1 2013, the current release of TREX is SAP NW 7.1.

==References==
{{Reflist}}

==External links==
* [http://www.sap.com/platform/netweaver/index.epx SAP NetWeaver]
* [http://www.sap.com/platform/netweaver/components/bi/index.epx SAP NetWeaver Business Intelligence]
* [http://www.sap.com/platform/netweaver/businessinformation.epx SAP NetWeaver Business Information Management]
* [http://scn.sap.com/docs/DOC-8489 Search and Classification (TREX) on SAP Community Network]

[[Category:SAP NetWeaver]]
[[Category:Information retrieval systems]]
[[Category:Business intelligence]]</text>
      <sha1>5pmkjw02zj6upmut7ig1zdm678dyz0k</sha1>
    </revision>
  </page>
  <page>
    <title>Collaborative search engine</title>
    <ns>0</ns>
    <id>22101925</id>
    <revision>
      <id>733812882</id>
      <parentid>725540619</parentid>
      <timestamp>2016-08-10T08:04:20Z</timestamp>
      <contributor>
        <username>Klamann</username>
        <id>10223845</id>
      </contributor>
      <comment>/* Community of practice */ jumper 2.0 has been renamed to apexkb</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="14749" xml:space="preserve">{{Recommender systems}}
'''Collaborative search engines''' (CSE) are [[Web search engine]]s and [[enterprise search]]es within company intranets that let users combine their efforts in [[information retrieval]] (IR) activities, share information resources collaboratively using [[knowledge tags]], and allow experts to guide less experienced people through their searches. Collaboration partners do so by providing query terms, collective tagging, adding comments or opinions, rating search results, and links clicked of former (successful) IR activities to users having the same or a related [[information need]].

== Models of collaboration ==

Collaborative search engines can be classified along several dimensions: intent (explicit and implicit) and synchronization
&lt;ref name=Golo2007&gt;{{citation
 | title = Collaborative Exploratory Search
 | year = 2007
 | author = Golovchinsky Gene
 | author2 = Pickens Jeremy
 | journal = Proceedings of HCIR 2007 workshop
 | pages = 
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = http://projects.csail.mit.edu/hcir/web/hcir07.pdf
}}&lt;/ref&gt; and depth of mediation 
,&lt;ref name=Pickens2008&gt;{{citation
 | title = Collaborative Exploratory Search
 | year = 2008
 | author = Pickens Jeremy
 | author2 = Golovchinsky Gene
 | author3 = Shah Chirag
 | author4 = Qvarfordt Pernilla
 | author5 = Back Maribeth
 | booktitle = SIGIR '08: Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval
 | pages = 315–322
 | volume = 
 | issue = 
 | doi = 10.1145/1390334.1390389
 | isbn = 
 9781605581644| url = http://portal.acm.org/citation.cfm?id=1390389
| chapter = Algorithmic mediation for collaborative exploratory search
 }}&lt;/ref&gt; task vs. trait,&lt;ref name=Morris2008&gt;{{citation
 | contribution = Understanding Groups’ Properties as a Means of Improving Collaborative Search Systems
 | year = 2008
 | author = Morris Meredith
 | author2 = Teevan Jaime
 | title = 1st International Workshop on Collaborative Information Retrieval, held in conjunction with [[Joint Confrence on Digital Libraries|JCDL]] 2008
 | pages = 
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | contribution-url = http://workshops.fxpal.com/jcdl2008/submissions/tmpDF.pdf
}}&lt;/ref&gt; and division of labor and sharing of knowledge.&lt;ref name=Foley2008&gt;{{citation
 | title = Division of Labour and Sharing of Knowledge for Synchronous Collaborative Information Retrieval
 | year = 2008
 | author = Foley Colum
 | booktitle = PhD Thesis, Dublin City University
 | pages = 
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = http://www.computing.dcu.ie/~cfoley/cfoley-PhD_thesis.pdf
}}&lt;/ref&gt;

=== Explicit vs. implicit collaboration ===

Implicit collaboration characterizes [[Collaborative filtering]] and [[recommendation systems]] in which the system infers similar information needs. I-Spy,&lt;ref name=Smith2003&gt;{{citation
 | title = Collaborative Web Search
 | year = 2003
 | author = Barry Smyth
 | author2 = Evelyn Balfe
 | author3 = Peter Briggs
 | author4 = Maurice Coyle
 | author5 = Jill Freyne
 | journal = IJCAI
 | pages = 1417–1419
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = 
}}&lt;/ref&gt; [[Jumper 2.0]], [[Seeks]], the Community Search Assistant,&lt;ref name=Glance2001&gt;{{citation
 | title = Community search assistant
 | year = 2001
 | author = Natalie S. Glance
 | journal = Workshop on AI for Web Search AAAI'02
 | pages = 
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = 
}}&lt;/ref&gt; the CSE of Burghardt et al.,&lt;ref name=BurghardtWI2008&gt;{{citation
 | title = Discovering the Scope of Privacy Needs in Collaborative Search
 | year = 2008
 | author = Thorben Burghardt
 | author2 = Erik Buchmann
 | author3 = Klemens Böhm
 | journal = Web Intelligence (WI)
 | pages = 
 910| volume = 
 | issue = 
 | doi = 10.1109/WIIAT.2008.165
 | isbn = 
 978-0-7695-3496-1}}&lt;/ref&gt; and the works of Longo et al.
&lt;ref name=Longo2009a&gt;{{citation
 | title = Toward Social Search - From Explicit to Implicit Collaboration to Predict Users' Interests
 | year = 2009
 | author = Longo Luca
 | author2 = Barrett Stephen
 | author3 = Dondio Pierpaolo
 | journal = ''[[Webist]]'' 2009 - Proceedings of the Fifth International Conference                on Web Information Systems and Technologies, Lisbon, Portugal,                March 23–26, 2009
 | pages = 693–696
 | volume = 1
 | issue = 
 | doi = 
 | isbn = 978-989-8111-81-4
 | url = 
}}&lt;/ref&gt; 
&lt;ref name=Longo2010&gt;{{citation
 | title = Enhancing Social Search: A Computational Collective Intelligence Model of Behavioural Traits, Trust and Time
 | year = 2010
 | author = Longo Luca
 | author2 = Barrett Stephen
 | author3 = Dondio Pierpaolo
 | journal = Transaction Computational Collective Intelligence II
 | pages = 46–69
 | volume = 2
 | issue = 
 | doi = 10.1007/978-3-642-17155-0_3
 | isbn = 
 978-3-642-17154-3| url = http://www.springerlink.com/content/e12233858017h042/
| series = Lecture Notes in Computer Science
 }}&lt;/ref&gt; 
&lt;ref name=Longo2009b&gt;{{citation
 | title = Information Foraging Theory as a Form of Collective Intelligence                for Social Search
 | year = 2009
 | author = Longo Luca
 | author2 = Barrett Stephen
 | author3 = Dondio Pierpaolo
 | journal = Computational Collective Intelligence. Semantic Web, Social                Networks and Multiagent Systems, First International Conference,                ICCCI 2009, Wroclaw, Poland, October 5–7, 2009. Proceedings
 | pages = 63–74
 | volume = 1
 | issue = 
 | doi = 
 | isbn = 978-3-642-04440-3
 | url = http://dl.acm.org/citation.cfm?id=1692026
}}&lt;/ref&gt; 
all represent examples of implicit collaboration. Systems that fall under this category identify similar users, queries and links clicked automatically, and recommend related queries and links to the searchers.

Explicit collaboration means that users share an agreed-upon information need and work together toward that goal. For example, in a chat-like application, query terms and links clicked are automatically exchanged. The most prominent example of this class is SearchTogether&lt;ref name=Morris2007&gt;{{citation
 | title = SearchTogether: An Interface for Collaborative Web Search
 | year = 2007
 | author = Meredith Ringel Morris
 | author2 = Eric Horvitz
 | journal = UIST
| url = http://portal.acm.org/citation.cfm?id=1294211.1294215
}}&lt;/ref&gt; published in 2007. SearchTogether offers an interface that combines search results from standard search engines and a chat to exchange queries and links. Reddy et al.&lt;ref name=Redy2008&gt;{{citation
 | title = The Role of Communication in Collaborative Information Searching
 | year = 2008
 | author = Madhu C. Reddy
 | author2 = Bernhard J. Jansen
 | author3 = Rashmi Krishnappa
 | journal = ASTIS
}}&lt;/ref&gt; (2008) follow a similar approach and compares two implementations of their CSE called MUSE and MUST. Reddy et al. focuses on the role of communication required for efficient CSEs. Representatives for the class of implicit collaboration are I-Spy,&lt;ref name="Smith2003"/&gt; the Community Search Assistant,&lt;ref name="Glance2001"/&gt; and the CSE of Burghardt et al.&lt;ref name="BurghardtWI2008" /&gt; Cerciamo &lt;ref name=Pickens2008 /&gt; supports explicit collaboration by allowing one person to concentrate on finding promising groups of documents, while having the other person make in-depth judgments of relevance on documents found by the first person.

However, in Papagelis et al.&lt;ref name=Papagelis2007&gt;{{citation| title = Searchius: A Collaborative Search Engine| year = 2007| author = Athanasios Papagelis| author2 = Christos Zaroliagis| journal = ENC '07: Proceedings of the Eighth Mexican International Conference on Current Trends in Computer Science| pages = 88–98| doi = 10.1109/ENC.2007.34| url = http://portal.acm.org/citation.cfm?id=1302894| isbn = 0-7695-2899-6}}&lt;/ref&gt; terms are used differently: they combine explicitly shared links and implicitly collected browsing histories of users to a hybrid CSE.

=== Community of practice  ===

Recent work in collaborative filtering and information retrieval has shown that sharing of search experiences among users having similar interests, typically called a [[community of practice]] or [[community of interest]], reduces the effort put in by a given user in retrieving the exact information of interest.&lt;ref name=Rohini&amp;Ambati&gt;{{citation
 | title = A Collaborative Filtering based Re-ranking Strategy for Search in Digital Libraries
 | year = 2002
 | author = Rohini U
 | author2 = Vamshi Ambati
 | journal = ICADL2005: the 8th International Conference on Asian Digital Libraries
 | pages = 
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = http://www.aaai.org/Papers/Workshops/2006/WS-06-10/WS06-10-004.pdf }}&lt;/ref&gt;

Collaborative search deployed within a community of practice deploys novel techniques for exploiting context during search by indexing and ranking search results based on the learned preferences of a community of users.&lt;ref name=Coyle2008&gt;{{citation
 | title = Social Aspects of a Collaborative, Community-Based Search Network
 | editor4-first = Eelco
 | editor3-first = Pearl
 | editor2-first = Judy
 | editor1-first = Wolfgang
 | year = 2008
 | editor1-last = Nejdl
 | author = Maurice Coyle
 | author2 = Barry Smyth
 | last-author-amp = yes
 | journal = Adaptive Hypermedia and Adaptive Web-Based Systems
 | pages =  103–112  
 | volume = 5149/2008
 | issue = 
 | series = | doi = 10.1007/978-3-540-70987-9
 | isbn = 978-3-540-70984-8
 | url = http://portal.acm.org/citation.cfm?id=1485050
 | editor2-last = Kay
 | editor4-last = Herder
 | editor3-last = Pu| display-editors = 3}}&lt;/ref&gt; The users benefit by sharing information, experiences and awareness to personalize result-lists to reflect the preferences of the community as a whole. The community representing a group of users who share common interests, similar professions.  The best known example is the open-source project [[ApexKB]] (previously known as Jumper 2.0).&lt;ref name=Jumper2010&gt;{{citation
 | title = Jumper Networks Releases Jumper 2.0.1.5 Platform with New Community Search Features
 | year = 2010
 | author = Jumper Networks Inc.
 | journal = Press release
 | pages = 
 | volume =
 | issue = 
 | doi =
 | isbn =
 | url = http://www.trilexnet.com/labs/jumper}}&lt;/ref&gt;

=== Depth of mediation ===

This refers to the degree that the CSE mediates search.&lt;ref name=Pickens2008 /&gt; SearchTogether&lt;ref name=Morris2007 /&gt; is an example of UI-level mediation: users exchange query results and judgments of relevance, but the system does not distinguish among users when they run queries. Cerchiamo&lt;ref name=Pickens2008 /&gt; and recommendation systems such as I-Spy&lt;ref name=Smith2003 /&gt; keep track of each person's search activity independently, and use that information to affect their search results. These are examples of deeper algorithmic mediation.

=== Task vs. trait ===

This model classifies people's membership in groups based on the task at hand vs. long-term interests; these may be correlated with explicit and implicit collaboration.&lt;ref name=Morris2008 /&gt;

== Privacy-aware collaborative search engines ==

Search terms and links clicked that are shared among users reveal their interests, habits, social
relations and intentions.&lt;ref name=EUArticle29&gt;{{citation
 | title = Article 29 EU Data Protection Working Party
 | year = 2008
 | author = Data Protection Working Party
 | journal = EU
 | pages = 
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = 
}}&lt;/ref&gt; In other words, CSEs put the privacy of the users at risk. Studies have shown that CSEs increase efficiency. 
&lt;ref name="Morris2007"/&gt;&lt;ref name=Smith2005&gt;{{citation
 | title = A Live-User Evaluation of Collaborative Web Search
 | year = 2005
 | author = Barry Smyth
 | author2 = Evelyn Balfe
 | author3 = Oisin Boydell
 | author4 = Keith Bradley
 | author5 = Peter Briggs
 | author6 = Maurice Coyle
 | author7 = Jill Freyne
 | journal = IJCAI
 | pages = 
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = 
}}&lt;/ref&gt;
&lt;ref name=Smith2006&gt;{{citation
 | title = Anonymous personalization in collaborative web search
 | year = 2005
 | author = Smyth, Barry
 | author2 = Balfe, Evelyn
 | last-author-amp = yes
 | journal = Inf. Retr.
 | pages = 165–190
 | volume = 9
 | issue = 2| doi = 10.1007/s10791-006-7148-z| isbn = 
 | url = 
}}&lt;/ref&gt;
&lt;ref name=Jung2004&gt;{{citation
 | title = Applying Collaborative Filtering for Efficient Document Search
 | year = 2004
 | author = Seikyung Jung
 | author2 = Juntae Kim
 | author3 = Herlocker, JL
 | journal = Inf. Retr.
 | pages = 640–643
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = 
}}&lt;/ref&gt; Unfortunately, by the lack of privacy enhancing technologies, a privacy aware user who wants to benefit from a CSE has to disclose his entire search log. (Note, even when explicitly sharing queries and links clicked, the whole (former) log is disclosed to any user that joins a search session).  Thus, sophisticated mechanisms that allow on a more fine grained level which information is disclosed to whom are desirable.

As CSEs are a new technology just entering the market, identifying user privacy preferences and integrating [[Privacy enhancing technologies]] (PETs) into collaborative search are in conflict. On one hand, PETs have to meet user preferences, on the other hand one cannot identify these preferences without using a CSE, i.e., implementing PETs into CSEs. Today, the only work addressing this problem comes from Burghardt et al.&lt;ref name=BurghardtCC2008&gt;{{citation
 | title = Collaborative Search And User Privacy: How Can They Be Reconciled?
 | year = 2008
 | author = Thorben Burghardt
 | author2 = Erik Buchmann
 | author3 = Klemens Böhm
 | author4 = Chris Clifton
 | journal = CollaborateCom
 | pages = 
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = http://dbis.ipd.uni-karlsruhe.de/1184.php
}}&lt;/ref&gt; They implemented a CSE with experts from the information system domain and derived the scope of possible privacy preferences in a user study with these experts. Results show that users define preferences referring to (i) their current context (e.g., being at work), (ii) the query content (e.g., users exclude topics from sharing), (iii) time constraints (e.g., do not publish the query X hours after the query has been issued, do not store longer than X days, do not share between working time), and that users intensively use the option to (iv) distinguish between different social groups when sharing information. Further, users require (v) anonymization and (vi) define reciprocal constraints, i.e., they refer to the behavior of other users, e.g., if a user would have shared the same query in turn.

== References ==
{{reflist|2}}
{{Internet search}}

[[Category:Information retrieval systems]]</text>
      <sha1>agj9vnwxx8zguckzd8ak74sc4hhysja</sha1>
    </revision>
  </page>
  <page>
    <title>Champion list</title>
    <ns>0</ns>
    <id>26304039</id>
    <revision>
      <id>729017937</id>
      <parentid>729017905</parentid>
      <timestamp>2016-07-09T07:52:28Z</timestamp>
      <contributor>
        <username>David.moreno72</username>
        <id>16075528</id>
      </contributor>
      <minor />
      <comment>Reverted edits by [[Special:Contributions/113.193.171.202|113.193.171.202]] ([[User talk:113.193.171.202|talk]]) ([[WP:HG|HG]]) (3.1.19)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="579" xml:space="preserve">{{orphan|date=January 2011}}

A '''champion list''', also called '''top doc''' or '''fancy list''' is a precomputed list sometimes used with the [[vector space model]] to avoid computing relevancy rankings for all documents each time a document collection is queried. The champion list contains a set of n documents with the highest weights for the given term. The number n can be chosen to be different for each term and is often higher for rarer terms. The weights can be calculated by for example [[tf-idf]].

[[Category:Information retrieval evaluation]]


{{computing-stub}}</text>
      <sha1>i9u628mnj62etw7iiqtpwor0y7f1eno</sha1>
    </revision>
  </page>
  <page>
    <title>Mean reciprocal rank</title>
    <ns>0</ns>
    <id>11184711</id>
    <revision>
      <id>740090554</id>
      <parentid>723517997</parentid>
      <timestamp>2016-09-19T00:53:39Z</timestamp>
      <contributor>
        <ip>1.186.132.102</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2340" xml:space="preserve">{{Refimprove|date=June 2007}}
The '''mean reciprocal rank''' is a [[statistic]] measure for evaluating any process that produces a list of possible responses to a sample of queries, ordered by probability of correctness. The reciprocal rank of a query response is the [[multiplicative inverse]] of the rank of the first correct answer. The mean reciprocal rank is the average of the reciprocal ranks of results for a sample of queries Q:&lt;ref&gt;{{cite conference | title=Proceedings of the 8th Text Retrieval Conference | booktitle=TREC-8 Question Answering Track Report | author=E.M. Voorhees |year=1999 | pages=77&amp;ndash;82}}&lt;/ref&gt;

:&lt;math&gt; \text{MRR} = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{\text{rank}_i}. \!&lt;/math&gt;

where &lt;math&gt; \text{rank}_i&lt;/math&gt; refers to the rank position of the ''first'' relevant document for the ''i''-th query.

The reciprocal value of the mean reciprocal rank corresponds to the [[harmonic mean]] of the ranks.

== Example ==
For example, suppose we have the following three sample queries for a system that tries to translate English words to their plurals.  In each case, the system makes three guesses, with the first one being the one it thinks is most likely correct:

{| class="wikitable"
|-
! Query
! Results
! Correct response
! Rank
! Reciprocal rank
|-
| cat
| catten, cati, '''cats'''
| cats
| 3
| 1/3
|-
|tori
| torii, '''tori''', toruses
| tori
| 2
| 1/2
|-
| virus
| '''viruses''', virii, viri
| viruses
| 1
| 1
|}

Given those three samples, we could calculate the mean reciprocal rank as (1/3&amp;nbsp;+&amp;nbsp;1/2&amp;nbsp;+&amp;nbsp;1)/3 = 11/18 or about 0.61.

This basic definition does not specify what to do if none of the proposed results are correct, though reciprocal rank 0 could be used in this situation.  It also does not specify what do to if there are multiple correct answers in the list. In this case, [[Information retrieval#Mean average precision|mean average precision]] is a potential alternative metric.

==See also==
* [[Information retrieval]]
* [[Question answering]]

==References==
{{Reflist}}

==External links==
* {{cite conference | title=Evaluating web-based question answering systems | booktitle=Proceedings of LREC |author1=D. R. Radev |author2=H. Qi |author3=H. Wu |author4=W. Fan |year=2002 }}

[[Category:Summary statistics]]
[[Category:Information retrieval evaluation]]</text>
      <sha1>r9qtxgr7ak2xbozt6i2cfaobhhtkn5b</sha1>
    </revision>
  </page>
  <page>
    <title>Mooers' law</title>
    <ns>0</ns>
    <id>11373842</id>
    <revision>
      <id>755331442</id>
      <parentid>747593630</parentid>
      <timestamp>2016-12-17T11:05:53Z</timestamp>
      <contributor>
        <username>Furrykef</username>
        <id>17163</id>
      </contributor>
      <minor />
      <comment>/* Original interpretation */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3622" xml:space="preserve">{{For|the observation regarding integrated circuits|Moore's law}}
{{Refimprove|date=September 2011}}

'''Mooers' law''' is an empirical observation of behavior made by American [[computer scientist]] [[Calvin Mooers]] in 1959. The observation is made in relation to [[information retrieval]] and the interpretation of the observation is used commonly throughout the information profession both within and outside its original context.

{{quote|An information retrieval system will tend not to be used whenever it is more painful and troublesome for a customer to have information than for him not to have it.|[[Calvin Mooers]]&lt;ref name="morville"&gt;{{cite book|url=https://books.google.com/books?id=xJNLJXXbhusC&amp;printsec=frontcover&amp;dq=isbn:9780596007652&amp;hl=en&amp;sa=X&amp;ei=qvWhT5DfHITs2QX1rNzPCA&amp;ved=0CDAQ6AEwAA#v=onepage&amp;q=mooers'%20law&amp;f=false |title= Ambient findability |series= O'Reilly Series. Marketing/Technology &amp; Society |author= Peter Morville |edition= illustrated |publisher= O'Reilly Media |year= 2005 |page= 44|isbn= 978-0-596-00765-2}}&lt;/ref&gt;}}

==Original interpretation==

Mooers argued that information is at risk of languishing unused due not only on the effort required to assimilate it but also to any fallout that could arise from the discovery of information that conflicts with the user's personal, academic or corporate interests. In interacting with new information, a user runs the risk of proving their work incorrect or even irrelevant. Instead, Mooers argued, users prefer to remain in a state of safety in which new arguments are ignored in an attempt to save potential embarrassment or reprisal from supervisors.&lt;ref&gt;{{cite web|last=Mooers|first=Calvin|title=Mooers Law, or Why some Retrieval Systems are Used and Others Are not|url=http://findarticles.com/p/articles/mi_qa3633/is_199610/ai_n8749122/|work=Business Library|accessdate=25 October 2011}}&lt;/ref&gt;

==Out-of-context interpretation==

The more commonly used interpretation of Mooers' law is considered to be a derivation of the [[principle of least effort]] first stated by [[George Kingsley Zipf]]. This interpretation focuses on the amount of effort that will be expended to use and understand a particular information retrieval system before the information seeker 'gives up', and the Law is often paraphrased to increase the focus on the retrieval system:

{{quote|The more difficult and time consuming it is for a customer to use an information system, the less likely it is that he will use that information system.|J. Michael Pemberton}}
{{quote|Mooers' Law tells us that information will be used in direct proportion to how easy it is to obtain.|Roger K. Summit &lt;ref name="morville"/&gt;}}

In this interpretation, "painful and troublesome" comes from ''using'' the retrieval system.

==References==
{{reflist}}

*{{cite journal |last=Austin |first=Brice |date=June 2001 |title=Mooers' Law: In and out of Context |journal=Journal of the American Society for Information Science and Technology |volume=25 |issue=8 |pages=607–609 |url=http://spot.colorado.edu/~norcirc/Mooers.html |accessdate=2007-05-23 |doi=10.1002/asi.1114}}

==External links==
* [http://special.lib.umn.edu/findaid/xml/cbi00081.xml Calvin N. Mooers Papers, 1930-1992] at the [[Charles Babbage Institute]], University of Minnesota.
* [http://purl.umn.edu/107510 Oral history interview with Calvin N. Mooers and Charlotte D. Mooers] at the [[Charles Babbage Institute]].  Interview discusses information retrieval and programming language research from World War II through the early 1990s.
[[Category:Empirical laws]]
[[Category:Information retrieval evaluation]]</text>
      <sha1>9py7o3ehrqar0r7k3ra8xcmibf9u8mt</sha1>
    </revision>
  </page>
  <page>
    <title>Queries per second</title>
    <ns>0</ns>
    <id>26039201</id>
    <revision>
      <id>715385301</id>
      <parentid>705017813</parentid>
      <timestamp>2016-04-15T13:08:34Z</timestamp>
      <contributor>
        <username>Rich Farmbrough</username>
        <id>82835</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="714" xml:space="preserve">{{distinguish-redirect|Query rate|Query throughput}}
'''Queries per second''' (QPS) is a common measure of the amount of search traffic an [[information retrieval]] system, such as a [[search engine]] or a [[database]], receives during one second.&lt;ref&gt;[http://www.microsoft.com/enterprisesearch/en/us/search-glossary.aspx#Q Microsoft's search glossary]&lt;/ref&gt; The term is used more broadly for any [[request–response]] system, more correctly called [[requests per second]] (RPS).

High-traffic systems must watch their QPS in order to know when to scale the system to handle more load.

== References ==
{{reflist}}

[[Category:Units of frequency]]
[[Category:Information retrieval evaluation]]

{{computer-stub}}</text>
      <sha1>sataj5lgtlryp2ngmkol7cthkdt7gu0</sha1>
    </revision>
  </page>
  <page>
    <title>Matthews correlation coefficient</title>
    <ns>0</ns>
    <id>12306500</id>
    <revision>
      <id>757297687</id>
      <parentid>757183084</parentid>
      <timestamp>2016-12-29T22:14:22Z</timestamp>
      <contributor>
        <username>Erotemic</username>
        <id>18610581</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7489" xml:space="preserve">The '''Matthews correlation coefficient''' is used in [[machine learning]] as a measure of the quality of binary (two-class) [[Binary classification|classifications]], introduced by biochemist [[Brian Matthews (biochemist)|Brian W. Matthews]] in 1975.&lt;ref name="Matthews1975"&gt;{{cite journal|last=Matthews|first=B. W.|title=Comparison of the predicted and observed secondary structure of T4 phage lysozyme|journal=Biochimica et Biophysica Acta (BBA) - Protein Structure|date=1975|volume=405|issue=2|pages=442–451|doi=10.1016/0005-2795(75)90109-9}}&lt;/ref&gt; It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The MCC is in essence a correlation coefficient between the observed and predicted binary classifications; it returns a value between &amp;minus;1 and +1. A coefficient of +1 represents a perfect prediction, 0 no better than random prediction and &amp;minus;1 indicates total disagreement between prediction and observation. The statistic is also known as the [[phi coefficient]]. MCC is related to the [[Pearson's chi-square test|chi-square statistic]] for a 2×2 [[contingency table]]

: &lt;math&gt;|\text{MCC}| = \sqrt{\frac{\chi^2}{n}}&lt;/math&gt;

where ''n'' is the total number of observations.

While there is no perfect way of describing the [[confusion matrix]] of true and false positives and negatives by a single number, the Matthews correlation coefficient is generally regarded as being one of the best such measures.&lt;ref name="Powers2011"/&gt; Other measures, such as the proportion of correct predictions (also termed [[accuracy]]), are not useful when the two classes are of very different sizes. For example, assigning every object to the larger set achieves a high proportion of correct predictions, but is not generally a useful classification.

The MCC can be calculated directly from the [[confusion matrix]] using the formula:

: &lt;math&gt;
\text{MCC} = \frac{ TP \times TN - FP \times FN } {\sqrt{ (TP + FP) ( TP + FN ) ( TN + FP ) ( TN + FN ) } }
&lt;/math&gt;

In this equation, ''TP'' is the number of [[true positive]]s, ''TN'' the number of [[true negative]]s, ''FP'' the number of [[false positive]]s and ''FN'' the number of [[false negative]]s. If any of the four sums in the denominator is zero, the denominator can be arbitrarily set to one; this results in a Matthews correlation coefficient of zero, which can be shown to be the correct limiting value.

The original formula as given by Matthews was:&lt;ref name=Matthews1975 /&gt;
: &lt;math&gt;
\text{N} = TN + TP + FN + FP
&lt;/math&gt;
: &lt;math&gt;
\text{S} = \frac{ TP + FN } { N }
&lt;/math&gt;
: &lt;math&gt;
\text{P} = \frac{ TP + FP } { N }
&lt;/math&gt;
: &lt;math&gt;
\text{MCC} = \frac{ TP / N - S \times P } {\sqrt{ P S  ( 1 - S)  ( 1 - P ) } }
&lt;/math&gt;

This is equal to the formula given above. As a [[Correlation and dependence|correlation coefficient]], the Matthews correlation coefficient is the [[geometric mean]] of the [[regression coefficient]]s of the problem and its [[Dual (mathematics)|dual]]. The component regression coefficients of the Matthews correlation coefficient are [[Markedness]] (Δp) and [[Youden's J statistic]] ([[Informedness]] or Δp').&lt;ref name="Powers2011"&gt;{{cite journal |first=David M W |last=Powers |date=2011 |title=Evaluation: From Precision, Recall and F-Measure  to ROC, Informedness, Markedness &amp; Correlation |journal=Journal of Machine Learning Technologies |volume=2 |issue=1 |pages=37–63 |url=http://www.flinders.edu.au/science_engineering/fms/School-CSEM/publications/tech_reps-research_artfcts/TRRA_2007.pdf}}&lt;/ref&gt;&lt;ref name="Perruchet2004"&gt;{{cite journal |first1=P. |last1=Perruchet |first2=R. |last2=Peereman |year=2004 |title=The exploitation of distributional information in syllable processing |journal=J. Neurolinguistics |volume=17 |pages=97–119 |doi=10.1016/s0911-6044(03)00059-9}}&lt;/ref&gt; [[Markedness]] and [[Informedness]] correspond to different directions of information flow and generalize [[Youden's J statistic]], the deltap statistics and (as their geometric mean) the Matthews Correlation Coefficient to more than two classes.&lt;ref name="Powers2011"/&gt;

== Confusion Matrix ==
{{main article|Confusion matrix}}

{{Confusion matrix terms|recall=}}

Let us define an experiment from '''P''' positive instances and '''N''' negative instances for some condition. The four outcomes can be formulated in a 2×2 ''[[contingency table]]'' or ''[[confusion matrix]]'', as follows:

{{DiagnosticTesting_Diagram}}

== Multiclass Case ==
The Matthews correlation coefficient has been generalized to the multiclass case. This generalization was called the  &lt;math&gt;R_K&lt;/math&gt; statistic (for K different classes) by the author, and defined in terms of a &lt;math&gt;K\times K&lt;/math&gt; confusion matrix &lt;math&gt;C&lt;/math&gt;
&lt;ref name="gorodkin2004comparing"&gt;{{cite journal|last=Gorodkin|first=Jan|title=Comparing two K-category assignments by a K-category correlation coefficient|journal=Computational biology and chemistry|date=2004|volume=28|number=5|pages=367–374|publisher=Elsevier}}&lt;/ref&gt;
.&lt;ref name="GorodkinRk2006"&gt;{{cite web|last1=Gorodkin|first1=Jan|title=The Rk Page|url=http://rk.kvl.dk/introduction/index.html|website=The Rk Page|accessdate=28 December 2016}}&lt;/ref&gt;

:&lt;math&gt;
\text{MCC} = \frac{\sum_{k}\sum_{l}\sum_{m} C_{kk}C_{lm} - C_{kl}C_{mk}}{
\sqrt{
\sum_{k}(\sum_l C_{kl} )(\sum_{k' | k' \neq k}\sum_{l'} C_{k'l'})
}
\sqrt{
\sum_{k}(\sum_l C_{lk} )(\sum_{k' | k' \neq k}\sum_{l'} C_{l'k'})
}
}
&lt;/math&gt;

When there are more than two labels the MCC will no longer range between -1 and +1. Instead the minimum value will be between -1 and 0 depending on the true distribution. The maximum value is always +1. 

&lt;!-- 
TODO: potentially un-comment later, for now just stick with referenced version

This formula can be more easily understood by defining intermediate variables: 
* &lt;math&gt;t_k=\sum_i C_{ik}&lt;/math&gt; the number of times class k truly occurred, 
* &lt;math&gt;p_k=\sum_i C_{ki}&lt;/math&gt; the number of times class k was predicted, 
* &lt;math&gt;c=\sum_{k} C_{kk}&lt;/math&gt; the total number of samples correctly predicted, 
* &lt;math&gt;s=\sum_i \sum_j C_{ij}&lt;/math&gt; the total number of samples. This allows the formula to be expressed as:

:&lt;math&gt;
\text{MCC} = \frac{cs - \vec{t} \cdot \vec{p}}{
\sqrt{s^2 - \vec{p} \cdot \vec{p}}
\sqrt{s^2 - \vec{t} \cdot \vec{t}}
}
&lt;/math&gt;
--&gt;

== See also ==
* [[Phi coefficient]]
* [[F1 score]]
* [[Cramér's V (statistics)|Cramér's V]], a similar measure of association between nominal variables.
* [[Cohen's kappa]]

== References ==

{{Reflist}}

&lt;!--should reference in the main text  === General References ===
* [[Pierre Baldi|Baldi, P.]]; Brunak, S.; Chauvin, Y.; Andersen, C. A. F.; Nielsen, H. Assessing the accuracy of prediction algorithms for classification: an overview" ''Bioinformatics'' 2000, 16, 412&amp;ndash;424. [http://bioinformatics.oxfordjournals.org/cgi/content/abstract/16/5/412]
* Carugo, O., Detailed estimation of bioinformatics prediction reliability through the Fragmented Prediction Performance Plots. BMC Bioinformatics 2007. [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2148069/]
--&gt;

{{DEFAULTSORT:Matthews Correlation Coefficient}}
[[Category:Machine learning]]
[[Category:Information retrieval evaluation]]
[[Category:Statistical classification]]
[[Category:Computational chemistry]]
[[Category:Cheminformatics]]
[[Category:Bioinformatics]]
[[Category:Statistical ratios]]
[[Category:Summary statistics for contingency tables]]</text>
      <sha1>tw7trbdj0pd9182rxy5aap9zj8uu3vt</sha1>
    </revision>
  </page>
  <page>
    <title>Universal IR Evaluation</title>
    <ns>0</ns>
    <id>26591446</id>
    <revision>
      <id>716876883</id>
      <parentid>641384455</parentid>
      <timestamp>2016-04-24T12:38:09Z</timestamp>
      <contributor>
        <username>Noyster</username>
        <id>19396915</id>
      </contributor>
      <comment>add lead sentence</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3840" xml:space="preserve">{{Multiple issues|
{{refimprove|date=April 2011}}
{{orphan|date=April 2010}}
}}

In [[computer science]], ''universal [[Information retrieval evaluation|IR (information retrieval) evaluation]]'' aims to develop measures of database retrieval performance that shall be comparable across all information retrieval tasks.

==Measures of "relevance"==
[[Information retrieval evaluation|IR (information retrieval) evaluation]] begins whenever a user submits a query (search term) to a [[database]]. If the user is able to determine the [[Relevance (information retrieval)|relevance]] of each document in the database (relevant or not relevant), then for each query, the complete set of documents is naturally divided into four distinct (mutually exclusive) subsets: relevant documents that are retrieved, not relevant documents that are retrieved, relevant documents that are not retrieved, and not relevant documents that are not retrieved. These four subsets (of documents) are denoted by the letters a,b,c,d respectively and are called Swets variables, named after their inventor.&lt;ref&gt;Swets, J.A. (1969). Effectiveness of information retrieval methods. ''American Documentation, 20''(1), 72-89.&lt;/ref&gt;

In addition to the Swets definitions, four relevance metrics have also been defined: [[Precision (information retrieval)|Precision]] refers to the fraction of relevant documents that are retrieved (a/(a+b)), and [[Precision (information retrieval)|Recall]] refers to the fraction of retrieved documents that are relevant (a/(a+c)). These are the most commonly used and well-known relevance metrics found in the IR evaluation literature. Two less commonly used metrics include the Fallout, i.e., the fraction of not relevant documents that are retrieved (b/(b+d)), and the Miss, which refers to the fraction of relevant documents that are not retrieved (c/(c+d)) during any given search.

==Universal IR evaluation techniques==
Universal IR evaluation addresses the mathematical possibilities and relationships among the four relevance metrics Precision, Recall, Fallout and Miss, denoted by P, R, F and M, respectively. One aspect of the problem involves finding a mathematical derivation of a complete set of universal IR evaluation points.&lt;ref&gt;Schatkun, M. (2010). A Second look at Egghe's universal IR surface and a simple derivation of a complete set of universal IR evaluation points. ''Information Processing &amp; Management, 46''(1), 110-114.&lt;/ref&gt; The complete set of 16 points, each one a quadruple of the form (P,R,F,M), describes all the possible universal IR outcomes. For example, many of us have had the experience of querying a database and not retrieving any documents at all. In this case, the Precision would take on the undetermined form 0/0, the Recall and Fallout would both be zero, and the Miss would be any value greater than zero and less than one (assuming a mix of relevant and not relevant documents were in the database, none of which were retrieved). This universal IR evaluation point would thus be denoted by (0/0, 0, 0, M), which represents only one of the 16 possible universal IR outcomes.

The mathematics of universal IR evaluation is a fairly new subject since the relevance metrics P,R,F,M were not analyzed collectively until recently (within the past decade). A lot of the theoretical groundwork has already been formulated, but new insights in this area await discovery. For a detailed mathematical analysis, a query in the [[ScienceDirect]] database for "universal IR evaluation" retrieves several relevant peer-reviewed papers.

==See also==
* [[Information retrieval]]
* [[Web search query]]

==References==
{{Reflist}}

==External links==
* [http://www.sciencedirect.com Science Direct]

{{DEFAULTSORT:Universal Ir Evaluation}}
[[Category:Databases]]
[[Category:Information retrieval evaluation]]</text>
      <sha1>3k9c67chjch0ddw1mi1r68hbwm7d1vd</sha1>
    </revision>
  </page>
  <page>
    <title>Query likelihood model</title>
    <ns>0</ns>
    <id>29979321</id>
    <revision>
      <id>702405809</id>
      <parentid>685020836</parentid>
      <timestamp>2016-01-30T10:48:50Z</timestamp>
      <contributor>
        <username>Pyrexed</username>
        <id>27449922</id>
      </contributor>
      <minor />
      <comment>/* Calculating the likelihood */ The formula had confused N with M.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2580" xml:space="preserve">The '''query likelihood model''' is a [[language model]] used in [[information retrieval]]. A language model is constructed for each document in the collection.  It is then possible to rank each document by the probability of specific documents given a query. This is interpreted as being the [[Likelihood function|likelihood]] of a document being relevant given a query.

==Calculating the likelihood==
Using [[Bayes' theorem|Bayes' rule]], the probability &lt;math&gt;P&lt;/math&gt; of a document &lt;math&gt;d&lt;/math&gt;, given a query &lt;math&gt;q&lt;/math&gt; can be written as follows:

:&lt;math&gt;
 P(d|q) = \frac{P(q|d) P(d)}{P(q)}
&lt;/math&gt;

Since the probability of the query P(q) is the same for all documents, this can be ignored. Further, it is typical to assume that the probability of documents is uniform. Thus, P(d) is also ignored.

:&lt;math&gt;
 P(d|q) = P(q|d)
&lt;/math&gt;

Documents are then ranked by the probability that a query is observed as a random sample from the document model. The multinomial unigram language model is commonly used to achieve this. We have:
:&lt;math&gt;
 P(q|M_d) = K_q \prod_{t \in V} P(t|M_d)^{tf_{t,q}}
&lt;/math&gt;,where the multinomial coefficient is &lt;math&gt;K_q = L_q!/(tf_{t1,q}!tf_{t2,q}!...tf_{tN,q}!)&lt;/math&gt; for query {{math|q}}, 

and &lt;math&gt;L_q = \sum_{1 \leq i \leq N}tf_{t_i,q}&lt;/math&gt; is the length of query {{math|q}} given the term frequencies {{math|tf}} in the query vocabulary {{math|N}}.

In practice the multinomial coefficient is usually removed from the calculation. The reason is that it is a constant for a given bag of words (such as all the words from a specific document &lt;math&gt;d&lt;/math&gt;). The language model &lt;math&gt;M_d&lt;/math&gt; should be the true language model calculated from the distribution of words underlying each retrieved document. In practice this language model is unknown, so it is usually approximated by considering each term (unigram) from the retrieved document together with its probability of appearance. So &lt;math&gt;P(t|M_d)&lt;/math&gt; is the probability of term &lt;math&gt;t&lt;/math&gt; being generated by the language model &lt;math&gt;M_d&lt;/math&gt; of document &lt;math&gt;d&lt;/math&gt;. This probability is multiplied for all terms from query &lt;math&gt;q&lt;/math&gt; to get a rank for document &lt;math&gt;d&lt;/math&gt; in the interval &lt;math&gt;[0,1]&lt;/math&gt;. The calculation is repeated for all documents to create a ranking of all documents in the document collection.

&lt;ref&gt;Christopher D. Manning, Prabhakar Raghavan, Hinrich Schütze: An Introduction to Information Retrieval, page 241. Cambridge University Press, 2009&lt;/ref&gt;

==References==
 &lt;references/&gt;

[[Category:Information retrieval techniques]]</text>
      <sha1>kx74px07ic8x0mxxsbnerzf1yanz736</sha1>
    </revision>
  </page>
  <page>
    <title>Divergence-from-randomness model</title>
    <ns>0</ns>
    <id>1798853</id>
    <revision>
      <id>666714973</id>
      <parentid>592501389</parentid>
      <timestamp>2015-06-13T03:55:33Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>move to Category:Information retrieval techniques</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="577" xml:space="preserve">In the field of [[information retrieval]], '''divergence from randomness''' is one type of [[probabilistic]] model.

Term weights are computed by measuring the divergence between a term distribution produced by a random process and the actual term distribution.

==External links==
*[http://terrier.org/docs/v3.5/dfr_description.html Terrier's DFR Web page]
*[http://ir.dcs.gla.ac.uk/wiki/DivergenceFromRandomness Glasgow IR group Wiki DFR page]

[[Category:Ranking functions]]
[[Category:Information retrieval techniques]]
[[Category:Probabilistic models]]


{{comp-sci-stub}}</text>
      <sha1>9jqoeehbfeci7kbkrbs27u56bpjzbin</sha1>
    </revision>
  </page>
  <page>
    <title>Ordered weighted averaging aggregation operator</title>
    <ns>0</ns>
    <id>14893994</id>
    <revision>
      <id>744927318</id>
      <parentid>738491971</parentid>
      <timestamp>2016-10-18T08:35:28Z</timestamp>
      <contributor>
        <ip>82.150.248.37</ip>
      </contributor>
      <comment>Undid revision 738491971 by [[Special:Contributions/122.252.249.67|122.252.249.67]] ([[User talk:122.252.249.67|talk]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9910" xml:space="preserve">In applied mathematics – specifically in [[fuzzy logic]] – the '''ordered weighted averaging (OWA) operators''' provide a [[parameter]]ized class of mean type aggregation operators. They were introduced by [[Ronald R. Yager]]. Many notable mean operators such as the max, [[arithmetic average]], median and min, are members of this class. They have been widely used in [[computational intelligence]] because of their ability to model linguistically expressed aggregation instructions.

== Definition ==

Formally an '''OWA''' operator of dimension &lt;math&gt; \ n &lt;/math&gt; is a mapping &lt;math&gt; F: R_n \rightarrow R &lt;/math&gt; that has an associated collection of weights &lt;math&gt; \  W = [w_1, \ldots, w_n] &lt;/math&gt; lying in the unit interval and summing to one and with 		

:&lt;math&gt; F(a_1, \ldots , a_n) =  \sum_{j=1}^n  w_j b_j&lt;/math&gt;

where &lt;math&gt; b_j &lt;/math&gt; is the ''j''&lt;sup&gt;th&lt;/sup&gt; largest of the &lt;math&gt; a_i &lt;/math&gt;.

By choosing different ''W'' one can implement different aggregation operators. The OWA operator is a non-linear operator as a result of the process of determining the ''b''&lt;sub&gt;''j''&lt;/sub&gt;.

== Properties ==

The OWA operator is a mean operator. It is [[Bounded operator|bounded]], [[monotonic]], [[symmetric operator|symmetric]], and [[idempotent]], as defined below.

{|class="wikitable"
|[[Bounded operator|Bounded]]
|&lt;math&gt;   \min(a_1, \ldots, a_n) \le F(a_1, \ldots, a_n) \le \max(a_1, \ldots, a_n) &lt;/math&gt;
|-
|[[Monotonic]]
|&lt;math&gt;   F(a_1, \ldots, a_n) \ge F(g_1, \ldots, g_n) &lt;/math&gt; if &lt;math&gt; a_i \ge g_i &lt;/math&gt; for &lt;math&gt;\ i = 1,2,\ldots,n &lt;/math&gt;
|-
|[[symmetric operator|Symmetric]]
|&lt;math&gt;   F(a_1, \ldots, a_n)  = F(a_\boldsymbol{\pi(1)}, \ldots, a_\boldsymbol{\pi(n)})&lt;/math&gt; if &lt;math&gt;\boldsymbol{\pi} &lt;/math&gt; is a permutation map
|-
|[[Idempotent]]
|&lt;math&gt;  \ F(a_1, \ldots, a_n)  =  a &lt;/math&gt; if all &lt;math&gt; \ a_i = a &lt;/math&gt;
|}

== Notable OWA operators ==
:&lt;math&gt; \ F(a_1, \ldots, a_n) = \max(a_1, \ldots, a_n) &lt;/math&gt; if &lt;math&gt; \ w_1 = 1 &lt;/math&gt; and &lt;math&gt; \ w_j = 0 &lt;/math&gt; for &lt;math&gt; j \ne 1 &lt;/math&gt;

:&lt;math&gt; \ F(a_1, \ldots, a_n) = \min(a_1, \ldots, a_n) &lt;/math&gt; if &lt;math&gt; \ w_n = 1 &lt;/math&gt; and &lt;math&gt; \ w_j = 0 &lt;/math&gt; for &lt;math&gt; j \ne n &lt;/math&gt;

== Characterizing features ==

Two features have been used to characterize the OWA operators. The first is the attitudinal character(orness).

This is defined as
:&lt;math&gt;A-C(W)= \frac{1}{n-1} \sum_{j=1}^n (n - j) w_j. &lt;/math&gt;

It is known that &lt;math&gt; A-C(W) \in [0, 1] &lt;/math&gt;.

In addition ''A''&amp;nbsp;&amp;minus;&amp;nbsp;''C''(max) = 1, A&amp;nbsp;&amp;minus;&amp;nbsp;C(ave) = A&amp;nbsp;&amp;minus;&amp;nbsp;C(med) = 0.5 and A&amp;nbsp;&amp;minus;&amp;nbsp;C(min) = 0. Thus the A&amp;nbsp;&amp;minus;&amp;nbsp;C goes from 1 to 0 as we go from Max to Min aggregation. The attitudinal character characterizes the similarity of aggregation to OR operation(OR is defined as the Max).

The second feature is the dispersion. This defined as

:&lt;math&gt;H(W) = -\sum_{j=1}^n w_j \ln (w_j).&lt;/math&gt;

An alternative definition is &lt;math&gt;E(W) = \sum_{j=1}^n w_j^2 .&lt;/math&gt; The dispersion characterizes how uniformly the arguments are being used
ÀĚ

== A literature survey: OWA (1988-2014)==
The historical reconstruction of scientific development of the OWA field, the identification of the dominant direction of knowledge accumulation that emerged since the publication of the first OWA paper, and to discover the most active lines of research has recently been published, (see: http://onlinelibrary.wiley.com/doi/10.1002/int.21673/full). The results suggest, as expected, that Yager's paper[1] (IEEE Trans. Systems Man Cybernet, 18(1), 183–190, 1988) is the most influential paper and the starting point of all other research using OWA. Starting from his contribution, other lines of research developed and we describe them. Full list of papers published in OWA is also available at http://onlinelibrary.wiley.com/doi/10.1002/int.21673/full)

== Type-1 OWA aggregation operators ==

The above Yager's OWA operators are used to aggregate the crisp values. Can we aggregate fuzzy sets in the OWA mechanism ? The
'''[[Type-1 OWA operators]]''' have been proposed for this purpose. So the '''[[type-1 OWA operators]]''' provides us with a new technique for directly aggregating uncertain information with uncertain weights via OWA mechanism in soft decision making and data mining, where these uncertain objects are modelled by fuzzy sets.

The '''[[Type-1 OWA operators|type-1 OWA operator]]''' is defined according to the alpha-cuts of fuzzy sets as follows:

Given the ''n'' linguistic weights &lt;math&gt;\left\{ {W^i} \right\}_{i =1}^n &lt;/math&gt; in the form of fuzzy sets defined on the domain of discourse &lt;math&gt;U = [0,\;\;1]&lt;/math&gt;, then for each &lt;math&gt;\alpha \in [0,\;1]&lt;/math&gt;, an &lt;math&gt;\alpha &lt;/math&gt;-level type-1 OWA operator with &lt;math&gt;\alpha &lt;/math&gt;-level sets &lt;math&gt;\left\{ {W_\alpha ^i } \right\}_{i = 1}^n &lt;/math&gt; to aggregate the &lt;math&gt;\alpha &lt;/math&gt;-cuts of fuzzy sets &lt;math&gt;\left\{ {A^i} \right\}_{i =1}^n &lt;/math&gt; is given as

: &lt;math&gt;
\Phi_\alpha \left( {A_\alpha ^1 , \ldots ,A_\alpha ^n } \right) =\left\{ {\frac{\sum\limits_{i = 1}^n {w_i a_{\sigma (i)} } }{\sum\limits_{i = 1}^n {w_i } }\left| {w_i \in W_\alpha ^i ,\;a_i } \right. \in A_\alpha ^i ,\;i = 1, \ldots ,n} \right\}&lt;/math&gt;

where &lt;math&gt;W_\alpha ^i= \{w| \mu_{W_i }(w) \geq \alpha \}, A_\alpha ^i=\{ x| \mu _{A_i }(x)\geq \alpha \}&lt;/math&gt;, and &lt;math&gt;\sigma :\{\;1, \ldots ,n\;\} \to \{\;1, \ldots ,n\;\}&lt;/math&gt; is a permutation function such that &lt;math&gt;a_{\sigma (i)} \ge a_{\sigma (i + 1)} ,\;\forall \;i = 1, \ldots ,n - 1&lt;/math&gt;, i.e., &lt;math&gt;a_{\sigma (i)} &lt;/math&gt; is the &lt;math&gt;i&lt;/math&gt;th largest
element in the set &lt;math&gt;\left\{ {a_1 , \ldots ,a_n } \right\}&lt;/math&gt;.

The computation of the '''[[Type-1 OWA operators|type-1 OWA]]''' output is implemented by computing the left end-points and right end-points of the intervals &lt;math&gt;\Phi _\alpha \left( {A_\alpha ^1 , \ldots ,A_\alpha ^n } \right)&lt;/math&gt;:
&lt;math&gt;\Phi _\alpha \left( {A_\alpha ^1 , \ldots ,A_\alpha ^n } \right)_{-} &lt;/math&gt; and &lt;math&gt;
\Phi _\alpha \left( {A_\alpha ^1 , \ldots ,A_\alpha ^n } \right)_ {+},&lt;/math&gt;
where &lt;math&gt;A_\alpha ^i=[A_{\alpha-}^i, A_{\alpha+}^i], W_\alpha ^i=[W_{\alpha-}^i, W_{\alpha+}^i]&lt;/math&gt;. Then membership function of resulting aggregation fuzzy set is:

:&lt;math&gt;\mu _{G} (x) = \mathop \vee _{\alpha :x \in \Phi _\alpha \left( {A_\alpha ^1 , \cdots
,A_\alpha ^n } \right)_\alpha } \alpha &lt;/math&gt;

For the left end-points, we need to solve the following programming problem:

:&lt;math&gt; \Phi _\alpha \left( {A_\alpha ^1 , \cdots ,A_\alpha ^n } \right)_{-} = \min\limits_{\begin{array}{l} W_{\alpha - }^i \le w_i \le W_{\alpha + }^i A_{\alpha - }^i \le a_i \le A_{\alpha + }^i  \end{array}} \sum\limits_{i = 1}^n {w_i a_{\sigma (i)} / \sum\limits_{i = 1}^n {w_i } } &lt;/math&gt;

while for the right end-points, we need to solve the following programming problem:

:&lt;math&gt;\Phi _\alpha \left( {A_\alpha ^1 , \cdots , A_\alpha ^n } \right)_{+} = \max\limits_{\begin{array}{l} W_{\alpha - }^i \le w_i \le W_{\alpha + }^i  A_{\alpha - }^i \le a_i \le A_{\alpha + }^i  \end{array}} \sum\limits_{i = 1}^n {w_i a_{\sigma (i)} / \sum\limits_{i =
1}^n {w_i } } &lt;/math&gt;

[http://dx.doi.org/10.1109/TKDE.2010.191 This paper] has presented a fast method to solve two programming problem so that the type-1 OWA aggregation operation can be performed efficiently.

== References ==

* Yager, R. R., "On ordered weighted averaging aggregation operators in multi-criteria decision making," IEEE Transactions on Systems, Man and Cybernetics 18, 183–190, 1988.
* Yager, R. R. and Kacprzyk, J., [http://www.amazon.com/dp/079239934X The Ordered Weighted Averaging Operators: Theory and Applications], Kluwer: Norwell, MA, 1997.
* Liu, X., "The solution equivalence of minimax disparity and minimum variance problems for OWA operators," International Journal of Approximate Reasoning 45, 68–81, 2007.
* Emrouznejad (2009) SAS/OWA: ordered weighted averaging in SAS optimization, Soft Computing [http://www.springerlink.com/content/7277l73334r108x5/]
* Emrouznejad, A. and M. Marra (2014), Ordered Weighted Averaging Operators 1988–2014: A citation-based literature survey, International Journal of Intelligent Systems, 29:994-1014 [http://onlinelibrary.wiley.com/doi/10.1002/int.21673/full  &amp; http://onlinelibrary.wiley.com/store/10.1002/int.21673/asset/supinfo/int21673-sup-0001-SupMat.docx?v=1&amp;s=c0d8bdd220a31c876eb5885521cfa16d191f334d].
* Torra, V. and Narukawa, Y., Modeling Decisions: Information Fusion and Aggregation Operators, Springer: Berlin, 2007.
* Majlender, P., "OWA operators with maximal Rényi entropy," Fuzzy Sets and Systems 155, 340–360, 2005.
* Szekely, G. J. and Buczolich, Z., " When is a weighted average of ordered sample elements a maximum likelihood estimator of the location parameter?" Advances in Applied Mathematics 10, 1989, 439–456.
* S.-M. Zhou, F. Chiclana, R. I. John and J. M. Garibaldi, "Type-1 OWA operators for aggregating uncertain information with uncertain weights induced by type-2 linguistic quantifiers," Fuzzy Sets and Systems, Vol.159, No.24, pp.&amp;nbsp;3281–3296, 2008 [http://dx.doi.org/10.1016/j.fss.2008.06.018]
* S.-M. Zhou, F. Chiclana, R. I. John and J. M. Garibaldi, "Alpha-level aggregation: a practical approach to type-1 OWA operation for aggregating uncertain information with applications to breast cancer treatments," IEEE Transactions on Knowledge and Data Engineering, vol. 23, no.10, 2011, pp.&amp;nbsp;1455–1468.[http://dx.doi.org/10.1109/TKDE.2010.191]
* S.-M. Zhou, R. I. John, F. Chiclana and J. M. Garibaldi, "On aggregating uncertain information by type-2 OWA operators for soft decision making," International Journal of Intelligent Systems, vol. 25, no.6, pp.&amp;nbsp;540–558, 2010.[http://dx.doi.org/10.1002/int.20420]

[[Category:Artificial intelligence]]
[[Category:Logic in computer science]]
[[Category:Fuzzy logic]]
[[Category:Information retrieval techniques]]</text>
      <sha1>05mic4dldwbuzqldfh0gk2p3xw0dkdl</sha1>
    </revision>
  </page>
  <page>
    <title>Stop words</title>
    <ns>0</ns>
    <id>1015600</id>
    <revision>
      <id>756242367</id>
      <parentid>756242223</parentid>
      <timestamp>2016-12-22T22:37:47Z</timestamp>
      <contributor>
        <ip>93.86.48.16</ip>
      </contributor>
      <comment>/* References */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4905" xml:space="preserve">{{distinguish|Safeword}}
In [[computing]], '''stop words''' are words which are filtered out before or after [[Natural language processing|processing of natural language]] data (text).&lt;ref&gt;{{Cite book | last1 = Rajaraman | first1 = A. | last2 = Ullman | first2 = J. D. | doi = 10.1017/CBO9781139058452.002 | chapter = Data Mining | title = Mining of Massive Datasets | pages = 1–17| year = 2011 | isbn = 9781139058452 | pmid =  | pmc = | url = http://i.stanford.edu/~ullman/mmds/ch1.pdf}}&lt;/ref&gt; Though '''stop words''' usually refer to the most common words in a language, there is no single universal list of stop words used by all [[natural language processing]] tools, and indeed not all tools even use such a list. Some tools specifically avoid removing these '''stop words''' to support [[phrase search]].

Any group of words can be chosen as the stop words for a given purpose. For some [[search engine]]s, these are some of the most common, short [[function word]]s, such as ''the'', ''is'', ''at'', ''which'', and ''on''. In this case, stop words can cause problems when searching for phrases that include them, particularly in names such as "[[The Who]]", "[[The The]]", or "[[Take That]]". Other search engines remove some of the most common words—including [[lexical word]]s, such as "want"—from a query in order to improve performance.&lt;ref&gt;[http://blog.stackoverflow.com/2008/12/podcast-32 Stackoverflow]: "One of our major performance optimizations for the "related questions" query is removing the top 10,000 most common English dictionary words (as determined by Google search) before submitting the query to the SQL Server 2008 full text engine. It’s shocking how little is left of most posts once you remove the top 10k English dictionary words. This helps limit and narrow the returned results, which makes the query dramatically faster".&lt;/ref&gt;

[[Hans Peter Luhn]], one of the pioneers in [[information retrieval]], is credited with coining the phrase and using the concept.&lt;ref&gt;{{Cite book|title = Keyword-in-Context Index for Technical Literature (KWIC Index)|last = Luhn|first = H. P.|publisher = International Business Machines Corp.|year = 1959|isbn = |location = Yorktown Heights, NY|pages = |doi = 10.1002/asi.5090110403}}&lt;/ref&gt; The phrase "stop word", which is not in Luhn's 1959 presentation, and the associated terms "stop list" and "stoplist" appear in the literature shortly afterwards.&lt;ref&gt;{{cite journal|last1=Flood|first1=Barbara J.|title=Historical note: The Start of a Stop List at Biological Abstracts|journal=Journal of the American Society for Information Science|date=1999|volume=50|issue=12|page=1066|doi=10.1002/(SICI)1097-4571(1999)50:12&lt;1066::AID-ASI5&gt;3.0.CO;2-A|url=http://dx.doi.org/10.1002/(SICI)1097-4571(1999)50:12&lt;1066::AID-ASI5&gt;3.0.CO;2-A|accessdate=16 February 2016}}&lt;/ref&gt;

A predecessor concept was used in creating some [[Bible concordance|concordance]]s. For example, the first Hebrew concordance, Me’ir nativ, contained a one-page list of unindexed words, with nonsubstantive prepositions and conjunctions which are similar to modern stop words.&lt;ref&gt;{{cite journal|last1=Weinberg|first1=Bella Hass|title=Predecessors of scientific indexing structures in the domain of religion|journal=Second Conference on the History and Heritage of Scientific and Technical Information Systems|date=2004|pages=126–134|url=https://www.asis.org/History/11-weinberg.pdf|accessdate=17 February 2016}}&lt;/ref&gt;

== See also ==
{{Div col|cols=3}}
* [[Text mining]]
* [[Concept mining]]
* [[Information extraction]]
* [[Natural language processing]]
* [[Query expansion]]
* [[Stemming]]
* [[Index (search engine)|Search engine indexing]]
* [[Poison words]]
* [[Function words]]
* [[Filler_(linguistics) | Filler]]
{{Div col end}}

==References==
{{Reflist|2}}

== External links ==
* [http://xpo6.com/list-of-english-stop-words/  List of English Stop Words (PHP array, CSV) ]
* [http://dev.mysql.com/doc/refman/5.5/en/fulltext-stopwords.html  Full-Text Stopwords in MySQL ]
* [http://www.textfixer.com/resources/common-english-words.txt English Stop Words (CSV)]
* [http://mail.sarai.net/private/prc/Week-of-Mon-20080204/001656.html Hindi Stop Words]
* [http://solariz.de/de/deutsche_stopwords.htm German Stop Words],[http://aniol-consulting.de/uebersicht-deutscher-stop-words/ German Stop Words and phrases], another list of [http://www.ranks.nl/stopwords/german.html German stop words]
* [[:pl:Wikipedia:Stopwords|Polish Stop Words]]
* [https://code.google.com/p/stop-words/ Collection of stop words in 29 languages] [https://web.archive.org/web/*/http://tonyb.sk/_my/ir/stop-words-collection-2014-02-24.zip]
* [http://www.text-analytics101.com/2014/10/all-about-stop-words-for-text-mining.html A Detailed Explanation of Stop Words by Kavita Ganesan]

{{Natural Language Processing}}
{{SearchEngineOptimization}}

[[Category:Information retrieval techniques]]</text>
      <sha1>kuho1bzliupsodxfii95nd3tpg59tw3</sha1>
    </revision>
  </page>
  <page>
    <title>Vocabulary mismatch</title>
    <ns>0</ns>
    <id>36749242</id>
    <revision>
      <id>666715749</id>
      <parentid>665814962</parentid>
      <timestamp>2015-06-13T04:06:49Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>clean up, move to Category:Information retrieval techniques</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2638" xml:space="preserve">{{refimprove|date=June 2015}}
'''Vocabulary mismatch''' is a common phenomenon in the usage of natural languages, occurring when different people name the same thing or concept differently.

Furnas et al. (1987) were perhaps the first to quantitatively study the vocabulary mismatch problem.&lt;ref&gt;Furnas, G., et al, The Vocabulary Problem in Human-System Communication, Communications of the ACM, 1987, 30(11), pp. 964-971.&lt;/ref&gt;  Their results show that on average 80% of the times different people (experts in the same field) will name the same thing differently.  There are usually tens of possible names that can be attributed to the same thing.  This research motivated the work on [[latent semantic indexing]].

The vocabulary mismatch between user created queries and relevant documents in a corpus causes the term mismatch problem in [[information retrieval]].  Zhao and Callan (2010)&lt;ref&gt;Zhao, L. and Callan, J., Term Necessity Prediction, Proceedings of the 19th ACM Conference on Information and Knowledge Management (CIKM 2010). Toronto, Canada, 2010.&lt;/ref&gt; were perhaps the first to quantitatively study the vocabulary mismatch problem in a retrieval setting.  Their results show that an average query term fails to appear in 30-40% of the documents that are relevant to the user query.  They also showed that this probability of mismatch is a central probability in one of the fundamental probabilistic retrieval models, the [[Binary Independence Model]].  They developed novel term weight prediction methods that can lead to potentially 50-80% accuracy gains in retrieval over strong keyword retrieval models.  Further research along the line shows that expert users can use Boolean Conjunctive Normal Form expansion to improve retrieval performance by 50-300% over unexpanded keyword queries.&lt;ref name="cnf"&gt;Zhao, L. and Callan, J., Automatic term mismatch diagnosis for selective query expansion, SIGIR 2012.&lt;/ref&gt;

== Techniques that solve mismatch ==

* [[Stemming]]
* [[Full-text indexing]] instead of only indexing keywords or abstracts
* Indexing text on inbound links from other documents (or other social tagging
* [[Query expansion]].  A 2012 study by Zhao and Callan&lt;ref name="cnf"/&gt; using expert created manual [[Conjunctive normal form]] queries has shown that searchonym expansion in the Boolean conjunctive normal form is much more effective than the traditional bag of word expansion e.g. [[Rocchio algorithm|Rocchio expansion]].
* Translation-based models

== References ==

{{Reflist}}

[[Category:Linguistic research]]
[[Category:Information retrieval techniques]]
[[Category:Natural language processing]]</text>
      <sha1>2lvjw6kza9z27499yrq0fv4wvmakz4u</sha1>
    </revision>
  </page>
  <page>
    <title>Type-1 OWA operators</title>
    <ns>0</ns>
    <id>33591382</id>
    <revision>
      <id>730987765</id>
      <parentid>730928859</parentid>
      <timestamp>2016-07-22T04:55:21Z</timestamp>
      <contributor>
        <username>Alvin Seville</username>
        <id>8629244</id>
      </contributor>
      <comment>removing spaces</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10595" xml:space="preserve">{{Underlinked|date=July 2016}}

The [[Ordered weighted averaging aggregation operator|Yager's OWA (ordered weighted averaging) operators]]{{R|yagerOWA}} are used to aggregate the crisp values in decision making schemes (such as multi-criteria decision making, multi-expert decision making and  multi-criteria/multi-expert decision making).{{R|Yager|YagerBeliakov}} It is widely accepted that [[Fuzzy set]]s{{R|Zadeh}} are more suitable for representing preferences of criteria in decision making.

The type-1 OWA operators{{R|fssT1OWA|kdeT1OWA}} have been proposed for this purpose. The type-1 OWA operators provides a technique for directly aggregating uncertain information with uncertain weights via OWA mechanism in soft decision making and [[data mining]], where these uncertain objects are modelled by fuzzy sets.

The two definitions for type-1 OWA operators are based on Zadeh's Extension Principle and &lt;math&gt;\alpha&lt;/math&gt;-cuts of fuzzy sets. The two definitions lead to equivalent results.

==Definitions==

===Definition 1===
Let &lt;math&gt;F(X)&lt;/math&gt; be the set of fuzzy sets with domain of discourse &lt;math&gt;X&lt;/math&gt;, a type-1 OWA operator is defined as follows:{{R|kdeT1OWA}}

Given n linguistic weights &lt;math&gt;\left\{ {W^i} \right\}_{i = 1}^n &lt;/math&gt; in the form of fuzzy sets defined on the domain of discourse &lt;math&gt;U = [0,1]&lt;/math&gt;, a type-1 OWA operator is a mapping, &lt;math&gt;\Phi&lt;/math&gt;,

:&lt;math&gt;\Phi \colon F(X)\times \cdots \times F(X)  \longrightarrow  F(X)&lt;/math&gt;
:&lt;math&gt;(A^1 , \cdots ,A^n)  \mapsto   Y&lt;/math&gt;

such that

:&lt;math&gt;\mu _{Y} (y) =\displaystyle \sup_{\displaystyle \sum_{k =1}^n \bar {w}_i a_{\sigma (i)}  = y }\left({\begin{array}{*{1}l}\mu _{W^1 } (w_1 )\wedge \cdots \wedge \mu_{W^n } (w_n )\wedge \mu _{A^1 } (a_1 )\wedge \cdots \wedge \mu _{A^n } (a_n )\end{array}}\right)&lt;/math&gt;

where &lt;math&gt;\bar {w}_i = \frac{w_i }{\sum_{i = 1}^n {w_i } }&lt;/math&gt;,and &lt;math&gt;\sigma \colon \{1, \cdots ,n\} \longrightarrow \{1, \cdots ,n\}&lt;/math&gt; is a permutation function such that &lt;math&gt;a_{\sigma (i)} \geq a_{\sigma (i + 1)},\ \forall i = 1, \cdots ,n - 1&lt;/math&gt;, i.e., &lt;math&gt;a_{\sigma(i)} &lt;/math&gt; is the &lt;math&gt;i&lt;/math&gt;th highest element in the set &lt;math&gt;\left\{ {a_1 , \cdots ,a_n } \right\}&lt;/math&gt;.

===Definition 2===
Using the alpha-cuts of fuzzy sets:{{R|kdeT1OWA}}

Given the n linguistic weights &lt;math&gt;\left\{ {W^i} \right\}_{i =1}^n &lt;/math&gt; in the form of fuzzy sets defined on the domain of discourse &lt;math&gt;U = [0,\;\;1]&lt;/math&gt;, then for each &lt;math&gt;\alpha \in [0,\;1]&lt;/math&gt;, an &lt;math&gt;\alpha &lt;/math&gt;-level type-1 OWA operator with &lt;math&gt;\alpha &lt;/math&gt;-level sets &lt;math&gt;\left\{ {W_\alpha ^i } \right\}_{i = 1}^n &lt;/math&gt; to aggregate the &lt;math&gt;\alpha &lt;/math&gt;-cuts of fuzzy sets &lt;math&gt;\left\{ {A^i} \right\}_{i =1}^n &lt;/math&gt; is:

: &lt;math&gt;
\Phi_\alpha \left( {A_\alpha ^1 , \ldots ,A_\alpha ^n } \right) =\left\{ {\frac{\sum\limits_{i = 1}^n {w_i a_{\sigma (i)} } }{\sum\limits_{i = 1}^n {w_i } }\left| {w_i \in W_\alpha ^i ,\;a_i } \right. \in A_\alpha ^i ,\;i = 1, \ldots ,n} \right\}&lt;/math&gt;

where  &lt;math&gt;W_\alpha ^i= \{w| \mu_{W_i }(w) \geq \alpha \}, A_\alpha ^i=\{ x| \mu _{A_i }(x)\geq \alpha \}&lt;/math&gt;, and &lt;math&gt;\sigma :\{\;1, \cdots ,n\;\} \to \{\;1, \cdots ,n\;\}&lt;/math&gt; is a permutation function such that &lt;math&gt;a_{\sigma (i)} \ge a_{\sigma (i + 1)} ,\;\forall \;i = 1, \cdots ,n - 1&lt;/math&gt;, i.e., &lt;math&gt;a_{\sigma (i)} &lt;/math&gt; is the &lt;math&gt;i&lt;/math&gt;th largest
element in the set &lt;math&gt;\left\{ {a_1 , \cdots ,a_n } \right\}&lt;/math&gt;.

== Representation theorem of Type-1 OWA operators==

Given the ''n'' linguistic weights &lt;math&gt;\left\{ {W^i} \right\}_{i =1}^n &lt;/math&gt; in the form of fuzzy sets defined on the domain of discourse &lt;math&gt;U = [0,\;\;1]&lt;/math&gt;, and the fuzzy sets &lt;math&gt;A^1, \cdots ,A^n&lt;/math&gt;, then we have that{{R|kdeT1OWA}}
:&lt;math&gt;Y=G&lt;/math&gt;

where &lt;math&gt;Y&lt;/math&gt; is the aggregation result obtained by Definition 1, and &lt;math&gt;G&lt;/math&gt; is the result obtained by in Definition 2.

==Programming problems for Type-1 OWA operators==

According to the Representation Theorem of Type-1 OWA Operators, a general type-1 OWA operator can be decomposed into a series of &lt;math&gt;\alpha&lt;/math&gt;-level type-1 OWA operators. In practice, this series of  &lt;math&gt;\alpha&lt;/math&gt;-level type-1 OWA operators is used to construct the resulting aggregation fuzzy set. So we only need to compute the left end-points and right end-points of the intervals &lt;math&gt;\Phi _\alpha \left( {A_\alpha ^1 , \cdots ,A_\alpha ^n } \right)&lt;/math&gt;. Then, the resulting aggregation fuzzy set is constructed with the membership function as follows:

:&lt;math&gt;\mu _{G} (x) = \operatorname{ \bigvee} \limits_{\alpha :x \in \Phi _\alpha \left( {A_\alpha ^1 , \cdots
,A_\alpha ^n } \right)_\alpha } \alpha &lt;/math&gt;

For the left end-points, we need to solve the following programming problem:
:&lt;math&gt; \Phi _\alpha \left( {A_\alpha ^1 , \cdots ,A_\alpha ^n } \right)_{-} = \operatorname {\min }\limits_{\begin{array}{l} W_{\alpha - }^i \le w_i \le W_{\alpha + }^i A_{\alpha - }^i \le a_i \le A_{\alpha + }^i  \end{array}} \sum\limits_{i = 1}^n {w_i a_{\sigma (i)} / \sum\limits_{i = 1}^n {w_i } } &lt;/math&gt;

while for the right end-points, we need to solve the following programming problem:
:&lt;math&gt;\Phi _\alpha \left( {A_\alpha ^1 , \cdots , A_\alpha ^n } \right)_{+} = \operatorname {\max }\limits_{\begin{array}{l} W_{\alpha - }^i \le w_i \le W_{\alpha + }^i  A_{\alpha - }^i \le a_i \le A_{\alpha + }^i  \end{array}} \sum\limits_{i = 1}^n {w_i a_{\sigma (i)} / \sum\limits_{i =
1}^n {w_i } } &lt;/math&gt;

A fast method has been presented to solve two programming problem so that the type-1 OWA aggregation operation can be performed efficiently, for details, please see the paper.{{R|kdeT1OWA}}

==Alpha-level approach to Type-1 OWA operation==

Three-step process:{{R|kdeT1OWA}}
* Step 1&amp;mdash;To set up the &lt;math&gt;\alpha &lt;/math&gt;- level resolution in [0, 1].
* Step 2&amp;mdash;For each &lt;math&gt;\alpha \in [0,1]&lt;/math&gt;,
:*Step 2.1&amp;mdash;To calculate &lt;math&gt;\rho _{\alpha +} ^{i_0^\ast } &lt;/math&gt;
# Let &lt;math&gt;i_0 = 1&lt;/math&gt;;
# If &lt;math&gt;\rho _{\alpha +} ^{i_0 } \ge A_{\alpha + }^{\sigma (i_0 )} &lt;/math&gt;, stop, &lt;math&gt;\rho _{\alpha +} ^{i_0 } &lt;/math&gt; is the solution; otherwise go to Step 2.1-3.
# &lt;math&gt;i_0 \leftarrow i_0 + 1&lt;/math&gt;, go to Step 2.1-2.

:*Step 2.2 To calculate&lt;math&gt;\rho _{\alpha -} ^{i_0^\ast } &lt;/math&gt;
# Let &lt;math&gt;i_0 = 1&lt;/math&gt;;
# If &lt;math&gt;\rho _{\alpha -} ^{i_0 } \ge A_{\alpha - }^{\sigma (i_0 )} &lt;/math&gt;, stop, &lt;math&gt;\rho _{\alpha -} ^{i_0 } &lt;/math&gt; is the solution; otherwise go to Step 2.2-3.
#&lt;math&gt;i_0 \leftarrow i_0 + 1&lt;/math&gt;, go to step Step 2.2-2.

* Step 3&amp;mdash;To construct the aggregation resulting fuzzy set &lt;math&gt;G&lt;/math&gt; based on all the available intervals &lt;math&gt;\left[ {\rho _{\alpha -} ^{i_0^\ast } ,\;\rho _{\alpha +} ^{i_0^\ast } } \right]&lt;/math&gt;:

:&lt;math&gt;\mu _{G} (x) = \operatorname \bigvee \limits_{\alpha :x \in \left[ {\rho _{\alpha -} ^{i_0^\ast } ,\;\rho _{\alpha +} ^{i_0^\ast } } \right]} \alpha &lt;/math&gt;

==Special cases==
* Any OWA operators, like maximum, minimum, mean operators;&lt;ref name="yagerOWA" /&gt;
* Join operators of (type-1) fuzzy sets,{{R|MT}} i.e., fuzzy maximum operators;
* Meet operators of (type-1) fuzzy sets,{{R|MT|zadehJ}} i.e., fuzzy minimum operators;
* Join-like operators of (type-1) fuzzy sets;{{R|kdeT1OWA|bookT1OWA}}
* Meet-like operators of (type-1) fuzzy sets.{{R|kdeT1OWA|bookT1OWA}}

==Generalizations==
Type-2 OWA operators{{R|Zhou}} have been suggested to aggregate the [[Type-2 fuzzy sets and systems|type-2 fuzzy sets]] for soft decision making.

==References==
{{reflist|30em|refs=

&lt;ref name="yagerOWA"&gt;{{cite journal|last=Yager|first=R.R|title=On ordered weighted averaging aggregation operators in multi-criteria decision making|journal=IEEE Transactions on Systems, Man and Cybernetics|year=1988|volume=18|pages=183–190|doi=10.1109/21.87068}}&lt;/ref&gt;

&lt;ref name=Yager&gt;{{cite book|last=Yager|first=R. R. and Kacprzyk, J|title=The Ordered Weighted Averaging Operators: Theory and Applications|year=1997|publisher=Kluwer: Norwell, MA}}&lt;/ref&gt;

&lt;ref name=YagerBeliakov&gt;{{cite book|last=Yager|first=R.R, Kacprzyk, J. and Beliakov, G|title=Recent Developments in the Ordered Weighted Averaging Operators-Theory and Practice|year=2011|publisher=Springer}}&lt;/ref&gt;

&lt;ref name=Zadeh&gt;{{cite journal|last=Zadeh|first=L.A|title=Fuzzy sets|journal=Information and Control |year=1965|volume=8 |pages=338–353|doi=10.1016/S0019-9958(65)90241-X}}&lt;/ref&gt;

&lt;ref name="kdeT1OWA"&gt;{{cite journal|last=Zhou|first=S. M. |author2=F. Chiclana |author3=R. I. John |author4=J. M. Garibaldi|title=Alpha-level aggregation: a practical approach to type-1 OWA operation for aggregating uncertain information with applications to breast cancer treatments|journal=IEEE Transactions on Knowledge and Data Engineering|year=2011|volume=23|issue=10|pages=1455–1468|doi=10.1109/TKDE.2010.191}}&lt;/ref&gt;

&lt;ref name="fssT1OWA"&gt;{{cite journal|last=Zhou|first=S. M. |author2=F. Chiclana |author3=R. I. John |author4=J. M. Garibaldi|title=Type-1 OWA operators for aggregating uncertain information with uncertain weights induced by type-2 linguistic quantifiers|journal=Fuzzy Sets and Systems|year=2008|volume=159|issue=24|pages=3281–3296|doi=10.1016/j.fss.2008.06.018}}&lt;/ref&gt;

&lt;ref name="MT"&gt;{{cite journal|last=Mizumoto|first=M.|author2=K. Tanaka |title=Some Properties of fuzzy sets of type 2|journal=Information and Control|year=1976|volume=31|pages=312–40|doi=10.1016/s0019-9958(76)80011-3}}&lt;/ref&gt;&lt;ref name="zadehJ"&gt;{{cite journal|last=Zadeh|first=L. A.|title=The concept of a linguistic variable and its application to approximate reasoning-1|journal=Information Sciences|year=1975|volume=8|pages=199–249|doi=10.1016/0020-0255(75)90036-5}}&lt;/ref&gt;

&lt;ref name="bookT1OWA"&gt;{{cite journal|last=Zhou|first=S. M.|author2=F. Chiclana |author3=R. I. John |author4=J. M. Garibaldi |title=Fuzzificcation of the OWA Operators in Aggregating Uncertain Information|journal=R. R. Yager, J. Kacprzyk and G. Beliakov (ed): Recent Developments in the Ordered Weighted Averaging Operators-Theory and Practice|year=2011|volume=Springer|pages=91–109|doi=10.1007/978-3-642-17910-5_5}}&lt;/ref&gt;

&lt;ref name=Zhou&gt;{{cite journal|last=Zhou|first=S.M. |author2=R. I. John |author3=F. Chiclana |author4=J. M. Garibaldi|title=On aggregating uncertain information by type-2 OWA operators for soft decision making|journal=International Journal of Intelligent Systems|year=2010|volume=25|issue=6|pages=540–558|doi=10.1002/int.20420}}&lt;/ref&gt;
}}

[[Category:Artificial intelligence]]
[[Category:Fuzzy logic]]
[[Category:Information retrieval techniques]]
[[Category:Logic in computer science]]</text>
      <sha1>6cbnswid9iqo62j67052btmyserl2dy</sha1>
    </revision>
  </page>
  <page>
    <title>Tag (metadata)</title>
    <ns>0</ns>
    <id>1707086</id>
    <revision>
      <id>760969232</id>
      <parentid>758748634</parentid>
      <timestamp>2017-01-20T02:54:01Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* Triple tags */HTTP&amp;rarr;HTTPS for [[Yahoo!]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="21508" xml:space="preserve">{{hatnote|Not to be confused with [[Markup language]] or [[HTML element]] tags.}}
[[File:Web 2.0 Map.svg|thumb|right|250px|A [[tag cloud]] with terms related to [[Web 2.0]]]]

In [[information system]]s, a '''tag''' is a non-hierarchical [[index term|keyword or term]] assigned to a piece of information (such as an [[Bookmark (World Wide Web)|Internet bookmark]], digital image, or [[computer file]]). This kind of [[metadata]] helps describe an item and allows it to be found again by browsing or searching. Tags are generally chosen informally and personally by the item's creator or by its viewer, depending on the system.

Tagging was popularized by websites associated with [[Web 2.0]] and is an important feature of many Web 2.0 services. It is now also part of some desktop software.

==History==

Labeling and tagging are carried out to perform functions such as aiding in [[Classification (machine learning)|classification]], marking ownership, noting boundaries, and indicating [[online identity]]. They may take the form of words, images, or other identifying marks. An analogous example of tags in the physical world is museum object tagging. In the organization of information and objects, the use of textual keywords as part of identification and classification long  predates computers. However, computer based searching made the use of keywords a rapid way of exploring records.

[[File:A Description of the Equator and Some Otherlands, collaborative hypercinema portal Upload page.jpg|thumb|A Description of the Equator and Some Otherlands, collaborative hypercinema portal, produced by documenta X, 1997. User upload page associating user contributed media with the term ''Tag''.]] Online and Internet databases and early websites deployed them as a way for publishers to help users find content. In 1997, the collaborative portal "A Description of the Equator and Some Other Lands" produced by [[documenta]] X, Germany, coined the folksonomic term ''Tag'' for its co-authors and guest authors on its Upload page. In "The Equator" the term ''Tag'' for user-input was described as an ''abstract literal or keyword'' to aid the user. Turned out in Web 1.0 days, all "Otherlands" users defined singular ''Tags'', and did not share ''Tags'' at that point.

In 2003, the [[social bookmarking]] website [[Delicious (website)|Delicious]] provided a way for its users to add "tags" to their bookmarks (as a way to help find them later); Delicious also provided browseable aggregated views of the bookmarks of all users featuring a particular tag.&lt;ref&gt;[http://flickr.com/photos/joshu/765809051/in/set-72157600740166824/ Screenshot of tags on del.icio.us] in 2004 and [http://flickr.com/photos/joshu/765817375/in/set-72157600740166824/ Screenshot of a tag page on del.icio.us], also in 2004, both published by [[Joshua Schachter]] on July 9, 2007.&lt;/ref&gt; [[Flickr]] allowed its users to add their own text tags to each of their pictures, constructing flexible and easy metadata that made the pictures highly searchable.&lt;ref&gt;[http://www.adaptivepath.com/ideas/essays/archives/000519.php "An Interview with Flickr's Eric Costello"] by Jesse James Garrett, published on August 4, 2005. Quote: "Tags were not in the initial version of Flickr. Stewart Butterfield...liked the way they worked on del.icio.us, the social bookmarking application. We added very simple tagging functionality, so you could tag your photos, and then look at all your photos with a particular tag, or any one person’s photos with a particular tag."&lt;/ref&gt; The success of Flickr and the influence of Delicious popularized the concept,&lt;ref&gt;An example is [http://www.adammathes.com/academic/computer-mediated-communication/folksonomies.html "Folksonomies - Cooperative Classification and Communication Through Shared Metadata"] by Adam Mathes, December 2004. It focuses on tagging in Delicious and Flickr.&lt;/ref&gt; and other [[social software]] websites&amp;nbsp;– such as [[YouTube]], [[Technorati]], and [[Last.fm]]&amp;nbsp;– also implemented tagging. Other traditional and web applications have incorporated the concept such as "Labels" in [[Gmail]] and the ability to add and edit tags in [[iTunes]] or [[Winamp]].

Tagging has gained wide popularity due to the growth of social networking, photography sharing and bookmarking sites. These sites allow users to create and manage labels (or “tags”) that categorize content using simple keywords. The use of keywords as part of an identification and classification system long predates computers. In the early days of the web keywords meta tags were used by web page designers to tell search engines what the web page was about. Today's tagging takes the meta keywords concept and re-uses it. The users add the tags. The tags are clearly visible, and are themselves links to other items that share that keyword tag.

Knowledge tags are an extension of [[Index term|keyword]] tags. They were first used by [[Jumper 2.0]], an [[open source]] [[Web 2.0]] software platform released by Jumper Networks on 29 September 2008.&lt;ref&gt;{{Citation|url=http://www.jumpernetworks.com/ NEWS-Jumper_Networks_Releases_Jumper_2.0_Platform.pdf|title=Jumper Networks Press Release for Jumper 2.0|publisher=Jumper Networks, Inc.|date=29 September 2008}}&lt;/ref&gt; Jumper 2.0 was the first [[collaborative search engine]] platform to use a method of expanded tagging for [[knowledge capture]].

Websites that include tags often display collections of tags as [[tag cloud]]s. A user's tags are useful both to them and to the larger community of the website's users.

Tags may be a "bottom-up" type of classification, compared to [[hierarchy|hierarchies]], which are "top-down". In a traditional hierarchical system ([[Taxonomy (general)|taxonomy]]), the designer sets out a limited number of terms to use for classification, and there is one correct way to classify each item. In a tagging system, there are an unlimited number of ways to classify an item, and there is no "wrong" choice. Instead of belonging to one category, an item may have several different tags. Some researchers and applications have experimented with combining structured hierarchy and "flat" tagging to aid in information retrieval.&lt;ref&gt;[http://infolab.stanford.edu/~heymann/taghierarchy.html Tag Hierarchies], research notes by Paul Heymann.&lt;/ref&gt;

==Examples==

===Within a blog===
Many [[blog]] systems allow authors to add free-form tags to a post, along with (or instead of) placing the post into categories. For example, a post may display that it has been tagged with ''baseball'' and ''tickets''. Each of those tags is usually a [[web link]] leading to an index page listing all of the posts associated with that tag. The blog may have a sidebar listing all the tags in use on that blog, with each tag leading to an index page. To reclassify a post, an author edits its list of tags. All connections between posts are automatically tracked and updated by the blog software; there is no need to relocate the page within a complex hierarchy of categories.

===For an event===
An official tag is a keyword adopted by events and conferences for participants to use in their web publications, such as blog entries, photos of the event, and presentation slides. Search engines can then index them to make relevant materials related to the event searchable in a uniform way. In this case, the tag is part of a [[controlled vocabulary]].

===In research===
A researcher may work with a large collection of items (e.g. press quotes, a bibliography, images) in digital form. If he/she wishes to associate each with a small number of themes (e.g. to chapters of a book, or to sub-themes of the overall subject), then a group of tags for these themes can be attached to each of the items in the larger collection. In this way, free form [[categorization|classification]] allows the author to manage what would otherwise be unwieldy amounts of information. Commercial, as well as some free computer applications are readily available to do this.

==Special types==

===Triple tags===
{{see also|Microformat}}
A '''triple tag''' or '''machine tag''' uses a special [[syntax]] to define extra [[semantic]] information about the tag, making it easier or more meaningful for interpretation by a computer program. Triple tags comprise three parts: a [[namespace]], a [[wikt:predicate|predicate]], and a value. For example, "&lt;nowiki&gt;geo:long=50.123456&lt;/nowiki&gt;" is a tag for the geographical [[longitude]] coordinate whose value is 50.123456. This triple structure is similar to the [[Resource Description Framework]] model for information.

The triple tag format was first devised for geolicious&lt;ref&gt;{{cite web |url=http://brainoff.com/weblog/2004/11/05/124 |title=geo.lici.us: geotagging hosted services |first1=Mikel |last1=Maron |date=November 5, 2004}}&lt;/ref&gt; in November 2004, to map [[Delicious (website)|Delicious]] bookmarks, and gained wider acceptance after its adoption by [http://stamen.com/projects/mappr Mappr] and GeoBloggers&lt;ref&gt;[http://web.archive.org/web/20071011024028/http://geobloggers.com/archives/2006/01/11/advanced-tagging-and-tripletags/ Advanced Tagging and TripleTags] by Reverend Dan Catt, ''Geobloggers'', January 11, 2006.&lt;/ref&gt; to map [[Flickr]] photos. In January 2007, [[Aaron Straup Cope]] at [[Flickr]] introduced the term ''machine tag'' as an alternative name for the triple tag, adding some questions and answers on purpose, syntax, and use.&lt;ref&gt;[https://www.flickr.com/groups/api/discuss/72157594497877875/ Machine tags], a post by Aaron Straup Cope in the Flickr API group, January 24, 2007.&lt;/ref&gt;

Specialized metadata for geographical identification is known as ''[[geotagging]]''; machine tags are also used for other purposes, such as identifying photos taken at a specific event or naming species using [[binomial nomenclature]].&lt;ref&gt;[https://www.flickr.com/groups/encyclopedia_of_life/rules/ Encyclopedia of Life use of machine tag], The Encyclopedia of Life project rules including the required use of a taxonomy machine tag, September 19, 2009.&lt;/ref&gt;

===Hashtags===
{{main|Hashtag}}
A hashtag is a kind of metadata tag marked by the prefix &lt;code&gt;#&lt;/code&gt;, sometimes known as a "hash" symbol. This form of tagging is used on [[microblogging]] and [[social networking service]]s such as [[Twitter]], [[Facebook]], [[Google+]], [[VK (social network)|VK]] and [[Instagram]].

===Knowledge tags===
A knowledge tag is a type of [[metadata|meta-information]] that describes or defines some aspect of an information resource (such as a [[document]], [[digital image]], [[database table|relational table]], or [[web page]]). Knowledge tags are more than traditional non-hierarchical [[index term|keywords or terms]]. They are a type of [[metadata]] that captures knowledge in the form of descriptions, categorizations, classifications, [[semantics]], comments, notes, annotations, [[hyperdata]], [[hyperlinks]], or references that are collected in tag profiles. These tag profiles reference an information resource that resides in a distributed, and often heterogeneous, storage repository. Knowledge tags are a [[knowledge management]] discipline that leverages [[Enterprise 2.0]] methodologies for users to capture insights, expertise, attributes, dependencies, or relationships associated with a data resource. It generally allows greater flexibility than other [[knowledge management]] classification systems.

Capturing knowledge in tags takes many different forms, there is factual knowledge (that found in books and data), conceptual knowledge (found in perspectives and concepts), expectational knowledge (needed to make judgments and hypothesis), and methodological knowledge (derived from reasoning and strategies).&lt;ref&gt;
{{Citation
 | last=Wiig | first=K. M.
 | year= 1997
 | title=Knowledge Management: An Introduction and Perspective
 | journal=Journal of Knowledge Management
 | volume=1 | issue=1
 | pages=6–14
 | url=http://www.mendeley.com/c/67997727/Wiig-1997-Knowledge-Management-An-Introduction-and-Perspective/
 | doi=10.1108/13673279710800682
}}
&lt;/ref&gt; These forms of [[knowledge]] often exist outside the data itself and are derived from personal experience, insight, or expertise.

Knowledge tags, in fact, manifest themselves in any number of ways – conceptual knowledge tags describe procedures, lessons learned, and facts that are related to the information resource. [[Tacit knowledge]] tags, manifests itself through skills, habits or learning by doing and represent experience or organizational intelligence. Anecdotal knowledge, is a memory of a particular case or event that may not surface without context.&lt;ref&gt;
{{citation
 | last=Getting | first=Brian
 | year= 2007
 | title=What Are "Tags" And What Is "Tagging?
 | publisher=Practical eCommerce
 | url=http://www.practicalecommerce.com/articles/589
}}
&lt;/ref&gt;

Knowledge can best be defined as information possessed in the mind of an individual: it is personalized or subjective information related to facts, procedures, concepts, interpretations, ideas, observations and judgments (which may or may not be unique, useful, accurate, or structurable). Knowledge tags are considered an expansion of the information itself that adds additional value, context, and meaning to the information. Knowledge tags are valuable for preserving organizational intelligence that is often lost due to turn-over, for sharing knowledge stored in the minds of individuals that is typically isolated and unharnessed by the organization, and for connecting knowledge that is often lost or disconnected from an information resource.&lt;ref&gt;
{{Citation
 | last=Alavi | first=Maryam
 | last2=Leidner
 | year= 1999
 | title=Knowledge Management Systems: Issues, Challenges, and Benefits
 | journal=Communications of the Association for Information Systems
 | volume=1 | issue=7
 | url=http://www.belkcollege.uncc.edu/jpfoley/Readings/artic07.pdf
}}
&lt;/ref&gt;

==Advantages and disadvantages==
{{procon|date=November 2012}}

In a typical tagging system, there is no explicit information about the meaning or [[semantics]] of each tag, and a user can apply new tags to an item as easily as applying older tags. Hierarchical classification systems can be slow to change, and are rooted in the culture and era that created them.&lt;ref name="Smith2008"&gt;Smith, Gene (2008). Tagging: People-Powered Metadata for the Social Web. Berkeley, CA: New Riders. ISBN 0-321-52917-0&lt;/ref&gt; The flexibility of tagging allows users to classify their collections of items in the ways that they find useful, but the personalized variety of terms can present challenges when searching and browsing.

When users can freely choose tags (creating a [[folksonomy]], as opposed to selecting terms from a [[controlled vocabulary]]), the resulting metadata can include [[homonym]]s (the same tags used with different meanings) and [[synonym]]s (multiple tags for the same concept), which may lead to inappropriate connections between items and inefficient searches for information about a subject.&lt;ref&gt;Golder, Scott A. Huberman, Bernardo A. (2005).
"[http://arxiv.org/abs/cs.DL/0508082 The Structure of Collaborative Tagging Systems]." Information Dynamics Lab, HP Labs. Visited November 24, 2005.&lt;/ref&gt; For example, the tag "orange" may refer to the [[Orange (fruit)|fruit]] or the [[Orange (colour)|color]], and items related to a version of the [[Linux kernel]] may be tagged "Linux", "kernel", "Penguin", "software", or a variety of other terms. Users can also choose tags that are different [[inflection]]s of words (such as singular and plural),&lt;ref&gt;[http://keithdevens.com/weblog/archive/2004/Dec/24/SvP.tags Singular vs. plural tags in a tag-based categorization system] by Keith Devens, December 24, 2004.&lt;/ref&gt; which can contribute to navigation difficulties if the system does not include [[stemming]] of tags when searching or browsing. Larger-scale folksonomies address some of the problems of tagging, in that users of tagging systems tend to notice the current use of "tag terms" within these systems, and thus use existing tags in order to easily form connections to related items. In this way, folksonomies collectively develop a partial set of tagging conventions.

===Complex system dynamics===

Despite the apparent lack of control, research has shown that a simple form of shared vocabularies emerges in social bookmarking systems. Collaborative tagging exhibits a form of [[complex system]]s dynamics,&lt;ref name="WWW07-ref"&gt;Harry Halpin, Valentin Robu, Hana Shepherd [http://portal.acm.org/citation.cfm?id=1242572.1242602 The Complex Dynamics of Collaborative Tagging], Proceedings of the 16th International Conference on the World Wide Web (WWW'07), Banff, Canada, pp. 211-220, ACM Press, 2007. Downloadable on [http://www2007.org/papers/paper635.pdf the conference's website]&lt;/ref&gt; (or [[Self-organization|self organizing]] dynamics). Thus, even if no central controlled vocabulary constrains the actions of individual users, the distribution of tags that describe different resources (e.g., websites) converges over time to stable [[power law]] distributions.&lt;ref name="WWW07-ref"/&gt; Once such stable distributions form, simple vocabularies can be extracted by examining the [[correlation]]s that form between different tags.  This informal collaborative system of tag creation and management has been called a [[folksonomy]].

===Spamming===

Tagging systems open to the public are also open to tag spam, in which people apply an excessive number of tags or unrelated tags to an item (such as a [[YouTube]] video) in order to attract viewers. This abuse can be mitigated using human or statistical identification of spam items.&lt;ref&gt;[http://heymann.stanford.edu/tagspam.html Tag Spam], research notes by Paul Heymann.&lt;/ref&gt; The number of tags allowed may also be limited to reduce spam.

==Syntax==
Some tagging systems provide a single [[text box]] to enter tags, so to be able to [[tokenize]] the string, a [[Wiktionary:separator|separator]] must be used. Two popular separators are the [[Space (punctuation)|space character]] and the [[comma]]. To enable the use of separators in the tags, a system may allow for higher-level separators (such as [[quotation mark]]s) or [[escape character]]s. Systems can avoid the use of separators by allowing only one tag to be added to each input [[Web widget|widget]] at a time, although this makes adding multiple tags more time-consuming.

A syntax for use within [[HTML]] is to use the '''rel-tag''' [[microformat]] which uses the [[Rel attribute|''rel'' attribute]] with value "tag" (i.e., &lt;code&gt;rel="tag"&lt;/code&gt;) to indicate that the linked-to page acts as a tag for the current context.&lt;ref&gt;[http://microformats.org/wiki/rel-tag rel tag microformat specification], Microformats Wiki, January 10, 2005.&lt;/ref&gt;

==See also==
{{colbegin||27em}}
* [[Collective intelligence]]
* [[Concept map]]
* [[Enterprise 2.0]]
* [[Enterprise bookmarking]]
* [[Explicit knowledge]]
* [[Faceted classification]]
* [[Folksonomy]]
* [[Information ecology]]
* [[Knowledge representation]]
* [[Knowledge transfer]]
* [[Metaknowledge]]
* [[Ontology (information science)]]
* [[Organisational memory]]
* [[Semantic web]]
* [[SciCrunch]]
* [[Tag cloud]]
* [[Web 2.0]]
{{colend}}
'''Others'''
{{colbegin||27em}}
* [[Collective unconscious]]
* [[Human-computer interaction]]
* [[Social network aggregation]]
* [[Enterprise social software]]
* [[Expert system]]
* [[Knowledge]]
* [[Knowledge base]]
* [[Knowledge worker]]
* [[Management information system]]
* [[Microformats]]
* [[Social network]]
* [[Social software]]
* [[Sociology of knowledge]]
* [[Tacit knowledge]]
{{colend}}

==References==
{{reflist|30em}}

'''General'''
{{refbegin}}
*{{Citation
 | surname1=Nonaka | given1=Ikujiro
 | year=1994
 | title= A dynamic theory of organizational knowledge creation
 | journal= Organization Science |volume=5 |issue=1
 | pages=14–37
 | url=http://papers.ssrn.com/sol3/papers.cfm?abstract_id=889992
 | doi=10.1287/orsc.5.1.14
}}
*{{Citation
 | surname1=Wigg | given1=Karl M
 | year=1993
 | title= Knowledge Management Foundations: Thinking About Thinking: How People and Organizations Create, Represent and Use Knowledge
 | journal= Arlington: Schema Press
 | pages=153
 | url=http://papers.ssrn.com/sol3/papers.cfm?abstract_id=889992
}}
*{{Citation
 | surname1=Alavi | given1=Maryam
 | surname2=Leidner | given2=Dorothy E.
 | year=1999
 | title=Knowledge management systems: issues, challenges, and benefits
 | journal=Communications of the AIS
 | volume=1| issue=2 | url=http://portal.acm.org/citation.cfm?id=374117
}}
*{{Citation
 | surname1=Kemsley | given1=Sandy
 | year=2009
 | title=Models, Social Tagging and Knowledge Management #BPM2009 #BPMS2’09
 | journal=BPM, Enterprise 2.0 and technology trends in business
 | url=http://www.column2.com/2009/09/models-social-tagging-and-knowledge-management-bpm2009-bpms209/
}}
{{refend}}

==External links==
* [http://www.inc.com/tech-blog/twitter-hashtag-techniques-for-businesses.html Hashtag Techniques for Businesses], Curt Finch. Inc Magazine. May 26, 2011.
* [http://www.tbray.org/tmp/tag-urn.html A Uniform Resource Name (URN) Namespace for Tag Metadata].  Tim Bray.  Internet draft, expired August 5, 2007.

{{Web syndication}}

{{DEFAULTSORT:Tag (Metadata)}}
[[Category:Collective intelligence]]
[[Category:Computer jargon]]
[[Category:Information retrieval techniques]]
[[Category:Knowledge representation]]
[[Category:Metadata]]
[[Category:Reference]]
[[Category:Web 2.0]]</text>
      <sha1>r6mrkxdlpdiyp94mjirwr2lw3m1hws0</sha1>
    </revision>
  </page>
  <page>
    <title>Search-oriented architecture</title>
    <ns>0</ns>
    <id>7470226</id>
    <revision>
      <id>666716905</id>
      <parentid>666710948</parentid>
      <timestamp>2015-06-13T04:24:35Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>Category:Information retrieval techniques</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1881" xml:space="preserve">{{unreferenced|date=October 2007}}
The use of [[search engine technology]] is the main integration component in an [[information system]]. In a traditional business environment the [[architectural layer]] usually occupied by a [[relational database management system]] (RDBMS) is supplemented or replaced with a search engine or the indexing technology used to build search engines. Queries for information which would usually be performed using [[Structured Query Language]] (SQL) are replaced by keyword or fielded (or field-enabled) searches for structured, [[Semi-structured model|semi-structured]], or unstructured data.

In a typical [[Multitier architecture|multi-tier]] or [[Multitier architecture|N tier]] architecture information is maintained in a data tier where it can be stored and retrieved from a database or file system. The data tier is queried by the logic or business tier when information is needed using a data retrieval language like SQL.

In a '''search-oriented architecture''' the data tier may be replaced or placed behind another tier which contains a search engine and search engine index which is queried instead of the database management system. Queries from the business tier are made in the search engine query language instead of SQL. The search engine itself crawls the relational database management system in addition to other traditional data sources such as web pages or traditional file systems and consolidates the results when queried.

The benefit of adding a search layer to the architecture stack is rapid response time large dynamic datasets made possible by search indexing technology such as an [[inverted index]]. 

== Contrast with ==
* [[Service-oriented architecture]] (SOA)
* [[Service-Oriented Modeling]]

== See also ==
* [[Hibernate search]]
 
[[Category:Software architecture]]
[[Category:Information retrieval techniques]]</text>
      <sha1>jt11nqrgnjyz9c6609g6wxayzuviups</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Substring indices</title>
    <ns>14</ns>
    <id>33958933</id>
    <revision>
      <id>666717294</id>
      <parentid>548118218</parentid>
      <timestamp>2015-06-13T04:30:37Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>Category:Information retrieval techniques</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="263" xml:space="preserve">{{cat main|Substring index}}

[[Category:String (computer science)]]
[[Category:Algorithms on strings]]
[[Category:String data structures]]
[[Category:Database index techniques]]
[[Category:Information retrieval techniques]]
[[Category:Bioinformatics algorithms]]</text>
      <sha1>qokg2koub79fsbjqukwkrt1h46576xh</sha1>
    </revision>
  </page>
  <page>
    <title>Subsetting</title>
    <ns>0</ns>
    <id>3231582</id>
    <revision>
      <id>745810698</id>
      <parentid>701827927</parentid>
      <timestamp>2016-10-23T12:53:07Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* top */http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1132" xml:space="preserve">In research communities (for example, [[earth science]]s, [[astronomy]], [[business]], and [[government]]), '''subsetting''' is the process of retrieving just the parts of large files which are of interest for a specific purpose. This occurs usually in a client—server setting, where the extraction of the parts of interest occurs on the server before the data is sent to the client over a network. The main purpose of subsetting is to save bandwidth on the network and storage space on the client computer.

Subsetting may be favorable for the following reasons:&lt;ref name="Institute2012"&gt;{{cite book|author=SAS Institute|title=SAS/ETS 12.1 User's Guide|url=https://books.google.com/books?id=OE0UfAhit4kC&amp;pg=PA70|date=1 August 2012|publisher=SAS Institute|isbn=978-1-61290-379-8|pages=70}}&lt;/ref&gt;
* restrict or divide the time range
* select [[Cross-sectional data|cross section]]s of data
* select particular kinds of [[time series]]
* exclude particular observations

==References==
{{reflist}}


==External links==
*[http://www.subset.org/index.jsp Subset.org]

[[Category:Information retrieval techniques]]

{{Statistics-stub}}</text>
      <sha1>ehtfvn3ht2czhn8f3f65c82ilqs52y5</sha1>
    </revision>
  </page>
  <page>
    <title>Statistical semantics</title>
    <ns>0</ns>
    <id>7271261</id>
    <revision>
      <id>753523331</id>
      <parentid>753522318</parentid>
      <timestamp>2016-12-07T18:13:23Z</timestamp>
      <contributor>
        <ip>82.2.1.89</ip>
      </contributor>
      <comment>/* External links */ Removed low value external links</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="13412" xml:space="preserve">{{linguistics}}
'''Statistical semantics''' is the study of "how the statistical patterns of human word usage can be used to figure out what people mean, at least to a level sufficient for information access" {{citation needed|date=July 2012}}&lt;!--([[George Furnas|Furnas]], 2006)--this page has been moved and the new version no longer contains this quotation--&gt;. How can we figure out what words mean, simply by looking at patterns of words in huge collections of text? What are the limits to this approach to understanding words?

==History==

The term ''statistical semantics'' was first used by [[Warren Weaver]] in his well-known paper on [[machine translation]].&lt;ref&gt;{{harvnb|Weaver|1955}}&lt;/ref&gt; He argued that [[word sense disambiguation]] for machine translation should be based on the [[co-occurrence]] frequency of the context words near a given target word. The underlying assumption that "a word is characterized by the company it keeps" was advocated by [[J. R. Firth|J.R. Firth]].&lt;ref&gt;{{harvnb|Firth|1957}}&lt;/ref&gt; This assumption is known in [[linguistics]] as the [[distributional hypothesis]].&lt;ref&gt;{{harvnb|Sahlgren|2008}}&lt;/ref&gt; Emile Delavenay defined ''statistical semantics'' as the "Statistical study of meanings of words and their frequency and order of recurrence."&lt;ref&gt;{{harvnb|Delavenay|1960}}&lt;/ref&gt; "[[George Furnas|Furnas]] et al. 1983" is frequently cited as a foundational contribution to statistical semantics.&lt;ref&gt;{{harvnb|Furnas|Landauer|Gomez|Dumais|1983}}&lt;/ref&gt;  An early success in the field was [[latent semantic analysis]].

==Applications==

Research in statistical semantics has resulted in a wide variety of algorithms that use the distributional hypothesis to discover many aspects of [[semantics]], by applying statistical techniques to [[Text corpus|large corpora]]:
* Measuring the [[Semantic similarity|similarity in word meanings]]&lt;ref&gt;{{harvnb|Lund|Burgess|Atchley|1995}}&lt;/ref&gt;&lt;ref&gt;{{harvnb|Landauer|Dumais|1997}}&lt;/ref&gt;&lt;ref&gt;{{harvnb|McDonald|Ramscar|2001}}&lt;/ref&gt;&lt;ref&gt;{{harvnb|Terra|Clarke|2003}}&lt;/ref&gt;
* Measuring the similarity in word relations &lt;ref&gt;{{harvnb|Turney|2006}}&lt;/ref&gt;
* Modeling [[similarity-based generalization]]&lt;ref&gt;{{harvnb|Yarlett|2008}}&lt;/ref&gt;
* Discovering words with a given relation&lt;ref&gt;{{harvnb|Hearst|1992}}&lt;/ref&gt;
* Classifying relations between words&lt;ref&gt;{{harvnb|Turney|Littman|2005}}&lt;/ref&gt;
* Extracting keywords from documents&lt;ref&gt;{{harvnb|Frank|Paynter|Witten|Gutwin|1999}}&lt;/ref&gt;&lt;ref&gt;{{harvnb|Turney|2000}}&lt;/ref&gt;
* Measuring the cohesiveness of text&lt;ref&gt;{{harvnb|Turney|2003}}&lt;/ref&gt;
* Discovering the different senses of words&lt;ref&gt;{{harvnb|Pantel|Lin|2002}}&lt;/ref&gt;
* Distinguishing the different senses of words&lt;ref&gt;{{harvnb|Turney|2004}}&lt;/ref&gt;
* Subcognitive aspects of words&lt;ref&gt;{{harvnb|Turney|2001}}&lt;/ref&gt;
* Distinguishing praise from criticism&lt;ref&gt;{{harvnb|Turney|Littman|2003}}&lt;/ref&gt;

==Related fields==

Statistical Semantics focuses on the meanings of common words and the relations between common words, unlike [[text mining]], which tends to focus on whole documents, document collections, or named entities (names of people, places, and organizations). Statistical Semantics is a subfield of [[computational semantics]], which is in turn a subfield of [[computational linguistics]] and [[natural language processing]].

Many of the applications of Statistical Semantics (listed above) can also be addressed by [[lexicon]]-based algorithms, instead of the [[text corpus|corpus]]-based algorithms of Statistical Semantics. One advantage of corpus-based algorithms is that they are typically not as labour-intensive as lexicon-based algorithms. Another advantage is that they are usually easier to adapt to new languages than lexicon-based algorithms. However, the best performance on an application is often achieved by combining the two approaches.&lt;ref&gt;{{harvnb|Turney|Littman|Bigham|Shnayder|2003}}&lt;/ref&gt;

==See also==
{{Portal|Linguistics}}
{{div col|3}}
*[[Co-occurrence]]
*[[Computational linguistics]]
*[[Information retrieval]]
*[[Latent semantic analysis]]
*[[Latent semantic indexing]]
*[[Natural language processing]]
*[[Semantic analytics]]
*[[Semantic similarity]]
*[[Text corpus]]
*[[Text mining]]
*[[Web mining]]
{{div col end}}

==References==
{{reflist|2}}

===Sources===
{{refbegin}}
* {{cite book | last = Delavenay | first = Emile | year = 1960 | title = An Introduction to Machine Translation | location = New York, NY | publisher = [[Thames and Hudson]] | oclc = 1001646 | ref = harv }}
* {{cite journal | last = Firth | first = John R. | authorlink = John Rupert Firth | year = 1957 | title = A synopsis of linguistic theory 1930-1955 | journal = [[Studies in Linguistic Analysis]] | pages = 1–32 | location = Oxford | publisher = [[Philological Society]] | ref = harv }}
*: Reprinted in {{cite book | editor1-first = F.R. | editor1-last = Palmer | title = Selected Papers of J.R. Firth 1952-1959 | location = London | publisher = Longman | year = 1968 | oclc = 123573912 }}
* {{cite conference | last1 = Frank | first1 = Eibe | last2 = Paynter | first2 = Gordon W. | last3 = Witten | first3 = Ian H. | last4 = Gutwin | first4 = Carl | last5 = Nevill-Manning | first5 = Craig G. | year = 1999 | title = Domain-specific keyphrase extraction | booktitle = Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence | conference = [[International Joint Conference on Artificial Intelligence|IJCAI-99]] | volume = 2 | pages = 668–673 | location = California | publisher = Morgan Kaufmann | isbn = 1-55860-613-0 | citeseerx = 10.1.1.148.3598 | ref = harv }}
* {{cite journal | last1 = Furnas | first1 = George W. | authorlink = George Furnas | last2 = Landauer | first2 = T. K. | last3 = Gomez | first3 = L. M. | last4 = Dumais | first4 = S. T. | year = 1983 | title = Statistical semantics: Analysis of the potential performance of keyword information systems | url = https://web.archive.org/web/*/http://furnas.people.si.umich.edu/Papers/FurnasEtAl1983_BSTJ_p1753.pdf | journal = [[Bell System Technical Journal]] | volume = 62 | issue = 6 | pages = 1753–1806 | ref = harv | doi=10.1002/j.1538-7305.1983.tb03513.x}}
* {{cite conference | last = Hearst | first = Marti A. | year = 1992 | title = Automatic Acquisition of Hyponyms from Large Text Corpora | booktitle = Proceedings of the Fourteenth International Conference on Computational Linguistics | conference = [[COLING|COLING '92]] | pages = 539–545 | location = Nantes, France | url = http://acl.ldc.upenn.edu/C/C92/C92-2082.pdf | doi = 10.3115/992133.992154 | citeseerx = 10.1.1.36.701 | ref = harv }}
* {{cite journal | last1 = Landauer | first1 = Thomas K. | last2 = Dumais | first2 = Susan T. | year = 1997 | title = A solution to Plato's problem: The latent semantic analysis theory of the acquisition, induction, and representation of knowledge | journal = [[Psychological Review]] | volume = 104 | issue = 2 | pages = 211–240 | url = http://lsa.colorado.edu/papers/plato/plato.annote.html | citeseerx = 10.1.1.184.4759 | ref = harv | doi=10.1037/0033-295x.104.2.211}}
* {{cite conference | last1 = Lund | first1 = Kevin | last2 = Burgess | first2 = Curt | last3 = Atchley | first3 = Ruth Ann | year = 1995 | title = Semantic and associative priming in high-dimensional semantic space | booktitle = Proceedings of the 17th Annual Conference of the Cognitive Science Society | publisher = [[Cognitive Science Society]] | pages = 660–665 | url = http://locutus.ucr.edu/reprintPDFs/lba95csp.pdf | ref = harv }}
* {{cite conference | last1 = McDonald | first1 = Scott | last2 = Ramscar | first2 = Michael | year = 2001 | title = Testing the distributional hypothesis: The influence of context on judgements of semantic similarity | booktitle = Proceedings of the 23rd Annual Conference of the Cognitive Science Society | pages = 611–616 | url = http://homepages.inf.ed.ac.uk/smcdonal/cogsci2001.pdf | citeseerx = 10.1.1.104.7535 | ref = harv }}
* {{cite conference | last1 = Pantel | first1 = Patrick | last2 = Lin | first2 = Dekang | year = 2002 | title = Discovering word senses from text | booktitle = Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining | isbn = 1-58113-567-X | conference = [[KDD Conference|KDD '02]] | pages = 613–619 | citeseerx = 10.1.1.12.6771 | doi = 10.1145/775047.775138 | ref = harv }}
* {{cite journal | last1 = Sahlgren | first1 = Magnus | year = 2008 | title = The Distributional Hypothesis | url = http://soda.swedish-ict.se/3941/1/sahlgren.distr-hypo.pdf | journal = Rivista di Linguistica | volume = 20 | issue = 1 | pages = 33–53 | ref = harv}}
* {{cite conference | last1 = Terra | first1 = Egidio L. | last2 = Clarke | first2 = Charles L. A. | year = 2003 | title = Frequency estimates for statistical word similarity measures | booktitle = Proceedings of the Human Language Technology and North American Chapter of Association of Computational Linguistics Conference 2003 | conference = HLT/NAACL 2003 | pages = 244–251 | url = http://acl.ldc.upenn.edu/N/N03/N03-1032.pdf | citeseerx = 10.1.1.12.9041 | doi = 10.3115/1073445.1073477 | ref = harv }}
* {{cite journal | last = Turney | first = Peter D. |date=May 2000 | title = Learning algorithms for keyphrase extraction | journal = [[Information Retrieval (journal)|Information Retrieval]] | volume = 2 | issue = 4 | pages = 303–336 | arxiv = cs/0212020 | citeseerx = 10.1.1.11.1829 | doi = 10.1023/A:1009976227802 | ref = harv }}
* {{cite journal | last = Turney | first = Peter D. | year = 2001 | title = Answering subcognitive Turing Test questions: A reply to French | journal = [[Journal of Experimental and Theoretical Artificial Intelligence]] | volume = 13 | issue = 4 | pages = 409–419 | arxiv = cs/0212015 | citeseerx = 10.1.1.12.8734 | ref = harv | doi=10.1080/09528130110100270}}
* {{cite conference | last = Turney | first = Peter D. | year = 2003 | title = Coherent keyphrase extraction via Web mining | booktitle = Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence | conference = IJCAI-03 | location = Acapulco, Mexico | pages = 434–439 | arxiv = cs/0308033 | citeseerx = 10.1.1.100.3751 | ref = harv }}
* {{cite conference | last = Turney | first = Peter D. | year = 2004 | title = Word sense disambiguation by Web mining for word co-occurrence probabilities | booktitle = Proceedings of the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text | conference = SENSEVAL-3 | location = Barcelona, Spain | pages = 239–242 | arxiv = cs/0407065 | url = http://cogprints.org/3732/ | ref = harv }}
* {{cite journal | last = Turney | first = Peter D. | year = 2006 | title = Similarity of semantic relations |journal = [[Computational Linguistics (journal)|Computational Linguistics]] | volume = 32 | issue = 3 | pages = 379–416 | arxiv = cs/0608100 | url = http://cogprints.org/5098/ | doi = 10.1162/coli.2006.32.3.379 | citeseerx = 10.1.1.75.8007 | ref = harv }}
* {{cite journal | last1 = Turney | first1 = Peter D. | last2 = Littman | first2 = Michael L. |date=October 2003 | title = Measuring praise and criticism: Inference of semantic orientation from association | journal = [[ACM Transactions on Information Systems]] | volume = 21 | issue = 4 | pages = 315–346 | arxiv = cs/0309034 | url = http://cogprints.org/3164/ | citeseerx = 10.1.1.9.6425 | doi = 10.1145/944012.944013 | ref = harv }}
* {{cite journal | last1 = Turney | first1 = Peter D. | last2 = Littman | first2 = Michael L. | year = 2005 | title = Corpus-based Learning of Analogies and Semantic Relations | journal = [[Machine Learning (journal)|Machine Learning]] | volume = 60 | issue = 1–3 | pages = 251–278 | arxiv = cs/0508103 | citeseerx = 10.1.1.90.9819 | doi = 10.1007/s10994-005-0913-1 | url = http://cogprints.org/4518/ | ref = harv }}
* {{cite conference | last1 = Turney | first1 = Peter D. | last2 = Littman | first2 = Michael L. | last3 = Bigham | first3 = Jeffrey | last4 = Shnayder | first4 = Victor | year = 2003 | title = Combining Independent Modules to Solve Multiple-choice Synonym and Analogy Problems | booktitle = Proceedings of the International Conference on Recent Advances in Natural Language Processing | conference = RANLP-03 | location = [[Borovets]], Bulgaria | pages = 482–489 | arxiv = cs/0309035 | citeseerx = 10.1.1.5.2939 | url = http://cogprints.org/3163/ | ref = harv }}
* {{cite book | last = Weaver | first = Warren | authorlink = Warren Weaver | year = 1955 | chapter = Translation | chapter-url = http://www.mt-archive.info/Weaver-1949.pdf | editor1-first = W.N. | editor1-last = Locke | editor2-first = D.A. | editor2-last = Booth | title = Machine Translation of Languages | location = [[Cambridge, Massachusetts]] | publisher = [[MIT Press]] | isbn = 0-8371-8434-7 | pages = 15–23 | ref = harv }}
* {{cite thesis | last = Yarlett | first = Daniel G. | year = 2008 | title = Language Learning Through Similarity-Based Generalization | url = http://psych.stanford.edu/~michael/papers/Draft_Yarlett_Similarity.pdf | degree = PhD | publisher = Stanford University | ref = harv }}
{{refend}}


{{DEFAULTSORT:Statistical Semantics}}
[[Category:Artificial intelligence applications]]
[[Category:Computational linguistics]]
[[Category:Information retrieval techniques]]
[[Category:Semantics]]
[[Category:Statistical natural language processing]]
[[Category:Applied statistics]]</text>
      <sha1>1y8u3rtxu122ixzhvlcjv2umaflm40q</sha1>
    </revision>
  </page>
  <page>
    <title>Semantic compression</title>
    <ns>0</ns>
    <id>34087348</id>
    <revision>
      <id>751833052</id>
      <parentid>666858233</parentid>
      <timestamp>2016-11-28T02:33:18Z</timestamp>
      <contributor>
        <username>Anticontradictor</username>
        <id>26909770</id>
      </contributor>
      <minor />
      <comment>A minor edit</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5233" xml:space="preserve">In [[natural language processing]], '''semantic compression''' is a process of compacting a lexicon used to build 
a textual document (or a set of documents) by reducing language heterogeneity, while maintaining text [[semantics]]. 
As a result, the same ideas can be represented using a smaller set of words.

Semantic compression is a [[lossy compression]], that is, some data is being discarded, and an original document 
cannot be reconstructed in a reverse process.

==Semantic compression by generalization==
Semantic compression is basically achieved in two steps, using [[frequency list|frequency dictionaries]] and [[semantic network]]:
#	determining cumulated term frequencies to identify target lexicon,
#	replacing less frequent terms with their hypernyms ([[generalization]]) from target lexicon.&lt;ref&gt;[http://dx.doi.org/10.1007/978-3-642-12090-9_10 D. Ceglarek, K. Haniewicz, W. Rutkowski, Semantic Compression for Specialised Information Retrieval Systems], Advances in Intelligent Information and Database Systems, vol. 283, p. 111-121, 2010&lt;/ref&gt;

Step 1 requires assembling word frequencies and 
information on semantic relationships, specifically [[hyponymy]]. Moving upwards in word hierarchy, 
a cumulative concept frequency is calculating by adding a sum of hyponyms' frequencies to frequency of their hypernym:
&lt;math&gt;cum f(k_{i}) = f(k_{i}) + \sum_{j} cum f(k_{j})&lt;/math&gt; where &lt;math&gt;k_{i}&lt;/math&gt; is a hypernym of &lt;math&gt;k_{j}&lt;/math&gt;.
Then, a desired number of words with top cumulated frequencies are chosen to build a targed lexicon.

In the second step, compression mapping rules are defined for the remaining words, in order to handle every occurrence 
of a less frequent hyponym as its hypernym in output text.

;Example

The below fragment of text has been processed by the semantic compression. Words in bold have been replaced by their hypernyms.

&lt;blockquote&gt;They are both '''nest''' building '''social insects''', but '''paper wasps''' and honey '''bees''' '''organize''' their '''colonies''' 
in very different '''ways'''. In a new study, researchers report that despite their '''differences''', these insects 
'''rely on''' the same network of genes to guide their '''social behavior'''.The study appears in the Proceedings of the 
'''Royal Society B''': Biological Sciences. Honey '''bees''' and '''paper wasps''' are separated by more than 100 million years of 
'''evolution''', and there are '''striking differences''' in how they divvy up the work of '''maintaining''' a '''colony'''.&lt;/blockquote&gt;

The procedure outputs the following text:

&lt;blockquote&gt;They are both '''facility''' building '''insect''', but '''insect'''s and honey '''insects''' '''arrange''' their '''biological groups''' 
in very different '''structure'''. In a new study, researchers report that despite their '''difference of opinions''', these insects 
'''act''' the same network of genes to '''steer''' their '''party demeanor'''. The study appears in the proceeding of the 
'''institution bacteria''' Biological Sciences. Honey '''insects''' and '''insect''' are separated by more than hundred million years of 
'''organic processes''', and there are '''impinging differences of opinions''' in how they divvy up the work of '''affirming''' a '''biological group'''.&lt;/blockquote&gt;

==Implicit semantic compression==
A natural tendency to keep natural language expressions concise can be perceived as a form of implicit semantic compression, by omitting unmeaningful words or redundant meaningful words (especially to avoid [[pleonasm]]s)
.&lt;ref&gt;[http://dx.doi.org/10.3115/990100.990155 N. N. Percova, On the types of semantic compression of text],
COLING '82 Proceedings of the 9th Conference on Computational Linguistics, vol. 2, p. 229-231, 1982&lt;/ref&gt;

==Applications and advantages==
In the [[vector space model]], compacting a lexicon leads to a reduction of [[curse of dimensionality|dimensionality]], which results in less 
[[Computational complexity theory|computational complexity]] and a positive influence on efficiency. 

Semantic compression is advantageous in information retrieval tasks, improving their effectiveness (in terms of both precision and recall).&lt;ref&gt;[http://dl.acm.org/citation.cfm?id=1947662.1947683 D. Ceglarek, K. Haniewicz, W. Rutkowski, Quality of semantic compression in classification] Proceedings of the 2nd International Conference on Computational Collective Intelligence: Technologies and Applications, vol. 1, p. 162-171, 2010&lt;/ref&gt; This is due to more precise descriptors (reduced effect of language diversity – limited language redundancy, a step towards a controlled dictionary).

As in the example above, it is possible to display the output as natural text (re-applying inflexion, adding stop words).

==See also==
* [[Text simplification]]
* [[Lexical substitution]]
* [[Information theory]]
* [[Quantities of information]]

==References==
&lt;references/&gt;

==External links==
* [http://semantic.net.pl/semantic_compression.php Semantic compression on Project SENECA (Semantic Networks and Categorization) website]

[[Category:Information retrieval techniques]]
[[Category:Natural language processing]]
[[Category:Quantitative linguistics]]
[[Category:Computational linguistics]]</text>
      <sha1>j4iipzm6qxr7dq1zzm2kxmrpmvw6z6s</sha1>
    </revision>
  </page>
  <page>
    <title>Document clustering</title>
    <ns>0</ns>
    <id>14663145</id>
    <revision>
      <id>756183038</id>
      <parentid>743106203</parentid>
      <timestamp>2016-12-22T14:59:59Z</timestamp>
      <contributor>
        <username>MrOllie</username>
        <id>6908984</id>
      </contributor>
      <comment>[[WP:NOT]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6232" xml:space="preserve">{{Multiple issues|
{{disputed|date=March 2014}}
{{more footnotes|date=March 2014}}
}}

'''Document clustering''' (or '''text clustering''') is the application of [[cluster analysis]] to textual documents. It has applications in automatic document organization, [[topic (linguistics)|topic]] extraction and fast [[information retrieval]] or filtering.

==Overview==
Document clustering involves the use of descriptors and descriptor extraction. Descriptors are sets of words that describe the contents within the cluster. Document clustering is generally considered to be a centralized process. Examples of document clustering include web document clustering for search users.

The application of document clustering can be categorized to two types, online and offline. Online applications are usually constrained by efficiency problems when compared to offline applications.

In general, there are two common algorithms. The first one is the hierarchical based algorithm, which includes single link, complete linkage, group average and Ward's method.  By aggregating or dividing, documents can be clustered into hierarchical structure, which is suitable for browsing. However, such an algorithm usually suffers from efficiency problems. The other algorithm is developed using the [[K-means algorithm]] and its variants. Generally hierarchical algorithms produce more in-depth information for detailed analyses, while algorithms based around variants of the [[K-means algorithm]] are more efficient and provide sufficient information for most purposes.&lt;ref name="manning"&gt;Manning, Chris, and Hinrich Schütze, ''Foundations of Statistical Natural Language Processing'', MIT Press. Cambridge, MA: May 1999.&lt;/ref&gt;{{rp|Ch.14}}

These algorithms can further be classified as hard or soft clustering algorithms. Hard clustering computes a hard assignment – each document is a member of exactly one cluster. The assignment of soft clustering algorithms is soft – a document’s assignment is a distribution over all clusters. In a soft assignment, a document has fractional membership in several clusters.&lt;ref name="manning"/&gt;{{rp|499}} [[Dimensionality reduction]] methods can be considered a subtype of soft clustering; for documents, these include [[latent semantic indexing]] ([[truncated singular value decomposition]] on term histograms)&lt;ref&gt;http://nlp.stanford.edu/IR-book/pdf/16flat.pdf&lt;/ref&gt; and [[topic model]]s.

Other algorithms involve graph based clustering, ontology supported clustering and order sensitive clustering.

Given a clustering, it can be beneficial to automatically derive human-readable labels for the clusters. [[Cluster labeling|Various methods]] exist for this purpose.

==Clustering in search engines==
A [[web search engine]] often  returns thousands of pages in response to a broad query, making it difficult for users to browse or to identify relevant information.  Clustering methods can be used to automatically group the retrieved documents into a list of meaningful categories, as is achieved by e.g. open source software such as [[Carrot2]].

==Procedures==
In practice, document clustering often takes the following steps:
 
1. [[Tokenization (lexical analysis)|Tokenization]]

Tokenization is the process of parsing text data into smaller units (tokens) such as words and phrases. Commonly used tokenization methods include [[Bag-of-words model]] and [[N-gram model]].

2. [[Stemming]] and [[lemmatization]]

Different tokens might carry out similar information (e.g. tokenization and tokenizing). And we can avoid calculating similar information repeatedly by reducing all tokens to its base form using various stemming and lemmatization dictionaries.

3. Removing [[stop words]] and [[punctuation]]

Some tokens are less important than others. For instance, common words such as "the" might not be very helpful for revealing the essential characteristics of a text. So usually it is a good idea to eliminate stop words and punctuation marks before doing further analysis.

4. Computing term frequencies or [[tf-idf]]

After pre-processing the text data, we can then proceed to generate features. For document clustering, one of the most common ways to generate features for a document is to calculate the term frequencies of all its tokens. Although not perfect, these frequencies can usually provide some clues about the topic of the document. And sometimes it is also useful to weight the term frequencies by the inverse document frequencies. See [[tf-idf]] for detailed discussions.

5. Clustering

We can then cluster different documents based on the features we have generated. See the algorithm section in [[cluster analysis]] for different types of clustering methods.

6. Evaluation and visualization

Finally, the clustering models can be assessed by various metrics. And it is sometimes helpful to visualize the results by plotting the clusters into low (two) dimensional space. See [[multidimensional scaling]] as a possible approach.

== Clustering v. Classifying ==
Clustering algorithms in computational text analysis groups documents into what are called subsets or ''clusters'' where the algorithm's goal is to create internally coherent clusters that are distinct from one another.&lt;ref&gt;{{Cite web|url=http://nlp.stanford.edu/IR-book/|title=Introduction to Information Retrieval|website=nlp.stanford.edu|pages=349|access-date=2016-05-03}}&lt;/ref&gt; Classification on the other hand, is a form of [[supervised learning]] where the features of the documents are used to predict the "type" of documents.

== References ==
{{reflist}}
Publications:
* Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze. ''Flat Clustering'' in &lt;u&gt;Introduction to Information Retrieval.&lt;/u&gt; Cambridge University Press. 2008
* Nicholas O. Andrews and Edward A. Fox, Recent Developments in Document Clustering, October 16, 2007 [http://eprints.cs.vt.edu/archive/00001000/01/docclust.pdf]
* Claudio Carpineto, Stanislaw Osiński, Giovanni Romano, Dawid Weiss. A survey of Web clustering engines. ACM Computing Surveys, Volume 41, Issue 3 (July 2009), Article No. 17, {{ISSN|0360-0300}}

==See also==
*[[Cluster Analysis]]
*[[Fuzzy clustering]]

[[Category:Information retrieval techniques]]</text>
      <sha1>sz15cayl5yo0x8phdkbo5emh4zk1bsn</sha1>
    </revision>
  </page>
  <page>
    <title>Compound term processing</title>
    <ns>0</ns>
    <id>18046649</id>
    <revision>
      <id>679091134</id>
      <parentid>666859577</parentid>
      <timestamp>2015-09-02T11:39:51Z</timestamp>
      <contributor>
        <username>Dexbot</username>
        <id>16752040</id>
      </contributor>
      <minor />
      <comment>Bot: Deprecating [[Template:Cite doi]] and some minor fixes</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5396" xml:space="preserve">'''Compound term processing''' refers to a category of techniques used in [[information retrieval]] applications to perform matching on the basis of [[compound term]]s. Compound terms are built by combining two or more simple terms; for example, "triple" is a single word term, but "triple heart bypass" is a compound term.

Compound term processing is a new approach to an old problem: how can one improve the relevance of search results while maintaining ease of use? Using this technique, a search for ''survival rates following a triple heart bypass in elderly people'' will locate documents about this topic even if this precise phrase is not contained in any document. This can be performed by a [[concept search]], which itself uses compound term processing. This will extract the key concepts automatically (in this case "survival rates", "triple heart bypass" and "elderly people") and use these concepts to select the most relevant documents.

== Techniques ==

In August 2003, [[Concept Searching Limited]] introduced the idea of using statistical Compound Term Processing.&lt;ref&gt;{{cite journal|url=http://www.conceptsearching.com/Web/UserFiles/File/Concept%20Searching%20Lateral%20Thinking.pdf|title=Lateral Thinking in Information Retrieval|journal=INFORMATION MANAGEMENT AND TECHNOLOGY|volume=36 PART 4}} The British Library Direct catalogue entry can be found here:[http://direct.bl.uk/bld/PlaceOrder.do?UIN=138451913&amp;ETOC=RN]&lt;/ref&gt;

CLAMOUR is a European collaborative project which aims to find a better way to classify when collecting and disseminating industrial information and statistics. CLAMOUR appears to use a linguistic approach, rather than one based on statistical modelling.&lt;ref&gt;[http://webarchive.nationalarchives.gov.uk/20040117000117/statistics.gov.uk/methods_quality/clamour/default.asp] National Statistics CLAMOUR project&lt;/ref&gt;

== History ==

Techniques for probabilistic weighting of single word terms date back to at least 1976 in the landmark publication by [[Stephen Robertson (computer scientist)|Stephen E. Robertson]] and [[Karen Spärck Jones]].&lt;ref&gt;{{Cite journal | doi = 10.1002/asi.4630270302| title = Relevance weighting of search terms| journal = Journal of the American Society for Information Science| volume = 27| issue = 3| pages = 129| year = 1976| last1 = Robertson | first1 = S. E. | authorlink1 = Stephen Robertson (computer scientist)| last2 = Spärck Jones | first2 = K. | authorlink2 = Karen Spärck Jones}}&lt;/ref&gt; Robertson stated that the assumption of word independence is not justified and exists as a matter of mathematical convenience. His objection to the term independence is not a new idea, dating back to at least 1964 when H. H. Williams stated that "[t]he assumption of independence of words in a document is usually made as a matter of mathematical convenience".&lt;ref&gt;{{cite journal|last=WILLIAMS |first=J.H. |title=Results of classifying documents with multiple discriminant functions |url=http://oai.dtic.mil/oai/oai?verb=getRecord&amp;metadataPrefix=html&amp;identifier=AD0612272 |journal= Statistical Association Methods for Mechanized Documentation, National Bureau of Standards |location=Washington |pp=217-224 |year=1965}}&lt;/ref&gt;

In 2004, Anna Lynn Patterson filed patents on "phrase-based searching in an information retrieval system"&lt;ref&gt;{{patent|US|20060031195}}&lt;/ref&gt; to which [[Google]] subsequently acquired the rights.&lt;ref&gt;[http://www.seobythesea.com/2012/02/google-acquires-cuil-patent-applications/ Google Acquires Cuil Patent Applications]&lt;/ref&gt;

== Adaptability ==

Statistical compound term processing is more adaptable than the process described by Patterson. Her process is targeted at searching the [[World Wide Web]] where an extensive statistical knowledge of common searches can be used to identify candidate phrases. Statistical compound term processing is more suited to [[enterprise search]] applications where such [[A priori and a posteriori|a priori]] knowledge is not available.

Statistical compound term processing is also more adaptable than the linguistic approach taken by the CLAMOUR project, which must consider the syntactic properties of the terms (i.e. part of speech, gender, number, etc.) and their combinations. CLAMOUR is highly language-dependent, whereas the statistical approach is language-independent.

== Applications ==
Compound Term Processing allows information retrieval applications, such as [[search engines]], to perform their matching on the basis of multi-word concepts, rather than on single words in isolation which can be highly ambiguous.

Early search engines looked for documents containing the words entered by the user into the search box . These are known as [[keyword search]] engines. [[Boolean search]] engines add a degree of sophistication by allowing the user to specify additional requirements. For example, "Tiger NEAR Woods AND (golf OR golfing) NOT Volkswagen" uses the operators "NEAR", "AND", "OR" and "NOT" to specify that these words must follow certain requirements. A [[phrase search]] is simpler to use, but requires that the exact phrase specified appear in the results.

==See also==
* [[Concept Searching Limited]]
* [[Enterprise search]]
* [[Information retrieval]]

== References ==
{{Reflist|30em}}

==  External links ==

{{Natural Language Processing}}

{{DEFAULTSORT:Compound Term Processing}}
[[Category:Information retrieval techniques]]</text>
      <sha1>jkmp0jintzy646bfl7omxn6eyj3d3sf</sha1>
    </revision>
  </page>
  <page>
    <title>Natural language user interface</title>
    <ns>0</ns>
    <id>18863997</id>
    <revision>
      <id>758322270</id>
      <parentid>758322160</parentid>
      <timestamp>2017-01-04T19:24:00Z</timestamp>
      <contributor>
        <username>Michal.on</username>
        <id>17441124</id>
      </contributor>
      <minor />
      <comment>/* Others */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="17215" xml:space="preserve">'''Natural language user interfaces''' ('''LUI''' or '''NLUI''') are a type of [[User interface|computer human interface]] where linguistic phenomena such as verbs, phrases and clauses act as UI controls for creating, selecting and modifying data in software applications.

In [[interface design]] natural language interfaces are sought after for their speed and ease of use, but most suffer the challenges to [[natural language understanding|understanding]] wide varieties of ambiguous input.&lt;ref&gt;Hill, I. (1983). "Natural language versus computer language." In M. Sime and M. Coombs (Eds.) Designing for Human-Computer Communication. Academic Press.&lt;/ref&gt;
Natural language interfaces are an active area of study in the field of [[natural language processing]] and [[computational linguistics]]. An intuitive general natural language interface is one of the active goals of the [[Semantic Web]].

Text interfaces are "natural" to varying degrees. Many formal (un-natural) programming languages incorporate idioms of natural human language. Likewise, a traditional [[keyword search]] engine could be described as a "shallow" natural language user interface.

==Overview==
A natural language search engine would in theory find targeted answers to user questions (as opposed to keyword search). For example, when confronted with a question of the form 'which [[United States|U.S.]] state has the highest [[income tax]]?', conventional search engines ignore the question and instead search on the [[index term|keywords]] 'state', 'income' and 'tax'. Natural language search, on the other hand, attempts to use natural language processing to understand the nature of the question and then to search and return a subset of the web that contains the answer to the question. If it works, results would have a higher relevance than results from a keyword search engine.{{Citation needed|date=October 2015}}

==History==

Prototype Nl interfaces had already appeared in the late sixties and early seventies.&lt;ref name="edin"&gt;Natural Language Interfaces to Databases – An Introduction,
I. Androutsopoulos,
G.D. Ritchie,
P. Thanisch,
Department of Artificial Intelligence, University of Edinburgh&lt;/ref&gt;

*[[SHRDLU]], a natural language interface that manipulates blocks in a virtual "blocks world"
*''Lunar'', a natural language interface to a database containing chemical analyses of Apollo-11 moon rocks by [http://parsecraft.com/ William A. Woods].
*''Chat-80'' transformed English questions into [[Prolog]] expressions, which were evaluated against the Prolog database.  The code of Chat-80 was circulated widely, and formed the basis of several other experimental Nl interfaces. An online demo is available on the LPA website.&lt;ref&gt;[http://www.lpa.co.uk/pws_dem5.htm Chat-80 demo]&lt;/ref&gt;
*[[ELIZA]], written at MIT by Joseph Weizenbaum between 1964 and 1966, mimicked a psychotherapist and was operated by processing users' responses to scripts. Using almost no information about human thought or emotion, the DOCTOR script sometimes provided a startlingly human-like interaction. An online demo is available on the LPA website.&lt;ref&gt;[http://www.lpa.co.uk/pws_dem4.htm ELIZA demo]&lt;/ref&gt;
* ''Janus'' is also one of the few systems to support temporal questions.
* ''Intellect'' from [[Trinzic]] (formed by the merger of AICorp and Aion).
* BBN’s ''Parlance'' built on experience from the development of the ''Rus''  and ''Irus'' systems.
* [[IBM]] ''Languageaccess''
* [[Q&amp;A (software)|Q&amp;A]] from [[Symantec]].
* ''Datatalker'' from Natural Language Inc.
* ''Loqui''  from [[Bim]].
* ''English Wizard'' from [[Linguistic Technology Corporation]].
* ''iAskWeb'' from Anserity Inc. fully implemented in [[Prolog]] was providing interactive recommendations in NL to users in tax and investment domains in 1999-2001&lt;ref&gt;{{cite book | last = Galitsky
 | first = Boris
 | title = Natural Language Question Answering: technique of semantic headers
 | publisher = Advance Knowledge International
 | date = 2003
 | location = Adelaide, Australia
 | url = http://www.amazon.com/Natural-Language-Question-Answering-system/dp/0868039799
 | isbn = 0868039799
  }}&lt;/ref&gt;

==Challenges==
Natural language interfaces have in the past led users to anthropomorphize the computer, or at least to attribute more intelligence to machines than is warranted. On the part of the user, this has led to unrealistic expectations of the capabilities of the system. Such expectations will make it difficult to learn the restrictions of the system if users attribute too much capability to it, and will ultimately lead to disappointment when the system fails to perform as expected as was the case in the [[AI winter]] of the 1970s and 80s.

A [http://arxiv.org/abs/cmp-lg/9503016 1995 paper] titled 'Natural Language Interfaces to Databases – An Introduction', describes some challenges:&lt;ref name="edin"/&gt;
;Modifier attachment
:The request "List all employees in the company with a driving licence" is ambiguous unless you know that companies can't have driving licences.
;Conjunction and disjunction
:"List all applicants who live in California and Arizona" is ambiguous unless you know that a person can't live in two places at once.
;[[Anaphora resolution]]
:resolve what a user means by 'he', 'she' or 'it', in a self-referential query.

Other goals to consider more generally are the speed and efficiency of the interface, in all algorithms these two points are the main point that will determine if some methods are better than others and therefore have greater success in the market. In addition, localisation across multiple language sites requires extra consideration - this is based on differing sentence structure and language syntax variations between most languages.

Finally, regarding the methods used, the main problem to be solved is creating a general algorithm that can recognize the entire spectrum of different voices, while disregarding nationality, gender or age. The significant differences between the extracted features - even from speakers who says the same word or phrase - must be successfully overcome.

==Uses and applications==

The natural language interface gives rise to technology used for many different applications.

Some of the main uses are:

* ''Dictation'', is the most common use for [[automated speech recognition]] (ASR) systems today. This includes medical transcriptions, legal and business dictation, and general word processing. In some cases special vocabularies are used to increase the accuracy of the system.
* ''Command and control'', ASR systems that are designed to perform functions and actions on the system are defined as command and control systems. Utterances like "Open Netscape" and "Start a new xterm" will do just that.
* ''Telephony'', some PBX/[[Voice Mail]] systems allow callers to speak commands instead of pressing buttons to send specific tones.
* ''Wearables'', because inputs are limited for wearable devices, speaking is a natural possibility.
* ''Medical, disabilities'', many people have difficulty typing due to physical limitations such as repetitive strain injuries (RSI), muscular dystrophy, and many others. For example, people with difficulty hearing could use a system connected to their telephone to convert a caller's speech to text.
* ''Embedded applications'', some new cellular phones include C&amp;C speech recognition that allow utterances such as "call home". This may be a major factor in the future of automatic speech recognition and [[Linux]].

Below are named and defined some of the applications that use natural language recognition, and so have integrated utilities listed above.

===Ubiquity===
{{main|Ubiquity (Firefox)}}
Ubiquity, an [[add-on (Mozilla)|add-on]] for [[Mozilla Firefox]], is a collection of quick and easy natural-language-derived commands that act as [[mashup (web application hybrid)|mashups]] of web services, thus allowing users to get information and relate it to current and other webpages.

===Wolfram Alpha===
{{main|Wolfram Alpha}}
Wolfram Alpha is an online service that answers factual queries directly by computing the answer from structured data, rather than providing a list of documents or web pages that might contain the answer as a [[search engine]] would.&lt;ref&gt;{{cite news |url=https://www.theguardian.com/technology/2009/mar/09/search-engine-google |title=British search engine 'could rival Google' |last=Johnson |first=Bobbie |date=2009-03-09 |work=[[The Guardian]] |accessdate=2009-03-09}}&lt;/ref&gt; It was announced in March 2009 by [[Stephen Wolfram]], and was released to the public on May 15, 2009.&lt;ref name="launch date"&gt;{{cite web|url=http://blog.wolframalpha.com/2009/05/08/so-much-for-a-quiet-launch/ |title=So Much for A Quiet Launch |publisher=Wolfram Alpha Blog |date=2009-05-08 |accessdate=2009-10-20}}&lt;/ref&gt;

===Siri===
{{main|Siri (software)}}
Siri is an [[intelligent personal assistant]] application integrated with operating system [[iOS]]. The application uses [[natural language processing]] to answer questions and make recommendations.

Siri's marketing claims include that it adapts to a user's individual preferences over time and personalizes results, and performs tasks such as making dinner reservations while trying to catch a cab.&lt;ref&gt;[https://www.apple.com/iphone/features/siri.html Siri webpage]&lt;/ref&gt;

===Others===
* [[Ask.com]] - The original idea behind Ask Jeeves (Ask.com) was traditional keyword searching with an ability to get answers to questions posed in everyday, natural language. The current Ask.com still supports this, with added support for math, dictionary, and conversion questions.
* [[Braina]]&lt;ref&gt;[http://www.brainasoft.com/braina/ Braina]&lt;/ref&gt; - Braina is a natural language interface for [[Windows OS]] that allows to type or speak English language sentences to perform a certain action or find information.
* [http://www.cmantik.com/ CMANTIK] - CMANTIK is a semantic information search engine which is trying to answer user's questions by looking up relevant information in Wikipedia and some news sources.
* [http://minock.github.io/c-phrase/ C-Phrase] - is a web-based natural language front end to relational databases. C-Phrase runs under Linux, connects with PostgreSQL databases via ODBC and supports both select queries and updates. Currently there is only support for English.
* [http://devtools.korzh.com/easyquery/ EasyQuery] - is a component library (for .NET framework first of all) which allows you to implement natural language query builder in your application. Works both with relational databases or ORM solutions like Entity Framework.
* [https://friendlydata.io/ FriendlyData] is a natural language interface for relational databases.
[[File:GNOME Do Classic.png|thumb|Screenshot of GNOME DO classic interface.]]
* [http://yagadi.com/ Enguage] - this is an open source text understanding interface for web/mobile devices, using publicly available speech-to-text and text-to-speech facilities. This is directed at controlling apps, rather than as a front-end database query or web search. The interpretation of utterances is programmed, and programmable, in natural language utterances; thus, it is (or at least asserts that language is) an [[autopoiesis|autopoietic]] system.&lt;ref&gt;http://www.academia.edu/10177437/An_Autopoietic_Repertoire&lt;/ref&gt; It can achieve a deep understanding of text.&lt;ref&gt;http://cit.srce.unizg.hr/index.php/CIT/article/view/2278/1658 if we are holding hands whose hand am i holding&lt;/ref&gt; A reference app is available on [https://play.google.com/store/apps/details?id=com.yagadi.iNeed Google Play]
* [[GNOME Do]] - Allows for quick finding miscellaneous artifacts of GNOME environment (applications, Evolution and Pidgin contacts, Firefox bookmarks, Rhythmbox artists and albums, and so on) and execute the basic actions on them (launch, open, email, chat, play, etc.).&lt;ref&gt;Ubuntu 10.04 Add/Remove Applications description for GNOME Do&lt;/ref&gt;
* [[hakia]] - hakia is an Internet search engine. The company has invented an alternative new infrastructure to indexing that uses SemanticRank algorithm, a solution mix from the disciplines of ontological semantics, fuzzy logic, computational linguistics, and mathematics.
* [[Lexxe]] - Lexxe is an Internet search engine that uses natural language processing for queries (semantic search). Searches can be made with keywords, phrases, and questions, such as "How old is Wikipedia?" When it comes to facts, Lexxe is quite effective, though needs much improvement in natural language analysis in the area of facts and in other areas.
* [http://www.mnemoo.com/ Mnemoo] - Mnemoo is an answer engine that aimed to directly answer questions posed in plain text (Natural Language), which is accomplished using a database of facts and an inference engine.
* [http://www.naturaldateandtime.com/ Natural Date and Time] - Natural language date and time zone engine. It allows you to ask questions about time, daylight saving information and to do time zone conversions via plain English questions such as 'What is the time in São Paulo when it is 6pm on the 2nd of June in Detroit'.
* [http://www.linguasys.com/web_production/server-item/NLUI%20Server NLUI Server] - an enterprise-oriented multilingual application server by LinguaSys for natural language user interface scripts, supporting English, Spanish, Portuguese, German, Japanese, Chinese, Pashto, Thai, Russian, Vietnamese, Malay, with Arabic, French, and more languages in development.
* [[Pikimal]] - Pikimal uses natural language tied to user preference to make search recommendations by template.
* [[Powerset (company)|Powerset]] — On May 11, 2008, the company unveiled a tool for searching a fixed subset of [[Wikipedia]] using conversational phrases rather than keywords.&lt;ref&gt;{{cite news |url=http://bits.blogs.nytimes.com/2008/05/12/powerset-debuts-with-search-of-wikipedia/ |title=Powerset Debuts With Search of Wikipedia |publisher=The New York Times |first=Miguel |last=Helft |date=May 12, 2008}}&lt;/ref&gt; On July 1, 2008, it was purchased by [[Microsoft]].&lt;ref&gt;{{cite web |url=http://www.powerset.com/blog/articles/2008/07/01/microsoft-to-acquire-powerset |archiveurl=https://web.archive.org/web/20090225064356/http://www.powerset.com/blog/articles/2008/07/01/microsoft-to-acquire-powerset |archivedate=February 25, 2009 |title=Microsoft to Acquire Powerset |publisher=Powerset Blog |first=Mark |last=Johnson |date=July 1, 2008}}&lt;/ref&gt;
* [[Q-go]] - The Q-go technology provides relevant answers to users in response to queries on a company’s internet website or corporate intranet, formulated in natural sentences or keyword input alike. Q-go was acquired by [[RightNow Technologies]] in 2011
* [[START (MIT project)]] - [http://start.csail.mit.edu/ START], Web-based question answering system. Unlike information retrieval systems such as search engines, START aims to supply users with "just the right information," instead of merely providing a list of hits. Currently, the system can answer millions of English questions about places, movies, people and dictionary definitions.
* [https://www.statmuse.com/ StatMuse] - Natural language analytics platform, currently in private beta with NBA data. Ask natural questions and get rich visualizations and raw data.
* [http://swingly.com/ Swingly] - Swingly is an answer engine designed to find exact answers to factual questions. Just ask a question in plain English - and Swingly will find you the answer (or answers) you're looking for (according to their site).
* [[Yebol]] - Yebol is a vertical "decision" search engine that had developed a knowledge-based, semantic search platform. Yebol's artificial intelligence human intelligence-infused algorithms automatically cluster and categorize search results, web sites, pages and content that it presents in a visually indexed format that is more aligned with initial human intent. Yebol uses association, ranking and clustering algorithms to analyze related keywords or web pages. Yebol integrates natural language processing, metasynthetic-engineered open complex systems, and machine algorithms with human knowledge for each query to establish a web directory that actually 'learns', using correlation, clustering and classification algorithms to automatically generate the knowledge query, which is retained and regenerated forward.&lt;ref&gt;Humphries, Matthew. [http://www.geek.com/articles/news/yebolcom-steps-into-the-search-market-20090731/ "Yebol.com steps into the search market"] ''Geek.com''. 31 July 2009.&lt;/ref&gt;

==See also==
*[[Attempto Controlled English]]
*[[Natural user interface]]
*[[Natural language programming]]
**[[xTalk]], a family of English-like programming languages
*[[Chatterbot]], a computer program that simulates human conversations
*[[Noisy text]]
*[[Question answering]]
*[[Selection-based search]]
*[[Semantic search]]
*[[Semantic query]]
*[[Semantic Web]]

==References==
{{reflist}}

{{Internet search}}
{{Computable knowledge}}

{{DEFAULTSORT:Natural language user interface}}
[[Category:User interfaces]]
[[Category:Artificial intelligence applications]]
[[Category:Natural language processing]]
[[Category:Computational linguistics]]
[[Category:Information retrieval techniques]]</text>
      <sha1>3gubgk0g4bzbev2zi5cim11f8g2zr7i</sha1>
    </revision>
  </page>
  <page>
    <title>Personalization</title>
    <ns>0</ns>
    <id>1656760</id>
    <revision>
      <id>759758478</id>
      <parentid>758632459</parentid>
      <timestamp>2017-01-13T00:52:40Z</timestamp>
      <contributor>
        <username>Ira Leviton</username>
        <id>25046916</id>
      </contributor>
      <minor />
      <comment>Fixed a typo.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="14711" xml:space="preserve">{{Multiple issues|
{{cleanup-reorganize|date=June 2008}}
{{refimprove|date=September 2015}}
{{tone|date=September 2015}}
}}

'''Personalization''', sometimes known as '''customization''', consists of tailoring a service or a product to accommodate specific individuals, sometimes tied to groups or segments of individuals.  A wide variety of organizations use personalization to improve [[customer satisfaction]], digital sales conversion, marketing results, branding, and improved website metrics as well as for [[advertising]]. Personalization is a key element in [[social media]] and [[recommender system]]s.

==Web pages==
[[Web page]]s can be personalized based on the characteristics (interests, social category, context, etc.), actions (click on button, open a link, etc.), intent (make a purchase, check status of an entity), or any other parameter that can be identified and associated with an individual, therefore providing them with a tailored user experience.  Note that the experience is rarely simply accommodation of the user but a relationship between the user and the desires of the site designers in driving specific actions to achieve objectives (e.g. Increase sales conversion on a page).  The term ''customization'' is often used when the site only uses explicit data such as product ratings or user preferences.

Technically, web personalization can be achieved by associating a visitor segment with a predefined action. Customizing the user experience based on behavioural, contextual and technical data is proven to have a positive impact on conversion rate optimization efforts. Associated actions can range from changing the content of a webpage, presenting a modal display, presenting interstitials, triggering a personalized email or even automating a phone call to the user.

According to a 2014 study from research firm Econsultancy, less than 30% of [[e-commerce]] websites have invested in the field of web personalization. However, many companies now offer services for web personalization as well as web and email recommendation systems that are based on personalization or anonymously-collected user behaviours.&lt;ref name=behaviours&gt;[http://online.wsj.com/article/SB10001424052748703294904575385532109190198.html ''Wall Street Journal'', “On the Web's Cutting Edge, Anonymity in Name Only”], August 4, 2010&lt;/ref&gt; According to a study done by Compass, e-commerce websites that use personalization can see an increase in revenue of as much as 29%. &lt;ref name=29%&gt;[http://blog.compass.co/improving-ecommerce-retention-revenue-personalization/ ''Compass Blog'', “Improving Ecommerce Retention and Revenue with Personalization”], August 11, 2016&lt;/ref&gt;

There are many categories of web personalization including
# Behavioral
# Contextual
# Technical
# Historic data
# Collaboratively filtered

There are several camps in defining and executing web personalization.  A few broad methods for web personalization may include:
# Implicit
# Explicit
# Hybrid

With implicit personalization, the web personalization is performed based on the different categories mentioned above. It can also be learned from direct interactions with the user based on implicit data, such as items purchased or pages viewed.&lt;ref&gt;{{cite web|last1=Flynn|first1=Lawrence|title=5 Things To Know About Siri And Google Now's Growing Intelligence|url=http://www.forbes.com/sites/parmyolson/2014/07/08/5-things-to-know-about-siri-and-google-nows-growing-intelligence/|website=Forbes}}&lt;/ref&gt; With explicit personalization, the web page (or information system) is changed by the user using the features provided by the system. Hybrid personalization combines the above two approaches to leverage the ''best of both worlds''.

Web personalization is can be linked to the notion of [[Adaptive hypermedia]] (AH). The main difference is that the former would usually work on what is considered an "open corpus hypermedia," whilst the latter would traditionally work on "closed corpus hypermedia." However, recent research directions in the AH domain take both closed and open corpus into account. Thus, the two fields are closely inter-related.

Personalization is also being considered for use in less overtly commercial applications to improve the user experience online.&lt;ref&gt;[[Jonathan Bowen|Bowen, J.P.]] and Filippini-Fantoni, S., [http://www.archimuse.com/mw2004/papers/bowen/bowen.html Personalization and the Web from a Museum Perspective]. In [[David Bearman]] and Jennifer Trant (eds.), ''[[Museums and the Web]] 2004: Selected Papers from an International Conference'', Arlington, Virginia, USA, 31 March – 3 April 2004. Archives &amp; Museum Informatics, pages 63–78, 2004.&lt;/ref&gt; Internet activist [[Eli Pariser]] has documented that search engines like [[Google]] and [[Yahoo! News]] give different results to different people (even when logged out). He also points out social media site [[Facebook]] changes user's friend feeds based on what it thinks they want to see. Pariser warns that these algorithms can create a "[[filter bubble]]" that prevents people from encountering a diversity of viewpoints beyond their own, or which only presents facts which confirm their existing views.

On an [[intranet]] or [[B2E]] [[Web portal#Enterprise Web portals|Enterprise Web portals]], personalization is often based on user attributes such as department, functional area, or role. The term "customization" in this context refers to the ability of users to modify the page layout or specify what content should be displayed.

==Digital media==
Another aspect of personalization is the increasing prevalence of [[open data]] on the Web. Many companies make their data available on the Web via [[API]]s, web services, and [[open data]] standards. One such example is Ordnance Survey Open Data.&lt;ref&gt;{{cite news| url=https://www.theguardian.com/news/datablog/2010/apr/02/ordnance-survey-open-data | location=London | work=The Guardian | first1=Chris | last1=Thorpe | first2=Simon | last2=Rogers | title=Ordnance Survey opendata maps: what does it actually include? | date=2 April 2010}}&lt;/ref&gt; Data made available in this way is structured to allow it to be inter-connected and re-used by third parties.&lt;ref&gt;{{cite web|url=http://www.cio.com/article/372363/Google_Opens_Up_Data_Center_For_Third_Party_Web_Applications |title=Google Opens Up Data Centre for Third Party Web Applications |publisher=Cio.com |date=2008-05-28 |accessdate=2013-01-16}}&lt;/ref&gt;

Data available from a user's personal [[social graph]] can be accessed by third-party [[application software]] to be suited to fit the personalized [[web page]] or [[information appliance]].

Current [[open data]] standards on the Web include:
# [[Attention Profiling Mark-up Language]] (APML)
# DataPortability
# [[OpenID]]
# [[OpenSocial]]

== Mobile phones ==

Over time mobile phones have seen an increased emphasis placed on user personalization. Far from the black and white screens and monophonic ringtones of the past, phones now offer interactive wallpapers and MP3 TruTones. In the UK and Asia, WeeMees have become popular. WeeMees are three-dimensional characters that are used as wallpaper and respond to the tendencies of the user. Video Graphics Array (VGA) picture quality allows people to change their background with ease without sacrificing quality. All of these services are downloaded through the provider with the goal to make the user feel connected to the phone.&lt;ref&gt;May, Harvey, and Greg Hearn. "The Mobile Phone as Media." International Journal of Cultural Studies 8.2 (2005): 195-211. Print.&lt;/ref&gt;

==Print media==
{{main|Mail merge}}

In print media, ranging from [[magazine]]s to [[admail|promotional publication]]s, personalization uses databases of individual recipients' information. Not only does the written document address itself by name to the reader, but the advertising is targeted to the recipient's demographics or interests using fields within the database, such as "first name", "last name", "company", etc.

The term "personalization" should not be confused with variable data, which is a much more granular method of marketing that leverages both images and text with the medium, not just fields within a database. Although personalized children's books are created by companies who are using and leveraging all the strengths of [[variable data printing|variable data printing (VDP)]]. This allows for full image and text variability within a printed book.
With the advent of online 3D printing services such as Shapeways and Ponoko we are seeing personalization enter into the realms of product design.

== Promotional merchandise ==
Promotional items ([[mug]]s, [[T-shirt]]s, [[keychain]]s, [[ball]]s etc.) are regularly personalized. Personalized children's storybooks—wherein the child becomes the [[protagonist]], with the name and image of the child personalized—are also popular. Personalized CDs for children also exist. With the advent of [[digital printing]], personalized calendars that start in any month, birthday cards, cards, e-cards, posters and photo books can also be obtained.

== 3D printing ==
3D printing is a production method that allows to create unique and personalized items on a global scale. Personalized apparel and accessories, such as jewellery, are increasing in popularity.&lt;ref&gt;{{cite web|url=http://www.jewellermagazine.com/Article.aspx?id=2167&amp;h=New-jewellery-website-targets-|title=New jewellery website targets 'customisers'|last=Weinman|first=Aaron|date=21 February 2012|publisher=Jeweller Magazine|language=|accessdate=6 January 2015}}&lt;/ref&gt; This kind of customization is also relevant in other areas like Consumer Electronics&lt;ref&gt;{{Cite web|url=http://www.3ders.org/articles/20160121-philips-launches-worlds-first-personalized-3d-printed-face-shaver-for-limited-edition-run.html|title=Philips launches the world's first personalized, 3D printed face shaver for limited edition run|website=3ders.org|language=en-US|access-date=2016-03-02}}&lt;/ref&gt; and Retail.&lt;ref&gt;{{Cite web|url=http://twikblog.twikit.com/belgian-3d-company-twikit-brings-3d-customization-french-retail/|title=Twikit brings 3D customization to French retail.|website=Twikit Blog {{!}} 3D Customization, 3D Printing|language=en-US|access-date=2016-03-02}}&lt;/ref&gt; By combining 3D printing with complex software a product can easily be customized by an end-user.

== Mass personalization ==

{{tone|section|date=January 2011}}

Mass personalization is defined as custom tailoring by a company in accordance with its end users tastes and preferences.&lt;ref&gt;{{cite web|url=http://www.answers.com/personalization&amp;r=67 |title=personalize: Definition, Synonyms from |publisher=Answers.com |date= |accessdate=2013-01-16}}&lt;/ref&gt; From collaborative engineering perspective, mass customization can be viewed as collaborative efforts between customers and manufacturers, who have different sets of priorities and need to jointly search for solutions that best match customers' individual specific needs with manufacturers' customization capabilities.&lt;ref&gt;	Chen, S., Y. Wang and M. M. Tseng. 2009. Mass Customization as a Collaborative Engineering Effort. International Journal of Collaborative Engineering, 1(2): 152-167&lt;/ref&gt; The main difference between mass customization and mass personalization is that customization is the ability for a company to give its customers an opportunity to create and choose product to certain specifications, but does have limits.&lt;ref&gt;Haag et al., ''Management Information Systems for the Information Age'', 3rd edition, 2006, page 331.&lt;/ref&gt;

A website knowing a user's location, and buying habits, will present offers and suggestions tailored to the user's demographics; this is an example of mass personalization. The personalization is not individual but rather the user is first classified and then the personalization is based on the group they belong to.&lt;ref&gt;{{cite news| url=http://www.telegraph.co.uk/foodanddrink/9808015/How-supermarkets-prop-up-our-class-system.html | location=London | work=The Daily Telegraph | first=Harry | last=Wallop | title=How supermarkets prop up our class system | date=2013-01-18}}&lt;/ref&gt;

[[Behavioral targeting]] represents a concept that is similar to mass personalization.

== Predictive personalization ==

Predictive personalization is defined as the ability to predict customer behavior, needs or wants - and tailor offers and communications very precisely.&lt;ref&gt;{{cite web|url=http://www.slideshare.net/jwtintelligence/jwt-10-trends-for-2013-executive-summary|title=10 Trends for 2013 Executive Summary: Definition, Projected Trends |publisher=JWTIntelligence.com |date= |accessdate=2012-12-04}}&lt;/ref&gt;  Social data is one source of providing this predictive analysis, particularly social data that is structured.  Predictive personalization is a much more recent means of personalization and can be used well to augment current personalization offerings.

== Map personalization ==
{{Expand section|date=September 2015}}Digital [[Web mapping|web maps]] are also being personalized. [[Google Maps]] change the content of the map based on previous searches and other profile information.&lt;ref&gt;{{Cite web|title = The Next Frontier For Google Maps Is Personalization|url = http://social.techcrunch.com/2013/02/01/the-next-frontier-for-google-maps-is-personalization/|website = TechCrunch|accessdate = 2015-09-13|first = Frederic|last = Lardinois}}&lt;/ref&gt; Technology writer [[Evgeny Morozov]] has criticized map personalization as a threat to [[public space]].&lt;ref&gt;{{Cite news|title = My Map or Yours?|url = http://www.slate.com/articles/technology/future_tense/2013/05/google_maps_personalization_will_hurt_public_space_and_engagement.html|newspaper = Slate|date = 2013-05-28|access-date = 2015-09-13|issn = 1091-2339|language = en|first = Evgeny|last = Morozov}}&lt;/ref&gt;

==See also==
* [[Adaptation (computer science)]]
* [[Mass customization]]
* [[Adaptive hypermedia]]
* [[Behavioral targeting]]
* [[Bespoke]]
* [[Collaborative filtering]]
* [[Configurator]]
* [[Personalized learning]]
* [[Preorder economy]]
* [[Real-time marketing]]
* [[Recommendation system]]
* [[User modeling]]

==References==
{{reflist|2}}

==External links==
* [http://www.iimcp.org International Institute on Mass Customization &amp; Personalization which organizes MCP, a biannual conference on customization and personalization]
* [http://www.umuai.org/ User Modeling and User-Adapted Interaction (UMUAI)] ''The Journal of Personalization Research''

[[Category:Human–computer interaction]]
[[Category:World Wide Web]]
[[Category:User interface techniques]]
[[Category:Usability|Personas]]
[[Category:Types of marketing]]
[[Category:Information retrieval techniques]]</text>
      <sha1>4o61uootwufctou2edfvdobhvups9g3</sha1>
    </revision>
  </page>
  <page>
    <title>Preference learning</title>
    <ns>0</ns>
    <id>34072838</id>
    <revision>
      <id>761915121</id>
      <parentid>747619165</parentid>
      <timestamp>2017-01-25T15:47:54Z</timestamp>
      <contributor>
        <ip>2A02:8109:8A40:54A4:30F5:4A84:F46D:95A2</ip>
      </contributor>
      <comment>made the english clearer</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9148" xml:space="preserve">'''Preference learning''' is a subfield in [[machine learning]] in which the goal is to learn a predictive [[Preference (economics)|preference]] model from observed preference information.&lt;ref&gt;[[Mehryar Mohri]], Afshin Rostamizadeh, Ameet Talwalkar (2012) ''Foundations of Machine Learning'', The
MIT Press ISBN 9780262018258.&lt;/ref&gt; In the view of [[supervised learning]], preference learning trains on a set of items which have preferences toward labels or other items and predicts the preferences for all items.

While the concept of preference learning has been emerged for some time in many fields such as [[economics]],&lt;ref name="SHOG00" /&gt; it's a relatively new topic in [[Artificial Intelligence]] research. Several workshops have been discussing preference learning and related topics in the past decade.&lt;ref name="WEB:WORKSHOP" /&gt;

==Tasks==

The main task in preference learning concerns problems in "[[learning to rank]]". According to different types of preference information observed, the tasks are categorized as three main problems in the book ''Preference Learning'':&lt;ref name="FURN11" /&gt;

===Label ranking===

In label ranking, the model has an instance space &lt;math&gt;X=\{x_i\}\,\!&lt;/math&gt; and a finite set of labels &lt;math&gt;Y=\{y_i|i=1,2,\cdots,k\}\,\!&lt;/math&gt;. The preference information is given in the form &lt;math&gt;y_i \succ_{x} y_j\,\!&lt;/math&gt; indicating instance &lt;math&gt;x\,\!&lt;/math&gt; shows preference in &lt;math&gt;y_i\,\!&lt;/math&gt; rather than &lt;math&gt;y_j\,\!&lt;/math&gt;. A set of preference information is used as training data in the model. The task of this model is to find a preference ranking among the labels for any instance.

It was observed some conventional [[Classification in machine learning|classification]] problems can be generalized in the framework of label ranking problem:&lt;ref name="HARP03" /&gt; if a training instance &lt;math&gt;x\,\!&lt;/math&gt; is labeled as class &lt;math&gt;y_i\,\!&lt;/math&gt;, it implies that &lt;math&gt;\forall j \neq i, y_i \succ_{x} y_j\,\!&lt;/math&gt;. In the [[Multi-label classification|multi-label]] case, &lt;math&gt;x\,\!&lt;/math&gt; is associated with a set of labels &lt;math&gt;L \subseteq Y\,\!&lt;/math&gt; and thus the model can extract a set of preference information &lt;math&gt;\{y_i \succ_{x} y_j | y_i \in L, y_j \in Y\backslash L\}\,\!&lt;/math&gt;. Training a preference model on this preference information and the classification result of an instance is just the corresponding top ranking label.

===Instance ranking===

Instance ranking also has the instance space &lt;math&gt;X\,\!&lt;/math&gt; and label set &lt;math&gt;Y\,\!&lt;/math&gt;. In this task, labels are defined to have a fixed order &lt;math&gt;y_1 \succ y_2 \succ \cdots \succ y_k\,\!&lt;/math&gt; and each instance &lt;math&gt;x_l\,\!&lt;/math&gt; is associated with a label &lt;math&gt;y_l\,\!&lt;/math&gt;. Giving a set of instances as training data, the goal of this task is to find the ranking order for a new set of instances.

===Object ranking===

Object ranking is similar to instance ranking except that no labels are associated with instances. Given a set of pairwise preference information in the form &lt;math&gt;x_i \succ x_j\,\!&lt;/math&gt; and the model should find out a ranking order among instances.

==Techniques==

There are two practical representations of the preference information &lt;math&gt;A \succ B\,\!&lt;/math&gt;. One is assigning &lt;math&gt;A\,\!&lt;/math&gt; and &lt;math&gt;B\,\!&lt;/math&gt; with two real numbers &lt;math&gt;a\,\!&lt;/math&gt; and &lt;math&gt;b\,\!&lt;/math&gt; respectively such that &lt;math&gt;a &gt; b\,\!&lt;/math&gt;. Another one is assigning a binary value &lt;math&gt;V(A,B) \in \{0,1\}\,\!&lt;/math&gt; for all pairs &lt;math&gt;(A,B)\,\!&lt;/math&gt; denoting whether &lt;math&gt;A \succ B\,\!&lt;/math&gt; or &lt;math&gt;B \succ A\,\!&lt;/math&gt;. Corresponding to these two different representations, there are two different techniques applied to the learning process.

===Utility function===

If we can find a mapping from data to real numbers, ranking the data can be solved by ranking the real numbers. This mapping is called [[utility function]]. For label ranking the mapping is a function &lt;math&gt;f: X \times Y \rightarrow \mathbb{R}\,\!&lt;/math&gt; such that &lt;math&gt;y_i \succ_x y_j \Rightarrow f(x,y_i) &gt; f(x,y_j)\,\!&lt;/math&gt;. For instance ranking and object ranking, the mapping is a function &lt;math&gt;f: X \rightarrow \mathbb{R}\,\!&lt;/math&gt;.

Finding the utility function is a [[Regression analysis|regression]] learning problem which is well developed in machine learning.

===Preference relations===

The binary representation of preference information is called preference relation. For each pair of alternatives (instances or labels), a binary predicate can be learned by conventional supervising learning approach. Fürnkranz, Johannes and Hüllermeier proposed this approach in label ranking problem.&lt;ref name="FURN03" /&gt; For object ranking, there is an early approach by Cohen et al.&lt;ref name="COHE98" /&gt;

Using preference relations to predict the ranking will not be so intuitive. Since preference relation is not transitive, it implies that the solution of ranking satisfying those relations would sometimes be unreachable, or there could be more than one solution. A more common approach is to find a ranking solution which is maximally consistent with the preference relations. This approach is a natural extension of pairwise classification.&lt;ref name="FURN03" /&gt;

==Uses==

Preference learning can be used in ranking search results according to feedback of user preference. Given a query and a set of documents, a learning model is used to find the ranking of documents corresponding to the relevance with this query. More discussions on research in this field can be found in Tie-Yan Liu's survey paper.&lt;ref name="LIU09" /&gt;

Another application of preference learning is [[recommender systems]].&lt;ref name="GEMM09" /&gt; Online store may analyze customer's purchase record to learn a preference model and then recommend similar products to customers. Internet content providers can make use of user's ratings to provide more user preferred contents.

==See also==
*[[Learning to rank]]

==References==

{{Reflist|
refs=

&lt;ref name="SHOG00"&gt;{{
cite journal
|last       = Shogren
|first      = Jason F. |author2=List, John A. |author3=Hayes, Dermot J.
|year       = 2000
|title      = Preference Learning in Consecutive Experimental Auctions
|url        = http://econpapers.repec.org/article/oupajagec/v_3a82_3ay_3a2000_3ai_3a4_3ap_3a1016-1021.htm
|journal    = American Journal of Agricultural Economics
|volume     = 82
|pages      = 1016–1021
|doi=10.1111/0002-9092.00099
}}&lt;/ref&gt;

&lt;ref name="WEB:WORKSHOP"&gt;{{
cite web
|title      = Preference learning workshops
|url        = http://www.preference-learning.org/#Workshops
}}&lt;/ref&gt;

&lt;ref name="FURN11"&gt;{{
cite book
|last       = F&amp;uuml;rnkranz
|first      = Johannes
|coauthors  = H&amp;uuml;llermeier, Eyke
|year       = 2011
|title      = Preference Learning
|url        = https://books.google.com/books?id=nc3XcH9XSgYC
|chapter    = Preference Learning: An Introduction
|chapterurl = https://books.google.com/books?id=nc3XcH9XSgYC&amp;pg=PA4
|publisher  = Springer-Verlag New York, Inc.
|pages      = 3–8
|isbn       = 978-3-642-14124-9
}}&lt;/ref&gt;

&lt;ref name="HARP03"&gt;{{
cite journal
|last       = Har-peled
|first      = Sariel |author2=Roth, Dan |author3=Zimak, Dav
|year       = 2003
|title      = Constraint classification for multiclass classification and ranking
|journal    = In Proceedings of the 16th Annual Conference on Neural Information Processing Systems, NIPS-02
|pages      = 785–792
}}&lt;/ref&gt;

&lt;ref name="FURN03"&gt;{{
cite journal
|last       = F&amp;uuml;rnkranz
|first      = Johannes
|coauthors  = H&amp;uuml;llermeier, Eyke
|year       = 2003
|title      = Pairwise Preference Learning and Ranking
|journal    = Proceedings of the 14th European Conference on Machine Learning
|pages      = 145–156
}}&lt;/ref&gt;

&lt;ref name="COHE98"&gt;{{
cite journal
|last       = Cohen
|first      = William W. |author2=Schapire, Robert E. |author3=Singer, Yoram
|year       = 1998
|title      = Learning to order things
|url        = http://dl.acm.org/citation.cfm?id=302528.302736
|journal    = In Proceedings of the 1997 Conference on Advances in Neural Information Processing Systems
|pages      = 451–457
}}&lt;/ref&gt;

&lt;ref name="LIU09"&gt;{{
cite journal
|last       = Liu
|first      = Tie-Yan
|year       = 2009
|title      = Learning to Rank for Information Retrieval
|url        = http://dl.acm.org/citation.cfm?id=1618303.1618304
|journal    = Foundations and Trends in Information Retrieval
|volume     = 3
|issue      = 3
|pages      = 225–331
|doi        = 10.1561/1500000016
}}&lt;/ref&gt;

&lt;ref name="GEMM09"&gt;{{
cite journal
|last       = Gemmis
|first      = Marco De
|author2=Iaquinta, Leo |author3=Lops, Pasquale |author4=Musto, Cataldo |author5=Narducci, Fedelucio |author6= Semeraro,Giovanni 
|year       = 2009
|title      = Preference Learning in Recommender Systems
|url        = http://www.ecmlpkdd2009.net/wp-content/uploads/2008/09/preference-learning.pdf#page=45
|journal    = PREFERENCE LEARNING
|volume     = 41
|pages      = 387–407
|doi=10.1007/978-3-642-14125-6_18
}}&lt;/ref&gt;

}}

==External links==
*[http://www.preference-learning.org/ Preference Learning site]

[[Category:Information retrieval techniques]]
[[Category:Machine learning]]</text>
      <sha1>1zke4qcza7oaylt47fe8dk0e4201sj2</sha1>
    </revision>
  </page>
  <page>
    <title>Cosine similarity</title>
    <ns>0</ns>
    <id>8966592</id>
    <revision>
      <id>746257427</id>
      <parentid>745993620</parentid>
      <timestamp>2016-10-26T07:39:27Z</timestamp>
      <contributor>
        <username>Pengo</username>
        <id>35807</id>
      </contributor>
      <comment>/* top */ copyedit so it doesn't sound like two editors fighting with each other</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="13288" xml:space="preserve">'''Cosine similarity''' is a measure of similarity between two non zero vectors of an [[inner product space]] that measures the [[cosine]] of the angle between them. The cosine of 0° is 1, and it is less than 1 for any other angle. It is thus a judgment of orientation and not magnitude: two vectors with the same orientation have a cosine similarity of 1, two vectors at 90° have a similarity of 0, and two vectors diametrically opposed have a similarity of -1, independent of their magnitude. Cosine similarity is particularly used in positive space, where the outcome is neatly bounded in [0,1]. The name derives from the term "direction cosine": in this case, note that unit vectors are maximally "similar" if they're parallel and maximally "dissimilar" if they're orthogonal (= perpendicular).  It should not escape the alert reader's attention that this is analogous to cosine, which is unity (maximum value) when the segments subtend a zero angle and zero (uncorrelated) when the segments are perpendicular.

Note that these bounds apply for any number of dimensions, and cosine similarity is most commonly used in high-dimensional positive spaces. For example, in [[information retrieval]] and [[text mining]], each term is notionally assigned a different dimension and a document is characterised by a vector where the value of each dimension corresponds to the number of times that term appears in the document. Cosine similarity then gives a useful measure of how similar two documents are likely to be in terms of their subject matter.&lt;ref&gt;Singhal, Amit (2001). "Modern Information Retrieval: A Brief Overview". Bulletin of the IEEE Computer Society Technical Committee on Data Engineering 24 (4): 35–43.&lt;/ref&gt;

The technique is also used to measure cohesion within clusters in the field of [[data mining]].&lt;ref&gt;P.-N. Tan, M. Steinbach &amp; V. Kumar, "Introduction to Data Mining", , Addison-Wesley (2005), ISBN 0-321-32136-7, chapter 8; page 500.&lt;/ref&gt;

''Cosine distance'' is a term often used for the complement in positive space, that is: &lt;math&gt;D_C(A,B) = 1 - S_C(A,B)&lt;/math&gt;. It is important to note, however, that this is not a proper [[distance metric]] as it does not have the [[triangle inequality]] property—or, more formally, the [[Schwarz inequality]]—and it violates the coincidence axiom; to repair the triangle inequality property while maintaining the same ordering, it is necessary to convert to angular distance (see below.)

One of the reasons for the popularity of cosine similarity is that it is very efficient to evaluate, especially for sparse vectors, as only the non-zero dimensions need to be considered.

==Definition==

The cosine of two non zero vectors can be derived by using the [[Euclidean vector#Dot product|Euclidean dot product]] formula:

:&lt;math&gt;\mathbf{a}\cdot\mathbf{b}
=\left\|\mathbf{a}\right\|\left\|\mathbf{b}\right\|\cos\theta&lt;/math&gt;

Given two [[Vector (geometric)|vectors]] of attributes, ''A'' and ''B'', the cosine similarity, ''cos(θ)'', is represented using a [[dot product]] and [[Magnitude (mathematics)#Euclidean vector space|magnitude]] as

:&lt;math&gt;\text{similarity} = \cos(\theta) = {\mathbf{A} \cdot \mathbf{B} \over \|\mathbf{A}\| \|\mathbf{B}\|} = \frac{ \sum\limits_{i=1}^{n}{A_i  B_i} }{ \sqrt{\sum\limits_{i=1}^{n}{A_i^2}}  \sqrt{\sum\limits_{i=1}^{n}{B_i^2}} }&lt;/math&gt; , where &lt;math&gt;A_i&lt;/math&gt; and &lt;math&gt;B_i&lt;/math&gt; are [[Euclidean vector#Decomposition|components]] of vector &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;B&lt;/math&gt; respectively.

The resulting similarity ranges from &amp;minus;1 meaning exactly opposite, to 1 meaning exactly the same, with 0 indicating orthogonality (decorrelation), and in-between values indicating intermediate similarity or dissimilarity.

For text matching, the attribute vectors ''A'' and ''B'' are usually the [[tf-idf|term frequency]] vectors of the documents.  The cosine similarity can be seen as a method of normalizing document length during comparison.

In the case of [[information retrieval]], the cosine similarity of two documents will range from 0 to 1, since the term frequencies ([[tf-idf]] weights) cannot be negative. The angle between two term frequency vectors cannot be greater than&amp;nbsp;90°.

If the attribute vectors are normalized by subtracting the vector means (e.g., &lt;math&gt;A - \bar{A}&lt;/math&gt;), the measure is called centered cosine similarity and is equivalent to the [[Pearson product-moment correlation coefficient#For a sample|Pearson Correlation Coefficient]].

=== Angular distance and similarity ===

The term "cosine similarity" is sometimes used to refer to different definition of similarity provided below. However the most common use of "cosine similarity" is as defined above and the similarity and distance metrics defined below are referred to as "angular similarity" and "angular distance" respectively. The normalized angle between the vectors is a formal [[distance metric]] and can be calculated from the similarity score defined above. This angular distance metric can then be used to compute a similarity function bounded between 0 and 1, inclusive.

When the vector elements may be positive or negative:

:&lt;math&gt;\text{distance} = \frac{ \cos^{-1}( \text{similarity} ) }{ \pi } &lt;/math&gt;

:&lt;math&gt;\text{similarity} = 1 - \text{distance} &lt;/math&gt;

Or, if the vector elements are always positive:

:&lt;math&gt;\text{distance} = \frac{ 2 \cdot \cos^{-1}( \text{similarity} ) }{ \pi }&lt;/math&gt;

:&lt;math&gt;\text{similarity} = 1 - \text{distance}&lt;/math&gt;

Although the term "cosine similarity" has been used for this angular distance, the term is oddly used as the cosine of the angle is used only as a convenient mechanism for calculating the angle itself and is no part of the meaning. The advantage of the angular similarity coefficient is that, when used as a difference coefficient (by subtracting it from 1) the resulting function is a proper [[distance metric]], which is not the case for the first meaning. However, for most uses this is not an important property. For any use where only the relative ordering of similarity or distance within a set of vectors is important, then which function is used is immaterial as the resulting order will be unaffected by the choice.

=== Confusion with "Tanimoto" coefficient ===

The cosine similarity may be easily confused with the Tanimoto metric - a specialised form of a similarity coefficient with a similar algebraic form:

:&lt;math&gt;T(A,B) = {A \cdot B \over \|A\|^2 +\|B\|^2 - A \cdot B}&lt;/math&gt;

In fact, this algebraic form [[Jaccard index#Tanimoto Similarity and Distance|was first defined by Tanimoto]] as a mechanism for calculating the [[Jaccard coefficient]] in the case where the sets being compared are represented as [[bit vector]]s. While the formula extends to vectors in general, it has quite different properties from cosine similarity and bears little relation other than its superficial appearance.

=== Ochiai coefficient ===
This coefficient is also known in biology as Ochiai coefficient, or Ochiai-Barkman coefficient, or Otsuka-Ochiai coefficient:&lt;ref&gt;''Ochiai A.'' Zoogeographical studies on the soleoid fishes found Japan and its neighboring regions. II // Bull. Jap. Soc. sci. Fish. 1957. V. 22. № 9. P. 526-530.&lt;/ref&gt;&lt;ref&gt;''Barkman J.J.'' Phytosociology and ecology of cryptogamic epiphytes, including a taxonomic survey and description of their vegetation units in Europe. – Assen. Van Gorcum. 1958. 628 p.&lt;/ref&gt;
:&lt;math&gt;K =\frac{n(A \cap B)}{\sqrt{n(A) \times n(B)}}&lt;/math&gt;
Here, &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;B&lt;/math&gt; are sets, and &lt;math&gt;n(A)&lt;/math&gt; is the number of elements in &lt;math&gt;A&lt;/math&gt;. If sets are represented as [[bit vector]]s, the Ochiai coefficient can be seen to be the same as the cosine similarity.

== Properties ==
Cosine similarity is related to [[Euclidean distance]] as follows. Denote Euclidean distance by the usual &lt;math&gt;\|A - B\|&lt;/math&gt;, and observe that

:&lt;math&gt;\|A - B\|^2 = (A - B)^\top (A - B) = \|A\|^2 + \|B\|^2 - 2 A^\top B&lt;/math&gt;

by [[Polynomial expansion|expansion]]. When {{mvar|A}} and {{mvar|B}} are normalized to unit length, &lt;math&gt;\|A\|^2 = \|B\|^2 = 1&lt;/math&gt; so the previous is equal to

:&lt;math&gt;2 (1 - \cos(A, B))&lt;/math&gt;

'''Null distribution:''' For data which can be negative as well as positive, the [[null distribution]] for cosine similarity is the distribution of the dot product of two independent random unit vectors. This distribution has a [[mean]] of zero and a [[variance]] of &lt;math&gt;1/n&lt;/math&gt; (where &lt;math&gt;n&lt;/math&gt; is the number of dimensions), and although the distribution is bounded between -1 and +1, as &lt;math&gt;n&lt;/math&gt; grows large the distribution is increasingly well-approximated by the [[normal distribution]].&lt;ref&gt;{{cite journal
 | author = Spruill, Marcus C
 | year = 2007
 | title = Asymptotic distribution of coordinates on high dimensional spheres
 | journal = Electronic communications in probability
 | volume = 12 | pages = 234–247
 | doi = 10.1214/ECP.v12-1294
}}&lt;/ref&gt;&lt;ref&gt;[http://stats.stackexchange.com/questions/85916/distribution-of-dot-products-between-two-random-unit-vectors-in-mathbbrd CrossValidated: Distribution of dot products between two random unit vectors in RD]&lt;/ref&gt;
For other types of data, such as bitstreams (taking values of 0 or 1 only), the null distribution will take a different form, and may have a nonzero mean.&lt;ref&gt;{{cite journal
 | author = Graham L. Giller 
 | year = 2012
 | title = The Statistical Properties of Random Bitstreams and the Sampling Distribution of Cosine Similarity
 | journal = Giller Investments Research Notes
 | number = 20121024/1
 | doi = 10.2139/ssrn.2167044
}}&lt;/ref&gt;

== Soft cosine measure ==
Soft cosine measure
is a measure of “soft” similarity between two vectors, i.e., the measure that considers similarity of pairs of features.&lt;ref&gt;{{cite journal|last1=Sidorov|first1=Grigori|last2=Gelbukh|first2=Alexander|last3=Gómez-Adorno|first3=Helena|last4=Pinto|first4=David|title=Soft Similarity and Soft Cosine Measure: Similarity of Features in Vector Space Model|journal=Computación y Sistemas|volume=18|issue=3|pages=491–504|doi=10.13053/CyS-18-3-2043|url=http://cys.cic.ipn.mx/ojs/index.php/CyS/article/view/2043|accessdate=7 October 2014}}&lt;/ref&gt; The traditional cosine similarity considers the [[vector space model]] (VSM) features as independent or completely different, while the soft cosine measure proposes considering the similarity of features in VSM, which allows generalization of the concepts of cosine measure and also the idea of similarity (soft similarity).

For example, in the field of [[natural language processing]] (NLP) the similarity among features is quite intuitive. Features such as words, n-grams or syntactic n-grams&lt;ref&gt;{{cite book|last1=Sidorov|first1=Grigori|last2=Velasquez|first2=Francisco|last3=Stamatatos|first3=Efstathios|last4=Gelbukh|first4=Alexander|last5=Chanona-Hernández|first5=Liliana|title=Syntactic Dependency-based N-grams as Classification Features|publisher=LNAI 7630|isbn=978-3-642-37798-3|pages=1–11|url=http://link.springer.com/chapter/10.1007%2F978-3-642-37798-3_1|accessdate=7 October 2014}}&lt;/ref&gt; can be quite similar, though formally they are considered as different features in the VSM. For example, words “play” and “game” are different words and thus are mapped to different dimensions in VSM; yet it is obvious that they are related semantically. In case of [[n-grams]] or syntactic n-grams, [[Levenshtein distance]] can be applied (in fact, Levenshtein distance can be applied to words as well).

For calculation of the soft cosine measure, the matrix {{math|'''s'''}} of similarity between features is introduced. It can be calculated using Levenshtein distance or other similarity measures, e.g., various [[WordNet]] similarity measures. Then we just multiply by this matrix.

Given two {{math|''N''}}-dimension vectors a and b, the soft cosine similarity is calculated as follows:

:&lt;math&gt;\begin{align}
    \operatorname{soft\_cosine}_1(a,b)=
    \frac{\sum\nolimits_{i,j}^N s_{ij}a_ib_j}{\sqrt{\sum\nolimits_{i,j}^N s_{ij}a_ia_j}\sqrt{\sum\nolimits_{i,j}^N s_{ij}b_ib_j}},
\end{align}
&lt;/math&gt;

where {{math|''s&lt;sub&gt;ij&lt;/sub&gt;'' {{=}} similarity(feature&lt;sub&gt;''i''&lt;/sub&gt;, feature&lt;sub&gt;''j''&lt;/sub&gt;)}}.

If there is no similarity between features ({{math|''s&lt;sub&gt;ii&lt;/sub&gt;'' {{=}} 1}}, {{math|''s&lt;sub&gt;ij&lt;/sub&gt;'' {{=}} 0}} for {{math|''i'' ≠ ''j''}}), the given equation is equivalent to the conventional cosine similarity formula.

The complexity of this measure is quadratic, which makes it perfectly applicable to real world tasks. The complexity can be transformed to subquadratic.{{citation needed|date=December 2015}}

== See also ==
* [[Sørensen similarity index|Sørensen's quotient of similarity]]
* [[Hamming distance]]
* [[Correlation]]
* [[Dice's coefficient]]
* [[Jaccard index]]
* [[SimRank]]
* [[Information retrieval]]

==References==
{{reflist}}

== External links ==
* [http://mathforum.org/kb/message.jspa?messageID=5658016&amp;tstart=0 Weighted cosine measure]
* [http://blog.christianperone.com/?p=2497 A tutorial on cosine similarity using Python]
* [http://www.rxnlp.com/api-reference/text-similarity-api-reference/ Web API to Compute Cosine, Jaccard and Dice for Text in Any Language]

{{DEFAULTSORT:Cosine Similarity}}
[[Category:Information retrieval techniques]]</text>
      <sha1>mjcpqykfp5mv0l3sm9lwhcbd12rj6yj</sha1>
    </revision>
  </page>
  <page>
    <title>Communication engine</title>
    <ns>0</ns>
    <id>20895417</id>
    <revision>
      <id>666862131</id>
      <parentid>601817015</parentid>
      <timestamp>2015-06-14T05:57:23Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>Category:Information retrieval techniques</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="788" xml:space="preserve">{{Orphan|date=February 2009}}
A '''communication engine''' is a tool that sends user requests to several other [[communication protocols]] and/or [[database]]s and aggregates the results into a single list or displays them according to their source. Communication engines enable users to enter communication account authorization once and access several communication avenues simultaneously. Communication engines operate on the premise that the [[World Wide Web]] is too large for any one engine to index it all and that more productive results can be obtained by combining the results from several engines dynamically. This may save the user from having to use multiple engines separately.

[[Category:Information retrieval techniques]]
[[Category:Computing terminology]]


{{Web-stub}}</text>
      <sha1>lkkw3xdyj3egmrkz5m27v1tcdii414r</sha1>
    </revision>
  </page>
  <page>
    <title>Personalized search</title>
    <ns>0</ns>
    <id>28010520</id>
    <revision>
      <id>761045176</id>
      <parentid>761045145</parentid>
      <timestamp>2017-01-20T15:23:38Z</timestamp>
      <contributor>
        <ip>168.184.14.110</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="33188" xml:space="preserve">{{Multiple issues|
{{essay-like|date=January 2015}}
{{original research|date=January 2015}}
}}

'''Personalized search''' refers to [[web search]] experiences that are tailored specifically to an individual's interests by incorporating information about the individual beyond specific query provided. Pitkow et al. describe two general approaches to [[personalizing]] search results, one involving modifying the user's query and the other re-ranking search results.&lt;ref&gt;{{cite journal|last=Pitokow|first=James|author2=Hinrich Schütze |author3=Todd Cass |author4=Rob Cooley |author5=Don Turnbull |author6=Andy Edmonds |author7=Eytan Adar |author8=Thomas Breuel |title=Personalized search|journal=Communications of the ACM |year=2002|volume=45|issue=9|pages=50–55|url=http://portal.acm.org/citation.cfm?doid=567498.567526}}&lt;/ref&gt;

==History==

[[Google]] introduced personalized search in 2004 and it was implemented in 2005 to Google search. Google has personalized search implemented for all users, not only those with a Google account. There is not very much information on how exactly Google personalizes their searches; however, it is believed that they use user language, location, and [[web history]].&lt;ref&gt;{{cite conference | url=http://personalization.ccs.neu.edu/paper.pdf | title=Measuring Personalization of Web Search | year=2013 | archiveurl=https://web.archive.org/web/20130425195202/http://personalization.ccs.neu.edu/paper.pdf | archivedate=April 25, 2013 | deadurl=y|author1=Aniko Hannak|author2=Piotr Sapiezynski|author3=Arash Molavi Kakhki|author4=Balachander Krishnamurthy|author5=David Lazer|author6=Alan Mislove|author7=Christo Wilson}}&lt;/ref&gt;

Early [[search engine]]s, like [[Google]] and [[AltaVista]], found results based only on key words. Personalized search, as pioneered by Google, has become far more complex with the goal to "understand exactly what you mean and give you exactly what you want."&lt;ref name=Remerowski&gt;{{cite AV media| last=Remerowski|first=Ted|title=National Geographic: Inside Google|year=2013}}&lt;/ref&gt; Using mathematical algorithms, search engines are now able to return results based on the number of links to and from sites; the more links a site has, the higher it is placed on the page.&lt;ref name=Remerowski/&gt; Search engines have two degrees of expertise: the shallow expert and the deep expert. An expert from the shallowest degree serves as a witness who knows some specific information on a given event. A deep expert, on the other hand, has comprehensible knowledge that gives it the capacity to deliver unique information that is relevant to each individual inquirer.&lt;ref name=Simpson&gt;{{cite journal|last=Simpson|first=Thomas|title=Evaluating Google as an epistemic tool|journal=Metaphilosophy|year=2012|volume=43|issue=4|pages=969–982}}&lt;/ref&gt; If a person knows what he or she wants than the search engine will act as a shallow expert and simply locate that information. But search engines are also capable of deep expertise in that they rank results indicating that those near the top are more relevant to a user's wants than those below.&lt;ref name=Simpson/&gt;

While many search engines take advantage of information about people in general, or about specific groups of people, personalized search depends on a user profile that is unique to the individual. Research systems that personalize search results model their users in different ways. Some rely on users explicitly specifying their interests or on demographic/cognitive characteristics.&lt;ref&gt;{{cite journal|last=Ma|first=Z.|author2=Pant, G. |author3=Sheng, O. |title=Interest-based personalized search.|journal=ACM TOIS|year=2007|volume=25|issue=5}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last=Frias-Martinez|first=E.|author2=Chen, S.Y. |author3=Liu, X. |title=Automatic cognitive style identification of digital library users for personalization.|journal=JASIST|year=2007|volume=58|issue=2|pages=237–251|doi=10.1002/asi.20477}}&lt;/ref&gt; However, user-supplied information can be difficult to collect and keep up to date. Others have built implicit user models based on content the user has read or their history of interaction with Web pages.&lt;ref&gt;{{cite journal|last=Chirita|first=P.|author2=Firan, C. |author3=Nejdl, W. |title=Summarizing local context to personalize global Web search|journal=SIGIR|year=2006|pages=287–296}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last=Dou|first=Z.|author2=Song, R. |author3=Wen, J.R. |title=A large-scale evaluation and analysis of personalized search strategies|journal=WWW|year=2007|pages=581–590}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last=Shen|first=X. |author2=Tan, B. |author3=Zhai, C.X.|title=Implicit user modeling for personalized search|journal=CIKM|year=2005|pages=824–831}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last=Sugiyama|first=K.|author2=Hatano, K. |author3=Yoshikawa, M. |title=Adaptive web search based on user profile constructed without any effort from the user|journal=WWW|year=2004|pages=675–684}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last=Teevan|first=J.|author2=Dumais, S.T. |author3=Horvitz, E. |title=Personalizing search via automated analysis of interests and activities|journal=SIGIR|year=2005|pages=415–422|url=http://people.csail.mit.edu/teevan/work/publications/papers/tochi10.pdf}}&lt;/ref&gt;

There are several publicly available systems for personalizing Web search results (e.g., [[Google Personalized Search]] and [[Bing (search engine)|Bing]]'s search result personalization&lt;ref&gt;{{cite web|last=Crook|first=Aidan, and Sanaz Ahari|title=Making search yours|url=http://www.bing.com/community/site_blogs/b/search/archive/2011/02/10/making-search-yours.aspx|publisher=Bing|accessdate=14 March 2011}}&lt;/ref&gt;). However, the technical details and evaluations of these commercial systems are proprietary. One technique Google uses to personalize searches for its users is to track log in time and if the user has enabled web history in his browser. If a user accesses the same site through a search result from Google many times, it believes that they like that page. So when users carry out certain searches, Google's personalized search algorithm gives the page a boost, moving it up through the ranks. Even if a user is signed out, Google may personalize their results because it keeps a 180-day record of what a particular web browser has searched for, linked to a cookie in that browser.&lt;ref&gt;{{cite web|last=Sullivan|first=Danny|title=Of "Magic Keywords" and Flavors Of Personalized Search At Google|url=http://searchengineland.com/flavors-of-google-personalized-search-139286|accessdate=21 April 2014}}&lt;/ref&gt;

In order to better understand how personalized search results are being presented to the users, a group of researchers at Northeastern University compared an aggregate set of searches from logged in users against a [[control group]]. The research team found that 11.7% of results show differences due to personalization; however, this varies widely by [[Web search query|search query]] and result ranking position.&lt;ref name=Briggs&gt;{{cite web|last=Briggs|first=Justin|title=A Better Understanding of Personalized Search|url=http://justinbriggs.org/better-understanding-personalized-search|date=24 June 2013|accessdate=21 April 2014}}&lt;/ref&gt; Of various factors tested, the two that had measurable impact were being logged in with a Google account and the [[IP address]] of the searching users. It should also be noted that results with high degrees of personalization include companies and politics. One of the factors driving personalization is localization of results, with company queries showing store locations relevant to the location of the user. So, for example, if a user searched for "used car sales", Google may produce results of local car dealerships in their area. On the other hand, queries with the least amount of personalization include factual queries ("what is") and health.&lt;ref name=Briggs/&gt;

When measuring personalization, it is important to eliminate background noise. In this context, one type of background noise is the carry-over effect. The carry-over effect can be defined as follows: when a user performs a search and follow it with a subsequent search, the results of the second search is influenced by the first search. A noteworthy point is that the top-ranked [[URL]]s are less likely to change based off personalization, with most personalization occurring at the lower ranks. This is a style of personalization based on recent search history, but it is not a consistent element of personalization because the phenomenon times out after 10 minutes, according to the researchers.&lt;ref name=Briggs/&gt;

==The filter bubble==
{{Main article|Filter bubble}}

Several concerns have been brought up regarding personalized search. It decreases the likelihood of finding new information by [[bias]]ing search results towards what the user has already found. It introduces potential privacy problems in which a user may not be aware that their search results are personalized for them, and wonder why the things that they are interested in have become so relevant. Such a problem has been coined as the "filter bubble" by author [[Eli Pariser]]. He argues that people are letting major websites drive their destiny and make decisions based on the vast amount of data they've collected on individuals. This can isolate users in their own worlds or "filter bubbles" where they only see information that they want to, such a consequence of "The Friendly World Syndrome". As a result, people are much less informed of problems in the developing world which can further widen the gap between the North (developed countries) and the South (developing countries).&lt;ref name=Pariser&gt;{{cite book| url=http://www.sp.uconn.edu/~jbl00001/pariser_the%20filter%20bubble_introduction.pdf | title=The Filter Bubble | archiveurl=https://web.archive.org/web/20131228150122/http://www.sp.uconn.edu/~jbl00001/pariser_the%20filter%20bubble_introduction.pdf|archivedate=December 28, 2013|author=E. Pariser|year=2011}}&lt;/ref&gt;

The methods of personalization, and how useful it is to "promote" certain results which have been showing up regularly in searches by like-minded individuals in the same community. The personalization method makes it very easy to understand how the filter bubble is created. As certain results are bumped up and viewed more by individuals, other results not favored by them are relegated to obscurity. As this happens on a community-wide level, it results in the community, consciously or not, sharing a skewed perspective of events.&lt;ref&gt;{{cite journal|last=Smyth|first=B.|title=Adaptive Information Access:: Personalization And Privacy |journal=International Journal of Pattern Recognition &amp; Artificial Intelligence |year=2007|pages=183–205}}&lt;/ref&gt;

An area of particular concern to some parts of the world is the use of personalized search as a form of control over the people utilizing the search by only giving them particular information ([[selective exposure]]). This can be used to give particular influence over highly talked about topics such as gun control or even gear people to side with a particular political regime in different countries.&lt;ref name=Pariser/&gt; While total control by a particular government just from personalized search is a stretch, control of the information readily available from searches can easily be controlled by the richest corporations. The biggest example of a corporation controlling the information is Google. Google is not only feeding you the information they want but they are at times using your personalized search to gear you towards their own companies or affiliates. This has led to a complete control of various parts of the web and a pushing out of their competitors such as how Google Maps took a major control over the online map and direction industry with MapQuest and others forced to take a backseat.&lt;ref name="Consumer Watchdog"&gt;{{cite web| title=Traffic Report: How Google is squeezing out competitors and muscling into new markets|url= http://www.consumerwatchdog.org/resources/TrafficStudy-Google.pdf|date=2 June 2010|accessdate= 27 April 2014|work=Consumer Watchdog}}&lt;/ref&gt;

Many search engines use concept-based user profiling strategies that derive only topics that users are highly interested in but for best results, according to researchers Wai-Tin and Dik Lun, both positive and negative preferences should be considered. Such profiles, applying negative and positive preferences, result in highest quality and most relevant results by separating alike queries from unalike queries. For example, typing in 'apple' could refer to either the fruit or the [[Macintosh]] computer and providing both preferences aids search engines' ability to learn which apple the user is really looking for based on the links clicked. One concept-strategy the researchers came up with to improve personalized search and yield both positive and negative preferences is the click-based method. This method captures a user's interests based on which links they click on in a results list, while downgrading unclicked links.&lt;ref&gt;{{cite journal|last=Wai-Tin|first=Kenneth|author2=Dik Lun, L|title=Deriving concept-based user profiles from search engine logs|journal=IEEE Transactions on Knowledge and Data Engineering|year=2010|volume=22|issue=7|pages=969–982|doi=10.1109/tkde.2009.144}}&lt;/ref&gt;

The feature also has profound effects on the [[search engine optimization]] industry, due to the fact that search results will no longer be ranked the same way for every user.&lt;ref&gt;[http://www.networkworld.com/news/2009/120709-google-personalized-results-could-be.html "Google Personalized Results Could Be Bad for Search"]. ''Network World''. Retrieved July 12, 2010.&lt;/ref&gt; An example of this is found in Eli Pariser's, The Filter Bubble, where he had two friends type in "BP" into Google's search bar. One friend found information on the BP oil spill in the Gulf of Mexico while the other retrieved investment information.&lt;ref name=Pariser/&gt;

Some have noted that personalized search results not only serve to customize a user's search results, but also [[Advertising|advertisements]].  This has been criticized as an [[Expectation of privacy|invasion on privacy]].&lt;ref&gt;{{cite web|url=http://www.seooptimizers.com/search-engines-and-customized-results-based-on-your-internet-history.html|title=Search Engines and Customized Results Based on Your Internet History|publisher=SEO Optimizers|accessdate=27 February 2013}}&lt;/ref&gt;

==The case of Google==
{{Main article|Google Personalized Search}}

An important example of search personalization is [[Google]]. There are a host of Google applications, all of which can be personalized and integrated with the help of a Google account. Personalizing search does not require an account. However, one is almost deprived of a choice, since so many useful Google products are only accessible if one has a Google account. The Google Dashboard, introduced in 2009, covers more than 20 products and services, including Gmail, Calendar, Docs, YouTube, etc.&lt;ref&gt;{{cite journal|last=Mattison|first=D.|title=Time, Space, And Google: Toward A Real-Time, Synchronous, Personalized, Collaborative Web. |journal=Searcher|year=2010|pages=20–31}}&lt;/ref&gt; that keeps track of all the information directly under one's name. The free Google Custom Search is available for individuals and big companies alike, providing the Search facility for individual websites and powering corporate sites such as that of the ''[[New York Times]]''. The high level of personalization that was available with Google played a significant part in helping remain the world's most favorite search engine.

One example of Google's ability to personalize searches is in its use of Google News. Google has geared its news to show everyone a few similar articles that can be deemed interesting, but as soon as the user scrolls down, it can be seen that the news articles begin to differ. Google takes into account past searches as well as the location of the user to make sure that local news gets to them first. This can lead to a much easier search and less time going through all of the news to find the information one want. The concern, however, is that the very important information can be held back because it does not match the criteria that the program sets for the particular user. This can create the "[[filter bubble]]" as described earlier.&lt;ref name=Pariser/&gt;

An interesting point about personalization that often gets overlooked is the privacy vs personalization battle. While the two do not have to be mutually exclusive, it is often the case that as one becomes more prominent, it compromises the other. Google provides a host of services to people, and many of these services do not require information to be collected about a person to be customizable. Since there is no threat of privacy invasion with these services, the balance has been tipped to favor personalization over privacy, even when it comes to search. As people reap the rewards of convenience from customizing their other Google services, they desire better search results, even if it comes at the expense of private information. Where to draw the line between the information versus search results tradeoff is new territory and Google gets to make that decision. Until people get the power to control the information that is being collected about them, Google is not truly protecting privacy.
Google's popularity as a search engine and Internet browser has allowed it to gain a lot of power. Their popularity has created millions of usernames, which have been used to collect vast amounts of information about individuals. Google can use multiple methods of personalization such as traditional, social, geographic, IP address, browser, cookies, time of day, year, behavioral, query history, bookmarks, and more. Although having Google personalize search results based on what users searched previously may have its benefits, there are negatives that come with it.&lt;ref&gt;{{cite web|last=Jackson|first=Mark|title=The Future of Google's Search Personalization|url=http://searchenginewatch.com/article/2067001/The-Future-of-Googles-Search-Personalization|accessdate=29 April 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Harry|first=David|title=Search Personalization and the User Experience|url=http://searchenginewatch.com/article/2118126/Search-Personalization-the-User-Experience|accessdate=29 April 2014}}&lt;/ref&gt;
With the power from this information, Google has chosen to enter other sectors it owned, such as videos, document sharing, shopping, maps, and many more. Google has done this by steering searchers to their own services offered as opposed to others such as MapQuest.

Using search personalization, Google has doubled its video market share to about eighty percent. The legal definition of a [[monopoly]] is when a firm gains control of seventy to eighty percent of the market. Google has reinforced this monopoly by creating significant barriers of entry such as manipulating search results to show their own services. This can be clearly seen with Google Maps being the first thing displayed in most searches.

The analytical firm Experian Hitwise stated that since 2007, MapQuest has had its traffic cut in half because of this. Other statistics from around the same time include Photobucket going from twenty percent of market share to only three percent, Myspace going from twelve percent market share to less than one percent, and ESPN from eight percent to four percent market share. In terms of images, Photobucket went from 31% in 2007 to 10% in 2010 and Yahoo Images has gone from 12% to 7%. It becomes apparent that the decline of these companies has come because of Google's increase in market share from 43% in 2007 to about 55% in 2009.

It can be said that Google is more dominant because they provide better services. However, Experian Hitwise has also created graphs to show the market share of about fifteen different companies at once. This has been done for every category for the market share of pictures, videos, product search, and more. The graph for product search is evidence enough for Google's influence because their numbers went from 1.3 million unique visitors to 11.9 unique visitors in one month. That kind of growth can only come with the change of a process.

In the end, there are two common themes with all of these graphs. The first is that Google's market share has a directly inverse relationship to the market share of the leading competitors. The second is that this directly inverse relationship began around 2007, which is around the time that Google began to use its "Universal Search" method.&lt;ref&gt;{{cite web|title=TRAFFIC REPORT:How Google is Squeezing out Competitors and Muscling into New Markets |url=https://courses.lis.illinois.edu/pluginfile.php/226148/mod_resource/content/1/TrafficStudy-Google.pdf|publisher=ConsumerWatchDog.org|accessdate=29 April 2014}}&lt;/ref&gt;

==Benefits==

One of the most critical benefits personalized search has is to improve the quality of decisions consumers make. The internet has made the transaction cost of obtaining information significantly lower than ever. However, human ability to process information has not expanded much.&lt;ref name=Diehl&gt;{{cite journal|author=Diehl, K.|year=2003|title=Personalization and Decision Support Tools: Effects on Search and Consumer Decision Making|journal=Advances In Consumer Research|volume=30|issue=1|pages=166–169}}&lt;/ref&gt; When facing overwhelming amount of information, consumers need a sophisticated tool to help them make high quality decisions. Two studies examined the effects of personalized screening and ordering tools, and the results show a [[positive correlation]] between personalized search and the quality of consumers' decisions.

The first study was conducted by Kristin Diehl from the [[University of South Carolina]]. Her research discovered that reducing search cost led to lower quality choices. The reason behind this discovery was that 'consumers make worse choices because lower search costs cause them to consider inferior options.' It also showed that if consumers have a specific goal in mind, they would further their search, resulting in an even worse decision.&lt;ref name=Diehl/&gt; The study by Gerald Haubl from the [[University of Alberta]] and Benedict G.C. Dellaert from [[Maastricht University]] mainly focused on recommendation systems. Both studies concluded that a personalized search and recommendation system significantly improved consumers' decision quality and reduced the number of products inspected.&lt;ref name=Diehl/&gt;

==Models==

Personalized search gains popularity because of the demand for more relevant information. Research has indicated low success rates among major search engines in providing relevant results; in 52% of 20,000 queries, searchers did not find any relevant results within the documents that Google returned.&lt;ref&gt;{{cite book|author1=Coyle, M.|author2=Smyth, B.|lastauthoramp=y|year=2007|chapter=Information recovery and discovery in collaborative web search|title=Advances in Information Retrieval|pp=356–367|doi=10.1007/978-3-540-71496-5_33|isbn=978-3-540-71494-1|series=Lecture Notes in Computer Science}}&lt;/ref&gt; Personalized search can improve search quality significantly and there are mainly two ways to achieve this goal.

The first model available is based on the users' historical searches and search locations. People are probably familiar with this model since they often find the results reflecting their current location and previous searches.

There is another way to personalize search results. In Bracha Shapira and Boaz Zabar's "Personalized Search: Integrating Collaboration and Social Networks", Shapira and Zabar focused on a model that utilizes a [[recommendation system]].&lt;ref&gt;{{cite journal|author1=Shapira, B.|author2=Zabar, B.|lastauthoramp=y|year=2011|title=Personalized search: Integrating collaboration and social networks|journal=Journal of the American Society for Information Science &amp; Technology|volume=62|issue=1|pages=146–160|doi=10.1002/asi.21446}}&lt;/ref&gt; This model shows results of other users who have searched for similar keywords. The authors examined keyword search, the recommendation system, and the recommendation system with social network working separately and compares the results in terms of search quality. The results show that a personalized search engine with the recommendation system produces better quality results than the standard search engine, and that the recommendation system with social network even improves more.

Recent paper “[https://arxiv.org/abs/1612.03597 Search personalization with embeddings]” shows that a new embedding model for search personalization, where users are embedded on a topical interest space, produces better search results than strong learning-to-rank models.

==Disadvantages==

While there are documented benefits of the implementation of search personalization, there are also arguments against its use. The foundation of this argument against its use is because it confines internet users' search engine results to material that aligns with the users' interests and history. It limits the users' ability to become exposed to material that would be relevant to the user's search query but due to the fact that some of this material differs from the user's interests and history, the material is not displayed to the user. Search personalization takes the objectivity out of the search engine and undermines the engine. "Objectivity matters little when you know what you are looking for, but its lack is problematic when you do not".&lt;ref&gt;{{cite journal|last=Simpson|first=Thomas W.|title=Evaluating Google As An Epistemic Tool|journal=Metaphilosophy|date=2012|volume=43.4|pages=426–445|doi=10.1111/j.1467-9973.2012.01759.x}}&lt;/ref&gt; Another criticism of search personalization is that it limits a core function of the web: the collection and sharing of information. Search personalization prevents users from easily accessing all the possible information that is available for a specific search query.  Search personalization adds a bias to user's search queries. If a user has a particular set of interests or internet history and uses the web to research a controversial issue, the user's search results will reflect that. The user may not be shown both sides of the issue and miss potentially important information if the user's interests lean to one side or another. A study done on search personalization and its effects on search results in Google News resulted in different orders of news stories being generated by different users, even though each user entered the same search query. According to Bates, "only 12% of the searchers had the same three stories in the same order. This to me is prima facie evidence that there is filtering going on".&lt;ref&gt;{{cite journal|last=Bates|first=Mary Ellen|title=Is Google Hiding My News?|year=2011|journal=Online|volume=35|issue=6|pages=64}}&lt;/ref&gt; If search personalization was not active, all the results in theory should have been the same stories in an identical order.

Another disadvantage of search personalization is that internet companies such as Google are gathering and potentially selling their users' internet interests and histories to other companies. This raises a privacy issue concerning whether people are comfortable with companies gathering and selling their internet information without their consent or knowledge.  Many web users are unaware of the use of search personalization and even fewer have knowledge that user data is a valuable commodity for internet companies.

==Sites that use it==

E. Pariser, author of ''The Filter Bubble'', explains how there are differences that search personalization has on both [[Facebook]] and Google. Facebook implements personalization when it comes to the amount of things people share and what pages they "like". An individual's [[social interaction]]s, whose profile they visit the most, who they message or chat with are all indicators that are used when Facebook uses personalization. Rather than what people share being an indicator of what is filtered out, Google takes into consideration what we "click" to filter out what comes up in our searches. In addition, Facebook searches are not necessarily as private as the Google ones. Facebook draws on the more public self and users share what other people want to see. Even while [[tag (metadata)|tag]]ging photographs, Facebook uses personalization and [[face recognition]] that will automatically assign a name to face. In terms of Google, users are provided similar websites and resources based on what they initially click on. There are even other websites that use the filter tactic to better adhere to user preferences. For example, [[Netflix]] also judges from the users search history to suggest movies that they may be interested in for the future. There are sites like [[Amazon.com|Amazon]] and personal [[shopping site]]s also use other peoples history in order to serve their interests better. [[Twitter]] also uses personalization by "suggesting" other people to follow. In addition, based on who one "follows", "tweets" and "retweets" at, Twitter filters out suggestions most relevant to the user.  [[Mark Zuckerberg]], founder of Facebook, believed that people only have one identity. E. Pariser argues that is completely false and search personalization is just another way to prove that isn't true. Although personalized search may seem helpful, it is not a very accurate representation of any person. There are instances where people also search things and share things in order to make themselves look better. For example, someone may look up and share political articles and other intellectual articles. There are many sites being used for different purposes and that do not make up one person's [[Personal identity|identity]] at all, but provide false representations instead.&lt;ref name=Pariser/&gt;

==Online shopping==
{{main article|Online shopping}}
Search engines such as Google and Yahoo! utilize personalized search to attract possible customers to products that fit their presumed desires. Based on a large amount of collected data aggregated from an individual's web clicks, search engines can use personalized search to put advertisements that may pique the interest of an individual. Utilizing personalized search can help consumers find what they want faster, as well as help match up products and services to individuals within more specialized and/or niche markets. Many of these products or services that are sold via personalized online results would struggle to sell in [[brick-and-mortar]] stores. These types of products and services are called long tail items.&lt;ref&gt;{{cite journal|author=Badke, William|title=Personalization and Information Literacy|journal=Online|volume=36|issue=1|page=47|date=February 2012}}&lt;/ref&gt; Using personalized search allows faster product and service discoveries for consumers, and reduces the amount of necessary advertisement money spent to reach those consumers. In addition, utilizing personalized search can help companies determine which individuals should be offered online coupon codes to their products and/or services. By tracking if an individual has perused their website, considered purchasing an item, or has previously made a purchase a company can post advertisements on other websites to reach that particular consumer in an attempt to have them make a purchase.

Aside from aiding consumers and businesses in finding one another, the search engines that provide personalized search benefit greatly. The more data collected on an individual, the more personalized results will be. In turn, this allows search engines to sell more advertisements because companies understand that they will have a better opportunity to sell to high percentage matched individuals then medium and low percentage matched individuals. This aspect of personalized search angers many scholars, such as William Badke and Eli Pariser, because they believe personalized search is driven by the desire to increase advertisement revenues. In addition, they believe that personalized search results are frequently utilized to sway individuals into using products and services that are offered by the particular search engine company or any other company in partnered with them. For example, Google searching any company with at least one brick-and-mortar location will offer a map portraying the closest company location using the Google Maps service as the first result to the query.&lt;ref"Consumer Watchdog"/&gt; In order to use other mapping services, such as MapQuest, a user would have to dig deeper into the results. Another example pertains to more vague queries. Searching the word "shoes" using the Google search engine will offer several advertisements to shoe companies that pay Google to link their website as a first result to consumer's queries.

==References==
{{reflist|30em}}

{{DEFAULTSORT:Personalized search}}
[[Category:Information retrieval techniques]]
[[Category:Internet search engines|*Personalized search]]
[[Category:Internet terminology]]
[[Category:Personalized search| ]]</text>
      <sha1>fl52676pccm6tci3b5zmtkds6s1gnrc</sha1>
    </revision>
  </page>
  <page>
    <title>Enterprise search</title>
    <ns>0</ns>
    <id>12101316</id>
    <revision>
      <id>751208693</id>
      <parentid>751208272</parentid>
      <timestamp>2016-11-24T02:04:16Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <comment>Filled in 3 bare reference(s) with [[:en:WP:REFILL|reFill]] ()</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9817" xml:space="preserve">{{original research|date=November 2015}}
'''Enterprise search''' is the practice of making content from multiple enterprise-type sources, such as [[database]]s and [[intranet]]s, searchable to a defined audience.

"Enterprise search" is used to describe the software of search information within an enterprise (though the search function and its results may still be public).&lt;ref&gt;{{cite web|url=http://www.aiim.org/What-is-Enterprise-Search|title=What is Enterprise Search?|publisher=}}&lt;/ref&gt; Enterprise search can be contrasted with [[web search]], which applies search technology to documents on the open web, and [[desktop search]], which applies search technology to the content on a single computer.

Enterprise search systems index data and documents from a variety of sources such as: [[file systems]], [[intranets]], [[document management system]]s, [[e-mail]], and [[databases]]. Many enterprise search systems integrate structured and unstructured data in their collections.&lt;ref&gt;[http://www.arma.org/bookstore/files/Delgado.pdf The New Face of Enterprise Search: Bridging Structured and Unstructured Information]&lt;/ref&gt; Enterprise search systems also use access controls to enforce a security policy on their users.&lt;ref&gt;{{cite web|url=http://www.ideaeng.com/tabId/98/itemId/118/Mapping-Security-Requirements-to-Enterprise-Search.aspx|title=Security Requirements to Enterprise Search: part 1 - New Idea Engineering|publisher=}}&lt;/ref&gt;

Enterprise search can be seen as a type of [[vertical search]] of an enterprise.

==Components of an enterprise search system==
In an enterprise search system, content goes through various phases from source repository to search results:

=== Content awareness ===
Content awareness (or "content collection") is usually either a push or pull model. In the push model, a source system is integrated with the search engine in such a way that it connects to it and pushes new content directly to its [[API]]s. This model is used when realtime indexing is important. In the pull model, the software gathers content from sources using a connector such as a [[web crawler]] or a [[database]] connector. The connector typically polls the source with certain intervals to look for new, updated or deleted content.&lt;ref&gt;{{cite web|url=http://www.information-management.com/issues/20_7/content_management_data_integration_indexing_metadata-10019105-1.html|title=Understanding Content Collection and Indexing|publisher=}}&lt;/ref&gt;

=== Content processing and analysis ===
Content from different sources may have many different formats or document types, such as XML, HTML, Office document formats or plain text. The content processing phase processes the incoming documents to plain text using document filters. It is also often necessary to normalize content in various ways to improve [[Recall (information retrieval)|recall]] or [[Precision (information retrieval)|precision]]. These may include [[stemming]], [[lemmatization]], [[synonym]] expansion, [[entity extraction]], [[part of speech]] tagging.

As part of processing and analysis, [[tokenization (lexical analysis)|tokenization]] is applied to split the content into [[Lexical analysis#Token|tokens]] which is the basic matching unit. It is also common to normalize tokens to lower case to provide case-insensitive search, as well as to normalize accents to provide better recall.

=== Indexing ===
The resulting text is stored in an [[Index (search engine)|index]], which is optimized for quick lookups without storing the full text of the document. The index may contain the dictionary of all unique words in the corpus as well as information about ranking and [[term frequency]].

=== Query processing ===
Using a web page, the user issues a [[Web search query|query]] to the system. The query consists of any terms the user enters as well as navigational actions such as [[faceted search|faceting]] and paging information.

=== Matching ===
The processed query is then compared to the stored index, and the search system returns results (or "hits") referencing source documents that match. Some systems are able to present the document as it was indexed.

==Differences from web search==
{{unreferenced section|date=November 2015}}
Beyond the difference in the kinds of materials being indexed, enterprise search systems also typically include functionality that is not associated with the mainstream [[web search engine]]s. These include:
*Adapters to index content from a variety of repositories, such as [[databases]] and [[content management systems]].
*[[Federated search]], which consists of
# transforming a query and broadcasting it to a group of disparate databases or external content sources with the appropriate syntax,
# merging the results collected from the databases,
# presenting them in a succinct and unified format with minimal duplication, and
# providing a means, performed either automatically or by the portal user, to sort the merged result set.
*[[Enterprise bookmarking]], collaborative [[tag (metadata)|tagging]] systems for capturing knowledge about structured and semi-structured enterprise data.
*[[Entity extraction]] that seeks to locate and classify elements in text into predefined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.
*[[Faceted search]], a technique for accessing a collection of information represented using a [[faceted classification]], allowing users to explore by filtering available information.
*Access control, usually in the form of an [[Access control list]] (ACL), is often required to restrict access to documents based on individual user identities. There are many types of access control mechanisms for different content sources making this a complex task to address comprehensively in an enterprise search environment (see below).
*[[Text clustering]], which groups the top several hundred search results into topics that are computed on the fly from the search-results descriptions, typically titles, excerpts (snippets), and meta-data.  This technique lets users navigate the content by topic rather than by the meta-data that is used in faceting. Clustering compensates for the problem of incompatible meta-data across multiple enterprise repositories, which hinders the usefulness of faceting.
*[[User interfaces]], which in web search are deliberately kept simple in order not to distract the user from clicking on ads, which generates the revenue.  Although the business model for enterprise search could include showing ads, in practice this is not done.  To enhance end user productivity, enterprise vendors continually experiment with rich UI functionality which occupies significant screen space, which would be problematic for web search.

==Relevance factors for enterprise search==
{{unreferenced section|date=November 2015}}
The factors that determine the relevance of search results within the context of an enterprise overlap with but are different from those that apply to web search. In general, enterprise search engines cannot take advantage of the rich [[hyperlink|link structure]] as is found on the web's [[hypertext]] content, however, a new breed of Enterprise search engines based on a bottom-up [[Web 2.0]] technology are providing both a contributory approach and [[hyperlink]]ing within the enterprise. Algorithms like [[PageRank]] exploit hyperlink structure to assign authority to documents, and then use that authority as a query-independent relevance factor. In contrast, enterprises typically have to use other query-independent factors, such as a document's recency or popularity, along with query-dependent factors traditionally associated with [[information retrieval]] algorithms.  Also, the rich functionality of enterprise search UIs, such as clustering and faceting, diminish reliance on ranking as the means to direct the user's attention.

==Access control: early binding vs late binding==
Security and restricted access to documents is an important matter in enterprise search. There are two main approaches to apply restricted access: early binding vs late binding.&lt;ref&gt;[http://enterprisesearch.co/enterprise-search-document-access-control/ Enterprise Search: document access control]&lt;/ref&gt;

===Late binding===
Permissions are analyzed and assigned to documents at query stage. Query engine generates a document set and before returning it to a user this set is filtered based on user access rights. It is costly process but accurate (based on user permissions at the moment of query).

===Early binding===
Permissions are analyzed and assigned to documents at indexing stage. It is much more effective than late binding, but could be inaccurate (user might be granted or revoked permissions between in the period between indexing and querying).

==Search relevance testing options==
Search application relevance can be determined by following relevance testing options like&lt;ref&gt;[http://searchhub.org/2009/09/02/debugging-search-application-relevance-issues/  Debugging Search Application Relevance Issues]&lt;/ref&gt;
*Focus groups
*Reference evaluation protocol (based on relevance judgements of results from agreed-upon queries performed against common document corpuses)
*Empirical testing
*[[A/B testing]]
*Log analysis on a Beta production site
*Online ratings

==See also==
*[[Collaborative search engine]]
*[[Comparison of enterprise search software]]
*[[Data defined storage]] 
*[[Enterprise bookmarking]]
*[[Enterprise information access]]
*[[Faceted search]]
*[[Information extraction]]
*[[Knowledge management]]
*[[List of enterprise search vendors]]
*[[List of search engines]]
*[[Text mining]]
*[[Vertical search]]

==References==
{{Reflist}}

{{DEFAULTSORT:Enterprise Search}}
[[Category:Information retrieval genres]]</text>
      <sha1>68jitf88jrca0kg84h7uh75yeg0h19s</sha1>
    </revision>
  </page>
  <page>
    <title>Multimodal search</title>
    <ns>0</ns>
    <id>34005384</id>
    <revision>
      <id>755822143</id>
      <parentid>715964389</parentid>
      <timestamp>2016-12-20T11:32:09Z</timestamp>
      <contributor>
        <username>Feminist</username>
        <id>25530780</id>
      </contributor>
      <minor />
      <comment>[[Talk:Bing (search engine)#Requested move 13 December 2016]], replaced: [[Bing]] → [[Bing (search engine)|Bing]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6407" xml:space="preserve">'''Multimodal search''' is a type of [[search engines|search]] that uses different methods to get relevant results. They can use any kind of search, [[keyword search|search by keyword]], [[concept search|search by concept]], [[Query by Example|search by example]],etc.

== Introduction ==

A multimodal search engine is designed to imitate the flexibility and agility of how the [[mind|human mind]] works to create, process and refuse irrelevant ideas. So, the more elements you have in the input of the search engine to can compare, the more [[Arithmetic precision|accurate]] the results can be.
Multimodal search engines use different inputs of different nature and methods of search at the same time with the possibility of combining the results by merging all of the input elements of the search. There are also engines that can use a feedback of the results with the evaluation of the user to perform a more appropriate and relevant search.

[[File:Schema of a simple search.jpg|thumb|Schema of a Simple Search]]

Nowadays, mobile devices have been developed to a point that they can perform infinite functions from any place at any time, thanks to the [[internet]] and [[GPS]] connections. Touch screens, motion sensors and voice recognition are now featured on mobile devices called [[smartphone]]s. All the features and functions make possible to can execute multimodal searches from any place of the world at any time.

=== Search elements ===

The use of text is an option, as well as [[multimedia search]]ing, [[image]]s, [[video]]s, [[Content (media)|audio]], [[voice]], [[document]]s. Even the location of the user can help the search engine to perform a more effective search, adaptable to every situation.
Nowadays, different ways to [[Human–computer interaction|interact]] with a search engine are being discovered, in terms of input elements of the search and in the variety of results obtained.

=== Personal context ===

Many queries from mobiles are [[location-based service|location-based]] (LBS), that use the location of the user to interact with the applications. If available, the browser uses the device GPS, or computes an approximate location based on cell tower triangulation, with the permission of the user, who must be agree to share his/her location with the application in the download.
Therefore, multimodal searches use not only audiovisual content that the user provides directly, but also the context where the user is, like his/her location, language, time at the moment, web site or document where the user is surfing, or other elements that can help to improve of a search in every situation.
[[File:Contextual query.jpg|Example of Contextual Query]]

== Classification of the results ==

The multimodal search engine works in parallel, whilst at the same time, performs a search of more to less relevance of every element introduced directly or indirectly (personal context). Afterwards, it provides a combination of all the results, merging every element with its associated weight for every descriptor.

The engine analyzes every element and tags them, so a comparison of the tags can be made with existent indexed information in databases. A classification of the results proceeds, to show them from more to less relevance.

[[File:Framework of Multimodal Search.jpg|thumb|Framework of a Multimodal Search]]

It’s necessary to define the importance of every input element. There are search engines that do this automatically, however there are also engines where the user can do it manually, giving more or less weight to every element of the search.
It’s also important that the user provides the appropriate and essential information for the search; too much information can confuse the system and provide unsatisfactory results.
With multimodal searches users can get better results than with a simple search, but multimodal searches must process more input information. It can also spend more time to process it and require more memory space.

An efficient search engine interprets the query of the users, realizes his/her intention and applies a strategy to use an appropriate search, i.e. the engine adapts to every input query and also to the combination of the elements and methods.

== Applications ==

Nowadays, existing multimodal search engines are not very complex, and some of them are in an experimental phase. Some of the more simple engines are [[Google Images]] [http://images.google.es/] or [[Bing (search engine)|Bing]] [http://www.bing.com], web interfaces that use text and images as inputs to find images in the output.

MMRetrieval [http://www.aviarampatzis.com/publications/p117-zagoris.pdf] is a multimodal experimental search engine that uses multilingual and multimedia information through a web interface. The engine searches the different inputs in parallel and merges all the results by different chosen methods. The engine also provides different multistage retrieval, as well as a single text index baseline to be able to compare all the different phases of search.

There are a lot of applications for mobile devices, using the context of the user, like based-location services, and using also text, images, audios or videos that the user provides at the moment or with saved files, or even interacting with the voice.

== References ==

* Query-Adaptive Fusion for Multimodal Search,Lyndon Kennedy, Student Member IEEE, Shih-Fu Chang, Fellow IEEE, and Apostol Natsev [http://www.ee.columbia.edu/~lyndon/pubs/pieee2008-queryadaptive.pdf]
* Context-aware Querying for Multimodal Search Engines, Jonas Etzold, Arnaud Brousseau, Paul Grimm and Thomas Steiner [http://www.lsi.upc.edu/~tsteiner/papers/2012/context-aware-querying-mmm2012.pdf]
* Apply Multimodal Search and Relevance Feedback In a Digital Video Library, Thesis of Yu Zhong [http://www.informedia.cs.cmu.edu/documents/zhong_thesis_may00.pdf]
* Aplicació rica d’internet per a la consulta amb text i imatge al repositori de vídeos de la Corporació Catalana de Mitjans Audiovisuals, Ramon Salla, Universitat Politècnica de Catalunya [http://upcommons.upc.edu/pfc/bitstream/2099.1/8766/1/PFC.pdf]

== External links ==
* MMRetrieval [http://www.mmretrieval.net]
* Google Images [http://images.google.es/]
* Bing [http://www.bing.com]

&lt;!--- Categories ---&gt;
[[Category:Information retrieval genres]]
[[Category:Internet search engines]]
[[Category:Multimedia]]</text>
      <sha1>8zsv59pmvhjv87436fqffx3cj0thyxx</sha1>
    </revision>
  </page>
  <page>
    <title>Visual search engine</title>
    <ns>0</ns>
    <id>17813833</id>
    <revision>
      <id>754255852</id>
      <parentid>747891104</parentid>
      <timestamp>2016-12-11T18:09:51Z</timestamp>
      <contributor>
        <ip>18.111.117.62</ip>
      </contributor>
      <comment>/* Applications */ Someone is defacing wikipedia by inserting blatant advertising</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9028" xml:space="preserve">{{multiple issues|
{{original research|date=February 2012}}
{{refimprove|date=February 2012}}
}}

A '''Visual Search Engine''' is a [[search engine (computing)|search engine]] designed to search for information on the [[World Wide Web]] through the input of an image or a search engine with a visual display of the search results. Information may consist of [[web page]]s, locations, other images and other types of documents. This type of search engines is mostly used to search on the mobile Internet through an image of an unknown object (unknown search query). Examples are buildings in a foreign city. These search engines often use techniques for [[CBIR|Content Based Image Retrieval]].

A visual search engine searches images, patterns based on an algorithm which it could recognize and gives relative information based on the selective or apply pattern match technique.

== Classification ==
Depending on the nature of the search engine there are two main groups, those which aim to find visual information  and those with a visual display of results.

== Visual information searchers ==
[[File:Imatge cercadors 1.jpg|thumb|Screenshot of results shown by the image searcher through example GOS]]

=== Image search ===
An image search is a search engine that is designed to find an image. The search can be based on keywords, a picture, or a web [[Hyperlink|link]] to a picture. The results depend on the search criterion, such as [[metadata]], distribution of color, shape, etc., and the search technique which the browser uses.
[[File:Imatge wiki 2.png|thumb|Diagram of a search realized through example based on detectable regions from an image]]

==== Image search techniques ====
Two techniques currently used in image search:

'''Search by metadata:''' Image search is based on comparison of metadata associated with the image as keywords, text, etc.. and it is obtained a set of images sorted by relevance. The metadata associated with each image can reference the title of the image, format, color, etc.. and can be generated manually or automatically. This metadata generation process is called audiovisual indexing.

'''Search by example:''' In this technique, also called [[content-based image retrieval]], the search results are obtained through the comparison between images using computer vision techniques. During the search it is examined the content of the image such as color, shape, texture or any visual information that can be extracted from the image. This system requires a higher [[Computational complexity theory|computational complexity]], but is more efficient and reliable than search by metadata.

There are image searchers that combine both search techniques, as the first search is done by entering a text, and then, from the images obtained can refine the search using as search parameters the images which appear as a result.

=== Video search ===
A video search is a [[search engine (computing)|search engine]] designed to search video on the net. Some video searchers process the search directly in the Internet, while others shelter the videos from which the search is done. Some searchers also enable to use as search parameters the [[File format|format]] or the length of the video. Usually the results come with a miniature capture of the video.

==== Video search techniques ====
Currently, almost all video searchers are based on keywords (search by metadata) to perform searches. These keywords can be found in the title of the video, text accompanying the video or can be defined by the author. An example of this type of search is [[YouTube]].

Some searchers generate keywords manually, while others use [[algorithms]] to analyze the audiovisual content of the video and to generate labels. The combination of these two processes improves the reliability of the search.

=== 3D Models searcher ===
A searcher of 3D models aims to find the file of a 3D modeling object from a [[database]] or network. At first glance the implementation of this type of searchers may seem unnecessary, but due to the continuous documentary inflation of the Internet, every day it becomes more necessary indexing information.

==== 3D Models search techniques ====
These have been used with traditional text-based searchers (keywords / tags), where the authors of the indexed material, or Internet users, have contributed these tags or keywords.  Because it is not always effective, it has recently been investigated in the implementation of search engines that combine the search using text with the search compared to 2D drawings, 3D drawings and 3D models.

[[Princeton University]] has developed a search engine that combines all these parameters to perform the search, thus increasing the efficiency of search.&lt;ref name= funk&gt;{{cite journal | last= Funkhouser | first= Thomas  | first2 = Patrick | last2 = Min | first3 = Michael | last3 = Kazhdan | first4 = Joyce | last4 = Chen | first5 = Alex | last5 = Halderman | first6 = David | last6 = Dobkin | first7 = David | last7 = Jacobs  | year= 2002 | title=  A Search Engine for 3D Models | journal= ACM Transactions on Graphics |url=https://www.cs.princeton.edu/~funk/tog03.pdf | volume= 22 | issue =1  | pages= 83-105  |doi = 10.1145/588272.588279 }}&lt;/ref&gt;

Imaginestics LLC created the world's first online shape search engine in the fall of 2005.&lt;ref&gt;{{cite web|url=http://www.purdue.edu/uns/html3month/2006/060824.Imaginestics.grant.html|title=Purdue Research Park's Imaginestics wins grant for research on search engines|work=purdue.edu}}&lt;/ref&gt; They currently use VizSeek search engine technology in the industrial and manufacturing settings to help discover parts using shape as the matching criteria.

=== Mobile visual search ===
A mobile image searcher is a type of [[search engine]] designed exclusively for mobile phones, through which you can find any information on [[Internet]], through an image made with the own [[mobile phone]] or using certain words ([[Keyword (computer programming)|keywords]]).

==== Introduction ====
Mobile phones have evolved into powerful image and video processing devices equipped with high-resolution cameras, color displays, and hardware-accelerated graphics. They are also increasingly equipped with a global positioning system and connected to broadband wireless networks. All this enables a new class of applications that use the camera phone to initiate search queries about objects in visual proximity to the user (Figure 1). Such applications can be used, e.g., for identifying products, comparison shopping, finding information about movies, compact disks (CDs), real estate, print media, or artworks.

==== Process ====
Typically, this type of search engine uses techniques of [[query by example]] or [[Content-based image retrieval|Image query by example]], which use the content, shape, texture and color of the image to compare them in a [[database]] and then deliver the approximate results from the query.

The process used in these searches in the [[mobile phone]]s is as follows:

First, the image is sent to the server application. Already on the server, the image will be analyzed by different analytical teams, as each one is specialized in different fields that make up an image. Then, each team will decide if the submitted image contains the fields of their speciality or not.

Once this whole procedure is done, a central computer will analyze the data and create a page of the results sorted with the efficiency of each team, to eventually be sent to the [[mobile phone]].

==== Applications ====
[[Google Goggles]] is the most popular application of image search engines{{citation required|date=November 2016}}, developed by [[Google labs|Google Labs]]. Available for [[Android (operating system)|Android]] only today. [[CamFind]] is a similar application available for both [[Android (operating system)|Android]] and [[iOS]].

JustVisual.com (formerly known as 'Superfish') and its LikeThat showcase apps are API for developers to create their own visual-search mobile app.

Other companies in the image recognition space are the [[reverse image search]]-engines [[TinEye]] and [[Google]]'s [[Google Images#Search by image|"search by image" feature of Google Images]].

== Visual display searchers ==
Another type of visual search is a search engine that shows results with a visual display image. This is an alternative to the traditional results of a sequence of links. Through some kind of image display, such as graphs, diagrams, previews of the websites, etc., it presents the results visually so that it is easier to find the desired material.
Such search engines like [http://www.kiddle.co/ Kiddle] and Manzia&lt;ref&gt;{{cite web|title=Manzia Search|website=http://www.manzia.com|accessdate=8 August 2014}}&lt;/ref&gt; present a new concept in the presentation of results, but the search techniques used are the same as in other search engines.

==References==
{{reflist}}

[[Category:Information retrieval genres]]
[[Category:Internet search engines]]
[[Category:Multimedia]]</text>
      <sha1>n6gbyyuv0mm2lbrapnpuhiy2rrkz4dg</sha1>
    </revision>
  </page>
  <page>
    <title>Concept search</title>
    <ns>0</ns>
    <id>17785794</id>
    <revision>
      <id>751208074</id>
      <parentid>751208015</parentid>
      <timestamp>2016-11-24T01:58:28Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <comment>/* See also */ linked already</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="24726" xml:space="preserve">A '''concept search''' (or '''conceptual search''') is an automated [[information retrieval]] method that is used to search electronically stored [[unstructured data|unstructured text]] (for example, [[digital archive]]s, email, scientific literature, etc.) for information that is conceptually similar to the information provided in a search query.  In other words, the ''ideas'' expressed in the information retrieved in response to a [[concept]] search query are relevant to the ideas contained in the text of the query.

__TOC__

==Development==
Concept search techniques were developed because of limitations imposed by classical Boolean [[Search algorithm|keyword search]] technologies when dealing with large, unstructured digital collections of text.  Keyword searches often return results that include many non-relevant items ([[false positive]]s) or that exclude too many relevant items (false negatives) because of the effects of [[synonymy]] and [[polysemy]].  Synonymy means that one of two or more words in the same language have the same meaning, and polysemy means that many individual words have more than one meaning.

Polysemy is a major obstacle for all computer systems that attempt to deal with human language.  In English, most frequently used terms have several common meanings.  For example, the word fire can mean: a combustion activity; to terminate employment; to launch, or to excite (as in fire up).  For the 200 most-polysemous terms in English, the typical verb has more than twelve common meanings, or senses.  The typical noun from this set has more than eight common senses.  For the 2000 most-polysemous terms in English, the typical verb has more than eight common senses and the typical noun has more than five.&lt;ref&gt;Bradford, R. B., Word Sense Disambiguation, [[Content Analyst Company]], LLC, U.S. Patent 7415462, 2008.&lt;/ref&gt;

In addition to the problems of polysemous and synonymy, keyword searches can exclude inadvertently [[misspelled]] words as well as the variations on the [[Stemming|stems]] (or roots) of words (for example, strike vs. striking).  Keyword searches are also susceptible to errors introduced by [[optical character recognition]] (OCR) scanning processes, which can introduce [[random error]]s into the text of documents (often referred to as [[noisy text]]) during the scanning process.

A concept search can overcome these challenges by employing [[word sense disambiguation]] (WSD),&lt;ref&gt;R. Navigli, [http://www.dsi.uniroma1.it/~navigli/pubs/ACM_Survey_2009_Navigli.pdf Word Sense Disambiguation: A Survey], ACM Computing Surveys, 41(2), 2009.&lt;/ref&gt; and other techniques, to help it derive the actual meanings of the words, and their underlying concepts, rather than by simply matching character strings like keyword search technologies.

==Approaches==
In general, [[information retrieval]] research and technology can be divided into two broad categories: semantic and statistical. Information retrieval systems that fall into the semantic category will attempt to implement some degree of syntactic and [[Semantic analysis (machine learning)|semantic analysis]] of the [[natural language]] text that a human user would provide (also see [[computational linguistics]]).  Systems that fall into the statistical category will find results based on statistical measures of how closely they match the query.  However, systems in the semantic category also often rely on statistical methods to help them find and retrieve information.&lt;ref&gt;Greengrass, E., Information Retrieval: A Survey, 2000.&lt;/ref&gt;

Efforts to provide information retrieval systems with semantic processing capabilities have basically used three different approaches:

* Auxiliary structures
* Local [[co-occurrence]] statistics
* Transform techniques (particularly [[matrix decomposition]]s)

===Auxiliary structures===
A variety of techniques based on [[artificial intelligence]] (AI) and [[natural language processing]] (NLP) have been applied to semantic processing, and most of them have relied on the use of auxiliary structures such as [[controlled vocabularies]] and [[Ontology (information science)|ontologies]].  Controlled vocabularies (dictionaries and thesauri), and ontologies allow broader terms, narrower terms, and related terms to be incorporated into queries.&lt;ref&gt;Dubois, C., The Use of Thesauri in Online Retrieval, Journal of Information Science, 8(2), 1984 March, pp. 63-66.&lt;/ref&gt; Controlled vocabularies are one way to overcome some of the most severe constraints of Boolean keyword queries.  Over the years, additional auxiliary structures of general interest, such as the large synonym sets of [[WordNet]], have been constructed.&lt;ref&gt;Miller, G., Special Issue, [http://www.mit.edu/~6.863/spring2009/readings/5papers.pdf WordNet: An On-line Lexical Database], Intl. Journal of Lexicography, 3(4), 1990.&lt;/ref&gt;  It was shown that concept search that is based on auxiliary structures, such as WordNet, can be efficiently implemented by reusing retrieval models and data structures of classical information retrieval.&lt;ref&gt;Fausto Giunchiglia, Uladzimir Kharkevich, and Ilya Zaihrayeu. [http://www.ulakha.com/concept-search-eswc2009.html Concept Search], In Proceedings of European Semantic Web Conference, 2009.&lt;/ref&gt;  Later approaches have implemented grammars to expand the range of semantic constructs.  The creation of data models that represent sets of concepts within a specific domain (''domain ontologies''), and which can incorporate the relationships among terms, has also been implemented in recent years.

Handcrafted controlled vocabularies contribute to the efficiency and comprehensiveness of information retrieval and related text analysis operations, but they work best when topics are narrowly defined and the terminology is standardized.  Controlled vocabularies require extensive human input and oversight to keep up with the rapid evolution of language.  They also are not well suited to the growing volumes of unstructured text covering an unlimited number of topics and containing thousands of unique terms because new terms and topics need to be constantly introduced.  Controlled vocabularies are also prone to capturing a particular world view at a specific point in time, which makes them difficult to modify if concepts in a certain topic area change.&lt;ref name="Bradford, R. B. 2008"&gt;Bradford, R. B., Why LSI? [[Latent Semantic Indexing]] and Information Retrieval, White Paper, [[Content Analyst Company]], LLC, 2008.&lt;/ref&gt;

===Local co-occurrence statistics===
Information retrieval systems incorporating this approach count the number of times that groups of terms appear together (co-occur) within a [[sliding window]] of terms or sentences (for example, ± 5 sentences or ± 50 words) within a document.  It is based on the idea that words that occur together in similar contexts have similar meanings.  It is local in the sense that the sliding window of terms and sentences used to determine the co-occurrence of terms is relatively small.

This approach is simple, but it captures only a small portion of the semantic information contained in a collection of text.  At the most basic level, numerous experiments have shown that approximately only ¼ of the information contained in text is local in nature.&lt;ref&gt;Landauer, T., and Dumais, S., A Solution to Plato's Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge, Psychological Review, 1997, 104(2), pp. 211-240.&lt;/ref&gt;   In addition, to be most effective, this method requires prior knowledge about the content of the text, which can be difficult with large, unstructured document collections.&lt;ref name="Bradford, R. B. 2008"/&gt;

===Transform techniques===
Some of the most powerful approaches to semantic processing are based on the use of mathematical transform techniques.  [[Matrix decomposition]] techniques have been the most successful.  Some widely used matrix decomposition techniques include the following:&lt;ref&gt;Skillicorn, D., Understanding Complex Datasets: Data Mining with Matrix Decompositions, CRC Publishing, 2007.&lt;/ref&gt;

* [[Independent component analysis]]
* Semi-discrete decomposition
* [[Non-negative matrix factorization]]
* [[Singular value decomposition]]

Matrix decomposition techniques are data-driven, which avoids many of the drawbacks associated with auxiliary structures.  They are also global in nature, which means they are capable of much more robust information extraction and representation of semantic information than techniques based on local co-occurrence statistics.&lt;ref name="Bradford, R. B. 2008"/&gt;

Independent component analysis is a technique that creates sparse representations in an automated fashion,&lt;ref&gt;Honkela, T., Hyvarinen, A. and Vayrynen, J. WordICA - Emergence of linguistic representations for words by independent component analysis. Natural Language Engineering, 16(3):277-308, 2010&lt;/ref&gt; and the semi-discrete and non-negative matrix approaches sacrifice accuracy of representation in order to reduce computational complexity.&lt;ref name="Bradford, R. B. 2008"/&gt;

Singular value decomposition (SVD) was first applied to text at Bell Labs in the late 1980s. It was used as the foundation for a technique called [[latent semantic indexing]] (LSI) because of its ability to find the semantic meaning that is latent in a collection of text.  At first, the SVD was slow to be adopted because of the resource requirements needed to work with large datasets.  However, the use of LSI has significantly expanded in recent years as earlier challenges in scalability and performance have been overcome.  LSI is being used in a variety of information retrieval and text processing applications, although its primary application has been for concept searching and automated document categorization.&lt;ref&gt;Dumais, S., Latent Semantic Analysis, ARIST Review of Information Science and Technology, vol. 38, Chapter 4, 2004.&lt;/ref&gt;

==Uses==
* '''[[eDiscovery]]''' – Concept-based search technologies are increasingly being used for Electronic Document Discovery (EDD or eDiscovery) to help enterprises prepare for litigation.  In eDiscovery, the ability to cluster, categorize, and search large collections of unstructured text on a conceptual basis is much more efficient than traditional linear review techniques.  Concept-based searching is becoming accepted as a reliable and efficient search method that is more likely to produce relevant results than keyword or Boolean searches.&lt;ref&gt;Magistrate Judge John M. Facciola of the U.S. District Court for the District of Washington, D.C.
Disability Rights Council v. Washington Metropolitan Transit Authority, 242 FRD 139 (D. D.C. 2007), citing George L. Paul &amp; Jason R. Baron, "Information Inflation: Can the Legal System Adapt?" 13 Rich. J.L. &amp; Tech. 10 (2007).&lt;/ref&gt;

* '''[[Enterprise Search]] and Enterprise Content Management (ECM)''' – Concept search technologies are being widely used in enterprise search.  As the volume of information within the enterprise grows, the ability to cluster, categorize, and search large collections of unstructured text on a conceptual basis has become essential.  In 2004 the Gartner Group estimated that professionals spend 30 percent of their time searching, retrieving, and managing information.&lt;ref name="Laplanche, R. 2004"&gt;Laplanche, R., Delgado, J., Turck, M., Concept Search Technology Goes Beyond Keywords, Information Outlook, July 2004.&lt;/ref&gt;  The research company IDC found that a 2,000-employee corporation can save up to $30 million per year by reducing the time employees spend trying to find information and duplicating existing documents.&lt;ref name="Laplanche, R. 2004"/&gt;
* '''[[Content-based image retrieval|Content-Based Image Retrieval (CBIR)]]''' – Content-based approaches are being used for the semantic retrieval of digitized images and video from large visual corpora.  One of the earliest content-based image retrieval systems to address the semantic problem was the ImageScape search engine.  In this system, the user could make direct queries for multiple visual objects such as sky, trees, water, etc. using spatially positioned icons in a WWW index containing more than ten million images and videos using keyframes.  The system used information theory to determine the best features for minimizing uncertainty in the classification.&lt;ref name="Lew, M. S. 2006"&gt;Lew, M. S., Sebe, N., Djeraba, C., Jain, R., Content-based Multimedia Information Retrieval: State of the Art and Challenges, ACM Transactions on Multimedia Computing, Communications, and Applications, February 2006.&lt;/ref&gt;  The semantic gap is often mentioned in regard to CBIR.  The semantic gap refers to the gap between the information that can be extracted from visual data and the interpretation that the same data have for a user in a given situation.&lt;ref&gt;Datta R., Joshi, D., Li J., Wang, J. Z., [http://infolab.stanford.edu/~wangz/project/imsearch/review/JOUR/datta.pdf Image Retrieval: Ideas, Influences, and Trends of the New Age], ACM Computing Surveys, Vol. 40, No. 2, April 2008.&lt;/ref&gt;  The [http://www.liacs.nl/~mir ACM SIGMM Workshop on Multimedia Information Retrieval] is dedicated to studies of CBIR.
* '''Multimedia and Publishing''' – Concept search is used by the multimedia and publishing industries to provide users with access to news, technical information, and subject matter expertise coming from a variety of unstructured sources.  Content-based methods for multimedia information retrieval (MIR) have become especially important when text annotations are missing or incomplete.&lt;ref name="Lew, M. S. 2006"/&gt;
* '''Digital Libraries and Archives''' – Images, videos, music, and text items in digital libraries and digital archives are being made accessible to large groups of users (especially on the Web) through the use of concept search techniques.  For example, the Executive Daily Brief (EDB), a business information monitoring and alerting product developed by EBSCO Publishing, uses concept search technology to provide corporate end users with access to a digital library containing a wide array of business content.  In a similar manner, the [[Music Genome Project]] spawned Pandora, which employs concept searching to spontaneously create individual music libraries or ''virtual'' radio stations.
* '''Genomic Information Retrieval (GIR)''' – Genomic Information Retrieval (GIR) uses concept search techniques applied to genomic literature databases to overcome the ambiguities of scientific literature.
* '''Human Resources Staffing and Recruiting''' – Many human resources staffing and recruiting organizations have adopted concept search technologies to produce highly relevant resume search results that provide more accurate and relevant candidate resumes than loosely related keyword results.

==Effective searching==
The effectiveness of a concept search can depend on a variety of elements including the dataset being searched and the search engine that is used to process queries and display results. However, most concept search engines work best for certain kinds of queries:

* Effective queries are composed of enough text to adequately convey the intended concepts.  Effective queries may include full sentences, paragraphs, or even entire documents.  Queries composed of just a few words are not as likely to return the most relevant results.
* Effective queries do not include concepts in a query that are not the object of the search.  Including too many unrelated concepts in a query can negatively affect the relevancy of the result items.  For example, searching for information about ''boating on the Mississippi River'' would be more likely to return relevant results than a search for ''boating on the Mississippi River on a rainy day in the middle of the summer in 1967.''
* Effective queries are expressed in a full-text, natural language style similar in style to the documents being searched.  For example, using queries composed of excerpts from an introductory science textbook would not be as effective for concept searching if the dataset being searched is made up of advanced, college-level science texts.  Substantial queries that better represent the overall concepts, styles, and language of the items for which the query is being conducted are generally more effective.

As with all search strategies, experienced searchers generally refine their queries through multiple searches, starting with an initial ''seed'' query to obtain conceptually relevant results that can then be used to compose and/or refine additional queries for increasingly more relevant results.  Depending on the search engine, using query concepts found in result documents can be as easy as selecting a document and performing a ''find similar'' function.  Changing a query by adding terms and concepts to improve result relevance is called ''[[query expansion]]''.&lt;ref&gt;[[Stephen Robertson (computer scientist)|Robertson, S. E.]], [[Karen Spärck Jones|Spärck Jones, K.]], Simple, Proven Approaches to Text Retrieval, Technical Report, University of Cambridge Computer Laboratory, December 1994.&lt;/ref&gt; The use of [[ontology (information science)|ontologies]] such as WordNet has been studied to expand queries with conceptually-related words.&lt;ref&gt;Navigli, R., Velardi, P. [http://www.dcs.shef.ac.uk/~fabio/ATEM03/navigli-ecml03-atem.pdf An Analysis of Ontology-based Query Expansion Strategies]. ''Proc. of Workshop on Adaptive Text Extraction and Mining (ATEM 2003)'', in the ''14th European Conference on Machine Learning (ECML 2003)'', Cavtat-Dubrovnik, Croatia, September 22-26th, 2003, pp.&amp;nbsp;42–49&lt;/ref&gt;

==Relevance feedback==
[[Relevance feedback]] is a feature that helps users determine if the results returned for their queries meet their information needs.  In other words, relevance is assessed relative to an information need, not a query.  A document is relevant if it addresses the stated information need, not because it just happens to contain all the words in the query.&lt;ref name="Manning, C. D. 2008"&gt;Manning, C. D., Raghavan P., Schütze H., Introduction to Information Retrieval, Cambridge University Press, 2008.&lt;/ref&gt;   It is a way to involve users in the retrieval process in order to improve the final result set.&lt;ref name="Manning, C. D. 2008"/&gt; Users can refine their queries based on their initial results to improve the quality of their final results.

In general, concept search relevance refers to the degree of similarity between the concepts expressed in the query and the concepts contained in the results returned for the query.  The more similar the concepts in the results are to the concepts contained in the query, the more relevant the results are considered to be.  Results are usually ranked and sorted by relevance so that the most relevant results are at the top of the list of results and the least relevant results are at the bottom of the list.

Relevance feedback has been shown to be very effective at improving the relevance of results.&lt;ref name="Manning, C. D. 2008"/&gt;   A concept search decreases the risk of missing important result items because all of the items that are related to the concepts in the query will be returned whether or not they contain the same words used in the query.&lt;ref name="Laplanche, R. 2004"/&gt;

[[Ranking]] will continue to be a part of any modern information retrieval system.  However, the problems of heterogeneous data, scale, and non-traditional discourse types reflected in the text, along with the fact that search engines will increasingly be integrated components of complex information management processes, not just stand-alone systems, will require new kinds of system responses to a query.  For example, one of the problems with ranked lists is that they might not reveal relations that exist among some of the result items.&lt;ref name="Callan, J. 2007"&gt;Callan, J., Allan, J., Clarke, C. L. A., Dumais, S., Evans, D., A., Sanderson, M., Zhai, C., Meeting of the MINDS: An Information Retrieval Research Agenda, ACM, SIGIR Forum, Vol. 41 No. 2, December 2007.&lt;/ref&gt;

==Guidelines for evaluating a concept search engine==
# Result items should be relevant to the information need expressed by the concepts contained in the query statements, even if the terminology used by the result items is different from the terminology used in the query.
# Result items should be sorted and ranked by relevance.
# Relevant result items should be quickly located and displayed.  Even complex queries should return relevant results fairly quickly.
# Query length should be ''non-fixed'', i.e., a query can be as long as deemed necessary.  A sentence, a paragraph, or even an entire document can be submitted as a query.
# A concept query should not require any special or complex syntax.  The concepts contained in the query can be clearly and prominently expressed without using any special rules.
# Combined queries using concepts, keywords, and metadata should be allowed.
# Relevant portions of result items should be usable as query text simply by selecting the item and telling the search engine to ''find similar'' items.
# Query-ready indexes should be created relatively quickly.
# The search engine should be capable of performing Federated searches.  Federated searching enables concept queries to be used for simultaneously searching multiple datasources for information, which are then merged, sorted, and displayed in the results.
# A concept search should not be affected by misspelled words, typographical errors, or OCR scanning errors in either the query text or in the text of the dataset being searched.

==Conferences and forums==
Formalized search engine evaluation has been ongoing for many years.  For example, the [[Text Retrieval Conference|Text REtrieval Conference (TREC)]] was started in 1992 to support research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies.  Most of today's commercial search engines include technology first developed in TREC.&lt;ref&gt;Croft, B., Metzler, D., Strohman, T., Search Engines, Information Retrieval in Practice, Addison Wesley, 2009.&lt;/ref&gt;

In 1997, a Japanese counterpart of TREC was launched, called National Institute of Informatics Test Collection for IR Systems (NTCIR).  NTCIR conducts a series of evaluation workshops for research in information retrieval, question answering, text summarization, etc.  A European series of workshops called the Cross Language Evaluation Forum (CLEF) was started in 2001 to aid research in multilingual information access.  In 2002, the Initiative for the Evaluation of XML Retrieval (INEX) was established for the evaluation of content-oriented XML retrieval systems.

Precision and recall have been two of the traditional performance measures for evaluating information retrieval systems.  Precision is the fraction of the retrieved result documents that are relevant to the user's information need.  Recall is defined as the fraction of relevant documents in the entire collection that are returned as result documents.&lt;ref name="Manning, C. D. 2008"/&gt;

Although the workshops and publicly available test collections used for search engine testing and evaluation have provided substantial insights into how information is managed and retrieved, the field has only scratched the surface of the challenges people and organizations face in finding, managing, and, using information now that so much information is available.&lt;ref name="Callan, J. 2007"/&gt;   Scientific data about how people use the information tools available to them today is still incomplete because experimental research methodologies haven’t been able to keep up with the rapid pace of change. Many challenges, such as contextualized search, personal information management, information integration, and task support, still need to be addressed.&lt;ref name="Callan, J. 2007"/&gt;

==See also==
{{div col|3}}
* [[Approximate string matching]]
* [[Compound term processing]]
* [[Concept mining]]
* [[Information extraction]]
* [[Latent semantic analysis]]
* [[Semantic network]]
* [[Semantic search]]
* [[Semantic Web]]
* [[Statistical semantics]]
* [[Text mining]]
{{div col end}}

==References==
{{Reflist|2}}

==External links==
* [http://trec.nist.gov/ Text Retrieval Conference (TREC)]
* [http://research.nii.ac.jp/ntcir/ National Institute of Informatics Test Collection for IR Systems (NTCIR)]
* [http://www.clef-campaign.org/ Cross Language Evaluation Forum (CLEF)]
* [http://inex.is.informatik.uni-duisburg.de/ Initiative for the Evaluation of XML Retrieval (INEX)]

[[Category:Information retrieval genres]]</text>
      <sha1>dk7wu21bdfwvjikhnvf1koqctfxtl83</sha1>
    </revision>
  </page>
  <page>
    <title>Cognitive models of information retrieval</title>
    <ns>0</ns>
    <id>24963841</id>
    <revision>
      <id>756183083</id>
      <parentid>723050831</parentid>
      <timestamp>2016-12-22T15:00:23Z</timestamp>
      <contributor>
        <username>MrOllie</username>
        <id>6908984</id>
      </contributor>
      <comment>rm disguised advert</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5000" xml:space="preserve">{{Orphan|date=September 2012}}

'''Cognitive models of information retrieval''' rest on the mix of areas such as [[cognitive science]], [[human-computer interaction]], [[information retrieval]], and  [[library science]]. They describe the relationship between a person's cognitive model of the information sought and the organization of this information in an information system.  These models attempt to understand how a person is searching for information so that the database and the search of this database can be designed in such a way as to best serve the user. [[Information retrieval]] may incorporate multiple tasks and cognitive problems, particularly because different people may have different methods for attempting to find this information and expect the information to be in different forms.  Cognitive models of information retrieval may be attempts at something as apparently prosaic as improving search results or may be something more complex, such as attempting to create a database which can be queried with natural language search.

==Berrypicking==
One way of understanding how users search for information has been described by [[Marcia Bates]]&lt;ref&gt;[[Marcia Bates]] (1989). "The Design of Browsing and Berrypicking Techniques for the Online Search Interface." https://pages.gseis.ucla.edu/faculty/bates/berrypicking.html&lt;/ref&gt; at the [[University of California at Los Angeles]]. Bates argues that "berrypicking" better reflects how users search for information than previous models of information retrieval.  This may be because previous models were strictly linear and did not incorporate cognitive questions.  For instance, one typical model is of a simple linear match between a query and a document.  However, Bates points out that there are simple modifications that can be made to this process.  For instance, Salton has argued that user feedback may help improve the search results.&lt;ref&gt;[[Gerard Salton]] (1968). ''Automatic Information and Retrieval'' (Computer Science). Dubuque, Iowa: Mcgraw-Hill Inc.&lt;/ref&gt;

Bates argues that searches are evolving and occur bit by bit.  That is to say, a person constantly changes his or her search terms in response to the results returned from the information retrieval system.  Thus, a simple linear model does not capture the nature of information retrieval because the very act of searching causes feedback which causes the user to modify his or her [[cognitive model]] of the information being searched for.  In addition, information retrieval can be bit by bit.  Bates gives a number of examples.  For instance, a user may look through footnotes and follow these sources.  Or, a user may scan through recent journal articles on the topic.  In each case, the user's question may change and thus the search evolves.

==Exploratory Search==
Researchers in the areas of [[human-computer interaction]] and [[cognitive science]] focus on how people explore for information when interacting with the WWW. This kind of search, sometimes called [[exploratory search]], focuses on how people iteratively refine their search activities and update their internal representations of the search problems.&lt;ref&gt;Qu, Yan &amp; Furnas, George. "Model-driven formative evaluation of exploratory search: A study under a sensemaking framework"&lt;/ref&gt; Existing search engines were designed based on traditional library science theories related to retrieval basic facts and simple information through an interface. However, exploratory information retrieval often involves ill-defined search goals and evolving criteria for evaluation of relevance. The interactions between humans and the information system will therefore involve more cognitive activity, and systems that support exploratory search will therefore need to take into account the cognitive complexities involved during the dynamic information retrieval process.

==Natural language searching==

Another way in which cognitive models of information may help in information retrieval is with natural language searching.  For instance, How Stuff Works imagines a world in which, rather than searching for local movies, reading the reviews, then searching for local Mexican restaurants, and reading their reviews, you will simply type ""I want to see a funny movie and then eat at a good Mexican restaurant. What are my options?" into your browser, and you will receive a useful and relevant response.&lt;ref&gt;Strickland, J. (n.d.). HowStuffWorks "How Web 3.0 Will Work". Howstuffworks "Computer". Retrieved November 4, 2009, from http://computer.howstuffworks.com/web-30.htm&lt;/ref&gt;  Although such a thing is not possible today, it represents a holy grail for researchers into cognitive models of information retrieval.  The goal is to somehow program information retrieval programs to respond to natural language searches.  This would require a fuller understanding of how people structure queries.

==Notes==
{{Reflist}}

[[Category:Information retrieval genres]]
[[Category:Cognitive modeling]]</text>
      <sha1>pu3jxdkwsvvcsu1kh1ne7cadpz7g9ec</sha1>
    </revision>
  </page>
  <page>
    <title>Expertise finding</title>
    <ns>0</ns>
    <id>20227676</id>
    <revision>
      <id>761687027</id>
      <parentid>761624067</parentid>
      <timestamp>2017-01-24T06:32:58Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor />
      <comment>/* Importance of expertise */[[WP:CHECKWIKI]] error fix for #61.  Punctuation goes before References. Do [[Wikipedia:GENFIXES|general fixes]] if a problem exists. -</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="13620" xml:space="preserve">{{multiple issues|
{{original research|date=June 2015}}
{{refimprove|date=June 2015}}
{{cleanup|date=November 2010}}{{External links|date=January 2012}}
}}
'''Expertise finding''' is the use of tools for finding and assessing individual [[expertise]], with particular focus on scientific expertise.

== Importance of expertise ==

It can be argued that human expertise is more valuable than capital, means of production or intellectual property. Contrary to expertise, all other aspects of capitalism are now relatively generic: access to capital is global, as is access to means of production for many areas of manufacturing.  [[Intellectual property]] can be similarly licensed.  Furthermore, expertise finding is also a key aspect of [[institutional memory]], as without its experts an institution is effectively decapitated.  However, finding and “licensing” expertise, the key to the effective use of these resources, remain much harder, starting with the very first step: finding expertise that you can trust.

Until very recently, finding expertise required a mix of individual, social and collaborative practices, a haphazard process at best.  Mostly, it involved contacting individuals one trusts and asking them for referrals, while hoping that one’s judgment about those individuals is justified and that their answers are thoughtful.

In the last fifteen years, a class of [[knowledge management]] software has emerged to facilitate and improve the quality of expertise finding, termed “expertise locating systems”.  These software range from [[Social network service|social networking systems]] to [[knowledge base]]s.  Some software, like those in the social networking realm, rely on users to connect each other, thus using social filtering to act as [[Recommender system|“recommender systems”]].

At the other end of the spectrum are specialized [[knowledge base]]s that rely on experts to populate a specialized type of [[database]] with their self-determined areas of expertise and contributions, and do not rely on user recommendations.  Hybrids that feature expert-populated content in conjunction with user recommendations also exist, and are arguably more valuable for doing so.

Still other expertise knowledge bases rely strictly on external manifestations of expertise, herein termed “gated objects”, e.g., [[citation impact]]s for scientific papers or [[data mining]] approaches wherein many of the work products of an expert are collated.  Such systems are more likely to be free of user-introduced biases (e.g., [http://researchscorecard.com/ ResearchScorecard] ), though the use of computational methods can introduce other biases.

More recently, LinkedIn Expertise Search introduces a hybrid approach based on user-generated data (e.g., member profiles), community-based signals (e.g., recommendations and skill endorsements) and personalized signals (e.g., social connection between searcher and results).&lt;ref name=":0"&gt;{{Cite journal|last=Ha-Thuc|first=Viet|last2=Venkataraman|first2=Ganesh|last3=Rodriguez|first3=Mario|last4=Sinha|first4=Shakti|last5=Sundaram|first5=Senthil|last6=Guo|first6=Lin|date=2016-02-15|title=Personalized Expertise Search at LinkedIn|url=http://arxiv.org/abs/1602.04572|journal=arXiv:1602.04572 [cs]}}&lt;/ref&gt; Given required [[LinkedIn#Skills|skills]] and other types of information need like location and industries, the system allows recruiters to search for hiring candidates amongst more than 450 million LinkedIn members.

Examples of the systems outlined above are listed in Table 1.

'''Table 1: A classification of expertise location systems'''

{| class="wikitable" border="1"
|-
! Type
! Application domain
! Data source
! Examples
|-
| Social networking
| Professional networking
| User-generated and community-generated
|
* [[LinkedIn]] &lt;ref name=":0" /&gt;
|-
| [[Scientific literature]]
| Identifying publications with strongest research impact
| Third-party generated
|
* [[Science Citation Index]] (Thomson Reuters)[http://www.thomsonreuters.com/products_services/science/science_products/a-z/science_citation_index]
|-
| [[Scientific literature]]
| Expertise search
| Software
|
* [[Arnetminer]][http://arnetminer.org]
|-
| Knowledge base
| Private expertise database
| User-Generated
|
* [http://www.mitre.org/news/the_edge/june_98/third.html MITRE Expert Finder] (MITRE Corporation)
* MIT ExpertFinder (ref. 3)
* Decisiv Search Matters &amp; Expertise ([[Recommind (software company)|Recommind]], Inc.)
* [http://www.profinda.com ProFinda] (ProFinda Ltd)
* [https://skillhive.com Skillhive] (Intunex)
* [[Tacit Software]] (Oracle Corporation)
* [http://www.guruscan.nl GuruScan] (GuruScan Social Expert Guide)
|-
| Knowledge base
| Publicly accessible expertise database
| User-generated
|
* [http://expertisefinder.com/ Expertise Finder]&lt;ref&gt;http://expertisefinder.com/&lt;/ref&gt;
* [[Community of Science]] Expertise [http://expertise.cos.com]
* [[ResearcherID]] (Thomson Reuters)[http://www.thomsonreuters.com/products_services/scientific/ResearcherID]
|-
| Knowledge base
| Private expertise database
| Third party-generated
|
* [http://www.mitre.org/news/the_edge/june_98/third.html MITRE Expert Finder] (MITRE Corporation)
* MIT ExpertFinder (ref. 3)
* MindServer Expertise ([[Recommind]], Inc.)
* Tacit Software
|-
| Knowledge base
| Publicly accessible expertise database
| Third party-generated
|
* [http://researchscorecard.com ResearchScorecard] (ResearchScorecard Inc.)
* [http://authoratory.com/ authoratory.com]
* [http://biomedexperts.com BiomedExperts] (Collexis Holdings Inc.)
* [http://www.hcarknowledgemesh.com/ KnowledgeMesh] (Hershey Center for Applied Research)
* [http://med.stanford.edu/profiles/ Community Academic Profiles] (Stanford School of Medicine)
* [https://web.archive.org/web/20081120175851/http://www.researchcrossroads.org/ ResearchCrossroads.org]  (Innolyst, Inc.)
|-
| Blog [[search engine]]s
|
| Third party-generated
|
* [[Technorati]] [http://technorati.com/]
|}

== Technical problems ==
A number of interesting problems follow from the use of expertise finding systems:

* The matching of questions from non-expert to the database of existing expertise is inherently difficult, especially when the database does not store the requisite expertise.  This problem grows even more acute with increasing ignorance on the part of the non-expert due to typical search problems involving use of keywords to search unstructured data that are not semantically normalized, as well as variability in how well an expert has set up their descriptive content pages.  Improved question matching is one reason why third-party semantically normalized systems such as [http://researchscorecard.com ResearchScorecard] and [[BiomedExperts]] should be able to provide better answers to queries from non-expert users.
* Avoiding expert-fatigue due to too many questions/requests from users of the system (ref. 1).
* Finding ways to avoid “gaming” of the system to reap unjustified expertise [[credibility]].
* Infer expertise on implicit skills. Since users typically do not declare all of the skills they have, it is important to infer their implicit skills that are highly related their explicit ones. The inference step can significantly improve [[Precision and recall|recall]] in expertise finding.

== Expertise ranking ==

Means of classifying and ranking expertise (and therefore experts) become essential if the number of experts returned by a query is greater than a handful.  This raises the following social problems associated with such systems:

* How can expertise be assessed objectively? Is that even possible?
* What are the consequences of relying on unstructured social assessments of expertise, such as user recommendations?
* How does one distinguish [[Authority|''authoritativeness'']] as a proxy metric of expertise from simple ''popularity'', which is often a function of one's ability to express oneself coupled with a good social sense?
* What are the potential consequences of the social or professional stigma associated with the use of an authority ranking, such as used in [http://technorati.com Technorati] and [http://researchscorecard.com ResearchScorecard])?
* How to make expertise ranking personalized to each individual searcher? This is particularly important for recruiting purpose since given the same skills, recruiters from different companies, industries, locations might have different preferences on candidates &lt;ref name=":0" /&gt;

== Sources of data for assessing expertise ==
Many types of data sources have been used to infer expertise.  They can be broadly categorized based on whether they measure "raw" contributions provided by the expert, or whether some sort of filter is applied to these contributions.

Unfiltered data sources that have been used to assess expertise, in no particular ranking order:

* user recommendations
* help desk tickets: what the problem was and who fixed it
* e-mail traffic between users
* documents, whether private or on the web, particularly publications
* user-maintained web pages
* reports (technical, marketing, etc.)

Filtered data sources, that is, contributions that require approval by third parties (grant committees, referees, patent office, etc.) are particularly valuable for measuring expertise in a way that minimizes biases that follow from popularity or other social factors:

* [[patent]]s, particularly if issued
* scientific publications
* issued grants (failed grant proposals are rarely know beyond the authors)
* [[clinical trial]]s
* product launches
* pharmaceutical drugs

== Approaches for creating expertise content ==
* Manual, either by experts themselves (e.g., [https://skillhive.com Skillhive]) or by a curator ([http://expertisefinder.com/ Expertise Finder])
* Automated, e.g., using [[software agent]]s (e.g., MIT's [http://web.media.mit.edu/~lieber/Lieberary/Expert-Finder/Expert-Finder-Intro.html ExpertFinder] and the [http://wiki.foaf-project.org/ExpertFinder ExpertFinder] initiative) or a combination of agents and human curation (e.g., [http://researchscorecard.com/ ResearchScorecard] )
* In industrial expertise search engines (e.g., LinkedIn), there are many signals coming into the ranking functions, such as, user-generated content (e.g., profiles), community-generated content (e.g., recommendations and skills endorsements) and personalized signals (e.g., social connections). Moreover, user queries might contain many other aspects rather required expertise, such as, locations, industries or companies. Thus, traditional [[information retrieval]] features like text matching are also important. [[Learning to rank]] is typically used to combine all of these signals together into a ranking function &lt;ref name=":0" /&gt;

== Interesting expertise systems over the years ==
In no particular order...

* [http://www.guruscan.nl/ GuruScan]
* Autonomy's IDOL
* AskMe
* [http://expertisefinder.com/ Expertise Finder]
* Tacit Knowledge Systems' ActiveNet
* Triviumsoft's SEE-K
* MIT’s [http://web.media.mit.edu/~lieber/Lieberary/Expert-Finder/Expert-Finder-Intro.html ExpertFinder] (ref 3)
* MITRE’s (ref 1) [http://www.mitre.org/news/the_edge/june_98/third.html Expert Finder]
* MITRE’s XpertNet
* Arnetminer (ref 2)
* Dataware II Knowledge Directory
* Thomson’s tool
* Hewlett-Packard’s CONNEX
* Microsoft’s SPUD project
* [http://www.profinda.com ProFinda]
* [http://www.xperscore.com Xperscore]
* [http://intunex.fi/skillhive/ Skillhive]
* LinkedIn&lt;ref name=":0" /&gt;

== Conferences ==
# [http://expertfinder.info/pickme2008 The ExpertFinder Initiative]

== References ==
{{Reflist}}

# Ackerman, Mark and McDonald, David (1998) "Just Talk to Me: A Field Study of Expertise Location" ''Proceedings of the 1998 ACM Conference on Computer Supported Cooperative Work''.
# Hughes, Gareth and Crowder, Richard (2003) "Experiences in designing highly adaptable expertise finder systems"  ''Proceedings of the DETC Conference 2003''.
# Maybury, M., D’Amore, R., House, D. (2002). "Awareness of organizational expertise." ''International Journal of Human-Computer Interaction'' '''14'''(2): 199-217.
# Maybury, M., D’Amore, R., House, D. (2000). Automating Expert Finding. ''International Journal of Technology Research Management.'' 43(6): 12-15.
# Maybury, M., D’Amore, R, and House, D. December (2001). Expert Finding for Collaborative Virtual Environments.  ''Communications of the ACM 14''(12): 55-56. In Ragusa, J. and Bochenek, G. (eds). Special Section on Collaboration Virtual Design Environments.
# Maybury, M., D’Amore, R. and House, D. (2002). Automated Discovery and Mapping of Expertise.  In Ackerman, M., Cohen, A., Pipek, V. and Wulf, V. (eds.). ''Beyond Knowledge Management: Sharing Expertise.'' Cambridge: MIT Press.
# Mattox, D., M. Maybury, ''et al.'' (1999). "Enterprise expert and knowledge discovery". ''Proceedings of the 8th International Conference on Human-Computer Interactions (HCI International 99)'', Munich, Germany.
# Tang, J., Zhang J., Yao L., Li J., Zhang L. and Su Z.(2008) "ArnetMiner: extraction and mining of academic social networks" ''Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining''.
# Viavacqua, A. (1999). "Agents for expertise location". ''Proceedings of the 1999 AAAI Spring Symposium on Intelligent Agents in Cyberspace'', Stanford, CA.

[[Category:Evaluation methods]]
[[Category:Metrics]]
[[Category:Analysis]]
[[Category:Impact assessment]]
[[Category:Knowledge sharing]]
[[Category:Library science]]
[[Category:Information retrieval genres]]
[[Category:Science studies]]</text>
      <sha1>5qwrh7ca3umvnhfygu2648br4addfdx</sha1>
    </revision>
  </page>
  <page>
    <title>Audio mining</title>
    <ns>0</ns>
    <id>14004969</id>
    <revision>
      <id>666861886</id>
      <parentid>545015261</parentid>
      <timestamp>2015-06-14T05:55:42Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>Category:Information retrieval genres, Category:Music information retrieval</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1827" xml:space="preserve">{{unreferenced|date=January 2012}}
'''Audio mining''' is a technique by which the content of an audio signal can be automatically analysed and searched. It is most commonly used in the field of [[speech recognition|automatic speech recognition]], where the analysis tries to identify any speech within the audio. The audio will typically be processed by a speech recognition system in order to identify word or [[phoneme]] units that are likely to occur in the spoken content. This information may either be used immediately in pre-defined searches for keywords or phrases (a real-time "word spotting" system), or the output of the speech recogniser may be stored in an index file. One or more audio mining index files can then be loaded at a later date in order to run searches for keywords or phrases.

The results of a search will normally be in terms of hits, which are regions within files that are good matches for the chosen keywords. The user may then be able to listen to the audio corresponding to these hits in order to verify if a correct match was found.

Audio mining systems used in the field of speech recognition are often divided into two groups: those that use [[Large Vocabulary Continuous Speech Recogniser]]s (LVCSR) and those that use phonetic recognition. 

Musical audio mining (also known as [[music information retrieval]]) relates to the identification of perceptually important characteristics of a piece of music such as melodic, harmonic or rhythmic structure. Searches can then be carried out to find pieces of music that are similar in terms of their melodic, harmonic and/or rhythmic characteristics.

==See also==
* [[Speech Analytics]]


[[Category:Speech recognition]]
[[Category:Music information retrieval]]
[[Category:Information retrieval genres]]
[[Category:Computational linguistics]]</text>
      <sha1>bqhv14kmvpgp0oeq98cum362ludlrz5</sha1>
    </revision>
  </page>
  <page>
    <title>Full-text search</title>
    <ns>0</ns>
    <id>1315248</id>
    <revision>
      <id>760868003</id>
      <parentid>760866952</parentid>
      <timestamp>2017-01-19T15:11:03Z</timestamp>
      <contributor>
        <username>Mean as custard</username>
        <id>10962546</id>
      </contributor>
      <comment>remove red links</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="12773" xml:space="preserve">{{Multiple issues|
{{refimprove|date=August 2012}}
{{cleanup|date=September 2009}}
}}

In [[text retrieval]], '''full-text search''' refers to techniques for searching a single [[computer]]-stored [[document]] or a collection in a [[full text database]]. Full-text search is distinguished from searches based on [[metadata]] or on parts of the original texts represented in databases (such as titles, abstracts, selected sections, or bibliographical references).

In a full-text search, a [[search engine]] examines all of the words in every stored document as it tries to match search criteria (for example, text specified by a user). Full-text-searching techniques became common in online [[bibliographic databases]] in the 1990s.{{Verify source|date=October 2008}} Many websites and application programs (such as [[word processing]] software) provide full-text-search capabilities. Some web search engines, such as [[AltaVista]], employ full-text-search techniques, while others index only a portion of the web pages examined by their indexing systems.&lt;ref&gt;In practice, it may be difficult to determine how a given search engine works. The [[search algorithms]] actually employed by web-search services are seldom fully disclosed out of fear that web entrepreneurs will use [[search engine optimization]] techniques to improve their prominence in retrieval lists.&lt;/ref&gt;

==Indexing==
When dealing with a small number of documents, it is possible for the full-text-search engine to directly scan the contents of the documents with each [[Information retrieval|query]], a strategy called "[[Serial memory processing|serial scanning]]". This is what some tools, such as [[grep]], do when searching.

However, when the number of documents to search is potentially large, or the quantity of search queries to perform is substantial, the problem of full-text search is often divided into two tasks: indexing and searching. The indexing stage will scan the text of all the documents and build a list of search terms (often called an [[Search index|index]], but more correctly named a [[concordance (publishing)|concordance]]). In the search stage, when performing a specific query, only the index is referenced, rather than the text of the original documents.&lt;ref name="Capabilities of Full Text Search System "&gt;[http://www.lucidimagination.com/full-text-search Capabilities of Full Text Search System] {{webarchive |url=https://web.archive.org/web/20101223192214/http://www.lucidimagination.com/full-text-search |date=December 23, 2010 }}&lt;/ref&gt;

The indexer will make an entry in the index for each term or word found in a document, and possibly note its relative position within the document. Usually the indexer will ignore [[stop words]] (such as "the" and "and") that are both common and insufficiently meaningful to be useful in searching. Some indexers also employ language-specific [[stemming]] on the words being indexed. For example, the words "drives", "drove", and "driven" will be recorded in the index under the single concept word "drive".

==The precision vs. recall tradeoff==
[[Image:Full-text-search-results.png|150px|thumb|right|Diagram of a low-precision, low-recall search]]
Recall measures the quantity of relevant results returned by a search, while precision is the measure of the quality of the results returned. Recall is the ratio of relevant results returned to all relevant results. Precision is the number of relevant results returned to the total number of results returned.

The diagram at right represents a low-precision, low-recall search. In the diagram the red and green dots represent the total population of potential search results for a given search. Red dots represent irrelevant results, and green dots represent relevant results. Relevancy is indicated by the proximity of search results to the center of the inner circle. Of all possible results shown, those that were actually returned by the search are shown on a light-blue background. In the example only 1 relevant result of 3 possible relevant results was returned, so the recall is a very low ratio of 1/3, or 33%. The precision for the example is a very low 1/4, or 25%, since only 1 of the 4 results returned was relevant.&lt;ref name="isbn1430215941"&gt;{{cite book|last=Coles|first=Michael|year=2008|title=Pro Full-Text Search in SQL Server 2008|edition=Version 1|publisher=[[Apress|Apress Publishing Company]]|isbn=1-4302-1594-1}}&lt;/ref&gt;

Due to the ambiguities of [[natural language]], full-text-search systems typically includes options like [[stop words]] to increase precision and [[stemming]] to increase recall. [[Controlled vocabulary|Controlled-vocabulary]] searching also helps alleviate low-precision issues by [[tag (metadata)|tagging]] documents in such a way that ambiguities are eliminated. The trade-off between precision and recall is simple: an increase in precision can lower overall recall, while an increase in recall lowers precision.&lt;ref name="YuwonoLee"&gt;{{Cite conference | first = Yuwono | last = B. |author2=Lee, D. L. | title = Search and ranking algorithms for locating resources on the World Wide Web | pages = 164 | publisher = 12th International Conference on Data Engineering (ICDE'96) | year = 1996}}&lt;/ref&gt;

{{See also|Precision and recall}}

==False-positive problem==

Free text searching is likely to retrieve many documents that are not [[relevance|relevant]] to the ''intended'' search question. Such documents are called ''false positives'' (see [[Type I and type II errors#Type I error|Type I error]]). The retrieval of irrelevant documents is often caused by the inherent ambiguity of [[natural language]]. In the sample diagram at right, false positives are represented by the irrelevant results (red dots) that were returned by the search (on a light-blue background).

Clustering techniques based on [[Bayesian inference|Bayesian]] algorithms can help reduce false positives. For a search term of "bank", clustering can be used to categorize the document/data universe into "financial institution", "place to sit", "place to store" etc. Depending on the occurrences of words relevant to the categories, search terms or a search result can be placed in one or more of the categories. This technique is being extensively deployed in the [[Electronic discovery|e-discovery]] domain.{{clarify|date=January 2012}}

==Performance improvements==

The deficiencies of free text searching have been addressed in two ways: By providing users with tools that enable them to express their search questions more precisely, and by developing new search algorithms that improve retrieval precision.

===Improved querying tools===

*[[Index term|Keyword]]s. Document creators (or trained indexers) are asked to supply a list of words that describe the subject of the text, including synonyms of words that describe this subject. Keywords improve recall, particularly if the keyword list includes a search word that is not in the document text.
* [[Field-restricted search]]. Some search engines enable users to limit free text searches to a particular [[field (computer science)|field]] within a stored [[Record (computer science)|data record]], such as "Title" or "Author."
* [[Boolean query|Boolean queries]]. Searches that use [[Boolean logic|Boolean]] operators (for example, &lt;tt&gt;"encyclopedia" [[Logical conjunction|AND]] "online" [[Negation|NOT]] "Encarta"&lt;tt&gt;) can dramatically increase the precision of a free text search. The &lt;tt&gt;AND&lt;/tt&gt; operator says, in effect, "Do not retrieve any document unless it contains both of these terms." The &lt;tt&gt;NOT&lt;/tt&gt; operator says, in effect, "Do not retrieve any document that contains this word." If the retrieval list retrieves too few documents, the &lt;tt&gt;OR&lt;/tt&gt; operator can be used to increase [[recall (information retrieval)|recall]]; consider, for example, &lt;tt&gt;"encyclopedia" AND "online" [[Logical disjunction|OR]] "Internet" NOT "Encarta"&lt;/tt&gt;. This search will retrieve documents about online encyclopedias that use the term "Internet" instead of "online." This increase in precision is very commonly counter-productive since it usually comes with a dramatic loss of recall.&lt;ref&gt;Studies have repeatedly shown that most users do not understand the negative impacts of boolean queries.[http://eprints.cs.vt.edu/archive/00000112/]&lt;/ref&gt;
* [[Phrase search]]. A phrase search matches only those documents that contain a specified phrase, such as &lt;tt&gt;"Wikipedia, the free encyclopedia."&lt;/tt&gt;
* [[Concept search]]. A search that is based on multi-word concepts, for example [[Compound term processing]]. This type of search is becoming popular in many e-Discovery solutions.
* [[Concordance search]]. A concordance search produces an alphabetical list of all principal words that occur in a [[Plain text|text]] with their immediate context.
* [[Proximity search (text)|Proximity search]]. A phrase search matches only those documents that contain two or more words that are separated by a specified number of words; a search for &lt;tt&gt;"Wikipedia" WITHIN2 "free"&lt;tt&gt; would retrieve only those documents in which the words &lt;tt&gt;"Wikipedia" and "free"&lt;/tt&gt; occur within two words of each other.
* [[Regular expression]]. A regular expression employs a complex but powerful querying [[syntax]] that can be used to specify retrieval conditions with precision.
* [[Fuzzy search]] will search for document that match the given terms and some variation around them (using for instance [[edit distance]] to threshold the multiple variation)
* [[Wildcard character|Wildcard search]]. A search that substitutes one or more characters in a search query for a wildcard character such as an [[asterisk]]. For example, using the asterisk in a search query &lt;tt&gt;"s*n"&lt;/tt&gt; will find "sin", "son", "sun", etc. in a text.

===Improved search algorithms===
The [[PageRank]] algorithm developed by [[Google]] gives more prominence to documents to which other [[Web page]]s have linked.&lt;ref&gt;{{Cite patent | inventor-last = Page | inventor-first = Lawrence | publication-date = 1/9/1998 | issue-date = 9/4/2001 | title = Method for node ranking in a linked database | country-code = US | description = A method assigns importance ranks to nodes in a linked database, such as any database of documents containing citations, the world wide web or any other hypermedia database. The rank assigned to a document is calculated from the ranks of documents citing it. In addition, the rank of a document is... | patent-number = 6285999 | postscript = &lt;!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. --&gt;{{inconsistent citations}}}}&lt;/ref&gt; See [[Search engine]] for additional examples.

==Software==

The following is a partial list of available software products whose predominant purpose is to perform full text indexing and searching. Some of these are accompanied with detailed descriptions of their theory of operation or internal algorithms, which can provide additional insight into how full text search may be accomplished.

{{col-float}}

=== Free and open source software ===
&lt;!--

Please do not add web links or products which do not have Wikipedia articles. They will be summarily deleted.

--&gt;
* [[BaseX]]
* [[Clusterpoint|Clusterpoint Database]]
* [[Elasticsearch]]
* [[Ht-//Dig|ht://Dig]]
* [[KinoSearch]]
* [[Lemur Project|Lemur/Indri]]
* [[Lucene]]
* [[mnoGoSearch]]
* [[Searchdaimon]]
* [[Sphinx (search engine)|Sphinx]]
* [[Swish-e]]
* [[Xapian]]
* [[Apache Solr]]

{{col-float-break}}

=== Proprietary software ===
&lt;!--

Please do not add web links or products which do not have Wikipedia articles. They will be summarily deleted.

--&gt;
* [[Algolia]]
* [[Autonomy Corporation]]
* [[Azure Search]]
* [[Bar Ilan Responsa Project]]
* [[Brainware]]
* [[BRS/Search]] 
* [[Concept Searching Limited]]
* [[Dieselpoint]]
* [[dtSearch]]
* [[Endeca]]
* [[Exalead]]
* [[Funnelback]]
* [[Fast Search &amp; Transfer]]
* [[Inktomi (company)|Inktomi]]
* [[Dan Wagner#Locayta|Locayta]](rebranded to [[ATTRAQT]] in 2014)
* [[Lucid Imagination]]
* [[MarkLogic]]
* [[SAP HANA]]&lt;ref&gt;http://www.martechadvisor.com/news/databases-big-data/sap-adds-hanabased-software-packages-to-iot-portfolio/&lt;/ref&gt;
* [[Swiftype]]
* [[Thunderstone Software LLC.]]
* [[Vivísimo]]
{{col-float-end}}

==Notes==
{{Reflist}}

==See also==
*[[Pattern matching]] and [[string matching]]
*[[Compound term processing]]
*[[Enterprise search]]
*[[Information extraction]]
*[[Information retrieval]]
*[[Faceted search]]
*[[List of enterprise search vendors]]
*[[WebCrawler]], first FTS engine
*[[Search engine indexing]] - how search engines generate indices to support full text searching

{{DEFAULTSORT:Full Text Search}}
[[Category:Text editor features]]
[[Category:Information retrieval genres]]</text>
      <sha1>5g8k8uk87xs5exxy8fhi4oaeeyjin4d</sha1>
    </revision>
  </page>
  <page>
    <title>Maarten de Rijke</title>
    <ns>0</ns>
    <id>31200516</id>
    <revision>
      <id>750205266</id>
      <parentid>719731370</parentid>
      <timestamp>2016-11-18T09:17:51Z</timestamp>
      <contributor>
        <username>1Veertje</username>
        <id>12358798</id>
      </contributor>
      <comment>+img</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3080" xml:space="preserve">[[File:Maarten de Rijke - CLEF 2011 (cropped).jpg|thumb|Maarten de Rijke, 2011]]'''Maarten de Rijke''' (born 1 August 1961) is a [[Netherlands|Dutch]] computer scientist. His work initially focused on [[modal logic]] and [[knowledge representation]], but since the early years of the 21st century he has worked mainly in [[information retrieval]]. His work is supported by grants from the [[Nederlandse Organisatie voor Wetenschappelijk Onderzoek]] (NWO), public-private partnerships, and the European Commission (under the Sixth and Seventh Framework programmes).


==Biography==
Maarten de Rijke was born in [[Vlissingen]].  He studied philosophy (MSc 1989) and mathematics (MSc 1990) and wrote a PhD thesis, defended in 1993, on extended modal logics, under the supervision of [[Johan van Benthem (logician)|Johan van Benthem]].

De Rijke worked as a postdoc at the [[Centrum Wiskunde &amp; Informatica]], before becoming a Warwick Research Fellow at the [[University of Warwick]]. He joined the [[University of Amsterdam]] in 1998, and was appointed professor of Information Processing and Internet at the [[Informatics Institute]] of the University of Amsterdam in 2004.&lt;ref name="MdR11Bio"&gt;[http://staff.science.uva.nl/~mdr/Bio/ Bio of Maarten de Rijke] at the University of Amsterdam. Retrieved 16 March 2011.&lt;/ref&gt;

He leads the Information and Language Processing group&lt;ref&gt;[http://ilps.science.uva.nl Information and Language Processing group]&lt;/ref&gt; at the University of Amsterdam, the Intelligent Systems Lab Amsterdam&lt;ref&gt;[http://isla.science.uva.nl Intelligent Systems Lab Amsterdam] within the Informatics Institute of the University of Amsterdam.&lt;/ref&gt; and the Center for Creation, Content and Technology.&lt;ref&gt;[http://www.ccct.uva.nl Center for Creation, Content and Technology] at the University of Amsterdam.&lt;/ref&gt;

==Work==
During the first ten years of his scientific career Maarten de Rijke worked on formal and applied aspects of modal logic. At the start of the 21st century, De Rijke switched to information retrieval. He has since worked on [[XML retrieval]], [[question answering]], [[expert finding]] and [[social media analysis]].

==Publications==
Maarten de Rijke has published more than 600 papers and books.&lt;ref name="MdR11Pubs"&gt;[http://staff.science.uva.nl/~mdr/Publications/ List of publications of Maarten de Rijke] at the University of Amsterdam". Retrieved 16 March 2011.&lt;/ref&gt;

==References==
{{reflist}}
*[http://albumacademicum.uva.nl/cgi/b/bib/bib-idx?type=simple;lang=en;c=ap;rgn1=entirerecord;q1=rijke;x=0;y=0;cc=ap;view=reslist;sort=achternaam;fmt=long;page=reslist;size=1;start=14 Prof. dr. M. de Rijke, 1961 -] at the [[University of Amsterdam]] ''Album Academicum'' website

==External links==
* [http://staff.science.uva.nl/~mdr Home page]

{{DEFAULTSORT:Rijke, Maarten De}}
[[Category:1961 births]]
[[Category:Living people]]
[[Category:Dutch computer scientists]]
[[Category:University of Amsterdam alumni]]
[[Category:University of Amsterdam faculty]]
[[Category:People from Vlissingen]]
[[Category:Information retrieval researchers]]</text>
      <sha1>98ri2nkzofz3isx1jtts5fy4tp59v1e</sha1>
    </revision>
  </page>
  <page>
    <title>Stephen Robertson (computer scientist)</title>
    <ns>0</ns>
    <id>24019253</id>
    <revision>
      <id>741150362</id>
      <parentid>717032186</parentid>
      <timestamp>2016-09-25T18:34:15Z</timestamp>
      <contributor>
        <username>Timrollpickering</username>
        <id>32005</id>
      </contributor>
      <minor />
      <comment>/* External links */rename cat per [[Wikipedia:Categories for discussion/Log/2016 September 2]], replaced: Category:Alumni of City University London → Category:Alumni of City, University of London, Category:Academics of C using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4019" xml:space="preserve">{{More footnotes|date=September 2012}}
{{Infobox scientist
| name = Stephen Robertson
| image =
| image_size = 100px
| residence = United Kingdom
| nationality = British
| field = Computer science
| alma_mater = Cambridge, City University, University College London
| doctoral_advisor = B.C (Bertie) Brookes
| doctoral_students = Ayse Göker, Andrew MacFarlane, Xiangji (Jimmy) Huang, Olga Vechtomova, Murat Karamuftuoglu, Micheline Beaulieu, Efthimis Efthimiadis, Anna Ritchie, Jagadeesh Gorla
| known_for  = Work on information retrieval and inverse document frequency
| prizes = [[Gerard Salton Award]] (2000), [[Tony Kent Strix award]] (1998), [[ACM Fellow]] (2013)
| website = {{URL|http://staff.city.ac.uk/~sb317}}
}}

'''Stephen Robertson''' is a [[United Kingdom|British]] computer scientist. He is known for his work on [[information retrieval]]&lt;ref&gt;{{Cite journal | doi = 10.1002/asi.4630270302| title = Relevance weighting of search terms| journal = Journal of the American Society for Information Science| volume = 27| issue = 3| pages = 129| year = 1976| last1 = Robertson | first1 = S. E. | authorlink1 = Stephen Robertson (computer scientist)| last2 = Spärck Jones | first2 = K. | authorlink2 = Karen Spärck Jones}}&lt;/ref&gt; and the [[Okapi BM25]] weighting model.&lt;ref&gt;{{Cite journal | doi = 10.1016/S0306-4573(00)00015-7| title = A probabilistic model of information retrieval: Development and comparative experiments: Part 1| journal = Information Processing &amp; Management| volume = 36| issue = 6| pages = 779–808| year = 2000| last1 = Spärck Jones | first1 = K. | authorlink1 = Karen Spärck Jones| last2 = Walker | first2 = S.| last3 = Robertson | first3 = S. E. | authorlink3 = Stephen Robertson (computer scientist)}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal | doi = 10.1016/S0306-4573(00)00016-9| title = A probabilistic model of information retrieval: Development and comparative experiments: Part 2| journal = Information Processing &amp; Management| volume = 36| issue = 6| pages = 809–840| year = 2000| last1 = Spärck Jones | first1 = K. | authorlink1 = Karen Spärck Jones| last2 = Walker | first2 = S. | last3 = Robertson | first3 = S. E. | authorlink3 = Stephen Robertson (computer scientist)}}&lt;/ref&gt;

After completing his undergraduate degree in mathematics at [[Cambridge university|Cambridge University]], he took an MS at [[City University, London|City University]], and then worked for [[ASLIB]]. He then studied for his PhD at [[University College London]] under the renowned statistician and scholar B. C. Brookes. He then returned to City University working there from 1978 until 1998 in the Department of [[Information Science]], continuing as a part-time professor and subsequently as professor emeritus. He is also a fellow of [[Girton College, Cambridge|Girton College]], Cambridge University.

From 1998 to 2013 he worked in the Cambridge laboratory of [[Microsoft Research]], where he led a group investigating core search processes such as term weighting, document scoring and ranking algorithms, combining evidence from different sources, and metrics and methods for the evaluation and optimisation of search. Much of his work has contributed to the [[Microsoft]] [[Web search engine|search engine]] [[Bing (search engine)|Bing]]. He participated a number of times in the [[Text Retrieval Conference|TREC conference]].

==References==
{{Reflist}}

== External links ==
* {{cite book|last1=Robertson|first1=Stephen|last2=Zaragoza|first2=Hugo|title=The Probabilistic Relevance Framework: BM25 and Beyond|date=2009|publisher=NOW Publishers, Inc.|isbn=978-1-60198-308-4|url=http://staff.city.ac.uk/~sb317/papers/foundations_bm25_review.pdf}}

{{DEFAULTSORT:Robertson, Stephen}}
[[Category:British computer scientists]]
[[Category:Alumni of University College London]]
[[Category:Fellows of Girton College, Cambridge]]
[[Category:Living people]]
[[Category:Alumni of City, University of London]]
[[Category:Academics of City, University of London]]
[[Category:Information retrieval researchers]]</text>
      <sha1>coqwj4mcnpl0qh4d62zf50s00r8ir3t</sha1>
    </revision>
  </page>
  <page>
    <title>C. J. van Rijsbergen</title>
    <ns>0</ns>
    <id>1514191</id>
    <revision>
      <id>705677791</id>
      <parentid>666734175</parentid>
      <timestamp>2016-02-18T22:31:26Z</timestamp>
      <contributor>
        <username>KasparBot</username>
        <id>24420788</id>
      </contributor>
      <comment>migrating [[Wikipedia:Persondata|Persondata]] to Wikidata, [[toollabs:kasparbot/persondata/|please help]], see [[toollabs:kasparbot/persondata/challenge.php/article/C. J. van Rijsbergen|challenges for this article]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3647" xml:space="preserve">{{Use dmy dates|date=March 2014}}
{{Use British English|date=March 2014}}
{{multiple issues|
{{BLP sources|date=January 2012}}
{{More footnotes|date=January 2012}}
}}

{{Infobox scientist
|name              = Cornelis Joost van Rijsbergen
|image             = C J van Rijsbergen.jpg
|image_size        =
|caption           = C. J. "Keith" van Rijsbergen
|birth_date        = {{birth year and age|1943}}
|birth_place       = [[Rotterdam]]
|residence         = 
|citizenship       =
|nationality       = 
|fields            = [[Information Retrieval]]
|workplaces        = [[Monash University]], [[University of Glasgow]]
|alma_mater        = [[University of Western Australia]], [[University of Cambridge]]
|doctoral_advisor  = 
|academic_advisors =
|doctoral_students =
|notable_students  =
|known_for         = 
|author_abbrev_bot =
|author_abbrev_zoo =
|influences        =
|influenced        =
|awards            =
|signature         = &lt;!--(filename only)--&gt;
|footnotes         =
}}

'''C. J. "Keith" van Rijsbergen''' [[FREng]]&lt;ref name=fellow&gt;{{cite web|title=List of Fellows|url=http://www.raeng.org.uk/about-us/people-council-committees/the-fellowship/list-of-fellows}}&lt;/ref&gt; ('''Cornelis Joost van Rijsbergen''') (born 1943) is a professor of [[computer science]] and the leader of the Glasgow Information Retrieval Group based at the [[University of Glasgow]]. He is one of the founders of modern [[Information Retrieval]] and the author of the seminal monograph ''Information Retrieval'' and of the textbook ''The Geometry of Information Retrieval''.

He was born in [[Rotterdam]], and educated in the [[Netherlands]], [[Indonesia]], [[Namibia]] and [[Australia]].
His first degree is in mathematics from the [[University of Western Australia]], and in 1972 he completed a
PhD in computer science at the [[University of Cambridge]].
He spent three years lecturing in information retrieval and artificial intelligence at [[Monash University]]
before returning to [[University of Cambridge|Cambridge]] to hold a [[Royal Society]] Information Research Fellowship. 
In 1980 he was appointed to the chair of computer science at [[University College Dublin]];
from there he moved in 1986 to [[Glasgow University]].
Since 2007 he has been Chairman of the Scientific Board of the [[Information Retrieval Facility]].

==Awards and honors==
In 2003 he was inducted as a Fellow of the [[Association for Computing Machinery]]. In 2004 he was awarded the [[Tony Kent Strix award]].
In 2004 he was appointed a [[Fellow]]&lt;ref name=fellow /&gt; of the [[Royal Academy of Engineering]].&lt;ref name=fellow /&gt;
In 2006, he was awarded the [[Gerard Salton Award]] for ''Quantum haystacks''.

==See also==
*[[F1 score]]

==References==
{{Reflist}}

==External links==
*[http://www.dcs.gla.ac.uk/~keith/ C. J. "Keith" van Rijsbergen - The University of Glasgow]
*[http://ir.dcs.gla.ac.uk/ Glasgow Information Retrieval Group]
*[http://www.dcs.gla.ac.uk/Keith/Preface.html Information Retrieval book - C. J. van Rijsbergen 1979]
*[http://www.ir-facility.org/ Information Retrieval Facility]
*{{worldcat id|id=lccn-n83-236586}}
* [http://www.alanmacfarlane.com/ancestors/rijsbergen.htm Keith van Rijsbergen interviewed by Alan Macfarlane 15 July 2009 (film)]

{{Authority control}}

{{DEFAULTSORT:Rijsbergen, C. J. van}}
[[Category:1943 births]]
[[Category:Living people]]
[[Category:Dutch computer scientists]]
[[Category:Fellows of the Association for Computing Machinery]]
[[Category:People from Rotterdam]]
[[Category:University of Western Australia alumni]]
[[Category:Information retrieval researchers]]


{{Netherlands-scientist-stub}}
{{compu-scientist-stub}}</text>
      <sha1>7cwi9xv7deved6pq65739qdqud7kboy</sha1>
    </revision>
  </page>
  <page>
    <title>Suggested Upper Merged Ontology</title>
    <ns>0</ns>
    <id>247601</id>
    <revision>
      <id>725922084</id>
      <parentid>696941014</parentid>
      <timestamp>2016-06-18T19:11:36Z</timestamp>
      <contributor>
        <ip>68.65.169.236</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2048" xml:space="preserve">The '''Suggested Upper Merged Ontology''' or '''SUMO''' is an [[Upper ontology (information science)|upper ontology]] intended as a foundation [[ontology (computer science)|ontology]] for a variety of computer information processing systems. It was originally developed by the Teknowledge Corporation and now is maintained by [http://www.articulatesoftware.com Articulate Software]. SUMO is [[open source]].

SUMO originally concerned itself with meta-level concepts (general entities that do not belong to a specific problem domain), and thereby would lead naturally to a categorization scheme for encyclopedias.  It has now been considerably expanded to include a mid-level ontology and dozens of domain ontologies.

SUMO was first released in December 2000. It defines a hierarchy of ''SUMO classes'' and related rules and relationships. These are formulated in a version of the language [[SUO-KIF]] which has a [[LISP]]-like syntax. A [[Map (mathematics)|mapping]] from [[WordNet]] [[synsets]]  to SUMO has also been defined.  

SUMO is organized for interoperability of automated [[reasoning engine]]s. To maximize compatibility, [[logical schema|schema]] designers can try to assure that their [[naming convention]]s use the same meanings as SUMO for identical words (for example, "agent" or "process").  SUMO has an associated open source [[Sigma knowledge engineering environment]].

==See also==
* [[Semantic translation]]
* [[Upper ontology]]

== External links ==
* [http://www.ontologyportal.org/ Main page for SUMO]
* [http://suo.ieee.org/ Home page of the IEEE Standard Upper Ontology working group]
* The [http://sigmakee.sourceforge.net Sigma] reasoning system for SUMO
* [http://54.183.42.206:8080/sigma/Browse.jsp?kb=SUMO Online browser for SUMO]
* [http://www.adampease.org/professional/ Adam Pease, current Technical Editor of the standard]
[[Category:Java platform software]]
[[Category:Knowledge representation]]
[[Category:Ontology (information science)]]
[[Category:Open data]]
[[Category:Knowledge bases]]
{{Compu-AI-stub}}</text>
      <sha1>5q9qt6792ttbjcq6d8o5pex6kx1tbol</sha1>
    </revision>
  </page>
  <page>
    <title>Medical algorithm</title>
    <ns>0</ns>
    <id>1551981</id>
    <revision>
      <id>745167466</id>
      <parentid>685986851</parentid>
      <timestamp>2016-10-19T17:12:10Z</timestamp>
      <contributor>
        <username>Biogeographist</username>
        <id>18201938</id>
      </contributor>
      <comment>/* See also */ removed link that is already in article body, per [[WP:NOTSEEALSO]]; removed red link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5698" xml:space="preserve">{{Expand Russian|Медицинский алгоритм|date=September 2015}}
{{Original research|date=October 2007}}
[[File:Assessment and treatment algorithm for overweight and obesity.png|thumb|450px|A medical algorithm for assessment and treatment of [[overweight]] and [[obesity]].]]
A '''medical algorithm''' is any [[computation]], [[formula]], [[statistical survey]], [[nomogram]], or [[look-up table]], useful in [[healthcare]].  [[Medical]] [[algorithm]]s include [[decision tree]] approaches to healthcare treatment (e.g., if [[symptom]]s A, B, and C are evident, then use treatment X) and also less clear-cut tools aimed at reducing or defining uncertainty.

==Scope==
Medical algorithms are part of a broader field which is usually fit under the aims of [[medical informatics]] and medical [[decision-making]]. Medical decisions occur in several areas of medical activity including medical test selection, [[diagnosis]], therapy and [[prognosis]], and [[automatic control]] of [[medical equipment]].

In relation to [[logic]]-based and [[artificial neural network]]-based [[clinical decision support system]], which are also computer applications to the medical decision-making field, algorithms are less complex in architecture, data structure and user interface. Medical algorithms are not necessarily implemented using digital computers. In fact, many of them can be represented on paper, in the form of diagrams, nomographs, etc.

==Examples==
A wealth of medical information exists in the form of published medical algorithms.  These algorithms range from simple [[calculation]]s to complex outcome [[prediction]]s.  Most [[clinician]]s use only a small subset routinely.

Examples of medical algorithms are:
* '''[[Calculators]],'''. e.g., an on-line or stand-alone calculator for [[body mass index]] (BMI) when stature and body weight are given;
* '''[[Flowcharts]],''' e.g., a [[Wiktionary:binary|binary]] [[decision tree]] for deciding what is the [[etiology]] of [[chest pain]]
* '''[[Look-up table]]s,''' e.g., for looking up [[food energy]] and nutritional contents of foodstuffs
* '''[[Nomogram]]s,''' e.g., a moving circular slide to calculate body surface area or drug dosages.

A common class of algorithms are embedded in guidelines on the choice of treatments produced by many national, state, financial and local healthcare organisations and provided as knowledge resources for day to day use and for induction of new physicians. A field which has gained particular attention is the choice of medications for psychiatric conditions. In the United Kingdom, guidelines or algorithms for this have been produced by most of the circa 500 primary care trusts, substantially all of the circa 100 secondary care psychiatric units and many of the circa 10 000 general practices. In the US, there is a national (federal) initiative to provide them for all states, and by 2005 six states were adapting the approach of the [[Texas Medication Algorithm Project]] or otherwise working on their production.

A grammar—the [[Arden syntax]]—exists for describing algorithms in terms of [[medical logic module]]s. An approach such as this should allow exchange of MLMs between doctors and establishments, and enrichment of the common stock of tools.

==Purpose==
The intended purpose of medical algorithms is to improve and standardize decisions made in the delivery of [[medical care]]. Medical algorithms assist in standardizing selection and application of treatment regimens, with algorithm [[automation]] intended to reduce potential introduction of errors.  Some attempt to predict the outcome, for example [[ICU scoring systems|critical care scoring systems]].

Computerized health diagnostics algorithms can provide timely clinical decision support, improve adherence to [[evidence-based medicine|evidence-based]] [[guideline (medical)|guidelines]], and be a resource for education and research. 

Medical algorithms based on best practice can assist everyone involved in delivery of standardized treatment via a wide range of clinical care providers. Many are presented as [[Clinical trial protocol|protocol]]s and it is a key task in training to ensure people step outside the protocol when necessary.  In our present state of knowledge, generating hints and producing guidelines may be less satisfying to the authors, but more appropriate.

==Cautions==
In common with most science and medicine, algorithms whose contents are not wholly available for scrutiny and open to improvement should be regarded with suspicion.  

[[Computation]]s obtained from medical algorithms should be compared with, and tempered by, clinical knowledge and [[physician]] judgment.

==See also==
* [[Consensus (medical)]]
* [[Evidence-based medicine]]
* [[Journal club]]
* [[Medical guideline]]
* [[Medical informatics]]
* [[Odds algorithm]]
* ''[[Treatment Guidelines from The Medical Letter]]''

==Further reading==
* {{cite journal| title=Automated Medical Algorithms:  Issues for Medical Errors| first1=Kathy A.| last1=Johnson| first2=John R.| last2=Svirbely| first3=M.G.| last3=Sriram| first4=Jack W.| last4=Smith| first5=Gareth |last5=Kantor| first6=Jorge Raul |last6=Rodriguez| journal=[[Journal of the American Medical Informatics Association]]| pmc=419420| doi=10.1197/jamia.M1228| volume=9| issue=6 Suppl 1| pages=s56-s57| date=November 2002}}

==External links==

* [http://www.alternativementalhealth.com/articles/fieldmanual.htm AlternativeMentalHealth.com] - 'Alternative Health Medical Evaluation Field Manual', Lorrin M. Koran, MD, [[Stanford University]] Medical Center (1991)

[[Category:Health informatics]]
[[Category:Algorithms]]
[[Category:Knowledge representation]]</text>
      <sha1>ehmdshujxlbvuff3xtx6axtsh64lzfq</sha1>
    </revision>
  </page>
  <page>
    <title>Ontic</title>
    <ns>0</ns>
    <id>2788896</id>
    <revision>
      <id>704554614</id>
      <parentid>692489657</parentid>
      <timestamp>2016-02-12T05:05:29Z</timestamp>
      <contributor>
        <username>Philebritite</username>
        <id>9149149</id>
      </contributor>
      <minor />
      <comment>/* Usage in philosophy of critical realism */  the archive cited is inaccessible.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6532" xml:space="preserve">{{The Works of Aristotle}}
In [[philosophy]], '''ontic''' (from the [[Greek language|Greek]] {{lang|grc|ὄν}}, genitive {{lang|grc|ὄντος}}: "of that which is") is physical, real, or factual existence.

"Ontic" describes what is there, as opposed to the nature or properties of that being. To illustrate:

*[[Roger Bacon]], observing that all languages are built upon a common grammar, stated that they share a foundation of ontically anchored linguistic structures.
*[[Martin Heidegger]] posited the concept of ''Sorge'', or caring, as the fundamental concept of the [[intentionality|intentional being]], and presupposed an ontological significance that distinguishes [[ontology|ontological]] being from mere "thinghood" of an ontic being. He uses the [[German language|German]] word "[[Dasein]]" for a being that is capable of ontology, that is, [[recursivity|recursively]] comprehending [[property (philosophy)|properties]] of the very fact of its own Being. For Heidegger, "ontical" signifies concrete, specific realities, whereas "ontological" signifies deeper underlying structures of reality. Ontological objects or subjects have an ontical dimension, but they also include aspects of being like self-awareness, evolutionary vestiges, future potentialities, and networks of relationship.&lt;ref&gt;{{cite web|title=Ontico-Ontological Distinction|url=http://www.blackwellreference.com/public/tocnode?id=g9781405106795_chunk_g978140510679516_ss1-33|publisher=Blackwell Reference|accessdate=26 February 2015}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last1=Duffy|first1=Michael|title=The Ontological and the Ontic|url=http://mikejohnduff.blogspot.com/2007/08/ontological.html|accessdate=26 February 2015}}&lt;/ref&gt;
* [[Nicolai Hartmann]] distinguishes among ontology, ontics, and metaphysics: (i) ontology concerns the categorical analysis of entities by means of the knowledge categories able to classify them, (ii) ontics refers to a pre-categorical and pre-objectual connection which is best expressed in the relation to transcendent acts, and (iii) metaphysics is that part of ontics or that part of ontology which concerns the residue of being that cannot be rationalized further according to categories.

== Usage in philosophy of science ==
[[Harald Atmanspacher]] writes extensively about the philosophy of science, especially as it relates to [[Chaos theory]], [[determinism]], [[Causality|causation]], and [[stochastic process|stochasticity]]. He explains that "''ontic'' states describe all properties of a physical system exhaustively. ('Exhaustive' in this context means that an ''ontic'' state is 'precisely the way it is,' without any reference to [[epistemic]] knowledge or ignorance.)"{{ref|autonumber}}

In an earlier paper, Atmanspacher portrays the difference between an [[epistemic]] perspective of a [[system]], and an ontic perspective:

:Philosophical [[discourse]] traditionally distinguishes between [[ontology]] and [[epistemology]] and generally enforces this distinction by keeping the two subject areas separated. However, the relationship between the two areas is of central importance to [[physics]] and [[philosophy of physics]]. For instance, many [[measurement]]-related problems force us to consider both our [[knowledge]] of the [[Classical mechanics|states]] and [[observables]] of a [[system]] ([[epistemic perspective]]) and its states and observables, independent of such knowledge (ontic perspective). This applies to [[quantum|quantum systems]] in particular.{{ref|autonumber}}

== Usage in philosophy of critical realism ==
The [[United Kingdom|British]] [[philosopher]] [[Roy Bhaskar]], who is closely associated with the philosophical [[Cultural movement|movement]] of [[Critical realism (philosophy of the social sciences)|Critical Realism]] writes:
:"I differentiate the 'ontic' ('ontical' etc.) from the 'ontological'. I employ the former to refer to

:# whatever pertains to being generally, rather than some distinctively philosophical (or scientific) theory of it (ontology), so that in this sense, that of the '''ontic&lt;sub&gt;1&lt;/sub&gt;''', we can speak of the ontic presuppositions of a work of art, a [[joke]] or a strike as much as a [[epistemology|theory of knowledge]]; and, within this [[rubric]], to
:# the [[intransitive]] [[object (philosophy)|object]]s of some specific, [[historically determinate]], [[scientific investigation]] (or set of such investigations), the '''ontic&lt;sub&gt;2&lt;/sub&gt;'''.

:"The ontic&lt;sub&gt;2&lt;/sub&gt; is always specified, and only identified, by its relation, as the intransitive object(s) of some or other (denumerable set of) particular transitive process(es) of enquiry. It is cognitive process-, and level-specific; whereas the ontological (like the ontic&lt;sub&gt;1&lt;/sub&gt;) is not."{{ref|autonumber}}

[[Ruth Groff]] offers this expansion of Bhaskar's note above:
:"'ontic&lt;sub&gt;2&lt;/sub&gt;' is an abstract way of denoting the [[object-domain]] of a particular [[scientific]] area, field, or inquiry. E.g.: [[molecules]] feature in the ontic&lt;sub&gt;2&lt;/sub&gt; of chemistry. He's just saying that the scientific undertaking ITSELF is not one of the objects of said, most narrowly construed, immediate object-domain. So [[chemistry]] itself is not part of the ontic&lt;sub&gt;2&lt;/sub&gt; of chemistry."

==See also==
*[[Ding an sich#Noumenon and the thing-in-itself|Ding an sich]]
*[[Ontologism]]
*[[Physical ontology]]
*[[Substance theory]]

==References==
{{Reflist}}
# {{Note|autonumber}} Atmanspacher, Dr. H., and Primas, H., 2003 [2005], "Epistemic and Ontic [[Quantum]] [[Reality|Realities]]", in Khrennikov, A (Ed.), ''Foundations of Probability and Physics'' ([[American Institute of Physics]] 2005, pp 49&amp;ndash;61, Originally published in ''Time, Quantum and Information'', edited by Lutz Castell and Otfried Ischebeck, Springer, Berlin, 2003, pp 301&amp;ndash;321
# {{Note|autonumber}} Atmanspacher, Harald (2001) ''[[Determinism]] Is Ontic, Determinability is [[Epistemic]]'' ([http://philsci-archive.pitt.edu/archive/00000939/00/determ.pdf [[University of Pittsburgh]] Archives])
# {{Note|autonumber}} Bhaskar, R.A., 1986, ''Scientific Realism and Human Emancipation'' (London: Verso), pp 36 and 37, as quoted by [[Howard Engelskirchen]] in the [http://archives.econ.utah.edu/archives/bhaskar/2001m11/msg00015.htm Bhaskar mailing list archive]
{{Continental philosophy}}
{{wiktionary}}

[[Category:Concepts in metaphysics]]
[[Category:Knowledge representation]]
[[Category:Martin Heidegger]]
[[Category:Modal logic]]
[[Category:Ontology]]
[[Category:Philosophy of science]]
[[Category:Reality]]</text>
      <sha1>f69zw6ifb5drs5quo5r5n2xtksj32uc</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Classification systems</title>
    <ns>14</ns>
    <id>3615452</id>
    <revision>
      <id>756364608</id>
      <parentid>756364448</parentid>
      <timestamp>2016-12-23T18:50:51Z</timestamp>
      <contributor>
        <username>Allforrous</username>
        <id>12120664</id>
      </contributor>
      <comment>new key for [[Category:Classification]]: "systems" using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="491" xml:space="preserve">'''Classification systems''' are [[system]]s with a distribution of classes created according to common relations or affinities.
{{Commons cat|Classification systems}}
See also: 
* [[Controlled vocabulary]]
* [[Scientific classification (disambiguation)]]
* [[Taxonomy (biology)|Taxonomy]]

[[Category:Classification|systems]]
[[Category:Conceptual systems]]
[[Category:Formal sciences]]
[[Category:Knowledge representation]]
[[Category:Information science]]
[[Category:Information systems]]</text>
      <sha1>hwcwnbxb99w6bsq28uescubnjtwjx04</sha1>
    </revision>
  </page>
  <page>
    <title>John F. Sowa</title>
    <ns>0</ns>
    <id>102392</id>
    <revision>
      <id>761071592</id>
      <parentid>761071502</parentid>
      <timestamp>2017-01-20T18:31:57Z</timestamp>
      <contributor>
        <username>Jefferythomas</username>
        <id>13631841</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10745" xml:space="preserve">{{Infobox person
 | name             = John F. Sowa
 | image            =
 | image_size       = 
 | caption          = 
 | birth_name       = John Florian Sowa
 | birth_date       = {{Birth date and age|mf=yes|1940|1|1}}
 | birth_place      = 
 | death_date       = 
 | death_place      = 
 | death_cause      = 
 | resting_place    = 
 | residence        = [[Croton-on-Hudson, New York]]
 | nationality      = 
 | other_names      =
 | known_for        = [[Conceptual graph]]s
 | education        = [[Massachusetts Institute of Technology]] BS 1962, [[Harvard University]] MA 1966, [[Vrije Universiteit Brussel]] PhD 1999
 | alma_mater       = 
 | employer         = 
 | occupation       = Computer Scientist
 | boards           = 
 | religion         = 
 | spouse           = [[Cora Angier Sowa]]
 | children         = 
 | parents          = 
 | relations        =
 | callsign         = 
 | awards           = 
 | signature        =
 | website          = {{URL|http://www.jfsowa.com/|JFSowa.com}}
| 
}}
'''John Florian Sowa''' (born 1940) is an American [[computer scientist]], an expert in [[artificial intelligence]] and [[computer design]], and the inventor of [[conceptual graph]]s.&lt;ref&gt;[[Kecheng Liu]] (2000) ''Semiotics in Information Systems Engineering''. p.54 states: ''Conceptual graphs are devised as a language of knowledge representation by Sowa (1984), based on philosophy, psychology and linguistics. Knowledge in conceptual graph form is highly structured by modelling specialised facts that can be subjected to generalised reasoning.&lt;/ref&gt;&lt;ref&gt;Marite Kirikova (2002) ''Information Systems Development: Advances in Methodologies, Components, and Management''. p.194. states: ''The original theory of conceptual graphs was introduced by Sowa (Sowa, 1984 ). A conceptual graph is a finite, connected, bipartite graph. It includes notions of concepts, relations, and actors...''&lt;/ref&gt;

== Biography ==
Sowa received a BS in mathematics from [[Massachusetts Institute of Technology]] in 1962, an MA in applied mathematics from [[Harvard University]] in 1966, and a PhD in [[computer science]] from the [[Vrije Universiteit Brussel]] in 1999 on a dissertation titled "Knowledge Representation: Logical, Philosophical, and Computational Foundations".&lt;ref&gt;Andreas Tolk, Lakhmi C. Jain (2011) ''Intelligent-Based Systems Engineering''. p.xxi&lt;/ref&gt;

Sowa spent most of his professional career at [[International Business Machines|IBM]], which started in 1962 at IBM's applied mathematics group. Over the decades he has researched and developed emerging fields of [[computer science]] from compiler, programming languages, and system architecture&lt;ref name="SoZa92"&gt;John F. Sowa and [[John Zachman]] (1992). [http://www.research.ibm.com/journal/sj/313/sowa.pdf "Extending and Formalizing the Framework for Information Systems Architecture"] In: ''IBM Systems Journal'', Vol 31, no.3, 1992. p. 590-616.&lt;/ref&gt; to artificial intelligence and knowledge representation. In the 1990s Sowa was associated with IBM Educational Center in New York. Over the years he taught courses at the IBM Systems Research Institute, [[Binghamton University]], [[Stanford University]], [[Linguistic Society of America]] and [[Université du Québec à Montréal]]. He is a fellow of the [[Association for the Advancement of Artificial Intelligence]].

After early retirement at IBM Sowa in 2001 cofounded VivoMind Intelligence, Inc. with [[Arun K. Majumdar]]. With this company he was developing data-mining and database technology, more specific high-level "[[ontology|ontologies]]" for [[artificial intelligence]] and automated [[natural language understanding]]. Currently Sowa is working with [http://kyndi.com/ Kyndi Inc.], also founded by Majumdar. 

John Sowa is married to the philologist Cora Angier Sowa,&lt;ref&gt;Cora Angier Sowa (1984) ''Traditional themes and the Homeric hymns''. p.iv&lt;/ref&gt; and they live in [[Croton-on-Hudson, New York]].

== Work ==
Sowa's research interest since the 1970s were in the field of [[artificial intelligence]], [[expert systems]] and [[database query]] linked to natural languages.&lt;ref name="SoZa92"/&gt; In his work he combines ideas from numerous disciplines and eras modern and ancient, for example, applying ideas from [[Aristotle]], the medieval [[Scholastics]] to [[Alfred North Whitehead]] and including [[logical schema|database schema]] theory, and incorporating the model of analogy of Islamic scholar [[Ibn Taymiyyah]] in his works.&lt;ref&gt;[http://www.jfsowa.com/pubs/analog.htm Analogical Reasoning]&lt;/ref&gt;

=== Conceptual graph ===
{{main|Conceptual graph}}
Sowa invented conceptual graphs, a graphic notation for logic and natural language, based on the structures in [[semantic network]]s and on the [[existential graph]]s of [[Charles Sanders Peirce|Charles S. Peirce]]. He published the concept in the 1976 article "Conceptual graphs for a data base interface" in the ''IBM Journal of Research and Development''.&lt;ref&gt;{{cite journal |last=Sowa |authorlink = |first= John F. |date=July 1976 |title=Conceptual Graphs for a Data Base Interface |journal=IBM Journal of Research and Development |volume=20 |issue=4 |pages=336–357 |url=http://www.research.ibm.com/journal/rd/204/ibmrd2004E.pdf |ref=harv |doi=10.1147/rd.204.0336}}&lt;/ref&gt; He further explained in the 1983 book ''Conceptual structures: information processing in mind and machine''.

In the 1980s this theory has "been adopted by a number of research and development groups throughout the world.&lt;ref name="SoZa92"/&gt; International conferences on conceptual graphs have been held for over a decade since before 1992.{{citation needed|date=November 2012}}

==={{anchor|law of standards}}Sowa's law of standards===
In 1991, Sowa first stated his ''Law of Standards'': 
: "Whenever a major organization develops a new system as an official [[Technical standard|standard]] for X, the primary result is the widespread adoption of some simpler system as a [[de facto]] standard for X."&lt;ref&gt;[http://www.jfsowa.com/computer/standard.htm Law of Standards]&lt;/ref&gt; 
Like [[Gall's law]], The Law of Standards is essentially an argument in favour of underspecification. Examples include:

*The introduction of [[PL/I]] resulting in [[COBOL]] and [[FORTRAN]] becoming the de facto standards for scientific and business programming
*The introduction of [[Algol-68]] resulting in [[Pascal (programming language)|Pascal]] becoming the de facto standard for academic programming
*The introduction of the [[Ada (programming language)|Ada language]] resulting in [[C (programming language)|C]] becoming the de facto standard for [[United States Department of Defense|DoD]] programming
*The introduction of [[OS/2]] resulting in [[Microsoft Windows|Windows]] becoming the de facto standard for [[desktop OS]]
*The introduction of [[X.400]] resulting in [[SMTP]] becoming the de facto standard for [[electronic mail]]
*The introduction of [[X.500]] resulting in [[LDAP]] becoming the de facto standard for [[directory services]]

== Publications ==
* 1984. ''Conceptual Structures - Information Processing in Mind and Machine''. The Systems Programming Series, Addison-Wesley&lt;ref&gt;[http://conceptualstructures.org/ Conceptual Structures Home Page]. Retrieved Nov 23, 2012.&lt;/ref&gt;
* 1991. ''Principles of Semantic Networks''. Morgan Kaufmann.
* {{Cite journal| editor1-last = Mineau | editor1-first = Guy W| editor2-last = Moulin | editor2-first = Bernard| editor3-last = Sowa | editor3-first = John F | editor3-link = John F. Sowa| title = Conceptual Graphs for Knowledge Representation| doi = 10.1007/3-540-56979-0| series = [[Lecture Notes in Computer Science|LNCS]]| volume = 699| year = 1993| isbn = 978-3-540-56979-4}}
* 1994. ''International Conference on Conceptual Structures (2nd : 1994 : College Park, Md.)	Conceptual structures, current practices : Second International Conference on Conceptual Structures, ICCS'94, College Park, Maryland, USA, August 16–20, 1994 : proceedings''. William M. Tepfenhart, Judith P. Dick, John F. Sowa, eds.
*{{Cite journal| editor1-last = Ellis | editor1-first = Gerard| editor2-last = Levinson | editor2-first = Robert| editor3-last = Rich | editor3-first = William| editor4-last = Sowa | editor4-first = John F | editor4-link = John F. Sowa| doi = 10.1007/3-540-60161-9| title = Conceptual Structures: Applications, Implementation and Theory| series = [[Lecture Notes in Computer Science|LNCS]]| volume = 954| year = 1995| isbn = 978-3-540-60161-6}}
*{{Cite journal| editor1-last = Lukose | editor1-first = Dickson| editor2-last = Delugach | editor2-first = Harry| editor3-last = Keeler | editor3-first = Mary| editor4-last = Searle | editor4-first = Leroy| editor5-last = Sowa | editor5-first = John | editor5-link = John F. Sowa| doi = 10.1007/BFb0027865| title = Conceptual Structures: Fulfilling Peirce's Dream| series = [[Lecture Notes in Computer Science|LNCS]]| volume = 1257| year = 1997| isbn = 3-540-63308-1}}
* 2000. ''Knowledge representation : logical, philosophical, and computational foundations'', Brooks Cole Publishing Co., Pacific Grove&lt;ref&gt;[http://www.jfsowa.com/krbook/ Knowledge Representation: Logical, Philosophical, and Computational Foundations] at jfsowa.com. Retrieved Nov 23, 2012.&lt;/ref&gt;

;Articles, a selection&lt;ref&gt;{{DBLP|name=John F. Sowa}}&lt;/ref&gt;
*{{Cite journal| last1 = Sowa | first1 = J. F. | author1-link = John F. Sowa| title = Conceptual Graphs for a Data Base Interface| doi = 10.1147/rd.204.0336| journal = IBM Journal of Research and Development| volume = 20| issue = 4| pages = 336–357| date=July 1976 }}
*{{Cite journal| last1 = Sowa | first1 = J. F. | author1-link = John F. Sowa| last2 = Zachman | first2 = J. A.| doi = 10.1147/sj.313.0590| title = Extending and formalizing the framework for information systems architecture| journal = IBM Systems Journal| volume = 31| issue = 3| pages = 590–616| year = 1992}}
* 1992. "[http://www.jfsowa.com/cg/cgif.htm Conceptual Graph Summary]"; In: T.E. Nagle et. al. (Eds.). ''Conceptual Structures: Current Research and Practice''. Chichester: Ellis Horwood. 
* 1995. "Top-level ontological categories." in: ''International journal of human-computer studies''. Vol. 43, Iss. 5–6, Nov. 1995, pp.&amp;nbsp;669–685
* 2006. "Semantic Networks". In: ''Encyclopedia of Cognitive Science.''. John Wiley &amp; Sons.

== References ==
{{reflist}}

== External links ==
{{Wikiquote}}
* [http://www.jfsowa.com/ John F. Sowa] homepage

{{Authority control}}

{{DEFAULTSORT:Sowa, John}}
[[Category:1940 births]]
[[Category:Artificial intelligence researchers]]
[[Category:Knowledge representation]]
[[Category:Living people]]
[[Category:People from Croton-on-Hudson, New York]]
[[Category:Harvard University alumni]]
[[Category:Binghamton University faculty]]</text>
      <sha1>ol0j0ajcaaqkhj2bulku86g7qudqbim</sha1>
    </revision>
  </page>
  <page>
    <title>Findability</title>
    <ns>0</ns>
    <id>1025538</id>
    <revision>
      <id>761083092</id>
      <parentid>760704391</parentid>
      <timestamp>2017-01-20T19:51:03Z</timestamp>
      <contributor>
        <username>Riceissa</username>
        <id>20698937</id>
      </contributor>
      <comment>/* History */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11863" xml:space="preserve">'''Findability''' is a term for the ease with which information contained on a [[website]] can be found, both from outside the website (using [[search engine]]s and the like) and by users already on the website.&lt;ref&gt;{{Cite journal|url = |title = Information architecture|author1=Jacob, Elin K.  |author2=Loehrlein, Aaron|date = 2009|journal = Annual Review of Information Science and Technology|doi = 10.1002/aris.2009.1440430110|pmid = |access-date = |publication-date = }}&lt;/ref&gt; Although findability has relevance outside the [[World Wide Web]], the term is usually used in that context. Most relevant websites do not come up in the top results because designers and engineers do not cater to the way ranking algorithms work currently.&lt;ref&gt;{{Cite book|title=Ambient Findability|last=Morville|first=Peter|publisher=Oreilly|year=2005|isbn=978-0-596-00765-2|location=Sebastopol, CA|pages=|quote=|via=}}&lt;/ref&gt; Its importance can be determined from the first law of [[e-commerce]], which states "If the user can’t find the product, the user can’t buy the product."&lt;ref&gt;{{Cite web|url = http://www.nngroup.com/reports/ecommerce|title = E-Commerce user experience: High-level strategy, Nielsen Norman Group|date = 2001|accessdate = |website = |publisher = }}&lt;/ref&gt; As of December 2014, out of 10.3 billion monthly [[Google]] searches by Internet users in the [[United States]], an estimated 78% are made to research products and services online.&lt;ref&gt;{{Cite web|url = http://www.cmocouncil.org/facts-stats-categories.php?category=internet-marketing|title = Internet Marketing|date = |accessdate = |website = |publisher = }}&lt;/ref&gt;

Findability encompasses aspects of [[information architecture]], [[user interface design]], [[accessibility]] and [[search engine optimization]] (SEO), among others.

==Introduction==
Findability is similar to, but different from [[discoverability]], which is defined as the ability of something, especially a piece of content or information, to be found. It is different from web search in that the word 'find' refers to locating something in a known space while 'search' is in an unknown space or not in an expected location.&lt;ref name="every-page" /&gt;

Mark Baker, the author of "Every Page is Page One",&lt;ref name="every-page"&gt;{{Cite book|title = Every Page is Page One|last = Baker|first = Mark|publisher = XML Press|year = 2013|isbn = 978-1937434281|location = |pages = }}&lt;/ref&gt; mentions that findability "is a content problem, not a search problem".&lt;ref&gt;{{cite web|last1=Baker|first1=Mark|title=Findability is a Content Problem, not a Search Problem|url=http://everypageispageone.com/2013/05/28/findability-is-a-content-problem-not-a-search-problem/|website=Every Page is Page One|accessdate=2015-04-25}}&lt;/ref&gt; Even when the right content is present, users often find themselves deep within the content of a website but not in the right place. He further adds that findability is intractable, perfect findability is unattainable, but we need to focus on reducing the effort for finding that a user would have to do for themselves.

Findability can be divided into external findability and on-site findability, based on where the customers need to find the information.

==History==
[[Heather Lutze]] is thought to have created the term in the early 2000s.&lt;ref&gt;{{cite web | url=http://www.huffingtonpost.com/liz-wainger/the-shtickiness-factor_b_3471675.html | title=The Shtickiness Factor |last1=Wainger | first1=Liz | publisher=The Huffington Post | date=20 June 2013 | accessdate=12 September 2013}}&lt;/ref&gt; The popularization of the term "findability" for the Web is usually credited to [[Peter Morville]].{{citation needed|date=April 2015}} In 2005 he defined it as: "the ability of users to identify an appropriate Web site and navigate the pages of the site to discover and retrieve relevant information resources", though it appears to have been first coined in a public context referring to the web and information retrieval by Alkis Papadopoullos in a 2005 article entitled "Findability".&lt;ref&gt;{{cite journal|author=Alkis Papadopoulos|title=The Key to Enterprise Search|journal=KM World|date=April 1, 2005|url=http://news-business.vlex.com/vid/findability-key-to-enterprise-search-62406335}}&lt;/ref&gt;&lt;ref&gt;Though the word has been used to mean "ease of finding information" since at least 1943: see Urban A. Avery, "The 'Findability' of the Law", ''Chicago Bar Record'' '''24''':272, April 1943, reprinted in the ''Journal of the American Judicature Society'' '''27''':25 [http://heinonline.org/HOL/LandingPage?collection=journals&amp;handle=hein.journals/judica27&amp;div=12&amp;id=&amp;page=]&lt;/ref&gt;

==External findability==
External findability is the domain of [[internet marketing]] and [[Search engine optimization|search engine optimization (SEO)]] tactics. Several factors affect external findability:&lt;ref&gt;{{cite web|title=Findability Factors Found|url=http://www.econtentstrategies.com/Article_FindabilityFactorsFoundFinal_EContent_200701.pdf}}&lt;/ref&gt;
#''Search Engine Indexing'': As the very first step, webpages need to be found by indexing crawler in order to be shown in the search results. It would be helpful to avoid factors that may lead to webpages being ignored by indexing crawlers. Those factors may include elements that require user interaction, such as entering log-in credentials. Algorithms for indexing vary by the search engine which means the number of webpages of a website successfully being indexed may be very different between Google and Yahoo!'s search engines. Also, in countries like [[China]], [[Great Firewall|government policies]] could significantly influence the indexing algorithms. In this case, local knowledge about laws and policies could be valuable.&lt;ref&gt;{{cite web|title=Online Marketing in China|url=http://chineseseoshifu.com/china-online-marketing/}}&lt;/ref&gt;
#''Page Descriptions in Search Results'': Now that the webpages are successfully indexed by web crawlers and show in the search results with decent ranking, the next step is to attract customers to click the link to the web pages. However, the customers can't see the whole web pages at this point; they can only see an excerpt of the webpage's content and metadata. Therefore, displaying meaningful information in a limited space, usually a couple of sentences, in search results is important for increasing click traffic of the webpages, and thus the findability of the web content on your webpages. 
#''Keyword Matching'': At a semantic level, terminology used by the searcher and the content producer be different. Bridging the gap between the terms used by customers and developers is helpful for making web content more findable to more potential content consumers.

==On-site findability==
On-site findability is concerned with the ability of a potential customer to find what they are looking for within a specific site. More than 90 percent of customers use internal searches in a website compared to browsing. Of those, only 50 percent find what they are looking for.&lt;ref name="findability-solution"&gt;{{cite web|title=The Findability Solution|url=http://marriottschool.byu.edu/strategy/docs/TheFindabilitySolution-StrategyWhitePaper.pdf}}&lt;/ref&gt; Improving the quality of on-site searches highly improves the business of the website. Several factors affect findability on a website:

#''Site search'': If searchers within a site do not find what they are looking for, they tend to leave rather than browse through the website. Users who had successful site searches are twice as likely to ultimately convert.&lt;ref name="findability-solution" /&gt;
#''Related Links and Products'': User experience can be enhanced by trying to understand the needs of the customer and provide suggestions for other, related information.
#''Site Match to Customer Needs and Preferences'': Site design, content creation, and recommendations are major factors for affecting the customer experience.
#''Cross Device Experience'': With the rise of computing devices other than desktop computers, companies like Microsoft have focused more on smoothing the transition between devices to increase customer satisfaction.&lt;ref&gt;{{Cite web|url = http://research.microsoft.com/en-us/projects/courier/|title = Cross-Device User Experiences|date = |accessdate = |website = |publisher = }}&lt;/ref&gt;

==Evaluation and measures==
'''Baseline Findability''' is the existing findability before changes are made in order to improve it. This is measured by participants who represent the customer base of the website, who try to locate a sample set of items using the existing navigation of the website.&lt;ref&gt;{{Cite book|title = Customer Analytics For Dummies|last = Sauro|first = Jeff|publisher = John Wiley &amp; Sons|year = |isbn = 978-1-118-93759-4|location = |pages = |url = http://www.wiley.com/WileyCDA/WileyTitle/productCd-1118937597.html}}&lt;/ref&gt;&lt;ref&gt;{{Cite web|url = http://www.measuringu.com/blog/measure-findability.php|title = How to Measure Findability|date = |accessdate = |website = |publisher = }}&lt;/ref&gt;

In order to evaluate how easily information can be found by searching a site using a search engine or information retrieval system, [[retrievability]] measures were developed, and similarly, navigability measures now measure ease of information access through browsing a site (e.g. [[PageRank]], MNav, InfoScent (see [[Information foraging|Information Foraging]]), etc.).

Findability also can be evaluated via the following techniques:
* [[Usability testing]]: Conducted to find out how and why users navigate through a website to accomplish tasks.
* [[Tree testing]]: An [[information architecture]] based technique, to determine if critical information can be found on the website.
* [[Card sorting|Closed card sorting]]: A usability technique based on information architecture, for evaluating the strength of categories.
* [[Click testing]]: Accounts for the implicit data collected through clicks on the user interface.&lt;ref&gt;{{Cite web|url = http://www.nngroup.com/articles/navigation-ia-tests/|title = Low Findability and Discoverability: Four Testing Methods to Identify the Causes|date = July 6, 2014|accessdate = |website = |publisher = }}&lt;/ref&gt;

==Beyond findability==
Findability Sciences defines a findability index in terms of each user's influence, context, and sentiments. For seamless search, current websites focus on a combination of structured hypertext-based information architectures and rich Internet application-enabled visualization techniques.&lt;ref&gt;{{Cite journal|url = http://journalofia.org/volume2/issue1/03-spagnolo/|title = Beyond Findability - Search-Enhanced Information Architecture for Content-Intensive Rich Internet Applications|date = 2010|journal = |doi = |pmid = |access-date = }}&lt;/ref&gt;

==See also==
* [[Information retrieval]]
* [[Knowledge mining]]
* [[Search engine optimization]]
* [[Subject (documents)]]
* [[Usability]]
* [[User interface]]

==References==
{{reflist}}

==Further reading==
* Morville, P. (2005) Ambient findability. Sebastopol, CA: O'Reilly
* Wurman, R.S. (1996). Information architects. New York: Graphis.

==External links==
* [http://findability.org/ findability.org]: a collection of links to people, software, organizations, and content related to findability
* [http://semanticstudios.com/publications/semantics/000007.php The age of findability] (article)
* [http://www.useit.com/alertbox/search-keywords.html Use Old Words When Writing for Findability] (article on the findability impact of a site's choice of words)
* [http://buildingfindablewebsites.com/ Building Findable Websites: Web Standards SEO and Beyond] (book)
* [http://www.FindabilityFormula.com The Findability Formula: The Easy, Non-Technical Guide to Search Engine Marketing by Heather Lutze]

[[Category:Web design]]
[[Category:Knowledge representation]]
[[Category:Information science]]
[[Category:Information architecture]]</text>
      <sha1>0wgm46rzdgvvvepnd9tdr6geo4h9fid</sha1>
    </revision>
  </page>
  <page>
    <title>Semantic interoperability</title>
    <ns>0</ns>
    <id>7233280</id>
    <revision>
      <id>746132961</id>
      <parentid>739686551</parentid>
      <timestamp>2016-10-25T12:51:59Z</timestamp>
      <contributor>
        <username>Padawan ch</username>
        <id>2036094</id>
      </contributor>
      <minor />
      <comment>Add link to "Syntax"</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="16024" xml:space="preserve">{{multiple issues|
{{underlinked|date=January 2013}}
{{more footnotes|date=February 2011}}
}}

'''Semantic interoperability''' is the ability of [[computer]] systems to exchange [[data]] with unambiguous, shared meaning. [[Semantic]] interoperability is a requirement to enable machine computable logic, inferencing, knowledge discovery, and data federation between information systems.&lt;ref&gt;NCOIC, [https://www.ncoic.org/technology/deliverables/scope/ "SCOPE"], [https://www.ncoic.org/home ''Network Centric Operations Industry Consortium''], 2008&lt;/ref&gt;

Semantic interoperability is therefore concerned not just with the packaging of data ([[syntax]]), but the simultaneous transmission of the meaning with the data (semantics).  This is accomplished by adding data about the data (metadata), linking each data element to a controlled, shared vocabulary.  The meaning of the data is transmitted with the data itself, in one self-describing "information package" that is independent of any information system.  It is this shared vocabulary, and its associated links to an ontology, which provides the foundation and capability of machine interpretation, inferencing, and logic.

Syntactic interoperability is a prerequisite for semantic interoperability.  Syntactic interoperability refers to the packaging and transmission mechanisms for data.  In healthcare, HL7 has been in use for over thirty years (which predates the internet and web technology), and uses the pipe character (|) as a data delimiter. The current internet standard for document markup is XML, which uses "&lt; &gt;" as a data delimiter.  The data delimiters convey no meaning to the data other than to structure the data.  Without a data dictionary to translate the contents of the delimiters, the data remains meaningless.  While there are many attempts at creating data dictionaries and information models to associate with these data packaging mechanisms, none have been practical to implement.  This has only perpetuated the ongoing "babelization" of data and inability to exchange of data with meaning.

Since the introduction of the Semantic Web concept by [[Tim Berners-Lee]] in 1999,&lt;ref&gt;{{cite book |last=Berners-Lee |first=Tim |authorlink=Tim Berners-Lee |author2=Fischetti, Mark |title=[[Tim Berners Lee#Weaving the Web|Weaving the Web]] |publisher=[[HarperSanFrancisco]] |year=1999 |pages=chapter 12 |isbn=978-0-06-251587-2 |nopp=true }}&lt;/ref&gt; there has been growing interest and application of the W3C (World Wide Web Consortium, [[WWWC]]) standards to provide web-scale semantic data exchange, federation, and inferencing capabilities.

== Semantic as a function of syntactic interoperability ==

Syntactic interoperability, provided by for instance [[XML]] or the [[SQL]] standards, is a pre-requisite to semantic.  It involves a common data format and common protocol to structure any data so that the manner of processing the information will be interpretable from the structure.  It also allows detection of syntactic errors, thus allowing receiving systems to request resending of any message that appears to be garbled or incomplete.  No semantic communication is possible if the syntax is garbled or unable to represent the data.  However, information represented in one syntax may in some cases be accurately translated into a different syntax.  Where accurate translation of syntaxes is possible, systems using different syntaxes may also interoperate accurately.  In some cases the ability to accurately translate information among systems using different syntaxes may be limited to one direction, when the formalisms used have different levels of ''expressivity'' (ability to express information).

A single ontology containing representations of every term used in every application is generally considered impossible, because of the rapid creation of new terms or assignments of new meanings to old terms.  However, though it is impossible to anticipate ''every'' concept that a user may wish to represent in a computer, there is the possibility of finding some finite set of "primitive" concept representations that can be combined to create any of the more specific concepts that users may need for any given set of applications or ontologies.  Having a foundation ontology (also called ''[[upper ontology]]'') that contains all those primitive elements would provide a sound basis for general semantic interoperability, and allow users to define any new terms they need by using the basic inventory of ontology elements, and still have those newly defined terms properly interpreted by any other computer system that can interpret the basic foundation ontology.  Whether the number of such primitive concept representations is in fact finite, or will expand indefinitely, is a question under active investigation.  If it is finite, then a stable foundation ontology suitable to support accurate and general semantic interoperability can evolve after some initial foundation ontology has been tested and used by a wide variety of users.  At the present time, no foundation ontology has been adopted by a wide community, so such a stable foundation ontology is still in the future.

== Words and Meanings ==

One persistent misunderstanding recurs in discussion of semantics - the confusion of words and meanings.  The meanings of words change, sometimes rapidly. But a formal language such as used in an ontology can encode the meanings (semantics) of concepts in a form that does not change.  In order to determine what is the meaning of a particular word (or term in a database, for example) it is necessary to label each fixed concept representation in an ontology with the word(s) or term(s) that may refer to that concept.  When multiple words refer to the same (fixed) concept, in language this is called synonymy; when one word is used to refer to more than one concept, that is called ambiguity.  Ambiguity and synonymy are among the factors that make computer understanding of language very difficult.  The use of words to refer to concepts (the meanings of the words used)is very sensitive to the context and the purpose of any use for many human-readable terms.  The use of ontologies in supporting semantic interoperability is to provide a fixed set of concepts whose meanings and relations are stable and can be agreed to by users.  The task of determining which terms in which contexts (each database is a different context) then is separated from the task of creating the ontology, and must be taken up by the designer of a database, or the designer of a form for data entry, or the developer of a program for language understanding.  When a word used in some interoperability context changes its meaning, then to preserve interoperability it is necessary to change the pointer to the ontology element(s) that specifies the meaning of that word.

== Knowledge representation requirements and languages ==

A knowledge representation language may be sufficiently expressive to describe nuances of meaning in well understood fields.  There are at least five levels of complexity of these{{specify|date=June 2014}}.

For general [[semi-structured data]] one may use a general purpose language such as XML.&lt;ref&gt;[http://www.cs.umd.edu/projects/plus/SHOE/pubs/extreme2000.pdf XML as a tool for Semantic Interoperability] Semantic Interoperability on the Web, Jeff Heflin and James Hendler&lt;/ref&gt;

Languages with the full power of first-order predicate logic may be required for many tasks.

Human languages are highly expressive, but are considered too ambiguous to allow the accurate interpretation desired, given the current level of human language technology.   In human languages the same word may be used to refer to different concepts (ambiguity), and the same concept may be referred to by different words (synonymy).

== Prior agreement not required ==
{{confusing|section|date=February 2016}}

Semantic interoperability may be distinguished from other forms of interoperability by considering whether the information transferred has, in its communicated form, all of the meaning required for the receiving system to interpret it correctly, even when the algorithms used by the receiving system are unknown to the sending system.  Consider sending one number:

If that number is intended to be the sum of money owed by one company to another, it implies some action or lack of action on the part of both those who send it and those who receive it.

It may be correctly interpreted if sent in response to a specific request, and received at the time and in the form expected.  This correct interpretation does not depend only on the number itself, which could represent almost any of millions of types of quantitative measure, rather it depends strictly on the circumstances of transmission.  That is, the interpretation depends on both systems expecting that the algorithms in the other system use the number in exactly the same sense, and it depends further on the entire envelope of transmissions that preceded the actual transmission of the bare number.  By contrast, if the transmitting system does not know how the information will be used by other systems, it is necessary to have a shared agreement on how information with some specific meaning (out of many possible meanings) will appear in a communication.  For a particular task, one solution is to standardize a form, such as a request for payment; that request would have to encode, in standardized fashion, all of the information needed to evaluate it, such as: the agent owing the money, the agent owed the money, the nature of the action giving rise to the debt, the agents, goods, services, and other participants in that action; the time of the action; the amount owed and currency in which the debt is reckoned; the time allowed for payment; the form of payment demanded; and other information.  When two or more systems have agreed on how to interpret the information in such a request, they can achieve semantic interoperability ''for that specific type of transaction''.  For semantic interoperability generally, it is necessary to provide standardized ways to describe the meanings of many more things than just commercial transactions, and the number of concepts whose representation needs to be agreed upon are at a minimum several thousand.

== Ontology research ==

How to achieve semantic interoperability for more than a few restricted scenarios is currently a matter of research and discussion.  For the problem of General Semantic Interoperability, some form of foundation ontology ('[[upper ontology]]') is required that is sufficiently comprehensive to provide the defining concepts for more specialized ontologies in multiple domains.  Over the past decade more than ten foundation ontologies have been developed, but none have as yet been adopted by a wide user base.

The need for a single comprehensive all-inclusive ontology to support Semantic Interoperability can be avoided by designing the common foundation ontology as a set of basic ("primitive") concepts that can be combined to create the logical descriptions of the meanings of terms used in local domain ontologies or local databases.  This tactic is based on the principle that:

'''If:'''
&lt;pre style="white-space:pre-wrap;"&gt;
(1) the meanings and usage of the primitive ontology elements in the foundation ontology are agreed on, and 
(2) the ontology elements in the  domain ontologies are constructed as logical
combinations of the elements in the foundation ontology,
&lt;/pre&gt;
'''Then:'''
&lt;pre style="white-space:pre-wrap;"&gt;
The intended meanings of the domain ontology elements can be computed automatically using an FOL reasoner, by any system that accepts the meanings of the elements in the foundation ontology, and has both the foundation ontology and the logical specifications of the elements in the domain ontology.
&lt;/pre&gt;
'''Therefore:'''
&lt;pre style="white-space:pre-wrap;"&gt;
Any system wishing to interoperate accurately with another system need transmit only the data to be communicated, plus any logical descriptions of terms used in that data that were created locally and are not already in the common foundation ontology.
&lt;/pre&gt;

This tactic then limits the need for prior agreement on meanings to only those ontology elements in the common Foundation Ontology (FO).  Based on several considerations, this is likely to be fewer than 10,000 elements (types and relations).

In practice, together with the FO focused on representations of the primitive concepts, a set of domain extension ontologies to the FO with elements specified using the FO elements will likely also be used.  Such pre-existing extensions will ease the cost of creating domain ontologies by providing existing elements with the intended meaning, and will reduce the chance of error by using elements that have already been tested.  Domain extension ontologies may be logically inconsistent with each other, and that needs to be determined if different domain extensions are used in any communication.

Whether use of such a single foundation ontology can itself be avoided by sophisticated mapping techniques among independently developed ontologies is also under investigation.

== Importance==

The practical significance of semantic interoperability has been measured by several studies that estimate the cost (in lost efficiency) due to lack of semantic interoperability.  One study,&lt;ref&gt;[http://content.healthaffairs.org/cgi/content/full/hlthaff.w5.10/DC1 Jan Walker, Eric Pan, Douglas Johnston, Julia Adler-Milstein, David W. Bates and Blackford Middleton, ''The Value of Healthcare Information Exchange and Interoperability'' Health Affairs, 19 January 2005]&lt;/ref&gt; focusing on the lost efficiency in the communication of healthcare information, estimated that US$77.8 billion per year could be saved by implementing an effective interoperability standard in that area.  Other studies, of the construction industry&lt;ref&gt;[http://www.bfrl.nist.gov/oae/publications/gcrs/04867.pdf Microsoft Word - 08657 Final Rpt_8-2-04.doc&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; and of the automobile manufacturing supply chain,&lt;ref&gt;http://www.nist.gov/director/prog-ofc/report99-1.pdf&lt;/ref&gt; estimate costs of over US$10 billion per year due to lack of semantic interoperability in those industries.  In total these numbers can be extrapolated to indicate that well over US$100 billion per year is lost because of the lack of a widely used semantic interoperability standard in the US alone.

There has not yet been a study about each policy field that might offer big cost savings applying semantic interoperability standards. But to see which policy fields are capable of profiting from semantic interoperability see '[[Interoperability]]' in general. Such policy fields are [[eGovernment]], health, security and many more. The EU also set up the [[Semantic Interoperability Centre Europe]] in June 2007.

==See also==
*[[Interoperability]], Interoperability generally
*[[Semantic Computing]]
*[[Upper ontology (computer science)]], Discussion of using an ''upper ontology''.
*[[Conceptual interoperability|Levels of Conceptual Interoperability]], A discussion describing an interoperability spectrum in the context of exchange of Modeling and Simulation information, in which ''semantic interoperability '' is not defined as fully independent of context, as described here.
*[[UDEF]], Universal Data Element Framework

==External links==
*[http://colab.cim3.net/cgi-bin/wiki.pl?OntologyTaxonomyCoordinatingWG/OntacGlossary the ONTACWG Glossary Other definitions of Semantic Interoperability]
*[http://marinemetadata.org/guides/vocabs/cvchooseimplement/cvsemint MMI Guide: Achieving Semantic Interoperability]

==References==
{{reflist|2}}

[[Category:Knowledge representation]]
[[Category:Technical communication]]
[[Category:Information science]]
[[Category:Ontology (information science)]]
[[Category:Computing terminology]]
[[Category:Telecommunication theory]]
[[Category:Interoperability]]</text>
      <sha1>jucsfxeolzoubx60bjddy4izlrttf7u</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Multi-agent systems</title>
    <ns>14</ns>
    <id>8050180</id>
    <revision>
      <id>547920225</id>
      <parentid>432040950</parentid>
      <timestamp>2013-03-31T02:46:51Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor />
      <comment>[[User:Addbot|Bot:]] Migrating 2 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q8645694]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="424" xml:space="preserve">This [[Wikipedia:category|category]] is about [[multi-agent system]]s, systems composed of several [[software agent]]s. 


[[Category:Computing platforms]]
[[Category:Artificial intelligence]]
[[Category:Knowledge representation]]
[[Category:Distributed computing architecture]]

== See also ==
*[[Cognitive architecture]]
*[[Intelligent agent]]
*[[Autonomous agent]]
*[[Internet bot]]
*[[Daemon (computer software)|Daemon]]</text>
      <sha1>hh9nxow18wd3uubje4ja7v5hzzgrre0</sha1>
    </revision>
  </page>
  <page>
    <title>F-logic</title>
    <ns>0</ns>
    <id>4880312</id>
    <revision>
      <id>746560901</id>
      <parentid>726999355</parentid>
      <timestamp>2016-10-28T04:43:12Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* top */http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5759" xml:space="preserve">'''F-logic''' ([[frame (data structure)|frame]] [[Logic programming|logic]]) is a [[knowledge representation]] and [[ontology language]]. F-logic combines the advantages of conceptual modeling with object-oriented, frame-based languages and offers a declarative, compact and simple syntax, as well as the well-defined semantics of a logic-based language. 
Features include, among others, object identity, complex objects, [[inheritance (computer science)|inheritance]], [[polymorphism (computer science)|polymorphism]], query methods, [[encapsulation (computer science)|encapsulation]]. F-logic stands in the same relationship to [[object-oriented programming]] as classical [[predicate calculus]] stands to [[relational database]] programming.

F-logic was developed by [[Michael Kifer]] at [[Stony Brook University]] and [[Georg Lausen]] at the [[University of Mannheim]]. F-logic was originally developed for deductive databases, but is now most frequently used for semantic technologies, especially the [[Semantic Web]]. F-logic is considered as one of the formalisms for [[Ontology (information science)|ontologies]], but [[description logic]] (DL) is more popular and accepted, as is the DL-based [[Web Ontology Language|OWL]].

A development environment for F-logic was developed in the NeOn project and is also used in a range of applications for information integration, [[question answering]] and [[semantic search]]. Prior to the version 4 of Protégé ontology editor, F-Logic is supported as one of the two kinds of ontology.

The frame syntax of the [[Rule Interchange Format]] Basic Logic Dialect (RIF BLD) standardized by the [[World Wide Web Consortium]] is based on F-logic; RIF BLD however does not include [[non-monotonic reasoning]] features of F-logic.&lt;ref name="Krötzsch2010"&gt;{{cite book|author=M. Krötzsch|title=Description Logic Rules|url=https://books.google.com/books?id=Z8h7AgAAQBAJ&amp;pg=PA10|year=October 2010|publisher=IOS Press|isbn=978-1-61499-342-1|page=10}}&lt;/ref&gt;

In contrast to [[description logic]] based ontology formalism the semantics of F-logic are normally that of a [[closed world assumption]] as opposed to DL's [[open world assumption]]. Also, F-logic is generally [[Undecidable problem|undecidable]]{{Citation needed|date=May 2014}}, whereas 
the [[SHOIN|SHOIN description logic]] that [[Web Ontology Language|OWL DL]] is based on is decidable. However it is possible to represent more expressive statements in F-logic than are possible with description logics.

The most comprehensive description of F-logic appears in.&lt;ref&gt;M. Kifer, G. Lausen, J. Wu (1995). ''Foundations of Object-Oriented and Frame-Based Languages]'', Journal of ACM, May 1995. [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.115.3586 PDF]&lt;/ref&gt; The preliminary paper &lt;ref&gt;M. Kifer and G. Lausen (1989). ''F-logic: a higher-order language for reasoning about objects, inheritance, and scheme'', ACM SIGMOD Conference, 1989. [http://dl.acm.org/citation.cfm?id=66939 PDF]&lt;/ref&gt; &lt;ref&gt;M. Kifer and G. Lausen (1997). ''F-logic: a higher-order language for reasoning about objects, inheritance, and scheme'', re-issued 1997. [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.48.7149 PDF]&lt;/ref&gt; has won the 1999 [http://www.sigmod.org/sigmod-awards/sigmod-awards#time Test of Time Award] from [[SIGMOD|ACM SIGMOD]]. A follow-up paper &lt;ref&gt;M. Kifer, W. Kim, Y. Sagiv (1992). ''Querying object-oriented databases'', ACM SIGMOD Conference, 1992. [http://dl.acm.org/citation.cfm?doid=130283.130342 PDF]&lt;/ref&gt; has won the 2002 [http://www.sigmod.org/sigmod-awards/sigmod-awards#time Test of Time Award] from [[SIGMOD|ACM SIGMOD]].

== F-logic syntax ==

Classes and individuals may be defined in F-logic as follows
 man::person.
 woman::person.
 brad:man.
 angelina:woman.
This states, that "men and women are persons" and that "Brad is a man", and "Angelina is a woman".

Statements about classes and individuals may be made as follows
 person[hasSon=&amp;gt;man].
 brad[hasSon-&amp;gt;&amp;gt;{maddox,pax}].
 married(brad,angelina).
This defines that "the son of a person is a man", "Maddox and Pax are the sons of Brad" and "Brad and Angelina are married". Note that &lt;code&gt;-&amp;gt;&amp;gt;&lt;/code&gt; is used for sets of values.

In addition it is possible to represent axioms in F-logic in the following manner
 man(X) &amp;lt;- person(X) AND NOT woman(X).
 FORALL X, Y &amp;lt;- X:person[hasFather-&gt;Y] &amp;lt;- Y:man[hasSon -&gt; X].
These mean "X is a man if X is a person but not a woman" and "if X is the son of Y then Y is the father of X".

The [[Flora-2]] system introduced a number of changes to the syntax of F-logic, making it more suitable for a knowledge representation and reasoning system as opposed to just a theoretical logic. In particular, variables became prefixed with a ?-mark, the distinction between functional and multi-valued properties was dropped and replaced by cardinality constraints, plus other important changes.

==F-logic based Languages==
* [[Flora-2]] is an extension of F-logic with [[HiLog]], [[Transaction logic]], and [[defeasible reasoning]].
* [http://pathlp.sourceforge.net/ PathLP] is a full logic programming language based on F-logic.
* [http://dbis.informatik.uni-freiburg.de/index.php?project=Florid FLORID] is a C++ — based implementation
* [http://www.wsmo.org/wsml/ Web Services Modeling Language (WSML)]
* [http://www.daml.org/services/swsl/ Semantic Web Services Language (SWSL)]
* [[ObjectLogic]] language is based on F-logic; [[OntoStudio]] is an ObjectLogic implementation by [[semafora systems GmbH]] (former [[Ontoprise GmbH]]).

== References ==
{{reflist}}

[[Category:Knowledge representation]]
[[Category:Semantic Web]]
[[Category:Logic programming languages]]
[[Category:Declarative programming languages]]</text>
      <sha1>p58gr8g2ipliwyz5gef3w6pukyrpc75</sha1>
    </revision>
  </page>
  <page>
    <title>Ramification problem</title>
    <ns>0</ns>
    <id>496055</id>
    <revision>
      <id>657214475</id>
      <parentid>522771267</parentid>
      <timestamp>2015-04-19T18:13:25Z</timestamp>
      <contributor>
        <username>Valoem</username>
        <id>1024002</id>
      </contributor>
      <comment>corrected link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1606" xml:space="preserve">{{Cleanup|date=April 2011}}

In [[philosophy]] and [[artificial intelligence]] (especially, knowledge based systems), the '''ramification problem''' is concerned with the indirect consequences of an action. It might also be posed as ''how to represent what happens implicitly due to an action'' or how to control the secondary and tertiary effects of an action. It is strongly connected to, and is opposite the [[qualification problem|qualification side]] of, the [[frame problem]].

Limit theory helps in [[operational]] usage. For instance, in [[Knowledge-based engineering|KBE]] derivation of a populated design (geometrical objects, etc., similar concerns apply in shape theory), equivalence assumptions allow convergence where potentially large, and perhaps even computationally indeterminate, solution sets are handled deftly. Yet, in a chain of computation, downstream events may very well find some types of results from earlier resolutions of '''ramification''' as problematic for their own algorithms.

==See also==
*[[Non-monotonic logic]]
*[[Ramification (mathematics)]]

==External links==
*Nikos Papadakis [http://csdl2.computer.org/persagen/DLAbsToc.jsp?resourcePath=/dl/proceedings/&amp;toc=comp/proceedings/ictai/2002/1849/00/1849toc.xml&amp;DOI=10.1109/TAI.2002.1180791 "Actions with Duration and Constraints: the Ramification Problem in Temporal Databases"] IEEE ICTAI'02
*Deepak Kumar "[http://blackcat.brynmawr.edu/~dkumar/UGAI/planning.html AI Planning]" Bryn Mawr College

[[Category:Logic programming]]
[[Category:Knowledge representation]]
[[Category:Epistemology]]


{{epistemology-stub}}</text>
      <sha1>5nci4nmwsrbnvkw7vke0adosv2fi29o</sha1>
    </revision>
  </page>
  <page>
    <title>Futures wheel</title>
    <ns>0</ns>
    <id>8612764</id>
    <revision>
      <id>738482537</id>
      <parentid>705943857</parentid>
      <timestamp>2016-09-09T06:09:27Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor />
      <comment>[[User:Green Cardamom/WaybackMedic 2|WaybackMedic 2]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2866" xml:space="preserve">[[Image:Futures wheel 01.svg|thumb|right|250px|A futures wheel as described by Jerome C. Glenn.]]
'''The Futures wheel''' is a method for graphical [[visualization (graphic)|visualisation]] of direct and indirect [[future]] '''consequences''' of a particular change or development. It was invented by [[Jerome C. Glenn]] in 1971, when he was a [[student]] at the Antioch Graduate School of Education (now [[Antioch University New England]]).
&lt;blockquote&gt;The Futures Wheel is a way of organizing thinking and questioning about the future – a kind of structured brainstorming. (Jerome C. Glenn (1994) The Futures Wheel)&lt;/blockquote&gt;

==Description==

To start a Futures wheel the central [[terminology|term]] describing the change to evaluate is positioned in the center of the page (or drawing area). Then, events or consequences following directly from that development are positioned around it. Next, the (indirect) consequences of the direct consequences are positioned around the first level consequences. The terms may be connected as nodes in a tree (or even a web). The levels will often be marked by concentric circles.

==Usage==

The Futures wheel is usually used to organize [[thought]]s about a future development or trend. With it, possible impacts can be collected and put down in a structured way. The use of interconnecting lines makes it possible to visualize interrelationships of the causes and resulting changes. Thus, Futures wheels can assist in developing multi-concepts about possible future development by offering a futures-conscious perspective and aiding in group [[brainstorming]].

==See also==

* [[Mind Mapping]]

==Bibliography==

* Glenn, Jerome C. ''Futurizing Teaching vs Futures Course'', Social Science Record, Syracuse University, Volume IX, No. 3 Spring 1972.
* Snyder, David Pearce. Monograph: ''The Futures Wheel: A Strategic Thinking Exercise'', The Snyder Family Enterprise, Bethesda, Maryland 1993.
* Glenn, Jerome C. ''Futures Wheel'', Futures Research Methodology Version 3.0, The Millennium Project, Washington, D.C. 2009.

==External links==
* [https://web.archive.org/web/20080612175450/http://www.ltag.education.tas.gov.au/glossary.htm Learning, Teaching and Assessment Guide Glossary] at Tasmania's [[Department of Education (Tasmania)|Department of Education]]'s homepage.
* Downloadable template of a [https://web.archive.org/web/20070927143447/http://www.globaleducation.edna.edu.au/globaled/go/cache/offonce/pid/1835;jsessionid=050A14CB101EAF863AE979C80461FCB3 Futures wheel] at the [[Australia]]n [http://www.globaleducation.edna.edu.au/ Global Education] website.
* Futures Wheel, Futures Research Methodology Version 3.0, The Millennium Project, Washington, DC 2009 [http://millennium-project.org/millennium/FRM-V3.html] 

[[Category:Knowledge representation]]
[[Category:Diagrams]]
[[Category:Futurology]]</text>
      <sha1>bomytieprjmu7nvh48m8btwcbbog9iq</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Argument mapping</title>
    <ns>14</ns>
    <id>11461295</id>
    <revision>
      <id>441884240</id>
      <parentid>133999634</parentid>
      <timestamp>2011-07-28T15:20:26Z</timestamp>
      <contributor>
        <username>Jm34harvey</username>
        <id>14284</id>
      </contributor>
      <comment>main article is Argument map</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="96" xml:space="preserve">{{Cat main|Argument map}}

[[Category:Knowledge representation]]
[[Category:Informal arguments]]</text>
      <sha1>keowc1rh3m0z24twk8tauwfgg8upx03</sha1>
    </revision>
  </page>
  <page>
    <title>Knowledge space</title>
    <ns>0</ns>
    <id>11851855</id>
    <revision>
      <id>755006919</id>
      <parentid>754947386</parentid>
      <timestamp>2016-12-15T19:07:19Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <minor />
      <comment>COI but very minor edit: Add two authorlinks</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6645" xml:space="preserve">{{about|knowledge spaces in mathematical psychology|the concept studied by philosopher Pierre Lévy|Knowledge space (philosophy)}}

In [[mathematical psychology]], a '''knowledge space''' is a [[antimatroid|combinatorial structure]] describing the possible states of knowledge of a human learner.&lt;ref&gt;{{citation|title=Knowledge Spaces|last1=Doignon|first1=J.-P.|last2=Falmagne|first2=J.-Cl.|author2-link=Jean-Claude Falmagne|publisher=Springer-Verlag|year=1999|isbn = 3-540-64501-2}}.&lt;/ref&gt;
To form a knowledge space, one models a domain of knowledge as a [[set (mathematics)|set]] of concepts, and a feasible state of knowledge as a [[subset]] of that set containing the concepts known or knowable by some individual. Typically, not all subsets are feasible, due to prerequisite relations among the concepts. The knowledge space is the family of all the feasible subsets. Knowledge spaces were introduced in 1985 by [[Jean-Paul Doignon]] and [[Jean-Claude Falmagne]]&lt;ref&gt;{{citation|last1=Doignon|first1=J.-P.|last2=Falmagne|first2=J.-Cl.|author2-link=Jean-Claude Falmagne|year=1985|title=Spaces for the assessment of knowledge|journal=International Journal of Man-Machine Studies|volume=23|issue=2|pages=175–196|doi=10.1016/S0020-7373(85)80031-6}}.&lt;/ref&gt; and have since been studied by many other researchers.&lt;ref&gt;{{citation|title=Knowledge Spaces. Applications in Education|last1=Falmagne|first1=J.-Cl.|author1-link=Jean-Claude Falmagne|last2=Albert|first2=D.|last3=Doble|first3=C.|last4=Eppstein|first4=D.|author4-link=David Eppstein|last5=Hu|first5=X.|publisher=Springer|year=2013}}.&lt;/ref&gt;&lt;ref&gt;A [http://kst.hockemeyer.at/kst-bib.html bibliography on knowledge spaces] maintained by Cord Hockemeyer contains over 400 publications on the subject.&lt;/ref&gt; They also form the basis for two computerized tutoring systems, [http://wundt.kfunigraz.ac.at/rath/ RATH] and [[ALEKS]].&lt;ref&gt;[http://wundt.uni-graz.at/MathPsych/cda/overview_sokrates.htm Introduction to Knowledge Spaces: Theory and Applications], Christof Körner, Gudrun Wesiak, and Cord Hockemeyer, 1999 and 2001.&lt;/ref&gt;

It is possible to interpret a knowledge space as a special form of a restricted [[latent class model]].&lt;ref&gt;{{citation|title=About the connection between knowledge structures and latent class models |last1=Schrepp |first1=M. |journal=Methodology|volume=1|issue=3|pages=92–102|year=2005|doi=10.1027/1614-2241.1.3.92}}.&lt;/ref&gt;

==Definitions==
Some basic definitions used in the knowledge space approach -
*A tuple &lt;math&gt;(Q, K)&lt;/math&gt; consisting of a non-empty set &lt;math&gt;Q&lt;/math&gt; and a set &lt;math&gt;K&lt;/math&gt; of subsets from &lt;math&gt;Q&lt;/math&gt; is called a ''knowledge structure'' if &lt;math&gt;K&lt;/math&gt; contains the empty set and &lt;math&gt;Q&lt;/math&gt;.
*A knowledge structure is called a ''knowledge space'' if it is closed under union, i.e. if &lt;math&gt;S, T \in Q&lt;/math&gt; implies &lt;math&gt;S\cup T \in Q&lt;/math&gt;.
*A knowledge space is called a ''quasi-ordinal knowledge space'' if it is in addition closed under intersection, i.e. if &lt;math&gt;S, T \in Q&lt;/math&gt; implies &lt;math&gt;S\cap T \in Q&lt;/math&gt;. Closure under both unions and intersections gives (''Q'',∪,∩) the structure of a [[distributive lattice]]; [[Birkhoff's representation theorem]] for distributive lattices shows that there is a one-to-one correspondence between the set of all [[preorder|quasiorders]] on Q and the set of all quasi-ordinal knowledge spaces on Q. I.e., each quasi-ordinal knowledge space can be represented by a quasi-order and vice versa.

An important subclass of knowledge spaces, the ''well-graded knowledge spaces'' or ''learning spaces'', can be defined as satisfying two additional mathematical axioms:
# If &lt;math&gt;S&lt;/math&gt; and &lt;math&gt;T&lt;/math&gt; are both feasible subsets of concepts, then &lt;math&gt;S\cup T&lt;/math&gt; is also feasible. In educational terms: if it is possible for someone to know all the concepts in ''S'', and someone else to know all the concepts in ''T'', then we can posit the potential existence of a third person who combines the knowledge of both people.
# If &lt;math&gt;S&lt;/math&gt; is a nonempty feasible subset of concepts, then there is some concept ''x'' in ''S'' such that &lt;math&gt;S\setminus\{x\}&lt;/math&gt; is also feasible. In educational terms: any feasible state of knowledge can be reached by learning one concept at a time, for a finite set of concepts to be learned.
A set family satisfying these two axioms forms a [[mathematical structure]] known as an [[antimatroid]].

==Construction of knowledge spaces==
In practice, there exist several methods to construct knowledge spaces. The most frequently used method is querying experts. There exist several querying algorithms that allow one or several experts to construct a knowledge space by answering a sequence of simple questions.&lt;ref&gt;{{citation|title=Extracting human expertise for constructing knowledge spaces: An algorithm |last1=Koppen |first1=M. |journal=Journal of Mathematical Psychology|volume=37|pages=1–20 |year=1993 |doi=10.1006/jmps.1993.1001}}.&lt;/ref&gt;&lt;ref&gt;{{citation|title=How to build a knowledge space by querying an expert |last1=Koppen |first1=M. |last2=Doignon |first2=J.-P. |journal=Journal of Mathematical Psychology|volume=34|issue=3|pages=311–331 |year=1990 |doi=10.1016/0022-2496(90)90035-8}}.&lt;/ref&gt;&lt;ref&gt;{{citation|title=A simulation study concerning the effect of errors on the establishment of knowledge spaces by querying experts |last1=Schrepp |first1=M. |last2=Held |first2=T. |journal=Journal of Mathematical Psychology|volume=39|issue=4|pages=376–382 |year=1995|doi=10.1006/jmps.1995.1035}}&lt;/ref&gt;

Another method is to construct the knowledge space by explorative data analysis (for example by [[Item tree analysis]]) from data.&lt;ref&gt;{{citation|title=Extracting knowledge structures from observed data |last1=Schrepp |first1=M. |journal=[[British journal of mathematical and statistical psychology]]|volume= 52|issue=2 |pages=213–224 |year=1999|doi=10.1348/000711099159071}}&lt;/ref&gt;&lt;ref&gt;{{citation|title=A method for the analysis of hierarchical dependencies between items of a questionnaire |last1=Schrepp |first1=M. |journal= Methods of Psychological Research Online|volume=19|pages=43–79  |year=2003|url=http://www.dgps.de/fachgruppen/methoden/mpr-online/issue19/art3/mpr106_04.pdf }}&lt;/ref&gt;
A third method is to derive the knowledge space from an analysis of the problem solving processes in the corresponding domain.&lt;ref&gt;{{citation|title=Knowledge Spaces: Theories, Empirical Research, Applications|last1=Albert|first1=D.|last2=Lukas|first2=J.|publisher=Lawrence Erlbaum Associates, Mahwah, NJ|year=1999}}&lt;/ref&gt;

==References==

{{reflist}}

[[Category:Cognition]]
[[Category:Knowledge representation]]</text>
      <sha1>nhgbphrb2pnlx5ojv2e0prn61hq7grs</sha1>
    </revision>
  </page>
  <page>
    <title>Default logic</title>
    <ns>0</ns>
    <id>889639</id>
    <revision>
      <id>708046143</id>
      <parentid>708045952</parentid>
      <timestamp>2016-03-03T08:20:35Z</timestamp>
      <contributor>
        <username>Cedar101</username>
        <id>374440</id>
      </contributor>
      <minor />
      <comment>/* Entailment */ &amp;not;</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="20570" xml:space="preserve">
'''Default logic''' is a [[non-monotonic logic]] proposed by [[Raymond Reiter]] to formalize reasoning with default assumptions.

Default logic can express facts like “by default, something is true”; by contrast, standard logic can only express that something is true or that something is false. This is a problem because reasoning often involves facts that are true in the majority of cases but not always. A classical example is: “birds typically fly”. This rule can be expressed in standard logic either by “all birds fly”, which is inconsistent with the fact that penguins do not fly, or by “all birds that are not penguins and not ostriches and ... fly”, which requires all exceptions to the rule to be specified. Default logic aims at formalizing inference rules like this one without explicitly mentioning all their exceptions.

==Syntax of default logic==
A default theory is a pair &lt;math&gt;\langle W, D \rangle&lt;/math&gt;. {{mvar|W}} is a set of logical formulae, called ''the background theory'', that formalize the facts that are known for sure. {{mvar|D}} is a set of ''default rules'', each one being of the form:

: &lt;math&gt;\frac{\mathrm{Prerequisite : Justification}_1, \dots , \mathrm{Justification}_n}{\mathrm{Conclusion}}&lt;/math&gt;

According to this default, if we believe that {{math|Prerequisite}} is true, and each of &lt;math&gt;\mathrm{Justification}_i&lt;/math&gt; is consistent with our current beliefs, we are led to believe that {{math|Conclusion}} is true.

The logical formulae in {{mvar|W}} and all formulae in a default were originally assumed to be [[first-order logic]] formulae, but they can potentially be formulae in an arbitrary formal logic. The case in which they are formulae in [[propositional logic]] is one of the most studied.

===Examples===
The default rule “birds typically fly” is formalized by the following default:

:&lt;math&gt;D = \left\{ \frac{\mathrm{Bird}(X) : \mathrm{Flies}(X)}{\mathrm{Flies}(X)} \right\}&lt;/math&gt;

This rule means that, if {{mvar|X}} is a bird, and it can be assumed that it flies, then we can conclude that it flies. A background theory containing some facts about birds is the following one:

:&lt;math&gt;W = \{ \mathrm{Bird}(\mathrm{Condor}), \mathrm{Bird}(\mathrm{Penguin}), \neg \mathrm{Flies}(\mathrm{Penguin}), \mathrm{Flies}(\mathrm{Bee}) \}&lt;/math&gt;.

According to this default rule, a condor flies because the precondition {{math|Bird(Condor)}} is true and the justification {{math|Flies(Condor)}} is not inconsistent with what is currently known. On the contrary, {{math|Bird(Penguin)}} does not allow concluding {{math|Flies(Penguin)}}: even if the precondition of the default {{math|Bird(Penguin)}} is true, the justification {{math|Flies(Penguin)}} is inconsistent with what is known.
From this background theory and this default, {{math|Bird(Bee)}} cannot be concluded because the default rule only allows deriving
{{math|Flies(''X'')}} from {{math|Bird(''X'')}}, but not vice versa. Deriving the antecedents of an inference rule from the consequences is a form of explanation of the consequences, and is the aim of [[abductive reasoning]].

A common default assumption is that what is not known to be true is believed to be false. This is known as the [[Closed World Assumption]], and is formalized in default logic using a default like the following one for every fact {{mvar|F}}.

: &lt;math&gt;\frac{:{\neg}F}{{\neg}F}&lt;/math&gt;

For example, the computer language [[Prolog]] uses a sort of default assumption when dealing with negation: if a negative atom cannot be proved to be true, then it is assumed to be false.
Note, however, that Prolog uses the so-called [[negation as failure]]: when the interpreter has to evaluate the atom &lt;math&gt;\neg F&lt;/math&gt;, it tries to prove that {{mvar|F}} is true, and conclude that &lt;math&gt;\neg F&lt;/math&gt; is true if it fails. In default logic, instead, a default having &lt;math&gt;\neg F&lt;/math&gt; as a justification can only be applied if &lt;math&gt;\neg F&lt;/math&gt; is consistent with the current knowledge.

===Restrictions===

A default is categorical or prerequisite-free if it has no prerequisite (or, equivalently, its prerequisite is [[tautology (logic)|tautological]]). A default is normal if it has a single justification that is equivalent to its conclusion. A default is supernormal if it is both categorical and normal. A default is seminormal if all its justifications entail its conclusion. A default theory is called categorical, normal, supernormal, or seminormal if all defaults it contains are categorical, normal, supernormal, or seminormal, respectively.

==Semantics of default logic==

A default rule can be applied to a theory if its precondition is entailed by the theory and its justifications are all '''''consistent with''''' the theory.  The application of a default rule leads to the addition of its consequence to the theory.  Other default rules may then be applied to the resulting theory.  '''When the theory is such that no other default can be applied, the theory is called an extension of the default theory.'''  The default rules may be applied in different order, and this may lead to different extensions. The [[Nixon diamond]] example is a default theory with two extensions:

:&lt;math&gt;
\left\langle
\left\{
\frac{\mathrm{Republican}(X):\neg \mathrm{Pacifist}(X)}{\neg \mathrm{Pacifist}(X)},
\frac{\mathrm{Quaker}(X):\mathrm{Pacifist}(X)}{\mathrm{Pacifist}(X)}
\right\},
\left\{\mathrm{Republican}(\mathrm{Nixon}), \mathrm{Quaker}(\mathrm{Nixon})\right\}
\right\rangle
&lt;/math&gt;

Since [[Richard Nixon|Nixon]] is both a [[American Republican|Republican]] and a [[Quaker]], both defaults can be applied. However, applying the first default leads to the conclusion that Nixon is not a pacifist, which makes the second default not applicable. In the same way, applying the second default we obtain that Nixon is a pacifist, thus making the first default not applicable. This particular default theory has therefore two extensions, one in which {{math|Pacifist(Nixon)}} is true, and one 
in which {{math|Pacifist(Nixon)}} is false. 

The original semantics of default logic was based on the [[Fixed point (mathematics)|fixed point]] of a function. The following is an equivalent algorithmic definition. If a default contains formulae with free variables, it is considered to represent the set of all defaults obtained by giving a value to all these variables. A default &lt;math&gt;\frac{\alpha:\beta_1,\ldots,\beta_n}{\gamma}&lt;/math&gt; is applicable to a propositional theory {{mvar|T}} if &lt;math&gt;T \models \alpha&lt;/math&gt; and
all theories &lt;math&gt;T \cup \{\beta_i\}&lt;/math&gt; are consistent. The application of this default to {{mvar|T}} leads to the theory &lt;math&gt;T \cup \{\gamma\}&lt;/math&gt;. An extension can be generated by applying the following algorithm:

 T=W           /* current theory */
 A=0           /* set of defaults applied so far */
 &amp;nbsp;
               /* apply a sequence of defaults */
 '''while''' there is a default d that is not in A and is applicable to T
   add the consequence of d to T
   add d to A
 &amp;nbsp;
               /* final consistency check */
 '''if''' 
   for every default d in A
     T is consistent with all justifications of d
 '''then'''
   output T

This algorithm is [[nondeterministic algorithm|non-deterministic]], as several defaults can alternatively be applied to a given theory {{mvar|T}}. In the Nixon diamond example, the application of the first default leads to a theory to which the second default cannot be applied and vice versa. As a result, two extensions are generated: one in which Nixon is a pacifist and one in which Nixon is not a pacifist.

The final check of consistency of the justifications of all defaults that have been applied implies that some theories do not have any extensions. In particular, this happens whenever this check fails for every possible sequence of applicable defaults. The following default theory has no extension:

:&lt;math&gt;
\left\langle 
\left\{
\frac{:A(b)}{\neg A(b)}
\right\},
\emptyset
\right\rangle
&lt;/math&gt;

Since &lt;math&gt;A(b)&lt;/math&gt; is consistent with the background theory, the default can be applied, thus leading to the conclusion that &lt;math&gt;A(b)&lt;/math&gt; is false. This result however undermines the assumption that has been made for applying the first default. Consequently, this theory has no extensions.

In a normal default theory, all defaults are normal: each default has the form &lt;math&gt;\frac{\phi : \psi}{\psi}&lt;/math&gt;. A normal default theory is guaranteed to have at least one extension. Furthermore, the extensions of a normal default theory are mutually inconsistent, i.e., inconsistent with each other.

===Entailment===

A default theory can have zero, one, or more extensions. [[Entailment]] of a formula from a default theory can be defined in two ways:

; Skeptical : a formula is entailed by a default theory if it is entailed by all its extensions;

; Credulous : a formula is entailed by a default theory if it is entailed by at least one of its extensions.

Thus, the Nixon diamond example theory has two extensions, one in which Nixon is a pacifist and one in which he is not a pacifist. Consequently, neither {{math|Pacifist(Nixon)}} nor {{math|&amp;not;Pacifist(Nixon)}} are skeptically entailed, while both of them are credulously entailed. As this example shows, the credulous consequences of a default theory may be inconsistent with each other.

===Alternative default inference rules===
&lt;!-- these are the alternative default inference rules that are based on the same original syntax of default logic --&gt;

The following alternative inference rules for default logic are all based on the same syntax as the original system.

; Justified: differs from the original one in that a default is not applied if thereby the set {{mvar|T}} becomes [[inconsistent]] with a justification of an applied default;

; Concise: a default is applied only if its consequence is not already entailed by {{mvar|T}} (the exact definition is more complicated than this one; this is only the main idea behind it);

; Constrained: a default is applied only if the set composed of the background theory, the justifications of all applied defaults, and the consequences of all applied defaults (including this one) is consistent;

; Rational: similar to constrained default logic, but the consequence of the default to add is not considered in the consistency check;

; Cautious: defaults that can be applied but are conflicting with each other (like the ones of the Nixon diamond example) are not applied.

The justified and constrained versions of the inference rule assign at least an extension to every default theory.

==Variants of default logic==
&lt;!-- these are the variants of default logic that differ from the original one both in syntax and semantics --&gt;

The following variants of default logic differ from the original one on both syntax and semantics.

; Assertional variants : An assertion is a pair &lt;math&gt;\langle p: \{r_1,\ldots,r_n\} \rangle&lt;/math&gt; composed of a formula and a set of formulae. Such a pair indicates that {{mvar|p}} is true while the formulae &lt;math&gt;r_1,\ldots,r_n&lt;/math&gt; have been assumed consistent to prove that {{mvar|p}} is true. An assertional default theory is composed of an assertional theory (a set of assertional formulae) called the background theory and a set of defaults defined as in the original syntax. Whenever a default is applied to an assertional theory, the pair composed of its consequence and its set of justifications is added to the theory. The following semantics use assertional theories:

*Cumulative default logic
*Commitment to assumptions default logic
*Quasi-default logic

; Weak extensions : rather than checking whether the preconditions are valid in the theory composed of the background theory and the consequences of the applied defaults, the preconditions are checked for validity in the extension that will be generated; in other words, the algorithm for generating extensions starts by guessing a theory and using it in place of the background theory; what results from the process of extension generation is actually an extension only if it is equivalent to the theory guessed at the beginning. This variant of default logic is related in principle to [[autoepistemic logic]], where a theory &lt;math&gt;\Box x \rightarrow x&lt;/math&gt; has the model in which {{mvar|x}} is true just because, assuming &lt;math&gt;\Box x&lt;/math&gt; true, the formula &lt;math&gt;\Box x \rightarrow x&lt;/math&gt; supports the initial assumption.

; Disjunctive default logic : the consequence of a default is a set of formulae instead of a single formula. Whenever the default is applied, at least one of its consequences is nondeterministically chosen and made true.

; Priorities on defaults : the relative priority of defaults can be explicitly specified; among the defaults that are applicable to a theory, only one of the most preferred ones can be applied. Some semantics of default logic do not require priorities to be explicitly specified; rather, more specific defaults (those that are applicable in fewer cases) are preferred over less specific ones.

; Statistical variant : a statistical default is a default with an attached upper bound on its frequency of error; in other words, the default is assumed to be an incorrect inference rule in at most that fraction of times it is applied.

==Translations==

Default theories can be translated into theories in other logics and vice versa. The following conditions on translations have been considered:

; Consequence-Preserving : the original and the translated theories have the same (propositional) consequences;

; Faithful : this condition only makes sense when translating between two variants of default logic or between default logic and a logic in which a concept similar to extension exists, e.g., models in modal logic; a translation is faithful if there exists a mapping (typically, a bijection) between the extensions (or models) of the original and translated theories;

; Modular : a translation from default logic to another logic is modular if the defaults and the background theory can be translated separately; moreover, the addition of formulae to the background theory only leads to adding the new formulae to the result of the translation;

; Same-Alphabet : the original and translated theories are built on the same alphabet;

; Polynomial : the running time of the translation or the size of the generated theory are required to be polynomial in the size of the original theory.

Translations are typically required to be faithful or at
least consequence-preserving, while the conditions of
modularity and same alphabet are sometimes ignored.

The translatability between propositional default logic and
the following logics have been studied:

* classical propositional logic;
* autoepistemic logic;
* propositional default logic restricted to seminormal theories;
* alternative semantics of default logic;
* circumscription.

Translations exist or not depending on which conditions are imposed. Translations from propositional default logic to classical propositional logic cannot always generate a polynomially sized propositional theory, unless the [[polynomial hierarchy]] collapses. Translations to autoepistemic logic exists or not depending on whether modularity or the use of the same alphabet is required.

==Complexity==

The [[Analysis of algorithms|computational complexity]] of the following problems about default logic is known:

; Existence of extensions : deciding whether a propositional default theory has at least one extension is &lt;math&gt;\Sigma^P_2&lt;/math&gt;-complete;

; Skeptical entailment : deciding whether a propositional default theory skeptically entails a [[propositional formula]] is &lt;math&gt;\Pi^P_2&lt;/math&gt;-complete;

; Credulous entailment : deciding whether a propositional default theory credulously entails a propositional formula is &lt;math&gt;\Sigma^P_2&lt;/math&gt;-complete;

; Extension checking : deciding whether a propositional formula is equivalent to an extension of a propositional default theory is &lt;math&gt;\Delta^{P[log]}_2&lt;/math&gt;-complete;

; Model checking : deciding whether a propositional interpretation is a model of an extension of a propositional default theory is &lt;math&gt;\Sigma^P_2&lt;/math&gt;-complete.

==Implementations==

Three systems implementing default logics are 
[ftp://www.cs.engr.uky.edu/cs/manuscripts/deres.ps DeReS],
[http://www.cs.uni-potsdam.de/wv/xray/ XRay] and
[http://www.info.univ-angers.fr/pub/stephan/Research/GADEL/GADEL_prolog.html GADeL]
&lt;!-- algorithms? other implemented systems? --&gt;

==See also==
* [[Answer set programming]]
* [[Defeasible logic]]
* [[Non-monotonic logic]]

==References==
* G. Antoniou (1999). A tutorial on default logics. ''ACM Computing Surveys'', 31(4):337-359.
* M. Cadoli, F. M. Donini, P. Liberatore, and M. Schaerf (2000). Space efficiency of propositional knowledge representation formalisms. ''Journal of Artificial Intelligence Research'', 13:1-31.
* P. Cholewinski, V. Marek, and M. Truszczynski (1996). Default reasoning system DeReS. In ''Proceedings of the Fifth International Conference on the Principles of Knowledge Representation and Reasoning (KR'96)'', pages 518-528.
* J. Delgrande and T. Schaub (2003). On the relation between Reiter's default logic and its (major) variants. In ''Seventh European Conference on Symbolic and Quantitative Approaches to Reasoning with Uncertainty (ECSQARU 2003)'', pages 452-463.
* J. P. Delgrande, T. Schaub, and W. K. Jackson (1994). Alternative approaches to default logic. ''Artificial Intelligence'', 70:167-237.
* G. Gottlob (1992). Complexity results for nonmonotonic logics. ''Journal of Logic and Computation'', 2:397-425.
* G. Gottlob (1995). Translating default logic into standard autoepistemic logic. ''Journal of the ACM'', 42:711-740.
* T. Imielinski (1987). Results on translating defaults to circumscription. ''Artificial Intelligence'', 32:131-146.
* T. Janhunen (1998). On the intertranslatability of autoepistemic, default and priority logics, and parallel circumscription. In ''Proceedings of the Sixth European Workshop on Logics in Artificial Intelligence (JELIA'98)'', pages 216-232.
* T. Janhunen (2003). Evaluating the effect of semi-normality on the expressiveness of defaults. ''Artificial Intelligence'', 144:233-250.
* H. E. Kyburg and C-M. Teng (2006). Nonmonotonic Logic and Statistical Inference. ''Computational Intelligence'', 22(1): 26-51.
* P. Liberatore and M. Schaerf (1998). The complexity of model checking for propositional default logics. In ''Proceedings of the Thirteenth European Conference on Artificial Intelligence (ECAI'98)'', pages 18–22.
* W. Lukaszewicz (1988). Considerations on default logic: an alternative approach. ''Computational Intelligence'', 4(1):1-16.
* W. Marek and M. Truszczynski (1993). ''Nonmonotonic Logics: Context-Dependent Reasoning''. Springer.
* A. Mikitiuk and M. Truszczynski (1995). Constrained and rational default logics. In ''Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI'95)'', pages 1509-1517.
* P. Nicolas, F. Saubion and I. Stéphan (2001). Heuristics for a Default Logic Reasoning System. ''International Journal on Artificial Intelligence Tools'', 10(4):503-523.
* R. Reiter (1980). A logic for default reasoning. ''Artificial Intelligence'', 13:81-132.
* T. Schaub, S. Brüning, and P. Nicolas (1996). XRay: A prolog technology theorem prover for default reasoning: A system description. In ''Proceedings of the Thirteenth International Conference on Automated Deduction (CADE'96)'', pages 293-297.
* G. Wheeler (2004). A resource bounded default logic. In ''Proceedings of the 10th International Workshop on Non-Monotonic Reasoning (NMR-04)'', Whistler, British Columbia, 416-422.
* G. Wheeler and C. Damasio (2004). An Implementation of Statistical Default Logic. In ''Proceedings of the 9th European Conference on Logics in Artificial Intelligence (JELIA 2004)'', LNCS Series, Springer, pages 121-133.

==External links==
* Schmidt, Charles F. [http://www.rci.rutgers.edu/~cfs/472_html/Logic_KR/DefaultTheory.html RCI.Rutgers.edu], Default Logic. Retrieved August 10, 2004.
* Ramsay, Allan (1999). [http://www.ccl.umist.ac.uk/teaching/material/5005/node33.html UMIST.ac.uk], Default Logic. Retrieved August 10, 2004.
* [http://plato.stanford.edu/entries/reasoning-defeasible/ Stanford.edu], Defeasible reasoning, [[Stanford Encyclopedia of Philosophy]].

[[Category:Logic programming]]
[[Category:Knowledge representation]]
[[Category:Logical calculi]]
[[Category:Non-classical logic]]</text>
      <sha1>eypulo2qp3a7g9s03ruggy91aj0tg3z</sha1>
    </revision>
  </page>
  <page>
    <title>Defeasible reasoning</title>
    <ns>0</ns>
    <id>2628057</id>
    <revision>
      <id>749302254</id>
      <parentid>741418890</parentid>
      <timestamp>2016-11-13T17:08:02Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* References */clean up; http&amp;rarr;https for [[Google Books]] and other Google services using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="18785" xml:space="preserve">{{No footnotes|date=April 2010}}
In [[logic]], '''defeasible reasoning''' is a kind of [[reasoning]] that is rationally compelling though not [[deductive reasoning|deductively valid]].&lt;ref&gt;{{cite web | url=http://plato.stanford.edu/entries/reasoning-defeasible | title="Defeasbile Reasoning," ''Stanford Encyclopedia of Philosophy | accessdate=1 July 2016 }}&lt;/ref&gt; The distinction between defeasibility and indefeasibility may be seen in the context of this joke:

:During a train trip through the countryside, an engineer, a physicist, and a mathematician observe a flock of sheep. The engineer remarks, "I see that the sheep in this region are white." The physicist offers a correction, "''Some'' sheep in this region are white." And the mathematician responds, "In this region there exist sheep that are white on at least one side."

The engineer in this story has reasoned defeasibly; since [[engineering]] is a highly practical discipline, it is receptive to generalizations. In particular, engineers cannot and need not defer decisions until they have acquired perfect and complete knowledge. But [[mathematics|mathematical reasoning]], having different goals, inclines one to account for even the rare and special cases, and thus typically leads to a stance that is indefeasible.

Defeasible reasoning is a particular kind of non-demonstrative reasoning, where the reasoning does not produce a full, complete, or final demonstration of a claim, i.e., where fallibility and corrigibility of a conclusion are acknowledged. In other words defeasible reasoning produces a [[Wiktionary:contingent|contingent]] statement or claim.  Other kinds of non-demonstrative reasoning are [[probabilistic reasoning]], [[inductive reasoning]], [[statistical]] reasoning, [[abductive reasoning]], and [[paraconsistent]] reasoning.  Defeasible reasoning is also a kind of [[ampliative]] reasoning because its conclusions reach beyond the pure meanings of the premises.

The differences between these kinds of reasoning correspond to differences about the conditional that each kind of reasoning uses, and on what premise (or on what authority) the conditional is adopted:
* ''[[deductive reasoning|Deductive]]'' (from meaning postulate, axiom, or contingent assertion): if ''p'' then ''q'' (i.e., ''q'' or ''not-p'')
* ''Defeasible'' (from authority): if ''p'' then (defeasibly) ''q''
* ''[[Probabilistic logic|Probabilistic]]'' (from combinatorics and indifference): if ''p'' then (probably) ''q''
* ''[[Statistics|Statistical]]'' (from data and presumption):  the frequency of ''q''s among ''p''s is high (or inference from a model fit to data); hence, (in the right context) if ''p'' then (probably) ''q''
* ''[[inductive reasoning|Inductive]]'' (theory formation; from data, coherence, simplicity, and confirmation): (inducibly) "if ''p'' then ''q''"; hence, if ''p'' then (deducibly-but-revisably) ''q''
* ''[[abductive reasoning|Abductive]]'' (from data and theory):  ''p'' and ''q'' are correlated, and ''q'' is sufficient for ''p''; hence, if ''p'' then (abducibly) ''q'' as cause

Defeasible reasoning finds its fullest expression in [[jurisprudence]], [[ethics]] and [[moral philosophy]], [[epistemology]], [[pragmatics]] and conversational [[Convention (norm)|conventions]] in [[linguistics]], [[Constructivist epistemology|constructivist]] [[Decision theory|decision theories]], and in [[knowledge representation]] and [[planning]] in [[artificial intelligence]].  It is also closely identified with [[prima facie]] (presumptive) reasoning (i.e., reasoning on the "face" of evidence), and [[ceteris paribus]] (default) reasoning (i.e., reasoning, all things "being equal").

== History ==

Though [[Aristotle]] differentiated the forms of reasoning that are valid for [[logic]] and [[philosophy]] from the more general ones that are used in everyday life (see [[dialectics]] and [[rhetoric]]), 20th century philosophers mainly concentrated on deductive reasoning. At the end of the 19th century, logic texts would typically survey both demonstrative and non-demonstrative reasoning, often giving more space to the latter. However, after the blossoming of [[mathematical logic]] at the hands of [[Bertrand Russell]], [[Alfred North Whitehead]] and [[Willard van Orman Quine]], latter-20th century logic texts paid little attention to the non-deductive modes of inference.

There are several notable exceptions. [[John Maynard Keynes]] wrote his dissertation on non-demonstrative reasoning, and influenced the thinking of [[Ludwig Wittgenstein]] on this subject. Wittgenstein, in turn, had many admirers, including the [[positivist]] legal scholar [[H.L.A. Hart]] and the [[speech act]] linguist [[John L. Austin]], [[Stephen Toulmin]] in rhetoric ([[Chaim Perelman]] too), the moral theorists [[W.D. Ross]] and [[C.L. Stevenson]], and the [[vagueness]] epistemologist/ontologist [[Friedrich Waismann]].

The etymology of ''defeasible'' usually refers to Middle English law of contracts, where a condition of defeasance is a clause that can invalidate or annul a contract or deed. Though ''defeat'', ''dominate'', ''defer'', ''defy'', ''deprecate'' and ''derogate'' are often used in the same contexts as ''defeasible,'' the verbs ''annul'' and ''invalidate'' (and ''nullify,'' ''overturn,'' ''rescind,'' ''vacate,'' ''repeal,'' ''debar'', ''void'', ''cancel'', ''countermand'', ''preempt'', etc.) are more properly correlated with the concept of defeasibility than those words beginning with the letter ''d''. Many dictionaries do contain the verb, ''to defease'' with past participle, ''defeased.''

Philosophers in moral theory and rhetoric had taken defeasibility largely for granted when American epistemologists rediscovered Wittgenstein's thinking on the subject: John Ladd, [[Roderick Chisholm]], [[Roderick Firth]], [[Ernest Sosa]], [[Robert Nozick]], and [[John L. Pollock]] all began writing with new conviction about how ''appearance as red'' was only a defeasible reason for believing something to be red.  More importantly Wittgenstein's orientation toward [[language-games]] (and away from [[semantics]]) emboldened these epistemologists to manage rather than to expurgate ''prima facie'' logical inconsistency.

At the same time (in the mid-1960s), two more students of Hart and Austin at Oxford, [[Brian Barry]] and [[David Gauthier]], were applying defeasible reasoning to political argument and practical reasoning (of action), respectively. [[Joel Feinberg]] and [[Joseph Raz]] were beginning to produce equally mature works in ethics and jurisprudence informed by defeasibility.  

By far the most significant works on defeasibility by the mid-1970s were in epistemology, where [[John L. Pollock|John Pollock]]'s 1974 ''Knowledge and Justification'' popularized his terminology of ''undercutting'' and ''rebutting'' (which mirrored the analysis of Toulmin). Pollock's work was significant precisely because it brought defeasibility so close to philosophical logicians. The failure of logicians to dismiss defeasibility in epistemology (as Cambridge's logicians had done to Hart decades earlier) landed defeasible reasoning in the philosophical mainstream.  

Defeasibility had always been closely related to argument, rhetoric, and law, except in epistemology, where the chains of reasons, and the origin of reasons, were not often discussed. [[Nicholas Rescher]]'s ''Dialectics'' is an example of how difficult it was for philosophers to contemplate more complex systems of defeasible reasoning. This was in part because proponents of [[informal logic]] became the keepers of argument and rhetoric while insisting that formalism was anathema to argument.

About this time, researchers in [[artificial intelligence]] became interested in [[non-monotonic reasoning]] and its [[semantics]]. With philosophers such as Pollock and Donald Nute (e.g., [[defeasible logic]]), dozens of computer scientists and logicians produced complex systems of defeasible reasoning between 1980 and 2000. No single system of defeasible reasoning would emerge in the same way that Quine's system of logic became a de facto standard. Nevertheless, the 100-year headstart on non-demonstrative logical calculi, due to [[George Boole]], [[Charles Sanders Peirce]], and [[Gottlob Frege]] was being closed: both demonstrative and non-demonstrative reasoning now have formal calculi.

There are related (and slightly competing) systems of reasoning that are newer than systems of defeasible reasoning, e.g., [[belief revision]] and [[dynamic logic (modal logic)|dynamic logic]]. The dialogue logics of [[Charles Leonard Hamblin|Charles Hamblin]] and Jim Mackenzie, and their colleagues, can also be tied closely to defeasible reasoning. Belief revision is a non-constructive specification of the desiderata with which, or constraints according to which, epistemic change takes place. Dynamic logic is related mainly because, like paraconsistent logic, the reordering of premises can change the set of justified conclusions. Dialogue logics introduce an adversary, but are like belief revision theories in their adherence to deductively consistent states of belief.

==Political and judicial use==
Many political philosophers have been fond of the word ''indefeasible'' when referring to rights, e.g., that were ''inalienable,'' ''divine,'' or ''indubitable.''  For example, in the 1776 [[Virginia Declaration of Rights]], "community hath an indubitable, inalienable, and indefeasible right to reform, alter or abolish government..." (also attributed to [[James Madison]]); and [[John Adams]], "The people have a right, an indisputable, unalienable, indefeasible, divine right to that most dreaded and envied kind of knowledge – I mean of the character and conduct of their rulers."
Also, [[Lord Aberdeen]]:  "indefeasible right inherent in the British Crown" and [[Gouverneur Morris]]:  "the Basis of our own Constitution is the indefeasible Right of the People."  Scholarship about [[Abraham Lincoln]] often cites these passages in the justification of secession.  Philosophers who use the word ''defeasible'' have historically had different world views from those who use the word ''indefeasible'' (and this distinction has often been mirrored by Oxford and Cambridge zeitgeist); hence it is rare to find authors who use both words.

In judicial opinions, the use of ''defeasible'' is commonplace.  There is however disagreement among legal logicians whether ''defeasible reasoning'' is central, e.g., in the consideration of ''open texture'', [[precedent]], [[wikt:exception|exceptions]], and ''rationales'', or whether it applies only to explicit defeasance clauses.  [[H.L.A. Hart]] in ''[[The Concept of Law]]'' gives two famous examples of defeasibility:  "No vehicles in the park" (except during parades); and "Offer, acceptance, and memorandum produce a contract" (except when the contract is illegal, the parties are minors, inebriated, or incapacitated, etc.).

== Specificity ==

One of the main disputes among those who produce systems of defeasible reasoning is the status of a ''rule of specificity.''  In its simplest form, it is the same rule as subclass [[inheritance (computer science)|inheritance]] preempting class inheritance:  

  (R1) if ''r'' then (defeasibly) ''q''                  e.g., if bird, then can fly
  (R2) if ''p'' then (defeasibly) ''not-q''              e.g., if penguin, then cannot fly
  (O1) if ''p'' then (deductively) ''r''                 e.g., if penguin, then bird
  (M1) arguably, p                               e.g., arguably, penguin
  (M2) R2 is a more specific reason than R1      e.g., R2 is better than R1
  (M3) therefore, arguably, not-q                e.g., therefore, arguably, not-flies

Approximately half of the systems of defeasible reasoning discussed today adopt a rule of specificity, while half expect that such ''preference'' rules be written explicitly by whoever provides the defeasible reasons.  For example, Rescher's dialectical system uses specificity, as do early systems of multiple inheritance (e.g., [[David Touretzky]]) and the early argument systems of Donald Nute and of [[Guillermo Simari]] and [[Ronald Loui]].  Defeasible reasoning accounts of  precedent ([[stare decisis]] and [[case-based reasoning]]) also make use of specificity (e.g., [[Joseph Raz]] and the work of Kevin D. Ashley and Edwina Rissland).  Meanwhile, the argument systems of Henry Prakken and Giovanni Sartor, of Bart Verheij and Jaap Hage, and the system of Phan Minh Dung do not adopt such a rule.

== Nature of defeasibility ==

There is a distinct difference between those who theorize about defeasible reasoning as if it were a system of confirmational revision (with affinities to [[belief revision]]), and those who theorize about defeasibility as if it were the result of further (non-empirical) investigation.  There are at least three kinds of further non-empirical investigation:  progress in a lexical/syntactic process, progress in a computational process, and progress in an adversary or legal proceeding.  

'''''Defeasibility as corrigibility:'''''  Here, a person learns something new that annuls a prior inference.  In this case, defeasible reasoning provides a constructive mechanism for belief revision, like a [[truth maintenance system]] as envisioned by Jon Doyle.

'''''Defeasibility as shorthand for preconditions:'''''  Here, the author of a set of rules or legislative code is writing rules with exceptions.  Sometimes a set of defeasible rules can be rewritten, with more cogency, with explicit (local) pre-conditions instead of (non-local) competing rules.  Many non-monotonic systems with [[fixed point (mathematics)|fixed-point]] or [[preferential]] semantics fit this view.  However, sometimes the rules govern a process of argument (the last view on this list), so that they cannot be re-compiled into a set of deductive rules lest they lose their force in situations with incomplete knowledge or incomplete derivation of preconditions.  

'''''Defeasibility as an [[anytime algorithm]]:'''''  Here, it is assumed that calculating arguments takes time, and at any given time, based on a subset of the potentially constructible arguments, a conclusion is defeasibly justified.  [[Isaac Levi]] has protested against this kind of defeasibility, but it is well-suited to the heuristic projects of, for example, [[Herbert A. Simon]].  On this view, the ''best move so far'' in a chess-playing program's analysis at a particular depth is a defeasibly justified conclusion.  This interpretation works with either the prior or the next semantical view.

'''''Defeasibility as a means of controlling an investigative or social process:'''''  Here, justification is the result of the right kind of procedure (e.g., a fair and efficient hearing), and defeasible reasoning provides impetus for pro and con responses to each other.  Defeasibility has to do with the alternation of verdict as locutions are made and cases presented, not the changing of a mind with respect to new (empirical) discovery.  Under this view, defeasible reasoning and defeasible argumentation refer to the same phenomenon.

==See also==
* [[Defeasible estate]]
* [[Indefeasible rights of use]]
* [[Argument (logic)]]
* [[Prima facie]]
* [[Practical reasoning]]
* [[Pragmatics]]
* [[Non-monotonic reasoning]]

== References ==
{{Reflist}}
* [http://www.springerlink.com/index/UQ708JX7XG823H5F.pdf Defeasible logic], Donald Nute, Lecture Notes in Computer Science, Springer, 2003.
* [http://portal.acm.org/citation.cfm?id=371581 Logical models of argument], Carlos Chesnevar, et al., ACM Computing Surveys 32:4, 2000.
* [https://books.google.com/books?hl=en&amp;lr=&amp;id=bQHce6eNhDIC&amp;oi=fnd&amp;pg=PA219&amp;dq=prakken&amp;ots=h7xemV-dM1&amp;sig=E6Ar7mAiBU0raO-rlNmzq8-8HG4 Logics for defeasible argumentation], Henry Prakken and Gerard Vreeswijk, in Handbook of Philosophical Logic, [[Dov M. Gabbay]], [[Franz Guenthner]], eds., Kluwer, 2002.
* [https://books.google.com/books?hl=en&amp;lr=&amp;id=DN5ERAAxUSYC&amp;oi=fnd&amp;pg=PR9&amp;dq=rescher&amp;ots=vRu4s0Ely-&amp;sig=__Dvw746CkFNdCaAdNLCGImTbFU Dialectics], [[Nicholas Rescher]], SUNY Press, 1977.
* [http://linkinghub.elsevier.com/retrieve/pii/S0364021387800174 Defeasible reasoning], John Pollock, Cognitive Science, 1987.
* [https://scholar.google.com/scholar?hl=en&amp;lr=&amp;cites=7198700474843277547 Knowledge and Justification], John Pollock, Princeton University Press, 1974.
* [http://webdigg.net/Defeasible/Defeasible-reasoning/ Abstract argumentation systems], Gerard Vreeswijk, Artificial Intelligence, 1997.
* [http://portal.acm.org/citation.cfm?id=222099 Hart's critics on defeasible concepts and ascriptivism], [[Ronald Loui]], Proc. 5th Intl. Conf. on AI and Law, 1995.
* [https://scholar.google.com/scholar?hl=en&amp;lr=&amp;cites=7525164436422571935 Political argument], [[Brian Barry]], Routledge &amp; Kegan Paul, 1970.
* [https://scholar.google.com/scholar?hl=en&amp;lr=&amp;cites=8944770465668267468 The uses of argument], [[Stephen Toulmin]], Cambridge University Press, 1958.
* [http://portal.acm.org/citation.cfm?id=981352&amp;dl= Discourse relations and defeasible knowledge], Alex Lascarides and Nicholas Asher, Proc. of the 29th Meeting of the Assn. for Comp. Ling., 1991.
* [http://journals.cambridge.org/action/displayAbstract;jsessionid=E68F5CAC6B0001D1ABEFD7C8C24F919F.tomcat1?fromPage=online&amp;aid=191503 Defeasible logic programming: an argumentative approach], Alejandro Garcia and [[Guillermo Simari]], Theory and Practice of Logic Programming 4:95–138, 2004. 
* [http://portal.acm.org/citation.cfm?id=180954.180957 Philosophical foundations of deontic logic and the logic of defeasible conditionals], Carlos Alchourron, in Deontic logic in computer science: normative system specification, J. Meyer, R. Wieringa, eds., Wiley, 1994.
* [http://www.sciencedirect.com/science?_ob=ArticleURL&amp;_udi=B6TYF-47YRKSD-7C&amp;_user=10&amp;_coverDate=02%2F29%2F1992&amp;_rdoc=1&amp;_fmt=high&amp;_orig=browse&amp;_origin=browse&amp;_zone=rslt_list_item&amp;_srch=doc-info%28%23toc%235617%231992%23999469997%23391734%23FLP%23display%23Volume%29&amp;_cdi=5617&amp;_sort=d&amp;_docanchor=&amp;_ct=16&amp;_acct=C000050221&amp;_version=1&amp;_urlVersion=0&amp;_userid=10&amp;md5=0735cebb41ce81bdfe8e260dbef2c71d&amp;searchtype=a A Mathematical Treatment of Defeasible Reasoning and its Implementation.] [[Guillermo Simari]], [[Ronald Loui]], Artificial Intelligence Journal, 53(2–3): 125–157 (1992).

== External links ==
* [http://plato.stanford.edu/entries/reasoning-defeasible/ Article on Defeasible Reasoning] in the [[Stanford Encyclopedia of Philosophy]]
* [http://william-king.www.drexel.edu/top/prin/txt/Intro/Eco112c.html An example of defeasible reasoning in action]

[[Category:Epistemology]]
[[Category:Logic]]
[[Category:Logic programming]]
[[Category:Knowledge representation]]
[[Category:Reasoning]]</text>
      <sha1>4tt9whmfinqaiusrdu0p14itaydgjd3</sha1>
    </revision>
  </page>
  <page>
    <title>Preferential entailment</title>
    <ns>0</ns>
    <id>3011353</id>
    <revision>
      <id>743599817</id>
      <parentid>631506043</parentid>
      <timestamp>2016-10-10T10:15:08Z</timestamp>
      <contributor>
        <username>Finlay McWalter</username>
        <id>22619</id>
      </contributor>
      <comment>fix misplaced newlines which broke templates</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1532" xml:space="preserve">'''Preferential entailment''' is a [[non-monotonic logic]] based on selecting only [[model (logic)|model]]s that are considered the most plausible. The plausibility of models is expressed by an ordering among models called a preference relation, hence the name preference entailment.

Formally, given a [[propositional formula]] &lt;math&gt;F&lt;/math&gt; and an ordering over propositional models &lt;math&gt;\leq&lt;/math&gt;, preferential [[entailment]] selects only the models of &lt;math&gt;F&lt;/math&gt; that are minimal according to &lt;math&gt;\leq&lt;/math&gt;. This selection leads to a non-monotonic inference relation: &lt;math&gt;F \models_\text{pref} G&lt;/math&gt; holds if and only if all minimal models of &lt;math&gt;F&lt;/math&gt; according to &lt;math&gt;\leq&lt;/math&gt; are also models of &lt;math&gt;G&lt;/math&gt;.&lt;ref name="s87"&gt;{{citation|last=Shoham|first=Y.|year=1987|contribution=Nonmonotonic logics: Meaning and utility|title=Proc. of the 10th Int. Joint Conf. on Artificial Intelligence (IJCAI’87)|pages=388–392|url=http://ijcai.org/Past%20Proceedings/IJCAI-87-VOL1/PDF/079.pdf}}.&lt;/ref&gt;

[[Circumscription (logic)|Circumscription]] can be seen as the particular case of preferential entailment when the ordering is based on containment of the sets of variables assigned to true (in the propositional case) or containment of the extensions of predicates (in the first-order logic case).&lt;ref name="s87"/&gt;

==See also==
* [[Rational consequence relation]]

==References==
{{reflist}}

[[Category:Logic in computer science]]
[[Category:Knowledge representation]]
[[Category:Non-classical logic]]</text>
      <sha1>9oif7oa68p6h2rwi6wti4glfsysefek</sha1>
    </revision>
  </page>
  <page>
    <title>Keyword AAA</title>
    <ns>0</ns>
    <id>13302227</id>
    <revision>
      <id>737088935</id>
      <parentid>714834082</parentid>
      <timestamp>2016-08-31T18:04:05Z</timestamp>
      <contributor>
        <username>Staszek Lem</username>
        <id>12536756</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="764" xml:space="preserve">{{notability|date=August 2016}}
{{Multiple issues|
{{Orphan|date=April 2016}}
{{no footnotes|date=December 2011}}
}}

'''Keyword AAA''' is a [[thesaurus]] created by the [[State Records Authority of New South Wales]]. It is often used to [[categorize|categorise]] documents in a [[document management system]]. The thesaurus is often implemented in terms of [[ISO 2788]].

==External links==
* [http://www.records.nsw.gov.au/recordkeeping/resources/keyword-products/keyword-aaa Keyword AAA Overview]
* [https://www.records.nsw.gov.au/recordkeeping/advice/records-classification/developing-and-implementing-a-keyword-thesaurus Developing and implementing a keyword thesaurus]

[[Category:Knowledge representation]]
[[Category:Library cataloging and classification]]</text>
      <sha1>89d6bf99t8parkgwg80s1460ct82d25</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Mereology</title>
    <ns>14</ns>
    <id>14239157</id>
    <revision>
      <id>548024178</id>
      <parentid>511766336</parentid>
      <timestamp>2013-03-31T19:14:24Z</timestamp>
      <contributor>
        <username>EmausBot</username>
        <id>11292982</id>
      </contributor>
      <minor />
      <comment>Bot: Migrating 2 langlinks, now provided by Wikidata on [[d:Q9053255]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="162" xml:space="preserve">{{Cat main|Mereology}}
[[Category:Philosophical logic]]
[[Category:Ontology]]
[[Category:Knowledge representation]]
[[Category:Materialism]]
[[Category:Quantity]]</text>
      <sha1>2ogptqad7xopvfhf7xr5ua9qk0abgra</sha1>
    </revision>
  </page>
  <page>
    <title>Allen's interval algebra</title>
    <ns>0</ns>
    <id>10483232</id>
    <revision>
      <id>741648490</id>
      <parentid>730568889</parentid>
      <timestamp>2016-09-28T20:47:52Z</timestamp>
      <contributor>
        <username>Zeno Gantner</username>
        <id>6435</id>
      </contributor>
      <minor />
      <comment>/* References */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4526" xml:space="preserve">{{Use dmy dates|date=June 2013}}
''For the type of boolean algebra called interval algebra, see [[Boolean algebra (structure)#Examples|Boolean algebra (structure)]]''

'''Allen's interval algebra''' is a [[calculus (disambiguation)|calculus]] for [[spatial-temporal reasoning|temporal reasoning]] that was introduced by [[James F. Allen]] in 1983.

The calculus defines possible relations between time intervals and provides a composition table that can be used as a basis
for reasoning about temporal descriptions of events.

==Formal description==

=== Relations ===

The following 13 base relations capture the possible relations between two intervals.

{| class="wikitable"
 !Relation
 !Illustration
 !Interpretation
 |-
 |&lt;math&gt;X \,\mathrel{\mathbf{&lt;}}\, Y&lt;/math&gt;
&lt;math&gt;Y \,\mathrel{\mathbf{&gt;}}\, X&lt;/math&gt;
 |[[Image:Allen calculus before.png|X takes place before Y]]
 |X takes place before Y
 |-
 |&lt;math&gt;X \,\mathrel{\mathbf{m}}\, Y&lt;/math&gt;
&lt;math&gt;Y \,\mathrel{\mathbf{mi}}\, X&lt;/math&gt;
 |[[Image:Allen calculus meet.png|X meets Y]]
 |X meets Y (''i'' stands for '''''i'''nverse'')
 |-
 |&lt;math&gt;X \,\mathrel{\mathbf{o}}\, Y&lt;/math&gt;
&lt;math&gt;Y \,\mathrel{\mathbf{oi}}\, X&lt;/math&gt;
 |[[Image:Allen calculus overlap.png|X overlaps with Y]]
 |X overlaps with Y
 |-
 |&lt;math&gt;X \,\mathrel{\mathbf{s}}\, Y&lt;/math&gt;
&lt;math&gt;Y \,\mathrel{\mathbf{si}}\, X&lt;/math&gt;
 |[[Image:Allen calculus start.png|X starts with Y]]
 |X starts Y
 |-
 |&lt;math&gt;X \,\mathrel{\mathbf{d}}\, Y&lt;/math&gt;
&lt;math&gt;Y \,\mathrel{\mathbf{di}}\, X&lt;/math&gt;
 |[[Image:Allen calculus during.png|X during Y]]
 |X during Y
 |-
 |&lt;math&gt;X \,\mathrel{\mathbf{f}}\, Y&lt;/math&gt;
&lt;math&gt;Y \,\mathrel{\mathbf{fi}}\, X&lt;/math&gt;
 |[[Image:Allen calculus finish.png|X finishes with Y]]
 |X finishes Y
 |-
 |&lt;math&gt;X \,\mathrel{\mathbf{=}}\, Y&lt;/math&gt;
 |[[Image:Allen calculus equal.png|X is equal to Y]]
 |X is equal to Y

 |}Using this calculus, given facts can be formalized and then used for automatic reasoning. Relations between intervals are formalized as sets of base relations.

The sentence
: ''During dinner, Peter reads the newspaper. Afterwards, he goes to bed.''
is formalized in Allen's Interval Algebra as follows:

&lt;math&gt;\mbox{newspaper } \mathbf{\{ \operatorname{d}, \operatorname{s}, \operatorname{f} \}} \mbox{ dinner}&lt;/math&gt;

&lt;math&gt;\mbox{dinner } \mathbf{\{ \operatorname{&lt;}, \operatorname{m} \}} \mbox{ bed}&lt;/math&gt;

In general, the number of different relations between n intervals is 1, 1, 13, 409, 23917, 2244361... [http://oeis.org/A055203 OEIS A055203]. The special case shown above is for n=2.

===Composition of relations between intervals===
For reasoning about the relations between temporal intervals, Allen's Interval Algebra provides a [[Relation composition|composition]] table. Given the relation between &lt;math&gt;X&lt;/math&gt; and &lt;math&gt;Y&lt;/math&gt; and the relation between &lt;math&gt;Y&lt;/math&gt; and &lt;math&gt;Z&lt;/math&gt;, the composition table allows for concluding about the relation between &lt;math&gt;X&lt;/math&gt; and &lt;math&gt;Z&lt;/math&gt;. Together with a [[Inverse relation|converse]] operation, this turns Allen's Interval Algebra into a [[relation algebra]].

For the example, one can infer &lt;math&gt;\mbox{newspaper } \mathbf{\{ \operatorname{&lt;}, \operatorname{m} \}} \mbox{ bed}&lt;/math&gt;.

==Extensions==
Allen's Interval Algebra can be used for the description of both temporal intervals and spatial configurations. For the latter use, the relations are interpreted as describing the relative position of spatial objects. This also works for three-dimensional objects by listing the relation for each coordinate separately.

==Implementation==
* [https://code.google.com/p/allenintervalrelationships/ A simple java library implementing the concept of Allen's temporal relations and the path consistency algorithm]

==See also==
* [[Temporal logic]]
* [[Logic]]
* [[Region Connection Calculus]].
* [[Spatial relation]] (analog)
* [[Commonsense reasoning]]

==References==
* James F. Allen: ''Maintaining knowledge about temporal intervals''. In: ''Communications of the ACM''. 26 November 1983. ACM Press. pp.&amp;nbsp;832–843, ISSN 0001-0782
* [[Bernhard Nebel]], Hans-Jürgen Bürckert: ''Reasoning about Temporal Relations: A Maximal Tractable Subclass of Allen's Interval Algebra.'' In: ''Journal of the ACM'' 42, pp.&amp;nbsp;43–66. 1995.
* Peter van Beek, Dennis W. Manchak: ''The design and experimental analysis of algorithms for temporal reasoning.'' In: ''Journal of Artificial Intelligence Research'' 4, pp.&amp;nbsp;1–18, 1996.

[[Category:Knowledge representation]]
[[Category:Constraint programming]]</text>
      <sha1>b26vb9qrrvm0g6gdbdoxxfk8jca5pm5</sha1>
    </revision>
  </page>
  <page>
    <title>Unique name assumption</title>
    <ns>0</ns>
    <id>15056340</id>
    <revision>
      <id>621641792</id>
      <parentid>526557033</parentid>
      <timestamp>2014-08-17T16:06:56Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <comment>copyedit, expand, add reference</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1351" xml:space="preserve">The '''unique name assumption''' is a simplifying assumption made in some [[ontology (computer science)|ontology]] languages and [[description logic]]s. In logics with the unique name assumption, different names always refer to different entities in the world.&lt;ref&gt;{{Cite AIMA |edition=2 |pages=333}}&lt;/ref&gt;

The standard ontology language [[Web Ontology Language|OWL]] does not make this assumption, but provides explicit constructs to express whether two names denote the same or distinct entities.&lt;ref&gt;{{cite conference |first1=Jiao |last1=Tao |first2=Evren |last2=Sirin |first3=Jie |last3=Bao |first4=Deborah L. |last4=McGuinness |title=Integrity constraints in OWL |conference=Proc. AAAI |year=2010 |url=http://www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/view/1931}}&lt;/ref&gt;&lt;ref&gt;[http://www.w3.org/TR/owl-ref/ OWL Web Ontology Language Reference]&lt;/ref&gt;
* &lt;code&gt;owl:sameAs&lt;/code&gt; is the OWL property that asserts that two given names or identifiers (e.g., URIs) refer to the same individual or entity.
* &lt;code&gt;owl:differentFrom&lt;/code&gt; is the OWL property that asserts that two given names or identifiers (e.g., URIs) refer to different individuals or entities.

==See also==
* [[Closed-world assumption]]
* [[Coreference]]

==References==
{{reflist}}

[[Category:Knowledge representation]]
[[Category:Ontology (information science)]]

{{logic-stub}}</text>
      <sha1>1yndkh76w1d0pnf7ez2v5d4gevwvbt1</sha1>
    </revision>
  </page>
  <page>
    <title>DogmaModeler</title>
    <ns>0</ns>
    <id>15967932</id>
    <revision>
      <id>664929732</id>
      <parentid>599383412</parentid>
      <timestamp>2015-05-31T22:32:46Z</timestamp>
      <contributor>
        <username>Magioladitis</username>
        <id>1862829</id>
      </contributor>
      <minor />
      <comment>clean up using [[Project:AWB|AWB]] (11023)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1726" xml:space="preserve">{{unreferenced|date=March 2014}}
[[Image:DogmaModeler1.jpg|thumb|DogmaModeler Screenshot]]
'''DogmaModeler''' is a free and open source ([[GNU GPL]]) [[Ontology (computer science)|ontology]] modeling tool based on [[object-role modeling]] (ORM). The philosophy of DogmaModeler is to enable non-IT experts to model ontologies with a little or no involvement of an ontology engineer. This challenge is tackled in DogmaModeler through well-defined methodological principles: the (1) [[Ontology double articulation|double-articulation]] and the (2) [[modularization]] principles. Other important features are: (3) the use of ORM as a graphical notation for ontology modeling; (4) the verbalization of ORM diagrams into pseudo natural language (supporting flexible verbalization templates for 11 human languages, including English, Dutch, German, French, Spanish, Arabic, Russian, etc.) that allows non-experts to check, validate, or build ontologies; (5)the automatic composition of ontology modules, through a well-defined composition operator; (6) the incorporation of linguistic resources in [[ontology engineering]]; (7) the automatic mapping of ORM diagrams into the DIG [[description logic]] interface and reasoning using [[RACER system|Racer]]; and many other functionalities.

The first version of DogmaModeler was developed at the [[Vrije Universiteit Brussel]].

== See also ==
* [[DOGMA]] 
* [[NORMA (software modeling tool)]]
* [[Protégé (software)|Protégé]]

==References==
{{reflist}}

== External links ==
* {{Official website|http://www.jarrar.info/Dogmamodeler/index.htm}}

[[Category:Knowledge representation]]
[[Category:Free integrated development environments]]
[[Category:Ontology (information science)]]</text>
      <sha1>nmrg7uv113wspgctaf4m3etx0oa344i</sha1>
    </revision>
  </page>
  <page>
    <title>FrameNet</title>
    <ns>0</ns>
    <id>2112884</id>
    <revision>
      <id>758680804</id>
      <parentid>758679587</parentid>
      <timestamp>2017-01-06T22:27:00Z</timestamp>
      <contributor>
        <username>Kingzoko</username>
        <id>30042487</id>
      </contributor>
      <comment>Made formatting of upper/lowercase and italicization of 'frames' and 'frame elements' more consistent.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10451" xml:space="preserve">{{refimprove|date=March 2012}}
In [[computational linguistics]], '''FrameNet''' is a project housed at the [[International Computer Science Institute]] in [[Berkeley, California]] which produces an electronic resource based on a theory of meaning called
[[Frame semantics (linguistics)|frame semantics]]. FrameNet reveals for example that the sentence "John sold a car to Mary" essentially describes the same basic situation (semantic frame) as "Mary bought a car from John", just from a different perspective. A semantic frame can be thought of as a conceptual structure describing an event, relation, or object and the participants in it. The FrameNet [[lexical database]] contains over 1,200 semantic ''frames'', 13,000 ''lexical units'' (a pairing of a [[word]] with a [[Meaning (linguistics)|meaning]]; [[Polysemy|polysemous]] words are represented by several ''lexical units'') and 202,000 example sentences. FrameNet is largely the creation of [[Charles J. Fillmore]], who developed the theory of frame semantics that the project is based on, and was initially the project leader when the project began in 1997.&lt;ref name="Goddard2011"&gt;{{cite book|author=Cliff Goddard|title=Semantic Analysis: A Practical Introduction|url=https://books.google.com/books?id=qfar1cmATvUC&amp;pg=PA78|accessdate=21 March 2012|date=25 September 2011|publisher=Oxford University Press|isbn=978-0-19-956028-8|pages=78–81}}&lt;/ref&gt; Collin Baker became the project manager in 2000.&lt;ref name="Linguistic Analysis"&gt;{{cite book|title=The Oxford Handbook of Linguistic Analysis|url=https://books.google.com/books?id=7plqH2gSq1wC&amp;pg=PP20|accessdate=21 March 2012|editor1-last=Heine|editor1-first=Bernd|editor2-last=Narrog|editor2-first=Heiko|publisher=Oxford University Press|isbn=978-0-19-160925-1|page=20}}&lt;/ref&gt; The FrameNet project has been influential in both linguistics and natural language processing, where it led to the task of automatic [[Semantic Role Labeling]].

==Concepts==

===Frames===
A frame is a schematic representation of a situation involving various participants, props, and other conceptual roles. Examples of frame names are &lt;tt&gt;Being_born&lt;/tt&gt; and &lt;tt&gt;Locative_relation&lt;/tt&gt;. A frame in FrameNet contains a textual description of what it represents (a frame definition), associated frame elements, lexical units, example sentences, and frame-to-frame relations.

===Frame elements===
Frame elements (FE) provide additional information to the semantic structure of a sentence. Each frame has a number of core and non-core FEs which can be thought of as semantic roles. Core FEs are essential to the meaning of the frame while non-core FEs are generally descriptive (such as time, place, manner, etc.).&lt;ref&gt;https://framenet.icsi.berkeley.edu/fndrupal/glossary#core&lt;/ref&gt;

Some examples include:

* The only core FE of the &lt;tt&gt;Being_born&lt;/tt&gt; frame is called &lt;tt&gt;Child&lt;/tt&gt;; non-core FEs being &lt;tt&gt;Time&lt;/tt&gt;, &lt;tt&gt;Place&lt;/tt&gt;, &lt;tt&gt;Relatives&lt;/tt&gt;, etc.&lt;ref&gt;https://framenet.icsi.berkeley.edu/fndrupal/index.php?q=frame_report&amp;name=Being_born&lt;/ref&gt;
* Core FEs of the &lt;tt&gt;Commerce_goods-transfer&lt;/tt&gt; include the &lt;tt&gt;Seller&lt;/tt&gt;, &lt;tt&gt;Buyer&lt;/tt&gt;, &lt;tt&gt;Goods&lt;/tt&gt;, among other things, while non-core FEs include a &lt;tt&gt;Place&lt;/tt&gt;, &lt;tt&gt;Purpose&lt;/tt&gt;, etc.&lt;ref&gt;https://framenet.icsi.berkeley.edu/fndrupal/index.php?q=frame_report&amp;name=Commerce_goods-transfer&lt;/ref&gt;

FrameNet includes shallow data on syntactic roles that frame elements play in the example sentences. For an example sentence like "She was born about AD 460", FrameNet would mark "She" as a [[noun phrase]] referring to the &lt;tt&gt;Child&lt;/tt&gt; FE, and "about AD 460" as a [[noun phrase]] corresponding to the &lt;tt&gt;Time&lt;/tt&gt; frame element. Details of how frame elements can be realized in a sentence are important because this reveals important information about the [[subcategorization frame]]s as well as possible [[diathesis alternation]]s (e.g. "John broke the window" vs. "The window broke")
of a verb.

===Lexical units===
Lexical units (LU) are lemmas, with their part of speech, that evoke a specific frame. In other words, when a LU is identified in a sentence, that specific LU can be associated with its specific frame(s). For each frame, there are many LUs associated to one frame and many frames that share multiple LUs, this is typically the case with LUs that have multiple word senses.&lt;ref&gt;https://framenet.icsi.berkeley.edu/fndrupal/glossary&lt;/ref&gt; Alongside the frame, each lexical unit is associated with specific frame elements by means of the annotated example sentences.

Example:

Lexical units that evoke the &lt;tt&gt;Complaining&lt;/tt&gt; frame (or more specific perspectivized versions of it, to be precise), include the verbs "complain", "grouse", "lament", and others.&lt;ref&gt;https://framenet2.icsi.berkeley.edu/fnReports/data/frameIndex.xml?frame=Complaining&lt;/ref&gt;

===Example sentences===
Frames are associated with example sentences and frame elements are marked within the sentences. Thus the sentence
:''She was '''born''' about AD 460''
is associated with the frame &lt;tt&gt;Being_born&lt;/tt&gt;, while "She" is marked as the frame element &lt;tt&gt;Child&lt;/tt&gt; and "about AD 460" is marked as &lt;tt&gt;Time&lt;/tt&gt;.
(See the [http://framenet.icsi.berkeley.edu/fnReports/displayReport.php?anno=9791 FrameNet Annotation Report] for &lt;tt&gt;born.v&lt;/tt&gt;.)
From the start, the FrameNet project has been committed to looking at evidence from actual language use as found in text collections like the [[British National Corpus]]. 
Based on such example sentences, automatic [[semantic role labeling]] tools are able to determine frames and mark frame elements in new sentences.

===Valences===
FrameNet also exposes the statistics on the ''valences'' of the ''frames'', that is the number and the position of the ''frame elements'' within example sentences. The sentence
:''She was '''born''' about AD 460''
falls in the valence pattern
:'''NP Ext, INI --, NP Dep'''
which occurs two times in the [https://framenet2.icsi.berkeley.edu/fnReports/data/lu/lu9791.xml example sentences] in FrameNet,
namely in:
:She'' was '''born''' ''about AD 460'', daughter and granddaughter of Roman and Byzantine emperors, whose family had been prominent in Roman politics for over 700 years.''
:''He was soon posted to north Africa, and never met their only child, ''a daughter'' '''born''' ''8 June 1941''.''

===Frame Relations===

FrameNet additionally captures relationships between different frames using relations. These include the following.

* Inheritance: When one frame is a more specific version of another, more abstract parent frame. Anything that is true about the parent frame must also be true about the child frame, and a mapping is specified between the frame elements of the parent and the frame elements of the child.
* Perspectivized_in: A neutral frame (like &lt;tt&gt;Commerce_transfer-goods&lt;/tt&gt;) is connected to a frame with a specific perspective of the same scenario (e.g. the &lt;tt&gt;Commerce_sell&lt;/tt&gt; frame, which assumes the perspective of the seller or the &lt;tt&gt;Commerce_buy&lt;/tt&gt; frame, which assumes the perspective of the buyer)
* Subframe: Some frames like the &lt;tt&gt;Criminal_process&lt;/tt&gt; frame refer to complex scenarios that consist of several individual states or events that can be described by separate frames like &lt;tt&gt;Arrest&lt;/tt&gt;, &lt;tt&gt;Trial&lt;/tt&gt;, and so on.
* Precedes: The Precedes relation captures a temporal order that holds between subframes of a complex scenario.
* Causative_of and Inchoative_of: There is a fairly systematic relationship between stative descriptions (e.g. the &lt;tt&gt;Position_on_a_scale&lt;/tt&gt; frame, "She had a high salary") and causative descriptions (&lt;tt&gt;Cause_change_of_scalar_position&lt;/tt&gt;, "She raised his salary") or inchoative descriptions (&lt;tt&gt;Change_position_on_a_scale&lt;/tt&gt;, e.g. "Her salary increased").
* Using: A relationship that holds between a frame that in some way involves another frame. For instance, the &lt;tt&gt;Judgment_communication&lt;/tt&gt; frame uses both the &lt;tt&gt;Judgment&lt;/tt&gt; frame and the &lt;tt&gt;Statement&lt;/tt&gt; frame, but does not inherit from either of them because there is no clear correspondence of the frame elements.
* See_also: Connects frames that bear some resemblance but need to be distinguished carefully.

==Applications==

FrameNet has proven useful in a number of computational applications, because computers need additional knowledge in order to recognize that "John sold a car to Mary" and "Mary bought a car from John" describe essentially the same situation, despite using two very different verbs, different prepositions and a different word order. FrameNet has been used in applications like [[question answering]], paraphrasing, recognizing textual entailment, and information extraction, either directly or by means of [[Semantic Role Labeling]] tools. The first automatic system for [[Semantic Role Labeling]] (SRL, sometimes also referred to as "shallow semantic parsing") was developed by Daniel Gildea and Daniel Jurafsky based on FrameNet in 2002, and Semantic Role Labelling has since become one of the standard tasks in natural language processing.

Since frames are essentially semantic descriptions, they are similar across languages, and several projects have arisen over the years that have relied on the original FrameNet as the basis for additional non-English FrameNets, for Spanish, Japanese, German, and Polish, among others.

==See also==
*[[BabelNet]]: a multilingual semantic network integrating FrameNet
*[[PropBank]]
*[[Null instantiation]]
*[[Frame language]]
*[[UBY]]: a database of 10 resources including FrameNet

==References==
{{Reflist}}

===Further reading===
*[https://framenet2.icsi.berkeley.edu/docs/r1.5/book.pdf FrameNet II: Extended Theory and Practice] (e-book)

==External links==
*[http://framenet.icsi.berkeley.edu/ FrameNet home page]
*[http://sccfn.sxu.edu.cn/ Chinese FrameNet]
*[http://framenet.dk/ Danish FrameNet]
*[http://gframenet.gmc.utexas.edu/ German FrameNet]
*[http://jfn.st.hc.keio.ac.jp/ Japanese FrameNet]
*[http://www.ramki.uw.edu.pl/en/index.html Polish FrameNet]
*[http://www.ufjf.br/framenetbr/ Portuguese FrameNet (Brazil)]
*[http://gemini.uab.es/SFN/ Spanish FrameNet]
*[http://spraakbanken.gu.se/eng/swefn/ Swedish FrameNet]

[[Category:Lexical databases]]
[[Category:Knowledge representation]]
[[Category:Corpus linguistics]]
[[Category:History of the Internet]]
[[Category:Hypertext]]
[[Category:Online dictionaries]]
[[Category:Science and technology in the San Francisco Bay Area]]</text>
      <sha1>kyoapzvr042m6d81tfmlugxxcv8ggd3</sha1>
    </revision>
  </page>
  <page>
    <title>Knowledge integration</title>
    <ns>0</ns>
    <id>4144848</id>
    <revision>
      <id>710817190</id>
      <parentid>705762078</parentid>
      <timestamp>2016-03-19T06:00:35Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor />
      <comment>Remove blank line(s) between list items per [[WP:LISTGAP]] to fix an accessibility issue for users of [[screen reader]]s. Do [[WP:GENFIXES]] and cleanup if needed. Discuss this at [[Wikipedia talk:WikiProject Accessibility#LISTGAP]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3782" xml:space="preserve">'''Knowledge integration''' is the process of synthesizing multiple [[knowledge model]]s (or representations) into a common model (representation).

Compared to [[information integration]], which involves merging information having different schemas and representation models, knowledge integration focuses more on synthesizing the understanding of a given subject from different perspectives.

For example, multiple interpretations are possible of a set of student grades, typically each from a certain perspective. An overall, integrated view and understanding of this information can be achieved if these interpretations can be put under a common model, say, a student performance index.

The [http://wise.berkeley.edu Web-based Inquiry Science Environment (WISE)], from the [[University of California at Berkeley]] has been developed along the lines of knowledge integration theory.

'''Knowledge integration''' has also been studied as the process of incorporating new information into a body of existing knowledge with an [[interdisciplinary]] approach.  This process involves determining how the new information and the existing knowledge interact, how existing knowledge should be modified to accommodate the new information, and how the new information should be modified in light of the existing knowledge.

A learning agent that actively investigates the consequences of new information can detect and exploit a variety of learning opportunities; e.g., to resolve knowledge conflicts and to fill knowledge gaps.  By exploiting these learning opportunities the learning agent is able to learn beyond the explicit content of the new information.

The [[machine learning]] program KI, developed by Murray and Porter at the [[University of Texas at Austin]], was created to study the use of automated and semi-automated knowledge integration to assist [[knowledge engineers]] constructing a large [[knowledge base]].

A possible technique which can be used is [[semantic matching]]. More recently, a technique useful to minimize the effort in mapping validation and visualization has been presented which is based on [[Minimal mappings|Minimal Mappings]]. Minimal mappings are high quality mappings such that i) all the other mappings can be computed from them in time linear in the size of the input graphs, and ii) none of them can be dropped without losing property i).

The [[University of Waterloo]] operates a Bachelor of Knowledge Integration [[undergraduate degree]] program as an academic major or minor. The program started in 2008.

==See also==
* [[Knowledge value chain]]

==References==
{{Reflist}}&lt;!--added under references heading by script-assisted edit--&gt;

==Further reading==
* Linn, M. C. (2006) The Knowledge Integration Perspective on Learning and Instruction. R. Sawyer (Ed.). In ''The Cambridge Handbook of the Learning Sciences.'' Cambridge, MA. Cambridge University Press
* Murray, K. S. (1996) KI: A tool for Knowledge Integration. Proceedings of the Thirteenth National Conference on Artificial Intelligence
* Murray, K. S. (1995) [http://www.ai.sri.com/pubs/files/1636.pdf Learning as Knowledge Integration], Technical Report TR-95-41, The University of Texas at Austin
* Murray, K. S. (1990) Improving Explanatory Competence, Proceedings of the Twelfth Annual Conference of the Cognitive Science Society
* Murray, K. S., Porter, B. W. (1990) Developing a Tool for Knowledge Integration: Initial Results. International Journal for Man-Machine Studies, volume 33
* Murray, K. S., Porter, B. W. (1989) Controlling Search for the Consequences of New Information during Knowledge Integration. Proceedings of the Sixth International Machine Learning Conference

[[Category:Knowledge representation]]
[[Category:Learning]]
[[Category:Machine learning]]</text>
      <sha1>9ta9v9xiakskoh5sz5stga0082mvhjv</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Knowledge bases</title>
    <ns>14</ns>
    <id>20750664</id>
    <revision>
      <id>732976277</id>
      <parentid>718313668</parentid>
      <timestamp>2016-08-04T14:41:23Z</timestamp>
      <contributor>
        <username>Narky Blert</username>
        <id>22041646</id>
      </contributor>
      <comment>DN tag</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="425" xml:space="preserve">A '''[[Knowledge base]]''' is a special kind of [[database]] for [[knowledge management]]. It provides the means for the computerized collection, organization, and [[retrieval]]{{dn|date=August 2016}} of [[knowledge]].  It is also used for specified information and as a [[personal knowledge base]]

[[Category:Online databases]]
[[Category:Semantic Web]]
[[Category:Knowledge representation]]
[[Category:Types of databases]]</text>
      <sha1>gic1r84zdmdzxvl302jwldsoh6nc0mv</sha1>
    </revision>
  </page>
  <page>
    <title>Darwin Core</title>
    <ns>0</ns>
    <id>21195116</id>
    <revision>
      <id>753442765</id>
      <parentid>744814321</parentid>
      <timestamp>2016-12-07T05:08:48Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 5 sources and tagging 0 as dead. #IABot (v1.2.7.1)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8006" xml:space="preserve">'''Darwin Core''' (often abbreviated to '''DwC''') is an extension of [[Dublin Core]] for [[biodiversity informatics]]. It is meant to provide a stable standard reference for sharing information on biological diversity.&lt;ref&gt;{{cite journal|last=Wieczorek|first=John|author2=D. Bloom |author3=R. Guralnick |author4=S. Blum |author5=M. Döring |author6=R. De Giovanni |author7=T. Robertson |author8=D. Vieglais |title=Darwin Core: An Evolving Community-developed Biodiversity Data Standard.|journal=[[PLoS ONE]] |year=2012|volume=7|issue=1|doi=10.1371/journal.pone.0029715|pmid=22238640|pmc=3253084}}&lt;/ref&gt; The terms described in this standard are a part of a larger set of vocabularies and technical specifications under development and maintained by [[Biodiversity Information Standards (TDWG)]] (formerly known as the Taxonomic Databases Working Group (TDWG)).

== Description ==
The Darwin Core is a body of standards. It includes a glossary of terms (in other contexts these might be called properties, elements, fields, columns, attributes, or concepts) intended to facilitate the sharing of information about biological diversity by providing reference definitions, examples, and commentaries. The Darwin Core is primarily based on [[taxon|taxa]], their occurrence in nature as documented by observations, specimens, and samples, and related information. Included in the standard are documents describing how these terms are managed, how the set of terms can be extended for new purposes, and how the terms can be used. The '''Simple Darwin Core''' &lt;ref name="simpledwc"&gt;[http://rs.tdwg.org/dwc/terms/simple/index.htm  The Simple Darwin Core]&lt;/ref&gt; is a specification for one particular way to use the terms and to share data about taxa and their occurrences in a simply-structured way. It is likely what is meant if someone were to suggest "formatting your data according to the Darwin Core".

Each '''term''' has a definition and commentaries that are meant to promote the consistent use of the terms across applications and disciplines. Evolving commentaries that discuss, refine, expand, or translate the definitions and examples are referred to through links in the Comments attribute of each term. This approach to documentation allows the standard to adapt to new purposes without disrupting existing applications. There is meant to be a clear separation between the terms defined in the standard and the applications that make use of them. For example, though the data types and constraints are not provided in the term definitions, recommendations are made about how to restrict the values where appropriate.

In practice, Darwin Core decouples the definition and semantics of individual terms from application of these terms in different technologies such as [[XML]], [[Resource Description Framework|RDF]] or simple [[Comma-separated values|CSV]] text files. Darwin Core provides separate guidelines on how to encode the terms as XML&lt;ref name="dwc-xml" &gt;[http://rs.tdwg.org/dwc/terms/guides/xml/index.htm Darwin Core XML Guide]&lt;/ref&gt; or text files.&lt;ref name="dwc-text"&gt;[http://rs.tdwg.org/dwc/terms/guides/text/index.htm Darwin Core Text Guide]&lt;/ref&gt;

== History ==
Darwin Core was originally created as a [[Z39.50]] profile by the Z39.50 Biology Implementers Group (ZBIG), supported by funding from a USA National Science Foundation award.&lt;ref name="zbig"&gt;An Experimental Z39.50 Information Retrieval Protocol Test Bed for Biological Collection and Taxonomic Data, #9811443 [http://nsf.gov/awardsearch/showAward.do?AwardNumber=9811443]&lt;/ref&gt;  The name "Darwin Core" was first coined by Allen Allison at the first meeting of the ZBIG held at the University of Kansas in 1998 while commenting on the profile's conceptual similarity with Dublin Core. The Darwin Core profile was later expressed as an XML Schema document for use by the Distributed Generic Information Retrieval (DiGIR) protocol. A [[TDWG]] task group was created to revise the Darwin Core, and a ratified metadata standard was officially released on 9 October 2009.

Though ratified as a TDWG/[[Biodiversity Information Standards]] standard since then, Darwin Core has had numerous previous versions in production usage. The published standard contains a history&lt;ref name="history"&gt;[http://rs.tdwg.org/dwc/terms/history/index.htm Darwin Core History]&lt;/ref&gt; with details of the versions leading to the current standard.

{| class="wikitable" style="text-align:left"
|+Darwin Core Versions
|-
! Name !! Namespace !! Number of terms !! XML Schema !! Date Issued
|-
! Darwin Core 1.0
| Not Applicable || 24 || (Z39.50 GRS-1) || 1998
|-
! Darwin Core 1.2 (Classic)
| http://digir.net/schema/conceptual/darwin/2003/1.0  {{dead link|date=October 2016}} || 46 || [http://digir.net/schema/conceptual/darwin/2003/1.0/darwin2.xsd] || 2001-09-11
|-
! Darwin Core 1.21 (MaNIS/HerpNet/ORNIS/FishNet2)
| http://digir.net/schema/conceptual/darwin/2003/1.0  {{dead link|date=October 2016}} || 63 || [http://digir.net/schema/conceptual/darwin/manis/1.21/darwin2.xsd] || 2003-03-15
|-
! Darwin Core OBIS
| http://www.iobis.org/obis  {{dead link|date=October 2016}} || 27 || [http://iobis.org/obis/obis.xsd] || 2005-07-10
|-
! Darwin Core 1.4 (Draft Standard)
| http://rs.tdwg.org/dwc/dwcore/  {{dead link|date=October 2016}} || 45 || [http://rs.tdwg.org/dwc/tdwg_dw_core.xsd] || 2005-07-10
|-
! Darwin Core Terms (properties)
| http://rs.tdwg.org/dwc/terms/ || 172 || [http://rs.tdwg.org/dwc/xsd/tdwg_dwcterms.xsd] || 2009-10-09
|-
|}

== Key Projects Using Darwin Core ==
* The [[Global Biodiversity Information Facility]] (GBIF)&lt;ref&gt;{{cite web|url=http://www.gbif.org/informatics/standards-and-tools/publishing-data/data-standards/darwin-core-archives/ |title=Darwin Core |publisher=[[Global Biodiversity Information Facility]] |accessdate=April 12, 2011 |deadurl=yes |archiveurl=https://web.archive.org/web/20110412022331/http://www.gbif.org:80/informatics/standards-and-tools/publishing-data/data-standards/darwin-core-archives |archivedate=April 12, 2011 |df= }}&lt;/ref&gt;
* The [[Ocean Biogeographic Information System]] (OBIS)&lt;ref&gt;{{cite web|url=http://www.iobis.org/data/schema-and-metadata|title=Data Schema and metadata|publisher=[[Ocean Biogeographic Information System]]|accessdate=April 12, 2011}}&lt;/ref&gt;
*[http://www.ala.org.au/datastandards.htm The Atlas of Living Australia (ALA)]
*[http://www3.interscience.wiley.com/cgi-bin/fulltext/120713092/PDFSTART Online Zoological Collections of Australian Museums (OZCAM)]
*[http://manisnet.org Mammal Networked Information System (MaNIS)]
*[http://ornisnet.org Ornithological Information System (ORNIS)]
*[http://www.fishnet2.net/index.html FishNet 2]
*[http://vertnet.org VertNet]
*[http://www.canadensys.net/ Canadensys]
*[http://w3.ufsm.br/herbarioflorestal/nature/site/ Sistema Nature 3.0]
*[http://eol.org Encyclopedia of Life]
*[https://www.idigbio.org Integrated Digitized Biocollections (iDigBio)] &lt;ref&gt;{{cite web|title=Data Ingestion Guidance|url=https://www.idigbio.org/wiki/index.php/Data_Ingestion_Guidance|publisher=[[iDigBio]]|accessdate=26 September 2016}}&lt;/ref&gt; &lt;ref&gt;{{cite web|title=Getting your data out there: Data publishing &amp; data standards with iDigBio|url=https://www.idigbio.org/content/getting-your-data-out-there-data-publishing-data-standards-idigbio|publisher=[[iDigBio]]|accessdate=26 September 2016}}&lt;/ref&gt;

== See also ==
* [[Darwin Core Archive]]
* [[Biodiversity Information Standards]] (TDWG)
* [[Biodiversity]]
* [[Biodiversity informatics]]
* [[Metadata standards]]

==References==
{{reflist}}

==External links==
*[http://rs.tdwg.org/dwc/terms/index.htm Darwin Core Quick Reference Guide]
*[https://github.com/tdwg/dwc/ Darwin Core Development Site]
*[http://www.tdwg.org/activities/darwincore/ Official Darwin Core Website]
*[http://www.tdwg.org/fileadmin/subgroups/dwc/exec_summary_dwc.doc Executive Summary of Darwin Core]

[[Category:Bioinformatics]]
[[Category:Knowledge representation]]
[[Category:Interoperability]]
[[Category:Metadata standards]]</text>
      <sha1>7an2l6iix6p09vpc675wf64tkl4uy3v</sha1>
    </revision>
  </page>
  <page>
    <title>Pretext</title>
    <ns>0</ns>
    <id>8280463</id>
    <revision>
      <id>755349161</id>
      <parentid>742699783</parentid>
      <timestamp>2016-12-17T14:03:21Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* United States */clean up; http&amp;rarr;https for [[The Guardian]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10862" xml:space="preserve">A '''pretext''' (adj: '''pretextual''') is an excuse to do something or say something that is not accurate. Pretexts may be based on a half-truth or developed in the context of a misleading fabrication. Pretexts have been used to conceal the true purpose or rationale behind actions and words.

In [[Law of the United States|US law]], a pretext usually describes false reasons that hide the true intentions or motivations for a legal action. If a party can establish a [[prima facie]] case for the proffered evidence, the opposing party must prove that these reasons were "pretextual" or false. This can be accomplished by directly demonstrating that the motivations behind the presentation of evidence is false, or indirectly by evidence that the motivations are not "credible".&lt;ref name=uslegal&gt;{{cite web|title=Pretext Law &amp; Legal Definition|url=http://definitions.uslegal.com/p/pretext/|publisher=uslegal.com|accessdate=13 March 2013}}&lt;/ref&gt; In
''Griffith v. Schnitzer'', an employment discrimination case, a jury award was reversed by a [[Court of Appeals]] because the evidence was not sufficient that the defendant's reasons were "pretextual". That is, the defendant's evidence was either undisputed, or the plaintiff's  was "irrelevant subjective assessments and opinions".&lt;ref&gt;[http://www.omwlaw.com/wp-content/uploads/2013/01/Defining-Pretext-In-Discrimination-Cases.pdf Defining "pretext" in discrimination cases] by Karen Sutherland (2013)&lt;/ref&gt;

A "pretextual" arrest by law enforcement officers is one carried out for illegal purposes such as to conduct an unjustified [[search and seizure]].&lt;ref&gt;[http://assets.wne.edu/161/8_note_Criminal.pdf Criminal law - Pretextual arrests and alternatives to the objective tests] by Robert D. Snook&lt;/ref&gt;&lt;ref name=oday&gt;{{cite web|last=O'Day|first=Kathleen M.|title=Pretextual traffic stops: injustice for minority drivers|url=http://academic.udayton.edu/race/03justice/s98oday.htm|publisher=The University of Dayton School of Law|accessdate=13 March 2013}}&lt;/ref&gt;

[[File:Marbleboot.jpg|thumb|right|140px|[[Marble Boat]] on [[Kunming Lake]] near Beijing.]]
As one example of pretext, in the 1880s, the Chinese government raised money on the pretext of modernizing the Chinese navy. Instead, these funds were diverted to repair a ship-shaped, two-story pavilion which had been originally constructed for [[Empress Xiaoshengxian|the mother]] of the [[Qianlong Emperor]]. This pretext and the Marble Barge are famously linked with [[Empress Dowager Cixi]]. This architectural [[folly]], known today as the [[Marble Boat]] (''Shifang''), is "moored" on Lake Kunming in what the empress renamed the "Garden for Cultivating Harmony" (''Yiheyuan'').&lt;ref&gt;Min, Anchee. (2007). [https://books.google.com/books?id=_H8TYkS84E4C&amp;pg=PA155&amp;dq=marble+barge&amp;client=firefox-a#PPA155,M1  ''The Last Empress,'' pp. 155-156;]&lt;/ref&gt;

Another example of pretext was demonstrated in the speeches of the Roman orator [[Cato the Elder]] (234-149 BC). For Cato, every public speech became a pretext for a comment about Carthage. The Roman statesman had come to believe that the prosperity of ancient Carthage represented an eventual and inevitable danger to Rome. In the Senate, Cato famously ended every speech by proclaiming his opinion that [[Carthage]] had to be destroyed (''[[Carthago delenda est]]''). This oft-repeated phrase was the ultimate conclusion of all logical argument in every oration, regardless of the subject of the speech. This pattern persisted until his death in 149, which was the year in which the Third Punic War began. In other words, any subject became a pretext for reminding his fellow senators of the dangers Carthage represented.&lt;ref&gt;Hooper, William Davis ''et al.'' (1934). [http://penelope.uchicago.edu/Thayer/E/Roman/Texts/Cato/De_Agricultura/Introduction*.html   "Introduction,"] in Cato's ''De Agricultura'' (online version of Loeb edition).&lt;/ref&gt;

==Uses in warfare==

[[File:Hokoji-Bell-M1767.jpg|thumb|right|140px|Temple bell at [[Hōkō-ji (Kyoto)|Hōkō-ji]].]][[File:Hokoji-BellDetail-M1767.jpg|thumb|right|140px|Inscription on bell at Hokoji in Kyoto]]
The early years of Japan's [[Tokugawa shogunate]] were unsettled, with warring factions battling for power. The causes for the fighting were in part pretextural, but the outcome brought diminished armed conflicts after the [[Siege of Osaka]] in 1614-1615.

* '''1614''' (''Keichō 19''): The Shogun vanquished Hideyori and set fire to [[Osaka Castle]], and then he returned for the winter to [[Edo]].&lt;ref name="t410"&gt;Titsingh, [https://books.google.com/books?id=18oNAAAAIAAJ&amp;pg=PP9&amp;dq=nipon+o+dai+itsi+ran#PRA1-PA410,M1  p. 410.]&lt;/ref&gt;
* '''August 24, 1614''' (''Keichō 19, 19th day of the 7th month''): A new bronze bell for the Hōkō-ji was cast successfully [http://oldphoto.lb.nagasaki-u.ac.jp/en/target.php?id=4771][http://oldphoto.lb.nagasaki-u.ac.jp/en/target.php?id=3093]; but despite the dedication ceremony planning, Ieyasu forbade any further actions concerning the great bell:
::"[T]he tablet over the Daibutsu-den and the bell bore the inscription ''"Kokka ankō"'' (meaning "the country and the house, peace and tranquility"), and at this [[Tokugawa Ieyasu]] affect to take umbrage, alleging that it was intended as a curse on him for the character 安 (''an,'' "peace") was placed between the two characters composing his own name 家康 (''"ka-kō",'' "house tranquility") [suggesting subtly perhaps that peace could only be attained by Ieyasu's dismemberment?] ... This incident of the inscription was, of course, a mere pretext, but Ieyasu realized that he could not enjoy the power he had usurped as long as Hideyori lived, and consequently, although the latter more than once Hideyori dispatched his vassal Katagiri Kastumoto to Ieyasu's residence ([[Sunpu Castle]]) with profuse apologies, Ieyasu refused to be placated."&lt;ref&gt;Ponsonby-Fane, Richard. (1956). ''Kyoto, the Old Capital of Japan,'' p. 292; Titsingh, [https://books.google.com/books?id=18oNAAAAIAAJ&amp;pg=PP9&amp;dq=nipon+o+dai+itsi+ran#PRA1-PA410,M1  p. 410.]&lt;/ref&gt;
* '''October 18, 1614''' (''Keichō 19, 25th day of the 10th month''): A strong earthquake shook Kyoto.&lt;ref name="t410"/&gt;
* '''1615''' (''Keichō 20''): Osaka Summer Battle begins.

The next two-and-a-half centuries of Japanese history were comparatively peaceful under the successors of Tokugawa Ieyasu and the [[bakufu]] government he established.

===United States===
*During the War of 1812, US President [[James Madison]] was often accused of using impressment of American sailors by the [[Royal Navy]] as a pretext to invade [[Canada]].

{{main|Pearl Harbor advance-knowledge debate}}
*Some have argued that United States President [[Franklin D. Roosevelt]] used the [[attack on Pearl Harbor]] by Japanese forces on December 7, 1941 as a pretext to enter [[World War II]].&lt;ref&gt;Bernstein, Richard. [http://www.nytimes.com/1999/12/15/books/books-of-the-times-on-dec-7-did-we-know-we-knew.html "On Dec. 7, Did We Know We Knew?"] ''New York Times.'' December 15, 1999.&lt;/ref&gt; American soldiers and supplies had been assisting British and Soviet operations for almost a year by this point, and the United States had thus "chosen a side", but due to the political climate in the States at the time and some campaign promises made by Roosevelt that he would not send American boys to fight in foreign wars. Roosevelt could not declare war for fear of public backlash. The attack on Pearl Harbor united the American people's resolve against the Axis powers and created the bellicose atmosphere in which to declare war.
* Critics have accused United States President [[George W. Bush]] of using the [[September 11th, 2001 attacks]] and faulty intelligence about the existence of [[weapons of mass destruction]] as a pretext for the [[Iraq war|war in Iraq]].&lt;ref&gt;Borger, Julian. (2006). [https://www.theguardian.com/world/2006/sep/07/usa.books  "Book says CIA tried to provoke Saddam to war,"] ''The Guardian'' (London). 7 September 2006.&lt;/ref&gt;

==Social engineering==
{{main|Social engineering (security)}}
A type of [[Social engineering (security)|social engineering]] called [[Social engineering (security)#Pretexting|pretexting]] uses a pretext to elicit information fraudulently  from a target. The pretext in this case includes research into the identity of a certain authorized person or personality type in order to establish legitimacy in the mind of the target.&lt;ref&gt;[[Federal Trade Commission]] (FTC):  [http://www.ftc.gov/bcp/edu/pubs/consumer/credit/cre10.shtm  "Pretexting: Your Personal Information Revealed."] February 2006.&lt;/ref&gt;

==See also==
{{wiktionary}}

* [[Plausible deniability]]
* [[Proximate cause]]
* [[Causes of the Franco-Prussian War]]

==Notes==
{{reflist|2}}

==References==
* [[James Bamford|Bamford]], James. (2004). [https://books.google.com/books?id=VuOxAAAACAAJ&amp;dq=Pretext+for+War:+9/11,+Iraq,+and+the+Abuse+of+America%27s+Intelligence+Agencies&amp;client=firefox-a  ''Pretext for War: 9/11, Iraq, and the Abuse of America's Intelligence Agencies.'']  New York: [[Doubleday Books]]. ISBN 978-0-385-50672-4; [http://www.worldcat.org/oclc/55068034?referer=di&amp;ht=edition OCLC 55068034]
* [[Cato the Elder|Cato]], Marcus Porcius. [https://books.google.com/books?id=D2mxAAAAIAAJ&amp;q=Hooper+and+De+Agricultura&amp;dq=Hooper+and+De+Agricultura&amp;lr=&amp;client=firefox-a&amp;pgis=1 ''On Agriculture'' (''De agricultura'')] trans,   William Davis Hooper and Harrison Boyd Ash. Cambridge: [[Harvard University Press]]. [http://www.worldcat.org/oclc/230499252 OCLC  230499252]
* [[Michael Isikoff|Isikoff]], Michael and [[David Corn]]. 2006. [https://books.google.com/books?id=Sa14AAAAMAAJ&amp;q=hubris&amp;dq=hubris&amp;pgis=1  ''Hubris: The Inside Story of Spin, Scandal, and the Selling of the Iraq War'']  New York: [[Crown Publishers]]. ISBN 978-0-307-34681-0
* Min, Anchee. (2007). [https://books.google.com/books?id=_H8TYkS84E4C&amp;client=firefox-a  ''The Last Empress.''] New York: [[Houghton Mifflin Harcourt]]. ISBN 978-0-618-53146-2
* [[Richard Ponsonby-Fane|Ponsonby-Fane]], Richard Arthur Brabazon. (1956). ''Kyoto, the Old Capital of Japan,'' Kyoto: Ponsonby Memorial Society.
* [[Robert Stinnett|Stinnett]] Robert B. (2001). [https://books.google.com/books?id=Q2UKN5daNHYC ''Day of Deceit: The Truth about FDR and Pearl Harbor''] New York: [[Simon &amp; Schuster]]. ISBN 978-0-7432-0129-2
* [[Isaac Titsingh|Titsingh]], Isaac. (1834). [Siyun-sai Rin-siyo/[[Hayashi Gahō]], 1652], ''[[Nipon o daï itsi ran]]; ou, [https://books.google.com/books?id=18oNAAAAIAAJ&amp;dq=nipon+o+dai+itsi+ran  Annales des empereurs du Japon.'']  Paris: [[Royal Asiatic Society|Oriental Translation Fund of Great Britain and Ireland]].

[[Category:Propaganda techniques]]
[[Category:Knowledge representation]]
[[Category:Cognition]]
[[Category:Attack on Pearl Harbor]]
[[Category:Social engineering (computer security)]]</text>
      <sha1>1uwz58p6p23abflahcznza64d91ib2z</sha1>
    </revision>
  </page>
  <page>
    <title>Reification (knowledge representation)</title>
    <ns>0</ns>
    <id>5662676</id>
    <revision>
      <id>746411798</id>
      <parentid>746411731</parentid>
      <timestamp>2016-10-27T07:10:10Z</timestamp>
      <contributor>
        <ip>88.79.92.56</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3104" xml:space="preserve">{{multiple issues|{{refimprove|date=December 2009}}
{{Expert-subject|Information Science|ex2=Cyc}}}}


'''Reification''' in [[knowledge representation]] is the process of turning a predicate into an object.&lt;ref&gt;{{cite web |last=Hunt |first=Matthew |title=Notes on Semantic Nets and Frames |url=http://www.eecs.qmul.ac.uk/~mmh/AINotes/AINotes4.pdf |date=1996 |access-date=15 June 2016}}&lt;/ref&gt; Reification involves the representation of factual assertions that are referred to by ''other'' assertions, which might then be manipulated in some way; e.g., comparing [[logical assertion]]s from different [[witness]]es in order to determine their [[credibility]].

The message "John is six feet tall" is an assertion involving truth that commits the speaker to its factuality, whereas the reified statement "Mary reports that John is six feet tall" defers such commitment to Mary. In this way, the statements can be incompatible without creating contradictions in [[reasoning]]. For example, the statements "John is six feet tall" and "John is five feet tall" are mutually exclusive (and thus incompatible), but the statements "Mary reports that John is six feet tall" and "Paul reports that John is five feet tall" are not incompatible, as they are both governed by a conclusive rationale that either Mary or Paul is (or both are), in fact, incorrect.

In Linguistics, reporting, telling, and saying are recognised as ''verbal processes that project a wording (or locution)''. If a person says that "Paul told x" and "Mary told y", this person stated only that the telling took place. In this case, the person who made these two statements did not represent a person inconsistently. In addition, if two people are talking to each other, let's say Paul and Mary, and Paul tells Mary "John is five feet tall" and Mary rejects Paul's statement by saying "No, he is actually six feet tall", the socially constructed model of John does not become inconsistent. The reason for that is that statements are to be understood as an attempt to convince the addressee of something (Austin's How to do things with words), alternatively as a request to add some attribute to the model of Paul. The response to a statement can be an acknowledgement, in which case the model is changed, or it can be a statement rejection, in which case the model does not get changed. Finally, the example above for which John is said to be "five feet tall" or "six feet tall" is only incompatible because John can only be a single number of feet tall. If the attribute were a possession as in "he has a dog" or "he also has a cat", a model inconsistency would not happen. In other words, the issue of model inconsistency has to do with our model of the domain element (John) and not with the ascription of different range elements (measurements such as "five feet tall" or "six feet tall") nor with statements.

==See also==
*[[Reification (computer science)]]
*[[Reification (fallacy)]]
*[[Reification (linguistics)]]

==References==
{{Reflist}}

{{DEFAULTSORT:Reification (Knowledge Representation)}}
[[Category:Knowledge representation]]</text>
      <sha1>20dwxx0gdkshltv3s60yprzuv0g2lo2</sha1>
    </revision>
  </page>
  <page>
    <title>GNOWSYS</title>
    <ns>0</ns>
    <id>450307</id>
    <revision>
      <id>706276314</id>
      <parentid>706263530</parentid>
      <timestamp>2016-02-22T12:29:57Z</timestamp>
      <contributor>
        <username>AnomieBOT</username>
        <id>7611264</id>
      </contributor>
      <comment>Rescuing orphaned refs ("gnu" from rev 671823950)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5338" xml:space="preserve">{{Refimprove|date=June 2011}}
{{Infobox software
|  name = GNOWSYS
|  logo = [[Image:Gnowsys-logo.png|100px]]
|  developer = [[GNU|The GNU Project]]
|  latest_release_version = 1.0 rc1
|  operating_system = [[Cross-platform]]
|  genre = [[Semantic web|Semantic computing]]
|  license = [[GNU General Public License|GPL]]
|  website = [https://www.gnu.org/software/gnowsys/ www.gnu.org/software/gnowsys/]
}}
'''GNOWSYS''' (Gnowledge Networking and Organizing system) is a specification for a generic [[distributed network]] based memory/[[knowledge management]]. It is developed as an application for developing and maintaining [[semantic web]] content. It is written in [[Python (programming language)|Python]]. It is implemented as a [[Django (web framework)|Django]] app.

The memory of GNOWSYS is designed as a node-oriented space. A node is described by other nodes to which it has links. The nodes are organized and processed according to a complex data structure called the neighborhood.&lt;ref name="gnu"&gt;[https://www.gnu.org/software/gnowsys/] GNOWSYS: A Kernel for Semantic Computing.&lt;/ref&gt;

==Applications==

The application can be used for web-based knowledge representation and content management projects, for developing structured knowledge bases, as a collaborative authoring tool, suitable for making electronic glossaries, dictionaries and encyclopedias, for managing large web sites or links, developing an online catalogue for a library of any thing including books, to make ontologies, classifying and networking any objects, etc. This tool is also intended to be used for an on-line tutoring system with dependency management between various concepts or software packages.  For example, the dependency relations between [[Debian GNU/Linux]] packages have been represented by the [http://www.gnowledge.org/search_debmap?val=1 gnowledge portal].

==Component Classes==
The kernel is designed to provide support to persistently store highly granular nodes of knowledge representation like terms, predicates and very complex propositional systems like arguments, rules, axiomatic systems, loosely held paragraphs, and more complex structured and consistent compositions. All the component classes in GNOWSYS are classified according to complexity into three groups, where the first two groups are used to express all possible well formed formulae permissible in a first order logic.&lt;ref name="conceptPaper"&gt;[http://www.hbcse.tifr.res.in/gn/concept_paper.pdf GNOWSYS: A System for Semantic Computing ]&lt;/ref&gt;

===Terms===
‘Object’, ‘Object Type’ for declarative knowledge, ‘Event’, ‘Event Type’, for temporal objects, and ‘Meta Types’ for expressing upper ontology. The
objects in this group are essentially any thing about which the [[knowledge engineer]] intends to express and store in the knowledge base, i.e., they are the objects of discourse. The instances of these component classes can be stored with or without expressing ‘instance of’ or ‘sub-class of’ relations among them.

===Predicates===
This group consists of ‘Relation’, and ‘Relation Type’ for expressing declarative knowledge, and ‘Function’ and ‘Function Type’ for expressing procedural knowledge. This group is to express qualitative and quantitative relations among the various instances stored in the knowledge base. While instantiating the predicates can be characterized by their logical properties of relations, quantifiers and cardinality as monadic predicates
of these predicate objects.

===Structures===
‘System’, ‘Encapsulated Class’, ‘Program’, and ‘Process’, are other base classes for complex structures, which can be combined iteratively to produce more complex systems. The component class ‘System’ is to store in the knowledge base a set of propositions composed into ontologies, axiomatic systems, complex systems like say a human body, an artifact like a vehicle etc., with or without consistency check. An ‘Encapsulated Class’ is to com-
pose declarative and behavioural objects in a flexible way to build classes. A ‘Program’ is not only to store the logic of any complete program or a component class, composed from the already available behavioural instances in the knowledge base with built-in connectives (conditions, and loops), but also execute them as web services. A ‘Process’ is to structure temporal objects with sequence, concurrency, synchronous or asynchronous specifications.

Every node in the database keeps the neighbourhood information, such as its super-class, sub-class, instance-of, and other relations, in which the object has a role, in the form of predicates. This feature makes computation of drawing graphs and inferences, on the one hand, and dependency and navigation paths on the other hand very easy.  All the data and metadata is indexed in a central catalogue making query and locating resources efficient.

==References==
{{Reflist}}

==External links==
{{Portal|Free software}}
* [http://www.gnowledge.org/ Welcome to Gnowledge!]
* [https://www.gnu.org/software/gnowsys/ GNOWSYS is part of the GNU project.]

{{GNU}}

{{DEFAULTSORT:Gnowsys}}
[[Category:Cross-platform free software]]
[[Category:Free network-related software]]
[[Category:GNU Project software]]
[[Category:Knowledge representation]]
[[Category:Semantic Web]]</text>
      <sha1>l5p2e8q32yapcqows945ig1uo0quf36</sha1>
    </revision>
  </page>
  <page>
    <title>Lumpers and splitters</title>
    <ns>0</ns>
    <id>558750</id>
    <revision>
      <id>751138146</id>
      <parentid>744553155</parentid>
      <timestamp>2016-11-23T16:35:03Z</timestamp>
      <contributor>
        <ip>163.1.120.19</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10482" xml:space="preserve">{{Refimprove|date=January 2014}}

'''Lumpers''' and '''splitters''' are opposing factions in any [[discipline]] that has to [[Categorization|place individual examples into rigorously defined categories]]. The lumper-splitter problem occurs when there is the need to create classifications and assign examples to them, for example schools of [[literature]], [[biology|biological]] [[taxon|taxa]] and so on. A "lumper" is an individual who takes a [[wikt:gestalt|gestalt]] view of a definition, and assigns examples broadly, assuming that differences are not as important as signature similarities. A "splitter" is an individual who takes precise definitions, and creates new categories to classify samples that differ in key ways.

==Origin of the terms==
The earliest use of these terms was by [[Charles Darwin]], in a letter to [[J. D. Hooker]] in 1857, "(Those who make many species are the 'splitters,' and those who make few are the 'lumpers.')" They were introduced more widely by [[George G. Simpson]] in his 1945 work "The Principles of Classification and a Classification of Mammals." As he put it, "splitters make very small units – their critics say that if they can tell two animals apart, they place them in different genera … and if they cannot tell them apart, they place them in different species. … Lumpers make large units – their critics say that if a [[Carnivora|carnivore]] is neither a dog nor a bear, they call it a cat."&lt;ref&gt;{{Cite journal|last=Simpson|first=George G.|title=The Principles of Classification and a Classification of Mammals|journal=Bulletin of the AMNH|volume=85|page=23|year=1945|publisher=American Museum of Natural History|location=New York}}&lt;/ref&gt;

Another early use can be found in the title of a 1969 paper by the medical geneticist [[Victor McKusick]]: "On lumpers and splitters, or the nosology of genetic disease."&lt;ref&gt;McKusick VA. On lumpers and splitters, or the nosology of genetic disease. Perspect Biol Med. 1969 Winter;12(2):298-312.&lt;/ref&gt;

Reference to lumpers and splitters also appeared in a debate in 1975 between [[J. H. Hexter]] and [[John Edward Christopher Hill|Christopher Hill]], in the ''[[Times Literary Supplement]]''. It followed from Hexter's detailed review of Hill's book ''Change and Continuity in Seventeenth Century England'', in which Hill developed [[Max Weber]]'s argument that the rise of capitalism was facilitated by [[Calvinist]] Puritanism. Hexter objected to Hill's 'mining' of sources to find evidence that supported his theories. Hexter argued that Hill plucked quotations from sources in a way that distorted their meaning. Hexter explained this as a mental habit that he called 'lumping'. According to him, lumpers rejected differences and chose to emphasize similarities. Any evidence that did not fit their arguments was ignored as aberrant. Splitters, by contrast, emphasised differences, and resisted simple schemes. While lumpers consistently tried to create coherent patterns, splitters preferred incoherent complexity.&lt;ref&gt;{{Cite journal|last=Chase|first=Bob|title=Upstart Antichrist|journal=History Workshop Journal|issue=60|date=Autumn 2005|pages=202-206}}&lt;/ref&gt;

==Usage in various fields==

===Biology===
{{anchor|Lumping and splitting in biology}}
{{main|Biological classification}}
The categorization and naming of a particular species should be regarded as a ''hypothesis'' about the [[evolution]]ary relationships and distinguishability of that group of organisms. As further information comes to hand, the hypothesis may be confirmed or refuted. Sometimes, especially in the past when communication was more difficult, taxonomists working in isolation have given two distinct names to individual [[organism]]s later identified as the same species. When two named species are agreed to be of the same species, the older species name is almost always retained dropping the newer species name honoring a convention known as "priority of nomenclature".  This form of lumping is technically called synonymization. Dividing a taxon into multiple, often new, taxa is called splitting. Taxonomists are often referred to as "lumpers" or "splitters" by their colleagues, depending on their personal approach to recognizing differences or commonalities between organisms.

=== History ===
{{main|Periodization}}

In history, lumpers are those who tend to create broad definitions that cover large periods of time and many disciplines, whereas splitters want to assign names to tight groups of inter-relationships. Lumping tends to create a more and more unwieldy definition, with members having less and less mutually in common. This can lead to definitions which are little more than conventionalities, or groups which join fundamentally different examples. Splitting often leads to "[[Distinction without a difference|distinctions without difference]]," ornate and fussy categories, and failure to see underlying similarities.

For example, in the arts, "[[Romanticism|Romantic]]" can refer specifically to a period of [[Germany|German]] poetry roughly from 1780–1810, but would exclude the later work of [[Goethe]], among other writers. In music it can mean every composer from [[Johann Nepomuk Hummel|Hummel]] through [[Sergei Rachmaninoff|Rachmaninoff]], plus many that came after.

=== Software modelling ===
[[Software engineering]] often proceeds by building models (sometimes known as [[model-driven architecture]]). A lumper is keen to generalize, and produces models with a small number of broadly defined objects. A splitter is reluctant to generalize, and produces models with a large number of narrowly defined objects. Conversion between the two styles is not necessarily symmetrical. For example, if error messages in two narrowly defined classes behave in the same way, the classes can be easily combined. But if some messages in a broad class behave differently, every object in the class must be examined before the class can be split. This illustrates the principle that "splits can be lumped more easily than lumps can be split."&lt;ref name=Pugh2005&gt;{{cite book|last1=Pugh|first1=Ken|title=Prefactoring|date=2005|publisher=O'Reilly Media|pages=14–15|url=https://books.google.com/books?id=8nykB7qerJYC&amp;pg=PA15#v=onepage&amp;q&amp;f=false|accessdate=2014-10-21}}&lt;/ref&gt;

=== Language classification ===
{{main|Language classification}}

There is no agreement among [[Historical linguistics|historical linguists]] about what amount of evidence is needed for two languages to be safely classified in the same [[language family]]. For this reason, many language families have had lumper–splitter controversies, including [[Altaic languages|Altaic]], [[Pama–Nyungan languages|Pama–Nyungan]], [[Nilo-Saharan]], and most of the larger [[Classification schemes for indigenous languages of the Americas|families of the Americas]]. At a completely different level, the splitting of a [[mutually intelligible]] [[dialect continuum]] into different languages, or lumping them into one, is also an issue that continually comes up, though the consensus in contemporary linguistics is that there is no completely objective way to settle the question.

Splitters regard the [[comparative method (linguistics)|comparative method]] (meaning not comparison in general, but only reconstruction of a common ancestor or [[protolanguage]]) as the only valid proof of kinship, and consider [[genetic (linguistics)|genetic]] relatedness to be the question of interest. American linguists of recent decades tend to be splitters.

Lumpers are more willing to admit techniques like [[mass lexical comparison]] or [[lexicostatistics]], and mass typological comparison, and to tolerate the uncertainty of whether relationships found by these methods are the result of [[linguistic divergence]] (descent from common ancestor) or [[language convergence]] (borrowing). Much long-range comparison work has been from Russian linguists like [[Vladislav Illich-Svitych]] and [[Sergei Starostin]]. In the US, [[Joseph Greenberg|Greenberg]]'s and [[Merritt Ruhlen|Ruhlen]]'s work has been met with little acceptance from linguists. Earlier American linguists like [[Morris Swadesh]] and [[Edward Sapir]] also pursued large-scale classifications like [[Classification schemes for indigenous languages of the Americas#Sapir .281929.29: Encyclop.C3.A6dia Britannica|Sapir's 1929 scheme for the Americas]], accompanied by controversy similar to that today.&lt;ref&gt;http://www.nostratic.ru/books/(137)ruhlen12.pdf [[Merritt Ruhlen]]: Is Algonquian Amerind?&lt;/ref&gt;

=== Liturgical studies ===
[[Paul F. Bradshaw]] suggests that the same principles of lumping and splitting apply to the study of early Christian [[liturgy]]. Lumpers, who tend to predominate, try to find a single line of texts from the apostolic age to the fourth century (and later). Splitters see many parallel and overlapping strands which intermingle and flow apart so that there is not a single coherent path in development of liturgical texts. Liturgical texts must not be taken solely at face value; often there are hidden agendas in texts.&lt;ref name="bradshaw"&gt;Bradshaw, Paul F., ''The Search for the Origins of Christian Worship'', Oxford University Press, 2002, p. ix. ISBN 0-19-521732-2&lt;/ref&gt;

The Hindu religion is essentially a lumper's concept, sometimes also known as [[Smartism]].  Hindu splitters, and individual adherents, often identify themselves as adherents of a religion such as [[Shaivism]], [[Vaishnavism]], or [[Shaktism]] according to which deity they believe to be the supreme creator of the universe.{{Citation needed|date=June 2012}}

===Philosophy===
[[Freeman Dyson]] has suggested that "observers of the philosophical scene" can be broadly, if over-simplistically, divided into splitters and lumpers, roughly corresponding to [[materialists]], who imagine the world as divided into atoms, and [[Platonists]], who regard the world as made up of ideas.

== See also ==
* [[Evolutionary biology]]
* [[Heterarchy]]
* [[Prototype theory]]
* [[Sorites paradox]]

==References==
{{Reflist|30em}}

==External links==
* [http://www.users.globalnet.co.uk/~rxv/infomgt/abstraction.htm#lumpersplitter Abstraction: Lumpers and Splitters]
* [http://www.tvtropes.org/pmwiki/pmwiki.php/Main/LumperVsSplitter Lumper Vs. Splitter] on [[TV Tropes|TV Tropes, a wiki dedicated to recurring themes in fiction, metafiction, and real life]]

{{DEFAULTSORT:Lumpers And Splitters}}
[[Category:Knowledge representation]]
[[Category:Taxonomy]]</text>
      <sha1>gwgs0a68juiiqoell17o8657zi86siw</sha1>
    </revision>
  </page>
  <page>
    <title>OntoCAPE</title>
    <ns>0</ns>
    <id>26200279</id>
    <revision>
      <id>713113942</id>
      <parentid>561738158</parentid>
      <timestamp>2016-04-02T00:57:09Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor />
      <comment>/* top */Remove blank line(s) between list items per [[WP:LISTGAP]] to fix an accessibility issue for users of [[screen reader]]s. Do [[WP:GENFIXES]] and cleanup if needed. Discuss this at [[Wikipedia talk:WikiProject Accessibility#LISTGAP]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1302" xml:space="preserve">'''OntoCAPE''' is a large-scale [[ontology (computer science)|ontology]] for the [[Domain knowledge|domain]] of [[Computer-Aided Process Engineering]] (CAPE). It can be downloaded free of charge from the [http://www.avt.rwth-aachen.de/AVT/index.php?id=730&amp;L=1 OntoCAPE Homepage].

OntoCAPE is partitioned into 62 sub-ontologies, which can be used individually or as an integrated suite. 
The sub-ontologies are organized across different [[abstraction layer]]s, which separate general knowledge from knowledge about particular domains and applications.

* The upper layers have the character of an [[Upper ontology (information science)|upper ontology]], covering general topics such  as mereotopology, systems theory, quantities and units.
* The lower layers conceptualize the domain of chemical process engineering, covering domain-specific topics such as materials, chemical reactions, or unit operations.

==Further reading==
* Marquardt et al. (2010). [http://www.springer.com/chemistry/book/978-3-642-04654-4 ''OntoCAPE: A Re-Usable Ontology for Chemical Process Engineering'']. Springer-Verlag, Berlin Heidelberg.

== External links ==
* [http://www.avt.rwth-aachen.de/AVT/index.php?id=730&amp;L=1 OntoCAPE Homepage]

[[Category:Knowledge representation]]
[[Category:Ontology (information science)]]</text>
      <sha1>82phacfuq6i7uiq8rog7py0efu7qlk0</sha1>
    </revision>
  </page>
  <page>
    <title>Polythematic Structured Subject Heading System</title>
    <ns>0</ns>
    <id>27847641</id>
    <revision>
      <id>724690730</id>
      <parentid>622097579</parentid>
      <timestamp>2016-06-10T21:14:28Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor />
      <comment>Robot - Speedily moving category Indexing to [[:Category:Index (publishing)]] per [[WP:CFDS|CFDS]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8755" xml:space="preserve">[[Image:PSH logo kratke.gif|thumb|right|PSH logo]]
'''Polythematic Structured Subject Heading System''' (abbreviated as '''PSH''' from the [[Czech language|Czech]] ''Polytematický Strukturovaný Heslář'') is a bilingual Czech–English [[controlled vocabulary]] of [[Index term|subject headings]] developed and maintained by the National Technical Library (the former State Technical Library) in [[Prague]]. It was designed for describing and searching information resources according to their subject. PSH contains more than 13,900 terms, which cover the main fields of human knowledge.

[[Image:Lod-datasets 2010-09-22 colored.png|thumb|The Linking Open Data cloud diagram]]
Thanks to its release in [[Simple Knowledge Organization System|SKOS]], PSH can be used not only for describing [[document]]s in a [[library]], but also for [[Web indexing|indexing web pages]]. Everyone can use PSH for free. PSH is a part of the Linking Open Data cloud diagram (LOD cloud diagaram). The image of the LOD cloud diagram shows datasets that have been published in [[Linked Data]] format, by contributors to the [http://esw.w3.org/topic/SweoIG/TaskForces/CommunityProjects/LinkingOpenData Linking Open Data] community project and other individuals and organisations.

== History and development ==
The PSH preparation project started in 1993, supported by several grants from the Czech Ministry of Culture and Czech Ministry of Education, Youth and Sport. Since 1995, PSH has been used for indexing the State Technical Library’s documents. Starting 1997,&lt;ref&gt;KLOUČKOVÁ, Zdenka. Polytematický strukturovaný heslář Státní technické knihovny. Čtenář. 1997, vol. 49, no. 4, p. 128-129. ISSN 0011-2321.&lt;/ref&gt; PSH has been distributed to other libraries and companies, originally as a commercial, paid product; since 2009&lt;ref&gt;MYNARZ, Jindřich; KAMRÁDKOVÁ, Kateřina; KOŽUCHOVÁ, Kristýna. [http://www.techlib.cz/files/download/id/649/psh-cc.pdf Polythematic Structured Subject Heading System &amp; Creative Commons]. In Seminář ke zpřístupňování šedé literatury [online]. 2008– [retrieved 2010-05-28]. Praha : Státní technická knihovna, 2008.&lt;/ref&gt; for free. In 2000, the State Technical Library received a grant from the Ministry of Culture to translate PSH into English. The next milestone in its development was its releasing in the [[Simple Knowledge Organization System|SKOS]] format, in 2009.&lt;ref name="mynarz"&gt;MYNARZ, Jindřich; KOŽUCHOVÁ, Kristýna; KAMRÁDKOVÁ, Kateřina. [http://www.ikaros.cz/node/5591 Novinky z oblasti Polytematického strukturovaného hesláře]. Ikaros [online]. 2009, vol. 13, no. 7 [retrieved 2010-05-28]. URN-NBN:cz-ik5591. ISSN 1212-5075.&lt;/ref&gt;

The vast majority of new subject headings is suggested and approved by the indexing experts from the National Technical Library. However, the users and public can also make suggestions, using an online form, which are then assessed by the experts. The main decisions about the development and the future of PSH are done by the Committee for Coordination of Polythematic Structured Subject Heading System. The Committee consists of specialists from the National Technical Library and cooperating institutions, and representatives from the libraries and companies which use PSH. The Committee meets once a year in the National Technical Library; in the meantime, the members communicate using an [[electronic mailing list]].&lt;ref name="mynarz" /&gt;

== Browsing PSH ==
[http://psh.ntkcz.cz/skos/ PSH Browser] was released in June 2009. It serves for browsing the PSH system and its distribution in SKOS format. This tool navigates users through PSH from general to specific terms. Users can also use the Search field. [http://pshmanager.techlib.cz/ PSH manager] tool was released in 2012. It serves as an indexing tool especially to catalogers. Catalogers can easy orient in its clear structure. All the terms in PSH manager contain link to the catalogue of NTK. There can be also viewed the record in MARC21 format.

== Autoindexing ==
In 2012 was released beta version of autoindexing application. It is accessible on [http://invenio.ntkcz.cz/indexer/ Autoindexing]. Users enter chosen text into indexing field and activate indexing. In few seconds the terms describing content are displayed.

== PSH structure ==
PSH is a [[tree structure]] with 44 thematic sections. Subject headings are included in a hierarchy of six (or seven) levels according to their [[Semantics|semantic]] content and specificity. There are hierarchical, associative ("see also") and [[equivalence relation|equivalence]] ("see") relations in PSH. Hierarchical relations are represented by broader and narrower terms (e.g. ''physical diagnostic methods'' is broader term to ''electrocardiography'', and on the other hand, ''electrocardiography'' is narrower term to ''physical diagnostic methods''). Equivalence relations link subject headings with their nonpreferred versions (e.g. ''electrocardiography'' and ''ECG''). Moreover, associative relations are used to link related subject headings from different parts of PSH, regardless their affiliation to a section, (e.g. ''electrocardiography'': see also ''cardiology''). Every subject heading belongs to just one section, which has its own two-character abbreviation, assigned to every subject heading of the section. This enables users to recognize affiliation of subject headings from lower levels to the thematic sections. The 44 thematic sections have following [[Tree (data structure)|root nodes]]: 
{{Col-begin}}
{{Col-break}}
* agriculture
* anthropology
* architecture and town planning
* art
* astronomy
* biology
* chemistry
* civil engineering
* communications
* computer technology
* consumer industry
* economic sciences
* electronics
* electrotechnics
* food industry
{{Col-break}}
* generalities
* geography
* geology
* geophysics
* health services
* history
* informatics
* information science
* law
* linguistics
* literature
* mathematics
* mechanical engineering
* metallurgy
* military affairs
{{Col-break}}
* mining engineering
* pedagogy
* philosophy
* physics
* politology
* power engineering
* psychology
* religion
* science and technology
* sociology
* sport
* theory of systems
* transport
* water management
{{Col-end}}

== PSH formats ==
The main format for storage, maintenance and sharing PSH is the [[MARC standards|MARC 21 Format for Authority Data]], which is implemented in [[integrated library system|library automated systems]]. PSH is also available in [[Simple Knowledge Organization System|SKOS]], using [[RDF/XML]] syntax, which is a version suitable for web distribution. Single headings can be accessed on the PSH website through [[Uniform Resource Identifier|URI]] links. Alternatively, the whole vocabulary can be downloaded in one file. It is possible to display tags from PSH ([[metadata]] snippets – [[Dublin Core]] and CommonTag), which can be embedded in an HTML document to provide its semantic description in a machine-readable way.

== New subject headings ==
New subject headings are primarily obtained through the log analysis in the National Technical Library's on-line catalogue of documents, which are the terms used by end-users when searching various documents. Google Analytics service is now used for gaining search queries used by users. Within the data analysis, users queries are divided into seven categories that contain the title of the document, person, subject, action, institution, geographical terms and others. Then the candidates for new preferred terms and non-preferred terms are identified in the subject category.

Users can suggest preferred or non-preferred terms through the [https://www.techlib.cz/en/82958-tech-subject-headings#tab_heading web form] or via e-mail psh(@)techlib.cz.

== PSH &amp; Creative Commons ==
PSH/SKOS has been available under the Creative Commons License CC BY 3.0 CZ (Attribution-ShareAlike 3.0 Czech Republic)since 2011. Users are free to copy, distribute, display and perform the work and make derivative works, but they must give the original author credit and if they alter, transform, or build upon this work, they have to distribute the resulting work only under a licence identical to this one. Users can download all data in one [https://www.techlib.cz/en/82958-tech-subject-headings#tab_documentation zip file], which is continuously updated.

== See also ==
*[[Thesaurus]]
*[[Library of Congress Subject Headings]]
*[[Information retrieval]]
*[[Semantic Web]]

== References ==
{{Reflist}}

== External links ==
* [https://www.techlib.cz/en/82958-tech-subject-headings/ PSH official web page]

[[Category:Index (publishing)]]
[[Category:Library cataloging and classification]]
[[Category:Knowledge representation]]</text>
      <sha1>exph44ywhdttofe4pl7d2t3fsy29kzn</sha1>
    </revision>
  </page>
  <page>
    <title>Procedural reasoning system</title>
    <ns>0</ns>
    <id>8233911</id>
    <revision>
      <id>753455537</id>
      <parentid>749301501</parentid>
      <timestamp>2016-12-07T07:17:16Z</timestamp>
      <contributor>
        <username>Jessicapierce</username>
        <id>2003421</id>
      </contributor>
      <minor />
      <comment>minor copy edits</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9990" xml:space="preserve">In [[artificial intelligence]], a '''procedural reasoning system''' ('''PRS''') is a framework for constructing real-time [[Reasoning system|reasoning systems]] that can perform complex tasks in dynamic environments. It is based on the notion of a [[rational agent]] or [[intelligent agent]] using the [[belief–desire–intention software model]].

A user application is predominately defined, and provided to a PRS system is a set of ''knowledge areas''.  Each knowledge area is a piece of [[procedural knowledge]] that specifies how to do something, e.g., how to navigate down a corridor, or how to plan a path (in contrast with [[robotic architectures]] where the [[computer programmer|programmer]] just provides a model of what the states of the world are and how the agent's primitive actions affect them).  Such a program, together with a PRS [[interpreter (computing)|interpreter]], is used to control the agent.

The interpreter is responsible for maintaining beliefs about the world state, choosing which goals to attempt to achieve next, and choosing which knowledge area to apply in the current situation.  How exactly these operations are performed might depend on domain-specific [[metaknowledge|meta-level]] knowledge areas.  Unlike traditional [[computer planning|AI planning]] systems that generate a complete plan at the beginning, and replan if unexpected things happen, PRS interleaves planning and doing actions in the world.  At any point, the system might only have a partially specified plan for the future.

PRS is based on the [[BDI software agent|BDI]] or belief–desire–intention framework for intelligent agents.  Beliefs consist of what the agent believes to be true about the current state of the world, desires consist of the agent's goals, and intentions consist of the agent's current plans for achieving those goals.  Furthermore, each of these three components is typically ''explicitly'' represented somewhere within the memory of the PRS agent at runtime, which is in contrast to purely reactive systems, such as the [[subsumption architecture]].

== History ==
The PRS concept was developed by the [[Artificial Intelligence Center]] at [[SRI International]] during the 1980s, by many workers including [[Michael Georgeff]], [[Amy L. Lansky]], and [[François Félix Ingrand]]. Their framework was responsible for exploiting and popularizing the BDI model in software for control of an [[intelligent agent]]. The seminal application of the framework was a fault detection system for the reaction control system of the [[NASA]] [[Space Shuttle Discovery]]. Development on this PRS continued at the [[Australian Artificial Intelligence Institute]] through to the late 1990s, which lead to the development of a [[C++]] implementation and extension called [[distributed multi-agent reasoning system|dMARS]].

== Architecture ==
[[Image:PRS.gif|thumb|Depiction of the PRS architecture]]
The system architecture of SRI's PRS includes the following components:
* '''Database''' for beliefs about the world, represented using first order predicate calculus.
* '''Goals''' to be realized by the system as conditions over an interval of time on internal and external state descriptions (desires).
* '''Knowledge areas''' (KAs) or plans that define sequences of low-level actions toward achieving a goal in specific situations.
* '''Intentions''' that include those KAs that have been selected for current and eventual execution.
* '''Interpreter''' or inference mechanism that manages the system.

== Features ==
SRI's PRS was developed for embedded application in dynamic and real-time environments. As such it specifically addressed the limitations of other contemporary control and reasoning architectures like [[expert system]]s and the [[blackboard system]]. The following define the general requirements for the development of their PRS:&lt;ref&gt;
{{cite journal
 | doi = 10.1109/64.180407
 | last = Ingrand
 | first = F.
 |author2=M. Georgeff |author3=A Rao
  | title = An architecture for real-time reasoning and system control
 | journal = IEEE Expert: Intelligent Systems and Their Applications
 | volume = 7
 | issue = 6
 | year = 1992
 | pages = 34–44 
 | publisher = IEEE Press
 | url = http://portal.acm.org/citation.cfm?id=629535.629890 }}
&lt;/ref&gt;

* asynchronous event handling
* guaranteed reaction and response types
* procedural representation of knowledge
* handling of multiple problems
* reactive and goal-directed behavior
* focus of attention
* reflective reasoning capabilities
* continuous embedded operation
* handling of incomplete or inaccurate data
* handling of transients
* modeling delayed feedback
* operator control

== Applications ==
The seminal application of SRI's PRS was a monitoring and fault detection system for the reaction control system (RCS) on the NASA space shuttle.&lt;ref&gt;
{{cite conference
  | last = Georgeff
  | first = M. P.
  |author2=F. F. Ingrand
  | title = Real-time reasoning: the monitoring and control of spacecraft systems
  | booktitle = Proceedings of the sixth conference on Artificial intelligence applications
  | year = 1990
  | pages = 198–204
  | url = http://portal.acm.org/citation.cfm?id=96782 }}
&lt;/ref&gt; The RCS provides propulsive forces from a collection of jet thrusters and controls altitude of the space shuttle. A PRS-based fault diagnostic system was developed and tested using a simulator. It included over 100 KAs and over 25 meta level KAs. RCS specific KAs were written by space shuttle mission controllers. It was implemented on the [[Symbolics]] 3600 Series [[LISP]] machine and used multiple communicating instances of PRS. The system maintained over 1000 facts about the RCS, over 650 facts for the forward RCS alone and half of which are updated continuously during the mission. A version of the PRS was used to monitor the reaction control system on the [[NASA]] [[Space Shuttle Discovery]].

PRS was tested on [[Shakey the robot]] including navigational and simulated jet malfunction scenarios based on the space shuttle.&lt;ref&gt;
{{cite conference
  | last = Georgeff
  | first = M. P.
  |author2=A. L. Lansky
  | title = Reactive reasoning and planning
  | booktitle = Proceedings of the Sixth National Conference on Artificial Intelligence (AAAI-87)
  | year = 1987
  | pages = 198–204
  | url = http://www.ai.sri.com/pubs/files/1364.pdf 
  | work = [[Artificial Intelligence Center]]
  | publisher = [[SRI International]] }}
&lt;/ref&gt; Later applications included a network management monitor called the Interactive Real-time Telecommunications Network Management System (IRTNMS) for [[Telecom Australia]].&lt;ref&gt;
{{cite conference
  | last = Rao
  | first = Anand S.
  |author2=Michael P. Georgeff 
  | title = Intelligent Real-Time Network Management
  | booktitle = Australian Artificial Intelligence Institute, Technical Note 15
  | year = 1991
  | citeseerx = 10.1.1.48.3297 }}
&lt;/ref&gt;

== Extensions ==
The following list the major implementations and extensions of the PRS architecture.&lt;ref&gt;
{{cite conference
  | last = Wobcke
  | first = W. R.
  | title = Reasoning about BDI Agents from a Programming Languages Perspective
  | booktitle = Proceedings of the AAAI 2007 Spring Symposium on Intentions in Intelligent Systems
  | year = 2007
  | url = http://www.cse.unsw.edu.au/~wobcke/papers/ss.07.pdf }}
&lt;/ref&gt;
* UM-PRS &lt;ref&gt;[http://www.marcush.net/IRS/irs_downloads.html]&lt;/ref&gt;
* OpenPRS (formerly C-PRS and Propice) &lt;ref&gt;[http://www.laas.fr/~felix/PRS]&lt;/ref&gt; &lt;ref&gt;[https://softs.laas.fr/openrobots/wiki/openprs]&lt;/ref&gt;
* [[AgentSpeak]]
* [[Distributed Multi-Agent Reasoning System]] (dMARS)
* JAM &lt;ref&gt;[http://www.marcush.net/IRS/irs_downloads.html]&lt;/ref&gt;
* [[JACK Intelligent Agents]]
* SRI Procedural Agent Realization Kit (SPARK) &lt;ref&gt;[http://www.ai.sri.com/~spark/]&lt;/ref&gt;
* PRS-CL &lt;ref&gt;[http://www.ai.sri.com/~prs/]&lt;/ref&gt;

== See also ==
* [[Distributed multi-agent reasoning system]]
* [[JACK Intelligent Agents]]
* [[Belief-desire-intention software model]]
* [[Intelligent agent]]

== References ==
{{reflist}}

==Further reading==
* M.P. Georgeff and A.L. Lansky. "A system for reasoning in dynamic domains: Fault diagnosis on the space shuttle" Technical Note 375, Artificial Intelligence Center, SRI International, 1986.
* Michael P. Georgeff, Amy L. Lansky, Marcel J. Schoppers. "[http://www.ai.sri.com/pubs/files/579.pdf Reasoning and Planning in Dynamic Domains: An Experiment with a Mobile Robot]" Technical Note 380, Artificial Intelligence Center, SRI International, 1987.
* M. Georgeff, and A. L. Lansky (1987). [http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=1457907 Procedural knowledge].  Proceedings of the IEEE 74(10):1383–1398, IEEE Press.
* Georgeff, Michael P.; Ingrand, Francois Felix. "[http://ntrs.nasa.gov/search.jsp?R=124384&amp;id=4&amp;as=false&amp;or=false&amp;qs=Ns%3DArchiveName%257c0%26N%3D4294823185 Research on procedural reasoning systems]" Final Report – Phase 1, Artificial Intelligence Center, SRI International, 1988.
* Michael P. Georgeff and François Félix Ingrand "[http://www.laas.fr/~felix/download.php/ijcai89.pdf Decision-Making in an Embedded Reasoning System]" Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, Detroit (Michigan), August 1989.
* K. L. Myers, [http://www.ai.sri.com/~prs/prs-manual.pdf User Guide for the Procedural Reasoning System] Technical Report, Artificial Intelligence Center, Technical Report, SRI International, Menlo Park, CA, 1997
* [http://www.sti.nasa.gov/tto/Spinoff2006/ch_2.html A Match Made in Space] Spinoff, NASA, 2006

== External links ==
* [http://www.ai.sri.com/~prs/ PRS-CL: A Procedural Reasoning System] An extension to PRS maintained by SRI International

[[Category:Knowledge representation]]
[[Category:Cognitive architecture]]
[[Category:Agent-based software]]
[[Category:Multi-agent systems]]
[[Category:Agent-oriented programming languages]]
[[Category:Agent-based programming languages]]
[[Category:SRI International software]]</text>
      <sha1>8m09j43ht5k9rllvgecnj3x1mpkgue0</sha1>
    </revision>
  </page>
  <page>
    <title>Knowledge value chain</title>
    <ns>0</ns>
    <id>5118075</id>
    <revision>
      <id>742241608</id>
      <parentid>588550636</parentid>
      <timestamp>2016-10-02T14:57:19Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <comment>removed [[Category:Information, knowledge, and uncertainty]], this is a subfield of microeconomics</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2302" xml:space="preserve">A '''[[knowledge value]] chain''' is a sequence of intellectual tasks by which [[knowledge workers]] build their employer's unique competitive advantage &lt;ref&gt;Carlucci, D., Marr, B. and Schiuma, G. (2004) 'The knowledge value chain: how intellectual capital impacts on business performance', ''Int J. Technology Management'', Vol. 27, Nos. 6/7, pp. 575&amp;ndash;690  [http://www.som.cranfield.ac.uk/som/dinamic-content/research/cbp/2004,%20Knowledge%20Value%20Chain%20(IJTM%2027,%206-7,%20Carlucci,%20Marr,%20Schiuma).pdf (pdf)]&lt;/ref&gt; and/or social and environmental benefit. As an example, the components of a research and development project form a knowledge value chain.

Productivity improvements in a knowledge value chain may come from [[knowledge integration]] in its original sense of data systems consolidation. Improvements also flow from the knowledge integration that occurs when [[knowledge management]] techniques are applied to the continuous improvement of a business process or processes.&lt;ref&gt;[http://www.edgeltd.com/performance-consultants-services/edge_service.php?service=3 Canada Edge Performance Consultants] - official page&lt;/ref&gt;

The term first started coming into common use around 1999, appearing in management-related talks and papers.&lt;ref&gt;[http://www.aurorawdc.com/kmworld99.htm 1999 KMWorld conference program], listing Powell's talk on "The Knowledge Value Chain - Aligning       Knowledge Workers with Competitive Strategy"&lt;/ref&gt;&lt;ref&gt;[http://www.ingentaconnect.com/content/mcb/026/2000/00000019/00000009/art00003 "Knowledge value chain"],''The Journal of Management Development'',            Volume 19, Number 9, 2000, pp. 783&amp;ndash;794(12)&lt;/ref&gt;&lt;ref&gt;Tim Powell, "Knowledge Value Chain", May 2001, Proceeding of 22nd National Online Meeting, Information Today ([http://www.knowledgeagency.com/pdf_center/Knowledge_Value_Chain.pdf pdf)]&lt;/ref&gt; It was registered as a trademark in 2004 by TW Powell Co., a [[Manhattan]] company.&lt;ref&gt;[http://www.knowledgeagency.com TW Powell Co. website]&lt;/ref&gt;&lt;ref&gt;U.S. Trademark, December 2004. 2,912,705&lt;/ref&gt;

'''Knowledge value chain processes'''
*Knowledge acquisition
*Knowledge storage
*Knowledge dissemination
*Knowledge application

==References==
{{reflist}}

{{DEFAULTSORT:Knowledge Value Chain}}
[[Category:Knowledge representation]]</text>
      <sha1>8zd225eevivv2s8qnx1tv4fp56yt96x</sha1>
    </revision>
  </page>
  <page>
    <title>FAO Country Profiles</title>
    <ns>0</ns>
    <id>24515769</id>
    <revision>
      <id>754597919</id>
      <parentid>713683980</parentid>
      <timestamp>2016-12-13T14:27:39Z</timestamp>
      <contributor>
        <username>Chris the speller</username>
        <id>525927</id>
      </contributor>
      <minor />
      <comment>punct, number fmt using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="21059" xml:space="preserve">{{ infobox software
| name                   = FAO Country Profiles 
| logo                   = [[File:FAO countryprofiles logo.jpg]]
| caption                = 
| developer              = [[FAO]] of the [[United Nations]]
| latest_release_version = 2012
| latest_release_date    = 2001
| operating_system       = 
| genre                  = [[Knowledge Representation]], [[Ontology]] Editor
| website                = [http://www.fao.org/countryprofiles/ FAO Country Profiles]
}}

The [[FAO]] [[Country]] Profiles is a [[multilingual]]&lt;ref&gt;Arabic, Chinese, English, French, Russian and Spanish are the languages of the Organization. See FAO's Basic texts http://www.fao.org/docrep/010/k1713e/k1713e02b.htm#47. The FAO Country Profiles system provides information in Arabic, Chinese, English, French and Spanish. Russian is in preparation.&lt;/ref&gt; [[web portal]] which repackages the [[Food and Agriculture Organization]] of the United Nations (FAO) vast archive of information on its global activities in [[agriculture]] and [[food security]] in a single area and catalogues it exclusively by [[country]] and thematic areas.

The portal's purpose is to offer decision-makers, [[researchers]] and project formulators around the world a fast and reliable way to access country-specific information on national [[food security]] situations without the need to search individual [[databases]] and [[systems]]. It gives added-value to [[FAO]]'s wealth of information by providing an easy-to-use [[User interface|interface]] containing [[interactive]] [[maps]] and [[charts]].&lt;ref&gt;For reviews of the [[FAO]] Country profiles initiatives, please see the [http://news.eoportal.org/didyouknow/080226_did2.html Sharing Earth and Observations Resources portal], [http://www.sciencecentral.com/category/962945 Science Central], [http://www.scinet.cc/dir/Science/Agriculture/ SciNet Science &amp; Technology Search, News, Articles], etc.&lt;/ref&gt;

== Background ==

[[FAO]] has always highlighted [[information]] and [[Knowledge sharing]] as priority areas in fighting [[hunger]] and achieving [[food security]].&lt;ref&gt;See ARTICLE I of FAO Constitution: The Organization shall collect, analyze, interpret, and disseminate information relating to nutrition, food and agriculture. http://www.fao.org/docrep/x5584E/x5584e0i.htm&lt;/ref&gt; In this context, [[FAO]] identified that [[countries]] could improve their national programmes on [[agriculture]] and [[food security]] if they could access [[FAO]]'s information through a cross-sectoral (or [[interdisciplinary]]) country-based approach.&lt;ref&gt;Programme of Work and Budget 2002–2003:  http://www.fao.org/docrep/meeting/003/y1194e/y1194e06b.htm#P11324_311453&lt;/ref&gt;&lt;ref&gt;Programme of Work and Budget 2004–2005: http://www.fao.org/DOCREP/MEETING/006/y9859e/Y9859e07a.htm#P10820_371793&lt;/ref&gt; However, despite the existence of a large number of country-based [[information systems]] in FAO, the information managed by the various systems lacked [[System integration|integration]]. Information tended to be generated and used in a circumscribed manner and tailored to a specific system, department or [[sector (economic)|sector]].

The [http://www.fao.org/countryprofiles/ FAO Country Profiles] portal, initially called FAO Country Profiles and Mapping Information System, was launched in 2002 responding to the Organization’s need to provide [[FAO]] web site’s users an easy to use mechanism to find FAO country-specific [[information]] without the need to [[Search engine technology|search]] individual [[FAO]] [[web sites]], [[databases]] or [[systems]]. The system was designed to integrate [[Scientific modelling|analytical]] and [[multilingual]] information with thematic databases and [[Digital data|digital]] [[map]] [[Disciplinary repository|repositories]] and to facilitate access to information on multiple factors contributing to national [[food insecurity]].

Since its launch, the system has grown by incorporating more and more [[FAO Country Profiles#Data sources|data sources]]. This was achieved thanks to a [[corporate]] effort to reduce [[information silo]]s and the adoption of [[international standards]] for country-based [[information management]] throughout the Organization.

== Country Profiles ==

The methodology behind the [[FAO]] Country Profiles is rather simple; it links, reuses and repackages data and information from most relevant existing [[FAO]] [[databases]] and [[systems]].

The [[FAO]] Country Profiles covers current FAO Members and Associated Nations.&lt;ref&gt;FAO membership as the 17 November 2007: http://www.fao.org/Legal/member-e.htm&lt;/ref&gt; Once a country is selected, the portal presents to the user [[documents]], news feeds, [[statistical data]], project details and [[maps]] from relevant [[FAO Country Profiles#Data sources|FAO databases and systems]] for the selected [[country]] and categorized according to thematic areas.

The thematic areas are grouped in two categories:

* FAO Core Activities: these correspond to  [[FAO]]'s main areas of expertise, such as, [[natural resources]], [[economics]], [[agriculture]], [[forestry]], [[fisheries]] and technical cooperation. This grouping is based on the work of the corresponding [[FAO]] departments.&lt;ref&gt;For a list of FAO departments and divisions, please see http://www.fao.org/about/depart/en/&lt;/ref&gt;
* Global issues: these are themes that [[FAO]] identified as priority areas for action, and include [[biodiversity]], [[biotechnology]], [[climate change]], [[diseases]] and [[Pest (organism)|pests]], [[emergency]] and aid, [[food security]] and [[food safety|safety]], [[trade]] and [[prices]], [[water management]]. These priority areas correspond to [[FAO]]'s strategic response to a fast-changing world where issues ranging from [[biotechnology]] to [[climate change]] and [[trade]] present new challenges and choices to governments and the general public.

===Data sources ===

Country pages provide access to or integrate the following thematic profiles and systems.&lt;ref name="Inventory of resources"&gt;[http://www.fao.org/countryprofiles/resources.asp Inventory of data sources used in the FAO country profiles]&lt;/ref&gt;

==== FAO data sources ====
* [http://www.fao.org/nr/water/aquastat/countries/index.stm Aquastat Country Profiles]: The AQUASTAT country profiles describe the state of [[water resources]] and [[agricultural]] [[water use]] in the respective country. Special attention is given to [[water resource]], [[irrigation]], and [[drainage]] sub-sectors.
* [http://www.fao.org/biotech/inventory_admin/dep/country_rep_search.asp?lang=en Biotechnology Country Profiles]: The objective of the profiles is to provide a platform on which [[developing country]] [[biotechnology]]-related [[policies]], [[regulations]] and activities can be readily accessed, directing the user to key, updated sources of information.
* [http://www.fao.org/biotech/inventory_admin/dep/default.asp?lang=en BIODEC Biotechnologies in Developing Countries]: FAO-BioDeC is a database meant to gather, store, organize and disseminate, updated baseline information on the state-of-the-art of [[crop]] [[biotechnology]] [[Product (business)|products]] and [[Scientific technique|techniques]], which are in use, or in the pipeline in [[developing countries]]. The database includes about 2000 entries from 70 [[developing countries]], including countries with [[economies in transition]].
* [http://www.fao.org/ag/AGP/AGPC/doc/Counprof/regions/index.htm Country Pasture/Forage Resource Profiles]: The Country [[Pasture]]/[[Forage]] Resource Profile provides a broad overview of relevant general, [[topographical]], [[climatic]] and [[agro-ecological]] information with focus on [[livestock]] production systems and the [[pasture]]/[[forage]] resources.
* [http://www.fao.org/documents FAO Corporate Document Repository]: The FAO Corporate Document Repository houses FAO documents and publications, as well as selected non-FAO publications, in electronic format.
* [http://www.fao.org/tc/tcom/index_en.htm FAO Projects in the country]: From the Field Programme Management Information System.
* [http://www.fao.org/faoterm/ FAO Terminology - Names of Countries]: In order to standardize and harmonize the vast quantity of terms used in FAO documents and publications, the Organization developed the [[terminology]] database [[FAOTERM]]. The Corporate NAMES OF COUNTRIES database also aims at facilitating the consultation and harmonization of country names throughout the Organization.
* [http://www.fao.org/fishery/countryprofiles/search/en Fisheries and Aquaculture Country Profiles]: FAO's [[Fisheries]] and [[Aquaculture]] Department prepares and publishes Fishery and Aquaculture Country Profiles. Each profile summarizes the Department's assessment of activities and trends in fisheries and aquaculture for the country concerned.  Economic and [[demographic data]] are based on [[UN]] or [[World Bank]] sources; data on fisheries are generally those published by the FAO Fisheries and Aquaculture Department.
* [http://www.fao.org/forestry/country/en/ Forestry Country Profiles]: The [[forestry]] country profiles provide detailed information on [[forests]] and the forest sector: [[forest cover]] (types, extent and change), [[forest management]], policies, products and trade, and more - in all some 30 pages for each country in the world.
* [http://www.fao.org/geonetwork/srv/en/main.home FAO-GeoNetwork]: FAO-GeoNetwork is a web-based Geographic Data and Information Management System. It enables easy access to local and distributed [[geospatial information]] catalogues and makes available data, graphics, documents for immediate download. FAO-GeoNetwork holds approximately 5000 standardized [[metadata]] records for digital and paper maps, most of them at the global, continent and national level.
* [http://www.fao.org/giews/english/index.htm Global Information and Early Warning System on Food and Agriculture (GIEWS)]: The System aims to provide policy-makers and policy-analysts with the most up-to-date information available on all aspects of [[food supply]] and demand, warning of imminent [[food crises]], so that timely interventions can be planned.
* [http://www.fao.org/ag/againfo/resources/en/pubs_sap.html Livestock Sector Briefs]: The purpose of the [[Livestock]] Sector Briefs is to provide a concise overview of livestock production in the selected countries through tables, maps and graphs.
* [http://www.fao.org/ag/agn/nutrition/profiles_en.stm Nutrition Country Profiles]: The [[Nutrition]] Country Profiles (NCP) provide concise analytical summaries describing the food and nutrition situation in individual countries.

==== Partnerships data sources ====
* [http://www.agrifeeds.org/ AgriFeeds]: AgriFeeds is a service that allows users to search and filter news and events from several agricultural information sources. It harvests, stores and re-aggregates news and events from feeds published by agricultural organizations and information services.
* [http://www.ipfsaph.org/En/default.jsp International Portal on Food Safety, Animal &amp; Plant Health (IPFSAPH)]: IPFSAPH facilitates trade in food and agriculture by providing a single access point to authorized official international and national information across the sectors of food safety, animal and plant health.  It has been developed by FAO in association with the organizations responsible for international standard setting in sanitary and phytosanitary matters.

==== Non-FAO data sources ====
* [http://earthtrends.wri.org/gsearch.php?kw=country&amp;action=results Earthtrends], [[World Resources Institute]]: EarthTrends is a comprehensive online database, maintained by the World Resources Institute, that focuses on the environmental, social, and economic trends that shape the world. The Earthtrends country profiles present environmental information about key variables for different topic areas.
* International Fund for Agricultural Development ([[IFAD]]): Rural poverty country profiles are produced by IFAD.

== Standards ==

[[File:Geopolitical Ontology in Country Profiles August 12 2009 v 1.png||thumb|200px|right| Geopolitical information section in the FAO Country Profiles.]]

There are various [[international standards]] and [[coding systems]] to manage country information. Historically, systems dealing with different types of data used different coding systems that were tailored to specific data type requirements. For example, [[statistical systems]] in the [[United Nations]] commonly use the M-49 classification and pigmentation&lt;ref&gt;Standard Country or Area Codes for Statistical Use http://unstats.un.org/unsd/methods/m49/m49.htm&lt;/ref&gt; (also known as [[UN]] code) or the [[FAOSTAT]] area classification;&lt;ref&gt;FAOSTAT standardized list of country/territories and groupings: http://faostat.fao.org/site/441/default.aspx&lt;/ref&gt; mapping systems could use [[geographic coordinates]] or [[Global Administrative Unit Layers (GAUL)|GAUL]] codes; textual systems (document repositories or web sites) could use [[ISO 3166-1 alpha-2]], [[ISO 3166-1 alpha-3]] or [[AGROVOC]] keywords; etc.

The FAO Country Profiles provide access to systems managing [[statistics]], [[documents]], [[maps]], [[news feeds]], etc., therefore one of its key aspects to succeed was the mapping of all these [[country codes]].

For this purpose a [[geopolitical ontology]] was developed.&lt;ref&gt;For linking country-based heterogeneous data at [[FAO]], please see:[http://www.semanticuniverse.com/articles-integrating-country-based-heterogeneous-data-united-nations-fao%E2%80%99s-geopolitical-ontology-and Integrating Country-based heterogeneous data at the United Nations: FAO's geopolitical ontology and services.]&lt;/ref&gt; This ontology, among other features, maps [[ISO 3166-1 alpha-2|ISO2]], [[ISO 3166-1 alpha-3|ISO3]], [[AGROVOC]], [[Food and Agriculture Organization Corporate Statistical Database|FAOSTAT]], [http://www.fao.org/faoterm/index.asp?lang=EN FAOTERM], [[Global Administrative Unit Layers (GAUL)|GAUL]], [[UN]], and [[UNDP]] codes for all countries.

== Global Resources ==

Besides the profiles for each country the portal also provides access to other important global resources, such as:

=== Low-Income Food Deficit Countries (LIFDC) ===
The FAO Country Profiles keeps updated for the public the list of [[LIFDC]] countries. This list is revised every year according to the methodology explained below. The new list of the LIFDC,&lt;ref&gt;For an updated list of Low-Income Food Deficit Countries, please check this page: http://www.fao.org/countryprofiles/lifdc/en/&lt;/ref&gt; stands at 62 countries, four less than in the (2012) list. These are: [[Georgia (country)|Georgia]], [[Syrian Arab Republic]], [[Timor-Leste]], [[Republic of Moldova]]. While [[Moldova]] graduated from the list on the basis of net food-exporter criterion, the other graduated based on income criterion.

==== LIFDC methodology====

The classification of a country as low-income food-deficit used for analytical purposes by [[FAO]] is traditionally determined by three criteria:

# A country should have a [[per capita income]] below the "historical" ceiling used by the [[World Bank]]&lt;ref&gt;For operational and analytical purposes, the World Bank’s main criterion for classifying economies is gross national income (GNI) per capita. Classifications are set each year on 1 July. These official analytical classifications are fixed during the World Bank's fiscal year (ending on 30 June), thus countries remain in the categories in which they are classified irrespective of any revisions to their per capita income data. (Source: [[The World Bank]])&lt;/ref&gt; to determine eligibility for [[International Development Association|IDA]] assistance and for 20-year [[IBRD]] terms, applied to countries included in the World Bank categories I and II.&lt;ref&gt;Several important distinctions among member countries are commonly used at the World Bank Group. Countries choose whether they are part of Part I or Part II primarily on the basis of their economic standing. Part I are almost all industrial countries and donors to IDA and they pay their contributions in freely convertible currency. Part II countries are almost all developing countries, some of which are donors to IDA. Part II countries are entitled to pay most of their contribution to IDA in local currency. Please see: "A Guide to the World Bank Group", The World Bank, 2003&lt;/ref&gt; For instance, the historical ceiling of per capita [[gross national income]] (GNI) for 2006, based on the World Bank Atlas method,&lt;ref&gt;Please see: [http://web.worldbank.org/WBSITE/EXTERNAL/DATASTATISTICS/0,,contentMDK:20452009~menuPK:64133156~pagePK:64133150~piPK:64133175~theSitePK:239419~isCURL:Y~isCURL:Y,00.html The World Bank Atlas Method]&lt;/ref&gt; was US$1,735, i.e. higher than the level established for 2005 ($1,675).
# The net food [[trade]]&lt;ref&gt;Net food trade refers to the gross imports less gross exports of food&lt;/ref&gt; position of a country averaged over the preceding three years for which statistics are available, in this case from 2003 to 2005. Trade volumes for a broad basket of basic foodstuffs ([[cereals]], [[root]]s and [[tubers]], [[pulses]], [[oilseeds]] and oils other than tree crop oils, [[meat]] and [[dairy products]]) are converted and aggregated by the [[calorie]] content of individual [[commodities]].
# A self-exclusion criterion is applied when countries that meet the above two criteria specifically request FAO to be excluded from the LIFDC category.

In order to avoid countries changing their LIFDC status too frequently - typically due to short-term, [[exogenous]] shocks - an additional factor was introduced in 2001. This factor, called "persistence of position", would postpone the "exit" of a LIFDC from the list, despite the country not meeting the LIFDC [[income]] criterion or the [[food-deficit]] criterion, until the change in its status is verified for three consecutive years.&lt;ref&gt;For a list of countries and economies sorted by their gross domestic product (GDP) at purchasing power parity (PPP) per capita, please see [[List of countries by GDP (PPP) per capita]]&lt;/ref&gt;

=== FAO Member Countries and Flags ===

The FAO Country Profiles is FAO's source for dissemination of [[FAO]]'s Member Nations and Associated Nations&lt;ref&gt;The list of FAO member countries and date of entry is available at: http://www.fao.org/Legal/member-e.htm&lt;/ref&gt; official flags.&lt;ref&gt;The list of FAO member countries and flags is available at http://www.fao.org/countryprofiles/flags/&lt;/ref&gt; The update of any [[country flag]] is coordinated with the other [[United Nations]] agencies. All flags are made available in a standardized manner which also aims to help web site owners to ensure that they always display the official country flag.

The standard URL for any given country flag would be composed by: the generic URL: "http://www.fao.org/countryprofiles/flags/" to which the [[ISO 3166-1 alpha-2|ISO 3166-1 Alpha-2]] code for the country is added, plus the image format suffix ".gif". For instance, the URL for the [[Argentine flag|Argentina flag]] would be: http://www.fao.org/countryprofiles/flags/AR.gif, with AR being the [[ISO 3166-1 alpha-2]] code of [[Argentina]].&lt;ref&gt;One of several international coding systems (some of the others being: [[ISO2]], [[ISO3]], [[AGROVOC]], [[FAOSTAT]], [[FAOTERM]], [[GAUL]], [[UN]], and [[UNDP]]) for territories and groups.&lt;/ref&gt;

== Criticism ==

Early criticism of the [[FAO]] Country Profiles was that, in its inception phase, it only contained very few resources. Since 2002, the number of available resources has increased to cover country-based information and data, directly linked from [[FAO]]'s web pages or [[FAO]]'s digital repositories.&lt;ref name="Inventory of resources"/&gt; Over the last years, another identified area for improvement was the simplicity of the system methodology, being the resources only linked from country pages and thus, lacking real integration. This need was addressed by starting to integrate additional data, such as, the fisheries charts or the news and events items taken from [[AgriFeeds]]. In addition, in order to provide more complete country profiles, the system started to link or integrate  non-[[FAO]] resources.

== See also ==
* [[Agricultural Information Management Standards]]
* [[AGROVOC]]
* [[Country codes]]
* [[Food and Agriculture Organization]]
* [[Forestry Information Centre]]
* [[Geopolitical ontology]]

== References ==
{{reflist|33em}}

==External links==
* [http://www.fao.org/countryprofiles/default.asp?lang=en FAO Country Profiles]
* [http://www.fao.org/Legal/member-e.htm FAO membership]
* [http://www.fao.org/countryprofiles/lifdc.asp?lang=en Low-Income Food-Deficit Countries (LIFDC)]
* [http://www.fao.org/sids/index_en.asp Small Island Developing States (SIDS)]

{{DEFAULTSORT:Fao Country Profiles}}
[[Category:Agriculture]]
[[Category:Agriculture by country| FAO]]
[[Category:Knowledge representation]]
[[Category:Information systems]]
[[Category:Food and Agriculture Organization]]
[[Category:Country codes]]</text>
      <sha1>43cb1dfm5b7we3lbfs0nllxpjxthyjs</sha1>
    </revision>
  </page>
  <page>
    <title>VoID</title>
    <ns>0</ns>
    <id>20451641</id>
    <revision>
      <id>662546561</id>
      <parentid>662545623</parentid>
      <timestamp>2015-05-16T05:33:53Z</timestamp>
      <contributor>
        <username>Voidxor</username>
        <id>329764</id>
      </contributor>
      <minor />
      <comment>Voidxor moved page [[VoiD]] to [[VoID]]: Capitalization per linked sources</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1026" xml:space="preserve">{{Other uses|Void (disambiguation)}}
{{Multiple issues|
{{Expert-subject|Internet|date=November 2008}}
{{Technical|date=November 2008}}
{{Unreferenced|date=December 2008}}
{{Orphan|date=February 2009}}
{{Underlinked|date=November 2013}}
}}

The '''Vocabulary of Interlinked Datasets''' ('''VoID''') is an [[Resource Description Framework|RDF]] vocabulary, and a set of instructions, that enables the discovery and usage of [[Linked Data|linked data]] sets. A linked dataset is a collection of data, published and maintained by a single provider, available as RDF on the Web, where at least some of the resources in the dataset are identified by dereferencable URIs.

==References==
{{Reflist}}

==External links==
* [http://www.w3.org/TR/void/ Describing Linked Datasets with the VoID Vocabulary, W3C TR]
* [http://semanticweb.org/wiki/VoID VoID at Semantic Web Wiki]

{{Semantic Web}}

{{DEFAULTSORT:Void}}
[[Category:Metadata]]
[[Category:Semantic Web]]
[[Category:Knowledge representation]]
[[Category:XML-based standards]]</text>
      <sha1>ba59t7lk5ix2uuzz6rekn6zlrcmicb5</sha1>
    </revision>
  </page>
  <page>
    <title>Integrated Operations in the High North</title>
    <ns>0</ns>
    <id>22713707</id>
    <revision>
      <id>641067747</id>
      <parentid>623363904</parentid>
      <timestamp>2015-01-05T10:11:53Z</timestamp>
      <contributor>
        <username>Omnipaedista</username>
        <id>8524693</id>
      </contributor>
      <comment>per [[MOS:BOLDSYN]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8436" xml:space="preserve">{{Infobox Organization
|name         = Integrated Operations in the High North
|image        = IOHN logo small.gif
|size         = 200
|alt          = Logo for Integrated Operations in the High North.
|caption      = Logo for Integrated Operations in the High North.
|abbreviation = IOHN or IO High North
|formation    = 2008-05-06
|status       = Project at [[Det Norske Veritas|Det Norske Veritas (DNV)]]
|purpose      = Designing, implementing and testing a Digital Platform for the next generation of [[Integrated Operations]]
|location     = Bærum, Norway
|region_served = Worldwide
|membership   = 22
|language     = English
|leader_title = Project Manager
|leader_name  = [http://www.linkedin.com/in/fredericverhelst Frédéric Verhelst]
|main_organ   = Steering Committee
|affiliations = &lt;!-- if any --&gt;
|num_staff    = 
|num_volunteers =
|budget       = 
|website      = http://www.IOHN.org/
}}
'''Integrated Operations in the High North''' ('''IOHN''', '''IO High North or IO in the High North''') is a unique collaboration project that during a four-year period starting May 2008 is working on designing, implementing and testing a Digital Platform for what in the [[Upstream (oil industry)|Upstream Oil and Gas Industry]] is called the next or second generation of [[Integrated Operations]].&lt;ref&gt;
{{cite web 
|url=http://www.olf.no/getfile.php/zKonvertert/www.olf.no/Rapporter/Dokumenter/070919%20IO%20and%20Ontology%20-%20Brosjyre.pdf 
|title=Integrated Operations and the Oil and Gas Ontology 
|author=The [[Norwegian Oil Industry Association]] (OLF) and POSC Caesar Association (PCA) 
|accessdate=2009-05-06 
|date=2007-09-19
}}&lt;/ref&gt; 
The work on the Digital platform is focussed on capture, transfer and integration of [[Real-time data]] from the remote production installations to the decision makers. A risk evaluation across the whole chain is also included. The platform is based on [[open standards]] and enables a higher degree of [[interoperability]]. Requirements for the digital platform come from use cases defined within the [[Oil_and_gas_well_drilling#Drilling|Drilling]] and [[Oil_and_gas_well_drilling#Completion|Completion]], Reservoir and Production and Operations and Maintenance domains. The platform will subsequently be demonstrated through pilots within these three domains.&lt;ref name="IOHNsite"&gt;
{{cite web 
|url=http://trac.posccaesar.org/wiki/IOHN
|title=Short introduction to the Integrated Operations in the High North (IOHN) project
|author=Integrated Operations in the High North (IOHN) 
|accessdate=2009-05-07 
}}&lt;/ref&gt; 

This new platform is considered an important enabler for safe and sustainable operations in remote, vulnerable and hazardous areas such as the [[Arctic|High North]],&lt;ref&gt;
{{cite web 
|url=http://www.olf.no/news/norway-takes-a-leading-role-in-next-generation-integrated-operations-article18586-291.html 
|title=Norway takes a leading role in next generation Integrated Operations 
|author=The [[Norwegian Oil Industry Association]] (OLF) 
|accessdate=2009-05-07 
|date=2008-08-26
}}&lt;/ref&gt;&lt;ref name="Rigzone"&gt;
{{cite web 
|url=http://www.rigzone.com/news/article.asp?a_id=65883 
|title=Norway Takes Reign to Provide Next Generation Integrated Operations 
|author=Rigzone E&amp;P News
|accessdate=2009-05-08 
|date=2008-08-26
}}&lt;/ref&gt;&lt;ref name="DEJ"&gt;
{{cite web 
|url=http://www.digitalenergyjournal.com/displaynews.php?NewsID=758&amp;PHPSESSID=9hhp6qe4qqpi7qnbgffgqhv1h7
|title=Norway developing next generation Integrated Operations 
|author=Digital Energy Journal
|accessdate=2009-05-08 
|date=2008-08-27
}}&lt;/ref&gt;&lt;ref name="EPMag"&gt;
{{cite web 
|url=http://www.epmag.com/Magazine/2008/12/item24047.php 
|title=Offshore R&amp;D pushes the limits 
|author=E&amp;P Magazine
|accessdate=2009-05-08 
|date=2008-12-02
}}&lt;/ref&gt; but the technology is clearly also applicable in more general applications.

The IOHN project consortium consists of 23 participants,&lt;ref name="IOHNmembers"&gt;
{{cite web 
|url=http://www.posccaesar.org/wiki/IOHN/participants 
|title=List of participating companies in the IOHN project 
|author=[[IOHN]] 
|accessdate=2010-03-10 
|date=2010-03-10
}}&lt;/ref&gt; including operators, service providers, software vendors, technology providers, research institutions and universities. In addition, the [[Norwegian Defence Force]] is working with the project to resolve common infrastructural and [[interoperability]] challenges.&lt;ref name="IOHNsite"/&gt;

The project is managed by [[DNV|Det Norske Veritas (DNV)]].&lt;ref&gt;
{{cite web 
|url=http://www.dnv.com/news_events/news/2008/dnvleadsintegratedoperationsdevelopment.asp
|title=DNV leads Integrated Operations development 
|author=[[Det Norske Veritas]] (DNV) 
|accessdate=2009-05-07 
|date=2008-08-26
}}&lt;/ref&gt; Nils Sandsmark was the project manager during the initiation and start-up phase. Frédéric Verhelst took over as project manager from the beginning of 2009.&lt;ref&gt;
{{cite web 
|url=http://www.linkedin.com/in/fredericverhelst
|title=Profile of Frédéric Verhelst 
|author=[[LinkedIn]]
|accessdate=2009-09-28 
}}&lt;/ref&gt;

Financing comes from the participants and the [[Research Council of Norway]] (RCN) for parts of the project (GOICT&lt;ref&gt;
{{cite web 
|url=http://www.forskningsradet.no/servlet/Satellite?c=Prosjekt&amp;cid=1207296035860&amp;pagename=verdikt/Hovedsidemal&amp;p=1226993814962 
|title=Dependable ICT for the Energy Sector (GOICT, RCN proj.no. 183235, VERDIKT-programme)
|author=The [[Research Council of Norway]] (RCN) 
|accessdate=2009-05-07 
}}&lt;/ref&gt;
and AutoConRig&lt;ref&gt;
{{cite web 
|url=http://www.forskningsradet.no/servlet/Satellite?c=Prosjekt&amp;cid=1198060412649&amp;pagename=ForskningsradetNorsk/Hovedsidemal&amp;p=1181730334233 
|title=Semi-autonomous control system for unmanned drilling rigs (AutoConRig, RCN proj.no. 187473, PETROMAKS-programme)
|author=The [[Research Council of Norway]] (RCN) 
|accessdate=2009-05-07 
}}&lt;/ref&gt;&lt;ref&gt;
{{cite web 
|url=http://www.posccaesar.org/wiki/IOHN/AutoConRig 
|title=RCN/NFR project "AutoConRig"
|author=Jens Ornæs (NOV)
|accessdate=2009-07-02 
}}&lt;/ref&gt;).

== Participants ==
The consortium consists of the following 22 participants&lt;ref name="IOHNmembers"/&gt; (in alphabetical order):&lt;br /&gt;
{| class="wikitable"
|-
| [[ABB Group|ABB]]
| [http://www.abelia.no Abelia]
| [[Baker Hughes]]
| [[Cisco]]
| [http://www.computas.no Computas]
|-
| [[Det Norske Veritas]]
| [[Eni|ENI]]
| [http://www.epsis.no Epsis]
| [[FMC Technologies]]
| [http://www.fsi.no FSI]
|-
| [http://www.ntnu.no/iocenter IO Center]
| [http://www.iris.no IRIS]
| [[National Oilwell Varco]]
| [[Norwegian University of Science and Technology|NTNU]]
| [http://www.olf.no OLF]
|-
| [[POSC Caesar Association]]
| [http://www.ptil.no Petroleum Safety Authority Norway]
| [[Siemens]]
| [[Statoil]]
| [[Norwegian Defence]]
|-
| [[University of Oslo]]
| [[University of Stavanger]]
|}

== See also ==
* [[Integrated Operations]]
* [[Semantic Web]]
* [[ISO 15926]] aka [[Oil and Gas Ontology]], an enabler for the next or second generation of [[Integrated Operations]] by integrating data across disciplines and business domains.
* [[Petroleum exploration in the Arctic]]
* [[POSC Caesar Association]], the custodian of [[ISO 15926]], the [[Oil and Gas Ontology]].

== References ==
{{reflist|2}}

== External links ==
* [http://www.IOHN.org/ Integrated Operations in the High North] website
* [[W3C]] workshop on [http://www.w3.org/2008/12/ogws-report.html Semantic Web in Oil and Gas industry], Houston, December 9–10, 2008. [http://www.w3.org/2008/12/ogws-report#papers Position papers] from several participants in IOHN.
* [http://www.posccaesar.org/wiki/PCA/SemanticDays2009/AboutSemanticDays Semantic Days 2009] conference, Stavanger, May 18–20, 2009. One [http://www.posccaesar.org/wiki/PCA/SemanticDays2009#Session6:SemantictechnologyforIOGeneration2 session] is devoted to IOHN.
* [http://www.ioconf.no/2009/ IO 09 Science and Practice] conference, Trondheim, September 29–30, 2009. One [http://ioconf.no/2009/parallel6 session] is devoted to IOHN.
* [http://www.oilit.com/2journal/2article/1003_16.htm#IOHN Integrated Operations in the High North—mid term report], ''Oil IT Journal'', March 2010.

{{DEFAULTSORT:Integrated Operations In The High North}}
[[Category:Petroleum organizations]]
[[Category:Petroleum engineering]]
[[Category:Semantic Web]]
[[Category:Knowledge engineering]]
[[Category:Information science]]
[[Category:Ontology (information science)]]
[[Category:Knowledge representation]]</text>
      <sha1>89kga6t8z3v8iuyxvjja8a1em9kr6cq</sha1>
    </revision>
  </page>
  <page>
    <title>Ishikawa diagram</title>
    <ns>0</ns>
    <id>57535</id>
    <revision>
      <id>762972655</id>
      <parentid>762972614</parentid>
      <timestamp>2017-01-31T18:43:56Z</timestamp>
      <contributor>
        <ip>130.44.210.14</ip>
      </contributor>
      <comment>/* The 5 Ss (used in service industry) */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5855" xml:space="preserve">{{Cleanup|date=March 2012}}
{{Infobox quality tool
| image =     Cause and effect diagram for defect XXX.svg
| category =  One of the [[Seven Basic Tools of Quality]]
| describer = [[Kaoru Ishikawa]]
| purpose =   To break down (in successive layers of detail) root causes that potentially contribute to a particular effect
}}
'''Ishikawa diagrams''' (also called '''fishbone diagrams''', '''herringbone diagrams''', '''cause-and-effect diagrams''', or '''Fishikawa''') are [[causal diagram]]s created by [[Kaoru Ishikawa]] (1968) that show the [[cause]]s of a specific [[wikt:event|event]].&lt;ref&gt;{{cite book | last = Ishikawa |first = Kaoru | title= Guide to Quality Control | year = 1968 | publisher = JUSE | location = Tokyo
}}&lt;/ref&gt;&lt;ref&gt;{{cite book | last = Ishikawa | first = Kaoru | title = Guide to Quality Control | publisher = Asian Productivity Organization | year =  1976 | isbn = 92-833-1036-5}}&lt;/ref&gt; Common uses of the Ishikawa diagram are [[product design]] and quality defect prevention to identify potential factors causing an overall effect. Each cause or reason for imperfection is a source of variation. Causes are usually grouped into major categories to identify these sources of variation. The categories typically include
*People: Anyone involved with the process
*Methods: How the process is performed and the specific requirements for doing it, such as policies, procedures, rules, regulations and laws
*Machines: Any equipment, computers, tools, etc. required to accomplish the job
*Materials: Raw materials, parts, pens, paper, etc. used to produce the final product
*Measurements: Data generated from the process that are used to evaluate its quality
*Environment: The conditions, such as location, time, temperature, and culture in which the process operates

==Overview==
[[File:Ishikawa Fishbone Diagram.svg|280px|thumb|left|Ishikawa diagram, in fishbone shape, showing factors of Equipment, Process, People, Materials, Environment and Management, all affecting the overall problem. Smaller arrows connect the sub-causes to major causes.]]
Ishikawa diagrams were popularized in the 1960s by [[Kaoru Ishikawa]],&lt;ref&gt;{{cite book |year=2001 |title=Infusion Therapy in Clinical Practice |first=Judy |last=Hankins |pages=42}}&lt;/ref&gt; who pioneered quality management processes in the [[Kawasaki Heavy Industries|Kawasaki]] shipyards, and in the process became one of the founding fathers of modern management.

The basic concept was first used in the 1920s, and is considered one of the [[Seven Basic Tools of Quality|seven basic tools]] of [[quality control]].&lt;ref&gt;{{cite web | url = http://www.asq.org/learn-about-quality/seven-basic-quality-tools/overview/overview.html |first=Nancy R. | last=Tague | title = Seven Basic Quality Tools | year = 2004 | work = The Quality Toolbox | publisher = American Society for Quality | location = Milwaukee, Wisconsin | page = 15 | accessdate = 2010-02-05}}&lt;/ref&gt; It is known as a fishbone diagram because of its shape, similar to the side view of a fish skeleton.

[[Mazda]] Motors famously used an Ishikawa diagram in the development of the [[Miata]] sports car, where the required result was "Jinba Ittai" (Horse and Rider as One — jap. 人馬一体). The main causes included such aspects as "touch" and "braking" with the lesser causes including highly granular factors such as "50/50 weight distribution" and "able to rest elbow on top of driver's door". Every factor identified in the diagram was included in the final design.{{citation needed|date=November 2015}}

==Causes==
Causes in the diagram are often categorized, such as to the 5 M's, described below. Cause-and-effect diagrams can reveal key relationships among various variables, and the possible causes provide additional insight into process behavior.

Causes can be derived from brainstorming sessions. These groups can then be labeled as categories of the fishbone.  They will typically be one of the traditional categories mentioned above but may be something unique to the application in a specific case.  Causes can be traced back to root causes with the [[5 Whys]] technique.

Typical categories are

===The 5 Ms (used in manufacturing industry)===
*Machine (technology)
*Method (process)
*Material (Includes Raw Material, Consumables and Information.)
*Man Power (physical work)/Mind Power (brain work): [[Kaizen]]s, Suggestions
*Measurement (Inspection)
The original 5 Ms used by the Toyota Production System have been expanded by some to include the following and are referred to as the 8 Ms. However, this is not globally recognized. It has been suggested to return to the roots of the tools and to keep the teaching simple while recognizing the original intent; most programs do not address the 8Ms.
*Milieu/Mother Nature(Environment)
*Management/Money Power
*Maintenance

"Milieu" is also used as the 6th M by industries for investigations taking the environment into account.

===The 8 Ps (used in marketing industry)===
*Product/Service
*Price 
*Place
*Promotion
*People/personnel
*Process
*Physical Evidence
*Packaging
The 8 Ps are primarily used in service marketing.

===The 5 Ss (used in service industry)===
*Surroundings
*Suppliers
*Systems
*Standard documentation skills
*Scope of work

==See also==
{{Portal|Thinking}}
* [[Seven Basic Tools of Quality]]
* [[Five whys]]

== References ==

=== Citations ===
{{Reflist|30em}}

=== Sources ===
* Ishikawa, Kaoru (1990); (Translator: J. H. Loftus); ''Introduction to Quality Control''; 448 p; ISBN 4-906224-61-X {{OCLC|61341428}}
* Dale, Barrie G. et al. (2007); ''Managing Quality 5th ed''; ISBN 978-1-4051-4279-3 {{OCLC|288977828}}

==External links==
{{Commons category|Ishikawa diagrams}}

{{DEFAULTSORT:Ishikawa Diagram}}
[[Category:Causal diagrams]]
[[Category:Causality]]
[[Category:Knowledge representation]]
[[Category:Quality control tools]]</text>
      <sha1>7om8evk4zqz0cqydaoc684elmde4smx</sha1>
    </revision>
  </page>
  <page>
    <title>Logico-linguistic modeling</title>
    <ns>0</ns>
    <id>30109665</id>
    <revision>
      <id>739519313</id>
      <parentid>708600196</parentid>
      <timestamp>2016-09-15T04:58:42Z</timestamp>
      <contributor>
        <username>Jonesey95</username>
        <id>9755426</id>
      </contributor>
      <comment>Fix ISSN</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11545" xml:space="preserve">'''Logico-linguistic modeling''' is a method for building knowledge-based systems with a learning capability using [[Conceptual model|Conceptual Models]] from [[Soft systems methods]], modal predicate logic and the Prolog artificial intelligence language.

== Overview==
Logico-linguistic modeling is a six stage method developed primarily for building [[knowledge-based systems]] (KBS), but it also has application in manual decision support systems and information source analysis. Logico-linguistic models have a superficial similarity to Sowa's&lt;ref&gt;Sowa, John F. (1984), ''Conceptual Structures:  Information Processing in Mind and Machine'', Addison-Wesley, Reading, MA, USA.&lt;/ref&gt; [[Conceptual Graphs]], both use bubble style diagrams, both are concerned with concepts, both can be expressed in logic and both can be used in artificial intelligence. However, logico-linguistic models are very different in both logical form and in their method of construction.
 
Logico-linguistic modeling was developed in order to solve theoretical problems found in the Soft Systems method for information system design. The main thrust of the research into has been to show how [[Soft Systems Methodology]] (SSM), a method of systems analysis, can be extended into artificial intelligence.

== Background ==

SSM employs three modeling devices i.e. rich pictures, root definitions, and Conceptual Models of human activity systems. The root definitions and conceptual models are built by stakeholders themselves in an iterative debate organized by a facilitator. The strengths of this method lie, firstly, in its flexibility, the fact that it can address any problem situation, and, secondly, in the fact that the solution belongs to the people in the organization and is not imposed by an outside analyst.&lt;ref name =  "source"&gt;Gregory, Frank Hutson and Lau, Sui Pong (1999) [http://logicalgregory.jimdo.com/publications/logical-ssm-for-isa/ Logical Soft Systems Modelling for Information Source Analysis - The Case of Hong Kong Telecom], Journal of the Operational Research Society, vol. 50 (2).&lt;/ref&gt;

Information Requirements Analysis (IRA)&lt;ref name="Wilson"&gt;Wilson, Brian ''Systems: Concepts, Methodologies and Applications'', John Wiley &amp; Sons Ltd. 1984, 1990. ISBN 0-471-92716-3&lt;/ref&gt;  took the basic SSM method a stage further and showed how the Conceptual Models could be developed into a detailed information system design. IRA calls for the addition of two modeling devices: "Information Categories" which show the required information inputs and outputs from the activities identified in an expanded conceptual model; and the "Maltese Cross" a matrix which shows the inputs and outputs from the information categories and shows where new information processing procedures are required. A completed Maltese Cross is sufficient for the detailed design of a transaction processing system.

The initial impetus to the development of logico-linguistic modeling was a concern with the theoretical problem of how an information system can have a connection to the physical world.&lt;ref&gt;Gregory, Frank Hutson (1995) [[s:Mapping Information Systems onto the Real World|Mapping Information Systems onto the Real World]]. Working Paper Series No. WP95/01. Dept. of Information Systems, City University of Hong Kong.&lt;/ref&gt; This is a problem in both IRA and more established methods (such as [[SSADM]]) because none base their information system design on models of the physical world. IRA designs are based on a notional conceptual model and SSADM is based on models of the movement of documents.

The solution to these problems provided a formula that was not limited to the design of transaction processing systems but could be used for the design of KBS with learning capability.&lt;ref name="know"&gt;Gregory, Frank Hutson (1993) SSM for Knowledge Elicitation &amp; Representation, Warwick Business School Research Paper No. 98 ({{ISSN|0265-5976}}). Later published as Soft Systems Models for Knowledge Elicitation and Representation in Journal of the Operational Research Society (1995) 46, 562-578.&lt;/ref&gt;

== The Six Stages of logico-linguistic modeling==
[[File:Fig 1. SSM model abstracted from Wilson.jpg|thumb|Fig 1. SSM Conceptual Model]]
The logico-linguistic modeling method comprises six stages.&lt;ref name="know"/&gt;

=== 1. Systems Analysis ===

In the first stage logico-linguistic modeling uses SSM for [[systems analysis]]. This stage seeks to structure the problem in the client organization by identifying stakeholders, modelling organizational objectives and discussing possible solutions. At this stage it not assumed that a KBS will be a solution and logico-linguistic modeling often produces solutions that do not require a computerized KBS.

[[Expert systems]] tend to capture the expertise, of individuals in different organizations, on the same topic. By contrast a KBS, produced by logico-linguistic modeling, seeks to capture the expertise of individuals in the same organization on different topics. The emphasis is on the [[elicitation technique|elicitation]] of organizational or group knowledge rather than individual experts. In logico-linguistic modeling the stakeholders become the experts.

The end point of this stage is an SSM style conceptual models such as figure 1.

=== 2. Language Creation ===
[[File:Fig 2. Logico-linguistic Model.jpeg|thumb|Fig 2. Logico-linguistic Model]]

According to the theory behind logico-linguistic modeling the SSM conceptual model building process is a Wittgensteinian [[language-game]] in which the stakeholders build a language to describe the problem situation.&lt;ref&gt;Gregory, Frank Hutson (1992) [[s:SSM to Information Systems: A Wittengsteinian Approach|SSM to Information Systems: A Wittengsteinian Approach. Warwick Business School Research Paper No. 65.]] With revisions and additions this paper was published in Journal of Information Systems (1993) 3, pp.&amp;nbsp;149–168.&lt;/ref&gt; The logico-linguistic model expresses this language as a set of definitions, see figure 2.

=== 3. Knowledge Elicitation===
After the model of the language has been built putative knowledge about the real world can be added by the stakeholders. Traditional SSM conceptual models contain only one logical connective (a necessary condition). In order to represent causal sequences, “[[sufficient condition]]” and “[[necessary and sufficient condition|necessary &amp; sufficient conditions]]” are also required.&lt;ref name="cause2"&gt;Gregory, Frank Hutson (1992) [[s:Cause, Effect, Efficiency &amp; Soft Systems Models|Cause, Effect, Efficiency &amp; Soft Systems Models. Warwick Business School Research Paper No. 42]]. Later published in Journal of the Operational Research Society (1993) 44 (4), pp 149-168&lt;/ref&gt; In logico-linguistic modeling this deficiency is remedied by two addition types of connective. The outcome of stage three is an empirical model, see figure 3.

=== 4. Knowledge Representation ===
[[File:Fig 3. Empirical Model.jpeg|thumb|Fig 3. Empirical Model]]

Modal predicate logic (a combination of [[modal logic]] and [[predicate logic]]) is used as the formal method of knowledge representation. The connectives from the language model are logically true (indicated by the “''L''” modal operator) and connective added at the knowledge elicitation stage are possibility true (indicated by the “''M''” modal operator). Before proceeding to stage 5, the models are expressed in logical formulae.

=== 5. Computer code ===

Formulae in predicate logic translate easily into the [[Prolog]] artificial intelligence language. The modality is expressed by two different types of Prolog rules. Rules taken from the language creation stage of  model building process are treated as incorrigible. While rules from the knowledge elicitation stage are marked as hypothetical rules. The system is not confined to decision support but has a built in learning capability.

=== 6. Verification ===

A knowledge based system built using this method verifies itself. [[Verification and Validation (software)|Verification]] takes place when the KBS is used by the clients. It is an ongoing process that continues throughout the life of the system. If the stakeholder beliefs about the real world are mistaken this will be brought out by the addition of Prolog facts that conflict with  the hypothetical rules. It operates in accordance to the classic principle of [[falsifiability]] found in the philosophy of science&lt;ref&gt;Gregory, Frank Hutson (1996) "The need for "Scientific" Information Systems" Proceedings of the Americas Conference on Information Systems, Aug 1996, Association for Information Systems, 1996. pp. 534-536.&lt;/ref&gt;

== Applications ==
* '''Knowledge-based computer systems'''
Logico-linguistic modeling has been used to produce fully operational computerized knowledge based systems, such as one for the management of diabetes patients in a hospital out-patients department.&lt;ref&gt;Choi, Mei Yee Sarah (1997) Logico-linguistic Modelling for building a Diabetes Mellitus Patient Management Knowledge Based System. M.A. Dissertation, Department of Information Systems, City University of Hong Kong.&lt;/ref&gt;

*'''Manual decision support'''
In other projects the need to move into Prolog was considered unnecessary because the printed logico-linguistic models provided an easy to use guide to decision making. For example, a system for mortgage loan approval&lt;ref&gt;Lee, Kam Shing Clive (1997) The Development of a Knowledge Based System on Mortgage Loan Approval. M.A. Dissertation, Department of Information Systems, City University of Hong Kong.&lt;/ref&gt;

*'''Information source analysis'''
In some cases a KBS could not be built because the organization did not have all the knowledge needed to support all their activities. In these cases logico-linguistic modeling showed shortcomings in the supply of information and where more was needed. For example, a planning department in a telecoms company&lt;ref name =  "source"/&gt;

== Criticism ==
While logico-linguistic modeling overcomes the problems found in SSM's transition from conceptual model to computer code, it does so at the expense of increased stakeholder constructed model complexity. The benefits of this complexity are questionable&lt;ref&gt;Klein, J. H. (1994) Cognitive processes and operational research: a human information processing perspective. Journal of the Operational Research Society. Vol. 45, No. 8.&lt;/ref&gt;
and this modeling method may be much harder to use than other methods.&lt;ref&gt;Klein, J. H. (1995) Over-simplistic cognitive science: A response.  Journal of the Operational Research Society. Vol. 46, No. 4. pp. 275-6.&lt;/ref&gt;

This contention has been exemplified by subsequent research. An attempt by researchers to model buying decisions across twelve companies using logico-linguistic modeling required simplification of the models and removal of the modal elements.&lt;ref&gt;Nakswasdi, Suravut (2004) [http://arrow.unisa.edu.au:8080/vital/access/manager/Repository/unisa:44235 Logical Soft Systems for Modeling Industrial Machinery Buying Decisions in Thailand]. Doctor of Business Administration thesis, University of South Australia.&lt;/ref&gt;

== References ==
{{Reflist}}

== Further reading ==
{{commons category}}
* Gregory, Frank Hutson  (1993) "[http://wrap.warwick.ac.uk/2888/ A logical analysis of soft systems modelling: implications for information system design and knowledge based system design]''. PhD thesis, University of Warwick.

[[Category:Knowledge representation]]
[[Category:Systems analysis]]
[[Category:Modal logic]]</text>
      <sha1>5xwgrr5pzmwoy6327rid64l1cx7bpor</sha1>
    </revision>
  </page>
  <page>
    <title>Issue trees</title>
    <ns>0</ns>
    <id>30713569</id>
    <revision>
      <id>752347555</id>
      <parentid>734051901</parentid>
      <timestamp>2016-11-30T19:36:02Z</timestamp>
      <contributor>
        <username>Joie67</username>
        <id>13011716</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2397" xml:space="preserve">{{Orphan|date=August 2012}}

'''An issue tree''', also called "logic tree" or "issue map", is a graphical breakdown of a question that dissects it into its different components vertically and that progresses into details as it reads to the right.&lt;ref&gt;[https://global.oup.com/academic/product/strategic-thinking-in-complex-problem-solving-9780190463908?q=chevallier&amp;lang=en&amp;cc=us  Chevallier, Arnaud (2016). Strategic Thinking in Complex Problem Solving. Oxford, UK, Oxford University Press. p.47]&lt;/ref&gt;

Issue trees are useful in [[problem solving]] to identify the root causes of a problem as well as to identify its potential solutions. They also provide a reference point to see how each piece fits into the whole picture of a problem.&lt;ref&gt;http://webarchive.nationalarchives.gov.uk/20060213205515/http://strategy.gov.uk/downloads/survivalguide/downloads/ssg_v2.1.pdf&lt;/ref&gt;

There are two types of issue trees: diagnostic ones and solution ones.

Diagnostic trees breakdown a "why" key question, identifying all the possible root causes for the problem.
Solution tree breakdown a "how" key question, identifying all the possible alternatives to fix the problem.

To be effective, an issue tree needs to obey four basic rules:&lt;ref&gt;http://powerful-problem-solving.com/build-logic-trees&lt;/ref&gt;
# Consistently answer a “why” or a “how” question
# Progress from the key question to the analysis as it moves to the right
# Have branches that are mutually exclusive and collectively exhaustive ([[MECE]])
# Use an insightful breakdown

The requirement for issue trees to be collectively exhaustive implies that [[divergent thinking]] is a critical skill.

A profitability tree is an example of an issue tree. It looks at different ways in which a company can increase its profitability. Starting from the key question on the right, it breaks it down between revenues and costs, and break these down into further details.
[[File:An issue tree showing how a company can increase profitability.png|thumb|An issue tree showing how a company can increase profitability]]

==References==
&lt;!--- See http://en.wikipedia.org/wiki/Wikipedia:Footnotes on how to create references using &lt;ref&gt;&lt;/ref&gt; tags which will then appear here automatically --&gt;
{{Reflist}}

[[Category:Articles created via the Article Wizard]]
[[Category:Knowledge representation]]
[[Category:Problem solving methods]]


{{logic-stub}}</text>
      <sha1>qd6z2gk8mviaahvfru8sd992s5gw4zp</sha1>
    </revision>
  </page>
  <page>
    <title>Visual hierarchy</title>
    <ns>0</ns>
    <id>18587056</id>
    <revision>
      <id>748353169</id>
      <parentid>742261112</parentid>
      <timestamp>2016-11-07T20:15:24Z</timestamp>
      <contributor>
        <username>DearPrudence</username>
        <id>410416</id>
      </contributor>
      <comment>Reverting erroneous wording</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3870" xml:space="preserve">'''Visual hierarchy''' refers to the arrangement or presentation of elements in a way that implies importance.&lt;ref&gt;{{cite web|url = http://support.esri.com/en/knowledgebase/GISDictionary/term/visual%20hierarchy|title = GIS Dictionary|accessdate = 2014-08-13|publisher=ESRI}}&lt;/ref&gt; In other words, visual hierarchy influences the order in which the human eye perceives what it sees. This order is created by the visual [[Contrast (vision)|contrast]] between forms in a field of perception. Objects with highest contrast to their surroundings are recognized first by the human mind. The term visual hierarchy is used most frequently in the discourse of the visual arts fields, notably so within the field of [[graphic design]].

==Theory==
The concept of visual hierarchy is based in [[Gestalt psychology|Gestalt psychological theory]], an early 20th-century German theory that proposes that the human brain has innate organizing tendencies that “structure individual elements, shapes or forms into a coherent, organized whole.” &lt;ref&gt;Jackson, Ian. “Gestalt—A  Learning Theory for Graphic Design Education.” ''International Journal of Art and Design Education''. Volume 27. Issue 1 (2008): 63-69. Digital.&lt;/ref&gt; The German word Gestalt translates into “form,” “pattern,” or “shape” in English.&lt;ref&gt;Pettersson, Rune. “Information Design—Principles and Guidelines.” ''Journal of Visual Literacy''. Volume 29. Issue 2 (2010): 167-182. Digital.&lt;/ref&gt; When an element in a visual field disconnects from the ‘whole’ created by the brain’s perceptual organization, it “stands out” to the viewer. The shapes that disconnect most severely from their surroundings stand out the most.

==Physical characteristics==
The brain disassociates objects from one another based upon the differences between their physical characteristics. These characteristics fall into four categories: color, size, alignment, and character. The category of color encompasses the [[hue]], [[Colorfulness|saturation]], [[Lightness (color)|value]], and perceived [[Texture (visual arts)|texture]] of forms. Size describes the surface area of a form. Alignment is the arrangement of forms with respect to their direction, orientation, or pattern.&lt;ref&gt;Feldsted, CJ. ''Design Fundamentals''. New York: Pittman Publishing Corporation, 1950.&lt;/ref&gt; Character is the [[Rectilinear polygon|rectilinearity]] and [[Curvilinear coordinates|curvilinearity]] of forms. Forms that have differences in these characteristics contrast each other.

==Application==
{{Unreferenced section|date=February 2015}}
Visual hierarchy is an important concept in the field of [[graphic design]], a field that specializes in visual organization. Designers attempt to control visual hierarchy to guide the eye to information in a specific order for a specific purpose. One could compare visual hierarchy in graphic design to grammatical structure in writing in terms of the importance of each principle to these fields.

===Examples===
[[Fluorescence|Fluorescent]] color contrasts highly against most naturally occurring colors. Fluorescent substances achieve this contrast by emitting light. Forms of this type of color are almost always high in visual hierarchy. [[Tennis ball]]s are fluorescent green for the perceptual ease of players, match officials, and spectators.

[[Camouflage]] patterns diminish the contrast between themselves and their surroundings. Camouflage describes a form that mimics the physical characteristics of its environment. These patterns are difficult and sometimes impossible to perceive. Certain animals and military forces have both developed their own camouflaged patterns as mechanisms of defense.

==See also==
*[[Bauhaus]]
*[[Cognitive psychology]]
*[[Pattern recognition]]

==References==
{{Reflist}}

[[Category:Page layout]]
[[Category:Knowledge representation]]</text>
      <sha1>ekxy3isiepp2omdcc2sm9v0vg0sdy73</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Thesauri</title>
    <ns>14</ns>
    <id>31375917</id>
    <revision>
      <id>606439556</id>
      <parentid>589035962</parentid>
      <timestamp>2014-04-30T06:29:45Z</timestamp>
      <contributor>
        <username>Good Olfactory</username>
        <id>6454287</id>
      </contributor>
      <comment>cat main</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="186" xml:space="preserve">{{cat main|Thesaurus}}
{{Commons category|Thesauri}}
[[Category:Knowledge representation]]
[[Category:Reference works]]
[[Category:Dictionaries by type]]
[[Category:Information science]]</text>
      <sha1>aryv16kjjb3ecg8d80oj35wq3ch301f</sha1>
    </revision>
  </page>
  <page>
    <title>Pinakes</title>
    <ns>0</ns>
    <id>19391789</id>
    <revision>
      <id>762495998</id>
      <parentid>744589821</parentid>
      <timestamp>2017-01-29T05:06:37Z</timestamp>
      <contributor>
        <username>Jg2904</username>
        <id>11940553</id>
      </contributor>
      <minor />
      <comment>/* Description */ Replaced "thusly" with "thus."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7418" xml:space="preserve">{{italic title}}
{{hatnote|"Pinakes" may be plural of [[pinax]], a votive tablet that served as a votive object deposited in a sanctuary or burial chamber.}}
[[Image:Ancientlibraryalex.jpg|thumb|Imaginary depiction of the [[Library of Alexandria]]]]
The '''''Pinakes''''' ({{lang-grc|Πίνακες}} "tables", plural of {{lang|grc|[[wikt:πίναξ|πίναξ]]}}) was a [[bibliography|bibliographic]] work composed by [[Callimachus]] (310/305–240 BCE) that is popularly considered to be the first [[library catalog]]; its contents were based upon the holdings of the [[Library of Alexandria]] during Callimachus' tenure there during the third century BCE.&lt;ref&gt;N. Krevans 2002: 173&lt;/ref&gt;

==History==

The Library of Alexandria had been founded by [[Ptolemy I Soter]] about 306 BCE. The first recorded librarian was [[Zenodotus]] of Ephesus. During Zenodotus' tenure, Callimachus, who was never the head librarian, compiled the ''Pinakes'', thus becoming the first bibliographer and the scholar who organized the library by authors and subjects about 245 BCE.&lt;ref&gt;Neil Hopkinson, ''A Hellenistic Anthology'' (CUP, 1988) 83.&lt;/ref&gt;&lt;ref name ="alexandria3"&gt;{{cite web|url= http://www.greekplanet.com.au/forum/lofiversion/index.php/t486.html|title=
Greek Inventions|accessdate= 2008-09-19}}&lt;/ref&gt; His work was 120 volumes long.&lt;ref&gt;Hopkinson&lt;/ref&gt;

[[Apollonius of Rhodes]] was the successor to Zenodotus. [[Eratosthenes]] of Cyrene succeeded Apollonius in 235 BCE and compiled his ''tetagmenos epi teis megaleis bibliothekeis'', the "scheme of the great bookshelves." In 195 BCE [[Aristophanes of Byzantium]] was the librarian and updated the ''Pinakes'',&lt;ref&gt;Pfeiffer, R. ''History of Classical Scholarship from the Beginnings to the End of the Hellenistic Age'' (OUP, 1968) 133.&lt;/ref&gt; although it is also possible that his work was not a supplement of Callimachus' ''Pinakes'' themselves, but an independent polemic against, or commentary upon, their contents.&lt;ref&gt;Slater, W.J. "Grammarians on Handwashing", ''Phoenix'' 43 (1989) 100&amp;ndash;11, at 102.&lt;/ref&gt;

==Description==

The collection at the Library of Alexandria contained nearly 500,000 [[papyrus]] scrolls, which were grouped together by subject matter and stored in bins.&lt;ref&gt;P.J. Parson, "Libraries", in the ''Oxford Classical Dictionary'', 3rd ed. (OUP, 1996) describes the evidence for the size of the library's holdings thus: "The first Ptolemies (see Ptolemy (1) ) collected ambitiously and systematically; the Alexandrian Library (see ALEXANDRIA (1) ) became legend, and *Callimachus (3)'s ''Pinakes'' made its content accessible. There were rivals at *Pella, *Antioch (1) (where *Euphorion (2) was librarian), and especially *Pergamum. Holdings were substantial: if the figures can be trusted, Pergamum held at least 200,000 rolls (Plut. ''Ant.'' 58. 9), the main library at Alexandria nearly 500,000 (*Tzetzes, ''Prolegomena de comoedia'' 11a. 2. 10–11 Koster)&amp;mdash;the equivalent, perhaps, of 100,000 modern books."&lt;/ref&gt; Each bin carried a label with painted tablets hung above the stored papyri. ''Pinakes'' was named after these tablets and are a set of index lists. The bins gave bibliographical information for every roll.&lt;ref&gt;Phillips, Heather A., [http://unllib.unl.edu/LPP/phillips.htm "The Great Library of Alexandria?". Library Philosophy and Practice, August 2010]&lt;/ref&gt; A typical entry started with a title and also provided the author's name, birthplace, father's name, any teachers trained under, and educational background. It contained a brief biography of the author and a list of the author's publications. The entry had the first line of the work, a summary of its contents, the name of the author, and information about the origin of the roll.&lt;ref name ="alexandria4"&gt;{{cite web|url= http://www.greece.org/hec01/www/arts-culture/alexandria/library/library11.htm|title= The Pinakes|accessdate= 2010-05-29}}&lt;/ref&gt;

Callimachus' system divided works into six [[genres]] and five sections of prose: rhetoric, law, epic, tragedy, comedy, lyric poetry, history, medicine, mathematics, natural science and miscellanies. Each category was alphabetized by author.

Callimachus composed two other works that were referred as ''pinakes'' and were probably somewhat similar in format to the ''Pinakes'' (of which they "may or may not be subsections"&lt;ref&gt;Nita Krevans, "Callimachus and the Pedestrian Muse," in M.A. harder et al., eds., ''Callimachus II'' (Hellenistica Groningana 7), 2002, p. [https://books.google.com/books?id=CL4A5I3K-KsC&amp;lpg=PA173&amp;dq=callimachus%20democritus%20catalog&amp;pg=PA173#v=onepage&amp;q&amp;f=false 173] n. 1.&lt;/ref&gt;), but were concerned with individual topics. These are listed by the ''[[Suda]]'' as: ''A Chronological Pinax and Description of [[Theatre director#The director in theatre history|Didaskaloi]] from the Beginning'' and ''Pinax of the Vocabulary and Treatises of [[Democritus]]''.&lt;ref&gt;[http://www.stoa.org/sol-entries/kappa/227 ''Suda'' On Line]&lt;/ref&gt;

==Later bibliographic ''pinakes''==
The term ''pinax'' was used for bibliographic catalogs beyond Callimachus. For example, [[Ptolemy-el-Garib]]'s catalog of [[Aristotle]]'s writings comes to us with the title ''Pinax (catalog) of Aristotle's writings''.&lt;ref&gt;[[Ingemar Düring]], ''Aristotle in the Ancient Biographical Tradition'' (Göteborg 1957), p. 221.&lt;/ref&gt;

==Legacy==
The ''Pinakes'' proved indispensable to librarians for centuries. They became a model to use all over the [[Mediterranean]]. Their later influence can be traced to medieval times, even to the Arabic counterpart of the tenth century: [[Ibn al-Nadim]]'s ''Al-Fihrist'' ("Index"). Variations on this system were used in libraries until the late 1800s when [[Melvil Dewey]] developed the [[Dewey Decimal Classification]] in 1876, which is still in use today.&lt;ref name ="alexandria4"/&gt;

==Notes==
{{reflist|35em}}

==Bibliography==

===Texts and translations===
* The evidence concerning the Pinakes is collected by [[Rudolf Pfeiffer]] (ed.), ''Callimachus, vol. I: Fragmenta'', Oxford: Clarendon Press 1949, frr. 429-456 (with reference to the most important literature).
* Witty, F. J. "The Pinakes of Callimachus", ''Library Quarterly'' 28:1/4 (1958), 132&amp;ndash;36.
* Witty, F. J. "The Other Pinakes and Reference Works of Callimachus", ''Library Quarterly'' 43:3 (1973), 237&amp;ndash;44.

===Studies===
* [[Roger S. Bagnall|Bagnall, R. S.]] [http://archive.nyu.edu/bitstream/2451/28263/2/D172-Alexandria%20Library%20of%20Dreams.pdf "Alexandria: Library of Dreams"], ''Proceedings of the American Philosophical Society'' 46 (2002) 348&amp;ndash;62.
* Blum, R. ''Kallimachos. The Alexandrian Library and the Origins of Bibliography'', trans. H.H. Wellisch (U. Wisconsin, 1991). ISBN 978-0-299-13170-8.
* Krevans, N. [https://books.google.com/books?id=CL4A5I3K-KsC&amp;lpg=PA173&amp;dq=callimachus%20democritus%20catalog&amp;pg=PA173#v=onepage&amp;q&amp;f=false "Callimachus and the Pedestrian Muse"], in: A. Harder et al. (eds.) ''Callimachus II'', Hellenistic Groningana 6 (Groningen, 2002) 173&amp;ndash;84.
* West, M. L. "The Sayings of Democritus", ''Classical Review'' (1969) 142.

{{coord missing|Egypt}}

{{Callimachus}}

[[Category:Defunct libraries]]
[[Category:Libraries in Egypt]]
[[Category:3rd-century BC books]]
[[Category:History of museums]]
[[Category:Ptolemaic Alexandria]]
[[Category:Library cataloging and classification]]
[[Category:Knowledge representation]]
[[Category:Bibliographies]]</text>
      <sha1>0uobw8vptn7r0f8mkoffjd41a974uvc</sha1>
    </revision>
  </page>
  <page>
    <title>Personal knowledge base</title>
    <ns>0</ns>
    <id>33562977</id>
    <revision>
      <id>755569068</id>
      <parentid>749153229</parentid>
      <timestamp>2016-12-18T22:01:30Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <minor />
      <comment>/* Spatial */ rm spaces</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="82612" xml:space="preserve">{{about|knowledge management software|the general concept|Personal knowledge management}}
{{Copypaste|url=http://www.cs.colorado.edu/department/publications/reports/docs/CU-CS-997-05.pdf|date=June 2016}}
A '''personal knowledge base''' ('''PKB''') is an electronic tool used to express, capture, and later retrieve the personal knowledge of an individual. It differs from a traditional [[database]] in that it contains subjective material particular to the owner, that others may not agree with nor care about. Importantly, a PKB consists primarily of knowledge, rather than [[information]]; in other words, it is not a collection of documents or other sources an individual has encountered, but rather an expression of the distilled knowledge the owner has extracted from those sources.&lt;ref name = "Davies 2005"/&gt;&lt;ref name = "Davies 2011"/&gt;

== Definition ==

The term ''personal knowledge base'' was mentioned as early as the 1980s,&lt;ref name = "Brooks 1985"/&gt;&lt;ref name = "Kruger 1986"/&gt;&lt;ref name = "Forman 1988"/&gt;&lt;ref name = "Smith 1991"/&gt; but the term came to prominence when it was described at length in publications by computer scientist Stephen Davies and colleagues,&lt;ref name = "Davies 2005"/&gt;&lt;ref name = "Davies 2011"/&gt; who defined the term as follows:{{efn|An earlier version of this article incorrectly stated that the term ''personal knowledge base'' was coined in 2011; in fact, Stephen Davies and colleagues wrote a paper on the subject in 2005,&lt;ref name = "Davies 2005"/&gt; and publications by other authors had mentioned the term as early as the 1980s.&lt;ref name = "Brooks 1985"/&gt;&lt;ref name = "Kruger 1986"/&gt;&lt;ref name = "Forman 1988"/&gt;&lt;ref name = "Smith 1991"/&gt; Much of the present article closely follows the publications by Davies and colleagues.}}

* '''personal''': a PKB is intended for private use, and its contents are custom-tailored to the individual. It contains trends, relationships, categories, and personal observations that its owner perceives but which no one else may agree with. It can be shared, just as one can explain one's own opinion to a hearer, but it is not jointly ''owned'' by anyone else any more than explaining one's opinion to a friend causes the friend to own one's mind.
* '''knowledge''': a PKB contains knowledge, not merely information. Its purpose is not simply to aggregate all the information sources one has seen, but to preserve the knowledge that one has ''learned'' from those sources. When a user returns to a PKB to retrieve knowledge she has stored, she is not merely pointed back to the original documents, where she must relocate, reread, reparse, and relearn the relevant passages. Instead, she is returned to the distilled version of the particular truth she is seeking, so that the mental model she originally had in mind can be easily reformed.
* '''base''': a PKB is a consolidated, integrated knowledge store. It is a reflection of its owner's memory, which, as Bush and many others have observed, can freely associate any two thoughts together, without restriction. Hence a PKB does not attempt to partition a user's field of knowledge into multiple segments that cannot reference one another. Rather, it can connect any two concepts without regard for artificial boundaries, and acts as a single, unified whole.

=== Contrast with other classes of systems ===

The following classes of systems ''cannot'' be classified as PKBs:&lt;ref name = "Davies 2005"/&gt;&lt;ref name = "Davies 2011"/&gt;

* collaborative efforts to build a universal objective space (as opposed to an individual's personal knowledge.) The World Wide Web itself is in this category, as were its predecessors HyperTIES&lt;ref name = "Schneiderman 1987"/&gt; and Xanadu,&lt;ref name = "Nelson 1987"/&gt; Web categorization systems like the [[Open Directory Project]], and collaborative information collections like [[Wikipedia]].
* search systems like Enfish and the Stuff I've Seen project&lt;ref name="Dumais et al 2003" /&gt; that index and search one's information sources on demand, but do not give the user the ability to craft and express personal knowledge.
* tools whose goal is to produce a design artifact rather than to maintain knowledge for its own sake. Systems like ART&lt;ref name="Nakakoji et al 2000" /&gt; and Writing Environment&lt;ref name="Smith et al 1987" /&gt; use intermediate knowledge representations as a means to an end, abandoning them once a final artifact has been produced, and hence are not suitable as PKBs.
* systems that focus on capturing transient information, rather than archiving knowledge that has long-term value. Examples would be Web logs&lt;ref name="Godwin-Jones 2003"/&gt; and e-diaries.&lt;ref name="Kovalainen et al 1998" /&gt;  Tools whose information domain is mostly limited to [[time management]] tasks (calendars, action items, contacts, etc.) rather than "general knowledge". Blandford and [[Thomas R.G. Green|Green]]&lt;ref name="Blandford and Green 2001" /&gt; and Palen&lt;ref name="Palen 1999" /&gt; give excellent surveys; common commercial examples would be [[Microsoft Outlook]], [[IBM Lotus Notes|Lotus Notes]], and [[Evolution (software)|Novell Evolution]].
* similarly, tools developed for a specific domain, such as bibliographic research rather than for "general knowledge".

==== Personal information management ====

PKM is similar to [[personal information management]], but is a distinct topic based on the "information" vs. "knowledge" difference. PKBs are about recording and managing the knowledge one derives from documents, whereas PIM is more about managing and retrieving the documents themselves.&lt;ref name = "Davies 2005"/&gt;&lt;ref name = "Davies 2011"/&gt;

== Historical influences ==

Non-electronic personal knowledge bases have probably existed in some form since the dawn of written language: [[Leonardo da vinci#Journals and notes|Da Vinci's notebooks]] are a famous example. More commonly, card files and personal annotated libraries have served this function in the pre-electronic age.

=== Bush's Memex ===

Undoubtedly the most famous early formulation of an electronic PKB was [[Vannevar Bush]]'s description of the "[[Memex]]" in 1945.&lt;ref name="Bush 1945" /&gt; Bush surveyed the post-World-War-II landscape and laid out what he viewed as the most important forthcoming challenges to humankind in ''[[The Atlantic Monthly]]''. The Memex was a theoretical (never implemented) design or a system to help tackle the [[information overload]] problem, already formidable in 1945. In Bush's own words:

&lt;blockquote&gt;Consider a future device for individual use, which is a sort of mechanized private file and library. ... [A] device in which an individual stores all his books, records, and communications, and which is mechanized so that it may be consulted with exceeding speed and flexibility. It is an enlarged intimate supplement to his memory.  &lt;/blockquote&gt;

Bush envisioned collaborative aspects as well, and even a world-wide system that scientists could freely consult.{{Citation needed|date=November 2012}} But an important emphasis throughout the article was on expanding our own powers of recollection: "Man needs to mechanize his record more fully," he says, if he is not to "become bogged down...by overtaxing his limited memory". With the Memex, the user could "add marginal notes and comments," and "build a trail of his interest" through the larger information space. She could share trails with friends, identify related works, and create personal annotations. Bush's Memex would give each individual the ability to create, categorize, classify, and relate his own set of information corresponding to his unique personal viewpoint.  Much of that information would in fact consist of bits and pieces from public documents, just as the majority of the knowledge inside our own heads has been imbibed from what we read and hear. But the Memex also allowed for the specialized recording of information that each individual perceived and needed to retain. The idea of supplementing our memory" was not a one-size-fits-all proposition, since no two people have the same interests, opinions, or memories.  Instead, it demanded a subjective expression of knowledge, unique to each individual.

=== Graphical knowledge capture tools ===

Great emphasis is placed on the pictorial nature of diagrams to represent abstract knowledge; the use of spatial layout, color, and images is said to strengthen understanding and promote creativity. Each of the three primary schools—[[mind map]]ping, [[concept map]]ping, and [[cognitive map]]ping—prescribes its own data model and procedures, and each boasts a number of software applications designed specifically to create compatible diagrams.

==== Mind mapping ====
[[Mind map]]ping was promoted by pop psychologist [[Tony Buzan]] in the 1960s, and commands the allegiance of an impressive number of adherents worldwide.  A mind map is essentially nothing more than a visual outline, in which a main idea or topic is written in the center of the diagram, and subtopics radiate outwards in increasing levels of specificity. The primary value is in the freeform, spatial layout (rather than a sequential, numbered outline), the ability for a software application to hide or reveal select levels of detail, and as mentioned above, graphical adornments. The basic data model is a [[Tree (graph theory)|tree]], rather than a [[Graph (discrete mathematics)|graph]], with all edges implicitly labeled "supertopic/subtopic". Numerous tools are available for constructing mind maps.

==== Concept mapping ====

[[Concept map]]s were developed by Cornell Professor [[Joseph D. Novak|Joseph Novak]],&lt;ref name="Novak 2003" /&gt; and based on [[David Ausubel]]'s assimilation theory of learning.&lt;ref name="Ausubel 1968" /&gt; An essential tenet is that newly encountered knowledge must be related to one's prior knowledge in order to be properly understood. Concept maps help depict such connections graphically. Like mind maps, they feature evocative words or phrases in boxes connected by lines. There are two principal differences, however: first, a concept map is properly a graph, not a tree, permitting arbitrary links between nodes rather than only parent/child relationships; and second, the links are labeled to identify the nature of the inter-concept relationship, typically with a verb phrase. In this way, the links on a diagram can be read as English sentences, with the upstream node as the subject and the downstream node as the direct object of the sentence.

There are many applications available that could be used for drawing these diagrams, not all of which directly acknowledge their support for concept maps in particular.&lt;ref name="Canas et al 2005"/&gt;&lt;ref name="Gaines and Shaw 1995" /&gt;

A concept map is virtually identical to the notion of a "[[semantic network]]",&lt;ref name="Woods 1985" /&gt; which has served as a cornerstone for much artificial intelligence work since its inception. Semantic networks, too, are directed graphs in which the nodes represent concepts and labeled edges the relationships between them. Much psychology research has strengthened the idea that the human mind internalizes knowledge in something very like this sort of framework. This likely explains the ease with which concept mapping techniques have been adopted by the uninitiated, since concept maps and semantic networks can be considered equivalent.

==== Cognitive mapping ====

[[Cognitive mapping]], developed by Fran Ackermann and Colin Eden at the University of Strathclyde, uses the same data model as does concept mapping, but with a new set of techniques. In cognitive maps, element names have two parts, separated by an ellipsis that is read "as opposed to" in order to further clarify the semantics of the node. ("Cold...hot" is different from "cold...freezing," for example.) Links are of three types—causal, temporal, connotative—the first of which is the most common and is read as "may lead to". Generally cognitive mapping is best suited to domains involving arguments and [[decision making]]. Cognitive mapping is not nearly as widespread as the other two paradigms. Together, these and related methods have brought into the mainstream the idea of breaking down knowledge into its fundamental elements, and representing them graphically. Students and workers from widely diverse backgrounds have experienced success in better articulating and examining their own knowledge, and in discovering how it relates to what else they know. Although architectural considerations prevent any of these tools from functioning as bona fide PKBs, the ideas they have contributed to a front-end interface mechanism cannot be overestimated.

=== Hypertext systems ===

Many in the hypertext community [[Hypertext#History|reference]] Vannevar Bush's article as the cornerstone of their heritage. Hence the development of hypertext techniques, while seldom applied specifically towards PKB solutions, is important.  There have basically been three types of hypertext systems: those that exploit features of non-linear text to create a dynamic, but coherent "hyperdocument";&lt;ref name="Schneiderman 1987" /&gt;&lt;ref name="Goodman 1988" /&gt; those
that prescribe ways of linking existing documents together for navigation and expression of affinities;&lt;ref name="Davis et al 1993" /&gt;&lt;ref name="Garrett et al 1986" /&gt;&lt;ref name="Pearl 1989" /&gt; and those that use the hypertext model specifically to model abstract knowledge. Though the first and especially the second category have dominated research efforts (and public enthusiasm) over the past several decades, it is this third class that is closest in spirit to the original vision of hypertext by its founders.

In a similar vein to [[Vannevar Bush|Bush]], [[Doug Engelbart]]'s focus was to develop computer systems to "help people think better".  He sought data models that more closely paralleled the human thought process, and settled on using hypertext as a way to represent and store abstract human knowledge. Although his "[[NLS (computer system)|Augment]]" system underwent many changes, the original purpose closely aligned with that of PKBs.&lt;ref name="Engelbart 1953"/&gt;

More recently, Randall Trigg's TextNet&lt;ref name="Trigg and Weiser 1986" /&gt; and [[NoteCards]]&lt;ref name="Halasz et al 1987" /&gt; systems further explored this idea. TextNet revolved around "primitive pieces of text connected with typed links to form a network similar in many ways to a semantic network".&lt;ref name="Conklin and Begeman 1988"/&gt; Though text-centric, it was clear that Trigg's goal was to model the associations between primitive ideas and hence to reflect the mind's understanding. "By using...structure, meaning can be extracted from the relationships between chunks (small pieces of text) rather than from the words making them up."&lt;ref name="Trigg and Weiser 1986" /&gt; The subsequent [[NoteCards]] effort was similarly designed to "formulate, structure, compare, and manage ideas". It was useful for "analyzing information, constructing models, formulating arguments, designing artifacts, and generally processing ideas".

Conklin and Begeman's [[gIBIS]] system was another early effort into true knowledge representation, specifically for the field of design deliberations and arguments.&lt;ref name="Conklin and Begeman 1988"/&gt; The project lived on in the later project QuestMap&lt;ref name="Selvin 1999" /&gt; and the more modern [[Compendium (software)|Compendium]], which has been primarily used for capturing group knowledge expressed in face-to-face meetings. In all these cases, systems use semantic hypertext in an attempt to capture shared knowledge in its most basic form. Other examples of knowledge-based hypertext tools include Mental Link,&lt;ref name="Dede and Jayaram 1990" /&gt; Aquanet,&lt;ref name="Marshall et al 1991" /&gt; and SPRINT,&lt;ref name="Carlson and Ram 1990" /&gt; as well
as a few current commercial tools such as [[PersonalBrain]] and [[Tinderbox (application software)|Tinderbox]]&lt;ref name="Bernstein 2003" /&gt; and open source tools such as [[TiddlyWiki]].

=== Note-taking applications ===

[[Electronic Notetaking|Note-taking applications]] allow a user to create snippets of text and then organize or categorize them in some way. These tools can be used to form PKBs that are composed of such text snippets.

Most of these tools are based on a [[Tree (graph theory)|tree]] hierarchy, in which the user can write pages of notes and then organize them into sections and subsections. The higher level sections or chapters often receive a colored tab exactly as a physical three-ring notebook might. Other designers eschew the tree model for a more flexible category-based approach (see section [[#Data models|data models]]). The primary purpose of all these tools is to offer the benefits of freeform note-taking with none of the deficiencies: users are free to brainstorm and jot down anything from bullet points to polished text, while still being able to search, rearrange, and restructure the entire notebook easily.

An important subcategory of note-taking tools is outliners (e.g., [[OmniOutliner]]), or applications specifically designed to organize ideas in a hierarchy. These tools typically show a two-pane display with a tree-like navigation widget in the left-pane and a list of items in the right-pane. Topics and subtopics can be rearranged, and each outline stored in its own file. Modern outliners feature the ability to add graphics and other formatting to an item, and even hyper links to external websites or documents. The once abandoned (but now resurrected) Ecco system was among the first to allow items to have typed attributes, displayed in columns. This gives the effect of a custom spreadsheet per topic, with the topic's items as rows and the columns as attributes. It allows the user to gracefully introduce structure to their information as it is identified.

Of particular interest are applications optimized for subsuming portions of an information space realm into a PKB, where they can be clustered and arranged according to the user's own perceptions. The Virtual Notebook System (VNS)&lt;ref name="Burger et al 1991" /&gt; was one of the first to emphasize this. VNS was designed for sharing information among scientists at the Baylor College of Medicine; a user's "personal notebook" could make references to specific sections of a "community notebook," and even include arbitrary segments of other documents through a cut-and-paste mechanism.

=== Document management systems ===
{{main|Document management system}}

Another influence on PKBs are systems whose primary purpose is to help users organize documents, rather than personal knowledge derived from those documents. Such systems do not encode subjective knowledge per se, but they do create a personal knowledge base of sorts by allowing users to organize and cross-reference their information artifacts.

These efforts provide alternative indexing mechanisms to the limited "directory path and file name" approach. Presto&lt;ref name="Dourish et al 1999" /&gt; replaces the directory hierarchy entirely with attributes that users assign to files. These key-value pairs represent user-perceived properties of the documents, and are used as a flexible means for retrieval and organization. William Jones' Memory Extender&lt;ref name="Jones 1986" /&gt; was similar in spirit, but it dynamically varied the "weight" of a file's keywords according to the user's context and perceived access patterns. In [[Haystack (MIT project)|Haystack]],&lt;ref name="Adar et al 1999" /&gt; users—in conjunction with automated software agents—build a graph-based network of associative links through which documents can be retrieved.

Metadata and multiple
categorization can also be applied to provide multiple retrieval paths customized to the way the individual thinks and works with their information sources. WebTop&lt;ref name="Wolber et al 2002" /&gt; allowed the user to create explicit links between documents, but then also merged these user-defined relationships with other types of associations. These included the hyperlinks contained in the documents, associations implied by structural relationships, and content similarities discovered by text analysis. The idea was that any way in which items can be considered "related" should be made available to the user for help with retrieval.

A subclass of these systems integrate the user's personal workspace with a search facility, blurring the distinction between information retrieval and information organization.  SketchTrieve,&lt;ref name="Hendry and Harper 1997" /&gt; DLITE,&lt;ref name="Cousins et al 1997" /&gt; and Garnet&lt;ref name="Buchanan et al 2004" /&gt; each materialized elements from the retrieval domain (repositories, queries, search results) into tangible, manipulatable screen objects. These could be introduced directly into a spatial layout that also included the information sources themselves. These systems can be seen as combining a spatial hypertext interface as in VIKI&lt;ref name="Marshall and Shipman 1995" /&gt; with direct access to digital library search facilities.  NaviQue&lt;ref name="Furnas and Rauch 1998" /&gt; was largely in the same vein, though it incorporated a powerful similarity engine to proactively aid the user in organization. CYCLADES&lt;ref name="Renda and Straccia 2005" /&gt; let users organize Web pages into folders, and then attempted to infer what each folder "means" to that user, based on a statistical textual analysis of its contents. This helps users locate other items similar to what's already in a folder, learn what other users have found interesting and have grouped together, etc.

All of these document management systems are principally concerned with organizing objective information sources rather than the expression of subjective knowledge. Yet their methods are useful to consider with respect to PKB systems, because such a large part of our knowledge comprises things we remember, assimilate, and repurpose from objective sources. Search environments like SketchTrieve, as well as snippet gatherers like YellowPen, address an important need in [[knowledge management]]: bridging the divide between the subjective and objective realms, so that the former can make reference to and bring structure to the latter.

== Claims and benefits ==

PKB systems make various claims about the advantages of using them. These can be classified as follows:&lt;ref name = "Davies 2005"/&gt;&lt;ref name = "Davies 2011"/&gt;

* '''Knowledge generation and formulation.''' Here the emphasis is on procedure, not persistence; it is the act of simply using the tool to express one's knowledge that helps, rather than the ability to retrieve it later.
* '''[[Knowledge capture]].''' PKBs do not merely allow one to express knowledge, but also to capture it before it elusively disappears. Often the emphasis is on a streamlined user interface, with few distractions and little encumbrance.  The point is to lower the burden of jotting down one's thoughts so that neither task nor thought process is interrupted.
* '''Knowledge organization.''' A 2003 study on note-taking habits found that "better organization" was the most commonly desired improvement in people's own information recording practices.&lt;ref name="Hayes et al 2003" /&gt;
* '''Knowledge management and [[knowledge retrieval|retrieval]].''' Perhaps the most critical aspect of a PKB is that the knowledge it stores is permanent and accessible, ready to be retrieved at any later time.
* '''[[Knowledge integration|Integrating]] heterogeneous sources.''' Recognizing that the knowledge people form comes from a variety of different places, many PKB systems emphasize that the information from diverse sources and of different types can be integrated into a single database and interface.

== Data models ==
{{see|Data model}}
PKB systems can be compared along a number of different axes, the most important of which is the underlying data model they support. This is what prescribes and constrains the nature of the knowledge they can contain: what types of knowledge elements are allowed, how they can be structured, and how the user perceives them and can interact with them.&lt;ref name = "Davies 2005"/&gt;&lt;ref name = "Davies 2011"/&gt;

Three aspects of data models can be identified: the ''structural framework'', which prescribes rules about how knowledge elements can be structured and interrelated; the ''knowledge elements'' themselves, or basic building blocks of information that a user creates and works with; and ''schema'', which involves the level of formal semantics introduced into the data model.

=== Structural frameworks ===

The following structural frameworks have been featured in one or more prominent PKB systems.

==== Tree ====

Systems that support a [[Tree (data structure)|tree]] model allow knowledge elements to be organized into a containment hierarchy, in which each element has one and only one "parent". This takes advantage of the mind's natural tendency to classify objects into groups, and to further break up each classification into subclassifications. It also mimics the way that a document can be broken up into chapters, sections, and subsections.  It tends to be natural for users to understand.

All of the applications for creating Buzan [[Mind Map|mind maps]] are based on a tree model, because a mind map ''is'' a tree. Each mind map has a "root" element in the center of the diagram (often called a "main topic") from which all other elements emanate as descendents.  Every knowledge element has one and only one place in this structure. Some tools, such as [[MindManager]], extend this paradigm by introducing "floating topics", which are not anchored to the hierarchy, and permitting "crosslinks" to arbitrary topics, similar to those in concept maps.

Other examples of tree-based systems are most personalized search interfaces,&lt;ref name="Renda and Straccia 2005" /&gt;&lt;ref name="Di Giacomo et al 2001" /&gt;&lt;ref name="Reyes-Farfan and Sanchez 2003"/&gt; outliners, and most of the "notebook-based" note-taking systems. By allowing them to partition their notes into sections and subsections, note-taking tools channel users into a tree hierarchy. In recognition of this confining limitation, many of these tools also permit a kind of "crosslink" between items, or employ some form of transclusion (see below) to allow items to co-exist in several places. The dominant paradigm in such tools, however, remains the simple parent-child hierarchy.

==== Graph ====

Graph-based systems allow users to create knowledge elements and then to interconnect them in arbitrary ways. The elements of a [[Graph (discrete mathematics)|graph]] are traditionally called "vertices," and connected by "arcs," though the terminology used by graph-based systems varies widely (see Table 1) and the hypertext community normally uses the terms "nodes" and "links". There are no restrictions on how many arcs one vertex can have with others, no notion of a "parent/child" relationship between vertices (unless the user chooses to label an arc with those semantics), and normally no "root" vertex. In many systems, arcs can optionally be labeled with a word or phrase indicating the nature of the relationship, and adorned with arrowheads on one or both ends to indicate navigability. (Neither of these adornments is necessary with a tree, since all relationships are implicitly labeled "parent/child" and are navigable from parent to child.) A graph is a more general form of a tree, and hence a strictly more powerful form of expression.

{| class="wikitable"
|+Terminology employed by a sampling of graph-based knowledge tools.
! System !! Vertex !! Arc !! Graph
|-
|[[Axon Idea Processor]]||object||link||diagram
|-
|Banxia Decision Explorer||concept||link||view
|-
|[[Compendium (software)|Compendium]]||node||link||view
|-
|[[Haystack (MIT project)|Haystack]]||needle||tie||bale
|-
|Idea Graph||idea||connection||ideagraph
|-
|Knowledge Manager||concept||relation||map
|-
|[[MyLifeBits]]||resource||link/annotation||story
|-
|[[NoteCards]]||note card||link||browser
|-
|[[PersonalBrain]]||thought||link||brain
|-
|RecallPlus||idea||association||diagram
|-
|SMART Ideas||symbol||connector||level
|}

This model is the defining characteristic of hypertext systems&lt;ref name="Halasz and Schwartz 1994" /&gt; including many of those used for document management.&lt;ref name="Wolber et al 2002" /&gt;&lt;ref name="Adar et al  1999" /&gt; It is also the underpinning of all concept-mapping tools, whether they actually acknowledge the name "concept maps"&lt;ref name="Canas et al 2005" /&gt;&lt;ref name="Gaines and Shaw 1995" /&gt; or advertise themselves simply as tools to draw knowledge diagrams. As mentioned previously, graphs draw their power from the fact that humans are thought to model knowledge as graphs (or equivalently, semantic networks) internally. In fact, it could be argued that all human knowledge can be ultimately reduced to a graph of some kind, which argues strongly for its sufficiency as a structural framework.&lt;ref name="Quillian 1968" /&gt;&lt;ref name="Nosek and Roth 1990" /&gt;

An interesting aspect of graph-based systems is whether or not they require a ''[[Connectivity (graph theory)|fully connected]]'' graph.  A fully connected graph is one in which every vertex can be reached from any other by simply performing enough arc traversals. There are no "islands" of vertices that are severed from each other. Most graph-based tools allow non-fully-connected graphs: knowledge elements are added to the system, and connected arbitrarily to each other, without constraint.  But a few tools, such as [[PersonalBrain]] and [[Compendium (software)|Compendium]], actually require a single network of information in which every knowledge element must be indirectly connected to every other. If one attempts to remove the last link that connects a body of nodes to the original root, the severed elements are either "forgotten" or else moved to a deleted objects heap where they can only be accessed by restoring a connection to the rest of the graph.

Some hypertext systems&lt;ref name="Garrett et al 1986" /&gt;&lt;ref name="Delisle and Schwartz 1986" /&gt; add precision to the basic linking mechanism by allowing nodes to reference not only other nodes, but sections within nodes.&lt;ref name="Halasz and Schwartz 1994" /&gt; This ability is especially useful if the nodes themselves contain sizeable content, and also for PKB elements making reference to fragments of objective sources.

==== Tree plus graph ====

Although graphs are a strict superset of trees, trees offer some important advantages in their own right: simplicity, familiarity, ease of navigation, and the ability to conceal details at any level of abstraction. Indeed, the problem of "disorientation" in hypertext navigation&lt;ref name="Conklin and Begeman 1988"/&gt;&lt;ref name="Mantei 1982" /&gt; largely disappears with the tree model; one is never confused about "where one is" in the larger structure, because traversing the parent hierarchy gives the context of the larger surroundings. For this reason, several graph-based systems have incorporated special support for trees as well, to combine the advantages of both approaches.  For instance, in concept mapping techniques, a generally hierarchical paradigm is prescribed, after which users are encouraged to identify "crosslinks" between distant concepts. Similarly, some systems using the mind mapping paradigm permit arbitrary relationships between nodes.

One of the earliest systems to combine tree and graph primitives was TEXTNET,&lt;ref name="Trigg and Weiser 1986" /&gt; which featured two types of nodes: "chunks" (which contained content to be browsed and organized) and "table of contents" nodes (or "tocs".) Any node could freely link to any other, permitting an unrestricted graph. But a group of tocs could be combined to form a tree-like hierarchy that bottomed out in various chunk nodes. In this way, any number of trees could be superimposed upon an arbitrary graph, allowing it to be viewed and browsed as a tree, with all the requisite advantages. Strictly speaking, a network of tocs formed a [[Directed acyclic graph|DAG]] rather than a tree. This means that a "chunk" could be represented in multiple places in the tree, if two different traversal paths ended up referring to the same chunk. A DAG is essentially the result of applying transclusion to the tree model. This is also true of NoteCards. NoteCards&lt;ref name="Halasz et al 1987" /&gt; offered a similar mechanism, using "FileBoxes" as the tree component that was overlaid upon the semantic network of notecards.

Brown University's IGD project explored various ways to combine and display unrestricted graphs with hierarchy, and used a visual metaphor of spatial containment to convey both graph and tree structure.&lt;ref name="Feiner 1988" /&gt; Their notion of "link inheritance" simplifies the way in which complex dual structures are displayed while still faithfully depicting their overall trends. Commercially, both [[PersonalBrain]] and Multicentrix&lt;ref name="Koy 1997" /&gt; provide explicit support for parent/child relationships in addition to arbitrary connections between elements, allowing tree and graph notions to coexist.  Some note-taking tools, while essentially tree-based, also permit crosslinks between notes.

==== Spatial ====
Some designers have shunned links between elements altogether, favoring instead spatial positioning as the sole organizational paradigm. Capitalizing on the human's tendency to implicitly organize through clustering, making piles, and spatially arranging, some tools offer a 2D workspace for placing and grouping items. This provides a less formal (and perhaps less intimidating) way for a user to gradually introduce structure into a set of items as it is discovered.

This approach originated from the spatial hypertext community, demonstrated in various projects,&lt;ref name="diSessa and Abelson 1986" /&gt; and VIKI/VKB&lt;ref name="Marshall and Shipman 1995" /&gt;&lt;ref name="Shipman et al 2000" /&gt; With these programs, users place information items on a canvas and can manipulate them to convey organization imprecisely. Some project&lt;ref name="Marshall and Shipman 1995" /&gt; could infer the structure from a user's freeform layout: a spatial parser examines which items have been clustered together, colored or otherwise adorned similarly, etc., and makes judgments about how to turn these observations into machine-processible assertions. While others (Pad&lt;ref name="Perlin and Fox 1993" /&gt;) allowed users to view different objects in varying levels of detail as they panned around the workspace.

Certain note-taking tools&lt;ref name="Burger et al 1991" /&gt;&lt;ref name="Akscyn et al 1987" /&gt; combine an overarching tree structure with spatial freedom on each "frame" or "page". Users can access a particular page of the notebook with basic search or tree navigation facilities, and then lay out notes and images on the page as desired. Many graph-based approaches (such as concept mapping tools) also allow for arbitrary spatial positioning of elements. This allows both kinds of relationships to be expressed: explicit links and less formal expression through creative use of the screen.

==== Categories ====

In category-based structural frameworks, rather than being described in terms of their relationships to other elements (as with a tree or graph), items are simply grouped together in one or more categories, indicating that they have something in common. This scheme is based on the branch of pure mathematics called [[set theory]], in which each of a body of objects either has, or does not have, membership in each of some number of sets. There is normally no restriction as to how many different categories a given item can belong to, as is the case with mathematical sets.

Users may think of categories as collections, in which the category somehow encloses or "owns" the items within it. Indeed, some systems depict categories in this fashion, such as the Vista interface&lt;ref name="Dourish et al 1999" /&gt; where icons standing for documents are enclosed within ovals that represent categories. This is merely a convention of display, however, and fundamentally, categories are the same as simple keywords.

The most popular application to embrace the category approach was the original [[Lotus Agenda|Agenda]].&lt;ref name="Kaplan et al 1990" /&gt; All information retrieval in Agenda was performed in terms of category membership. Users specified queries that were lists of categories to include (or exclude), and only items that satisfied those criteria were displayed. Agenda was particularly sophisticated in that the categories themselves formed a tree hierarchy, rather than a flat namespace. Assigning an item to a category also implicitly assigned it to all ancestors in the hierarchy.

[[Personal Knowbase]] is a more modern commercial product based solely on a keyword (category) paradigm, though it uses a simple flat keyword structure rather than an inheritance hierarchy like Agenda. [[Haystack (MIT project)|Haystack]]&lt;ref name="Adar et al 1999" /&gt; and [[Open Source Applications Foundation|Chandler]] are other information management tools which use categorization in important ways. William Jones' Memory Extender&lt;ref name="Jones 1986" /&gt; took an artificial intelligence twist on the whole notion of keywords/categories, by allowing an item's keywords to be weighted, and adjusted over time by both the user and the system. This allowed the strength of category membership to vary dynamically for each of an item's assignments, in an attempt to yield more precise retrieval.

==== Chronological ====

Yale University's Lifestreams project&lt;ref name="Fertig et al 1996" /&gt; used timestamps as the principal means of organization and retrieval of personal documents. In Fertig et al.'s own words:

&lt;blockquote&gt;A [[lifestreaming|lifestream]] is a time-ordered stream of documents that functions as a diary of your electronic life; every document you create is stored in your lifestream, as are the documents other people send you. The tail of your stream contains documents from the past, perhaps starting with your electronic birth certificate.  Moving away from the tail and toward the present, your stream contains more recent documents such as papers in progress or the latest electronic mail you've received...&lt;/blockquote&gt;

Documents are thus always ordered and accessed chronologically. Metadata-based queries on the collection produce "substreams," or chronologically ordered subsets of the original documents. The rationale for time-based ordering is that "time is a natural guide to experience; it is the attribute that comes closest to a universal skeleton-key for stored experience".&lt;ref name="Freeman and Gelernter 1996" /&gt; Whether chronology is our principal or even a common natural coding mechanism psychologically can be debated. But since any PKB system can easily create such an index, it seems worthwhile to follow Lifestreams' lead and allow the user to sort and retrieve based on time, as many systems have done. If nothing else, it relieves the user from having to create names for knowledge elements, since the timestamp is always an implicit identifying mark. PlanPlus, based on the Franklin-Covey planner system, is also chronologically modeled, and a number of products based on other data models&lt;ref name="Kaplan et al 1990" /&gt; offer chronological indexing in addition to their core paradigm.

==== Aquanet's framework ====

Though advertised as a hypertext system, Marshall ''et al.'''s Aquanet&lt;ref name="Marshall et al 1991" /&gt; went far beyond the traditional node-link graph model.  Knowledge expressed in Aquanet is centered around "relations," or n-ary links between objects in which the semantics of each participant in the relation is specified by the relation type. Each type of relation specifies a physical display (i.e., how it will be drawn on the screen, and the spatial positioning of each of its participants), and a number of "slots" into which participants can be plugged. Each participant in a relation can be either a base object, or another relation. Users can thus define a schema of relation types, and then build a complex semantic model out of relations and objects. Since relation types can be specified to associate any number of nodes (instead of just two, as in the graph model), this potentially allows more complex relationships to be expressed.

It should be noted, however, that the same effect can be achieved in the basic graph model by simply taking the n-ary relations and "reifying" them (i.e., turning them into nodes in their own right.) For instance, suppose we define a relation type "assassination," with slot types of "assassin," "victim," "location," and "weapon". We could then create a relation based on this type where the participants are "John Wilkes Booth," "Abraham Lincoln," "Ford's Theatre," and "derringer". This allows us to express a complex relationship between multiple objects in Aquanet. But we can express the same knowledge with the basic graph model by simply creating a node called "Lincoln's assassination" and then creating typed links between that node and the other four labeled "assassin," "victim," etc. Aquanet's biggest achievement in this area is the ability to express the schema of relation types, so that the types of objects an "assassination" relation can connect are consistent and enforced.

=== Knowledge elements ===

There are several options for specifying what knowledge elements consist of, and what kind of internal structure, if any, they possess:

# '''Word/phrase/concept'''.  Most systems engineered for knowledge representation encourage structures to be composed of very simple elements, usually words or phrases.  This is in the spirit of both mind mapping and concept mapping, where users are encouraged to use simple phrases to stand for mental concepts.
# '''Free text notes'''. Nearly all systems permit large amounts of free text to exist in the PKB, either as the contents of the elements themselves (NoteCards,&lt;ref name="Halasz et al 1987" /&gt; Hypercard,&lt;ref name="Goodman 1988" /&gt; TreePad) or attached to elements as separate, supplementary pages (Agenda,&lt;ref name="Kaplan et al 1990" /&gt; Zoot, HogBay).
# '''Links to an information space'''.  Since a user's knowledge base is to correspond to her mental perceptions, it seems profitable for the PKB to point to entities in the information space from which she formed those perceptions.  Many systems do in fact allow their knowledge elements to point to the original sources in some way. There are three common techniques:
##The knowledge element actually ''represents'' an original source.  This is the case for document management systems (WebTop,&lt;ref name="Wolber et al 2002" /&gt; MyLifeBits,&lt;ref name="Gemmell et al 2002" /&gt; Haystack&lt;ref name="Adar et al 1999" /&gt;), integrated search facilities (NaviQue,&lt;ref name="Furnas and Rauch 1998" /&gt; CYCLADES&lt;ref name="Renda and Straccia 2005" /&gt;), VIKI/VKB.&lt;ref name="Marshall and Shipman 1995" /&gt;&lt;ref name="Shipman et al 2000" /&gt;  Tinderbox&lt;ref name="Bernstein 2003" /&gt; will also allow one of its notes to be a URL, and the user can control whether its contents should be captured once, or "auto-fetched" as to receive constant web updates.  Many systems, in addition to storing a page of free text for each knowledge element, also permit any number of hyperlinks to be attached to a knowledge element (e.g., [[FreeMind|Freemind]], [[PersonalBrain]], Inspiration). VNS,&lt;ref name="Burger et al  1991" /&gt; which allows users to point to a community notebook page from within their personal notebook, gives similar functionality.
## The knowledge element is a repurposed snippet from an original source. This is potentially the most powerful form, but is rare among fully featured PKB systems.  Cartagio, Hunter-Gatherer,&lt;ref name="Schraefel et al 2002" /&gt; and YellowPen all allow Web page excerpts to be assimilated and organized, although they primarily only do that, without allowing them to easily be combined with other subjective knowledge. DEVONThink and MyBase's WebCollect plug-in add similar functionality to their more general-purpose, tree-based information managers. Both of these systems, when a snippet is captured, archive the entire Web page locally so it can be returned to later. The user interfaces of CircusPonies and StickyBrain have been heavily optimized towards grabbing information from other applications and bringing them into the PKB without disturbing the user's workflow.
# '''Composites''' Some programs allow a user to embed knowledge elements (and perhaps other information as well) inside a knowledge element to form an implicit hierarchy. Trees by themselves fall into this category, of course, since each node in the tree can be considered a "composite" of its content and children. But a few graph-based tools offer composite functionality as well. In Aquanet,&lt;ref name="Marshall et al  1991" /&gt; "relations" form the fundamental means of connection, and the units that are plugged into a relation can be not only objects, but other relations as well. This lends a recursive quality to a user's modeling. VIKI/VKB's spatial environment offers "subspaces" which let a user partition their visual workspace into subregions, whose internal contents can be viewed at a glance from the parent. Boxer's&lt;ref name="diSessa and Abelson 1986" /&gt; paradigm is similar. Tinderbox is a graph-based tool that supports hierarchical composite structures, and [[Compendium (software)|Compendium]] extends this even further by allowing transclusion of "views" as well as of nodes. Unlike the other tools, in Compendium the composite hierarchy does not form a [[Directed acyclic graph|DAG]], but rather an arbitrary graph: view A can appear on view B, and B can in turn appear on A. The user's intuitive notion of "inside" must be adapted somewhat in this case.

=== Schema ===

In the context of PKBs, "schema" means the ability for a user to specify types and introduce structure to aspects of the data model. It is a form of metadata whereby more precise semantics can be applied to various elements of the system. This facilitates more formal knowledge expression, ensures consistency across items of the same kind, and can better allows automated agents to process the information.

Both knowledge elements, and links, can contain various aspects of schema.

==== Schema for knowledge elements ====

===== Types, and related schema =====

In a PKB, a "[[type system]]" allows users to specify that a knowledge element is a member of a specific class or category or items, to provide a built-in method of organization and retrieval. Generally speaking, systems can make knowledge elements untyped, rigidly typed, or flexibly typed. In addition, they can incorporate some notion of inheritance among elements and their types.  There is a distinction between types and categories here. A category-based scheme, typically allows any number of categories/keywords to be assigned to an item. There are two differences between this and the notion of type. First, items are normally restricted to being of a single type, and this usually indicates a more intrinsic, permanent property of an item than simply its presence in a category collection.  (For example, one could imagine an item called "XYZ Corporation" shifting into and out of categories like "competitors", "overseas distributors," or "delinquent debtors" over time, but its core type of "company" would probably be static for all time.) Second, types often carry structural specifications with them: if an item is of a given type, this means it will have values for certain attributes appropriate to that type.  Some systems that do not allow typing offer the ability to approximate this function through categories.

Untyped elements are typical among informal knowledge capture tools, since they are designed to stimulate brainstorming and help users discover their nascent mental models.  These tools normally want to avoid forcing the user to commit to structure prematurely.  Most mind mapping and many concept mapping tools are in this category: a concept is simply a word or phrase, with no other semantic information (e.g., [[Visual Mind]]). Note-taking tools also usually take this approach, with all units of information being of the same type "note".

At the other extreme are tools which, like older relational database technology, require all items to be declared as of a specific type when they are created. Often this type dictates the internal structure of the element. These tools are better suited to domains in which the structure of knowledge to be captured is predictable, well-understood, and known in advance. For PKB systems, they are probably overly restrictive. KMap&lt;ref name="Gaines and Shaw 1995" /&gt; and Compendium are examples of tools that allow (and require) each item to be typed; in their case, the type controls the visual appearance of the item, rather than any internal structure.

In between these two poles are systems that permit typed and untyped elements to co-exist. NoteTaker is such a product; it holds simple free-text pages of notes, without any structure, but also lets the user define "templates" with predefined fields that can be used to instantiate uniformly structured forms. TreePad has a similar feature. Some other systems blur the distinction between typed and untyped, allowing the graceful introduction of structure as it is discovered. VKB,&lt;ref name="Shipman et al 2000" /&gt; for example, supports an elegant, flexible typing scheme, well suited to PKBs.  Items in general consist of an arbitrary number of [[attribute–value pair]]s. But when consistent patterns emerge across a set of objects, the user can create a type for that group, and with it a list of expected attributes and default values. This structure can be selectively overridden by individual objects, however, which means that even objects assigned to a particular type have flexible customization available to them. Tinderbox offers an alternate way of achieving this flexibility, as described below.

Finally, the [[object-oriented]] notion of [[Inheritance (computer science)|type inheritance]] is available in a few solutions. The different card types in NoteCards are arranged into an inheritance hierarchy, so that new types can be created as extensions of old. Aquanet extends this to multiple inheritance among types; the "slots" that an object contains are those of its type, plus those of all supertypes. SPRINT and Tinderbox also use a frame-based approach, and allow default values for attributes to be inherited from supertypes. This way, an item need not define values for all its attributes explicitly: unless overridden, an item's slot will have the shared, default value for all items of that type.

===== Other forms of schema =====

In addition to the structure that is controlled by an item's type, other forms of metadata and schema can be applied to knowledge elements.

* '''Keywords'''. Many systems let users annotate items with user-defined keywords. Here the distinction between an item's contents and the overall knowledge structure becomes blurred, since an item keyword could be considered either a property of the item, or an organizational mechanism that groups it into a category with like items. Systems using the category data model (e.g., Agenda) can employ keywords for the latter purpose. Some systems based on other data models also use keywords to achieve category-like functionality.
* '''Attribute/value pairs'''. Arbitrary attribute/value pairs can also be attached to elements in many systems, which gives a PKB the ability to define semantic structure that can be queried.  Frame-based systems like SPRINT and Aquanet are examples, as well as NoteTaker, VKB, and Tinderbox. MindPad[AKS-Labs 2005] is notable for taking the basic concept mapping paradigm and introducing schema to it via its "model editor". As mentioned earlier, adding user-defined attribute/value pairs to the items in an outliner yields spreadsheet-like functionality, as in Ecco and [[OmniOutliner]]. Some systems feature attribute/value pairs, but only in the form of system-defined attributes, not user-defined ones.
* '''Knowledge element appearance'''. Some tools modify a knowledge element's visual appearance on the screen in order to convey meaning to the user. SMART Ideas and [[Visual Mind]] let the user freely choose each element's icon from a variety of graphics, while KMap&lt;ref name="Gaines and Shaw 1995" /&gt; ties the icon directly to its underlying type. Other graphical aspects that can be modified include color (VIKI&lt;ref name="Marshall and Shipman 1995" /&gt;), the set of attributes shown in a particular context (VKB&lt;ref name="Shipman et al 2000" /&gt;), and the spatial positioning of objects in a relation (Aquanet&lt;ref name="Marshall et al 1991" /&gt;).

==== Schema for links ====

In addition to prescribing schema for knowledge elements, many systems allow some form of information to be attached to the links that connect them.

In most of the early hypertext systems, links were unnamed and untyped, their function being merely to associate two items in an unspecified manner. The mind mapping paradigm also does not name links, but for a different reason: the implicit type of every link is one of generalization/specialization, associating a topic with a subtopic. Hence specifying types for the links would be redundant, and labeling them would clutter the diagram.

Concept mapping prescribes the naming of links, such that the precise nature of the relationship between two concepts is made clear. As mentioned above, portions of a concept map are meant to be read as English sentences, with the name of the link serving as a verb phrase connecting the two concepts. Numerous systems thus allow a word or phrase to decorate the links connecting elements.

Named links can be distinguished from ''typed'' links, however. If the text attached to a link is an arbitrary string of characters, unrelated to that of any other link, it can be considered the link name. Some systems, however, encourage the re-use of link names that the user has defined previously. In [[PersonalBrain]], for instance, before specifying the nature of a link, the user must create an appropriate "link type" (associated with a color to be used in presentation) in the system-wide database, and then assign that type to the link in question. This promotes consistency among the names chosen for links, so that the same logical relationship types will hopefully have the same tags throughout the knowledge base. This feature also facilitates searches based on link type, among other things. Other systems, especially those suited for specific domains such as decision modeling ([[gIBIS]]&lt;ref name="Conklin and Begeman 1988" /&gt; and Banxia Decision Explorer), predefine a set of link types that can be assigned (but not altered) by the user.

Some more advanced systems allow links to bear attribute/value pairs themselves, and even embedded structure, similar to those of the items they connect. In Haystack&lt;ref name="Adar et al 1999" /&gt; this is the case, since links ("ties") and nodes ("needles") are actually defined as subtypes of a common type ("straw").

KMap similarly defines a link as a subclass of node, which allows links to represent n-ary relationships between nodes, and enables recursive structure within a link itself. It is unclear how much value this adds in knowledge modeling, or how often users take advantage of such a feature. Neptune&lt;ref name="Delisle and Schwartz 1986" /&gt; and Intermedia&lt;ref name="Garrett et
al 1986" /&gt; are two older systems that also support attributes for links, albeit in a simpler manner.

Another aspect of links that generated much fervor in the early hypertext systems was that of link ''precision'': rather than merely connecting one element to another, systems like Intermedia defined anchors within documents, so that a particular snippet within a larger element could be linked to another snippet. The Dexter model&lt;ref name="Halasz and Schwartz 1994" /&gt; covers this issue in detail. For PKB purposes, this seems to be most relevant as regards links to the objective space, as discussed previously. If the PKB truly contains knowledge, expressed in appropriately fine-grained parts, then link precision between elements in the knowledge base is much less of a consideration.

This discussion on links has only considered connections between knowledge elements in the system, where the system has total control over both ends of the connection. As described in the previous section, numerous systems provide the ability to "link" from a knowledge element inside the system to some external resource: a file or a URL, say. These external links typically cannot be enhanced with any additional information, and serve only as convenient retrieval paths, rather than as aspects of knowledge representation.

== Architecture ==

The idea of a PKB gives rise to some important architectural considerations. While not constraining the nature of what knowledge can be expressed, the architecture nevertheless affects more mundane matters such as availability and workflow. But even more importantly, the system's architecture determines whether it can truly function as a lifelong, integrated knowledge store—the "base" aspect of the personal knowledge base defined above.

=== File-based ===

Traditionally, most electronic PKB systems have employed a simple storage mechanism based on flat files in a filesystem. This is true of virtually all of the mind mapping tools ([[MindManager]]), concept mapping tools, and even a number of hypertext tools (NoteCards,&lt;ref name="Halasz et al 1987" /&gt; Hypercard,&lt;ref name="Goodman 1988" /&gt; Tinderbox&lt;ref name="Bernstein 2003" /&gt;). Typically, the main "unit" of a user's knowledge design—whether that be a mind map, a concept map, an outline, or a "notebook"—is stored in its own file somewhere in the filesystem. The application can find and load such files via the familiar "File | Open..." paradigm, at which point it typically maintains the entire knowledge structure in memory.

The advantage of such a paradigm is familiarity and ease of use; the disadvantage is a possibly negative influence on knowledge formulation. Users must choose one of two basic strategies: either store all of their knowledge in a single file; or else break up their knowledge and store it across a number of different files, presumably according to subject matter and/or time period. The first choice can result in scalability problems—consider how much knowledge a user might collect over a decade, if they stored things related to their personal life, hobbies, relationships, reading materials, vacations, academic course notes, multiple work-related projects, future planning, etc. It seems unrealistic to keep adding this kind of volume to a single, ever-growing multi-gigabyte file. The other option, however, is also constraining: each bit of knowledge can be stored in only one of the files (or else redundantly, which leads to synchronization problems), and the user is forced to choose this at knowledge capture time.

=== Database-based ===

If a PKB's data is stored in a database system, then knowledge elements reside in a global space, which allows any idea to relate to any other: now a user can relate a book he read on productivity not only to other books on productivity, but also to "that hotel in Orlando that our family stayed in last spring," because that is where he remembers having read the book. Though such a relationship may seem "out of bounds" in traditional knowledge organization, it is exactly the kind of retrieval path that humans often employ in retrieving memories.&lt;ref name="Lorayne and Lucas 1974" /&gt;&lt;ref name="Anderson 1990" /&gt;&lt;ref name="Conway et al 1991" /&gt; The database architecture enables a PKB to truly form an integrated knowledge base, and contain the full range of relationships.

Agenda&lt;ref name="Kaplan et al 1990" /&gt; and [[gIBIS]]&lt;ref name="Conklin and Begeman 1988"/&gt; were two early tools that subsumed a database backend in their architecture. More recently, the MyLifeBits project&lt;ref name="Gemmell et al 2002" /&gt; uses Microsoft SQL Server as its storage layer, and [[Compendium (software)|Compendium]] interfaces with the open source MySQL database.  A few note-taking applications also store information in an integrated database rather than in user-named files. The only significant drawback to this architectural choice (other than the modest footprint of the database management system) is that data is more difficult to copy and share across systems.  This is one true advantage of files: it is a simple matter to copy them across a network, or include them as an e-mail attachment, where they can be read by the same application on a different machine. This problem is solved by some of the following architectural choices.

=== Client–server ===

Decoupling the actual knowledge store from the PKB user interface can achieve architectural flexibility. As with all client-server architectures, the benefits include load distribution, platform interoperability, data sharing, and ubiquitous availability.  Increased complexity and latency are among the liabilities, which can indeed be considerable factors in PKB design.

One of the earliest and best examples of a client-server knowledge base was the Neptune hypertext system.&lt;ref name="Delisle and Schwartz 1986" /&gt; Neptune was tailored to the task of maintaining shared information within software engineering teams, rather than to personal knowledge storage, but the elegant implementation of its "Hypertext Abstract Machine" (HAM) was a significant and relevant achievement. The HAM was a generic hypertext storage layer that provided node and link storage and maintained version history of all changes. Application layers and user interfaces were to be built on top of the HAM. Architecturally, the HAM provided distributed network access so that client applications could run from remote locations and still access the central store. Another, more recent example, is the Scholarly Ontologies Project&lt;ref name="Uren et al  2004" /&gt;&lt;ref name="Sereno et al 2005" /&gt; whose ClaiMapper and ClaiMaker components form a similar distributed solution in order to support collaboration.

These systems implemented a distributed architecture primarily in order to share data among colleagues. For PKBs, the prime motive is rather user mobility. This is a key consideration, since if a user is to store all of their knowledge into a single integrated store, they will certainly need access to it in a variety of settings.  MyBase Networking Edition is one example of how this might be achieved. A central server hosts the user's data, and allows network access from any client machine. Clients can view the knowledge base from within the MyBase application, or through a Web browser (with limited functionality.)

The Haystack project&lt;ref name="Adar et al 1999" /&gt; outlines a three-tiered architecture, which allows the persistent store, the Haystack data model itself, and the clients that access it to reside on separate machines. The interface to the middle tier is flexible enough that a number of different persistent storage models can be used, including relational databases, semistructured databases, and object-oriented databases. Presto's architecture&lt;ref name="Dourish et al 1999" /&gt; exhibits similar features.

==== Web-based ====

A variation of the client-server approach is Web-based systems, in which the client system consists of nothing but a (possibly enhanced) browser. This gives the same ubiquitous availability that client-server approaches do, while minimizing (or eliminating) the setup and installation required on each client machine.

KMap&lt;ref name="Gaines and Shaw 1995" /&gt; was one of the first knowledge systems to integrate with the World Wide Web. It allowed concept maps to be shared, edited, and remotely stored using the HTTP protocol. Concept maps were still created using a standalone client application for the Macintosh, but they could be uploaded to a central server, and then rendered in browsers as "clickable GIFs". Clicking on a concept within the map image in the browser window would have the same navigation effect as clicking on it locally inside the client application.  The user's knowledge expressions are stored on a central server in nearly all cases, rather than locally on the browser's machine.

=== Handheld devices ===

Lastly, mobile devices are a possible PKB architecture. Storing all of one's personal knowledge on a PDA would solve the availability problem, of course, and even more completely than would a client-server or web-based architecture.  The safety of the information is an issue, since if the device were to be lost or destroyed, the user could face irrevocable data loss; this is easily remedied, however, by periodically synchronizing the device's contents with a host computer.

Most handheld applications are simple note-taking software, with far fewer features than their desktop counterparts. BugMe! is an immensely popular note-taking tool that simply lets users enter text or scribble onto "notes" (screenfulls of space) and then organize them in primitive ways. Screen shots can be captured and included as graphics, and the tool features an array of drawing tools, clip art libraries, etc. The value add for this and similar tools is purely the size and convenience of the handheld device, not the ability to manage large amounts of information.

Perhaps the most effective use of a handheld architecture would be as a satellite data capture and retrieval utility. A user would normally employ a fully functional desktop application for personal knowledge management, but when "on the go," they could capture knowledge into a compatible handheld application and upload it to their PKB at a later convenient time. To enable mobile knowledge retrieval, either select information would need to be downloaded to the device before the user needed it, or else a wireless client-server solution could deliver any part of the PKB on demand. This is essentially the approach taken by software like KeySuite, which supplements a feature-rich desktop information management tool (e.g. [[Microsoft outlook|Microsoft Outlook]]) by providing access to that information on the mobile device.

== See also ==
* [[Commonplace book]]
* [[Lifelog]]
* [[Notetaking]]
** [[Comparison of notetaking software]]
* [[Outliner]]
* [[Personal knowledge management]]
* [[Personal wiki]]
** {{section link|List of wiki software|Personal wiki software}}
* {{section link|Tag (metadata)|Knowledge tags}}

== Notes ==
{{notelist}}

== References ==
{{reflist|30em|
refs=
&lt;!-- Converted to LDR format
     using [[User:PleaseStand/References segregator]] --&gt;

&lt;ref name = "Davies 2005"&gt;Davies, S., Velez-Morales, J. and King, R. [http://www.cs.colorado.edu/department/publications/reports/docs/CU-CS-997-05.pdf Building the Memex sixty years later: trends and directions in personal knowledge bases]. Technical Report CU-CS-997-05. Boulder, Colorado: Department of Computer Science, University of Colorado at Boulder, August 2005.&lt;/ref&gt;

&lt;ref name = "Davies 2011"&gt;Davies, S. Still Building the Memex. ''Communications of the ACM'', vol. 53, issue 2, February 2011, 80-88.&lt;/ref&gt;

&lt;ref name = "Brooks 1985"&gt;Brooks, T. New technologies and their implications for local area networks. ''Computer Communications'', vol. 8, no. 2, 1985, 82-87.&lt;/ref&gt;

&lt;ref name = "Kruger 1986"&gt;Krüger, G. Future information technology—motor of the "information society". in ''Employment and the Transfer of Technology''. Berlin: Springer, 1986, 39-52.&lt;/ref&gt;

&lt;ref name = "Forman 1988"&gt;Forman, G. Making intuitive knowledge explicit through future technology. in ''Constructivism in the Computer Age''. Hillsdale, New Jersey: L. Erlbaum, 1988, 83-101.&lt;/ref&gt;

&lt;ref name = "Smith 1991"&gt;Smith, C.F. Reconceiving hypertext. in ''Evolving Perspectives on Computers and Composition Studies: Questions for the 1990s''. Urbana, Illinois: National Council of Teachers of English, 1991, 224-260.&lt;/ref&gt;

&lt;ref name="Schneiderman 1987"&gt;Schneiderman, B., User interface design for the Hyperties electronic encyclopedia. in ''Proceedings of the ACM Conference on Hypertext''. Chapel Hill, North Carolina, 1987, 189-194.&lt;/ref&gt;

&lt;ref name = "Nelson 1987"&gt;Nelson, T.H. ''Literary machines: the report on, and of, Project Xanadu concerning word processing, electronic publishing, hypertext, thinkertoys, tomorrow's intellectual revolution, and certain other topics including knowledge, education and freedom''. Swarthmore, Pennsylvania: Theodor H. Nelson, 1987.&lt;/ref&gt;

&lt;ref name="Dumais et al 2003"&gt;Dumais, S.T., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R. and Robbins, D.C. Stuff I've Seen: a system for personal information retrieval and re-use. in ''Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval''. Toronto, Canada, 2003, 72-79.&lt;/ref&gt;

&lt;ref name="Nakakoji et al 2000"&gt;Nakakoji, K., Yamamoto, Y., Takada, S. and Reeves, B.N. Two-dimensional spatial positioning as a means for reflection in design. in ''Proceedings of the Conference on Designing Interactive Systems: Processes, Practices, Methods, and Techniques''. New York: ACM, 2000, 145-154.&lt;/ref&gt;

&lt;ref name="Smith et al 1987"&gt;Smith, J.B., Weiss, S.F. and Ferguson, G.J. A hypertext writing environment and its cognitive basis. in ''Proceedings of the ACM Conference on Hypertext''. Chapel Hill, North Carolina, 1987, 195-214.&lt;/ref&gt;

&lt;ref name="Godwin-Jones 2003"&gt;Godwin-Jones, B. Blogs and wikis: environments for on-line collaboration. ''Language Learning and Technology'', vol. 7, no. 2 (May 2003), 12-16.&lt;/ref&gt;

&lt;ref name="Kovalainen et al 1998"&gt;Kovalainen, M., Robinson, M. and Auramaki, E. Diaries at work. in ''Proceedings of the 1998 ACM Conference on Computer Supported Collaborative Work'', Seattle, Washington, 1998, 49-58.&lt;/ref&gt;

&lt;ref name="Blandford and Green 2001"&gt;Blandford, A.E. and [[Thomas R.G. Green|Green, T.R.G.]]. Group and individual time management tools: what you get is not what you need. ''Personal and Ubiquitous Computing'', vol. 5, no. 4 (December 2001), 213-230.&lt;/ref&gt;

&lt;ref name="Palen 1999"&gt;Palen, L. Social, individual and technological issues for groupware calendar systems. In ''Proceedings of the SIGCHI conference on Human Factors in Computing Systems'', pp. 17-24. ACM, 1999.&lt;/ref&gt;

&lt;ref name="Bush 1945"&gt;Bush, V. As we may think. ''The Atlantic Monthly'', July 1945, 101-108.&lt;/ref&gt;

&lt;ref name="Novak 2003"&gt;Novak, J.D. The theory underlying concept maps and how to construct them. Institute for Human and Machine Cognition, University of West Florida, 2003.&lt;/ref&gt;

&lt;ref name="Ausubel 1968"&gt;Ausubel, D.P. ''Educational Psychology: A Cognitive View''. New York: Holt, Rinehart, and Winston, 1968&lt;/ref&gt;

&lt;ref name="Canas et al 2005"&gt;Cañas, A.J., Hill, G., Carff, R., Suri, N., Lott, J., Gomez, G., Eskridge, T.C., Arroyo, M. and Carvajal, R. CmapTools: a knowledge modeling and sharing environment. in ''Proceedings of the First International Conference on Concept Mapping'', Pamplona, Spain, 2005, 125-133&lt;/ref&gt;

&lt;ref name="Woods 1985"&gt;Woods, W.A. What's in a Link: Foundations for Semantic Networks. in Brachman, R.J. and Levesque, J. eds. ''Readings in Knowledge Representation'', Morgan Kaufmann, 1985.&lt;/ref&gt;

&lt;ref name="Goodman 1988"&gt;Goodman, D. ''The Complete Hypercard Handbook''. New York: Bantam Books, 1988.&lt;/ref&gt;

&lt;ref name="Garrett et al 1986"&gt;Garrett, L.N., Smith, K.E. and Meyrowitz, N. Intermedia: Issues, strategies, and tactics in the design of a hypermedia document system. in ''Proceedings of the Conference on Computer-Supported Cooperative Work'', 1986, 163-174.&lt;/ref&gt;

&lt;ref name="Davis et al 1993"&gt;Davis, H., Hall, W., Heath, I., Hill, G. and Wilkins, R. MICROCOSM: an open hypermedia environment for information integration. in ''Proceedings of the INTERCHI Conference on Human Factors in Computing Systems''. ACM Press, 1993.&lt;/ref&gt;

&lt;ref name="Pearl 1989"&gt;Pearl, A. Sun's Link Service: a protocol for open linking. in ''Proceedings of the Second Annual ACM Conference on Hypertext'', Pittsburgh, Pennsylvania, 1989, 137-146.&lt;/ref&gt;

&lt;ref name="Engelbart 1953"&gt;Engelbart, D.C. A conceptual framework for the augmentation of man's intellect. in Howerton, P.W. ed. ''Vistas in Information Handling'', Spartan Books, Washington, D.C., 1963, 1-29.&lt;/ref&gt;

&lt;ref name="Trigg and Weiser 1986"&gt;Trigg, R.H. and Weiser, M. TEXTNET: a network-based approach to text handling. ''ACM Transactions on Information Systems'', vol. 4, no. 1, 1986, 1-23.&lt;/ref&gt;

&lt;ref name="Halasz et al 1987"&gt;Halasz, F.G., Moran, T.P. and Trigg, R.H. NoteCards in a Nutshell. ''ACM SIGCHI Bulletin'', 17, 1986, 45-52.&lt;/ref&gt;

&lt;ref name="Conklin and Begeman 1988"&gt;Conklin, J. and Begeman, M.L. gIBIS: a hypertext tool for exploratory policy discussion. in ''Proceedings of the 1988 ACM Conference on Computer-supported Cooperative Work'', Portland, Oregon, 1988, 140-152.&lt;/ref&gt;

&lt;ref name="Dede and Jayaram 1990"&gt;Dede, C.J. and Jayaram, G. Designing a training tool for imaging mental models. Air Force Human Resources Laboratory, Brooks Air Force Base, Texas, 1990.&lt;/ref&gt;

&lt;ref name="Marshall et al 1991"&gt;Marshall, C., Halasz, F.G., Rogers, R.A. and Janssen, W.C. Aquanet: a hypertext took to hold your knowledge in place. in ''Proceedings of the Third Annual ACM Conference on Hypertext'', San Antonio, Texas, 1991, 261-275.&lt;/ref&gt;

&lt;ref name="Carlson and Ram 1990"&gt;Carlson, D.A. and Ram, S. HyperIntelligence: the next frontier. ''Communications of the ACM'', vol. 33, no. 3, 1990, 311-321.&lt;/ref&gt;

&lt;ref name="Bernstein 2003"&gt;Bernstein, M. Collages, composites, construction. in ''Proceedings of the Fourteenth ACM Conference on Hypertext and Hypermedia'', Nottingham, UK, August 2003, 121-123.&lt;/ref&gt;

&lt;ref name="Kaplan et al 1990"&gt;Kaplan, S.J., Kapor, M.D., Belove, E.J., Landsman, R.A. and Drake, T.R. Agenda: a personal information manager. ''Communications of the ACM'', vol. 33, no. 7, 1990, 105-116.&lt;/ref&gt;

&lt;ref name="Burger et al 1991"&gt;Burger, A.M., Meyer, B.D., Jung, C.P. and Long, K.B. The virtual notebook system. in ''Proceedings of the Third Annual ACM Conference on Hypertext'', San Antonio, Texas, 1991, 395-401.&lt;/ref&gt;

&lt;ref name="Schraefel et al 2002"&gt;Schraefel, M.C., Zhu, Y., Modjeska, D., Widgdor, D. and Zhao, S. Hunter Gatherer: interaction support for the creation and management of within-Webpage collections. in ''Proceedings of the Eleventh International Conference on the World Wide Web'', 2002, 172-181.&lt;/ref&gt;

&lt;ref name="Dourish et al 1999"&gt;Dourish, P., Edwards, W.K., LaMarca, A. and Salisbury, M. Presto: an experimental architecture for fluid interactive document spaces. ''ACM Transactions on Computer-Human Interaction'', 6, 2, 133-161.&lt;/ref&gt;

&lt;ref name="Jones 1986"&gt;Jones, W.P. The Memory Extender personal filing system. in ''Proceedings of the SIGCHI Conference on Human Factors in Computing Systems'', Boston, Massachusetts, 1986, 298-305.&lt;/ref&gt;

&lt;ref name="Adar et al 1999"&gt;Adar, E., Karger, D. and Stein, L.A. Haystack: per-user information environments. in ''Proceedings of the Eighth International Conference on Information Knowledge Management'', Kansas City, Missouri, 1999, 413-422.&lt;/ref&gt;

&lt;ref name="Wolber et al 2002"&gt;Wolber, D., Kepe, M. and Ranitovic, I. Exposing document context in the personal web. in Proceedings of the 7th International Conference on Intelligent User Interfaces, San Francisco, California, 2002, 151-158.&lt;/ref&gt;

&lt;ref name="Hendry and Harper 1997"&gt;Hendry, D.G. and Harper, D.J. An informal information-seeking environment. ''Journal of the American Society for Information Science'', vol. 48, no. 11, 1997, 1036-1048.&lt;/ref&gt;

&lt;ref name="Cousins et al 1997"&gt;Cousins, S.B., Paepcke, A., Winograd, T., Bier, E.A. and Pier, K. The digital library integrated task environment (DLITE). in ''Proceedings of the Second ACM International Conference on Digital Libraries'', Philadelphia, Pennsylvania, 1997, 142-151.&lt;/ref&gt;

&lt;ref name="Buchanan et al 2004"&gt;Buchanan, G., Blandford, A.E., Thimbleby, H. and Jones, M. Integrating information seeking and structuring: exploring the role of spatial hypertext in a digital library. in ''Proceedings of the Fifteenth ACM Conference on Hypertext and Hypermedia'', Santa Cruz, California, 2004, 225-234.&lt;/ref&gt;

&lt;ref name="Marshall and Shipman 1995"&gt;Marshall, C. and Shipman, F. Spatial hypertext: designing for change. ''Communications of the ACM'', vol. 38, no. 8, 1995, 88-97.&lt;/ref&gt;

&lt;ref name="Furnas and Rauch 1998"&gt;Furnas, G.W. and Rauch, S.J. Considerations for information environments and the NaviQue workspace. in ''Proceedings of the ACM Conference on Digital Libraries'', 1998, 79-88.&lt;/ref&gt;

&lt;ref name="Renda and Straccia 2005"&gt;Renda, M.E. and Straccia, U. A personalized collaborative digital library environment: a model and an application. ''Information Processing and Management: an International Journal'', vol. 41, no. 1, 2005, 5-21.&lt;/ref&gt;

&lt;ref name="Hayes et al 2003"&gt;Hayes, G., Pierce, J.S. and Abowd, G.D. Practices for capturing short important thoughts. in ''CHI '03 Extended Abstracts on Human Factors in Computing Systems'', Ft. Lauderdale, Florida, 2003, 904-905.&lt;/ref&gt;

&lt;ref name="Reyes-Farfan and Sanchez 2003"&gt;Reyes-Farfan, N. and Sanchez, J.A. Personal spaces in the context of OAI. in ''Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries'', 2003, 182-183.&lt;/ref&gt;

&lt;ref name="Halasz and Schwartz 1994"&gt;Halasz, F.G. and Schwartz, M. The Dexter hypertext reference model. ''Communications of the ACM'', vol. 37, no. 2, February 1994, 30-39.&lt;/ref&gt;

&lt;ref name="Adar et al 1999"&gt;Adar, E., Karger, D. and Stein, L.A. Haystack: per-user information environments. in ''Proceedings of the Eighth International Conference on Information Knowledge Management'', Kansas City, Missouri, 1999, 413-422.&lt;/ref&gt;

&lt;ref name="Gaines and Shaw 1995"&gt;Gaines, B.R. and Shaw, M.L.G. Concept maps as hypermedia components. ''International Journal of Human Computer Studies'', vol. 43, no. 3, 1995, 323-361.&lt;/ref&gt;

&lt;ref name="Quillian 1968"&gt;Quillian, M.R. Semantic memory. in ''Semantic Information Processing'', Cambridge, Massachusetts: MIT Press, 1968, 227-270.&lt;/ref&gt;

&lt;ref name="Nosek and Roth 1990"&gt;Nosek, J.T. and Roth, I. A comparison of formal knowledge representationschemes as communication tools: predicate logic vs semantic network. ''International Journal of Man-Machine Studies'', vol. 33, no. 2, 1990, 227-239.&lt;/ref&gt;

&lt;ref name="Delisle and Schwartz 1986"&gt;Delisle, N. and Schwartz, M. Neptune: a hypertext system for CAD applications. in ''Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data'', Washington, D.C., 1986, 132-143.&lt;/ref&gt;

&lt;ref name="Mantei 1982"&gt;Mantei, M.M. ''Disorientation behavior in person–computer interaction''. Ph.D. thesis. Communications Department, University of Southern California, 1982.&lt;/ref&gt;

&lt;ref name="Feiner 1988"&gt;Feiner, S. Seeing the forest for the trees: hierarchical display of hypertext structure. in ''Proceedings of the ACM SIGOIS and IEEECS TC-OA 1988 Conference on Office Information Systems'', Palo Alto, California, 1988, 205-212.&lt;/ref&gt;

&lt;ref name="Koy 1997"&gt;Koy, A.K. Computer aided thinking. in ''Proceedings of the 7th International Conference on Thinking'', Singapore, 1997.&lt;/ref&gt;

&lt;ref name="Selvin 1999"&gt;Selvin, A.M. Supporting collaborative analysis and design with hypertext functionality. ''Journal of Digital Information'', vol. 1, no. 4, 1999.&lt;/ref&gt;

&lt;ref name="Di Giacomo et al 2001"&gt;Di Giacomo, M., Mahoney, D., Bollen, J., Monroy-Hernandez, A. and Meraz, C.M.R. MyLibrary, a personalization service for digital library environments. In ''DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries'', June 2001.&lt;/ref&gt;

&lt;ref name="Akscyn et al 1987"&gt;Akscyn, R., McCracken, D. and Yoder, E. KMS: a distributed hypermedia system for managing knowledge in organizations. in ''Proceedings of the ACM Conference on Hypertext'', Chapel Hill, North Carolina, 1987.&lt;/ref&gt;

&lt;ref name="Fertig et al 1996"&gt;Fertig, S., Freeman, E. and Gelernter, D. Lifestreams: An alternative to the desktop metaphor. in ''Proceedings of the Conference on Human Factors in Computing Systems (CHI96)'', Vancouver, British Columbia, 1996, 410-411.&lt;/ref&gt;

&lt;ref name="Freeman and Gelernter 1996"&gt;Freeman, E. and Gelernter, D. Lifestreams: a storage model for personal data. ACM SIGMOD Record, 25, 1.(March 1996), 80-86.&lt;/ref&gt;

&lt;ref name="Gemmell et al 2002"&gt;Gemmell, J., Bell, G., Lueder, R., Drucker, S. and Wong, C. MyLifebits: Fulfilling the Memex vision. in ''Proceedings of the 2002 ACM Workshops on Multimedia'', 2002, 235-238.&lt;/ref&gt;

&lt;ref name="Shipman et al 2000"&gt;Shipman, F., Hsieh, H. and Airhart, R. Analytic workspaces: supporting the emergence of interpretation in the Visual Knowledge Builder. Department of Computer Science and Center for the Study of Digital Libraries, Texas A&amp;M University, 2000.&lt;/ref&gt;

&lt;ref name="diSessa and Abelson 1986"&gt;diSessa, A.A. and Abelson, H. Boxer: a reconstructible computational medium. ''Communications of the ACM'', vol. 29, no. 9, 1986, 859-868.&lt;/ref&gt;

&lt;ref name="Anderson 1990"&gt;Anderson, J.R. ''Cognitive Psychology and Its Implications'', 3rd Ed. New York: W.H. Freeman, 1990.&lt;/ref&gt;

&lt;ref name="Lorayne and Lucas 1974"&gt;Lorayne, H. and Lucas, J. ''The Memory Book''. New York: Stein and Day, 1974.&lt;/ref&gt;

&lt;ref name="Conway et al 1991"&gt;Conway, M.A., Kahney, H., Bruce, K. and Duce, H. Imaging objects, routines, and locations. in Logie, R.H. and Denis, M. eds. ''Mental Images in Human Cognition'', New York: Elsevier Science Publishing, 1991, 171-182.&lt;/ref&gt;

&lt;ref name="Uren et al 2004"&gt;Uren, V., Buckingham Shum, S., Li, G. and Bachler, M. Sensemaking tools for understanding research literatures: design, implementation, and user evaluation. Knowledge Media Institute, The Open University, 2004, 1-42.&lt;/ref&gt;

&lt;ref name="Sereno et al 2005"&gt;Sereno, B., Buckingham Shum, S. and Motta, E. ClaimSpotter: an environment to support sensemaking with knowledge triples. in ''Proceedings of the International Conference on Intelligent User Interfaces'', San Diego, California, 2005, 1999-1206.&lt;/ref&gt;

&lt;ref name="Perlin and Fox 1993"&gt;Perlin, K. and Fox, D. Pad: an alternative approach to the computer interface. in ''Proceedings of the 20th annual conference on computer graphics and interactive techniques'', 1993, 57-64.&lt;/ref&gt;
}}

{{Computable knowledge}}

[[Category:Knowledge representation]]</text>
      <sha1>8mch4u581kf0ces0ztyrx9a8t7xxu8l</sha1>
    </revision>
  </page>
  <page>
    <title>Social History and Industrial Classification</title>
    <ns>0</ns>
    <id>34821434</id>
    <revision>
      <id>704733684</id>
      <parentid>688105394</parentid>
      <timestamp>2016-02-13T07:17:20Z</timestamp>
      <contributor>
        <username>Jabberwoch</username>
        <id>6441698</id>
      </contributor>
      <comment>Adding/improving reference(s)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1986" xml:space="preserve">{{Underlinked|date=April 2014}}

'''Social History and Industrial Classification''' (SHIC) is a classification system used by many British museums for social history and industrial collections.
It was first published in 1983.&lt;ref&gt;{{cite web|title=SHIC Home|url=http://www.holm.demon.co.uk/shic/}}&lt;/ref&gt;

==Purpose==
SHIC was classifies materials (books, objects, recordings etc.) by their interaction with the people who used them. For example, a carpenter's hammer is classified with other tools of the carpenter, and not with a blacksmith's hammer.&lt;ref&gt;{{cite web|title=SHIC Section A|url=http://www.holm.demon.co.uk/shic/shicint.htm}}&lt;/ref&gt; In contrast other classification systems, for example the [[Dewey Decimal Classification]], might class all hammers together and close to the classification for other percussive tools. The specialist subject network, Social History Curator's Group (SHCG), obtained funding in 2012 to develop an on-line version, now on their website http://www.shcg.org.uk/&lt;ref&gt;{{cite web|title=Social History Curators' Group - SHCG|url=http://www.shcg.org.uk/About-SHIC|accessdate=29 October 2012}}&lt;/ref&gt;

==Scheme==
Materials are classified under four major category numbers:
#Community life
#Domestic and family life
#Personal life
#Working life
 
Further classification within a category is by the use of further numbers after the decimal point.&lt;ref&gt;{{cite web|title=SHIC Section B|url=http://www.holm.demon.co.uk/shic/shicint.htm}}&lt;/ref&gt; 

It is permissible to assign more than one classification in cases where the object had more than one use.&lt;ref&gt;{{cite web|title=SHIC Section F |url=http://www.holm.demon.co.uk/shic/shicint.htm}}&lt;/ref&gt;

==References==
{{reflist}}
*''Social history and industrial classification (SHIC), a subject classification for museum collections'', University of Sheffield, 1983

[[Category:Library cataloging and classification]]
[[Category:Knowledge representation]]
[[Category:Social history]]
[[Category:Industrial history]]</text>
      <sha1>4uyekv4xt100u6ri2mjy6w85780att7</sha1>
    </revision>
  </page>
  <page>
    <title>Dublin Core</title>
    <ns>0</ns>
    <id>8742</id>
    <revision>
      <id>741172920</id>
      <parentid>737374976</parentid>
      <timestamp>2016-09-25T21:10:48Z</timestamp>
      <contributor>
        <username>RobbieIanMorrison</username>
        <id>27789191</id>
      </contributor>
      <minor />
      <comment>/* Further reading */ removed honorific for 'Luca Dini' (as per [[MOS:CREDENTIAL]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="17366" xml:space="preserve">{{Use dmy dates|date=July 2012}}
The '''Dublin Core Schema''' is a small set of vocabulary terms that can be used to describe web resources (video, images, web pages, etc.), as well as physical resources such as books or CDs, and objects like artworks.&lt;ref&gt;{{cite web|url=http://dublincore.org/documents/dcmi-type-vocabulary/index.shtml |title=DCMI Metadata Terms |publisher=Dublincore.org |accessdate=5 April 2013}}&lt;/ref&gt; The full set of Dublin Core metadata terms can be found on the Dublin Core Metadata Initiative (DCMI) website.&lt;ref&gt;{{cite web|url=http://dublincore.org/documents/dcmi-terms/ |title=DCMI Metadata Terms |publisher=Dublincore.org |accessdate=5 April 2013}}&lt;/ref&gt; The original set of 15 classic&lt;ref&gt;{{cite web|url=http://dublincore.org/specifications/ |title=DCMI Specifications |publisher=Dublincore.org |date=14 December 2009 |accessdate=5 April 2013}}&lt;/ref&gt; metadata terms, known as the Dublin Core Metadata Element Set&lt;ref name="DCMES"&gt;{{cite web|url=http://dublincore.org/documents/dces/ |title=Dublin Core Metadata Element Set, Version 1.1 |publisher=Dublincore.org |accessdate=5 April 2013}}&lt;/ref&gt; are endorsed in the following standards documents:

* IETF RFC 5013&lt;ref&gt;[http://www.ietf.org/rfc/rfc5013.txt The Dublin Core Metadata Element Set], Dublin Core Metadata Initiative, August 2007&lt;/ref&gt;
* ISO Standard 15836-2009&lt;ref&gt;{{cite web|url=http://www.iso.org/iso/iso_catalogue/catalogue_ics/catalogue_detail_ics.htm?csnumber=52142 |title=ISO 15836:2009 - Information and documentation - The Dublin Core metadata element set |publisher=Iso.org |date=18 February 2009 |accessdate=5 April 2013}}&lt;/ref&gt;
* NISO Standard Z39.85&lt;ref&gt;{{cite web|url=http://www.niso.org/kst/reports/standards?step=2&amp;gid=None&amp;project_key=9b7bffcd2daeca6198b4ee5a848f9beec2f600e5 |title=NISO Standards - National Information Standards Organization |publisher=Niso.org |date=22 May 2007 |accessdate=5 April 2013}}&lt;/ref&gt;

Dublin Core Metadata may be used for multiple purposes, from simple resource description, to combining metadata vocabularies of different [[Metadata#Metadata standards|metadata standards]], to providing interoperability for metadata vocabularies in the [[Linked Data]] cloud and [[Semantic Web]] implementations.

== Background ==
"Dublin" refers to [[Dublin, Ohio]], USA where the schema originated during the 1995 invitational OCLC/NCSA Metadata Workshop,&lt;ref&gt;[http://dublincore.org/workshops/dc1/ OCLC/NCSA Metadata Workshop]&lt;/ref&gt; hosted by the [[Online Computer Library Center]] (OCLC), a library consortium based in Dublin, and the [[National Center for Supercomputing Applications]] (NCSA).  "Core" refers to the metadata terms as "broad and generic being usable for describing a wide range of resources".&lt;ref name="DCMES"/&gt; The semantics of Dublin Core were established and are maintained by an international, cross-disciplinary group of professionals from [[librarianship]], [[computer science]], [[text encoding]], [[museum]]s, and other related fields of scholarship and practice.

Starting in 2000, the Dublin Core community focused on "[[application profile]]s" &amp;ndash; the idea that metadata records would use Dublin Core together with other specialized vocabularies to meet particular implementation requirements. During that time, the World Wide Web Consortium's work on a generic data model for metadata, the [[Resource Description Framework]] (RDF), was maturing. As part of an extended set of DCMI Metadata Terms, Dublin Core became one of the most popular vocabularies for use with RDF, more recently in the context of the Linked Data movement.&lt;ref&gt;{{cite web|title=DCMI Metadata Basics|publisher=dublincore.org/metadata-basics/}}&lt;/ref&gt;

The '''Dublin Core Metadata Initiative''' (DCMI)&lt;ref&gt;{{cite web|url=http://dublincore.org/ |title=DCMI Home: Dublin Core® Metadata Initiative (DCMI) |publisher=Dublincore.org |date= |accessdate=2015-12-04}}&lt;/ref&gt; provides an open forum for the development of interoperable online [[metadata standards]] for a broad range of purposes and of business models. DCMI's activities include consensus-driven working groups, global conferences and workshops, standards liaison, and educational efforts to promote widespread acceptance of metadata standards and practices. In 2008, DCMI separated from OCLC and incorporated as an independent entity.&lt;ref&gt;{{cite web | title=OCLC Research and the Dublin Core Metadata Initiative | url=http://www.oclc.org/research/activities/past/orprojects/dublincore/default.htm | accessdate=21 April 2010}}&lt;/ref&gt;

Currently, any and all changes that are made to the Dublin Core standard, are reviewed by a DCMI Usage Board within the context of a DCMI Namespace Policy (DCMI-NAMESPACE). This policy describes how terms are assigned and also sets limits on the amount of editorial changes allowed to the labels, definitions, and usage comments.&lt;ref&gt;{{cite web|url=http://dublincore.org/documents/dces/ |title=Dublin Core Metadata Element Set, Version 1.1 |publisher=Dublincore.org |date= |accessdate=2015-12-04}}&lt;/ref&gt;

== Levels of the standard ==
The Dublin Core standard originally includes two levels: Simple and Qualified. '''Simple Dublin Core''' comprised 15 elements; '''Qualified Dublin Core''' included three additional elements (Audience, Provenance and RightsHolder), as well as a group of element refinements (also called qualifiers) that could refine the semantics of the elements in ways that may be useful in resource discovery.

Since 2012 the two have been incorporated into the DCMI Metadata Terms as a single set of terms using the [[Resource Description Framework]] (RDF).&lt;ref name="dublincore1"&gt;{{cite web|url=http://dublincore.org/documents/dcmi-terms/ |title=DCMI Metadata Terms |publisher=Dublincore.org |date= |accessdate=2015-12-04}}&lt;/ref&gt; The full set of elements is found under the namespace http://purl.org/dc/terms/. Because the definition of the terms often contains domains and ranges, which may not be compatible with the pre-RDF definitions used for the original 15 Dublin Core elements, there is a separate namespace for the original 15 elements as previously defined: http://purl.org/dc/elements/1.1/.&lt;ref&gt;[http://dublincore.org/documents/dces/ Dublin Core Metadata Element Set, version 1.1]&lt;/ref&gt;

=== Dublin Core Metadata Element Set Version 1.1===
The original '''Dublin Core Metadata Element Set''' consists of 15 metadata elements:&lt;ref name="DCMES"/&gt;
# Title
# Creator
# Subject
# Description
# Publisher
# Contributor
# Date
# Type
# Format
# Identifier
# Source
# Language
# Relation
# Coverage
# Rights

Each Dublin Core element is optional and may be repeated. The DCMI has established standard ways to refine elements and encourage the use of encoding and vocabulary schemes. There is no prescribed order in Dublin Core for presenting or using the elements. The Dublin Core became ISO 15836 standard in 2006 and is used as a base-level data element set for the description of learning resources in the [[ISO/IEC 19788]]-2 Metadata for learning resources (MLR) &amp;ndash; Part 2: Dublin Core elements, prepared by the [[ISO/IEC JTC1/SC36|ISO/IEC JTC1 SC36]].

Full information on element definitions and term relationships can be found in the Dublin Core Metadata Registry.&lt;ref name="registry"&gt;[http://dcmi.kc.tsukuba.ac.jp/dcregistry/ Dublin Core Metadata Registry]&lt;/ref&gt;

==== Example of code ====
: {{code|2=html4strict|1=&lt;meta name="DC.Format" content="video/mpeg; 10 minutes"&gt;}}
: {{code|2=html4strict|1=&lt;meta name="DC.Language" content="en" &gt;}}
: {{code|2=html4strict|1=&lt;meta name="DC.Publisher" content="publisher-name" &gt;}}
: {{code|2=html4strict|1=&lt;meta name="DC.Title" content="HYP" &gt;}}

==== An example of use [and mention] of D.C. (by [[WebCite]]) ====

At the web page which serves as the "archive" form for [[WebCite]],&lt;ref name="WebCite_archive_form_(web_page)"&gt;{{cite web
| url          = http://webcitation.org/archive
| title        = WebCite® archive form
| quote        = Metadata (optional)&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;These are Dublin Core elements. [...]
| publisher    = [[WebCite]]
}}
&lt;/ref&gt; it says, in part: "Metadata (optional)&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;These are Dublin Core elements. [...]".

=== Qualified Dublin Core (deprecated in 2012&lt;ref&gt;{{cite web|url=http://dublincore.org/documents/2000/07/11/dcmes-qualifiers/ |title=Dublin Core Qualifiers |publisher=Dublincore.org |date= |accessdate=2015-12-04}}&lt;/ref&gt;)===
Subsequent to the specification of the original 15 elements, an ongoing process to develop exemplary terms extending or refining the Dublin Core Metadata Element Set (DCMES) was begun. The additional terms were identified, generally in working groups of the Dublin Core Metadata Initiative, and judged by the DCMI Usage Board to be in conformance with principles of good practice for the qualification of Dublin Core metadata elements.

Elements refinements make the meaning of an element narrower or more specific. A refined element shares the meaning of the unqualified element, but with a more restricted scope. The guiding principle for the qualification of Dublin Core elements, colloquially known as the ''Dumb-Down Principle'',&lt;ref&gt;[http://dublincore.org/workshops/dc8/dcgrammar/tsld008.html Dumb-Down Principle for qualifiers]&lt;/ref&gt; states that an application that does not understand a specific element refinement term should be able to ignore the qualifier and treat the metadata value as if it were an unqualified (broader) element. While this may result in some loss of specificity, the remaining element value (without the qualifier) should continue to be generally correct and useful for discovery.

In addition to element refinements, Qualified Dublin Core includes a set of recommended encoding schemes, designed to aid in the interpretation of an element value. These schemes include controlled vocabularies and formal notations or parsing rules. A value expressed using an encoding scheme may thus be a token selected from a controlled vocabulary (for example, a term from a classification system or set of subject headings) or a string formatted in accordance with a formal notation, for example, "2000-12-31" as the ISO standard expression of a date. If an encoding scheme is not understood by an application, the value may still be useful to '''human reader'''.

'''Audience, Provenance''' and '''RightsHolder''' are elements, but not part of the Simple Dublin Core 15 elements. Use Audience, Provenance and RightsHolder only when using Qualified Dublin Core.
DCMI also maintains a small, general vocabulary recommended for use within the element Type. This vocabulary currently consists of 12 terms.&lt;ref name="registry"/&gt;

=== DCMI Metadata Terms ===
The Dublin Core Metadata Initiative (DCMI) Metadata Terms is the current set of the Dublin Core vocabulary.&lt;ref name="dublincore1"/&gt; This set includes the fifteen terms of the Dublin Core Metadata Element Set (in ''italic''), as well as the qualified terms. Each term has a unique URI in the namespace http://purl.org/dc/terms, and all are defined as [[Resource Description Framework|RDF]] properties.

{{columns-list|4|
*abstract
*accessRights
*accrualMethod
*accrualPeriodicity
*accrualPolicy
*alternative
*audience
*available
*bibliographicCitation
*conformsTo
*''contributor''
*''coverage''
*created
*''creator''
*''date''
*dateAccepted
*dateCopyrighted
*dateSubmitted
*''description''
*educationLevel
*extent
*''format''
*hasFormat
*hasPart
*hasVersion
*''identifier''
*instructionalMethod
*isFormatOf
*isPartOf
*isReferencedBy
*isReplacedBy
*isRequiredBy
*issued
*isVersionOf
*''language''
*license
*mediator
*medium
*modified
*provenance
*''publisher''
*references
*''relation''
*replaces
*requires
*''rights''
*rightsHolder
*''source''
*spatial
*''subject''
*tableOfContents
*temporal
*''title''
*''type''
*valid
}}

== Syntax ==
Syntax choices for Dublin Core metadata depends on a number of variables, and "one size fits all" prescriptions rarely apply. When considering an appropriate syntax, it is important to note that Dublin Core concepts and semantics are designed to be syntax independent and are equally applicable in a variety of contexts, as long as the metadata is in a form suitable for interpretation both by machines and by human beings.

The '''Dublin Core Abstract Model'''&lt;ref&gt;[http://dublincore.org/documents/abstract-model/ Dublin Core Abstract Model]&lt;/ref&gt; provides a reference model against which particular Dublin Core encoding guidelines can be compared, independent of any particular encoding syntax. Such a reference model allows implementers to gain a better understanding of the kinds of descriptions they are trying to encode and facilitates the development of better mappings and translations between different syntax.

== Some applications ==
One [[Document Type Definition]] based on Dublin Core is the [http://www.ibiblio.org/osrt/omf/ Open Source Metadata Framework] (OMF) specification. OMF is in turn used by [[Rarian]] (superseding [[ScrollKeeper]]), which is used by the [[GNOME]] desktop and [[KDE]] help browsers and the ScrollServer documentation server. [[PBCore]] is also based on Dublin Core. The [[Zope]] [[Zope Content Management Framework|CMF's]] Metadata products, used by the [[Plone (content management system)|Plone]], [[ERP5]], the Nuxeo CPS [[Content management system]]s, [[SimpleDL]], and [[FedoraCommons]] also implement Dublin Core. The [[EPUB]] [[e-book]] format uses Dublin Core metadata in the [[OPF (file format)|OPF file]].&lt;ref&gt;{{cite web|url=http://www.idpf.org/epub/20/spec/OPF_2.0_latest.htm#Section2.2|title=Open Packaging Format (OPF) 2.0.1 – 2.2: Publication Metadata|publisher=[[International Digital Publishing Forum]]|accessdate=12 September 2013}}&lt;/ref&gt; [[eXo Platform]] also implements Dublin Core.

DCMI also maintains a list of projects using Dublin Core&lt;ref&gt;{{cite web|url=http://dublincore.org/projects/|title=DCMI Projects - Alphabetical|publisher=DCMI|accessdate=15 March 2013}}&lt;/ref&gt; on its website.

== See also ==
* [[Metadata registry]]
* [[Metadata Object Description Schema]]
* [[Wikiversity:Digital Libraries/Metadata|Metadata from Wikiversity]]
* [[Semantic Web]]
* [[Ontology (information science)]]
* [[Open Archives Initiative]]
* [[Controlled vocabulary]]
* [[Interoperability]]
* [[Asset Description Metadata Schema]] ([http://www.w3.org/TR/vocab-adms/ ADMS]), a metadata standard maintained by the [[World Wide Web Consortium]] for describing semantic standards. Implemented on Joinup.&lt;ref&gt;{{cite web|url=https://joinup.ec.europa.eu/catalogue/all?filters=bs_current_version:true{{!}}Joinup |title=Joinup &amp;#124; Joinup |publisher=Joinup.ec.europa.eu |date=2015-10-22 |accessdate=2015-12-04}}&lt;/ref&gt; 
* [[Metadata Encoding and Transmission Standard]] (METS), maintained by the [[Library of Congress]] for the [[Digital Library Federation]]
* [[Preservation Metadata: Implementation Strategies]] (PREMIS)

=== Related software ===
* [[Dublin Core Meta Toolkit]] (Conversion of Access, MySQL, or CSV data to DublinCore metadata)
* [[Fedora (software)|Fedora]] repository architecture and Project (An open-source software system capable of implementing [[OAI-PMH]] (and thus Dublin Core).
* [[Omeka]], A free, open-source, unqualified Dublin-Core compliant web-publishing system for digital archives.
* The [http://archiviststoolkit.org/ Archivist's Toolkit] is a self-described as an "Archival Data Management system" able to work with the Dublin Core format. It will soon be merged with [[Archon (software)|Archon]], which is ambiguous as to its OAI support.
* [[ICA-AtoM]], a web-based archival description/publication software that can serve as an OAI-PMH repository and uses OAI-PMH as the main language for remote data exchange

== References ==
{{Reflist|2}}

== Further reading ==
* {{cite book |title= Organising Knowledge in a Global Society |last= Harvey  |first= Ross |authorlink= |author2=Philip Hider |year= 2004 |publisher= Charles Sturt University |location= Wagga Wagga NSW |isbn= 1-876938-66-8 |page= |pages= |url= |accessdate=}}
* [https://www.inf.unibz.it/courses/images/stories/2005_2006/Digital_Libraries/dini-less-5-6.ppt "Lecture slides about Dublin Core"], by Luca Dini, lecturer at the [[Free University of Bolzano]]

== External links ==
* [http://dublincore.org/ Dublin Core Metadata Initiative]
* [http://wiki.dublincore.org/index.php/User_Guide Dublin Core usage guide]
* [http://xml.coverpages.org/ni2005-03-21-a.html Dublin Core Metadata Initiative Publishes DCMI Abstract Model] (''Cover Pages'', March 2005)
* [http://www.loc.gov/standards/mods/v3/mods-userguide-3-0.html Metadata Object Description Schema (MODS)]
*[http://www.dublincoregenerator.com/ The Dublin Core Generator: A tool for generating Dublin Core code]
*[http://library.kr.ua/dc/dceditunie.html The Dublin Core Generator-Editor: Free tool for extracting-editing Dublin Core HTML code]

{{Semantic Web}}
{{Authority control}}

[[Category:Archival science]]
[[Category:Bibliography file formats]]
[[Category:Digital libraries]]
[[Category:Information management]]
[[Category:Interoperability]]
[[Category:ISO standards]]
[[Category:Knowledge representation]]
[[Category:Library cataloging and classification]]
[[Category:Metadata standards]]
[[Category:Museology]]
[[Category:Records management]]
[[Category:Reference models]]
[[Category:Semantic Web]]</text>
      <sha1>07wppgdn2zhp046au1rfvvyb4mddp3p</sha1>
    </revision>
  </page>
  <page>
    <title>Library classification</title>
    <ns>0</ns>
    <id>18328</id>
    <revision>
      <id>761468138</id>
      <parentid>761004176</parentid>
      <timestamp>2017-01-23T03:55:38Z</timestamp>
      <contributor>
        <ip>85.53.34.106</ip>
      </contributor>
      <comment>Fix header format.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="12928" xml:space="preserve">{{refimprove|date=March 2012}}
[[Image:HK Wan Chai Library Inside Bookcase a.jpg|thumb|A library book shelf in [[Hong Kong]] arranged using the [[Dewey Decimal Classification|Dewey classification]]]]

A '''library classification''' is a [[system]] by which library resources are arranged according to subject. Library classifications use a notational system that represents the order of topics in the classification and allows items to be stored in that order. Library classification systems group related materials together, typically arranged in a hierarchical tree structure. A different kind of classification system, called a [[faceted classification]] system, is also widely used which allows the assignment of multiple classifications to an object, enabling the classifications to be ordered in multiple ways. The library classification numbers can be considered identifiers for resources but are distinct from the [[International Standard Book Number]] (ISBN) or [[International Standard Serial Number]] (ISSN) system.

== Description ==
Library classification is an aspect of [[library and information science]]. It is distinct from [[taxonomy (general)|scientific classification]] in that it has as its goal to provide a useful ordering of documents rather than a theoretical organization of [[knowledge]].&lt;ref&gt;{{Citation
 | first =Ganesh
 | last =Bhattacharya
 | first2 =S R
 | last2 =Ranganathan
 | author2-link=S R Ranganathan
 | editor-last =Wojciechowski
 | editor-first =Jerzy A.
 | title =From knowledge classification to library classification
 | series =Ottawa Conference on the Conceptual Basis of the Classification of Knowledge, 1971
 | year =1974
 | pages =119–143
 | place =Munich
 | publisher =Verlag Dokumentation
}}&lt;/ref&gt; Although it has the practical purpose of creating a physical ordering of documents, it does generally attempt to adhere to accepted scientific knowledge.&lt;ref&gt;{{cite book
 | last = Bliss
 | first = Henry Evelyn
 | title = The organization of knowledge in libraries
 | publisher = H. W. Wilson
 | location = New Yorka
 | date = 1933
}}&lt;/ref&gt;

Library classification is distinct from the application of [[Index term|subject headings]] in that classification organizes knowledge into a systematic order, while subject headings provide access to intellectual materials through vocabulary terms that may or may not be organized as a knowledge system.&lt;ref name=chan &gt;{{Citation
 |publisher = The Scarecrow Press, Inc.
 |isbn = 9780810859449
 |title = Cataloging and classification
 |url = http://openlibrary.org/books/OL9558667M/Cataloging_and_Classification
 |author = Lois Mai Chan
 |edition = Cataloging and Classification
 |publication-date = September 28, 2007
 |id = 0810859440
 }}&lt;/ref&gt;

==History==


Library classifications were preceded by classifications used by bibliographers such as [[Conrad Gessner]]. The earliest library classification schemes organized books in broad subject categories. The increase in available printed materials made such broad classification unworkable, and more granular classifications for library materials had to be developed in the nineteenth century.&lt;ref name=shera&gt;{{cite book|last1=Shera|first1=Jesse H|title=Libraries and the organization of knowledge|date=1965|publisher=Archon Books|location=Hamden, Conn.}}&lt;/ref&gt;

Although libraries created order within their collections from as early as the fifth century B.C.,&lt;ref name=shera /&gt; the Paris Bookseller's classification, developed in 1842 by [[Jacques Charles Brunet]], is generally seen as the first of the modern book classifications. Brunet provided five major classes: theology, jurisprudence, sciences and arts, belles-lettres, and history.&lt;ref name=sayers&gt;{{cite book|last1=Sayers|first1=Berwick|title=An introduction to library classification|date=1918|publisher=H. W. Wilson|location=New York}}&lt;/ref&gt;

==Types== 
There are many standard systems of library classification in use, and many more have been proposed over the years. However, in general, classification systems can be divided into three types depending on how they are used:

* '''Universal schemes''' which cover all subjects, for example the [[Dewey Decimal Classification]], [[Universal Decimal Classification]] and [[Library of Congress Classification]]
* '''Specific classification schemes''' which cover particular subjects or types of materials, for example Iconclass, [[British Catalogue of Music Classification]], and [[Dickinson classification]], or the [[NLM Classification]] for medicine. 
* '''National schemes''' which are specially created for certain countries, for example the [[Sweden|Swedish]] library classification system, SAB (Sveriges Allmänna Biblioteksförening).

In terms of functionality, classification systems are often described as:

*'''[[Enumeration|enumerative]]''': subject headings are listed alphabetically, with numbers assigned to each heading in alphabetical order.
*'''[[Hierarchy|hierarchical]]''': subjects are divided hierarchically, from most general to most specific.
*'''[[Faceted classification|faceted]]''' or '''analytico-synthetic''': subjects are divided into mutually exclusive orthogonal facets.

There are few completely enumerative systems or faceted systems; most systems are a blend but favouring one type or the other. The most common classification systems, LCC and DDC, are essentially enumerative, though with some hierarchical and faceted elements (more so for DDC), especially at the broadest and most general level. The first true faceted system was the [[Colon classification]] of [[S. R. Ranganathan]].

==Methods or Systems==

Classification types denote the classification or categorization according the form or characteristics or qualities of a classification scheme or schemes. Method and system has similar meaning. Method or methods or system means the classification schemes like Dewey Decimal Classification or Universal Decimal Classification. The types of classification is for identifying and understanding or education or research purposes while classification method means those classification schemes like DDC, UDC.
 
===English language universal classification systems===
The most common systems in [[English language|English]]-speaking countries are:
* [[Dewey Decimal Classification]] (DDC)
* [[Library of Congress Classification]] (LCC)
* [[Colon classification]] (CC)
* [[Universal Decimal Classification]] (UDC)

Other systems include:
* [[Harvard-Yenching Classification]], an English classification system for Chinese language materials.
* V-LIB 1.2 (2008 Vartavan Library Classification for over 700 fields of knowledge, currently sold under license in the UK by Rosecastle Ltd. (see http://rosecastle.atspace.com/index_files/VartavanLibrary.htm)).

===Non-English universal classification systems===
* A system of book classification for Chinese libraries (Liu's Classification) library classification for user
** [[New Classification Scheme for Chinese Libraries]]
* [[Nippon Decimal Classification]] (NDC)
* [[Chinese Library Classification]] (CLC)
* [[Korean Decimal Classification]] (KDC)
* Russian [[:ru:Библиотечно-библиографическая классификация|Library-Bibliographical Classification]] (BBK)

===Universal classification systems that rely on synthesis (faceted systems)===
* [[Bliss bibliographic classification]]
* [[Colon classification]]
* [[Cutter Expansive Classification]]
* [[Universal Decimal Classification]]

Newer classification systems tend to use the principle of synthesis (combining codes from different lists to represent the different attributes of a work) heavily, which is comparatively lacking in LC or DDC.

==The practice of classifying==

Library classification is associated with library (descriptive) cataloging under the rubric of ''cataloging and classification'', sometimes grouped together as ''technical services''. The library professional who engages in the process of cataloging and classifying library materials is called a ''cataloger'' or ''catalog librarian''. Library classification systems are one of the two tools used to facilitate [[subject access]].  The other consists of alphabetical indexing languages such as Thesauri and Subject Headings systems.

Library classification of a piece of work consists of two steps. Firstly, the "aboutness" of the material is ascertained. Next, a call number (essentially a book's address) based on the classification system in use at the particular library will be assigned to the work using the notation of the system.

It is important to note that unlike subject heading or thesauri where multiple terms can be assigned to the same work, in library classification systems, each work can only be placed in one class. This is due to shelving purposes: A book can have only one physical place. However, in classified catalogs one may have main entries as well as added entries. Most classification systems like the [[Dewey Decimal Classification]] (DDC) and [[Library of Congress Classification]] also add a [[cutter number]] to each work which adds a code for the author of the work.

Classification systems in libraries generally play two roles. Firstly, they facilitate [[subject access]] by allowing the user to find out what works or documents the library has on a certain subject.&lt;ref&gt;{{cite web|url=http://www.iva.dk/bh/lifeboat_ko/concepts/subject_access_points.htm|title=Subject access points|work=iva.dk}}&lt;/ref&gt; Secondly, they provide a known location for the information source to be located (e.g. where it is shelved).

Until the 19th century, most libraries had closed stacks, so the library classification only served to organize the subject [[library catalog|catalog]]. In the 20th century, libraries opened their stacks to the public and started to shelve library material itself according to some library classification to simplify subject browsing.

Some classification systems are more suitable for aiding subject access, rather than for shelf location. For example, [[Universal Decimal Classification]], which uses a complicated notation of pluses and colons, is more difficult to use for the purpose of shelf arrangement but is more expressive compared to DDC in terms of showing relationships between subjects. Similarly [[faceted classification]] schemes are more difficult to use for shelf arrangement, unless the user has knowledge of the citation order.

Depending on the size of the library collection, some libraries might use classification systems solely for one purpose or the other. In extreme cases, a public library with a small collection might just use a classification system for location of resources but might not use a complicated subject classification system. Instead all resources might just be put into a couple of wide classes (travel, crime, magazines etc.). This is known as a "mark and park" classification method, more formally called reader interest classification.&lt;ref&gt;Lynch, Sarah N., and Eugene Mulero. [http://www.nytimes.com/2007/07/14/us/14dewey.html "Dewey? At This Library With a Very Different Outlook, They Don't"] ''[[The New York Times]]'', July 14, 2007.&lt;/ref&gt;

== Comparing classification systems ==
As a result of differences in notation, history, use of enumeration, hierarchy, and facets, classification systems can differ in the following ways:
* Type of Notation: Notation can be pure (consisting of only numerals, for example) or mixed (consisting of letters and numerals, or letters, numerals, and other symbols). 
* Expressiveness: This is the degree to which the notation can express relationship between concepts or structure.
* Whether they support mnemonics: For example, the number 44 in DDC notation often means it concerns some aspect of France. For example, in the Dewey classification 598.0944 concerns "Birds in France", the 09 signifies geographical division, and 44 represents France.
* Hospitality: The degree to which the system is able to accommodate new subjects.
* Brevity: The length of the notation to express the same concept.
* Speed of updates and degree of support: The better classification systems are frequently being reviewed.
* Consistency 
* Simplicity
* Usability

== See also ==&lt;!-- PLEASE RESPECT ALPHABETICAL ORDER --&gt;
{{Wikipedia books}}
* [[Attribute-value system]]
* [[Categorization]]
* [[Document classification]]
* [[Knowledge organization]]
* [[Library management]]
* [[Library of Congress Subject Headings]]

==Notes==
{{reflist}}

==References==
{{commons category|Library cataloging and classification}}
* Chan, Lois Mai. (1994)''Cataloging and Classification: An Introduction'', second ed. New York: McGraw-Hill, . ISBN 978-0-07-010506-5, ISBN 978-0-07-113253-4.

{{Library classification systems}}
{{Computable knowledge}}

{{Authority control}}

{{DEFAULTSORT:Library Classification}}
[[Category:Library cataloging and classification| ]]
[[Category:Knowledge representation]]</text>
      <sha1>pjkno57bwutemn6w9p47mo0xkbl802t</sha1>
    </revision>
  </page>
  <page>
    <title>Template:InfoMaps</title>
    <ns>10</ns>
    <id>36485231</id>
    <revision>
      <id>761261938</id>
      <parentid>761261767</parentid>
      <timestamp>2017-01-21T23:54:29Z</timestamp>
      <contributor>
        <username>Merbst</username>
        <id>296729</id>
      </contributor>
      <minor />
      <comment>I realised this is case sensitive.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1748" xml:space="preserve">{{Sidebar
|name     = InfoMaps
|topimage = [[Image:Screen_Shot_2012-07-19_at_5.56.57_PM.png |165px|Part of "School of Athens" by Raphael (Raffaelo Sanzio, 1483-1520)]]
|title    = [[Information mapping]]
|bodyclass = hlist
|titleclass= navbox-title
|headingstyle = background:transparent;

|heading1 = Topics &amp; fields
|content1style = padding-bottom:0.9em;
|content1 = 
* [[Business decision mapping]]  
* [[Cognitive map]]  
* [[Data visualization]]  
* [[Decision tree]] 
* [[Educational psychology]] 
* [[Educational technology]] 
* [[Graphic communication]] 
* [[Information design]]  
* [[Information graphics]]  
* [[Interactive visualization]]  
* [[Knowledge visualization]] 
* [[Mental model]]  
* [[Morphological analysis (problem-solving)|Morphological analysis]]  
* [[Visual analytics]]  
* [[Visual language]]

|heading2 = Tree-like approaches
|content2style = padding-bottom:0.9em;
|content2 = 
* [[Cladistics]]  
* [[Argument map]] 
* [[Cognitive map]]
* [[Concept lattice]] 
* [[Concept map]]ping 
* [[Conceptual graph]] 
* [[Dendrogram]]  
* [[Graph drawing]]  
* [[Hyperbolic tree]]  
* [[Layered graph drawing]]  
* [[Mental model]]  
* [[Mind map]]ping 
* [[Object-role modeling]] 
* [[Organizational chart]]  
* [[Radial tree]] 
* [[Semantic network]] 
* [[Sociogram]]  
* [[Timeline]]   
* [[Topic Maps]]  
* [[Tree structure]]   

|heading3 =See also
|content3style = padding-bottom:0.9em;
|content3 =
* [[Diagrammatic reasoning]]
* [[Entity-relationship model]]
* [[Geovisualization]]  
* [[List of concept- and mind-mapping software]]  
* [[Olog]]  
* [[Semantic web]]  
* [[Treemapping]]  
* [[Wicked problem]]  

|tnavbarstyle = border-top:1px solid #aaa;
}}&lt;noinclude&gt;
[[Category:Knowledge representation]]
&lt;/noinclude&gt;</text>
      <sha1>4efzfk10r4ehu9474xv55wxddvanp56</sha1>
    </revision>
  </page>
  <page>
    <title>Prezi</title>
    <ns>0</ns>
    <id>23948922</id>
    <revision>
      <id>759572827</id>
      <parentid>758173897</parentid>
      <timestamp>2017-01-12T00:04:13Z</timestamp>
      <contributor>
        <ip>128.177.170.230</ip>
      </contributor>
      <comment>/* History */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="13712" xml:space="preserve">{{Infobox Website
| name           = Prezi
| logo           = [[File:Prezi logo transparent 2012.svg|frameless|150px]]
| url            = {{url|https://www.prezi.com/}}
| origin         = [[Hungary]]
| founder        = [[Adam Somlai-Fischer]]&lt;br&gt;Peter Halacsy&lt;br&gt;[[Peter Arvai]]
| language       = [[English language|English]], [[Portuguese language|Portuguese]], [[Spanish language|Spanish]], [[Korean language|Korean]], [[Japanese language|Japanese]], [[German language|German]], [[Italian language|Italian]], [[French language|French]], [[Hungarian language|Hungarian]]
| type           = [[Presentation Software|Presentation]] [[Collaboration tool|Collaboration]]
| launch date    = {{start date and age|2009|4|5}}
| current status = Active
}}

'''Prezi''' is a visual storytelling software alternative to traditional slide-based presentation formats. Prezi presentations feature a map-like, schematic overview that lets users pan between topics at will, zoom in on desired details, and pull back to reveal context.

This freedom of movement enables “conversational presenting,” a new presentation style in which presentations follow the flow of dialogue, instead of vice-versa.

Founded in 2009, and with offices in San Francisco, Budapest, and Mexico City, Prezi now fosters a community of over 75 million users with more than 260 million prezis around the world.

The company launched Prezi Business in 2016, with a suite of creation, collaboration, and analytics tools for teams. Prezi Business is an HTML5 application that runs on JavaScript.

The word ''Prezi'' is the short form of “presentation” in Hungarian.

== History ==

Prezi was founded in 2008 in Budapest, Hungary by Adam Somlai-Fischer, Peter Halacsy, and Peter Arvai. 

The earliest zooming presentation prototype had been previously developed by Somlai-Fischer to showcase his media-art pieces. Halacsy, an engineer, saw one of these presentations and proposed to improve the software. They were joined by entrepreneur and future CEO Arvai with the goal of making Prezi a globally recognized SaaS company.

The company established incorporation on May 20, 2009 and received its first major investment from TED two months later. A San Francisco office was opened that December. 

Early 2011 saw the launch of Prezi’s first iPad application, followed by $14M in Series B funding led by Accel Partners. A Prezi iPhone app was launched in late 2012. 

In March of 2014, Prezi pledged $100M in free licenses to Title 1 schools as part of the Obama administration’s ConnectED program. November of that year saw the announcement of $57M in new funding from Spectrum Equity and Accel Partners. 

Prezi for Android was launched in 2015, and in June of 2016, the company launched Prezi Business. As of June 2, 2016, Prezi reports 75 million registered users and 1 billion ‘prezi’ presentation views worldwide.

== Products and features ==
[[File:Path Tool.png|thumb|Prezi Path Tool]]

=== Prezi ZUI ===
The Prezi online and offline ZUI editors employ a common tool palette, allowing users to pan and zoom, and to size, rotate, or edit an object. The user places objects on a canvas and navigates between videos, images, texts and other presentation media. Frames allow grouping of presentation media together as a single presentation object. Paths are navigational sequences that connect presentation objects for the purposes of structuring a linear presentation.

=== Prezi Desktop ===
Prezi Desktop&lt;ref&gt;{{cite web|url=http://prezi.com/desktop/ |title=Desktop |publisher=Prezi |date= |accessdate=2012-06-05}}&lt;/ref&gt; allows Prezi Pro or Edu Pro subscribers to work off-line and create and save their presentations on their own [[Microsoft Windows|Windows]] or [[Mac OS X|Mac]] systems. Prezi Desktop Editor allows users to work on the presentation off-line in a .pez file format. Users can have files up to 500 MB in size when signing up with a school-affiliated e-mail address. This storage capability doesn't affect when users use an appropriate third-party conversion software with [[FLV]] or [[SWF]] format.&lt;ref&gt;{{cite web|url=http://prezi.com/desktop/ |title=Desktop |publisher=Prezi |date= |accessdate=2012-07-23}}&lt;/ref&gt;

=== Prezi Collaborate ===
Prezi Collaborate is an online collaboration feature that allows up to ten people (co-located or geographically separated) to co-edit and show their presentations in real time. Users participate in a prezi simultaneously, and each is visually represented in the presentation window by a small avatar. Although Prezi Meetings can be done simultaneously, that is not the only option. Participants can be invited to edit the Prezi presentation at a later time if they wish. A link will be sent and the participant has up to ten days to edit the presentation. Prezi Meeting is included in all license types.

===Prezi Viewer for iPad===
Prezi Viewer&lt;ref&gt;{{cite web|url=http://prezi.com/ipad/ |title=Viewer for iPad |publisher=Prezi |date= |accessdate=2012-06-05}}&lt;/ref&gt; is an app developed for the [[iPad]] for viewing prezis created on one's Prezi online account. The iPad [[touchscreen]] and [[multi-touch]] [[user interface]] enables users to pan, and pinch to zoom in or out of their media.

Prezzip also offers templates for PreziU, with tool kits and visuals for file presentations.&lt;ref&gt;{{cite web|url=http://www.prezzip.com/index.php/footer-pages/about/prezi-ipad-viewer/|title=Prezi iPad viewer |publisher=Prezzip |accessdate=25 July 2012}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.prezzip.com/index.php/footer-pages/about/what-we-do/ |title=what we do |publisher=Prezzip |date= |accessdate=2012-07-25}}&lt;/ref&gt;

== Revenue model ==
Prezi uses the [[freemium]] model. Customers who use the product's Public license must publish their work on the Prezi.com website, which is publicly viewable. Customers who pay for a Prezi Enjoy or Prezi Pro can make their presentations private. Only Pro license users have access to Prezi Desktop, which enables offline editing. Prezi also offers an educational license for students and educators.

== Uses ==

=== Business and conferences ===
Some users at the [[World Economic Forum]] are currently using Prezi for their presentations.&lt;ref&gt;{{cite web|url=http://blog.prezi.com/2009/11/03/how-to-create-a-good-prezi-for-the-world-economic-forum/ |title=zoomintoprezi - Latest - How to create a good prezi - World Economic Forum |publisher=Blog.prezi.com |date=2009-11-03 |accessdate=2012-06-05}}&lt;/ref&gt; Many [[TED (conference)|TED Conference]] speakers have used Prezi, including TED curator [[Chris Anderson (entrepreneur)|Chris Anderson]], who used a Prezi for his TEDGlobal 2010 presentation: How Web Video Powers Global Innovation.&lt;ref&gt;{{cite web|author= |url=https://www.youtube.com/watch?annotation_id=annotation_962757&amp;feature=iv&amp;src_vid=X6Zo53M0lcY&amp;v=LnQcCgS7aPQ |title=Chris Anderson: How YouTube is driving innovation |publisher=YouTube |date=2010-09-14 |accessdate=2015-05-06}}&lt;/ref&gt; Michael Chasen, President/CEO of [[Blackboard, Inc.]], used Prezi to deliver the keynote at their BbWorld 2011 annual users' conference.&lt;ref&gt;{{cite web|author= |url=https://www.youtube.com/watch?v=rlGA9_p_--c |title=BbWorld 2011 Corporate Keynote |publisher=YouTube |date=2011-07-26 |accessdate=2012-06-05}}&lt;/ref&gt; [[FBLA]] members have recently started using this software.{{citation needed|reason=|date=September 2013}}

=== Education ===
Prezi is used at [[Oregon State University]],&lt;ref&gt;{{cite web|author= not fuly true. should expand further|url=http://calendar.oregonstate.edu/event/63614 |title=Prezi in the  classroom |publisher=Oregon University State University calendar |date= |accessdate=2012-07-24}}&lt;/ref&gt; as well as at the [[Dwight School]]&lt;ref&gt;{{cite news|last=Anderson |first=Jenny |url=http://cityroom.blogs.nytimes.com/2011/06/21/at-a-private-school-virtual-learning-and-the-rock/ |title=At Dwight School, Virtual Learning and the Rock - NYTimes.com |location=Manhattan (NYC) |publisher=Cityroom.blogs.nytimes.com |date=2011-06-21 |accessdate=2012-06-05}}&lt;/ref&gt; and elsewhere in primary education and higher education.&lt;ref&gt;{{cite web|author=Zoltan Radnai|url=http://edu.prezi.com/article/27827/-Prezi-makes-you-stop-and-think/|title=Prezi makes you stop and think|publisher=Prezi|accessdate=23 July 2012}}&lt;/ref&gt; It can be used by teachers and students to collaborate on presentations with multiple users able to access and edit the same presentation,&lt;ref&gt;{{cite web|author=Tilt |url=https://www.youtube.com/watch?v=lZyv6MTVsjc |title=Student Web 2.0 |publisher=YouTube |date= |accessdate=2012-07-18}}&lt;/ref&gt; and to allow students to construct and present their knowledge in different learning styles.&lt;ref&gt;{{cite web|url=http://www.nactateachers.org/attachments/article/1060/NACTA%20Journal%20Vol%2055%20Sup%201.pdf/|title=Thinking outside of slide |publisher=NACTA |accessdate=24 July 2012}}&lt;/ref&gt; The product is also being used in [[e-learning]] and [[edutainment]].&lt;ref&gt;{{cite web|author= |url=https://www.youtube.com/watch?v=s7nDT_KgPpk |title=Daniel Gallichan - 1. Platz beim 1. Freiburger Science Slam |publisher=YouTube |date= |accessdate=2012-06-05}}&lt;/ref&gt; However note that Prezi is considered by Web2Access to be an 'inaccessible service'.&lt;ref&gt;{{cite web|url=http://www.web2access.org.uk/product/172/ |title=Results for Prezi|publisher=Web2Access JISC TechDis |date= |accessdate=2014-03-01}}&lt;/ref&gt; Educators have been advised that Prezi is not ADA/508 compliant and that an accessible PowerPoint version of the presentation should be provided online for students where a Prezi has been used.&lt;ref&gt;{{cite web|url=http://webaccessibility.gmu.edu/prezi.html |title=Prezi Known Accessibility Issues|publisher=George Mason University |date= |accessdate=2014-03-01}}&lt;/ref&gt;

=== Information visualization ===
In July 2011, ''[[The Guardian]]'' used Prezi to publish a new world map graphic on their website, for an article about the newly independent South Sudan.&lt;ref&gt;{{cite news|author=Simon Rogers, Jenny Ridley |url=https://www.theguardian.com/news/datablog/interactive/2011/jul/08/world-map-new-south-sudan |title=The new world map: download it for yourself &amp;#124; World news &amp;#124; guardian.co.uk |publisher=Guardian |date= 2011-07-08|accessdate=2012-06-05 |location=London}}&lt;/ref&gt;

== Platform compatibility ==
Prezi is developed in Adobe Flash, Adobe AIR and built on top of Django. It is compatible with most modern computers and web browsers. 

Prezi Business is an HTML5 application which runs on JavaScript. It also is compatible with most modern systems.

== Criticism ==
The company has acknowledged that the “[[zooming user interface]] (ZUI)” has the potential to induce nausea, and offers tutorials with recommendations for use of layout to avoid excessive visual stimulation.&lt;ref&gt;{{cite web|url=http://prezi.com/learn/grouping-and-layering/ |title=Why the Best Prezis use Grouping &amp; Layering &amp;#124; Prezi Learn Center |publisher=Prezi.com |date= |accessdate=2012-06-05}}&lt;/ref&gt;
There has also been criticism of Prezi’s lack of font and color options. Notably, Presentation Zen author Garr Reynolds once stated that he had never seen a good presentation using Prezi and was looking for one;&lt;ref&gt;{{cite web|url=http://garr.posterous.com/have-you-ever-seen-a-great-talk-given-with-th |title=Have you ever seen a great talk given with the help of Prezi? Do you have a link? - Garr's posterous |publisher=Garr.posterous.com |date=2010-09-10 |accessdate=2012-06-05 |deadurl=unfit |archiveurl=https://web.archive.org/web/20120402080355/http://garr.posterous.com/have-you-ever-seen-a-great-talk-given-with-th |archivedate=April 2, 2012 }}
&lt;/ref&gt; in a later post, he refers to Chris Anderson’s talk at TED Global 2010 as one of the best TED talks ever, commenting that it was a good use of Prezi.&lt;ref&gt;
{{cite web|url=http://garr.posterous.com/on-train-to-tokyo-watching-one-of-the-best-te |title=On train to Tokyo watching one of the best TED talks ever - Garr's posterous |publisher=Garr.posterous.com |date=2010-09-14 |accessdate=2012-06-05 |deadurl=unfit |archiveurl=https://web.archive.org/web/20120402080641/http://garr.posterous.com/on-train-to-tokyo-watching-one-of-the-best-te |archivedate=April 2, 2012 }}
&lt;/ref&gt;

As Prezi is a Flash-based online zooming tool, most elements of the presentation cannot be read aloud by users with disabilities by means of a screen reader (e.g. it is not possible to add [[alt attribute]]s to images and [[iframe]]s used for the page design, and templates have been built to work without [[accessibility]] options). Prezi is considered by Web2Access to be an 'inaccessible service'.&lt;ref&gt;{{cite web|url=http://www.web2access.org.uk/product/172/ |title=Results for Prezi|publisher=Web2Access JISC TechDis |date= |accessdate=2014-03-01}}&lt;/ref&gt; American educators have been advised that Prezi is not compliant with the Americans With Disabilities Act (ADA/508) and that an accessible PowerPoint version of the presentation should be provided online for students where a Prezi has been used.&lt;ref&gt;{{cite web|url=http://barrydahl.com/2015/01/08/accessibility-concerns-of-using-prezi-in-education/|title=Accessibility Concerns of Using Prezi in Education |publisher=Barry Dahl |date= |accessdate=2015-10-07}}&lt;/ref&gt;

==See also==
* [[Scientific visualization]]
* [[Data Presentation Architecture]]

==References==
{{reflist|30em}}

==External links==
{{Commons category|Mind maps}}
* {{Official website|https://www.prezi.com}}

{{Mindmaps}}
{{Presentation software}}
{{Notetaking softwares}}

[[Category:Zoomable user interfaces]]
[[Category:Panorama viewers]]
[[Category:Presentation software]]
[[Category:Knowledge representation]]
[[Category:Diagrams]]
[[Category:Note-taking software]]</text>
      <sha1>fgmapfwmy8vvvdokww6xvpdml1vgj8p</sha1>
    </revision>
  </page>
  <page>
    <title>SMW+</title>
    <ns>0</ns>
    <id>21890258</id>
    <revision>
      <id>736878305</id>
      <parentid>736878299</parentid>
      <timestamp>2016-08-30T12:33:07Z</timestamp>
      <contributor>
        <username>David.moreno72</username>
        <id>16075528</id>
      </contributor>
      <minor />
      <comment>Reverted edits by [[Special:Contributions/208.131.186.116|208.131.186.116]] ([[User talk:208.131.186.116|talk]]) ([[WP:HG|HG]]) (3.1.19)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4317" xml:space="preserve">{{Infobox software
|name= SMW+
|screenshot=[[File:Fr Smw plus screenshot.png|300px|SMW+]]
|caption=
|developer=[http://www.diqa-pm.com/en/Main_Page DIQA-Projektmanagement GmbH]
|latest release version = 1.7.0
|latest release date = {{release date|2012|04|25}}
|discontinued = yes
|operating_system= [[Cross-platform]]
|programming language=[[PHP]]
|database=[[MySQL]]
|genre=[[MediaWiki]] extension
|license=[[GNU General Public License|GPL]]
|website=[http://semanticweb.org/wiki/SMW%2B SMW+ homepage]
}}

'''SMW+''' is an [[open source]] [[Software suite|software bundle]] composed of the [[wiki software|wiki application]] [[MediaWiki]] along with a number of its extensions, that was developed by the [[Germany|German]] software company [[Ontoprise GmbH]] from 2007 to 2012. In 2012, Ontoprise GmbH filed for bankruptcy&lt;ref&gt;{{cite web |url=http://www.econo.de/no_cache/nachrichten/einzelansicht/article/ontoprise-stellt-insolvenzantrag.html|title=Ontoprise stellt Insolvenzantrag |trans_title=Ontoprise starts insolvency proceedings |language=German |date=3 May 2012 |publisher=econo |accessdate=30 July 2012}}&lt;/ref&gt; and went out of business. DIQA-Projektmanagement GmbH, a start-up founded by former Ontoprise employees,&lt;ref&gt;{{cite web |author=Michael Erdmann |title=SMW+ is dead, long live SMW+ |url=http://sourceforge.net/mailarchive/message.php?msg_id=29589354 |work=[[MediaWiki]] users mailing list |date=25 July 2012 |accessdate=30 July 2012}}&lt;/ref&gt; now offers support for the software in SMW+, though under the name "[http://diqa-pm.com/en/DataWiki DataWiki]".

== Details ==

SMW+'s extensions include, most notably, [[Semantic MediaWiki]] and the [http://semanticweb.org/wiki/Halo_Extension Halo Extension]. Cumulatively, SMW+ functions as a [[semantic wiki]], and is also meant to serve as an [[enterprise wiki]] for use within companies, for applications such as [[knowledge management]] and [[project management]].

The SMW+ platform was available in a number of formats including a Windows installer, Linux installer and [[VMware]] image.

SMW+ emerged from [[Project Halo]], a research project meant to provide a platform for collaborative knowledge engineering for [[Subject matter expert|domain experts]] in the [[biology]], [[chemistry]] and [[physics]] at the first stage.

SMW+ is used by the [[Intergovernmental Oceanographic Commission]] of [[UNESCO]] to power an online encyclopedia for [[oceanography]].{{citation needed|date=April 2013}}

== References ==
{{reflist}}
* [http://smwplus.net/index.php/Business_applications_with_SMW%2B ''Business applications with SMW+, a Semantic Enterprise Wiki'']. Michael Erdmann, Daniel Hansch.
* [http://smwplus.net/index.php/Practical_applications_of_Semantic_MediaWiki_in_commercial_environments ''Practical applications of Semantic MediaWiki in commercial environments - Case Study: semantic-based project management'']. Daniel Hansch, Hans-Peter Schnurr. Presented at the ESTC 2009.
* [http://swui.webscience.org/SWUI2008CHI/Pfisterer.pdf ''User-Centered Design and Evaluation of Interface Enhancements to the Semantic MediaWiki'']. Frederik Pfisterer, Markus Nitsche, Anthony Jameson and Catalin Barbu. Presented at the CHI2008 (Computer Human Interaction Conference).
* [http://www.academypublisher.com/jetwi/vol1/no1/jetwi01019496.pdf ''Semantic Wikis: A Comprehensible Introduction with Examples from the Health Sciences'']. [[Maged N. Kamel Boulos]]. Journal of Emerging Technologies in Web Intelligence, Vol. 1, No. 1, August 2009
* [http://smwplus.net/index.php/Towards_a_Collaborative_Semantic_Wiki-based_Approach_to_IT_Service_Management ''Towards a Collaborative Semantic Wiki-based Approach to IT Service Management'']. Frank Kleiner, Andreas Abecker. Proceedings of I-SEMANTICS ’09.

==External links==
* [http://semanticweb.org/wiki/SMW%2B SMW+] on semanticweb.org
* [http://www.semanticweb.com/main/semantic_mediawiki_development_picks_up_steam_138918.asp Article at semanticweb.com about SMW+]
* [http://videolectures.net/iswc08_greaves_swfttsotsw/ "Semantic Wikis: Fusing the two strands of the Semantic Web"] - Talk given by Mark Greaves at the ISWC 2008

{{DEFAULTSORT:Smw}}
[[Category:Semantic wiki software]]
[[Category:Knowledge representation]]
[[Category:Free software programmed in PHP]]
[[Category:MediaWiki extensions]]</text>
      <sha1>a745bqbvaxaj4kc171ic8bjbrick2v2</sha1>
    </revision>
  </page>
  <page>
    <title>Type–token distinction</title>
    <ns>0</ns>
    <id>14934822</id>
    <revision>
      <id>761239431</id>
      <parentid>760006896</parentid>
      <timestamp>2017-01-21T20:51:54Z</timestamp>
      <contributor>
        <username>KevinHaller</username>
        <id>30158544</id>
      </contributor>
      <minor />
      <comment>I think there was a typo.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9477" xml:space="preserve">[[File:Flock of birds at Rome.ogg|right|300px|thumb|Although this flock is made of the same ''type'' of bird, each individual bird is a different ''token''. (50 MB video of a [[Flock (birds)|flock of birds]] in Rome)|thumbtime=6]]
The '''type–token distinction''' is used in disciplines such as [[logic]], [[linguistics]], [[metalogic]], [[typography]], and [[computer programming]] to clarify what words mean.

The sentence "''they drive the same car''" is ambiguous. Do they drive the same ''type'' of car (the same model) or the same instance of a car type (a single vehicle)? Clarity requires us to distinguish words that represent abstract types from words that represent objects that embody or exemplify types. The type–token distinction separates types (representing abstract descriptive concepts) from tokens (representing objects that instantiate concepts).

For example: "bicycle" is a type that represents the concept of a bicycle; whereas "my bicycle" is a token that represents an object that instantiates that type. In the sentence "the bicycle is becoming more popular" the word "bicycle" is a type representing a concept; whereas in the sentence "the bicycle is in the garage" the word "bicycle" is a token representing a particular object.

(The distinction in [[computer programming]] between [[class (software)|classes]] and [[object (computer science)|objects]] is related, though in this context, "class" sometimes refers to a set of objects (with class-level attribute or operations) rather than a description of an object in the set.)

The words type, concept, property, quality, feature and attribute (all used in describing things) tend to be used with different verbs. E.g. Suppose a rose bush is defined as a plant that is "thorny", "flowering" and "bushy". You might say a rose bush ''instantiates'' these three types, or ''embodies'' these three concepts, or ''exhibits'' these three properties, or ''possesses'' these three qualities, features or attributes.

Property types (e.g "height in metres" or "thorny") are often understood [[ontology|ontologically]] as concepts. Property instances (e.g. height = 1.74) are sometimes understood as measured values, and sometimes understood as sensations or observations of reality.

Some say types exist in descriptions of objects, but not as tangible [[physical object]]s. They say one can show someone a particular bicycle, but cannot show someone the type "bicycle", as in "''the bicycle'' is popular.". However types do exist in sense that they appear in mental and documented models.

Some say tokens represent objects that are tangible, exist in space and time as physical matter and/or energy. However, tokens can represent intangible objects of types such as "thought", "tennis match", "government" and "act of kindness".

== Occurrences ==
There is a related distinction very closely connected with the type-token distinction. This distinction is the distinction between an object, or type of object, and an '''''occurrence''''' of it. In this sense, an occurrence is not necessarily a token. Considering the sentence: "[[A rose is a rose is a rose]]". We may equally correctly state that there are eight or three words in the sentence. There are, in fact, three word types in the sentence: "rose", "is" and "a". There are eight word tokens in a token copy of the line. The line itself is a type. There are not eight word types in the line. It contains (as stated) only the three word types, 'a', 'is' and 'rose', each of which is unique. So what do we call what there are eight of? They are occurrences of words. There are three occurrences of the word type 'a', two of 'is' and three of 'rose'.

The need to distinguish tokens of types from occurrences of types arises, not just in linguistics, but whenever types of things have other types of things occurring in them.&lt;ref&gt;Stanford Encyclopedia of Philosophy, ''[http://plato.stanford.edu/entries/types-tokens Types and Tokens]''&lt;/ref&gt; Reflection on the simple case of occurrences of [[numeral]]s is often helpful.{{citation needed|date=January 2017|reason=Claim made with no support or citation, term used without definition.}}

== Typography ==
In [[typography]], the type–token distinction is used to determine the presence of a text printed by [[movable type]]:&lt;ref&gt;[[Herbert E. Brekle|Brekle, Herbert E.]]: ''Die Prüfeninger Weiheinschrift von 1119. Eine paläographisch-typographische Untersuchung'', Scriptorium Verlag für Kultur und Wissenschaft, Regensburg 2005, ISBN 3-937527-06-0, p.&amp;nbsp;23&lt;/ref&gt;

{{quote|The defining criteria which a typographic print has to fulfill is that of the type identity of the various [[letter form]]s which make up the printed text. In other words: each letter form which appears in the text has to be shown as a particular instance ("token") of one and the same type which contains a reverse image of the printed [[Letter (alphabet)|letter]].}}

== Charles Sanders Peirce ==
:''There are only 26 letters in the [[English alphabet]] and yet there are more than 26 letters in this [[sentence (linguistics)|sentence]]. Moreover, every time a child writes the alphabet 26 new letters have been created.''

The word 'letters' was used three times in the above paragraph, each time in a different meaning. The word 'letters' is one of many words having "type–token ambiguity". This section disambiguates 'letters' by separating the three senses using terminology standard in logic today. The key distinctions were first made by the American logician-philosopher [[Charles Sanders Peirce]] in 1906 using terminology that he established.&lt;ref&gt;Charles Sanders Peirce, Prolegomena to an apology for pragmaticism, Monist, vol.16 (1906), pp. 492–546.&lt;/ref&gt;

The letters that are created by writing are physical objects that can be destroyed by various means: these are letter TOKENS or letter INSCRIPTIONS. The 26 letters of the alphabet are letter TYPES or letter FORMS.

'''Peirce's type–token distinction''', also applies to words, sentences, paragraphs, and so on: to anything in a universe of discourse of character-string theory, or [[concatenation theory]]. There is only one word type spelled el-ee-tee-tee-ee-ar,&lt;ref&gt;Using a variant of [[Alfred Tarski]]'s structural-descriptive naming found in [[John Corcoran (logician)|John Corcoran]] , Schemata: the Concept of Schema in the History of Logic, Bulletin of Symbolic Logic, vol. 12 (2006), pp. 219–40.&lt;/ref&gt; namely, 'letter'; but every time that word type is written, a new word token has been created.

Some logicians consider a word type to be the class of its tokens. Other logicians counter that the word type has a permanence and constancy not found in the class of its tokens. The type remains the same while the class of its tokens is continually gaining new members and losing old members.

The word type 'letter' uses only four letter types: el, ee, tee, and ar. Nevertheless, it uses ee twice and tee twice. In standard terminology, the word type 'letter' has six letter OCCURRENCES and the letter type ee OCCURS twice in the word type 'letter'. Whenever a word type is inscribed, the number of letter tokens created equals the number of letter occurrences in the word type.

Peirce's original words are the following.
"A common mode of estimating the amount of matter in a ... printed book is to count the number of words. There will ordinarily be about twenty 'thes' on a page, and, of course, they count as twenty words. In another sense of the word 'word,' however, there is but one word 'the' in the English language; and it is impossible that this word should lie visibly on a page, or be heard in any voice .... Such a ... Form, I propose to term a Type. A Single ... Object ... such as this or that word on a single line of a single page of a single copy of a book, I will venture to call a Token. .... In order that a Type may be used, it has to be embodied in a Token which shall be a sign of the Type, and thereby of the object the Type signifies." – Peirce 1906, Ogden-Richards, 1923, 280-1.

These distinctions are subtle but solid and easy to master. This section ends using the new terminology to disambiguate the first paragraph.

:''There are 26 letter types in the English alphabet and yet there are more than 26 letter occurrences in this sentence type. Moreover, every time a child writes the alphabet 26 new letter tokens have been created.''

== See also ==
* [[Formalism (philosophy)]]
* [[Is-a]]
* [[Class (philosophy)]]
* [[Type theory]]
* [[Type physicalism]]
* [[Mental model]]
* [[Map–territory relation]]
* [[Problem of universals#Peirce]]

==References==
{{reflist}}

===Sources===
*Baggin J., and Fosl, P. (2003) ''The Philosopher's Toolkit''. Blackwell: 171-73. ISBN 978-0-631-22874-5.
*Peper F., Lee J., Adachi S.,Isokawa T. (2004) ''Token-Based Computing on Nanometer Scales'', Proceeding of the ToBaCo 2004 Workshop on Token Based Computing, Vol.1 pp.&amp;nbsp;1–18.

== External links ==
*[[The Stanford Encyclopedia of Philosophy]]: "[http://plato.stanford.edu/entries/types-tokens/ Types and Tokens]" by Linda Wetzel.

{{Metalogic}}
{{Metaphysics}}

{{DEFAULTSORT:Type-token distinction}}
[[Category:Metalogic]]
[[Category:Conceptual distinctions]]
[[Category:Knowledge representation]]
[[Category:Abstraction]]
[[Category:Concepts in metaphysics]]
[[Category:Articles containing video clips]]
[[Category:Philosophy of logic]]
[[Category:Philosophy of language]]
[[Category:Linguistics]]</text>
      <sha1>tp6fnz64fyz3bikxh0oj18zkc7t9u1w</sha1>
    </revision>
  </page>
  <page>
    <title>Designated Community</title>
    <ns>0</ns>
    <id>39348172</id>
    <revision>
      <id>572881136</id>
      <parentid>554361624</parentid>
      <timestamp>2013-09-14T12:49:16Z</timestamp>
      <contributor>
        <username>Trivialist</username>
        <id>5360838</id>
      </contributor>
      <comment>moving {{library-stub}} to bottom</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="600" xml:space="preserve">In information and archival communities, a '''Designated Community''' is an identified group of potential consumers who should be able to understand a particular set of information. These consumers may consist of multiple communities, are designated by the archive, and may change over time.&lt;ref&gt;{{cite web|title=Reference Model for an Open Archival Information System (ISO 14721:2012)|url=http://public.ccsds.org/publications/archive/650x0m2.pdf}}&lt;/ref&gt;


==References==
{{reflist}}
{{library-stub}}

[[Category:Archival science]]
[[Category:Knowledge representation]]
[[Category:Digital libraries]]</text>
      <sha1>ny3k6isb4zgs5mybbw7c8h2l9aedaf0</sha1>
    </revision>
  </page>
  <page>
    <title>Knowledge representation and reasoning</title>
    <ns>0</ns>
    <id>16920</id>
    <revision>
      <id>758047669</id>
      <parentid>756355431</parentid>
      <timestamp>2017-01-03T04:45:23Z</timestamp>
      <contributor>
        <username>BrianPansky</username>
        <id>22359645</id>
      </contributor>
      <comment>/* Ontology engineering */ removed original research, personal essay type thing.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="31797" xml:space="preserve">'''Knowledge representation and reasoning''' ('''KR''') is the field of [[artificial intelligence]] (AI) dedicated to representing information about the world in a form that a computer system can utilize to solve complex tasks such as diagnosing a medical condition or having a dialog in a [[natural language]]. Knowledge representation incorporates findings from psychology{{citation needed|date=February 2016}} about how humans solve problems and represent knowledge in order to design [[Formalism (mathematics)|formalisms]] that will make complex systems easier to design and build.  Knowledge representation and reasoning also incorporates findings from [[logic]] to automate various kinds of ''reasoning'', such as the application of rules or the relations of [[Set theory|sets]] and [[subset]]s.

Examples of knowledge representation formalisms include [[Semantic network|semantic nets]], [[systems architecture]], [[Frame (artificial intelligence)|Frames]], Rules, and [[Ontology (information science)|ontologies]]. Examples of [[automated reasoning]] engines include [[inference engine]]s, [[automated theorem proving|theorem prover]]s, and classifiers.

== History ==

The earliest work in computerized knowledge representation was focused on general problem solvers such as the [[General Problem Solver]] (GPS) system developed by [[Allen Newell]] and [[Herbert A. Simon]] in 1959. These systems featured data structures for planning and decomposition. The system would begin with a goal. It would then decompose that goal into sub-goals and then set out to construct strategies that could accomplish each subgoal.

In these early days of AI, general search algorithms such as [[A*]] were also developed.  However, the amorphous problem definitions for systems such as GPS meant that they worked only for very constrained toy domains (e.g. the "[[blocks world]]"). In order to tackle non-toy problems, AI researchers such as [[Ed Feigenbaum]] and [[Rick Hayes-Roth|Frederick Hayes-Roth]] realized that it was necessary to focus systems on more constrained problems.

It was the failure of these efforts that led to the [[cognitive revolution]] in psychology and to the phase of AI focused on knowledge representation that resulted in [[expert systems]] in the 1970s and 80s, [[Production system (computer science)|production systems]], [[frame language]]s, etc. Rather than general problem solvers, AI changed its focus to expert systems that could match human competence on a specific task, such as medical diagnosis.

Expert systems gave us the terminology still in use today where AI systems are divided into a Knowledge Base with facts about the world and rules and an inference engine that applies the rules to the [[knowledge base]] in order to answer questions and solve problems. In these early systems the knowledge base tended to be a fairly flat structure, essentially assertions about the values of variables used by the rules.&lt;ref&gt;{{cite book|last=Hayes-Roth|first=Frederick|title=Building Expert Systems|year=1983|publisher=Addison-Wesley|isbn=0-201-10686-8|author2=Donald Waterman |author3=Douglas Lenat }}&lt;/ref&gt;

In addition to expert systems, other researchers developed the concept of [[Frame language|Frame based languages]] in the mid 1980s. A frame is similar to an object class: It is an abstract description of a category describing things in the world, problems, and potential solutions. Frames were originally used on systems geared toward human interaction, e.g. [[natural language understanding|understanding natural language]] and the social settings in which various default expectations such as ordering food in a restaurant narrow the search space and allow the system to choose appropriate responses to dynamic situations.

It wasn't long before the frame communities and the rule-based researchers realized that there was synergy between their approaches. Frames were good for representing the real world, described as classes, subclasses, slots (data values) with various constraints on possible values. Rules were good for representing and utilizing complex logic such as the process to make a medical diagnosis. Integrated systems were developed that combined Frames and Rules. One of the most powerful and well known was the 1983 [[Knowledge Engineering Environment]] (KEE) from [[IntelliCorp (software)|Intellicorp]]. KEE had a complete rule engine with [[forward chaining|forward]] and [[backward chaining]]. It also had a complete frame based knowledge base with triggers, slots (data values), inheritance, and message passing. Although message passing originated in the object-oriented community rather than AI it was quickly embraced by AI researchers as well in environments such as KEE and in the operating systems for Lisp machines from [[Symbolics]], [[Xerox]], and [[Texas Instruments]].&lt;ref&gt;{{cite journal|last=Mettrey|first=William|title=An Assessment of Tools for Building Large Knowledge-Based Systems|journal=AI Magazine|year=1987|volume= 8| issue = 4|url=http://www.aaai.org/ojs/index.php/aimagazine/article/viewArticle/625}}&lt;/ref&gt;

The integration of Frames, rules, and object-oriented programming was significantly driven by commercial ventures such as KEE and Symbolics spun off from various research projects. At the same time as this was occurring, there was another strain of research which was less commercially focused and was driven by mathematical logic and automated theorem proving.  One of the most influential languages in this research was the [[KL-ONE]] language of the mid 80's. KL-ONE was a [[frame language]] that had a rigorous semantics, formal definitions for concepts such as an [[Is-a|Is-A relation]].&lt;ref&gt;{{cite journal|last=Brachman|first=Ron|title=A Structural Paradigm for Representing Knowledge|journal=Bolt, Beranek, and Neumann Technical Report|year=1978|issue=3605}}&lt;/ref&gt; KL-ONE and languages that were influenced by it such as Loom had an automated reasoning engine that was based on formal logic rather than on IF-THEN rules. This reasoner is called the classifier. A classifier can analyze a set of declarations and infer new assertions, for example, redefine a class to be a subclass or superclass of some other class that wasn't formally specified. In this way the classifier can function as an inference engine, deducing new facts from an existing knowledge base. The classifier can also provide consistency checking on a knowledge base (which in the case of KL-ONE languages is also referred to as an Ontology).&lt;ref&gt;{{cite journal|last=MacGregor|first=Robert|title=Using a description classifier to enhance knowledge representation|journal=IEEE Expert|date=June 1991|volume=6|issue=3|url=http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=87683&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D87683|accessdate=10 November 2013|doi=10.1109/64.87683|pages=41–46}}&lt;/ref&gt;

Another area of knowledge representation research was the problem of common sense reasoning.  One of the first realizations from trying to make software that can function with human natural language was that humans regularly draw on an extensive foundation of knowledge about the real world that we simply take for granted but that is not at all obvious to an artificial agent.  Basic principles of common sense physics, causality, intentions, etc. An example is the [[Frame problem]], that in an event driven logic there need to be axioms that state things maintain position from one moment to the next unless they are moved by some external force. In order to make a true artificial intelligence agent that can converse with humans using natural language and can process basic statements and questions about the world it is essential to represent this kind of knowledge. One of the most ambitious programs to tackle this problem was Doug Lenat's [[Cyc]] project. Cyc established its own Frame language and had large numbers of analysts document various areas of common sense reasoning in that language. The knowledge recorded in Cyc included common sense models of time, causality, physics, intentions, and many others.&lt;ref&gt;{{cite book|last=Lenat|first=Doug|title=Building Large Knowledge-Based Systems: Representation and Inference in the Cyc Project|publisher=Addison-Wesley|isbn=978-0201517521|author2=R. V. Guha |date=January 1990}}&lt;/ref&gt;

The starting point for knowledge representation is the ''knowledge representation hypothesis'' first formalized by [[Brian Cantwell Smith|Brian C. Smith]] in 1985:&lt;ref&gt;{{cite book|last=Smith|first=Brian C.|title=Readings in Knowledge Representation|year=1985|publisher=Morgan Kaufmann|isbn=0-934613-01-X|pages=31–40|editor=Ronald Brachman and Hector J. Levesque|chapter=Prologue to Reflections and Semantics in a Procedural Language}}&lt;/ref&gt;

&lt;blockquote&gt;''Any mechanically embodied intelligent process will be {{sic|comprised |hide=y|of}} structural ingredients that a) we as external observers naturally take to represent a propositional account of the knowledge that the overall process exhibits, and b) independent of such external semantic attribution, play a formal but causal and essential role in engendering the behavior that manifests that knowledge.''&lt;/blockquote&gt;

Currently one of the most active areas of knowledge representation research are projects associated with the [[Semantic web]]. The semantic web seeks to add a layer of semantics (meaning) on top of the current Internet. Rather than indexing web sites and pages via keywords, the semantic web creates large [[ontologies]] of concepts. Searching for a concept will be more effective than traditional text only searches. Frame languages and automatic classification play a big part in the vision for the future semantic web. The automatic classification gives developers technology to provide order on a constantly evolving network of knowledge. Defining ontologies that are static and incapable of evolving on the fly would be very limiting for Internet-based systems. The classifier technology provides the ability to deal with the dynamic environment of the Internet.

Recent projects funded primarily by the [[Defense Advanced Research Projects Agency]] (DARPA) have integrated frame languages and classifiers with markup languages based on XML. The [[Resource Description Framework]] (RDF) provides the basic capability to define classes, subclasses, and properties of objects. The [[Web Ontology Language]] (OWL) provides additional levels of semantics and enables integration with classification engines.&lt;ref&gt;{{cite journal|last=Berners-Lee|first=Tim |author2=James Hendler |author3=Ora Lassila|title=The Semantic Web A new form of Web content that is meaningful to computers will unleash a revolution of new possibilities|journal=Scientific American|date=May 17, 2001|url=http://www.cs.umd.edu/~golbeck/LBSC690/SemanticWeb.html|doi=10.1038/scientificamerican0501-34|volume=284|pages=34–43}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.w3.org/2001/sw/BestPractices/SE/ODSD/|title=A Semantic Web Primer for Object-Oriented Software Developers|last1=Knublauch|first1=Holger|last2=Oberle|first2=Daniel|last3=Tetlow|first3=Phil|last4=Wallace|first4=Evan|publisher=[[W3C]]|date=2006-03-09|accessdate=2008-07-30}}&lt;/ref&gt;

== Overview ==
Knowledge-representation is the field of artificial intelligence that focuses on designing computer representations that capture information about the world that can be used to solve complex problems. The justification for knowledge representation is that conventional [[procedural code]] is not the best formalism to use to solve complex problems. Knowledge representation makes complex software easier to define and maintain than procedural code and can be used in [[expert systems]].

For example, talking to experts in terms of business rules rather than code lessens the semantic gap between users and developers and makes development of complex systems more practical.

Knowledge representation goes hand in hand with [[automated reasoning]] because one of the main purposes of explicitly representing knowledge is to be able to reason about that knowledge, to make inferences, assert new knowledge, etc. Virtually all [[knowledge representation language]]s have a reasoning or inference engine as part of the system.&lt;ref&gt;{{cite book|last=Hayes-Roth|first=Frederick|pages=6–7|title=Building Expert Systems|year=1983|publisher=Addison-Wesley|isbn=0-201-10686-8|author2=Donald Waterman |author3=Douglas Lenat }}&lt;/ref&gt;

A key trade-off in the design of a knowledge representation formalism is that between expressivity and practicality. The ultimate knowledge representation formalism in terms of expressive power and compactness is First Order Logic (FOL).  There is no more powerful formalism than that used by mathematicians to define general propositions about the world. However, FOL has two drawbacks as a knowledge representation formalism:  ease of use and practicality of implementation.  First order logic can be intimidating even for many software developers. Languages which do not have the complete formal power of FOL can still provide close to the same expressive power with a user interface that is more practical for the average developer to understand. The issue of practicality of implementation is that FOL in some ways is too expressive. With FOL it is possible to create statements (e.g. quantification over infinite sets) that would cause a system to never terminate if it attempted to verify them.

Thus, a subset of FOL can be both easier to use and more practical to implement. This was a driving motivation behind rule-based expert systems. IF-THEN rules provide a subset of FOL but a very useful one that is also very intuitive.  The history of most of the early AI knowledge representation formalisms; from databases to semantic nets to theorem provers and production systems can be viewed as various design decisions on whether to emphasize expressive power or computability and efficiency.&lt;ref&gt;{{cite book|last=Levesque|first=Hector|title=Reading in Knowledge Representation|year=1985|publisher=Morgan Kaufmann|isbn=0-934613-01-X|page=49|author2=Ronald Brachman |editor=Ronald Brachman and Hector J. Levesque|chapter=A Fundamental Tradeoff in Knowledge Representation and Reasoning|quote=The good news in reducing KR service to theorem proving is that we now have a very clear, very specific notion of what the KR system should do; the bad new is that it is also clear that the services can not be provided... deciding whether or not a sentence in FOL is a theorem... is unsolvable.}}&lt;/ref&gt;

In a key 1993 paper on the topic, Randall Davis of [[Massachusetts Institute of Technology|MIT]] outlined five distinct roles to analyze a knowledge representation framework:&lt;ref&gt;{{cite journal|last=Davis|first=Randall|author2=Howard Shrobe |author3=Peter Szolovits |title=What Is a Knowledge Representation?|journal=AI Magazine|date=Spring 1993|volume=14|issue=1|pages=17–33|url=http://www.aaai.org/ojs/index.php/aimagazine/article/view/1029/947}}&lt;/ref&gt; 
* A knowledge representation (KR) is most fundamentally a surrogate, a substitute for the thing itself, used to enable an entity to determine consequences by thinking rather than acting, i.e., by reasoning about the world rather than taking action in it.
* It is a set of ontological commitments, i.e., an answer to the question: In what terms should I think about the world?
* It is a fragmentary theory of intelligent reasoning, expressed in terms of three components: (i) the representation's fundamental conception of intelligent reasoning; (ii) the set of inferences the representation sanctions; and (iii) the set of inferences it recommends.
* It is a medium for pragmatically efficient computation, i.e., the computational environment in which thinking is accomplished. One contribution to this pragmatic efficiency is supplied by the guidance a representation provides for organizing information so as to facilitate making the recommended inferences.
* It is a medium of human expression, i.e., a language in which we say things about the world."

Knowledge representation and reasoning are a key enabling technology for the [[Semantic web]]. Languages based on the Frame model with automatic classification provide a layer of semantics on top of the existing Internet. Rather than searching via text strings as is typical today it will be possible to define logical queries and find pages that map to those queries.&lt;ref&gt;{{cite journal|last=Berners-Lee|first=Tim |author2=James Hendler |author3=Ora Lassila|title=The Semantic Web A new form of Web content that is meaningful to computers will unleash a revolution of new possibilities|journal=Scientific American|date=May 17, 2001|url=http://www.cs.umd.edu/~golbeck/LBSC690/SemanticWeb.html|doi=10.1038/scientificamerican0501-34|volume=284|pages=34–43}}&lt;/ref&gt; The automated reasoning component in these systems is an engine known as the classifier. Classifiers focus on the [[Subsumption relation|subsumption]] relations in a knowledge base rather than rules. A classifier can infer new classes and dynamically change the ontology as new information becomes available. This capability is ideal for the ever changing and evolving information space of the Internet.&lt;ref&gt;{{cite web|last=Macgregor|first=Robert|title=Retrospective on Loom|url=http://www.isi.edu/isd/LOOM/papers/macgregor/Loom_Retrospective.html|work=isi.edu|publisher=Information Sciences Institute|accessdate=10 December 2013|date=August 13, 1999}}&lt;/ref&gt;

The Semantic web integrates concepts from knowledge representation and reasoning with markup languages based on XML.  The [[Resource Description Framework]] (RDF) provides the basic capabilities to define knowledge-based objects on the Internet with basic features such as Is-A relations and object properties. The [[Web Ontology Language]] (OWL) adds additional semantics and integrates with automatic classification reasoners.&lt;ref&gt;{{cite web|url=http://www.w3.org/2001/sw/BestPractices/SE/ODSD/|title=A Semantic Web Primer for Object-Oriented Software Developers|last1=Knublauch|first1=Holger|last2=Oberle|first2=Daniel|last3=Tetlow|first3=Phil|last4=Wallace|first4=Evan|publisher=[[W3C]]|date=2006-03-09|accessdate=2008-07-30}}&lt;/ref&gt;

== Characteristics ==
In 1985, Ron Brachman categorized the core issues for knowledge representation as follows:&lt;ref&gt;{{cite book|last=Brachman|first=Ron|title=Readings in Knowledge Representation|year=1985|publisher=Morgan Kaufmann|isbn=0-934613-01-X|pages=XVI-XVII|editor=Ronald Brachman and Hector J. Levesque|chapter=Introduction}}&lt;/ref&gt; 
*Primitives. What is the underlying framework used to represent knowledge? [[Semantic network]]s were one of the first knowledge representation primitives. Also, data structures and algorithms for general fast search. In this area there is a strong overlap with research in data structures and algorithms in computer science. In early systems the Lisp programming language which was modeled after the [[lambda calculus]] was often used as a form of functional knowledge representation. Frames and Rules were the next kind of primitive. Frame languages had various mechanisms for expressing and enforcing constraints on frame data. All data in frames are stored in slots. Slots are analogous to relations in entity-relation modeling and to object properties in object-oriented modeling. Another technique for primitives is to define languages that are modeled after [[First Order Logic]] (FOL). The most well known example is Prolog but there are also many special purpose theorem proving environments. These environments can validate logical models and can deduce new theories from existing models. Essentially they automate the process a logician would go through in analyzing a model. Theorem proving technology had some specific practical applications in the areas of software engineering. For example, it is possible to prove that a software program rigidly adheres to a formal logical specification.
*Meta-Representation.  This is also known as the issue of [[Reflection (computer programming)|reflection]] in computer science. It refers to the capability of a formalism to have access to information about its own state. An example would be the meta-object protocol in [[Smalltalk]] and [[CLOS]] that gives developers run time access to the class objects and enables them to dynamically redefine the structure of the knowledge base even at run time. Meta-representation means the knowledge representation language is itself expressed in that language. For example, in most Frame based environments all frames would be instances of a frame class. That class object can be inspected at run time so that the object can understand and even change its internal structure or the structure of other parts of the model. In rule-based environments the rules were also usually instances of rule classes. Part of the meta protocol for rules were the meta rules that prioritized rule firing. 
*[[Completeness (logic)|Incompleteness]]. Traditional logic requires additional axioms and constraints to deal with the real world as opposed to the world of mathematics. Also, it is often useful to associate degrees of confidence with a statement. I.e., not simply say "Socrates is Human" but rather "Socrates is Human with confidence 50%". This was one of the early innovations from [[expert system]]s research which migrated to some commercial tools, the ability to associate certainty factors with rules and conclusions. Later research in this area is known as [[Fuzzy Logic]].&lt;ref&gt;{{cite journal|last=Bih|first=Joseph|title=Paradigm Shift: An Introduction to Fuzzy Logic|journal=IEEE POTENTIALS|year=2006|url=http://www.cse.unr.edu/~bebis/CS365/Papers/FuzzyLogic.pdf|accessdate=24 December 2013}}&lt;/ref&gt; 
*Definitions and [[Universals]] vs. facts and defaults.  Universals are general statements about the world such as "All humans are mortal". Facts are specific examples of universals such as "Socrates is a human and therefore mortal". In logical terms definitions and universals are about universal quantification while facts and defaults are about existential quantifications. All forms of knowledge representation must deal with this aspect and most do so with some variant of set theory, modeling universals as sets and subsets and definitions as elements in those sets. 
*[[Non-monotonic logic|Non-Monotonic reasoning]]. Non-monotonic reasoning allows various kinds of hypothetical reasoning. The system associates facts asserted with the rules and facts used to justify them and as those facts change updates the dependent knowledge as well. In rule based systems this capability is known as a [[truth maintenance system]].&lt;ref&gt;{{cite journal|last=Zlatarva|first=Nellie|title=Truth Maintenance Systems and their Application for Verifying Expert System Knowledge Bases|journal=Artificial Intelligence Review|year=1992|volume=6|pages=67–110|url=http://link.springer.com/article/10.1007%2FBF00155580#page-2|accessdate=25 December 2013|doi=10.1007/bf00155580}}&lt;/ref&gt; 
*[[Functional completeness|Expressive Adequacy]]. The standard that Brachman and most AI researchers use to measure expressive adequacy is usually First Order Logic (FOL). Theoretical limitations mean that a full implementation of FOL is not practical. Researchers should be clear about how expressive (how much of full FOL expressive power) they intend their representation to be.&lt;ref&gt;{{cite book|last=Levesque|first=Hector|title=Reading in Knowledge Representation|year=1985|publisher=Morgan Kaufmann|isbn=0-934613-01-X|pages = 41–70|author2=Ronald Brachman |editor=Ronald Brachman and Hector J. Levesque|chapter=A Fundamental Tradeoff in Knowledge Representation and Reasoning}}&lt;/ref&gt;
*Reasoning Efficiency. This refers to the run time efficiency of the system. The ability of the knowledge base to be updated and the reasoner to develop new inferences in a reasonable period of time. In some ways this is the flip side of expressive adequacy. In general the more powerful a representation, the more it has expressive adequacy, the less efficient its [[automated reasoning]] engine will be. Efficiency was often an issue, especially for early applications of knowledge representation technology. They were usually implemented in interpreted environments such as Lisp which were slow compared to more traditional platforms of the time.

== Ontology engineering ==
{{main article|Ontology engineering|Ontology language}}

In the early years of knowledge-based systems the knowledge-bases were fairly small. The knowledge-bases that were meant to actually solve real problems rather than do proof of concept demonstrations needed to focus on well defined problems. So for example, not just medical diagnosis as a whole topic but medical diagnosis of certain kinds of diseases.

As knowledge-based technology scaled up the need for larger knowledge bases and for modular knowledge bases that could communicate and integrate with each other became apparent. This gave rise to the discipline of ontology engineering, designing and building large knowledge bases that could be used by multiple projects. One of the leading research projects in this area was the Cyc project. Cyc was an attempt to build a huge encyclopedic knowledge base that would contain not just expert knowledge but common sense knowledge. In designing an artificial intelligence agent it was soon realized that representing common sense knowledge, knowledge that humans simply take for granted, was essential to make an AI that could interact with humans using natural language. Cyc was meant to address this problem. The language they defined was known as [[CycL]].

After CycL, a number of [[ontology language]]s have been developed.  Most are [[declarative language]]s, and are either [[frame language]]s, or are based on [[first-order logic]]. Modularity—the ability to define boundaries around specific domains and problem spaces—is essential for these languages because as stated by Tom Gruber, "Every ontology is a treaty- a social agreement among people with common motive in sharing." There are always many competing and differing views that make any general purpose ontology impossible. A general purpose ontology would have to be applicable in any domain and different areas of knowledge need to be unified.&lt;ref&gt;Russell, Stuart J.; Norvig, Peter (2010), Artificial Intelligence: A Modern Approach (3rd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-604259-7, p. 437-439&lt;/ref&gt;

There is a long history of work attempting to build ontologies for a variety of task domains, e.g., an ontology for liquids,&lt;ref&gt;Hayes P, Naive physics I: Ontology for liquids. University of Essex report, 1978, Essex, UK.&lt;/ref&gt; the lumped element model widely used in representing electronic circuits (e.g.,&lt;ref&gt;Davis R, Shrobe H E, Representing Structure and Behavior of Digital Hardware, IEEE Computer, Special Issue on Knowledge Representation, 16(10):75-82.&lt;/ref&gt;), as well as ontologies for time, belief, and even programming itself. Each of these offers a way to see some part of the world.
The lumped element model, for instance, suggests that we think of circuits in terms of components with connections between them, with signals flowing instantaneously along the connections. This is a useful view, but not the only possible one. A different ontology arises if we need to attend to the electrodynamics in the device: Here signals propagate at finite speed and an object (like a resistor) that was previously viewed as a single component with an I/O behavior may now have to be thought of as an extended medium through which an electromagnetic wave flows.

Ontologies can of course be written down in a wide variety of languages and notations (e.g., logic, LISP, etc.); the essential information is not the form of that language but the content, i.e., the set of concepts offered as a way of thinking about the world. Simply put, the important part is notions like connections and components, not the choice between writing them as predicates or LISP constructs.

The commitment made selecting one or another ontology can produce a sharply different view of the task at hand. Consider the difference that arises in selecting the lumped element view of a circuit rather than the electrodynamic view of the same device. As a second example, medical diagnosis viewed in terms of rules (e.g., [[MYCIN]]) looks substantially different from the same task viewed in terms of frames (e.g., [[INTERNIST]]). Where MYCIN sees the medical world as made up of empirical associations connecting symptom to disease, INTERNIST sees a set of prototypes, in particular prototypical diseases, to be matched against the case at hand.

== See also ==
* [[Chunking (psychology)]]
* [[Commonsense knowledge base]]
* [[Personal knowledge base]]
* [[Valuation-based system]]
* [[Conceptual Graph]]

== References ==

&lt;references/&gt;

== Further reading ==
* [[Ronald J. Brachman]]; [http://citeseer.nj.nec.com/context/177306/0 What IS-A is and isn't. An Analysis of Taxonomic Links in Semantic Networks]; IEEE Computer, 16 (10); October 1983
* [[Ronald J. Brachman]], [[Hector J. Levesque]] ''Knowledge Representation and Reasoning'', Morgan Kaufmann, 2004 ISBN 978-1-55860-932-7
* [[Ronald J. Brachman]], [[Hector J. Levesque]] (eds) ''Readings in Knowledge Representation'', Morgan Kaufmann, 1985, ISBN 0-934613-01-X
* Chein, M., Mugnier, M.-L. (2009),''[http://www.lirmm.fr/gbkrbook/ Graph-based Knowledge Representation: Computational Foundations of Conceptual Graphs]'', Springer, 2009,ISBN 978-1-84800-285-2.
* Randall Davis, Howard Shrobe, and Peter Szolovits; [http://citeseer.ist.psu.edu/davis93what.html What Is a Knowledge Representation?] AI Magazine, 14(1):17-33,1993
* [[Ronald Fagin]], [[Joseph Y. Halpern]], [[Yoram Moses]], [[Moshe Y. Vardi]] ''Reasoning About Knowledge'', MIT Press, 1995, ISBN 0-262-06162-7
* Jean-Luc Hainaut, Jean-Marc Hick, Vincent Englebert, Jean Henrard, Didier Roland: [http://www.informatik.uni-trier.de/~ley/db/conf/er/HainautHEHR96.html Understanding Implementations of IS-A Relations]. ER 1996: 42-57
* Hermann Helbig: ''Knowledge Representation and the Semantics of Natural Language'', Springer, Berlin, Heidelberg, New York 2006
* Arthur B. Markman: ''Knowledge Representation''  Lawrence Erlbaum Associates, 1998
* [[John F. Sowa]]: ''Knowledge Representation'': Logical, Philosophical, and Computational Foundations. Brooks/Cole: New York, 2000
* Adrian Walker, Michael McCord, [[John F. Sowa]], and Walter G. Wilson: ''Knowledge Systems and Prolog'', Second Edition, Addison-Wesley, 1990

== External links ==
* [http://medg.lcs.mit.edu/ftp/psz/k-rep.html What is a Knowledge Representation?] by Randall Davis and others
* [http://www.makhfi.com/KCM_intro.htm Introduction to Knowledge Modeling] by Pejman Makhfi
* [http://www.inf.unibz.it/~franconi/dl/course/ Introduction to Description Logics course] by Enrico Franconi, Faculty of Computer Science, Free University of Bolzano, Italy
* [http://www.ccl.kuleuven.ac.be/LKR/html/datr.html DATR Lexical knowledge representation language]
* [http://www.isi.edu/isd/LOOM/LOOM-HOME.html Loom Project Home Page]
* [http://www.research.att.com/sw/tools/classic/tm/ijcai-95-with-scenario.html Description Logic in Practice: A CLASSIC Application]
* [http://www.dfki.uni-kl.de/ruleml/ The Rule Markup Initiative]
* [http://nelements.org Nelements KOS] - a non-free 3d knowledge representation system

{{Computer science}}
{{computable knowledge}}
{{Commons category|Knowledge representation}}

{{DEFAULTSORT:Knowledge Representation And Reasoning}}
[[Category:Knowledge representation| ]]
[[Category:Scientific modeling]]
[[Category:Programming paradigms]]
[[Category:Reasoning]]</text>
      <sha1>agiji1qopnxfbr9o82lq4yy4db64dyr</sha1>
    </revision>
  </page>
  <page>
    <title>East Pole–West Pole divide</title>
    <ns>0</ns>
    <id>21766677</id>
    <revision>
      <id>759259404</id>
      <parentid>759042875</parentid>
      <timestamp>2017-01-10T03:50:09Z</timestamp>
      <contributor>
        <username>AnomieBOT</username>
        <id>7611264</id>
      </contributor>
      <minor />
      <comment>[[User:AnomieBOT/docs/TemplateSubster|Substing templates]]: {{Multicol-end}}. See [[User:AnomieBOT/docs/TemplateSubster]] for info.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4635" xml:space="preserve">{{redirect|West Pole|the album by The Gathering|The West Pole|the location in Texas|The West Pole, Texas}}

The '''East Pole–West Pole divide''' in the fields of [[cognitive psychology]] and [[cognitive neuroscience]] is an intellectual schism between researchers subscribing to the [[psychological nativism|nativist]] and [[empiricism|empiricist]] schools of thought.  The term arose from the fact that much of the theory and research supporting [[psychological nativism|nativism]], [[modularity of mind]], and [[computational theory of mind]] originated at several universities located on the East Coast, including [[Harvard University]], the [[University of Michigan]], [[Massachusetts Institute of Technology]], and [[Tufts University]]. Conversely, much of the research and theory supporting [[empiricism]], [[emergentism]], and [[embodied cognition]] originated at several universities located on the West Coast, including the [[University of California, Berkeley]], the [[Salk Institute]], and, most notably, the [[University of California, San Diego]].  In reality, the divide is not so clear, with many universities and scholars on both coasts (as well as the Midwest and around the world) supporting each position, as well as more moderate positions in between the two extremes.  The phrase was coined by [[Jerry Fodor]] at an [[MIT]] conference on [[cognition]], at which he referred to another researcher as a "West Coast theorist," apparently unaware that the researcher worked at [[Yale University]].&lt;ref&gt;{{cite book |title=The Blank Slate:The Modern Denial of Human Nature |last=Pinker |first=Steven |authorlink=Steven Pinker |year=2003 |publisher=Penguin |location=New York |isbn=978-0-14-200334-3 }}&lt;/ref&gt;

Very few researchers adhere strictly to the extreme positions highlighted by the East Pole–West Pole debate.  That is, there are very few empiricists who believe in the [[John Locke|Lockean]] ideal of the ''[[tabula rasa]]'' (namely, that children are born with no innate knowledge or constraints), and there are very few nativists who agree with [[Jerry Fodor|Fodor's]] assertion that all concepts that are learned over the course of life are present in the mind prior to birth.  Nevertheless, most scholars within the fields of [[cognitive science]] and [[developmental psychology]] affiliate themselves with one of the two positions through the means of their research.

The two books best known for espousing the empiricist and nativist positions within the context of cognitive psychology are ''[[Rethinking Innateness]]'' by [[Jeffrey Elman]] et al. and ''[[The Modularity of Mind]]'' by [[Jerry Fodor]], respectively.  Incidentally, the authors are affiliated with the two institutions on which the East Pole–West Pole metaphor is based, [[UCSD]] and [[MIT]], affirming the relevance and pervasiveness of this moniker for the intellectual divide.

==Notable scholars with affiliations==

{{Col-begin}}
{{Col-break}}
'''Nativists'''
*[[Jerry Fodor]], [[Massachusetts Institute of Technology]]
*[[Steven Pinker]], [[Harvard University]]
*[[Lila R. Gleitman]], [[University of Pennsylvania]]
*[[Leda Cosmides]], [[University of California, Santa Barbara]]
*[[Elizabeth Spelke]], [[Harvard University]]
*[[Thomas Bever]], [[University of Arizona]]
*[[Daniel Dennett]], [[Tufts University]]
*[[Nancy Kanwisher]], [[Massachusetts Institute of Technology]]

{{Col-break}}
'''Empiricists'''
*[[Elizabeth Bates]], [[University of California, San Diego]]
*[[George Lakoff]], [[University of California, Berkeley]]
*[[Brian MacWhinney]], [[Carnegie Mellon University]]
*[[Jeffrey Elman]], [[University of California, San Diego]]
*[[Ronald Langacker]], [[University of California, San Diego]]
*[[Dan Slobin]], [[University of California, Berkeley]]
*[[David Rumelhart]], [[Stanford University]]
*[[James McClelland (psychologist)|James McClelland]], [[Stanford University]]

{{col-end}}

==See also==
*[[Nature and nurture]]
*[[Empiricism]]
*[[Psychological nativism]]
*[[Computational theory of mind]]
*[[Embodied cognition]]
*[[Reductionism]]
*[[Emergentism]]

==References==
{{reflist}}

==External links==
*[http://query.nytimes.com/gst/fullpage.html?res=9B05E4DA1230F937A35752C1A961958260&amp;sec=&amp;spon=&amp;pagewanted=all Recipe for a Brain: Cups of genes or a dash of experience? NY Times article]
*[http://www.edge.org/3rd_culture/lakoff/lakoff_p4.html George Lakoff's discussion of the philosophical roots of embodied cognition]

{{DEFAULTSORT:East Pole-West Pole divide}}
[[Category:Cognition]]
[[Category:Cognitive science]]
[[Category:Knowledge representation]]
[[Category:Arguments in philosophy of mind]]</text>
      <sha1>h064qwb0lgg4orcsor9jijmdnk0gsjq</sha1>
    </revision>
  </page>
  <page>
    <title>HiLog</title>
    <ns>0</ns>
    <id>34697893</id>
    <revision>
      <id>594331762</id>
      <parentid>591711354</parentid>
      <timestamp>2014-02-07T06:50:20Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor />
      <comment>[[WP:CHECKWIKI]] error fix for #61.  Punctuation goes before References. Do [[Wikipedia:GENFIXES|general fixes]] if a problem exists. - using [[Project:AWB|AWB]] (9916)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3842" xml:space="preserve">'''HiLog''' is a programming [[logic]] with higher-order syntax, which allows arbitrary terms to appear in predicate and function positions. However, the [[model theory]] of HiLog is first-order. Although syntactically HiLog strictly extends [[first order logic]], HiLog can be embedded into this logic.

HiLog is described in detail in
&lt;ref name="hilog-jlp"&gt;
W. Chen, M. Kifer and D.S. Warren (1993), [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.52.7860 ''HiLog: A Foundation for Higher-Order Logic Programming'']. Journal of Logic Programming, 1993.
&lt;/ref&gt;
.&lt;ref&gt;
W. Chen, M. Kifer and D.S. Warren (1989), [http://citeseerx.ist.psu.edu/showciting?cid=2016805 ''HiLog: A first order semantics for higher-order logic programming constructs'']. Proc. North American Logic Programming Conference, 1989.
&lt;/ref&gt; 
It was later extended in the direction of [[many-sorted logic]] in.&lt;ref&gt;
W. Chen and M. Kifer (1994), [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.56.4332 ''Sorted HiLog: Sorts in Higher-Order Logic Data Languages'']. Int’l Conference on Database Theory, Springer Lecture Notes in Computer Science #893.
&lt;/ref&gt;
Other contributions to the theory of HiLog include
&lt;ref&gt;
K.A. Ross (1994), [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.55.2148 ''On Negation in HiLog'']. Journal of Logic Programming, 1994.
&lt;/ref&gt;
.&lt;ref&gt;
J. de Bruijn and S. Heymans (2008), [http://www.kr.tuwien.ac.at/staff/bruijn/priv/publications/frames-predicates-fi.pdf ''On the Relationship between Description Logic-based and F-Logic-based Ontologies'']. Fundamenta Informaticae 82:3, 2008, pp. 213-236.
&lt;/ref&gt;

The [[XSB|XSB System]] parses HiLog syntax, but the integration of HiLog into XSB is only partial. In particular, HiLog is not integrated with the XSB module system. A full implementation of HiLog is available in the [[Flora-2|Flora-2 system]].

In,&lt;ref name="hilog-jlp"/&gt; it has been shown that HiLog can be embedded into [[first-order logic]] through a fairly simple transformation. For instance, &lt;tt&gt;p(X)(Y,Z(V)(W))&lt;/tt&gt; gets embedded as the following first-order term:

  apply(p(X),Y,apply(apply(Z,V),W))

Details can be found in.&lt;ref name="hilog-jlp"/&gt;

The [[Rule Interchange Format#FLD|Framework for Logic-Based Dialects]] (RIF-FLD) of the [[Rule Interchange Format]] (RIF) is largely based on the ideas underlying HiLog and [[F-logic]].

== Examples ==

In all the examples, below, capitalized symbols denote variables and the comma denotes [[logical conjunction]], as in most [[logic programming]] languages. The first and the second examples show that variables can appear in predicate positions. Predicates can even be complex terms, such as &lt;tt&gt;closure(P)&lt;/tt&gt; or &lt;tt&gt;maplist(F)&lt;/tt&gt; below. The third example shows that variables can also appear in place of atomic formulas, while the fourth example illustrates the use of variables in place of function symbols. The first example defines a generic transitive closure operator, which can be applied to an arbitrary binary predicate. The second example is similar. It defines a [[LISP]]-like mapping operator, which applies to an arbitrary binary predicate. The third example shows that the [[Prolog]] meta-predicate &lt;tt&gt;call/1&lt;/tt&gt; can be expressed in HiLog in a natural way and without the use of extra-logical features. The last example defines a predicate that traverses arbitrary binary trees represented as [[Term (logic)|first-order term]]s.
&lt;source lang="prolog"&gt;
  closure(P)(X,Y) &lt;- P(X,Y).
  closure(P)(X,Y) &lt;- P(X,Z), closure(P)(Z,Y).

  maplist(F)([],[]).
  maplist(F)([X|R],[Y|Z]) &lt;- F(X,Y), maplist(F)(R,Z).

  call(X) &lt;- X.

  traverse(X(L,R)) &lt;- traverse(L), traverse(R).
&lt;/source&gt;

==References==
{{reflist}}

[[Category:Logic programming languages]]
[[Category:Declarative programming languages]]
[[Category:Knowledge representation]]</text>
      <sha1>o1v2lx074q6c4wasjgdnpf7yxtcm1rf</sha1>
    </revision>
  </page>
  <page>
    <title>Enterprise Interoperability Framework</title>
    <ns>0</ns>
    <id>43071232</id>
    <revision>
      <id>756512284</id>
      <parentid>742707670</parentid>
      <timestamp>2016-12-24T20:18:54Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 1 sources and tagging 0 as dead. #IABot (v1.2.7.1)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6952" xml:space="preserve">{{Underlinked|date=June 2014}}
The '''Enterprise Interoperability Framework''' is used as a guideline for collecting and structuring knowledge/solution for [[enterprise interoperability]]. The Enterprise Interoperability Framework defines the domain and sub-domains for [[interoperability]] research and development in order to identify a set of pieces of knowledge for solving enterprise interoperability problems by removing barriers to interoperability.

==Existing Interoperability Frameworks==

Some existing works on interoperability have been carried out to define interoperability framework or reference models, in particular, the LISI &lt;ref name="C4ISR1998"/&gt; reference model, [[European Interoperability Framework]] (EIF),&lt;ref name="EIF2004"/&gt; IDEAS interoperability framework,&lt;ref name=" IDEAS2003"/&gt; [[Model Driven Interoperability|ATHENA]] interoperability framework,&lt;ref name="ATHENA2003"/&gt; and E-Health Interoperability Framework.&lt;ref name="NEHTA2006"/&gt; These existing approaches constitute the basis for the Enterprise Interoperability Framework.

The necessity to elaborate the Enterprise Interoperability Framework has been discussed in.&lt;ref name="INTEROP"/&gt; Existing interoperability frameworks do not explicitly address barriers to interoperability, which is a basic assumption of this research; they are not aimed at structuring interoperability knowledge with respect to their ability to remove various barriers.

The Enterprise Interoperability framework has three basic [[dimensions]]:

# Interoperability concerns defined the content (or aspect) of interoperation that may take place at various levels of the [[Business|enterprise]]. In the domain of Enterprise Interoperability, the following four interoperability concerns are identified : Data, Service, Process, and Business.&lt;ref name=" Guglielmina2005"/&gt; [[File:Interoperability Concerns Data, Service, Process, and Business.jpg|thumb|Interoperability Concerns:  Data, Service, Process, and Business]]
# Interoperability barriers: Interoperability barrier is a fundamental concept in defining the interoperability domain. Many interoperability issues are specific to particular application domains. These can be things like support for particular attributes, or particular access control regimes. Nevertheless, general barriers and problems of interoperability can be identified; and most of them being already addressed,&lt;ref name="EIF2004"/&gt;&lt;ref name=" Kasunic2004"/&gt;&lt;ref name="ERISA2004"/&gt; Consequently, the objective is to identify common barriers to interoperability. By the term ‘barrier’ we mean an ‘incompatibility’ or ‘mismatch’ which obstructs the sharing and exchanging of information. Three categories of barriers are identified: conceptual, technological and organisational.
# Interoperability approaches represents the different ways in which barriers can be removed (integrated, unified, and federated) 
 [[File:Basic Approaches to Develop Interoperability.jpg|thumb|Basic Approaches to Develop Interoperability]]

The Enterprise Interoperability Framework with its three basic dimensions is shown.
[[File:Enterprise Interoperability Framework.jpg|thumb|Enterprise Interoperability Framework]]

== Enterprise Interoperability Framework Use ==

The Enterprise Interoperability Framework allows to:

* Capture and structure interoperability knowledge/solutions in the framework through a barrier-driven approach 
* Provide support to enterprise interoperability engineers and industry end users to carry out their interoperability projects.

The Enterprise Interoperability Framework not only aims at structuring concepts, defining research domain and capturing knowledge, but also at helping [[industry|industries]] to solve their interoperability problems. When carrying out an interoperability project involving two particular enterprises, interoperability concerns and interoperability barriers between the two enterprises will be identified first and mapped to this Enterprise Interoperability Framework. Using the [[Enterprise architecture framework|framework]], existing interoperability degree can be characterised and targeted interoperability degree can be defined as the [[Goal|objective]] to meet. Then knowledge/solutions associated to the barriers and concerns can be searched in the framework, and solutions found will be proposed to users for possible adaptation and/or combination with other solutions to remove the identified barriers so that the required interoperability can be established.

== References ==
{{reflist|
refs=
&lt;ref name= C4ISR1998&gt;C4ISR (1998), Architecture Working Group (AWG), Levels of Information Systems Interoperability (LISI), 30 March 1998.&lt;/ref&gt;
&lt;ref name= EIF2004&gt;EIF (2004), European Interoperability Framework for PAN-European EGovernment services, IDA working document - Version 4.2 – January 2004.&lt;/ref&gt;
&lt;ref name= IDEAS2003&gt;IDEAS (2003), IDEAS Project Deliverables (WP1-WP7), Public reports, www.ideas-road map.net.&lt;/ref&gt;
&lt;ref name= ATHENA2003 &gt;ATHENA (2003): Advanced Technologies for Interoperability of Heterogeneous Enterprise Networks and their Applications, FP6-2002-IST-1, Integrated Project Proposal, April 2003.  Deriverable.&lt;/ref&gt;
&lt;ref name=NEHTA2006&gt;NEHTA (2006), Towards a Health Interop Framework, ({{cite web|url=http://www |title=Archived copy |accessdate=2011-05-24 |deadurl=yes |archiveurl=https://web.archive.org/web/20060326070849/http://www |archivedate=2006-03-26 |df= }}. providersedge.com/ehdocs/.../Towards_an_Interoperability_Framework.pdf)&lt;/ref&gt;
&lt;ref name= INTEROP &gt;INTEROP D1.1, Knowledge map of research in interoperability in the INTEROP NoE, WP1, Version 1, August 11th 2004.&lt;/ref&gt;
&lt;ref name= Guglielmina2005&gt;C. Guglielmina and A. Berre, Project A4 (Slide presentation), ATHENA Intermediate Audit 29.-30. September 2005, Athens, Greece.&lt;/ref&gt;
&lt;ref name= Kasunic2004&gt;Kasunic, M., Anderson, W.,: Measuring systems interoperability: challenges and opportunities, Software engineering measurement and analysis initiative, April 2004&lt;/ref&gt;
&lt;ref name= EIF2004&gt;EIF: European Interoperability Framework, Write Paper, Brussels, 18, Feb. 2004, http://www.comptia.org&lt;/ref&gt;
&lt;ref name= ERISA2004&gt;ERISA (The European Regional Information Society Association), A guide to Interoperability for Regional Initiatives, Brussels, September 2004.&lt;/ref&gt;
}}

== External links ==
* [http://www.interop-vlab.eu INTEROP-VLab]
* [http://interop-vlab.eu/ei_public_deliverables/interop-noe-deliverables/di-domain-interoperability/di-2-enterprise-interoperability-framework-and-knowledge-corpus/ DI.2.Enterprise Interoperability Framework and knowledge corpus]
* [http://interop-vlab.eu/ei_public_deliverables/interop-noe-deliverables/di-domain-interoperability/di-3_enterprise-interoperability-framework-and-knowledge-corpus/ DI.3.Enterprise Interoperability Framework and knowledge corpus]

[[Category:Interoperability]]
[[Category:Enterprise modelling]]
[[Category:Knowledge representation]]</text>
      <sha1>0wdfrz57tnbqvshxb3lxqc55hyhihln</sha1>
    </revision>
  </page>
  <page>
    <title>Open-world assumption</title>
    <ns>0</ns>
    <id>2692616</id>
    <revision>
      <id>626271390</id>
      <parentid>617470824</parentid>
      <timestamp>2014-09-19T23:05:16Z</timestamp>
      <contributor>
        <username>Lambiam</username>
        <id>745100</id>
      </contributor>
      <comment>/* top */ ce</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4199" xml:space="preserve">In a [[Mathematical logic|formal system of logic]] used for [[knowledge representation]], the '''open-world assumption''' is the assumption that the [[truth value]] of a [[statement (logic)|statement]] may be true irrespective of whether or not it is ''known'' to be true. It is the opposite of the [[closed-world assumption]], which holds that any statement that is true is also known to be true.

The open-world assumption (OWA) codifies the informal notion that in general no single agent or observer has complete knowledge, and therefore cannot make the closed-world assumption. The OWA limits the kinds of inference and deductions an agent can make to those that follow from statements that are known to the agent to be true. In contrast, the closed world assumption allows an agent to infer, from its lack of knowledge of a statement being true, anything that [[Logical consequence|follows from]] that statement being false.

Heuristically, the open-world assumption applies when we represent knowledge within a system as we discover it, and where we cannot guarantee that we have discovered or will discover complete information. In the OWA, statements about knowledge that are not included in or inferred from the knowledge explicitly recorded in the system may be considered unknown, rather than wrong or false. 

[[Semantic Web]] languages such as [[Web Ontology Language|OWL]] make the open-world assumption. The absence of a particular statement within the web means, in principle, that the statement has not been made explicitly yet, irrespective of whether it would be true or not, and irrespective of whether we believe that it would be true or not. In essence, from the absence of a statement alone, a deductive reasoner cannot (and must not) infer that the statement is false.

Many [[procedural programming language]]s and [[database]]s make the closed-world assumption. For example, if a typical airline database does not contain a seat assignment for a traveler, it is assumed that the traveler has not checked in. The closed-world assumption typically applies when a system has complete control over information; this is the case with many database applications where the [[database transaction]] system acts as a central broker and arbiter of concurrent requests by multiple independent clients (e.g., airline booking agents). There are, however, many databases with incomplete information: for example, one cannot assume that because there is no mention on a patient's history of a particular allergy, that the patient does not suffer from that allergy.

'''Example'''
  Statement: "Mary" "is a citizen of" "France"

  Question: Is Paul a citizen of France?

  "Closed world" (for example SQL) answer: No.
  "Open world" answer: Unknown.

Under OWA, failure to derive a fact does not imply the opposite. For example, assume we only know that Mary is a citizen of France. From this information we can neither conclude that Paul is not a citizen of France, nor that he is. Therefore, we admit the fact that our knowledge of the world is incomplete. The open-world assumption is closely related to the [[Monotonicity of entailment|monotonic]] nature of [[first-order logic]]: adding new information never falsifies a previous conclusion. Namely, if we subsequently learn that Paul is also a citizen of France, this does not change any earlier positive or negative conclusions.

The language of logic programs with [[Stable_model_semantics#Strong_negation|strong negation]] allows us to postulate the closed-world assumption for some predicates and leave the other predicates in the realm of the open-world assumption.

==See also==
*[[Closed-world assumption]]

==References==
*{{cite book |last1=Russell |first1=Stuart J. |authorlink1=Stuart J. Russell |last2=Norvig |first2=Peter |authorlink2=Peter Norvig |title=Artificial Intelligence: A Modern Approach |year=2010 |publisher=Prentice Hall |location=Upper Saddle River |isbn=9780136042594 |url=http://www.pearsonhighered.com/educator/product/Artificial-Intelligence-A-Modern-Approach/9780136042594.page |edition=3rd}}

{{DEFAULTSORT:Open-world assumption}}
[[Category:Logic programming]]
[[Category:Knowledge representation]]</text>
      <sha1>kxeeien3b4vb6mmmyaqtovwhd91yenw</sha1>
    </revision>
  </page>
  <page>
    <title>Frame (artificial intelligence)</title>
    <ns>0</ns>
    <id>9924067</id>
    <revision>
      <id>738214685</id>
      <parentid>714198912</parentid>
      <timestamp>2016-09-07T16:24:20Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <minor />
      <comment>capitalization</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5462" xml:space="preserve">'''Frames''' were proposed by [[Marvin Minsky]] in his 1974 article "A Framework for Representing Knowledge." A frame is an [[artificial intelligence]] [[data structure]] used to divide [[knowledge]] into substructures by representing "[[stereotype]]d situations." Frames are the primary data structure used in artificial intelligence [[frame language]]s. 

Frames are also an extensive part of [[knowledge representation and reasoning]] schemes. Frames were originally derived from semantic networks and are therefore part of structure based knowledge representations. According to Russell and Norvig's "Artificial Intelligence, A Modern Approach," structural representations assemble "...facts about particular object and even types and arrange the types into a large taxonomic hierarchy analogous to a biological taxonomy."

== Frame structure ==

The frame contains information on how to use the frame, what to expect next, and what to do when these expectations are not met. Some information in the frame is generally unchanged while other information, stored in "terminals,"{{clarify|date=January 2010|reason=Which type of 'terminal' is meant?}} usually change. Different frames may share the same terminals.

Each piece of information about a particular frame is held in a slot. The information can contain:

* Facts or Data
** Values (called facets)
* Procedures (also called procedural attachments)
** IF-NEEDED : deferred evaluation
** IF-ADDED : updates linked information
* Default Values
** For Data
** For Procedures
* Other Frames or Subframes

== Features and advantages ==

A frame's terminals are already filled with default values, which is based on how the human mind works. For example, when a person is told "a boy kicks a ball," most people will visualize a particular ball (such as a familiar [[soccer ball]]) rather than imagining some abstract ball with no attributes.

One particular strength of frame based knowledge representations is that, unlike semantic networks, they allow for exceptions in particular
instances. This gives frames an amount of flexibility that allow representations of real world phenomena to be reflected more accurately.

Like [[semantic networks]], frames can be queried using spreading activation. Following the rules of inheritance, any value given to a slot that is inherited by subframes will be updated (IF-ADDED) to the corresponding slots in the subframes and any new instances of a particular frame will feature that new value as the default.

Because frames are structurally based, it is possible to generate a semantic network given a set of frames even though it lacks explicit arcs. The reference to Minsky's teacher [[Noam Chomsky]] and his [[generative grammar]] of 1950 is generally missing in Minsky's publications. However, the semantic strength is originated by that concept. 

The simplified structures of frames allow for easy analogical reasoning, a much prized feature in any intelligent agent. The procedural attachments provided by frames also allow a degree of flexibility that makes for a more realistic representation and gives a natural affordance for programming applications.

== Example ==

Worth noticing here is the easy analogical reasoning (comparison) that can be done between a boy and a monkey just by having similarly named slots.

Also notice that Alex, an instance of a boy, inherits default values like "Sex" from the more general parent object Boy,
but the boy may also have different instance values in the form of exceptions such as the number of legs.

{| class="wikitable"
|-
! Slot !! Value !! Type
|-
| ALEX  || _ || (This Frame)
|-
| NAME || Alex || (key value)
|-
| ISA || Boy || (parent frame)
|-
| SEX || Male || (inheritance value)
|-
| AGE || IF-NEEDED: Subtract(current,BIRTHDATE); || (procedural attachment)
|-
| HOME || 100 Main St. || (instance value)
|-
| BIRTHDATE || 8/4/2000 || (instance value)
|-
| FAVORITE_FOOD || Spaghetti || (instance value)
|-
| CLIMBS || Trees || (instance value)
|-
| BODY_TYPE || Wiry || (instance value)
|-
| NUM_LEGS || 1 || (exception)
|}

{| class="wikitable"
|-
! Slot !! Value !! Type
|-
| BOY  || _ || (This Frame)
|-
| ISA || Person || (parent frame)
|-
| SEX || Male || (instance value)
|-
| AGE || Under 12 yrs. || (procedural attachment - sets constraint)
|-
| HOME || A Place || (frame)
|-
| NUM_LEGS || Default = 2 || (default, inherited from Person frame)
|}

{| class="wikitable"
|-
! Slot !! Value !! Type
|-
| MONKEY  || _ || (This Frame)
|-
| ISA || Primate || (parent frame)
|-
| SEX || OneOf(Male,Female) || (procedural attachment)
|-
| AGE || an integer || (procedural attachment - sets constraint)
|-
| HABITAT || Default = Jungle || (default)
|-
| FAVORITE_FOOD || Default = Bananas || (default)
|-
| CLIMBS || Trees || _ 
|-
| BODY_TYPE || Default = Wiry || (default)
|-
| NUM_LEGS || Default = 2 || (default)
|}

== See also ==
* [[Frame language]]
* [[Frame problem]]

== References ==
Russell, Stuart J.; Norvig, Peter (2010), Artificial Intelligence: A Modern Approach (2nd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-604259-7, http://aima.cs.berkeley.edu/ , chpt. 1

== External links ==
* [http://web.media.mit.edu/~minsky/papers/Frames/frames.html Minsky's "A Framework for Representing Knowledge"]
* [http://aima.cs.berkeley.edu/ Artificial Intelligence: A Modern Approach Website]

[[Category:Knowledge representation]]
[[Category:History of artificial intelligence]]</text>
      <sha1>s06rzlmj9rqnrmrfk27y48rw8cpwnt4</sha1>
    </revision>
  </page>
  <page>
    <title>User profile</title>
    <ns>0</ns>
    <id>35773358</id>
    <revision>
      <id>760668334</id>
      <parentid>684921790</parentid>
      <timestamp>2017-01-18T10:42:49Z</timestamp>
      <contributor>
        <username>Kku</username>
        <id>5846</id>
      </contributor>
      <comment>lx</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1479" xml:space="preserve">{{pp-move-indef|small=yes}}
{{pp-semi-indef|small=yes}}
{{for|Wikipedia's guideline on its own user pages|WP:USERPAGE}}

{{unreferenced|date=September 2014}}
[[File:User Profile Info Model.png|thumb|User Profile Info Model]]

A '''user profile''' is a visual display of [[personal data]] associated with a specific [[User (computing)|user]], or a [[customized]] [[desktop environment]]. A profile refers therefore to the explicit digital representation of a person's [[Online identity|identity]]. A user profile can also be considered as the computer representation of a [[user modeling|user model]].

A profile can be used to store the description of the characteristics of person. This information can be exploited by systems taking into account the persons' characteristics and preferences.

[[Profiling (information science)|Profiling]] is the process that refers to construction of a profile via the extraction from a set of data.

User profiles can be found on [[operating system]]s, [[computer program]]s, [[recommender system]]s, or [[Website|dynamic websites]] (such as [[Social network service|online social networking]] sites or [[bulletin board]]s).

==See also==
*[[Online identity]]
*[[Online identity management]]
*[[Personally identifiable information]]
*[[Web mining]]
*[[Internet privacy]]

{{Online social networking}}
{{Social networking}}

[[Category:Identity management]]
[[Category:Knowledge representation]]
[[Category:Software features]]


{{compu-stub}}</text>
      <sha1>9nzmlzbtrl2tuapy9i8l27d62343qku</sha1>
    </revision>
  </page>
  <page>
    <title>Logic Programming Associates</title>
    <ns>0</ns>
    <id>1899829</id>
    <revision>
      <id>761388029</id>
      <parentid>736093565</parentid>
      <timestamp>2017-01-22T18:42:33Z</timestamp>
      <contributor>
        <username>Ira Leviton</username>
        <id>25046916</id>
      </contributor>
      <minor />
      <comment>Fixed a spelling error</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4889" xml:space="preserve">{{multiple issues|
{{notability|Companies|date=January 2012}}
{{more footnotes|date=April 2013}}
}}

{{Infobox company
|name   = Logic Programming Associates Ltd
|type   = [[Private company|Private]]
|foundation = 1981
|directors= Diane Reeve &lt;br /&gt;Clive Spenser &lt;br /&gt;Brian Steel 
|location = London SW18 3SX
|area_served = UK, United States, [[Europe, the Middle East and Africa|EMEA]]
|industry             = [[Computer software]] 
|products = [[VisiRule]], [[Flex expert system|Flex expert system toolkit]], [[Flint toolkit]], LPA Prolog for Windows
|website = [http://www.lpa.co.uk www.lpa.co.uk]}}

'''Logic Programming Associates''' ('''LPA''') is a company specializing in [[logic programming]] and [[artificial intelligence]] software. LPA was founded in 1980 and is widely known for its range of [[Prolog]] compilers and more recently for [[VisiRule]].

LPA was established to exploit research at Imperial College, London into [[logic programming]] carried out under the supervision of [[Robert Kowalski|Prof Robert Kowalski]]. One of the first implementations made available by LPA was micro-PROLOG&lt;ref name = "Prolog implementations"&gt;{{citation |url=http://www.berghel.com/publications/micropro/micropro_ncc87.pdf | title= Microcomputer PROLOG implementations | accessdate=2013-04-29}}&lt;/ref&gt; which ran on popular 8-bit home computers such as the [[Sinclair Spectrum]]&lt;ref name = "micro-PROLOG for Sinclair Spectrum"&gt;{{citation |url=http://www.worldofspectrum.org/infoseekid.cgi?id=0008429 | title= micro-PROLOG for Sinclair Spectrum | accessdate=2013-04-29}}&lt;/ref&gt; and [[Apple II]]. This was followed by micro-PROLOG Professional one of the first Prolog implementations for MS-DOS.

As well as continue with Prolog compiler technology development, LPA has a track record of creating innovative associated tools and products to address specific challenges and opportunities.

In 1989, LPA developed the [[Flex expert system|Flex expert system toolkit]], which incorporated [[Frame language|frame-based]] reasoning with inheritance, [[rule-based programming]] and data-driven procedures. Flex has its own English-like Knowledge Specification Language (KSL) which means that knowledge and rules are defined in an easy-to-read and understand way.

In 1992, LPA helped set up the Prolog Vendors Group,&lt;ref name = "PVG launched"&gt;{{citation |url=http://iospress.metapress.com/content/c1p1351212770518/fulltext.pdf | title=Prolog Vendors Group Launched | accessdate=2013-04-29}}&lt;/ref&gt; a not-for-profit organization whose aim was to help promote Prolog by making people aware of its usage in industry.

In 2000, LPA helped set up [[Business Integrity]], now a leading supplier of document assembly and contract creation software solutions for the legal market.

LPA's core product is LPA Prolog for Windows,&lt;ref name = "WIN-PROLOG"&gt;{{citation |url=http://www.lpa.co.uk/win.htm | title= LPA Prolog for Windows | accessdate=2013-04-29}}&lt;/ref&gt; a compiler and development system for the Microsoft Windows platform. The current LPA software range comprises an integrated AI toolset which covers various aspects of [[Artificial Intelligence]] including Logic Programming, [[Expert Systems]], [[Knowledge-based Systems]], Data Mining, Agents and [[Case-based reasoning]] etc.

In 2004, LPA launched [[VisiRule]] &lt;ref name = "VisiRule"&gt;{{citation |url=http://www.lpa.co.uk/vsr.htm | title= LPA VisiRule | accessdate=2013-04-29}}&lt;/ref&gt; a graphical tool for developing knowledge-based and decision support systems. VisiRule has been used in various sectors, to build [[legal expert systems]], machine diagnostic programs, medical and financial advice systems, etc.

==Customers==
For many years, LPA has worked closely with [[Valdis Krebs]], an American-Latvian researcher, author, and consultant in the field of social and organizational network analysis. Valdis is the founder and chief scientist of Orgnet, and the creator of the popular Inflow &lt;ref name = "InFlow"&gt;[http://www.orgnet.com/inflow3.html InFlow]&lt;/ref&gt; software package.

==External links==
*[http://www.lpa.co.uk/ind_pro.htm LPA home page]
*[http://www.lpa.co.uk/abo_lpa.htm About LPA]
*[[:es:Micro-PROLOG|Micro-PROLOG (in Spanish)]]
*[http://www.teamethno-online.org.uk/Issue2/Rouchy.pdf Aspects of PROLOG History]
*[http://www.lpa.co.uk/vrs_dem.htm VisiRule demos]
*[http://dssresources.com/news/83.php VisiRule: a new graphical business rules tool from LPA]
*[http://dl.acm.org/citation.cfm?id=297981 A flex-based expert system for sewage treatment works support]
*[http://www.lamsade.dauphine.fr/~tsoukias/papers/esse.pdf ESSE: An Expert System for Software Evaluation]

== References ==
{{Reflist}}

[[Category:Information technology organisations]]
[[Category:Software companies of the United Kingdom]]
[[Category:Expert systems]]
[[Category:Knowledge engineering]]
[[Category:Knowledge representation]]


{{compu-ai-stub}}</text>
      <sha1>9rfel3urx7rc9ijnc7p3ucx591ttpfs</sha1>
    </revision>
  </page>
  <page>
    <title>Issue-Based Information System</title>
    <ns>0</ns>
    <id>17477796</id>
    <revision>
      <id>749431662</id>
      <parentid>727398955</parentid>
      <timestamp>2016-11-14T08:20:57Z</timestamp>
      <contributor>
        <username>Pintoch</username>
        <id>16990030</id>
      </contributor>
      <minor />
      <comment>/* External links */change |id={{citeseerx}} to |citeseerx= using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8029" xml:space="preserve">'''Issue-Based Information System''' (IBIS)  was invented by Werner Kunz and  [[Horst Rittel]] as an argumentation-based approach to tackling [[wicked problem]]s &amp;ndash; complex, ill-defined problems that involve multiple [[stakeholder (corporate)|stakeholders]].&lt;ref&gt;Werner, Kunz and Rittel, Horst, Issues as Elements of Information Systems, Working paper No. 131, Studiengruppe für Systemforschung, Heidelberg, Germany, July 1970 (Reprinted May 1979)&lt;/ref&gt; 

To quote from their original paper, ''"Issue-Based Information Systems (IBIS) are meant to support coordination and [[planning]] of [[political]] decision processes. IBIS guides the identification, structuring, and settling of issues raised by problem-solving groups, and provides information pertinent to the [[discourse]]..."''. 

Subsequently, the understanding of [[planning]] and [[design]] as a process of [[argumentation]] (of the designer with himself or with others) has led to the use of IBIS as a [[design rationale]].&lt;ref&gt;Noble, Douglas and Rittel, Horst W.J.  1988, Issue-Based Information Systems for Design, Proceedings of the ACADIA `88 Conference, Association for Computer Aided Design in Architecture, University of Michigan, October 1988. Also published as Working Paper #492, Institute of Urban and Regional Development, College of Environmental Design, University of California, Berkeley. November 1988.&lt;/ref&gt;

The basic structure of IBIS is a [[Graph (discrete mathematics)|graph]]. It is therefore quite suitable to be manipulated by [[computer]].

==Overview==
The elements of IBIS are issues (or questions that need to be answered), each of which are associated with alternative positions (or possible answers).  These in turn are associated with arguments which support or object to a given position (or another argument).  In the course of the treatment of issues, new issues come up which are treated likewise.

Issue-Based Information Systems are used as a means of widening the coverage of a problem.  By encouraging a greater degree of participation, particularly in the earlier phases of the process, the designer is increasing the opportunity that difficulties of his proposed solution, unseen by him, will be discovered by others.  Since the problem observed by a designer can always be treated as merely a symptom of another higher-level problem, the argumentative approach also increases the likelihood that someone will attempt to attack the problem from this point of view.  Another desirable characteristic of the Issue-Based Information System is that it helps to make the design process “transparent.”  Transparency here refers to the ability of observers as well as participants to trace back the process of decision-making. 

IBIS is used in issue mapping,&lt;ref&gt;Okada, A., Shum, S.J.B. and Sherborne, T. (Eds.),  "Knowledge Cartography: software tools and mapping techniques,"  Springer;  2008, ISBN 978-1-84800-148-0&lt;/ref&gt;  an argument visualization technique related to [[argument mapping]]. It is also the basis of a facilitation technique called dialogue mapping.&lt;ref&gt;Conklin, J., "Dialog Mapping: Reflections on an Industrial Strength Case Study", in Visualizing Argumentation – Tools for Collaborative and Educational Sense-Making, P. Kirschner, S.J.B Shum,C.S. Carr (Eds), Springer-Verlag, London (2003)&lt;/ref&gt;

==History==
Rittel’s interest lay in the area of public policy and planning, which is also the context in which he defined [[wicked problem]]s.&lt;ref&gt;Rittel, Horst, and Melvin Webber; "Dilemmas in a General Theory of Planning," pp. 155-169, Policy Sciences, Vol. 4, Elsevier Scientific Publishing Company, Inc., Amsterdam, 1973. Reprinted in N. Cross (ed.), Developments in Design Methodology, J. Wiley &amp; Sons, Chichester, 1984, pp. 135-144&lt;/ref&gt; So it is no surprise that Rittel and Kunz envisaged IBIS as the: 

''"...type of information system meant to support the work of cooperatives like governmental or administrative agencies or committees, planning groups, etc., that are confronted with a problem complex in order to arrive at a plan for decision..."''.&lt;ref&gt;Werner, Kunz and Rittel, Horst, Issues as Elements of Information Systems, Working paper No. 131, Studiengruppe für Systemforschung, Heidelberg, Germany, July 1970 (Reprinted May 1979)&lt;/ref&gt; 

When the paper was written, there were three manual, paper-based IBIS-type systems in use—two in government agencies and one in a university. 

A renewed interest in IBIS-type systems came about in the following decade, when advances in technology made it possible to design relatively inexpensive, computer-based IBIS-type systems. Jeff Conklin and co-workers adapted the IBIS structure for use in software engineering, creating the [[gIBIS]] (graphical IBIS) hypertext system in the late 1980s.&lt;ref&gt;Conklin, J. and Begeman, M.L., gIBIS: A hypertext tool for team design deliberation, Proceedings of the ACM conference on Hypertext, 1987&lt;/ref&gt;   Several other graphical IBIS-type systems were developed once it was realised that such systems facilitated collaborative design and problem solving.&lt;ref&gt;Shum, S.J.B.,Selvin, Albert, M.,  Sierhuis, M., Conklin, J., Haley, C. B. and Nuseibeh, B.,  Hypermedia support for argumentation-based rationale: 15 years on from gIBIS and QOC, Rationale Management in Software Engineering, Springer, 2006&lt;/ref&gt; These efforts culminated in the creation of the open source [[Compendium (software)]]  tool which supports—among other things—a graphical IBIS notation. Similar tools which do not rely on a database for storage include DRed &lt;ref&gt;http://www3.imperial.ac.uk/designengineering/tools/dred&lt;/ref&gt; and designVUE.&lt;ref&gt;http://www3.imperial.ac.uk/portal/page/portallive/designengineering/tools/designvue&lt;/ref&gt;

In recent years, there has been a renewed interest in IBIS-type systems, particularly in the context of [[sensemaking]] and collaborative problem solving in a variety of social and technical contexts. Of particular note is facilitation method called dialogue mapping which uses the IBIS notation to map out a design (or any other) dialogue as it evolves.&lt;ref&gt;Conklin, Jeff; "Dialogue Mapping: Building Shared Understanding of Wicked Problems," Wiley; 1st edition, 18 November 2005, ISBN 978-0-470-01768-5&lt;/ref&gt;

Lately, online versions of dialogue- and issue-mapping tools have appeared, for example, Glyma and bCisive (see the links below).

==See also==
{{colbegin||30em}}
*[[Argument mapping]]
*[[Collaborative software]]
*[[Compendium (software)]]
*[[Computational sociology]]
*[[Creative problem solving]]
*[[Critical thinking]]
*[[Decision making]]
*[[Design]]
*[[Design rationale]]
*[[Graph database]]
*[[Knowledge base]]
*[[Planning]]
*[[Problem solving]]
*[[Wicked problem]]
{{colend}}{{clear right}}

==References==
&lt;references /&gt;

== External links ==
* {{cite web | title = Issues as elements of information systems | citeseerx = 10.1.1.134.1741 }}
* [http://cognexus.org/issue_mapping.htm Cognexus Institute – Issue Mapping]
* [http://www.cleverworkarounds.com/2009/03/04/the-one-best-practice-to-rule-them-all-part-4/ Cleverworkarounds – the one best practice to rule them all – part 4]
* [http://eight2late.wordpress.com/2009/07/08/the-what-and-whence-of-issue-based-information-systems/ Eight to Late – The what and whence of issue-based information systems]
* [http://eight2late.wordpress.com/2009/06/25/visualising-arguments-using-issue-maps-an-example-and-some-general-comments/ Eight to Late – Visualising arguments using issue maps – an example and some general comments]
* [http://eight2late.wordpress.com/2009/04/07/issues-ideas-and-arguments-a-communication-centric-approach-to-tackling-project-complexity/ Eight to Late – Issues, Ideas and Arguments – a communication-centric approach to tackling project complexity]
* [http://bcisiveonline.com bCisive Online]
* [http://glyma.co/gettingstarted Glyma]

[[Category:Argument mapping]]
[[Category:Information systems]]
[[Category:Knowledge representation]]
[[Category:Problem structuring methods]]</text>
      <sha1>lv9f84ly0v548vi2dm2okfm23zohnrp</sha1>
    </revision>
  </page>
  <page>
    <title>Faceted metadata</title>
    <ns>0</ns>
    <id>47571988</id>
    <redirect title="Faceted classification" />
    <revision>
      <id>676887756</id>
      <parentid>676887715</parentid>
      <timestamp>2015-08-19T19:51:32Z</timestamp>
      <contributor>
        <username>Andy Dingley</username>
        <id>3606755</id>
      </contributor>
      <comment>added [[Category:Knowledge representation]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="131" xml:space="preserve">#REDIRECT [[Faceted classification]]

{{R with possibilities}}

[[Category:Metadata|Faceted]]
[[Category:Knowledge representation]]</text>
      <sha1>rwa5w75ouq5qeewb4u4340a02wcmnim</sha1>
    </revision>
  </page>
  <page>
    <title>Mind map</title>
    <ns>0</ns>
    <id>19688</id>
    <revision>
      <id>762676897</id>
      <parentid>762674991</parentid>
      <timestamp>2017-01-30T03:35:09Z</timestamp>
      <contributor>
        <username>Roxy the dog</username>
        <id>6200168</id>
      </contributor>
      <comment>Reverted to revision 759635400 by [[Special:Contributions/CAPTAIN RAJU|CAPTAIN RAJU]] ([[User talk:CAPTAIN RAJU|talk]]). ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="16116" xml:space="preserve">{{about|the visual diagram|the geographical concept|Mental mapping}}
[[File:Tennis-mindmap.png|thumb|upright=1.8|A mind map about the sport of [[tennis]]]]

A '''mind map''' is a  [[diagram]] used to visually organize information. A mind map is hierarchical and shows relationships among pieces of the whole.&lt;ref&gt;Carolyn H. Hopper, Practicing College Learning Strategies, 7th Edition, ISBN 9781305109599, Ch. 7&lt;/ref&gt; It is often created around a single concept, drawn as an image in the center of a blank page, to which associated representations of ideas such as images, words and parts of words are added. Major ideas are connected directly to the central concept, and other ideas branch out from those.

Mind maps can be drawn by hand, either as "rough notes" during a lecture, meeting or planning session, for example, or as higher quality pictures when more time is available. Mind maps are considered to be a type of [[spider diagram]].&lt;ref&gt;{{cite web|url=http://dictionary.cambridge.org/dictionary/british/mind-map?q=mind+map |title=Mind Map noun - definition in the British English Dictionary &amp; Thesaurus - Cambridge Dictionaries Online |publisher=Dictionary.cambridge.org |accessdate=2013-07-10}}&lt;/ref&gt; A similar concept in the 1970s was "idea [[sunburst chart|sun bursting]]".&lt;ref&gt;{{cite web|url=http://www.mind-mapping.org/mindmapping-learning-study-memory/who-invented-mind-mapping.html |title=Who invented mind mapping |publisher=Mind-mapping.org |accessdate=2013-07-10}}&lt;/ref&gt;

== Origins ==

Although the term "mind map" was first popularized by British [[popular psychology]] author and television personality [[Tony Buzan]], the use of diagrams that visually "map" information using branching and [[Radial tree|radial maps]] traces back centuries. These pictorial methods record knowledge and model systems, and have a long history in learning, [[brainstorming]], [[memory]], [[visual thinking]], and [[problem solving]] by educators, engineers, psychologists, and others. Some of the earliest examples of such graphical records were developed by [[Porphyry of Tyros]], a noted thinker of the 3rd century, as he graphically visualized the concept [[Categories (Aristotle)|categories of Aristotle]]. Philosopher [[Ramon Llull]] (1235–1315) also used such techniques.

The [[semantic network]] was developed in the late 1950s as a theory to understand human learning and developed further by [[Allan M. Collins]] and [[M. Ross Quillian]] during the early 1960s. Mind maps are similar in radial structure to [[concept map]]s, developed by learning experts in the 1970s, but differ in that the former are simplified by focusing around a single central key concept.

== Popularisation of the term "mind map" ==

Buzan's specific approach, and the introduction of the term "mind map" arose during a 1974 BBC TV series he hosted, called ''Use Your Head''.&lt;ref&gt;{{cite web|url=http://www.mind-mapping.org/blog/mapping-history/roots-of-visual-mapping/ |title=Roots of visual mapping - The mind-mapping.org Blog |publisher=Mind-mapping.org |date=2004-05-23 |accessdate=2013-07-10}}&lt;/ref&gt;&lt;ref&gt;Buzan, Tony 1974. Use your head. London: BBC Books.&lt;/ref&gt; In this show, and companion book series, Buzan promoted his conception of radial tree, diagramming key words in a colorful, radiant, tree-like structure.&lt;ref&gt;[http://www.knowledgeboard.com/item/2980 Buzan claims mind mapping his invention in interview.] ''KnowledgeBoard'' retrieved Jan. 2010.&lt;/ref&gt;

Buzan says the idea was inspired by [[Alfred Korzybski]]'s [[general semantics]] as popularized in science fiction novels, such as those of [[Robert A. Heinlein]] and [[A. E. van Vogt]]. He argues that while "traditional" outlines force readers to scan left to right and top to bottom, readers actually tend to scan the entire page in a non-linear fashion. Buzan's treatment also uses then-popular assumptions about the functions of [[cerebral hemispheres]] in order to explain the claimed increased effectiveness of mind mapping over other forms of note making.

==Mind map guidelines==
Buzan suggests the following guidelines for creating mind maps:

# Start in the center with an image of the topic, using at least 3 colors.
# Use images, symbols, codes, and dimensions throughout your mind map.
# Select key words and print using upper or lower case letters.
# Each word/image is best alone and sitting on its own line.
# The lines should be connected, starting from the central image. The lines become thinner as they radiate out from the center.
# Make the lines the same length as the word/image they support.
# Use multiple colors throughout the mind map, for visual stimulation and also for encoding or grouping.
# Develop your own personal style of mind mapping.
# Use emphasis and show associations in your mind map.
# Keep the mind map clear by using radial hierarchy or outlines to embrace your branches.

== Uses ==

[[File:Mindmap.gif|thumb|Rough mindmap notes taken during a course session]]
As with other diagramming tools, mind maps can be used to [[generation|generate]], [[creative visualization|visualize]], [[structure]], and [[taxonomic classification|classify]] ideas, and as an aid to [[study skills|studying]]&lt;ref&gt;'Mind maps as active learning tools', by Willis, CL. Journal of computing sciences in colleges. {{ISSN|1937-4771}}. 2006. Volume:
21 Issue: 4&lt;/ref&gt; and [[organization|organizing]] information, [[problem solving|solving problems]], [[decision making|making decisions]], and writing.

Mind maps have many applications in personal, family, [[education]]al, and [[business]] situations, including [[notetaking]], brainstorming (wherein ideas are inserted into the map radially around the center node, without the implicit prioritization that comes from hierarchy or sequential arrangements, and wherein grouping and organizing is reserved for later stages), summarizing, as a [[mnemonic technique]], or to sort out a complicated idea. Mind maps are also promoted as a way to collaborate in color pen creativity sessions.

In addition to these direct use cases, data retrieved from mind maps can be used to enhance several other applications; for instance [[expert system|expert search systems]], [[search engine]]s and search and tag query recommender.&lt;ref name=Beel2009&gt;{{Cite journal| first=Jöran | last=Beel | first2=Bela| last2=Gipp | first3=Jan-Olaf |last3= Stiller | contribution=Information Retrieval On Mind Maps - What Could It Be Good For? | contribution-url=http://www.sciplore.org/publications_en.php | title=Proceedings of the 5th International Conference on Collaborative Computing: Networking, Applications and Worksharing (CollaborateCom'09) | year=2009 | publisher=IEEE | place=Washington | postscript=. --&gt;}}&lt;/ref&gt; To do so, mind maps can be analysed with classic methods of [[information retrieval]] to classify a mind map's author or documents that are linked from within the mind map.&lt;ref name=Beel2009 /&gt;

==Differences from other visualizations==

* '''Concept maps''' - Mind maps differ from [[concept maps]] in that mind maps focus on ''only'' one word or idea, whereas concept maps connect multiple words or ideas. Also, concept maps typically have text labels on their connecting lines/arms. Mind maps are based on radial hierarchies and [[tree structure]]s denoting relationships with a central governing concept, whereas concept maps are based on connections between concepts in more diverse patterns.  However, either can be part of a larger [[personal knowledge base]] system.
* '''Modelling graphs''' - There is no rigorous right or wrong with mind maps, relying on the arbitrariness of [[mnemonic]] systems. A [[UML diagram]] or a [[semantic network]] has structured elements modelling relationships, with lines connecting objects to indicate relationship. This is generally done in black and white with a clear and agreed iconography. Mind maps serve a different purpose: they help with memory and organization. Mind maps are collections of words structured by the mental context of the author with visual mnemonics, and, through the use of colour, icons and visual links, are informal and necessary to the proper functioning of the mind map.

==Research==

'''Effectiveness''' - Cunningham (2005) conducted a user study in which 80% of the students thought "mindmapping helped them understand concepts and ideas in science".&lt;ref name="Cunningham05"&gt;{{cite thesis| type=Ph.D.| author={G}lennis {E}dge {C}unningham| title=Mindmapping: Its Effects on Student Achievement in High School Biology| year=2005| publisher=The University of Texas at Austin| accessdate=1 November 2013}}&lt;/ref&gt; Other studies also report positive effects through the use of mind maps.&lt;ref name="Holland2004"&gt;{{cite journal| author={B}rian {H}olland, {L}ynda {H}olland, {J}enny {D}avies| title=An investigation into the concept of mind mapping and the use of mind mapping software to support and improve student academic performance| year=2004| accessdate=1 November 2013}}&lt;/ref&gt;&lt;ref name="Antoni2006"&gt;{{cite journal| author=D'Antoni, A.V., Zipp, G.P.| title=Applications of the Mind Map Learning Technique in Chiropractic Education: A Pilot Study and Literature| year=2006| accessdate=1 November 2013}}&lt;/ref&gt; Farrand, Hussain, and Hennessy (2002) found that [[spider diagram]]s (similar to concept maps) had limited, but significant, impact on memory recall in undergraduate students (a 10% increase over baseline for a 600-word text only) as compared to preferred study methods (a 6% increase over baseline).&lt;ref name= Farrand2002&gt;{{cite journal |author=Farrand, P. |author2=Hussain, F. |author3=Hennessy, E.  |year=2002 |title=The efficacy of the mind map study technique |journal=Medical Education |volume=36 |issue=5 |pages=426–431 |url=http://www3.interscience.wiley.com/journal/118952400/abstract |accessdate=2009-02-16 |doi=10.1046/j.1365-2923.2002.01205.x |pmid=12028392}}&lt;/ref&gt; This improvement was only robust after a week for those in the diagram group and there was a significant decrease in motivation compared to the subjects' preferred methods of note taking. A meta study about [[concept map]]ping concluded that concept mapping is more effective than "reading text passages, attending lectures, and participating in class discussions".&lt;ref name="Nesbit06"&gt;{{cite journal| author={N}esbit, {J}.{C}., {A}desope, {O}.{O}.| title=Learning with concept and knowledge maps: A meta-analysis| journal=Review of Educational Research| year=2006| volume=76| number=3| pages=413| publisher=Sage Publications| doi=10.3102/00346543076003413}}&lt;/ref&gt; The same study also concluded that concept mapping is slightly more effective "than other constructive activities such as writing summaries and outlines". In addition, they concluded that low-ability students may benefit more from mind mapping than high-ability students.

'''Features of Mind Maps''' - Beel &amp; Langer (2011) conducted a comprehensive analysis of the content of mind maps.&lt;ref name="Beel2011d"&gt;{{cite book| author={J}oeran {B}eel, {S}tefan {L}anger| chapter=An Exploratory Analysis of Mind Maps| title=Proceedings of the 11th ACM Symposium on Document Engineering (DocEng'11)| year=2011| publisher=ACM| url=http://docear.org/papers/An%20Exploratory%20Analysis%20of%20Mind%20Maps%20--%20preprint.pdf | accessdate=1 November 2013}}&lt;/ref&gt; They analysed 19,379 mind maps from 11,179 users of the mind mapping applications [[SciPlore MindMapping]] (now [[Docear]]) and [[MindMeister]]. Results include that average users create only a few mind maps (mean=2.7), average mind maps are rather small (31 nodes) with each node containing about 3 words (median). However, there were exceptions. One user created more than 200 mind maps, the largest mind map consisted of more than 50,000 nodes and the largest node contained ~7500 words. The study also showed that between different mind mapping applications ([[Docear]] vs [[MindMeister]]) significant differences exist related to how users create mind maps.

'''Automatic Creating of Mind Maps''' - There have been some attempts to create mind maps automatically. Brucks &amp; Schommer created mind maps automatically from full-text streams.&lt;ref name="Brucks2008"&gt;{{cite journal| author={C}laudine {B}rucks, {C}hristoph {S}chommer| title=Assembling Actor-based Mind-Maps from Text Stream| journal=CoRR| year=2008| volume=abs/0810.4616| accessdate=1 November 2013}}&lt;/ref&gt; Rothenberger et al. extracted the main story of a text and presented it as mind map.&lt;ref name="Rothenberger2008"&gt;{{cite journal| author=Rothenberger, T, Oez, S, Tahirovic, E, Schommer, Christoph| title=Figuring out Actors in Text Streams: Using Collocations to establish Incremental Mind-maps| journal= | arxiv=0803.2856| year=2008}}&lt;/ref&gt; And there is a patent about automatically creating sub-topics in mind maps.&lt;ref name="Plotkin09"&gt;{{cite journal|year=2009|title=Software tool for creating outlines and mind maps that generates subtopics automatically|journal=USPTO Application: 20090119584|volume=|author={R}obert {P}lotkin|accessdate=1 November 2013}}&lt;/ref&gt;

'''Pen and Paper vs Computer''' - There are two studies that analyze whether electronic mind mapping or pen based mind mapping is more effective.&lt;ref name="Mahler09"&gt;{{cite book| author={M}ahler, {T}., {W}eber, {M}.| chapter=Dimian-Direct Manipulation and Interaction in Pen Based Mind Mapping| title=Proceedings of the 17th World Congress on Ergonomics, IEA 2009| year=2009| accessdate=1 November 2013}}&lt;/ref&gt;&lt;ref name="Shih09"&gt;{{cite journal| author={S}hih, {P}.{C}., {N}guyen, {D}.{H}., {H}irano, {S}.{H}. and {R}edmiles, {D}.{F}., {H}ayes, {G}.{R}.| title=Groupmind: supporting idea generation through a collaborative mind-mapping tool| year=2009| pages=139–148| accessdate=1 November 2013}}&lt;/ref&gt;

==Tools==
[[List of concept- and mind-mapping software|Mind-mapping software]] can be used to organize large amounts of information, combining spatial organization, dynamic hierarchical structuring and node folding. Software packages can extend the concept of mind-mapping by allowing individuals to map more than thoughts and ideas with information on their computers and the Internet, like spreadsheets, documents, Internet sites and images.&lt;ref&gt;{{cite web|url=http://www.imdevin.com/top-10-totally-free-mind-mapping-software-tools/|title=Top 10 Totally Free Mind Mapping Software Tools|last=Santos|first=Devin|date=15 February 2013|publisher=IMDevin|accessdate=10 July 2013}}&lt;/ref&gt; It has been suggested that mind-mapping can improve learning/study efficiency up to 15% over conventional [[note-taking]].&lt;ref&gt;
{{cite journal
  | last = Farrand
  | first = Paul |author2=Hussain, Fearzana |author3=Hennessy, Enid
  | title = The efficacy of the 'mind map' study technique
  | journal = Medical Education
  | volume = 36
  | issue = 5
  | pages = 426–431
  | date = May 2002
  | doi = 10.1046/j.1365-2923.2002.01205.x
  | pmid = 12028392
}}&lt;/ref&gt;

==See also==
{{Portal|Education}}
* [[Brainstorming]]
* [[Graph (discrete mathematics)]]
* [[Idea]]
* [[List of concept mapping and mind mapping software]]
* [[Mental literacy]]
* [[Personal wiki]]

; Related diagrams
* [[Argument map]]
* [[Cognitive map]]
* [[Concept map]]
* [[Nodal organizational structure]]
* [[Radial tree]]
* [[Rhizome (philosophy)]]
* [[Semantic network]]
* [[Social map]]
* [[Spider mapping]]
* [[Tree structure]]

==References==
{{reflist|30em}}

==Further reading==
* {{cite journal |last= Novak |first= J.D. |date= 1993 |title= How do we learn our lesson?: Taking students through the process |journal= [[The Science Teacher]] |volume= 60 |number= 3 |pages= 50–55 |issn= 0036-8555 }}

==External links==
*{{Commons category-inline|Mind maps}}

{{Mindmaps}}

{{Authority control}}

{{DEFAULTSORT:Mind Map}}
[[Category:Articles with inconsistent citation formats]]
[[Category:Knowledge representation]]
[[Category:Games of mental skill]]
[[Category:Creativity]]
[[Category:Design]]
[[Category:Educational technology]]
[[Category:Diagrams]]
[[Category:Thought]]
[[Category:Note-taking]]
[[Category:Reading (process)]]
[[Category:Zoomable user interfaces]]
[[Category:Educational devices]]</text>
      <sha1>dvtizr68kj2j8s8ygnq0ih5v6uat3uh</sha1>
    </revision>
  </page>
  <page>
    <title>Parallel Tree Contraction</title>
    <ns>0</ns>
    <id>48789236</id>
    <revision>
      <id>751669211</id>
      <parentid>733407508</parentid>
      <timestamp>2016-11-27T04:39:15Z</timestamp>
      <contributor>
        <username>Unready</username>
        <id>8433548</id>
      </contributor>
      <minor />
      <comment>&lt;pre&gt; for no lang</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="13629" xml:space="preserve">{{Orphan|date=December 2015}}

In [[computer science]], '''parallel tree contraction''' is a broadly applicable technique for the parallel solution of a large number of [[tree]] problems, and is used as an algorithm design technique for the design of a large number of parallel [[graph (discrete mathematics)|graph]] algorithms.  Parallel tree contraction was introduced by [[Gary L. Miller]] and [[John H. Reif]],&lt;ref name="Miller89book"&gt;[[Gary L. Miller]] and [[John H. Reif]], Parallel Tree Contraction--Part I: Fundamentals., 1989&lt;/ref&gt;  and has subsequently been modified to improve efficiency by X. He and Y. Yesha,&lt;ref&gt;X. He and Y. Yesha, "Binary tree algebraic computation and parallel algorithms for simple graphs.", Journal of Algorithms, 1988, pp 92-113&lt;/ref&gt; Hillel Gazit, Gary L. Miller and Shang-Hua Teng&lt;ref&gt;Hillel Gazit, Gary L. Miller and Shang-Hua Teng, Optimal tree contraction in the EREW model, Springer, 1988&lt;/ref&gt; and many others.&lt;ref&gt;Karl Abrahamson and et al., "A simple parallel tree contraction algorithm.", Journal of Algorithms, 1989, pp 287-302&lt;/ref&gt;

Tree contraction has been used in designing many efficient [[parallel algorithms]], including [[Expression (mathematics)|expression]] evaluation, finding [[lowest common ancestors]], tree isomorphism, [[graph isomorphism]], [[maximal subtree isomorphism]], [[common subexpression elimination]], computing the 3-connected components of a graph, and finding an explicit planar embedding of a [[planar graph]]&lt;ref name="Reif94dynamic"&gt;John H. Reif and Stephen R. Tate, Dynamic parallel tree contraction, Proceedings of the sixth annual ACM symposium on Parallel algorithms and architectures (ACM), 1994&lt;/ref&gt;

Based on the research and work on parallel tree contraction, various algorithms have been proposed targeting to improve the efficiency or simplicity of this topic. This article hereby focuses on a particular solution, which is a variant of the algorithm by Miller and Reif, and its application.

==Introduction==
Over the past several decades there has been significant research on deriving new parallel algorithms for a variety of problems, with the goal of designing highly parallel ([[polylogarithmic depth]]), work-efficient (linear in the sequential running time) algorithms.&lt;ref name="Miller89book" /&gt; For some problems, tree turns out to be a nice solution. Addressing these problems, we can sometimes get more parallelism simply by representing our problem as a tree.

Considering a generic definition of a tree, there is a root vertex, and several child vertices attached to the root.&lt;ref&gt;[[Thomas H. Cormen]], [[Charles E. Leiserson]], [[Ronald L. Rivest]], and [[Clifford Stein]]. ''[[Introduction to Algorithms]]'', Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7 . Section 10.4: Representing rooted trees, pp.&amp;nbsp;214–217. Chapters 12–14 (Binary Search Trees, Red-Black Trees, Augmenting Data Structures), pp.&amp;nbsp;253–320.&lt;/ref&gt; And the child vertices might have children themselves, and so on so forth. Eventually, the paths come down to leaves, which are defined to be the terminal of a tree. Then based on this generic tree, we can further come up with some special cases: (1) [[balanced binary tree]]; (2) [[linked list]].&lt;ref&gt;[[Donald Knuth]]. ''[[The Art of Computer Programming]]: Fundamental Algorithms'', Third Edition. Addison-Wesley, 1997. ISBN 0-201-89683-4 . Section 2.3: Trees, pp.&amp;nbsp;308–423.&lt;/ref&gt; A balanced binary tree has exactly two branches for each vertex except for leaves. This gives a O(log n) bound on the depth of the tree.&lt;ref&gt;{{citation
 | last1 = Nešetřil | first1 = Jaroslav | author1-link = Jaroslav Nešetřil
 | last2 = Ossona de Mendez | first2 = Patrice | author2-link = Patrice Ossona de Mendez
 | contribution = Chapter 6. Bounded height trees and tree-depth
 | doi = 10.1007/978-3-642-27875-4
 | isbn = 978-3-642-27874-7
 | location = Heidelberg
 | mr = 2920058
 | pages = 115–144
 | publisher = Springer
 | series = Algorithms and Combinatorics
 | title = Sparsity: Graphs, Structures, and Algorithms
 | volume = 28
 | year = 2012}}.&lt;/ref&gt; A linked list is also a tree where every vertex has only one child. We can also achieve O(log n) depth using [[symmetry breaking]].&lt;ref&gt;Andrew Goldberg, Serge Plotkin, and Gregory Shannon, Parallel symmetry-breaking in sparse graphs, Proceedings of the nineteenth annual ACM symposium on Theory of computing (ACM), 1987&lt;/ref&gt;

Given the general case of a tree, we would like to keep the bound at O(log n) no matter it is unbalanced or list-like or a mix of both. To address this problem, we make use of an algorithm called [[prefix sum]] by using the [[Euler tour technique]].&lt;ref&gt;[http://courses.csail.mit.edu/6.851/spring07/scribe/lec05.pdf Euler tour trees] - in Lecture Notes in Advanced Data Structures. Prof. Erik Demaine; Scribe: Katherine Lai.&lt;/ref&gt; With the Euler tour technique, a tree could be represented in a flat style, and thus prefix sum could be applied to an arbitrary tree in this format. In fact, prefix sum can be used on any set of values and binary operation which form a group: the binary operation must be associative, every value must have an inverse, and there exists an identity value.

With a bit of thought, we can find some exceptional cases where prefix sum becomes incapable or inefficient. Consider the example of multiplication when the set of values includes 0. Or there are some commonly desired operations are max() and min() which do not have [[inverses]]. The goal is to seek an algorithm which works on all trees, in expected O(n) work and O(log n) depth. In the following sections, a Rake/Compress algorithm will be proposed to fulfill this goal.&lt;ref name="Miller85app"&gt;Gary L. Miller and John H. Reif, Parallel tree contraction and its application, Defense Technical Information Center, 1985&lt;/ref&gt;

==Definitions==

[[File:Rake-1.png|480*360px|thumbnail|right|Fig. 1: Rake Operation]]
[[File:Compress-1.png|480*360px|thumbnail|right|Fig. 2: Compress Operation]]
Before going into the algorithm itself, we first look at a few terminologies that will be used later.

* '''Rake'''&lt;ref name="cmutrees"&gt;[https://www.cs.cmu.edu/afs/cs/academic/class/15499-s09/www/scribe/lec11/lec11.pdf Parallel Algorithms: Tree Operations], Guy Blelloch, Carnegie Mellon University, 2009&lt;/ref&gt; – Rake step joins every left leaf of binary nodes to the parent. By join, we mean that it undergoes a functional process which achieves the operation we want to make. An example of rake is given in Figure 1.
* '''Compress'''&lt;ref name="cmutrees" /&gt; – Compress step is actually a sequence of several events: (1) Find an independent set of unary nodes. (Independence here is defined such that no two are neighbors, meaning no parent to child relation) (2) Join each node in independent set with its child (Note that independent set is not unique). An example of compress is given in Figure 2.

And in order to solve actual problems using tree contraction, the algorithm has a structure:

&lt;pre&gt;
Repeat until tree becomes a unary node
{
    Rake;
    Compress;
}
&lt;/pre&gt;


==Analysis==
For the moment, let us assume that all nodes have less than three children, namely binary. Generally speaking, as long as the degree is bounded, the bounds will hold.&lt;ref&gt;MORIHATA, Akimasa, and Kiminori MATSUZAKI, A Parallel Tree Contraction Algorithm on Non-Binary Trees, MATHEMATICAL ENGINEERING
TECHNICAL REPORTS, 2008&lt;/ref&gt; But we will analyze the binary case for simplicity. In the two “degenerate” cases listed above, the rake is the best tool for dealing with balanced binary trees, and compress is the best for linked lists. However, arbitrary trees will have to require a combination of these operations. By this combination, we claim a theorem that
* '''Theorem''': After O(log n) expected rake and compress steps, a tree is reduced to a single node.
Now rephrase the tree contraction algorithm as follows:
* Input: A binary tree rooted at r
* Output: A single node
* Operation:  A sequence of contraction steps, each consisting of a rake operation and a compress operation (in any order). The rake operation removes all the leaf nodes in parallel. The compress operation finds an [[Independent set (graph theory)|independent set]] of unary nodes and splice out the selected nodes.
To approach the theorem, we first take a look at a property of a binary tree. Given a binary tree T, we can partition the nodes of T into 3 groups: {{tmath|T_0}} contains all leaf nodes, {{tmath|T_1}} contains all nodes with 1 child, and {{tmath|T_2}} contains all nodes with 2 children. It is easy to see that: &lt;math&gt;V(T) = T_0  \cup T_1 \cup T_2&lt;/math&gt;. Now we propose:
* Claim: &lt;math&gt;|T_0| = |T_2|  + 1&lt;/math&gt;
This claim can be proved by strong induction on the number of nodes. It is easy to see that the base case of n=1 trivially holds. And we further assume the claim also holds for any tree with at most n nodes. Then given a tree with n+1 nodes rooted at r, there appears to be two cases:
# If r has only one subtree, consider the subtree of r. We know that the subtree has the same number of binary nodes and the same number of leaf nodes as the whole tree itself. This is true since the root is a unary node. And based the previous assumption, a unary node does not change either {{tmath|T_0}} or {{tmath|T_2}}.
# If r has two subtrees, we define {{tmath|T_0^L, T_2^L}} to be the leaf nodes and binary nodes in the left subtree, respectively. Similarly, we define the same {{tmath|T_0^R, T_2^R}} for the right subtree. From previous, there is &lt;math&gt;|T_0^L| = |T_2^L| + 1&lt;/math&gt; and &lt;math&gt;|T_0^R| = |T_2^R| + 1&lt;/math&gt;. Also we know that T has &lt;math&gt;|T_0^L| + |T_0^R|&lt;/math&gt; leaf nodes and &lt;math&gt;|T_2^L| + |T_2^R| + 1&lt;/math&gt; binary nodes. Thus, we can derive:

:&lt;math&gt;|T_0^L| + |T_0^R| = |T_2^L| + 1 + |T_2^R| + 1 = (|T_2^L| + |T_2^R| + 1) + 1&lt;/math&gt;

which proves the claim.

Following the claim, we then prove a lemma, which leads us to the theorem.
* Lemma: The number of nodes of after a contraction step is reduced by a constant factor in expectation.
Assume the number of nodes before the contraction to be m, and m' after the contraction. By definition, the rake operation deletes all {{tmath|T_0}} and the compress operation deletes at least 1/4 of {{tmath|T_1}} in expectation. All {{tmath|T_2}} remains. Therefore, we can see:

:&lt;math&gt;E[m'] \leq |T_2| + \tfrac{3}{4}*|T_1| \leq \tfrac{3}{4} + \tfrac{3}{4}*|T_1| + \tfrac{3}{2}*|T_2| = \tfrac{3}{4}(1 + |T_1| + 2*|T_2|) = \tfrac{3}{4}(|T_0| + |T_1| + |T_2|) = \tfrac{3}{4}m&lt;/math&gt;

Finally, based on this lemma, we can conclude that if the nodes are reduced by a constant factor in each iteration, after {{tmath|O(\log n)}}, there will be only one node left.&lt;ref&gt;[https://www.cs.cmu.edu/afs/cs/academic/class/15492-f07/www/scribe/lec9/lecture9.pdf Parallel Algorithms: Analyzing Parallel Tree Contraction], Guy Blelloch, 2007&lt;/ref&gt;

==Applications==

===Expression Evaluation===
To evaluate an expression given as a binary tree (this problem also known as [[binary expression tree]]),&lt;ref&gt;S Buss, Algorithms for boolean formula evaluation and for tree contraction, Arithmetic, Proof Theory, and Computational Complexity, 1993, pp. 96-115&lt;/ref&gt; consider that:
An arithmetic expression is a tree where the leaves have values from some domain and each internal vertex has two children and a label from {+, x, %}. And further assume that these binary operations can be performed in constant time.

We now show the evaluation can be done with parallel tree contraction.&lt;ref&gt;Bader, David A., Sukanya Sreshta, and Nina R. Weisse-Bernstein, Evaluating arithmetic expressions using tree contraction: A fast and scalable parallel implementation for symmetric multiprocessors (SMPs), High Performance Computing—HiPC 2002. Springer Berlin Heidelberg, 2002, pp. 63-75.&lt;/ref&gt;
* Step 1. Assign expressions to every node. The expression of a leaf is simply the value that it contains. Write L + R, L − R, or L × R for the operators, where L and R are the values of the expressions in the left and right subtrees, respectively.
* Step 2. When a left (right) child with 0 children is merged into an operator, replace L (R) with the value of the child.
* Step 3. When a node has 1 child, it has an expression that is a function of one variable. When a left (right) child with 1 child is merged into an operator, replace L (R) with the expression and change the variable in the expression to L (R) if appropriate.

In a node with 2 children, the operands in the expression are f(L) and g(R), where f and g are linear functions, and in a node with 1 child, the expression is h(x), where h is a linear function and x is either L or R. We prove this invariant by induction. At the beginning, the invariant is clearly satisfied. There are three types of merges that result in a not fully evaluated expression. (1) A 1-child node is merged into a 2-children node. (2) A leaf is merged into a 2-children node. (3) A 1-child node is merged into a 1-child node. All three types of merges do not change the invariant. Therefore, every merge simply evaluates or composes linear functions, which takes constant time &lt;ref&gt;[http://math.mit.edu/~rpeng/18434/applicationsParallelTreeContraction.pdf Applications of Parallel Tree Contraction], Samuel Yeom, 2015&lt;/ref&gt;

==References==
{{Reflist}}
{{refbegin}}
{{refend}}

==External links==
{{Commons category|Tree structures}}
* [http://courses.csail.mit.edu/6.851/spring07/scribe/lec05.pdf 6.851: Advanced Data Structures] by Prof. Erik Demaine

{{CS-Trees}}

[[Category:Knowledge representation]]

[[de:Baum (Graphentheorie)]]</text>
      <sha1>pm7z4afke6zpabd0n2znolgmshte22e</sha1>
    </revision>
  </page>
  <page>
    <title>Brian Deer Classification System</title>
    <ns>0</ns>
    <id>49726563</id>
    <revision>
      <id>718070242</id>
      <parentid>709686234</parentid>
      <timestamp>2016-05-01T10:30:29Z</timestamp>
      <contributor>
        <username>Themightyquill</username>
        <id>1212157</id>
      </contributor>
      <comment>removed [[Category:Library science]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2521" xml:space="preserve">The '''Brian Deer Classification System''' is a [[library classification]] system created for use in Indigenous contexts by Canadian [[Mohawk people|Kahnawake Mohawk]] librarian A. Brian Deer.&lt;ref name="auto"&gt;{{cite journal|last1=Doyle|first1=Ann M.|last2=Lawson|first2=Kimberley|last3=Dupont|first3=Sarah|title=Indigenization of Knowledge Organization at the Xwi7xwa Library|journal=Journal of Library and Information Studies|date=December 2015|volume=13|issue=2|page=112|doi=10.6182/jlis.2015.13(2).107|url=https://open.library.ubc.ca/cIRcle/collections/ubclibraryandarchives/29962/items/1.0103204|accessdate=11 March 2016}}&lt;/ref&gt;

== History ==
Deer designed his classification system while working in the library of the [[National Indian Brotherhood]] from 1974-1976, with the goal of reflecting indigenous viewpoints and values in knowledge organization. Between 1978 and 1980, the system was adapted for use in [[British Columbia]] by Gene Joseph and Keltie McCall while working at the [[Union of British Columbia Indian Chiefs]].&lt;ref name="auto"/&gt;

Variations of the Brian Deer Classification are in use at the [[University of British Columbia Library|Xwi7xwa Library]] at the [[University of British Columbia]];&lt;ref name="auto"/&gt; the [[Union of British Columbia Indian Chiefs]] Resource Centre;&lt;ref&gt;{{cite journal|last1=Cherry|first1=Alissa|last2=Mukunda|first2=Keshav|title=A Case Study in Indigenous Classification: Revisiting and Reviving the Brian Deer Scheme|journal=Cataloging &amp; Classification Quarterly|date=31 Jul 2015|volume=53|issue=5-6|pages=pages 548–567|doi=10.1080/01639374.2015.1008717|url=http://www.tandfonline.com/doi/abs/10.1080/01639374.2015.1008717?journalCode=wccq20|accessdate=11 March 2016}}&lt;/ref&gt; and the  Aanischaaukamikw Cree Cultural Institute in [[Oujé-Bougoumou, Quebec]].&lt;ref&gt;{{cite journal|last1=Swanson|first1=Raegan|title=Adapting the Brian Deer Classification System for Aanischaaukamikw Cree Cultural Institute|journal=Cataloging &amp; Classification Quarterly|date=31 Jul 2015|volume=53|issue=5-6|pages=568–579|doi=10.1080/01639374.2015.1009669|url=http://www.tandfonline.com/doi/abs/10.1080/01639374.2015.1009669|accessdate=11 March 2016}}&lt;/ref&gt;

== References ==
{{Reflist}}

== External links ==
* [http://xwi7xwa.library.ubc.ca/files/2011/09/deer.pdf Brian Deer Classification System]

&lt;!--- Categories ---&gt;
[[Category:Library cataloging and classification]]
[[Category:Information science]]
[[Category:Knowledge representation]]
[[Category:Aboriginal peoples in Canada]]</text>
      <sha1>1no085plym3xfjr1t18gxgyaci3nt92</sha1>
    </revision>
  </page>
  <page>
    <title>Tree (data structure)</title>
    <ns>0</ns>
    <id>30806</id>
    <revision>
      <id>761693076</id>
      <parentid>761689495</parentid>
      <timestamp>2017-01-24T07:47:48Z</timestamp>
      <contributor>
        <username>Jochen Burghardt</username>
        <id>17350134</id>
      </contributor>
      <comment>Undid revision 761689495 by [[Special:Contributions/45.127.106.179|45.127.106.179]] ([[User talk:45.127.106.179|talk]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="21632" xml:space="preserve">{{hatnote|Not to be confused with [[trie]], a specific type of tree data structure.}}
{{Refimprove|date=August 2010}}
[[File:binary tree.svg|right|192|thumb|A simple unordered tree; in this diagram, the node labeled 7 has two children, labeled 2 and 6, and one parent, labeled 2. The root node, at the top, has no parent.]]
In [[computer science]], a '''tree''' is a widely used [[abstract data type]] (ADT)—or [[data structure]] implementing this ADT—that simulates a hierarchical [[tree structure]], with a root value and [[subtrees]] of children with a parent node, represented as a set of linked [[Vertex (graph theory)|nodes]].

A tree data structure can be defined [[Recursion|recursively]] (locally) as a collection of [[node (computer science)|nodes]] (starting at a root node), where each node is a data structure consisting of a value, together with a list of references to nodes (the "children"), with the constraints that no reference is duplicated, and none points to the root.

Alternatively, a tree can be defined abstractly as a whole (globally) as an [[ordered tree]], with a value assigned to each node. Both these perspectives are useful: while a tree can be analyzed mathematically as a whole, when actually represented as a data structure it is usually represented and worked with separately by node (rather than as a list of nodes and an [[adjacency list]] of edges between nodes, as one may represent a [[#Digraphs|digraph]], for instance). For example, looking at a tree as a whole, one can talk about "the parent node" of a given node, but in general as a data structure a given node only contains the list of its children, but does not contain a reference to its parent (if any).

==Definition==
{| style="float:right"
| [[File:Directed graph, disjoint.svg|thumb|x100px|{{color|#800000|Not a tree}}: two non-[[Connectivity (graph theory)#Definitions of components, cuts and connectivity|connected]] parts, A→B and C→D→E. There is more than one root.]]
|}
{| style="float:right"
| [[File:Directed graph with branching SVG.svg|thumb|x100px|{{color|#800000|Not a tree}}: undirected cycle 1-2-4-3. 4 has more than one parent (inbound edge).]]
|}
{| style="float:right"
| [[File:Directed graph, cyclic.svg|thumb|x100px|{{color|#800000|Not a tree}}: cycle B→C→E→D→B. B has more than one parent (inbound edge).]]
|}
{| style="float:right"
| [[File:Graph single node.svg|thumb|x50px|{{color|#800000|Not a tree}}: cycle A→A. A is the root but it also has a parent.]]
|}
{| style="float:right"
| [[File:Directed Graph Edge.svg|thumb|x50px|Each linear list is trivially {{color|#008000|a tree}}]]
|}

A tree is a (possibly non-linear) data structure made up of nodes or vertices and edges without having any cycle. The tree with no nodes is called the '''null''' or '''empty''' tree. A tree that is not empty consists of a root node and potentially many levels of additional nodes that form a hierarchy.

==Terminology used in Trees==
{{term|Root}} {{defn|The top node in a tree.}}
{{term|Child}} {{defn|A node directly connected to another node when moving away from the Root.}}
{{term|Parent}} {{defn|The converse notion of a ''child''.}}
{{term|Siblings}} {{defn| A group of nodes with the same parent.}}
{{term|Descendant}} {{defn|A node reachable by repeated proceeding from parent to child.}}
{{term|Ancestor}} {{defn|A node reachable by repeated proceeding from child to parent.}}
{{term|Leaf}} {{term|(less commonly called External node)|multi=y}} {{defn|A node with no children.}}
{{term|Branch}} {{term|Internal node|multi=y}} {{defn|A node with at least one child.}}
{{term|Degree}} {{defn|The number of sub trees of a node.}}
{{term|Edge}} {{defn|The connection between one node and another.}}
{{term|Path}} {{defn|A sequence of nodes and edges connecting a node with a descendant.}}
{{term|Level}} {{defn|The level of a node is defined by 1 + (the number of connections between the node and the root).}}
{{term|Height of node}} {{defn|The height of a node is the number of edges on the longest path between that node and a leaf.}}
{{term|Height of tree}} {{defn|The height of a tree is the height of its root node.}}
{{term|Depth}} {{defn|The depth of a node is the number of edges from the tree's root node to the node.}}
{{term|Forest}} {{defn|A forest is a set of n ≥ 0 disjoint trees.}}

===Data type vs. data structure===
There is a distinction between a tree as an abstract data type and as a concrete data structure, analogous to the distinction between a [[List (abstract data type)|list]] and a [[linked list]].
As a data type, a tree has a value and children, and the children are themselves trees; the value and children of the tree are interpreted as the value of the root node and the subtrees of the children of the root node. To allow finite trees, one must either allow the list of children to be empty (in which case trees can be required to be non-empty, an "empty tree" instead being represented by a forest of zero trees), or allow trees to be empty, in which case the list of children can be of fixed size ([[branching factor]], especially 2 or "binary"), if desired.

As a data structure, a linked tree is a group of [[Node (computer science)|nodes]], where each node has a value and a list of [[Reference (computer science)|references]] to other nodes (its children). This data structure actually defines a directed graph,{{efn|Properly, a rooted, ordered directed graph.}} because it may have loops or several references to the same node, just as a linked list may have a loop. Thus there is also the requirement that no two references point to the same node (that each node has at most a single parent, and in fact exactly one parent, except for the root), and a tree that violates this is "corrupt".

Due to the use of ''references'' to trees in the linked tree data structure, trees are often discussed implicitly assuming that they are being represented by references to the root node, as this is often how they are actually implemented. For example, rather than an empty tree, one may have a null reference: a tree is always non-empty, but a reference to a tree may be null.

===Recursive===

Recursively, as a data type a tree is defined as a value (of some data type, possibly empty), together with a list of trees (possibly an empty list), the subtrees of its children; symbolically:
 t: v &lt;nowiki&gt;[t[1], ..., t[k]]&lt;/nowiki&gt;
(A tree ''t'' consists of a value ''v'' and a list of other trees.)

More elegantly, via [[mutual recursion]], of which a tree is one of the most basic examples, a tree can be defined in terms of a forest (a list of trees), where a tree consists of a value and a forest (the subtrees of its children):
 f: &lt;nowiki&gt;[t[1], ..., t[k]]&lt;/nowiki&gt;
 t: v f

Note that this definition is in terms of values, and is appropriate in [[functional language]]s (it assumes [[Referential transparency (computer science)|referential transparency]]); different trees have no connections, as they are simply lists of values.

As a data structure, a tree is defined as a node (the root), which itself consists of a value (of some data type, possibly empty), together with a list of references to other nodes (list possibly empty, references possibly null); symbolically:
 n: v &lt;nowiki&gt;[&amp;amp;n[1], ..., &amp;amp;n[k]]&lt;/nowiki&gt;
(A node ''n'' consists of a value ''v'' and a list of references to other nodes.)

This data structure defines a directed graph,{{efn|Properly, a rooted, ordered directed graph.}} and for it to be a tree one must add a condition on its global structure (its topology), namely that at most one reference can point to any given node (a node has at most a single parent), and no node in the tree point to the root. In fact, every node (other than the root) must have exactly one parent, and the root must have no parents.

Indeed, given a list of nodes, and for each node a list of references to its children, one cannot tell if this structure is a tree or not without analyzing its global structure and  that it is in fact topologically a tree, as defined below.

===Type theory===
As an [[Abstract data type|ADT]], the abstract tree type ''T'' with values of some type ''E'' is defined, using the  abstract forest type ''F'' (list of trees), by the functions:
:value: ''T'' → ''E''
:children: ''T'' → ''F''
:nil: () → ''F''
:node: ''E'' × ''F'' → ''T''
with the axioms:
:value(node(''e'', ''f'')) = ''e''
:children(node(''e'', ''f'')) = ''f''
In terms of [[type theory]], a tree is an [[Recursive data type|inductive type]] defined by the constructors ''nil'' (empty forest) and ''node'' (tree with root node with given value and children).

===Mathematical===
Viewed as a whole, a tree data structure is an [[ordered tree]], generally with values attached to each node. Concretely, it is (if required to be non-empty):
* A [[rooted tree]] with the "away from root" direction (a more narrow term is an "[[Arborescence (graph theory)|arborescence]]"), meaning:
** A [[directed graph]],
** whose underlying [[undirected graph]] is a [[tree (graph theory)|tree]] (any two vertices are connected by exactly one simple path),
** with a distinguished root (one vertex is designated as the root),
** which determines the direction on the edges (arrows point away from the root; given an edge, the node that the edge points from is called the ''parent'' and the node that the edge points to is called the ''child''),
together with:
* an ordering on the child nodes of a given node, and
* a value (of some data type) at each node.
Often trees have a fixed (more properly, bounded) [[branching factor]] ([[outdegree]]), particularly always having two child nodes (possibly empty, hence ''at most'' two ''non-empty'' child nodes), hence a "binary tree".

Allowing empty trees makes some definitions simpler, some more complicated: a rooted tree must be non-empty, hence if empty trees are allowed the above definition instead becomes "an empty tree, or a rooted tree such that ...". On the other hand, empty trees simplify defining fixed branching factor: with empty trees allowed, a binary tree is a tree such that every node has exactly two children, each of which is a tree (possibly empty).The complete sets of operations on tree must include fork operation.

==Terminology==
A '''[[node (computer science)|node]]''' is a structure which may contain a value or condition, or represent a separate data structure (which could be a tree of its own). Each node in a tree has zero or more '''child nodes''', which are below it in the tree (by convention, trees are drawn growing downwards). A node that has a child is called the child's '''parent node''' (or ''ancestor node'', or [[Superior (hierarchy)|superior]]). A node has at most one parent.

An '''internal node''' (also known as an '''inner node''', '''inode''' for short, or '''branch node''') is any node of a tree that has child nodes. Similarly, an '''external node''' (also known as an '''outer node''', '''leaf node''', or '''terminal node''') is any node that does not have child nodes.

The topmost node in a tree is called the '''root node'''. Depending on definition, a tree may be required to have a root node (in which case all trees are non-empty), or may be allowed to be empty, in which case it does not necessarily have a root node. Being the topmost node, the root node will not have a parent. It is the node at which algorithms on the tree begin, since as a data structure, one can only pass from parents to children. Note that some algorithms (such as post-order depth-first search) begin at the root, but first visit leaf nodes (access the value of leaf nodes), only visit the root last (i.e., they first access the children of the root, but only access the ''value'' of the root last). All other nodes can be reached from it by following '''edges''' or '''links'''. (In the formal definition, each such path is also unique.) In diagrams, the root node is conventionally drawn at the top. In some trees, such as [[heap (data structure)|heaps]], the root node has special properties. Every node in a tree can be seen as the root node of the subtree rooted at that node.

The '''height''' of a node is the length of the longest downward path to a leaf from that node. The height of the root is the height of the tree. The '''depth''' of a node is the length of the path to its root (i.e., its ''root path''). This is commonly needed in the manipulation of the various self-balancing trees, [[AVL Trees]] in particular. The root node has depth zero, leaf nodes have height zero, and a tree with only a single node (hence both a root and leaf) has depth and height zero. Conventionally, an empty tree (tree with no nodes, if such are allowed) has depth and height −1.

A '''subtree''' of a tree ''T'' is a tree consisting of a node in ''T'' and all of its descendants in ''T''.{{efn|This is different from the formal definition of subtree used in graph theory, which is a subgraph that forms a tree – it need not include all descendants. For example, the root node by itself is a subtree in the graph theory sense, but not in the data structure sense (unless there are no descendants).}}&lt;ref&gt;{{MathWorld|id=Subtree|title=Subtree}}&lt;/ref&gt; Nodes thus correspond to subtrees (each node corresponds to the subtree of itself and all its descendants) – the subtree corresponding to the root node is the entire tree, and each node is the root node of the subtree it determines; the subtree corresponding to any other node is called a '''proper subtree''' (by analogy to a [[proper subset]]).

==Drawing Trees==
Trees are often drawn in the plane. Ordered trees can be represented essentially uniquely in the plane, and are hence called ''plane trees,'' as follows: if one fixes a conventional order (say, counterclockwise), and arranges the child nodes in that order (first incoming parent edge, then first child edge, etc.), this yields an embedding of the tree in the plane, unique up to [[ambient isotopy]]. Conversely, such an embedding determines an ordering of the child nodes.

If one places the root at the top (parents above children, as in a [[family tree]]) and places all nodes that are a given distance from the root (in terms of number of edges: the "level" of a tree) on a given horizontal line, one obtains a standard drawing of the tree. Given a binary tree, the first child is on the left (the "left node"), and the second child is on the right (the "right node").

==Representations==
There are many different ways to represent trees; common representations represent the nodes as [[Dynamic memory allocation|dynamically allocated]] records with pointers to their children, their parents, or both, or as items in an [[Array data structure|array]], with relationships between them determined by their positions in the array (e.g., [[binary heap]]).

Indeed, a binary tree can be implemented as a list of lists (a list where the values are lists): the head of a list (the value of the first term) is the left child (subtree), while the tail (the list of second and subsequent terms) is the right child (subtree). This can be modified to allow values as well, as in Lisp [[S-expression]]s, where the head (value of first term) is the value of the node, the head of the tail (value of second term) is the left child, and the tail of the tail (list of third and subsequent terms) is the right child.

In general a node in a tree will not have pointers to its parents, but this information can be included (expanding the data structure to also include a pointer to the parent) or stored separately. Alternatively, upward links can be included in the child node data, as in a [[threaded binary tree]].

==Generalizations==

===Digraphs===
If edges (to child nodes) are thought of as references, then a tree is a special case of a digraph, and the tree data structure can be generalized to represent [[directed graph]]s by removing the constraints that a node may have at most one parent, and that no cycles are allowed. Edges are still abstractly considered as pairs of nodes, however, the terms ''parent'' and ''child'' are usually replaced by different terminology (for example, ''source'' and ''target''). Different [[graph (data structure)#Representations|implementation strategies]] exist: a digraph can be represented by the same local data structure as a tree (node with value and list of children), assuming that "list of children" is a list of references, or globally by such structures as [[adjacency list]]s.

In [[graph theory]], a [[tree (graph theory)|tree]] is a connected acyclic [[Graph (data structure)|graph]]; unless stated otherwise, in graph theory trees and graphs are assumed undirected. There is no one-to-one correspondence between such trees and trees as data structure. We can take an arbitrary undirected tree, arbitrarily pick one of its [[vertex (graph theory)|vertices]] as the ''root'', make all its edges directed by making them point away from the root node – producing an [[Arborescence (graph theory)|arborescence]] – and assign an order to all the nodes. The result corresponds to a tree data structure. Picking a different root or different ordering produces a different one.

Given a node in a tree, its children define an ordered forest (the union of subtrees given by all the children, or equivalently taking the subtree given by the node itself and erasing the root). Just as subtrees are natural for recursion (as in a depth-first search), forests are natural for [[corecursion]] (as in a breadth-first search).

Via [[mutual recursion]], a forest can be defined as a list of trees (represented by root nodes), where a node (of a tree) consists of a value and a forest (its children):
 f: &lt;nowiki&gt;[n[1], ..., n[k]]&lt;/nowiki&gt;
 n: v f

==Traversal methods==
{{Main article|Tree traversal}}
Stepping through the items of a tree, by means of the connections between parents and children, is called '''walking the tree''', and the action is a '''walk''' of the tree. Often, an operation might be performed when a pointer arrives at a particular node. A walk in which each parent node is traversed before its children is called a '''pre-order''' walk; a walk in which the children are traversed before their respective parents are traversed is called a '''post-order''' walk; a walk in which a node's left subtree, then the node itself, and finally its right subtree are traversed is called an '''in-order''' traversal. (This last scenario, referring to exactly two subtrees, a left subtree and a right subtree, assumes specifically a [[binary tree]].)
A '''level-order''' walk effectively performs a [[breadth-first search]] over the entirety of a tree; nodes are traversed level by level, where the root node is visited first, followed by its direct child nodes and their siblings, followed by its grandchild nodes and their siblings, etc., until all nodes in the tree have been traversed.

==Common operations==
* Enumerating all the items
* Enumerating a section of a tree
* Searching for an item
* Adding a new item at a certain position on the tree
* Deleting an item
* [[Pruning (algorithm)|Pruning]]: Removing a whole section of a tree
* [[Grafting (algorithm)|Grafting]]: Adding a whole section to a tree
* Finding the root for any node

==Common uses==
* Representing [[hierarchical]] data
* Storing data in a way that makes it efficiently [[search algorithm|searchable]] (see [[binary search tree]] and [[tree traversal]])
* Representing [[sorting algorithm|sorted lists]] of data
* As a workflow for [[Digital compositing|compositing]] digital images for [[visual effects]]
* [[Routing]] algorithms

==See also==
* [[Tree structure]]
* [[Tree (graph theory)]]
* [[Tree (set theory)]]
* [[Hierarchy (mathematics)]]
* [[Dialog tree]]
* [[Single inheritance]]
* [[Generative grammar]]
* [[Hierarchical clustering]]
* [[Binary space partition tree]]
* [[Recursion]]

===Other trees===
* [[Trie]]
* [[DSW algorithm]]
* [[Enfilade (Xanadu)|Enfilade]]
* [[Left child-right sibling binary tree]]
* [[Hierarchical temporal memory]]

==Notes==
{{notelist}}

==References==
{{Reflist}}
{{refbegin}}
* [[Donald Knuth]]. ''[[The Art of Computer Programming]]: Fundamental Algorithms'', Third Edition. Addison-Wesley, 1997. ISBN 0-201-89683-4 . Section 2.3: Trees, pp.&amp;nbsp;308–423.
* [[Thomas H. Cormen]], [[Charles E. Leiserson]], [[Ronald L. Rivest]], and [[Clifford Stein]]. ''[[Introduction to Algorithms]]'', Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7 . Section 10.4: Representing rooted trees, pp.&amp;nbsp;214–217. Chapters 12–14 (Binary Search Trees, Red-Black Trees, Augmenting Data Structures), pp.&amp;nbsp;253–320.
{{refend}}

==External links==
{{Commons category|Tree structures}}
* [http://www.community-of-knowledge.de/beitrag/data-trees-as-a-means-of-presenting-complex-data-analysis/ Data Trees as a Means of Presenting Complex Data Analysis] by Sally Knipe
* [https://xlinux.nist.gov/dads/HTML/tree.html Description] from the [[Dictionary of Algorithms and Data Structures]]
* [http://www.ipub.com/data.tree data.tree] implementation of a tree data structure in the R programming language
* [http://wormweb.org/celllineage WormWeb.org: Interactive Visualization of the ''C. elegans'' Cell Tree] – Visualize the entire cell lineage tree of the nematode ''C. elegans'' (javascript)
* [http://www.allisons.org/ll/AlgDS/Tree/ ''Binary Trees'' by L. Allison]

{{CS-Trees}}
{{Data structures}}

{{DEFAULTSORT:Tree (Data Structure)}}
[[Category:Data types]]
[[Category:Trees (data structures)| ]]
[[Category:Knowledge representation]]

[[de:Baum (Graphentheorie)]]</text>
      <sha1>5mw8vhows2qc8x38az6c5u48rb9q1jv</sha1>
    </revision>
  </page>
  <page>
    <title>Semantic triple</title>
    <ns>0</ns>
    <id>51445651</id>
    <revision>
      <id>738439464</id>
      <parentid>738439326</parentid>
      <timestamp>2016-09-08T23:48:38Z</timestamp>
      <contributor>
        <username>Waldir</username>
        <id>182472</id>
      </contributor>
      <minor />
      <comment>add {{Semantic Web}} navbox</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2417" xml:space="preserve">A '''semantic triple''', or simply '''triple''', is the atomic data entity in the [[Resource Description Framework]] (RDF) data model.&lt;ref&gt;http://www.w3.org/TR/PR-rdf-syntax/ "Resource Description Framework (RDF) Model and Syntax Specification"&lt;/ref&gt; As its name indicates, a triple is a [[tuple|set of three entities]] that codifies a [[statement (programming)|statement]] about [[Data model|semantic data]] in the form of subject–predicate–object expressions (e.g. "Bob is 35", or "Bob knows John").

This format enables [[Knowledge representation and reasoning|knowledge to be represented]] in a machine-readable way. Particularly, every part of an RDF triple is individually addressable via unique [[Uniform Resource Identifier|URIs]] — for example, the second statement above might be represented in RDF as &lt;code&gt;&lt;nowiki&gt;http://example.name#BobSmith12 http://xmlns.com/foaf/0.1/knows http://example.name#JohnDoe34&lt;/nowiki&gt;&lt;/code&gt;.
Given this precise representation, semantic data can be unambiguously [[Semantic query|queried]] and [[Semantic reasoner|reasoned]] about.

The components of a triple, such as the statement "The sky has the color blue", consist of a [[Subject (grammar)|subject]] ("the sky"), a [[Predicate (grammar)|predicate]] ("has the color"), and an [[Object (grammar)|object]] ("blue"). This is similar to the classical notation of an [[entity–attribute–value model]] within [[object-oriented design]], where this example would be expressed as an entity (sky), an attribute (color) and a value (blue). From this basic structure, triples can be composed into [[Semantic network|more complex models]], by using triples as objects or subjects of other triples — for example, &lt;code&gt;Mike → said → (triples → can be → objects)&lt;/code&gt;.

Given their particular, consistent structure, a collection of triples is often stored in purpose-built databases called [[Triplestore]]s.

== See also ==
* [[Named graph#Named graphs and quads]], an extension to semantic triples to also include a context node as a fourth element.

== References ==
{{reflist}}

== External links ==
* {{cite web |url = https://www.w3.org/TR/rdf11-primer/#section-triple |title = RDF 1.1 Primer § Triples |publisher = [[World Wide Web Consortium|W3C]] }}

{{Semantic Web}}

[[Category:Semantic Web]]
[[Category:Data modeling]]
[[Category:Resource Description Framework]]
[[Category:Knowledge representation]]</text>
      <sha1>ewf0urdtisoxc9mwki02g6dsaujcbxd</sha1>
    </revision>
  </page>
  <page>
    <title>Library branch</title>
    <ns>0</ns>
    <id>52141053</id>
    <revision>
      <id>758403356</id>
      <parentid>747981181</parentid>
      <timestamp>2017-01-05T05:55:16Z</timestamp>
      <contributor>
        <username>Level C</username>
        <id>18054835</id>
      </contributor>
      <comment>/* See also */  links already occur in the article text - is against the Manual of Style to have them in See also section</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1758" xml:space="preserve">[[File:New York Public Library May 2011.JPG|200 px|thumb|right|The [[New York Public Library Main Branch]] in [[Bryant Park]], [[Manhattan]]]]
'''Library branches''' are libraries that form part of a [[library system]] but are not located in the same area, building or city, but use the same [[Library classification]] for their catalogs and are interconnected with all the branches of the system that form part of the systems and to library patrons through a [[integrated library system]].&lt;ref&gt;{{cite web|url=http://www.merriam-webster.com/dictionary/branch |title=Branch &amp;#124; Definition of Branch by Merriam-Webster |website=Merriam-webster.com |date= |accessdate=2016-11-05}}&lt;/ref&gt;

Most of [[County|counties]] of every country have their own [[library system]] that usually have between to 20 libraries on every city of their counties, some of them are; London Public Library (on Canada) with 16 library branches, [[Helsinki Metropolitan Area Libraries]] with 63 libraries,&lt;ref&gt;{{cite web| url=http://www.iii.com/news/pr_display.php?id=559 | title=Helsinki Metropolitan Area Libraries (Finland) Upgrades to Sierra Services Platform | publisher=Innovative | type= Press release | date=5 February 2013 | accessdate=1 August 2014 }}&lt;/ref&gt; [[National Library of Venezuela]] with 685 branches.

Some popular library branches includ [[New York Public Library Main Branch]], part of [[New York Public Library|New York Public Library System]], and [[Martin Luther King Jr. Memorial Library]], a branch of [[District of Columbia Public Library|District of Columbia Public Library System]].

==References==
{{Reflist}}

[[Category:Public libraries]]
[[Category:Private libraries]]
[[Category:Libraries]]
[[Category:Culture]]
[[Category:Knowledge representation]]</text>
      <sha1>9ncejnd70o9liod61zzgve0rd8n4xvu</sha1>
    </revision>
  </page>
  </mediawiki>