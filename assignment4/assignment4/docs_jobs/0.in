<?xml version='1.0' encoding='utf8'?>
<mediawiki><siteinfo>
    <sitename>Wikipedia</sitename>
    <dbname>enwiki</dbname>
    <base>https://en.wikipedia.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.29.0-wmf.9</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace case="first-letter" key="-2">Media</namespace>
      <namespace case="first-letter" key="-1">Special</namespace>
      <namespace case="first-letter" key="0" />
      <namespace case="first-letter" key="1">Talk</namespace>
      <namespace case="first-letter" key="2">User</namespace>
      <namespace case="first-letter" key="3">User talk</namespace>
      <namespace case="first-letter" key="4">Wikipedia</namespace>
      <namespace case="first-letter" key="5">Wikipedia talk</namespace>
      <namespace case="first-letter" key="6">File</namespace>
      <namespace case="first-letter" key="7">File talk</namespace>
      <namespace case="first-letter" key="8">MediaWiki</namespace>
      <namespace case="first-letter" key="9">MediaWiki talk</namespace>
      <namespace case="first-letter" key="10">Template</namespace>
      <namespace case="first-letter" key="11">Template talk</namespace>
      <namespace case="first-letter" key="12">Help</namespace>
      <namespace case="first-letter" key="13">Help talk</namespace>
      <namespace case="first-letter" key="14">Category</namespace>
      <namespace case="first-letter" key="15">Category talk</namespace>
      <namespace case="first-letter" key="100">Portal</namespace>
      <namespace case="first-letter" key="101">Portal talk</namespace>
      <namespace case="first-letter" key="108">Book</namespace>
      <namespace case="first-letter" key="109">Book talk</namespace>
      <namespace case="first-letter" key="118">Draft</namespace>
      <namespace case="first-letter" key="119">Draft talk</namespace>
      <namespace case="first-letter" key="446">Education Program</namespace>
      <namespace case="first-letter" key="447">Education Program talk</namespace>
      <namespace case="first-letter" key="710">TimedText</namespace>
      <namespace case="first-letter" key="711">TimedText talk</namespace>
      <namespace case="first-letter" key="828">Module</namespace>
      <namespace case="first-letter" key="829">Module talk</namespace>
      <namespace case="first-letter" key="2300">Gadget</namespace>
      <namespace case="first-letter" key="2301">Gadget talk</namespace>
      <namespace case="case-sensitive" key="2302">Gadget definition</namespace>
      <namespace case="case-sensitive" key="2303">Gadget definition talk</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>Category:Data management</title>
    <ns>14</ns>
    <id>762162</id>
    <revision>
      <id>761378461</id>
      <parentid>728513377</parentid>
      <timestamp>2017-01-22T17:37:48Z</timestamp>
      <contributor>
        <username>JustBerry</username>
        <id>19075131</id>
      </contributor>
      <minor />
      <comment>/* top */Cleaning up... using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="399" xml:space="preserve">{{Commons cat|Data management}}
*'''[[Data management]]''' — all the disciplines related to managing '''{{C|Data|data}}''' as a valuable resource.

{{clr}}
:::::{{Catmain|Data management}}
{{catdiffuse}}
{{CategoryTOC}}

{{Database}}
{{Databases}}

[[Category:Data|Management]]
[[Category:Computer data|Management]]
[[Category:Information retrieval]]
[[Category:Information technology management]]</text>
      <sha1>mji0q95ez6jn8lhp588vshpg3l5y4i2</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Internet search</title>
    <ns>14</ns>
    <id>8321034</id>
    <revision>
      <id>666703046</id>
      <parentid>547928112</parentid>
      <timestamp>2015-06-13T01:34:55Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>move to Category:Information retrieval, tidy up comment</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="228" xml:space="preserve">{{Cat main|Internet search}}

[[Category:Web services]]
[[Category:Information retrieval]]
[[Category:World Wide Web|Search]] &lt;!-- searching is a web function. Note that [[Internet search]] redirects to [[Web search engine]] --&gt;</text>
      <sha1>6bp9rbz93mtxeowt780j0keu5nkf5cq</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Information retrieval organizations</title>
    <ns>14</ns>
    <id>46964829</id>
    <revision>
      <id>747326238</id>
      <parentid>666703545</parentid>
      <timestamp>2016-11-01T19:13:47Z</timestamp>
      <contributor>
        <ip>50.53.1.33</ip>
      </contributor>
      <comment>[[:Category:Organizations by subject]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="72" xml:space="preserve">[[Category:Information retrieval]]
[[Category:Organizations by subject]]</text>
      <sha1>6oxv3bk2f696b15yla517vsopfxwpr9</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Information retrieval systems</title>
    <ns>14</ns>
    <id>46964839</id>
    <revision>
      <id>666703925</id>
      <timestamp>2015-06-13T01:42:50Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>Category:Information retrieval</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="34" xml:space="preserve">[[Category:Information retrieval]]</text>
      <sha1>dobw7kl3saam8hrak0oyyn1jbi7dqlz</sha1>
    </revision>
  </page>
  <page>
    <title>Evaluation measures (information retrieval)</title>
    <ns>0</ns>
    <id>50716473</id>
    <revision>
      <id>749401100</id>
      <parentid>730899398</parentid>
      <timestamp>2016-11-14T03:37:41Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor />
      <comment>1 archive template merged to {{[[template:webarchive|webarchive]]}} ([[User:Green_Cardamom/Webarchive_template_merge|WAM]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="16733" xml:space="preserve">{{Orphan|date=June 2016}}

The '''evaluation measures''' of an information retrieval system is the process of assessing how well the search results satisfied the user's query intent. The metrics are often split in to multiple categories. Online metrics measure actual users' interactions with the search system. Offline metrics measure the relevance of the search engine by having expert judges measure how likely each result (or the SERP page as a whole) is to meet the information needs of the user.

The mathematical symbols used in the formulas below mean:
* &lt;math&gt;X \cap Y&lt;/math&gt; - [[Intersection (set theory)|Intersection]] - in this case, specifying the documents in ''both'' sets X and Y
* &lt;math&gt;| X |&lt;/math&gt; - [[Cardinality]] - in this case, the number of documents in set X
* &lt;math&gt;\int&lt;/math&gt; - [[Integral]]
* &lt;math&gt;\sum&lt;/math&gt; - [[Summation]]
* &lt;math&gt;\Delta&lt;/math&gt; - [[Symmetric difference]]

== Online metrics ==
Online metrics are generally created from data mined from search logs. The metrics are often used to determine the success of an [[A/B testing|A/B test]].

=== Session abandonment rate ===
Session abandonment rate is a ratio of search session which do not result in a click.

=== Click-through rate ===
Click-through rate ('''CTR''') is the ratio of users who click on a specific link to the number of total users who view a page, email, or advertisement. It is commonly used to measure the success of an [[online advertising]] campaign for a particular website as well as the effectiveness of email campaigns.&lt;ref name=AMA&gt;[[American Marketing Association]] Dictionary. http://www.marketingpower.com/_layouts/Dictionary.aspx.{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }} Retrieved 2012-11-02. The [[Marketing Accountability Standards Board (MASB)]] endorses this definition as part of its ongoing [http://www.commonlanguage.wikispaces.net/ Common Language in Marketing Project].&lt;/ref&gt;

=== Session success rate ===
Session success rate measures the ratio of user sessions that lead to a success. Defining "success" is often dependent on context, but for search a successful result is often measured using [[Dwell time (information retrieval)|dwell time]] as a primary factor along with secondary user interaction, for instance, the user copying the result URL is considered a successful result, as is copy/pasting from the snippet.

=== Zero result rate ===
Zero result rate ('''ZRR''') is the ratio of [[Search engine results page|SERPs]] which returned with zero results. The metric either indicates a [[Precision and recall|recall]] issue, or that the information being searched for is not in the index.

== Offline metrics ==
Offline metrics are generally created from relevance judgement sessions where the judges score the quality of the search results.  The judges often score each result of a query as either binary (good/bad), or on a multi-level scale of satisfying the needs of the searcher. In practice, queries may be [[ill-posed]] and there may be different shades of relevancy. For instance there is ambiguity in the query "mars", the judge does not know if the user is search for [[Mars]] the planet, [[Mars (chocolate bar)|Mars]] the chocolate bar, or [[Bruno Mars]] the singer.

=== Precision ===
{{main|Precision and recall}}

Precision is the fraction of the documents retrieved that are [[Relevance (information retrieval)|relevant]] to the user's information need.

:&lt;math&gt; \mbox{precision}=\frac{|\{\mbox{relevant documents}\}\cap\{\mbox{retrieved documents}\}|}{|\{\mbox{retrieved documents}\}|} &lt;/math&gt;

In [[binary classification]], precision is analogous to [[positive predictive value]]. Precision takes all retrieved documents into account. It can also be evaluated at a given cut-off rank, considering only the topmost results returned by the system. This measure is called ''precision at n'' or ''P@n''.

Note that the meaning and usage of "precision" in the field of information retrieval differs from the definition of [[accuracy and precision]] within other branches of science and [[statistics]].

=== Recall ===
{{main|Precision and recall}}

Recall is the fraction of the documents that are relevant to the query that are successfully retrieved.

:&lt;math&gt;\mbox{recall}=\frac{|\{\mbox{relevant documents}\}\cap\{\mbox{retrieved documents}\}|}{|\{\mbox{relevant documents}\}|} &lt;/math&gt;

In binary classification, recall is often called [[sensitivity and specificity|sensitivity]]. So it can be looked at as ''the probability that a relevant document is retrieved by the query''.

It is trivial to achieve recall of 100% by returning all documents in response to any query. Therefore, recall alone is not enough but one needs to measure the number of non-relevant documents also, for example by computing the precision.

=== Fall-out ===
The proportion of non-relevant documents that are retrieved, out of all non-relevant documents available:

:&lt;math&gt; \mbox{fall-out}=\frac{|\{\mbox{non-relevant documents}\}\cap\{\mbox{retrieved documents}\}|}{|\{\mbox{non-relevant documents}\}|} &lt;/math&gt;

In binary classification, fall-out is closely related to [[sensitivity and specificity|specificity]] and is equal to &lt;math&gt;(1-\mbox{specificity})&lt;/math&gt;. It can be looked at as ''the probability that a non-relevant document is retrieved by the query''.

It is trivial to achieve fall-out of 0% by returning zero documents in response to any query.

=== F-score / F-measure ===
{{main|F-score}}
The weighted [[harmonic mean]] of precision and recall, the traditional F-measure or balanced F-score is:

:&lt;math&gt;F = \frac{2 \cdot \mathrm{precision} \cdot \mathrm{recall}}{(\mathrm{precision} + \mathrm{recall})}&lt;/math&gt;

This is also known as the &lt;math&gt;F_1&lt;/math&gt; measure, because recall and precision are evenly weighted.

The general formula for non-negative real &lt;math&gt;\beta&lt;/math&gt; is:
:&lt;math&gt;F_\beta = \frac{(1 + \beta^2) \cdot (\mathrm{precision} \cdot \mathrm{recall})}{(\beta^2 \cdot \mathrm{precision} + \mathrm{recall})}\,&lt;/math&gt;

Two other commonly used F measures are the &lt;math&gt;F_{2}&lt;/math&gt; measure, which weights recall twice as much as precision, and the &lt;math&gt;F_{0.5}&lt;/math&gt; measure, which weights precision twice as much as recall.

The F-measure was derived by van Rijsbergen (1979) so that &lt;math&gt;F_\beta&lt;/math&gt; "measures the effectiveness of retrieval with respect to a user who attaches &lt;math&gt;\beta&lt;/math&gt; times as much importance to recall as precision".  It is based on van Rijsbergen's effectiveness measure &lt;math&gt;E = 1 - \frac{1}{\frac{\alpha}{P} + \frac{1-\alpha}{R}}&lt;/math&gt;.  Their relationship is:
:&lt;math&gt;F_\beta = 1 - E&lt;/math&gt; where &lt;math&gt;\alpha=\frac{1}{1 + \beta^2}&lt;/math&gt;

F-measure can be a better single metric when compared to precision and recall; both precision and recall give different information that can complement each other when combined. If one of them excels more than the other, F-measure will reflect it.{{citation needed|date=June 2015}}

=== Average precision ===
&lt;!-- [[Average precision]] redirects here --&gt;
Precision and recall are single-value metrics based on the whole list of documents returned by the system. For systems that return a ranked sequence of documents, it is desirable to also consider the order in which the returned documents are presented. By computing a precision and recall at every position in the ranked sequence of documents, one can plot a precision-recall curve, plotting precision &lt;math&gt;p(r)&lt;/math&gt; as a function of recall &lt;math&gt;r&lt;/math&gt;. Average precision computes the average value of &lt;math&gt;p(r)&lt;/math&gt; over the interval from &lt;math&gt;r=0&lt;/math&gt; to &lt;math&gt;r=1&lt;/math&gt;:&lt;ref name="zhu2004"&gt;{{cite journal |first=Mu |last=Zhu |title=Recall, Precision and Average Precision |url=http://sas.uwaterloo.ca/stats_navigation/techreports/04WorkingPapers/2004-09.pdf |year=2004 }}&lt;/ref&gt;
:&lt;math&gt;\operatorname{AveP} = \int_0^1 p(r)dr&lt;/math&gt;
That is the area under the precision-recall curve.
This integral is in practice replaced with a finite sum over every position in the ranked sequence of documents:
:&lt;math&gt;\operatorname{AveP} = \sum_{k=1}^n P(k) \Delta r(k)&lt;/math&gt;
where &lt;math&gt;k&lt;/math&gt; is the rank in the sequence of retrieved documents, &lt;math&gt;n&lt;/math&gt; is the number of retrieved documents, &lt;math&gt;P(k)&lt;/math&gt; is the precision at cut-off &lt;math&gt;k&lt;/math&gt; in the list, and &lt;math&gt;\Delta r(k)&lt;/math&gt; is the change in recall from items &lt;math&gt;k-1&lt;/math&gt; to &lt;math&gt;k&lt;/math&gt;.&lt;ref name="zhu2004" /&gt;

This finite sum is equivalent to:
:&lt;math&gt; \operatorname{AveP} = \frac{\sum_{k=1}^n (P(k) \times \operatorname{rel}(k))}{\mbox{number of relevant documents}} \!&lt;/math&gt;
where &lt;math&gt;\operatorname{rel}(k)&lt;/math&gt; is an indicator function equaling 1 if the item at rank &lt;math&gt;k&lt;/math&gt; is a relevant document, zero otherwise.&lt;ref name="Turpin2006"&gt;{{cite journal |last=Turpin |first=Andrew |last2=Scholer |first2=Falk |title=User performance versus precision measures for simple search tasks |journal=Proceedings of the 29th Annual international ACM SIGIR Conference on Research and Development in information Retrieval (Seattle, WA, August 06–11, 2006) |publisher=ACM |location=New York, NY |pages=11–18 |doi=10.1145/1148170.1148176 |year=2006 |isbn=1-59593-369-7 }}&lt;/ref&gt; Note that the average is over all relevant documents and the relevant documents not retrieved get a precision score of zero.

Some authors choose to interpolate the &lt;math&gt;p(r)&lt;/math&gt; function to reduce the impact of "wiggles" in the curve.&lt;ref name=voc2010&gt;{{cite journal |last=Everingham |first=Mark |last2=Van Gool |first2=Luc |last3=Williams |first3=Christopher K. I. |last4=Winn |first4=John |last5=Zisserman |first5=Andrew |title=The PASCAL Visual Object Classes (VOC) Challenge |journal=International Journal of Computer Vision |volume=88 |issue=2 |pages=303–338 |publisher=Springer |date=June 2010 |url=http://pascallin.ecs.soton.ac.uk/challenges/VOC/pubs/everingham10.pdf |accessdate=2011-08-29 |doi=10.1007/s11263-009-0275-4 }}&lt;/ref&gt;&lt;ref name="nlpbook"&gt;{{cite book |last=Manning |first=Christopher D. |last2=Raghavan |first2=Prabhakar |last3=Schütze |first3=Hinrich |title=Introduction to Information Retrieval |publisher=Cambridge University Press |year=2008 |url=http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html }}&lt;/ref&gt; For example, the PASCAL Visual Object Classes challenge (a benchmark for computer vision object detection) computes average precision by averaging the precision over a set of evenly spaced recall levels {0, 0.1, 0.2, ... 1.0}:&lt;ref name="voc2010" /&gt;&lt;ref name="nlpbook" /&gt;
:&lt;math&gt;\operatorname{AveP} = \frac{1}{11} \sum_{r \in \{0, 0.1, \ldots, 1.0\}} p_{\operatorname{interp}}(r)&lt;/math&gt;
where &lt;math&gt;p_{\operatorname{interp}}(r)&lt;/math&gt; is an interpolated precision that takes the maximum precision over all recalls greater than &lt;math&gt;r&lt;/math&gt;:
:&lt;math&gt;p_{\operatorname{interp}}(r) = \operatorname{max}_{\tilde{r}:\tilde{r} \geq r} p(\tilde{r})&lt;/math&gt;.

An alternative is to derive an analytical &lt;math&gt;p(r)&lt;/math&gt; function by assuming a particular parametric distribution for the underlying decision values. For example, a ''binormal precision-recall curve'' can be obtained by assuming decision values in both classes to follow a Gaussian distribution.&lt;ref&gt;K.H. Brodersen, C.S. Ong, K.E. Stephan, J.M. Buhmann (2010). [http://icpr2010.org/pdfs/icpr2010_ThBCT8.28.pdf The binormal assumption on precision-recall curves] {{webarchive |url=https://web.archive.org/web/20121208201457/http://icpr2010.org/pdfs/icpr2010_ThBCT8.28.pdf |date=December 8, 2012 }}. ''Proceedings of the 20th International Conference on Pattern Recognition'', 4263-4266.&lt;/ref&gt;

=== Precision at K ===

For modern (Web-scale) information retrieval, recall is no longer a meaningful metric, as many queries have thousands of relevant documents, and few users will be interested in reading all of them. [[Precision and recall#Precision|Precision]] at k documents (P@k) is still a useful metric (e.g., P@10 or "Precision at 10" corresponds to the number of relevant results on the first search results page), but fails to take into account the positions of the relevant documents among the top k.{{citation needed|date=June 2015}}  Another shortcoming is that on a query with fewer relevant results than k, even a perfect system will have a score less than 1.&lt;ref name="stanford" /&gt;  It is easier to score manually since only the top k results need to be examined to determine if they are relevant or not.

=== R-Precision ===

R-precision requires knowing all documents that are relevant to a query.  The number of relevant documents, &lt;math&gt;R&lt;/math&gt;, is used as the cutoff for calculation, and this varies from query to query.  For example, if there are 15 documents relevant to "red" in a corpus (R=15), R-precision for "red" looks at the top 15 documents returned, counts the number that are relevant &lt;math&gt;r&lt;/math&gt; turns that into a relevancy fraction: &lt;math&gt;r/R = r/15&lt;/math&gt;.&lt;ref name="trec15"/&gt;

Precision is equal to recall at the '''R'''-th position.&lt;ref name="stanford"&gt;{{cite web|url=http://nlp.stanford.edu/IR-book/pdf/08eval.pdf|title=Chapter 8: Evaluation in information retrieval|accessdate=2015-06-14|date=2009|authors=Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze}}  Part of ''Introduction to Information Retrieval'' [http://nlp.stanford.edu/IR-book/]&lt;/ref&gt;

Empirically, this measure is often highly correlated to mean average precision.&lt;ref name="stanford" /&gt;

=== Mean average precision ===
&lt;!-- [[Mean average precision]] redirects here --&gt;
Mean average precision for a set of queries is the mean of the average precision scores for each query.
:&lt;math&gt; \operatorname{MAP} = \frac{\sum_{q=1}^Q \operatorname{AveP(q)}}{Q} \!&lt;/math&gt;
where ''Q'' is the number of queries.

=== Discounted cumulative gain ===
{{main|Discounted cumulative gain}}
DCG uses a graded relevance scale of documents from the result set to evaluate the usefulness, or gain, of a document based on its position in the result list. The premise of DCG is that highly relevant documents appearing lower in a search result list should be penalized as the graded relevance value is reduced logarithmically proportional to the position of the result.

The DCG accumulated at a particular rank position &lt;math&gt;p&lt;/math&gt; is defined as:

:&lt;math&gt; \mathrm{DCG_{p}} = rel_{1} + \sum_{i=2}^{p} \frac{rel_{i}}{\log_{2}i}. &lt;/math&gt;

Since result set may vary in size among different queries or systems, to compare performances the normalised version of DCG uses an ideal DCG. To this end, it sorts documents of a result list by relevance, producing an ideal DCG at position p (&lt;math&gt;IDCG_p&lt;/math&gt;), which normalizes the score:

:&lt;math&gt; \mathrm{nDCG_{p}} = \frac{DCG_{p}}{IDCG{p}}. &lt;/math&gt;

The nDCG values for all queries can be averaged to obtain a measure of the average performance of a ranking algorithm. Note that in a perfect ranking algorithm, the &lt;math&gt;DCG_p&lt;/math&gt; will be the same as the &lt;math&gt;IDCG_p&lt;/math&gt; producing an nDCG of 1.0. All nDCG calculations are then relative values on the interval 0.0 to 1.0 and so are cross-query comparable.

=== Other measures ===
{{Confusion matrix terms}}
* [[Mean reciprocal rank]]
* [[Spearman's rank correlation coefficient]]
* bpref - a summation-based measure of how many relevant documents are ranked before irrelevant documents&lt;ref name="trec15"&gt;http://trec.nist.gov/pubs/trec15/appendices/CE.MEASURES06.pdf&lt;/ref&gt;
* GMAP - geometric mean of (per-topic) average precision&lt;ref name="trec15" /&gt;
* Measures based on marginal relevance and document diversity - see {{section link|Relevance (information retrieval)|Problems and alternatives}}

===Visualization===

Visualizations of information retrieval performance include:
* Graphs which chart precision on one axis and recall on the other&lt;ref name="trec15" /&gt;
* Histograms of average precision over various topics&lt;ref name="trec15" /&gt;
* [[Receiver operating characteristic]] (ROC curve)
* [[Confusion matrix]]

== Non-metrics ==

=== Top queries list ===
Top queries is noting the most common queries over a fixed amount of time. The top queries list assists in knowing the style of queries entered by users.

== Non-relevance metrics ==

=== Queries per time ===
Measuring how many queries are performed on the search system per (month/day/hour/minute/sec) tracks the utilization of the search system. It can be used for diagnostics to indicate an unexpected spike in queries, or simply as a baseline when comparing with other metrics, like query latency. For example, a spike in query traffic, may be used to explain a spike in query latency.

==References==
&lt;references /&gt;

[[Category:Information retrieval]]
[[Category:Information retrieval evaluation]]
[[Category:Internet search engines]]</text>
      <sha1>7gm642zlzcutuefyvrtwsslvuv412p7</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Query languages</title>
    <ns>14</ns>
    <id>911721</id>
    <revision>
      <id>546489366</id>
      <parentid>494452511</parentid>
      <timestamp>2013-03-23T07:13:36Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor />
      <comment>[[User:Addbot|Bot:]] Migrating 21 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q7142646]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="258" xml:space="preserve">This [[Wikipedia:Category|category]] lists those [[domain-specific programming language]]s targeted at performing [[database]] [[query language|queries]].

[[Category:Domain-specific programming languages]]
[[Category:Data management]]
[[Category:Databases]]</text>
      <sha1>1z0upln47suqadco8ygku8ac77orck1</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Concurrency control</title>
    <ns>14</ns>
    <id>1462863</id>
    <revision>
      <id>546502164</id>
      <parentid>481028162</parentid>
      <timestamp>2013-03-23T09:06:21Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor />
      <comment>[[User:Addbot|Bot:]] Migrating 8 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q7285247]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="220" xml:space="preserve">{{Cat main|Concurrency control}}

[[Category:Data management]]
[[Category:Synchronization]]
[[Category:Operating system technology]]
[[Category:Concurrency (computer science)]]
[[Category:Distributed computing problems]]</text>
      <sha1>qs9409ni8e4xzljoornfpvxcg39n7bd</sha1>
    </revision>
  </page>
  <page>
    <title>Enterprise Objects Framework</title>
    <ns>0</ns>
    <id>59561</id>
    <revision>
      <id>760026911</id>
      <parentid>687688793</parentid>
      <timestamp>2017-01-14T14:56:57Z</timestamp>
      <contributor>
        <username>演歌ビニール</username>
        <id>23956730</id>
      </contributor>
      <minor />
      <comment>"Mac OS X" -&gt; "macOS"</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9613" xml:space="preserve">The '''Enterprise Objects Framework''', or more commonly simply '''EOF''', was introduced by [[NeXT]] in 1994 as a pioneering [[object-relational mapping]] product for its [[NeXTSTEP]] and [[OpenStep]] development platforms. EOF abstracts the process of interacting with a [[relational database]], mapping database rows to [[Java (programming language)|Java]] or [[Objective-C]] [[Object (computer science)|objects]]. This largely relieves developers from writing low-level [[SQL]] code. EOF enjoyed some niche success in the mid-1990s among financial institutions who were attracted to the rapid application development advantages of NeXT's object-oriented platform. Since [[Apple Inc]]'s merger with NeXT in 1996, EOF has evolved into a fully integrated part of [[WebObjects]], an application server also originally from NeXT.

== History ==
In the early 1990s [[NeXT]] Computer recognized that connecting to databases was essential to most businesses and yet also potentially complex.  Every data source has a different data-access language (or [[Application programming interface|API]]), driving up the costs to learn and use each vendor's product. The NeXT engineers wanted to apply the advantages of [[object-oriented programming]], by getting objects to "talk" to relational databases. As the two technologies are very different, the solution was to create an abstraction layer, insulating developers from writing the low-level procedural code ([[SQL]]) specific to each data source.

The first attempt came in 1992 with the release of Database Kit (DBKit), which wrapped an object-oriented framework around any database. Unfortunately, [[NEXTSTEP]] at the time was not powerful enough and DBKit had serious design flaws.

NeXT's second attempt came in 1994 with the Enterprise Objects Framework (EOF) version 1, a [[Rewrite (programming)|complete rewrite]] that was far more modular and [[OpenStep]] compatible. EOF 1.0 was the first product released by [[NeXT]] using the Foundation Kit and introduced autoreleased objects to the developer community. The development team at the time was only four people: Jack Greenfield, Rich Williamson, Linus Upson and Dan Willhite. EOF 2.0, released in late 1995, further refined the architecture, introducing the editing context. At that point, the development team consisted of Dan Willhite, [[Craig Federighi]], Eric Noyau and Charly Kleissner.

EOF achieved a modest level of popularity in the financial programming community in the mid-1990s, but it would come into its own with the emergence of the [[World Wide Web]] and the concept of [[web application]]s. It was clear that EOF could help companies plug their legacy databases into the Web without any rewriting of that data. With the addition of frameworks to do state management, load balancing and dynamic HTML generation, NeXT was able to launch the first object-oriented Web application server, [[WebObjects]], in 1996, with EOF at its core.

In 2000, Apple Inc. (which had merged with NeXT) officially dropped EOF as a standalone product, meaning that developers would be unable to use it to create desktop applications for the forthcoming [[macOS|Mac OS X]].  It would, however, continue to be an integral part of a major new release of WebObjects.  WebObjects 5, released in 2001, was significant for the fact that its frameworks had been ported from their native [[Objective-C]] programming language to the [[Java (programming language)|Java]] language.  Critics of this change argue that most of the power of EOF was a side effect of its Objective-C roots, and that EOF lost the beauty or simplicity it once had.  Third-party tools, such as [[EOGenerator]], help fill the deficiencies introduced by Java (mainly due to the loss of [[Objective-C#Categories|categories]]).

The Objective-C code base was re-introduced with some modifications to desktop application developers as [[Core Data]], part of Apple's [[Cocoa (API)|Cocoa API]], with the release of [[Mac OS X Tiger]] in April 2005.

==How EOF works==

Enterprise Objects provides tools and frameworks for object-relational mapping. The technology specializes in providing mechanisms to retrieve data from various data sources, such as relational databases via JDBC and JNDI directories, and mechanisms to commit data back to those data sources. These mechanisms are designed in a layered, abstract approach that allows developers to think about data retrieval and commitment at a higher level than a specific data source or data source vendor.

&lt;!--  Commented out because image was deleted: [[Image:EoModeler.png|frame|right|EOModeler application icon (Mac OS X)]] --&gt;Central to this mapping is a model file (an "EOModel") that you build with a visual tool &amp;mdash; either EOModeler, or the EOModeler plug-in to [[Xcode]]. The mapping works as follows:

* Database tables are mapped to classes.
* Database columns are mapped to class attributes.
* Database rows are mapped to objects (or class instances).

You can build data models based on existing data sources or you can build data models from scratch, which you then use to create data structures (tables, columns, joins) in a data source. The result is that database records can be transposed into Java objects.

The advantage of using data models is that applications are isolated from the idiosyncrasies of the data sources they access. This separation of an application's business logic from database logic allows developers to change the database an application accesses without needing to change the application.

EOF provides a level of database transparency not seen in other tools and allows the same model to be used to access different vendor databases and even allows relationships across different vendor databases without changing source code.  

Its power comes from exposing the underlying data sources as managed graphs of persistent objects.  In simple terms, this means that it organizes the application's model layer into a set of defined in-memory data objects.  It then tracks changes to these objects and can reverse those changes on demand, such as when a user performs an undo command.  Then, when it is time to save changes to the application's data, it archives the objects to the underlying data sources.

===Using Inheritance===

In designing Enterprise Objects developers can leverage the object-oriented feature known as [[Inheritance (computer science)|inheritance]]. A Customer object and an Employee object, for example, might both inherit certain characteristics from a more generic Person object, such as name, address, and phone number. While this kind of thinking is inherent in object-oriented design, relational databases have no explicit support for inheritance. However, using Enterprise Objects, you can build data models that reflect object hierarchies. That is, you can design database tables to support inheritance by also designing enterprise objects that map to multiple tables or particular views of a database table.

==What is an Enterprise Object (EO)? ==

An Enterprise Object is analogous to what is often known in object-oriented programming as a [[Business object (computer science)|business object]] &amp;mdash; a class which models a physical or [[conceptual object]] in the business domain (e.g. a customer, an order, an item, etc.). What makes an EO different from other objects is that its instance data maps to a data store. Typically, an enterprise object contains key-value pairs that represent a row in a relational database. The key is basically the column name, and the value is what was in that row in the database. So it can be said that an EO's properties persist beyond the life of any particular running application.

More precisely, an Enterprise Object is an instance of a class that implements the com.webobjects.eocontrol.EOEnterpriseObject interface.

An Enterprise Object has a corresponding model (called an EOModel) that defines the mapping between the class's object model and the database schema. However, an enterprise object doesn't explicitly know about its model. This level of abstraction means that database vendors can be switched without it affecting the developer's code. This gives Enterprise Objects a high degree of reusability.

== EOF and Core Data ==

Despite their common origins, the two technologies diverged, with each technology retaining a subset of the features of the original Objective-C code base, while adding some new features.

=== Features Supported Only by EOF ===

EOF supports custom SQL; shared editing contexts; nested editing contexts; and pre-fetching and batch faulting of relationships, all features of the original Objective-C implementation not supported by Core Data.  Core Data also does not provide the equivalent of an EOModelGroup—the NSManagedObjectModel class provides methods for merging models from existing models, and for retrieving merged models from bundles.

=== Features Supported Only by Core Data ===

Core Data supports fetched properties; multiple configurations within a managed object model; local stores; and store aggregation (the data for a given entity may be spread across multiple stores); customization and localization of property names and validation warnings; and the use of predicates for property validation.  These features of the original Objective-C implementation are not supported by the Java implementation.

== External links ==
* [http://www.linuxjournal.com/article.php?sid=7101&amp;mode=thread&amp;order=0&amp;thold=0 article in linuxjournal about GDL2]

[[Category:Data management]]
[[Category:NeXT]]
[[Category:Apple Inc. software]]</text>
      <sha1>1l0lmu5f9v0ud3bptxs3rb9uatgy2ar</sha1>
    </revision>
  </page>
  <page>
    <title>Nested transaction</title>
    <ns>0</ns>
    <id>1867103</id>
    <revision>
      <id>762483030</id>
      <parentid>733812145</parentid>
      <timestamp>2017-01-29T02:56:50Z</timestamp>
      <contributor>
        <username>Chris the speller</username>
        <id>525927</id>
      </contributor>
      <minor />
      <comment>/* top */replaced: component based → component-based using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3169" xml:space="preserve">A '''nested transaction''' is a [[database transaction]] that is started by an instruction within the scope of an already started transaction.

Nested transactions are implemented differently in different databases. However, they have in common that the changes are not made visible to any unrelated transactions until the outermost transaction has committed. This means that a commit in an inner transaction does not necessarily persist updates to the database.

In some databases, changes made by the nested transaction are not seen by the 'host' transaction until the nested transaction is committed. According to some,{{Who|date=November 2009}} this follows from the isolation property of transactions.

The capability to handle nested transactions properly is a prerequisite for true component-based application architectures. In a component-based encapsulated architecture, nested transactions can occur without the programmer knowing it. A component function may or may not contain a database transaction (this is the encapsulated secret of the component. See [[Information hiding]]). If a call to such a component function is made inside a BEGIN - COMMIT bracket, nested transactions occur. Since popular databases like [[MySQL]]&lt;ref&gt;
{{cite web
 |url=http://dev.mysql.com/doc/refman/4.1/en/implicit-commit.html
 |title=Statements That Cause an Implicit Commit
 |author=
 |work=MySQL 4.1 Reference Manual
 |publisher=Oracle
 |accessdate=5 December 2010
}}
&lt;/ref&gt; do not allow nesting BEGIN - COMMIT brackets, a framework or a transaction monitor is needed to handle this. When we speak about nested transactions, it should be made clear that this feature is DBMS dependent and is not available for all databases.

Theory for nested transactions is similar to the theory for flat transactions.&lt;ref&gt;{{Cite journal
  | last = Resende | first = R.F. | last2 = El Abbadi | first2 = A.
  | title = On the serializability theorem for nested transactions
  | journal = Information Processing Letters | volume = 50 | issue = 4
  | pages = 177–183 | date = 1994-05-25 
  | doi = 10.1016/0020-0190(94)00033-6 }}&lt;/ref&gt;

The banking industry usually processes financial transactions using ''open nested transactions'',{{Citation needed|date=August 2015}} which is a looser variant of the nested transaction model that provides higher performance while accepting the accompanying trade-offs of inconsistency.&lt;ref&gt;{{cite journal
  | last = Weikum | first = Gerhard |author2=Hans-J. Schek
  | title = Concepts and Applications of Multilevel Transactions and Open Nested Transactions
  | journal = Database Transaction Models for Advanced Applications
  | pages = 515–553 | publisher = Morgan Kaufmann | year = 1992
  | url = http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.17.7962 | isbn = 1-55860-214-3
  | accessdate = 2007-11-13 }}&lt;/ref&gt;

==Further reading==
* Gerhard Weikum, Gottfried Vossen, ''Transactional information systems: theory, algorithms, and the practice of concurrency control and recovery'', Morgan Kaufmann, 2002, ISBN 1-55860-508-8

==References==
{{reflist}}

[[Category:Data management]]
[[Category:Transaction processing]]

{{database-stub}}</text>
      <sha1>t656gl2qtd642vt29se0ofhx9cldvqt</sha1>
    </revision>
  </page>
  <page>
    <title>Universal Data Element Framework</title>
    <ns>0</ns>
    <id>2570284</id>
    <revision>
      <id>752566784</id>
      <parentid>733648601</parentid>
      <timestamp>2016-12-02T00:18:29Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* External links */clean up; http&amp;rarr;https for [[YouTube]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9231" xml:space="preserve">The '''Universal Data Element Framework''' ('''UDEF''') provides the foundation for building an enterprise-wide [[controlled vocabulary]]. It is a standard way of indexing enterprise information that can produce big cost savings. UDEF simplifies information management through consistent classification and assignment of a global standard identifier to the data names and then relating them to similar data element concepts defined by other organizations. Though this approach is a small part of the overall picture, it is potentially a crucial enabler of semantic [[interoperability]].

== How UDEF works ==
UDEF provides semantic links, through assigning an intelligent, derived ID as an attribute of the data element, essentially labeling the element as a specific data element concept. When this UDEF ID exists in both source and target formats, it can then be used as an easy analysis point via a [[match report]], and then as the primary pivot point for transformations between source and target.

UDEF takes a list of high-level root object classes and assigns an integer to each class plus alpha characters to each specialization modifier.  It then also assigns integers to property word plus integers to each specialization modifier.  These object class alpha-integers are concatenated together with the property integers to form a dewey-decimal like code for each data element concept.

===Examples===
For the following examples, go to http://www.opengroup.org/udefinfo/htm/en_defs.htm and expand the applicable UDEF object and property trees

Assuming an application used by a hospital needs to map the data element concepts to the UDEF, the last name and first name (within UDEF you will find Family Name and Given Name under the UDEF property Name) of several people that are likely to appear on a medical record that could include the following example data element concepts –

*Patient Person Family Name – find the word “Patient” under the UDEF object “Person” and find the word “Family” under the UDEF property “Name”
*Patient Person Given Name – find the word “Patient” under the UDEF object “Person” and find the word “Given” under the UDEF property “Name”
*Doctor Person Family Name – find the word “Doctor” under the UDEF object “Person” and find the word “Family” under the UDEF property “Name”
*Doctor Person Given Name – find the word “Doctor” under the UDEF object “Person” and find the word “Given” under the UDEF property “Name”

The associated UDEF IDs for the above are derived by walking up each tree respectively and using an underscore to separate the object from the property. For the examples above, the following data element concepts are available within the current UDEF – see http://www.opengroup.org/udefinfo/htm/en_ob5.htm and http://www.opengroup.org/udefinfo/htm/en_pr10.htm

*“Patient Person Family Name” the UDEF ID is “au.5_11.10”
*“Patient Person Given Name” the UDEF ID is “au.5_12.10”
*“Doctor Person Family Name” the UDEF ID is “aq.5_11.10”
*“Doctor Person Given Name” the UDEF ID is “aq.5_12.10”

== Six basic steps to map enterprise data to the UDEF ==

There are six basic steps to follow when mapping data element concepts to the UDEF.

1. Identify the applicable UDEF property word that characterizes the dominant attribute (property) of the data element concept. For example: Name, Identifier, Date, etc.

2. Identify the dominant UDEF object word that the dominant property (selected in step 1) is describing. For example, Person_Name, Product_Identifier, Document_Date, etc.

3. By reviewing the UDEF tree for the selected property identified in step 1, identify applicable qualifiers that are necessary to describe the property word term unambiguously. For example, Family Name.

4. By reviewing the UDEF tree for the selected object identified in step 2, identify applicable qualifiers that are necessary to describe the object word term unambiguously. For example, Customer Person.

5. Concatenate the object term and the property term to create a UDEF naming convention compliant name where it is recognized that the name may seem artificially long. For example, Customer Person_Family Name.

6. Derive a structured ID based on the UDEF taxonomy that carries the UDEF inherited indexing scheme. For example &lt;CustomerPersonFamilyName UDEFID=”as.5_11.10”&gt;.

== The Open Group UDEF Project objectives ==

The UDEF Project aims to establish the Universal Data Element Framework (UDEF) as the universally-used classification system for data element concepts. It focuses on developing and maintaining the UDEF as an open standard, advocating and promoting it, putting in place a technical infrastructure to support it, implementing a Registry for it, and setting up education programs to train information professionals in its use.

Organizations that implement UDEF will likely realize the greatest benefit by defining their controlled vocabulary based on the UDEF. To help an organization manage its UDEF based controlled vocabulary, it should seriously consider a metadata registry that is based on ISO/IEC 11179-5.

== History of UDEF ==

Ron Schuldt, Sr. Enterprise Data Architect, Lockheed Martin, originated the UDEF concept based on [[ISO/IEC 11179]] Metadata standards in the early 1990s. Currently, he is a Senior Partner with Femto-Data LLC

== Ownership of UDEF intellectual property ==

The Open Group assumed from the [[Association for Enterprise Information]] (AFEI) the right to grant public use licensing of the UDEF.

The Supplier Management Council Electronic Enterprise Working Group of the [[Aerospace Industry Association]] (AIA) supports the UDEF as the naming convention solution to [[XML]] interoperability between standards that include all functions throughout a product's life-cycle and is working through a well defined process to obtain approval of this position from AIA and its member companies.

== Criticism ==

Classification in UDEF is sometimes hampered by ad hoc decisions that might produce problems. 
Example: 
* b.be.5 is "United-Kingdom Citizen Person" and
* c.be.5 is "European Union Citizen Person"
As the United Kingdom is part of the European Union, the classification is not unique.
Response:
The UDEF is flexible and is designed to match the semantics and behaviour of existing systems. Therefore, if one system has a table for United Kingdom Citizens and a different system has a table for European Union Citizens, the UDEF can handle both situations.

Some of the concepts in UDEF are not as universal as it is claimed. They show a lot of bias to Anglo-American tradition and way of thinking and are not easily transferable to other languages.
Example: The following part of the hierarchy shows the concept of an officer.

* j.5        Officer.Person
* a.j.5      Contracting.Officer.Person
* a.a.j.5    Procuring.Contracting.Officer.Person
* a.a.a.j.5  Government.Procuring.Contracting.Officer.Person
* b.a.j.5    Administrative.Contracting.Officer.Person
* b.j.5      Police.Officer.Person
* c.j.5      Military.Officer.Person
In many cultures, the part of the tree below "a.j.5 Contracting Officer Person" would not be placed under j.5 (see [[Officer (disambiguation)|officer]]) as b.j.5 (see [[Law enforcement officer]]) or c.j.5 (see [[Officer (armed forces)]]).

==See also==
* [[Data integration]]
* [[ISO/IEC 11179]]
* [[National Information Exchange Model]]
* [[Metadata]]
* [[Naming conventions (programming)]]
* [[Semantics]]
* [[Semantic web]]
* [[Semantic equivalency]]
* [[Data element]]
* [[Representation term]]
* [[Controlled vocabulary]]

== External links ==
* [http://www.opengroup.org/udefinfo/ UDEF Project of The Open Group]
* [https://www.youtube.com/embed/y6hID5qzAzQ YouTube - UDEF Tutorial, Part 1]
* [https://www.youtube.com/embed/d6dH_U8TqhY YouTube - UDEF Tutorial, Part 2]
* [http://www.opengroup.org/udefinfo/faq.htm UDEF Frequently Asked Questions]
* [https://udef-it.com/UDEF_Tools.html - Obtain Enhanced UDEF Gap Analysis Tool in English, Dutch, or French]

== Further reading ==
* {{cite book |title=UDEF - Six Steps to Cost Effective Data Integration |author=Ronald Schuldt |publisher=CreateSpace |date=November 15, 2011 |isbn=978-1-4664-6762-0 |url=https://www.createspace.com/3711806}}
* {{cite book |title=UDEF - Six Steps to Cost Effective Data Integration |author=Ronald Schuldt and Roberta Shauger |publisher=Amazon Digital Services |date=January 16, 2012 |isbn=1-4664-6762-2 |url=http://www.amazon.com/dp/B006YK6YOQ}}
* {{cite book |title=UDEF Concepts Defined - Reference Guide |author=Roberta Shauger |publisher=CreateSpace |date=December 20, 2011 |isbn=978-1-4681-1483-6 |url=https://www.createspace.com/3753707}}
* {{cite book |title=UDEF Concepts Defined - Reference Guide |author=Roberta Shauger and Ronald Schuldt |publisher=Amazon Digital Services |date=January 14, 2012 |isbn=1-4681-1483-2 |url=http://www.amazon.com/dp/B006XXMLQE}}

[[Category:Data management]]
[[Category:Interoperability]]
[[Category:Knowledge representation]]
[[Category:Metadata]]
[[Category:Open Group standards]]
[[Category:Software that uses Motif]]
[[Category:Technical communication]]</text>
      <sha1>4f9xyq8lyn2r8u7p34rz6wgw3fcaqno</sha1>
    </revision>
  </page>
  <page>
    <title>VMDS</title>
    <ns>0</ns>
    <id>3047078</id>
    <revision>
      <id>729653219</id>
      <parentid>692677501</parentid>
      <timestamp>2016-07-13T17:41:05Z</timestamp>
      <contributor>
        <username>HitroMilanese</username>
        <id>1305296</id>
      </contributor>
      <comment>Added {{[[Template:COI|COI]]}} and {{[[Template:unreferenced|unreferenced]]}} tags to article ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6574" xml:space="preserve">{{COI|date=July 2016}}
{{unreferenced|date=July 2016}}
'''VMDS''' abbreviates the relational database technology called '''Version Managed Data Store''' provided by [[GE Energy]] as part of its [[Smallworld]] technology platform and was designed from the outset to store and analyse the highly complex spatial and topological networks typically used by enterprise utilities such as power distribution and telecommunications.

VMDS was originally introduced in 1990 as has been improved and updated over the years. Its current version is 6.0.

VMDS has been designed as a [[spatial database]]. This gives VMDS a number of distinctive characteristics when compared to conventional attribute only relational databases.

==Distributed server processing==
VMDS is composed of two parts: a simple, highly scalable data block server called '''SWMFS''' (Smallworld Master File Server) and an intelligent client [[Application programming interface|API]] written in [[C (programming language)|C]] and [[Magik programming language|Magik]]. Spatial and attribute data are stored in data blocks that reside in special files called data store files on the server. When the client application requests data it has sufficient intelligence to work out the optimum set of data blocks that are required. This request is then made to SWMFS which returns the data to the client via the network for processing.

This approach is particularly efficient and scalable when dealing with spatial and topological data which tends to flow in larger volumes and require more processing then plain attribute data (for example during a map redraw operation). This approach makes VMDS well suited to enterprise deployment that might involve hundreds or even thousands of concurrent clients.

==Support for long transactions==
Relational databases support [[Database transaction|short transactions]] in which changes to data are relatively small and are brief in terms in duration (the maximum period between the start and the end of a transaction is typically a few seconds or less).

VMDS supports long transactions in which the volume of data involved in the transaction can be substantial and the duration of the transaction can be significant (days, weeks or even months). These types of transaction are common in advanced network applications used by, for example, power distribution utilities.

Due to the time span of a long transaction in this context the amount of change can be significant (not only within the scope of the transaction, but also within the context of the database as a whole). Accordingly, it is likely that the same record might be changed more than once. To cope with this scenario VMDS has inbuilt support for automatically managing such conflicts and allows applications to review changes and accept only those edits that are correct.

==Spatial and topological capabilities==
As well as conventional relational database features such as attribute querying, join fields, triggers and calculated fields, VMDS has numerous spatial and topological capabilities. This allows spatial data such as points, texts, polylines, polygons and raster data to be stored and analysed.

Spatial functions include: find all features within a polygon, calculate the [[Voronoi polygon]]s of a set of sites and perform a [[cluster analysis]] on a set of points.

Vector spatial data such as points, polylines and polygons can be given topological attributes that allow complex networks to be modelled. Network analysis engines are provided to answer questions such as find the shortest path between two nodes or how to optimize a delivery route (the [[travelling salesman problem]]). A topology engine can be configured with a set of rules that define how topological entities interact with each other when new data is added or existing data edited.

==Data abstraction==
In VMDS all data is presented to the application as objects. This is different from many relational databases that present the data as rows from a table or query result using say [[JDBC]]. VMDS provides a data modelling tool and underlying infrastructure as part of the [[Smallworld]] technology platform that allows administrators to associate a table in the database with a Magik exemplar (or class). Magik get and set methods for the Magik exemplar can be automatically generated that expose a table's field (or column). Each VMDS ''row'' manifests itself to the application as an instance of a [[Magik programming language|Magik]] object and is known as an '''RWO''' (or real world object). Tables are known as collections in Smallworld parlance.

  # all_rwos hold all the rwos in the database and is heterogeneous
  all_rwos &lt;&lt; my_application.rwo_set()
 
  # valve_collection holds the valve collection
  valves &lt;&lt; all_rwos.select(:collection, {:valve})
  number_of_valves &lt;&lt; valves.size

Queries are built up using predicate objects:

  # find 'open' valves.
  open_valves &lt;&lt; valves.select(predicate.eq(:operating_status, "open"))
  number_of_open_valves &lt;&lt; open_valves.size

  _for valve _over open_valves.elements()
  _loop
    write(valve.id)
  _endloop

Joins are implemented as methods on the parent RWO. For example a manager might have several employees who report to him:

  # get the employee collection.
  employees &lt;&lt; my_application.database.collection(:gis, :employees)

  # find a manager called 'Steve' and get the first matching element
  steve &lt;&lt; employees.select(predicate.eq(:name, "Steve").and(predicate.eq(:role, "manager")).an_element()

  # display the names of his direct reports. name is a field (or column)
  # on the employee collection (or table)
  _for employee _over steve.direct_reports.elements()
  _loop
     write(employee.name)
  _endloop

Performing a transaction:

  # each key in the hash table corresponds to the name of the field (or column) in
  # the collection (or table)
  valve_data &lt;&lt; hash_table.new_with(
    :asset_id, 57648576,
    :material, "Iron")

  # get the valve collection directly
  valve_collection &lt;&lt; my_application.database.collection(:gis, :valve)

  # create an insert transaction to insert a new valve record into the collection a
  # comment can be provide that describes the transaction
  transaction &lt;&lt; record_transaction.new_insert(valve_collection, valve_data, "Inserted a new valve")
  transaction.run()

==See also==
* [[List of relational database management systems]]
* [[List of object-relational database management systems]]
* [[Spatial database]]
* [[Multiversion concurrency control]]

[[Category:Data management]]
[[Category:GIS software]]</text>
      <sha1>1h1tqqyc5os5dy4jeeqld9rv078calq</sha1>
    </revision>
  </page>
  <page>
    <title>Control break</title>
    <ns>0</ns>
    <id>3584856</id>
    <revision>
      <id>736655742</id>
      <parentid>544248641</parentid>
      <timestamp>2016-08-29T00:10:41Z</timestamp>
      <contributor>
        <username>Peter Flass</username>
        <id>7557079</id>
      </contributor>
      <comment>fix wording, add unreferenced tag</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1160" xml:space="preserve">{{unreferenced|date=August 2016}}
{{dablink|Not to be confused with 'Control-Break' displayed in MS-DOS when cancelling an ongoing task by pressing [[break key|Ctrl+Break]] key combination.}}
In [[computer programming]] a '''control break''' is a change in the value of one of the [[Key field|key]]s on which a file is sorted which requires some extra processing.  For example, with an input file sorted by post code, the number of items found in each postal district might need to be printed on a report, and a heading shown for the next district.  Quite often there is a hierarchy of nested control breaks in a program, e.g. streets within districts within areas, with the need for a grand total at the end. [[Structured programming]] techniques have been developed to ensure correct processing of control breaks in languages such as [[COBOL]] and to ensure that conditions such as empty input files and [[sequence error]]s are handled properly.

With [[fourth generation language]]s such as [[SQL]], the programming language should handle most of the details of control breaks automatically.

[[Category:Conditional constructs]]
[[Category:Data management]]</text>
      <sha1>dgwy3ct9b3hmkf821ncyul5s5qbd0ih</sha1>
    </revision>
  </page>
  <page>
    <title>Distributed database</title>
    <ns>0</ns>
    <id>41054</id>
    <revision>
      <id>762561201</id>
      <parentid>750229994</parentid>
      <timestamp>2017-01-29T15:16:40Z</timestamp>
      <contributor>
        <ip>197.210.29.112</ip>
      </contributor>
      <comment>/* Homogeneous Distributed Databases Management System */Fixed punctuation</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="16643" xml:space="preserve">{{multiple issues|
{{refimprove|date=August 2010}}
{{Cleanup|date=June 2009}}
}}

A '''distributed database''' is a [[database]]  in which [[computer data storage|storage devices]] are not all attached to a common [[Processor (computing)|processor]].&lt;ref&gt;http://www.its.bldrdoc.gov/fs-1037/dir-012/_1750.htm&lt;/ref&gt; It may be stored in multiple [[computers]], located in the same physical location; or may be dispersed over a [[computer network|network]] of interconnected computers. Unlike [[Parallel computing|parallel systems]], in which the processors are tightly coupled and constitute a single database system, a distributed database system consists of loosely coupled sites that share no physical components.

System administrators can distribute collections of data (e.g. in a database) across multiple physical locations. A distributed database can reside on organized [[network servers]] or [[blockchain (database)|decentralized independent computers]] on the [[Internet]], on corporate [[intranets]] or [[extranets]], or on other organization [[Computer network|networks]]. Because they store data across multiple computers, distributed databases may improve performance at [[end-user]] worksites by allowing transactions to be processed on many machines, instead of being limited to one.&lt;ref name="obrien"&gt;
O'Brien, J. &amp; Marakas, G.M.(2008) Management Information Systems (pp. 185-189). New York, NY: McGraw-Hill Irwin&lt;/ref&gt;

Two processes ensure that the distributed databases remain up-to-date and current: [[Replication (computing)|replication]] and [[Data transmission|duplication]].

# Replication involves using specialized software that looks for changes in the distributive database. Once the changes have been identified, the replication process makes all the databases look the same. The replication process can be complex and time-consuming depending on the size and number of the distributed databases. This process can also require a lot of time and computer resources.  
# Duplication, on the other hand, has less complexity. It basically identifies one database as a [[master-slave (technology)|master]] and then duplicates that database. The duplication process is normally done at a set time after hours.  This is to ensure that each distributed location has the same data.  In the duplication process, users may change only the master database. This ensures that local data will not be overwritten.

Both replication and duplication can keep the data current in all distributive locations.&lt;ref name="obrien" /&gt;

Besides distributed database replication and fragmentation, there are many other distributed database design technologies. For example, local autonomy, synchronous and asynchronous distributed database technologies. These technologies' implementations can and do depend on the needs of the business and the sensitivity/[[confidentiality]] of the data stored in the database, and the price the business is willing to spend on ensuring [[data security]], [[data consistency|consistency]] and [[data integrity|integrity]].

When discussing access to distributed databases, [[Microsoft]] favors the term '''distributed query''', which it defines in protocol-specific manner as "[a]ny SELECT, INSERT, UPDATE, or DELETE statement that references tables and rowsets from one or more external OLE DB data sources".&lt;ref&gt;
{{cite web
 |url          = http://technet.microsoft.com/en-us/library/cc966484.aspx
 |title        = TechNet Glossary
 |publisher    = Microsoft
 |accessdate   = 2013-07-16
 |quote        = distributed query[:] Any SELECT, INSERT, UPDATE, or DELETE statement that references tables and rowsets from one or more external OLE DB data sources.
}}
&lt;/ref&gt;
[[Oracle Database|Oracle]] provides a more language-centric view in which distributed queries and [[distributed transaction]]s form part of '''distributed SQL'''.&lt;ref&gt;
{{cite web
 |url          = http://docs.oracle.com/cd/E11882_01/server.112/e25789/toc.htm
 |title        = Oracle Database Concepts, 11g Release 2 (11.2)
 |last1        = Ashdown
 |first1       = Lance
 |last2        = Kyte
 |first2       = Tom
 |date=September 2011
 |publisher    = Oracle Corporation
 |accessdate   = 2013-07-17
 |quote        = Distributed SQL synchronously accesses and updates data distributed among multiple databases. [...] Distributed SQL includes distributed queries and distributed transactions. 
}}
&lt;/ref&gt;

Today the distributed [[DBMS]] market is evolving dramatically, with new, innovative entrants and incumbents supporting the growing use of unstructured data and [[NoSQL]] DBMS engines, as well as [[XML database]]s and [[NewSQL|NewSQL databases]]. These databases are increasingly supporting distributed database architecture that provides [[high availability]] and [[fault tolerance]] through [[replication (computing)|replication]] and scale out ability.  Some examples are [[Aerospike database|Aerospike]],&lt;ref&gt;{{cite web|title=Aerospike distributed database|url=http://www.aerospike.com|website=Aerospike}}&lt;/ref&gt; [[Apache Cassandra|Cassandra]],&lt;ref&gt;{{cite web |url=http://cassandra.apache.org/ |title=Apache Cassandra database menagement system |publisher=Apache.org}}&lt;/ref&gt; [[Clusterpoint]],&lt;ref&gt;{{cite web |url=http://www.clusterpoint.com |title=Clusterpoint XML distributed database |publisher=Clusterpoint}}&lt;/ref&gt; [[Clustrix|ClustrixDB]],&lt;ref&gt;{{cite web|title=Frequently Asked Questions about ClustrixDB - Clustrix Documentation|url=http://docs.clustrix.com/display/CLXDOC/Frequently+Asked+Questions+about+ClustrixDB#FrequentlyAskedQuestionsaboutClustrixDB-WhatisClustrixDB?|publisher=Clustrix, Inc.}}&lt;/ref&gt; [[Couchbase Server|Couchbase]],&lt;ref&gt;{{cite web|title=Couchbase distributed database|url=http://www.couchbase.com|website=Couchbase}}&lt;/ref&gt; [[Druid (open-source data store)]],&lt;ref&gt;{{cite web |url=http://druid.io |title=Druid distributed datastore/database |publisher=The Druid Community}}&lt;/ref&gt; [[FoundationDB]],&lt;ref&gt;
{{cite web |url=https://foundationdb.com |title=FoundationDB database |publisher=FoundationDB}}&lt;/ref&gt; [[NuoDB]],&lt;ref&gt;Clark, Jack. [http://www.theregister.co.uk/2014/02/26/nuodb_funding/ "NuoDB slurps European cash for database expansion"] The Register. Feb. 26, 2014&lt;/ref&gt; [[Riak]]&lt;ref&gt;
{{cite web |url=http://www.basho.com |title=Basho Riak Distributed database |publisher=Basho}}&lt;/ref&gt; and [[OrientDB]].&lt;ref&gt;
{{cite web |url=http://www.orientdb.com |title=OrientDB database |publisher=OrientDB}}&lt;/ref&gt; The [[Blockchain (database)|blockchain]] technology popularised by [[bitcoin]] is an implementation of a distributed database.&lt;ref&gt;{{cite news|last1=Margaret|first1=Alyson|title=How Bitcoin and the blockchain are a transformative technology |url=http://blog.blockchain.com/2015/06/23/how-bitcoin-and-the-block-chain-are-a-transformative-technology/|accessdate=23 July 2015|date=23 June 2015}}&lt;/ref&gt;

== Architecture ==
A database user accesses the distributed database through:
;Local applications
:applications which do not require data from other sites.
;Global applications
:applications which do require data from other sites.

A '''homogeneous distributed database''' has identical software and hardware running all databases instances, and may appear through a single interface as if it were a single database.  A '''heterogeneous distributed database''' may have different hardware, operating systems, database management systems, and even data models for different databases.

===Homogeneous Distributed Databases Management System===
In homogeneous distributed database, all sites have identical software and are aware of each other and agree to cooperate in processing user requests. Each site surrenders part of its autonomy in terms of right to change schema or software. A homogeneous DBMS appears to the user as a single system. The homogeneous system is much easier to design and manage. The following conditions must be satisfied for homogeneous database:
*The operating system used at each location must be same or compatible.{{According to whom|date=March 2013}}{{Elucidate|date=March 2013}}
*The data structures used at each location must be same or compatible.
*The database application (or DBMS) used at each location must be same or compatible.

===Heterogeneous DDBMS===
{{See also|Heterogeneous database system}}
In a heterogeneous distributed database, different sites may use different schema and software. Difference in schema is a major problem for query processing and transaction processing. Sites may not be aware of each other and may provide only limited facilities for cooperation in transaction processing. In heterogeneous systems, different nodes may have different hardware &amp; software and data structures at various nodes or locations are also incompatible. Different computers and operating systems, database applications or data models may be used at each of the locations. For example, one location may have the latest relational database management technology, while another location may store data using conventional files or old version of database management system. Similarly, one location may have the Windows 10 operating system, while another may have UNIX. Heterogeneous systems are usually used when individual sites use their own hardware and software. On heterogeneous system, translations are required to allow communication between different sites (or DBMS). In this system, the users must be able to make requests in a database language at their local sites. Usually the SQL database language is used for this purpose. If the hardware is different, then the translation is straightforward, in which computer codes and word-length is changed. The heterogeneous system is often not technically or economically feasible. In this system, a user at one location may be able to read but not update the data at another location.

== Important considerations ==
Care with a distributed database must be taken to ensure the following:
* The distribution is transparent — users must be able to interact with the system as if it were one logical system.  This applies to the system's performance, and methods of access among other things.
* [[Database transaction|Transaction]]s are transparent — each transaction must maintain [[database integrity]] across multiple databases.  Transactions must also be divided into sub-transactions, each sub-transaction affecting one database system.

There are two principal approaches to store a relation r in a distributed database system:

:A) [[database replication|Replication]]
:B) Fragmentation/[[Partition (database)|Partitioning]]

A) Replication: In replication, the system maintains several identical replicas of the same relation r in different sites.
:* Data is more available in this scheme.
:* Parallelism is increased when read request is served.
:* Increases overhead on update operations as each site containing the replica needed to be updated in order to maintain consistency.
:* Multi-datacenter replication provides geographical diversity, like in [[Clusterpoint]]&lt;ref&gt;
{{cite web |url=http://www.clusterpoint.com/solutions/distributed-storage |title=Clusterpoint database distributed storage multi-datacenter replication|publisher=Clusterpoint}}&lt;/ref&gt; or [[Riak]].&lt;ref&gt;
{{cite web |url=http://basho.com/tag/multi-datacenter-replication/ |title=Riak database multi-datacenter replication|publisher=Basho}}&lt;/ref&gt;

B) Fragmentation: The relation r is fragmented into several relations r&lt;sub&gt;1&lt;/sub&gt;, r&lt;sub&gt;2&lt;/sub&gt;, r&lt;sub&gt;3&lt;/sub&gt;....r&lt;sub&gt;n&lt;/sub&gt; in such a way that the actual relation could be reconstructed from the fragments and then the fragments are scattered to different locations. There are basically two schemes of fragmentation:

:* Horizontal fragmentation - splits the relation by assigning each tuple of r to one or more fragments.
:* Vertical fragmentation - splits the relation by decomposing the schema R of relation r.

A distributed database can be run by independent or even competing parties as, for example, in [[bitcoin]] or [[Hasq]].

== Advantages ==
* Management of distributed data with different levels of transparency like network transparency, fragmentation transparency, replication transparency, etc.
* Increase reliability and availability
* Easier expansion
* Reflects organizational structure — database fragments potentially stored within the departments they relate to
* Local autonomy or site autonomy — a department can control the data about them (as they are the ones familiar with it)
* Protection of valuable data — if there were ever a catastrophic event such as a fire, all of the data would not be in one place, but distributed in multiple locations
* Improved performance — data is located near the site of greatest demand, and the database systems themselves are parallelized, allowing load on the databases to be balanced among servers.  (A high load on one module of the database won't affect other modules of the database in a distributed database)
* Economics — it may cost less to create a network of smaller computers with the power of a single large computer
* Modularity — systems can be modified, added and removed from the distributed database without affecting other modules (systems)
* Reliable transactions - due to replication of the database
* Hardware, operating-system, network, fragmentation, DBMS, replication and location independence
* Continuous operation, even if some nodes go offline (depending on design)
* Distributed query processing can improve performance
* Single-site failure does not affect performance of system.
* For those systems that support full distributed transactions, operations enjoy the [[ACID]] properties:
** A-atomicity, the transaction takes place as a whole or not at all
** C-consistency, maps one consistent DB state to another
** I-isolation, each transaction sees a consistent DB
** D-durability, the results of a transaction must survive system failures

The Merge Replication Method is popularly used to consolidate the data between databases.{{citation needed|date=July 2013}}

== Disadvantages ==
* Complexity — [[Database administrator|DBAs]] may have to do extra work to ensure that the distributed nature of the system is transparent.  Extra work must also be done to maintain multiple [[disparate system]]s, instead of one big one.  Extra database design work must also be done to account for the disconnected nature of the database — for example, joins become prohibitively expensive when performed across multiple systems.
* Economics — increased complexity and a more extensive infrastructure means extra labour costs
* Security — remote database fragments must be secured, and they are not centralized so the remote sites must be secured as well.  The infrastructure must also be secured (for example, by encrypting the network links between remote sites).
* Difficult to maintain integrity — but in a distributed database, enforcing integrity over a network may require too much of the network's resources to be feasible
* Inexperience — distributed databases are difficult to work with, and in such a young field there is not much readily available experience in "proper" practice
* Lack of standards — there are no tools or methodologies yet to help users convert a centralized DBMS into a distributed DBMS{{citation needed|date=July 2013}}
* Database design more complex — In addition to traditional database design challenges, the design of a distributed database has to consider fragmentation of data, allocation of fragments to specific sites and data replication
* Additional software is required
* Operating system should support distributed environment
* [[Concurrency control]] poses a major issue. It can be solved by [[Lock (database)|locking]] and [[timestamp]]ing.
* Distributed access to data
* Analysis of distributed data

==See also==
*[[Centralized database]]
*[[Data grid]]
*[[Distributed data store]]
*[[Distributed cache]]
*[[Routing protocol]]
*[[Distributed hash table]]

==References==
{{Reflist|30em}}
{{more footnotes|date=April 2013}}
*M. T. Özsu and P. Valduriez, ''Principles of Distributed Databases'' (3rd edition) (2011), Springer, ISBN 978-1-4419-8833-1
*Elmasri and Navathe, ''Fundamentals of database systems'' (3rd edition), Addison-Wesley Longman, ISBN 0-201-54263-3
*''Oracle Database Administrator's Guide 10g'' (Release 1), http://docs.oracle.com/cd/B14117_01/server.101/b10739/ds_concepts.htm

{{Databases}}

[[Category:Data management]]
[[Category:Types of databases]]
[[Category:Distributed computing architecture]]
[[Category:Applications of distributed computing]]
[[Category:Database management systems]]</text>
      <sha1>mjb296mj2vyjtxcivlo5jjra2b0mvht</sha1>
    </revision>
  </page>
  <page>
    <title>Serializability</title>
    <ns>0</ns>
    <id>4367801</id>
    <revision>
      <id>748314450</id>
      <parentid>742617910</parentid>
      <timestamp>2016-11-07T15:39:37Z</timestamp>
      <contributor>
        <ip>137.122.64.80</ip>
      </contributor>
      <comment>Typos and formatting errors</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="32740" xml:space="preserve">{{About|serializability of database transactions|serialization of objects in object-oriented languages|serialization}}

In [[concurrency control]] of [[database]]s,&lt;ref name=Bernstein87&gt;[[Phil Bernstein|Philip A. Bernstein]], Vassos Hadzilacos, Nathan Goodman (1987): [http://research.microsoft.com/en-us/people/philbe/ccontrol.aspx ''Concurrency Control and Recovery in Database Systems''] (free PDF download), Addison Wesley Publishing Company, ISBN 0-201-10715-5&lt;/ref&gt;&lt;ref name=Weikum01&gt;[[Gerhard Weikum]], Gottfried Vossen (2001): [http://www.elsevier.com/wps/find/bookdescription.cws_home/677937/description#description ''Transactional Information Systems''], Elsevier, ISBN 1-55860-508-8&lt;/ref&gt; [[transaction processing]] (transaction management), and various [[Database transaction|transactional]] applications (e.g., [[transactional memory]]&lt;ref name=Herlihy1993&gt;[[Maurice Herlihy]] and J. Eliot B. Moss. ''Transactional memory: architectural support for lock-free data structures.'' Proceedings of the 20th annual international symposium on Computer architecture (ISCA '93). Volume 21, Issue 2, May 1993.&lt;/ref&gt; and [[software transactional memory]]), both centralized and [[Distributed computing|distributed]], a transaction [[Schedule (computer science)|schedule]] is '''serializable''' if its outcome (e.g., the resulting database state) is equal to the outcome of its transactions executed serially, i.e., sequentially without overlapping in time. Transactions are normally executed concurrently (they overlap), since this is the most efficient way. Serializability is the major correctness criterion for concurrent transactions' executions. It is considered the highest level of [[isolation (computer science)|isolation]] between [[Database transaction|transactions]], and plays an essential role in [[concurrency control]]. As such it is supported in all general purpose database systems. ''[[Two-phase locking|Strong strict two-phase locking]]'' (SS2PL) is a popular serializability mechanism utilized in most of the database systems (in various variants) since their early days in the 1970s.

'''Serializability theory''' provides the formal framework to reason about and analyze serializability and its techniques. Though it is [[Mathematics|mathematical]] in nature, its fundamentals are informally (without mathematics notation) introduced below.

==Database transaction==
{{main|Database transaction}}

A '''database transaction''' is a specific intended run (with specific parameters, e.g., with transaction identification, at least) of a computer program (or programs) that accesses a database (or databases). Such a program is written with the assumption that it is running in ''isolation'' from other executing programs, i.e., when running, its accessed data (after the access) are not changed by other running programs. Without this assumption the transaction's results are unpredictable and can be wrong. The same transaction can be executed in different situations, e.g., in different times and locations, in parallel with different programs. A ''live'' transaction (i.e., exists in a computing environment with already allocated computing resources; to distinguish from a ''transaction request'', waiting to get execution resources) can be in one of three states, or phases:
#''Running'' - Its program(s) is (are) executing.
#''Ready'' - Its program's execution has ended, and it is waiting to be ''Ended (Completed)''.
#''Ended'' (or ''Completed'') - It is either ''Committed'' or ''Aborted (Rolled-back)'', depending whether the execution is considered a success or not, respectively . When committed, all its ''recoverable'' (i.e., with states that can be controlled for this purpose), ''durable'' resources (typically ''database data'') are put in their ''final'' states, states after running. When aborted, all its recoverable resources are put back in their ''initial'' states, as before running.

A failure in transaction's computing environment before ending typically results in its abort. However, a transaction may be aborted also for other reasons as well (e.g., see below).

Upon being ended (completed), transaction's allocated computing resources are released and the transaction disappears from the computing environment. However, the effects of a committed transaction remain in the database, while the effects of an aborted (rolled-back) transaction disappear from the database. The concept of ''atomic transaction'' ("all or nothing" semantics) was designed to exactly achieve this behavior, in order to control correctness in complex faulty systems.

==Correctness==

===Serializability===
'''Serializability''' is used to keep the data in the data item in a consistent state.  Serializability is a property of a transaction [[Schedule (computer science)|schedule]] (history). It relates to the ''[[Isolation (database systems)|isolation]]'' property of a [[database transaction]].
:'''Serializability''' of a schedule means equivalence (in the outcome, the database state, data values) to a ''serial schedule'' (i.e., sequential with no transaction overlap in time) with the same transactions. It is the major criterion for the correctness of concurrent transactions' schedule, and thus supported in all general purpose database systems.

:'''The rationale behind serializability''' is the following:
:If each transaction is correct by itself, i.e., meets certain integrity conditions, then a schedule that comprises any ''serial'' execution of these transactions is correct (its transactions still meet their conditions): "Serial" means that transactions do not overlap in time and cannot interfere with each other, i.e, complete ''isolation'' between each other exists. Any order of the transactions is legitimate, if no dependencies among them exist, which is assumed (see comment below). As a result, a schedule that comprises any execution (not necessarily serial) that is equivalent (in its outcome) to any serial execution of these transactions, is correct.

Schedules that are not serializable are likely to generate erroneous outcomes. Well known examples are with transactions that debit and credit accounts with money: If the related schedules are not serializable, then the total sum of money may not be preserved. Money could disappear, or be generated from nowhere. This and violations of possibly needed other [[invariant (computer science)|invariant]] preservations are caused by one transaction writing, and "stepping on" and erasing what has been written by another transaction before it has become permanent in the database. It does not happen if serializability is maintained.

If any specific order between some transactions is requested by an application, then it is enforced independently of the underlying serializability mechanisms. These mechanisms are typically indifferent to any specific order, and generate some unpredictable [[partial order]] that is typically compatible with multiple serial orders of these transactions. This partial order results from the scheduling orders of concurrent transactions' data access operations, which depend on many factors.

A major characteristic of a database transaction is ''[[Atomicity (database systems)|atomicity]]'', which means that it either ''commits'', i.e., all its operations' results take effect in the database, or ''aborts'' (rolled-back), all its operations' results do not have any effect on the database ("all or nothing" semantics of a transaction). In all real systems transactions can abort for many reasons, and serializability by itself is not sufficient for correctness. Schedules also need to possess the ''[[Schedule (computer science)#Recoverable|recoverability]]'' (from abort) property. '''Recoverability''' means that committed transactions have not read data written by aborted transactions (whose effects do not exist in the resulting database states). While serializability is currently compromised on purpose in many applications for better performance (only in cases when application's correctness is not harmed), compromising recoverability would quickly violate the database's integrity, as well as that of transactions' results external to the database. A schedule with the recoverability property (a ''recoverable'' schedule) "recovers" from aborts by itself, i.e., aborts do not harm the integrity of its committed transactions and resulting database. This is false without recoverability, where the likely integrity violations (resulting incorrect database data) need special, typically manual, corrective actions in the database.

Implementing recoverability in its general form may result in ''cascading aborts'': Aborting one transaction may result in a need to abort a second transaction, and then a third, and so on. This results in a waste of already partially executed transactions, and may result also in a performance penalty. '''[[Schedule (computer science)#Avoids cascading aborts (rollbacks)|Avoiding cascading aborts]]''' (ACA, or Cascadelessness) is a special case of recoverability that exactly prevents such phenomenon. Often in practice a special case of ACA is utilized: '''[[Schedule (computer science)#Strict|Strictness]]'''. Strictness allows an efficient database recovery from failure.

Note that the ''recoverability'' property is needed even if no database failure occurs and no database ''recovery'' from failure is needed. It is rather needed to correctly automatically handle aborts, which may be unrelated to database failure and recovery from failure.

===Relaxing serializability===

In many applications, unlike with finances, absolute correctness is not needed. For example, when retrieving a list of products according to specification, in most cases it does not matter much if a product, whose data was updated a short time ago, does not appear in the list, even if it meets the specification. It will typically appear in such a list when tried again a short time later. Commercial databases provide concurrency control with a whole range of [[isolation (computer science)#Isolation levels|isolation levels]] which are in fact (controlled) serializability violations in order to achieve higher performance. Higher performance means better transaction execution rate and shorter average transaction response time (transaction duration). ''[[Snapshot isolation]]'' is an example of a popular, widely utilized efficient relaxed serializability method with many characteristics of full serializability, but still short of some, and unfit in many situations.

Another common reason nowadays for [[Serializability#Distributed serializability|distributed serializability]] relaxation (see below) is the requirement of [[availability]] of [[internet]] products and [[Internet service provider|services]]. This requirement is typically answered by large-scale data [[Replication (computer science)|replication]]. The straightforward solution for synchronizing replicas' updates of a same database object is including all these updates in a single atomic [[distributed transaction]]. However, with many replicas such a transaction is very large, and may span several [[computer]]s and [[computer network|networks]] that some of them are likely to be unavailable. Thus such a transaction is likely to end with abort and miss its purpose.&lt;ref name=Gray1996&gt;{{cite conference
 | author = [[Jim Gray (computer scientist)|Gray, J.]]
 | coauthors = Helland, P.; [[Patrick O'Neil|O’Neil, P.]]; [[Dennis Shasha|Shasha, D.]]
 | year = 1996
 | title = The dangers of replication and a solution
 | conference = Proceedings of the 1996 [[ACM SIGMOD International Conference on Management of Data]]
 | pages = 173–182
 | url = ftp://ftp.research.microsoft.com/pub/tr/tr-96-17.pdf
 | doi = 10.1145/233269.233330
 }}&lt;/ref&gt;
Consequently, [[Optimistic replication]] (Lazy replication) is often utilized (e.g., in many products and services by [[Google]], [[Amazon.com|Amazon]], [[Yahoo]], and alike), while serializability is relaxed and compromised for [[eventual consistency]]. Again in this case, relaxation is done only for applications that are not expected to be harmed by this technique.

Classes of schedules defined by ''relaxed serializability'' properties either contain the serializability class, or are incomparable with it.

==View and conflict serializability ==

Mechanisms that enforce serializability need to execute in [[Real-time computing|real time]], or almost in real time, while transactions are running at high rates. In order to meet this requirement special cases of serializability, sufficient conditions for serializability which can be enforced effectively, are utilized.

Two major types of serializability exist: ''view-serializability'', and ''conflict-serializability''. View-serializability matches the general definition of serializability given above. Conflict-serializability is a broad special case, i.e., any schedule that is conflict-serializable is also view-serializable, but not necessarily the opposite. Conflict-serializability is widely utilized because it is easier to determine and covers a substantial portion of the view-serializable schedules. Determining view-serializability of a schedule is an [[NP-complete]] problem (a class of problems with only difficult-to-compute, excessively time-consuming known solutions).

:'''View-serializability''' of a schedule is defined by equivalence to a serial schedule (no overlapping transactions) with the same transactions, such that respective transactions in the two schedules read and write the same data values ("view" the same data values).

:'''Conflict-serializability''' is defined by equivalence to a serial schedule (no overlapping transactions) with the same transactions, such that both schedules have the same sets of respective chronologically ordered pairs of conflicting operations (same precedence relations of respective conflicting operations).

Operations upon data are ''read'' or ''write'' (a write: either ''insert'' or ''modify'' or ''delete''). Two operations are ''conflicting'', if they are of different transactions, upon the same datum (data item), and at least one of them is ''write''. Each such pair of conflicting operations has a ''conflict type'': It is either a ''read-write'', or ''write-read'', or a ''write-write'' conflict. The transaction of the second operation in the pair is said to be ''in conflict'' with the transaction of the first operation. A more general definition of conflicting operations (also for complex operations, which may consist each of several "simple" read/write operations) requires that they are [[noncommutative]] (changing their order also changes their combined result). Each such operation needs to be atomic by itself (by proper system support) in order to be considered an operation for a commutativity check. For example, read-read operations are commutative (unlike read-write and the other possibilities) and thus read-read is not a conflict. Another more complex example: the operations ''increment'' and ''decrement'' of a ''counter'' are both ''write'' operations (both modify the counter), but do not need to be considered conflicting (write-write conflict type) since they are commutative (thus increment-decrement is not a conflict; e.g., already has been supported in the old [[IBM Information Management System|IBM's IMS "fast path"]]). Only precedence (time order) in pairs of conflicting (non-commutative) operations is important when checking equivalence to a serial schedule, since different schedules consisting of the same transactions can be transformed from one to another by changing orders between different transactions' operations (different transactions' interleaving), and since changing orders of commutative operations (non-conflicting) does not change an overall operation sequence result, i.e., a schedule outcome (the outcome is preserved through order change between non-conflicting operations, but typically not when conflicting operations change order). This means that if a schedule can be transformed to any serial schedule without changing orders of conflicting operations (but changing orders of non-conflicting, while preserving operation order inside each transaction), then the outcome of both schedules is the same, and the schedule is conflict-serializable by definition.

Conflicts are the reason for blocking transactions and delays (non-materialized conflicts), or for aborting transactions due to serializability violations prevention. Both possibilities reduce performance. Thus reducing the number of conflicts, e.g., by commutativity (when possible), is a way to increase performance.

A transaction can issue/request a conflicting operation and be ''in conflict'' with another transaction while its conflicting operation is delayed and not executed (e.g., blocked by a [[Lock (computer science)|lock]]). Only executed (''materialized'') conflicting operations are relevant to ''conflict serializability'' (see more below).

==Enforcing conflict serializability==

===Testing conflict serializability===

Schedule compliance with conflict serializability can be tested with the [[precedence graph]] (''serializability graph'', ''serialization graph'', ''conflict graph'') for committed transactions of the schedule. It is the [[directed graph]] representing precedence of transactions in the schedule, as reflected by precedence of conflicting operations in the transactions.

:In the '''[[precedence graph]]'''  transactions are nodes and precedence relations are directed edges. There exists an edge from a first transaction to a second transaction, if the second transaction is ''in conflict'' with the first (see Conflict serializability above), and the conflict is '''materialized''' (i.e., if the requested conflicting operation is actually executed: in many cases a requested/issued conflicting operation by a transaction is delayed and even never executed, typically by a [[Lock (computer science)|lock]] on the operation's object, held by another transaction, or when writing to a transaction's temporary private workspace and materializing, copying to the database itself, upon commit; as long as a requested/issued conflicting operation is not executed upon the database itself, the conflict is '''non-materialized'''; non-materialized conflicts are not represented by an edge in the precedence graph).

:'''Comment:''' In many text books only ''committed transactions'' are included in the precedence graph. Here all transactions are included for convenience in later discussions.

The following observation is a '''key characterization of conflict serializability''':

:A schedule is ''conflict-serializable'' [[if and only if]] its precedence graph of ''committed transactions'' (when only ''committed'' transactions are considered) is ''[[directed acyclic graph|acyclic]]''. This means that a cycle consisting of committed transactions only is generated in the (general) precedence graph, if and only if conflict-serializability is violated.

Cycles of committed transactions can be prevented by aborting an ''undecided'' (neither committed, nor aborted) transaction on each cycle in the precedence graph of all the transactions, which can otherwise turn into a cycle of committed transactions (and a committed transaction cannot be aborted). One transaction aborted per cycle is both required and sufficient number to break and eliminate the cycle (more aborts are possible, and can happen in some mechanisms, but unnecessary for serializability). The probability of cycle generation is typically low, but nevertheless, such a situation is carefully handled, typically with a considerable overhead, since correctness is involved. Transactions aborted due to serializability violation prevention are ''restarted'' and executed again immediately.

Serializability enforcing mechanisms typically do not maintain a precedence graph as a data structure, but rather prevent or break cycles implicitly (e.g., SS2PL below).

===Common mechanism - SS2PL===
{{main|Two-phase locking}}

''Strong strict two phase locking'' (SS2PL) is a common mechanism utilized in database systems since their early days in the 1970s (the "SS" in the name SS2PL is newer though) to enforce both conflict serializability and ''[[Schedule (computer science)#Strict|strictness]]'' (a special case of recoverability which allows effective database recovery from failure) of a schedule. In this mechanism each datum is locked by a transaction before accessing it (any read or write operation): The item is marked by, associated with a ''[[lock (computer science)|lock]]'' of a certain type, depending on operation (and the specific implementation; various models with different lock types exist; in some models locks may change type during the transaction's life). As a result, access by another transaction may be blocked, typically upon a conflict (the lock delays or completely prevents the conflict from being materialized and be reflected in the precedence graph by blocking the conflicting operation), depending on lock type and the other transaction's access operation type. Employing an SS2PL mechanism means that all locks on data on behalf of a transaction are released only after the transaction has ended (either committed or aborted).

SS2PL is the name of the resulting schedule property as well, which is also called ''rigorousness''. SS2PL is a special case ([[proper subset]]) of [[Two-phase locking]] (2PL)

Mutual blocking between transactions results in a ''deadlock'', where execution of these transactions is stalled, and no completion can be reached. Thus deadlocks need to be resolved to complete these transactions' execution and release related computing resources. A deadlock is a reflection of a potential cycle in the precedence graph, that would occur without the blocking when conflicts are materialized. A deadlock is resolved by aborting a transaction involved with such potential cycle, and breaking the cycle. It is often detected using a ''[[wait-for graph]]'' (a graph of conflicts blocked by locks from being materialized; it can be also defined as the graph of non-materialized conflicts; conflicts not materialized are not reflected in the precedence graph and do not affect serializability), which indicates which transaction is "waiting for" lock release by which transaction, and a cycle means a deadlock. Aborting one transaction per cycle is sufficient to break the cycle. Transactions aborted due to deadlock resolution are ''restarted'' and executed again immediately.

===Other enforcing techniques===

Other known mechanisms include:
* [[Precedence graph]] (or Serializability graph, Conflict graph) cycle elimination
* [[Two-phase locking]] (2PL)
* [[Timestamp-based concurrency control|Timestamp ordering]] (TO)
* [[Snapshot isolation#Making Snapshot Isolation Serializable|Serializable snapshot isolation]]&lt;ref name=Cahill08&gt;Michael J. Cahill, Uwe Röhm, Alan D. Fekete (2008): [http://portal.acm.org/citation.cfm?id=1376690  "Serializable isolation for snapshot databases"], ''Proceedings of the 2008 ACM SIGMOD international conference on Management of data'', pp. 729-738, Vancouver, Canada, June 2008, ISBN 978-1-60558-102-6 (SIGMOD 2008 best paper award)&lt;/ref&gt; (SerializableSI)

The above (conflict) serializability techniques in their general form do not provide recoverability. Special enhancements are needed for adding recoverability.

====Optimistic versus pessimistic techniques====

Concurrency control techniques are of three major types:
# ''Pessimistic'': In Pessimistic concurrency control a transaction blocks data access operations of other transactions upon conflicts, and conflicts are ''non-materialized'' until blocking is removed. This is done to ensure that operations that may violate serializability (and in practice also recoverability) do not occur.
# ''Optimistic'': In [[Optimistic concurrency control]] data access operations of other transactions are not blocked upon conflicts, and conflicts are immediately ''materialized''. When the transaction reaches the ''ready'' state, i.e., its ''running'' state has been completed, possible serializability (and in practice also recoverability) violation by the transaction's operations (relatively to other running transactions) is checked: If violation has occurred, the transaction is typically ''aborted'' (sometimes aborting ''another'' transaction to handle serializability violation is preferred). Otherwise it is ''committed''.
# ''Semi-optimistic'': Mechanisms that mix blocking in certain situations with not blocking in other situations and employ both materialized and non-materialized conflicts

The main differences between the technique types is the conflict types that are generated by them. A pessimistic method blocks a transaction operation upon conflict and generates a non-materialized conflict, while an optimistic method does not block and generates a materialized conflict. A semi-optimistic method generates both conflict types. Both conflict types are generated by the chronological orders in which transaction operations are invoked, independently of the type of conflict. A cycle of committed transactions (with materialized conflicts) in the ''[[precedence graph]]'' (conflict graph) represents a serializability violation, and should be avoided for maintaining serializability. A cycle of (non-materialized) conflicts in the ''[[wait-for graph]]'' represents a deadlock situation, which should be resolved by breaking the cycle. Both cycle types result from conflicts, and should be broken. At any technique type conflicts should be detected and considered, with similar overhead for both materialized and non-materialized conflicts (typically by using mechanisms like locking, while either blocking for locks, or not blocking but recording conflict for materialized conflicts). In a blocking method typically a [[context switch]]ing occurs upon conflict, with (additional) incurred overhead. Otherwise blocked transactions' related computing resources remain idle, unutilized, which may be a worse alternative. When conflicts do not occur frequently, optimistic methods typically have an advantage. With different transactions loads (mixes of transaction types) one technique type (i.e., either optimistic or pessimistic) may provide better performance than the other.

Unless schedule classes are ''inherently blocking'' (i.e., they cannot be implemented without data-access operations blocking; e.g., 2PL, SS2PL and SCO above; see chart), they can be implemented also using optimistic techniques (e.g., Serializability, Recoverability).

====Serializable multi-version concurrency control====

:See also [[Multiversion concurrency control]] (partial coverage)
:and [[Snapshot isolation#Serializable Snapshot Isolation|Serializable_Snapshot_Isolation]] in [[Snapshot isolation]]

'''Multi-version concurrency control''' (MVCC) is a common way today to increase concurrency and performance by generating a new version of a database object each time the object is written, and allowing transactions' read operations of several last relevant versions (of each object), depending on scheduling method. MVCC can be combined with all the serializability techniques listed above (except SerializableSI which is originally MVCC based). It is utilized in most general-purpose DBMS products.

MVCC is especially popular nowadays through the ''relaxed serializability'' (see above) method ''[[Snapshot isolation]]'' (SI) which provides better performance than most known serializability mechanisms (at the cost of possible serializability violation in certain cases). [[Snapshot isolation#Making Snapshot Isolation Serializable|SerializableSI]], which is an efficient enhancement of SI to make it serializable, is intended to provide an efficient serializable solution. [[Snapshot isolation#Making Snapshot Isolation Serializable|SerializableSI has been analyzed]]&lt;ref name=Cahill08/&gt;&lt;ref name=fekete2009&gt;Alan Fekete (2009), [http://www.it.usyd.edu.au/~fekete/teaching/serializableSI-Fekete.pdf "Snapshot Isolation and Serializable Execution"], Presentation, Page 4, 2009, The university of Sydney (Australia). Retrieved 16 September 2009&lt;/ref&gt; via a general theory of MVCC

==Distributed serializability==

===Overview===

'''Distributed serializability''' is the serializability of a schedule of a transactional [[distributed system]] (e.g., a [[distributed database]] system). Such system is characterized by ''[[distributed transaction]]s'' (also called ''global transactions''), i.e., transactions that span computer processes (a process abstraction in a general sense, depending on computing environment; e.g., [[operating system]]'s [[Thread (computer science)|thread]]) and possibly network nodes. A distributed transaction comprises more than one ''local sub-transactions'' that each has states as described above for a [[Serializability#Database transaction|database transaction]]. A local sub-transaction comprises a single process, or more processes that typically fail together (e.g., in a single [[processor core]]). Distributed transactions imply a need in [[Atomic commit]] protocol to reach consensus among its local sub-transactions on whether to commit or abort. Such protocols can vary from a simple (one-phase) hand-shake among processes that fail together, to more sophisticated protocols, like [[Two-phase commit]], to handle more complicated cases of failure (e.g., process, node, communication, etc. failure). Distributed serializability is a major goal of [[distributed concurrency control]] for correctness. With the proliferation of the [[Internet]], [[Cloud computing]], [[Grid computing]], and small, portable, powerful computing devices (e.g., [[smartphone]]s) the need for effective distributed serializability techniques to ensure correctness in and among distributed applications seems to increase.

Distributed serializability is achieved by implementing distributed versions of the known centralized techniques.&lt;ref name=Bernstein87 /&gt;&lt;ref name=Weikum01 /&gt; Typically all such distributed versions require utilizing conflict information (either of materialized or non-materialized conflicts, or equivalently, transaction precedence or blocking information; conflict serializability is usually utilized) that is not generated locally, but rather in different processes, and remote locations. Thus information distribution is needed (e.g., precedence relations, lock information, timestamps, or tickets). When the distributed system is of a relatively small scale, and message delays across the system are small, the centralized concurrency control methods can be used unchanged, while certain processes or nodes in the system manage the related algorithms. However, in a large-scale system (e.g., ''Grid'' and ''Cloud''), due to the distribution of such information, substantial performance penalty is typically incurred, even when distributed versions of the methods (Vs. centralized) are used, primarily due to computer and communication [[latency (engineering)|latency]]. Also, when such information is distributed, related techniques typically do not scale well. A well-known example with scalability problems is a [[distributed lock manager]], which distributes lock (non-materialized conflict) information across the distributed system to implement locking techniques.

==See also==

*[[Two-phase locking|Strong strict two-phase locking]] (SS2PL or Rigorousness).
*[[Snapshot isolation#Making Snapshot Isolation Serializable|Making snapshot isolation serializable]]&lt;ref name=Cahill08 /&gt; in [[Snapshot isolation]].
*[[Global serializability]], where the ''Global serializability problem'' and its proposed solutions are described.
* [[Linearizability]], a more general concept in [[concurrent computing]]

==Notes==
{{reflist}}

==References==
{{more footnotes|date=November 2011}}
&lt;!--Supposed sources for most of the material about centralized (vs. distributed) serializability:--&gt;
*[[Phil Bernstein|Philip A. Bernstein]], Vassos Hadzilacos, Nathan Goodman (1987): [http://research.microsoft.com/en-us/people/philbe/ccontrol.aspx ''Concurrency Control and Recovery in Database Systems''], Addison Wesley Publishing Company, ISBN 0-201-10715-5
*[[Gerhard Weikum]], Gottfried Vossen (2001): [http://www.elsevier.com/wps/find/bookdescription.cws_home/677937/description#description ''Transactional Information Systems''], Elsevier, ISBN 1-55860-508-8

[[Category:Data management]]
[[Category:Databases]]
[[Category:Concurrency control]]
[[Category:Transaction processing]]
[[Category:Distributed computing problems]]

[[el:Σειριοποιησιμότητα Συγκρούσεων]]</text>
      <sha1>g4ugf2nm2acc7dl596e3t5q480d9ixb</sha1>
    </revision>
  </page>
  <page>
    <title>Enterprise information integration</title>
    <ns>0</ns>
    <id>773166</id>
    <revision>
      <id>757143791</id>
      <parentid>757143029</parentid>
      <timestamp>2016-12-29T01:35:08Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <comment>cols, integrate/rm items linked already, combine tags</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7242" xml:space="preserve">{{multiple issues|{{refimprove|date=February 2015}}
{{POV|date=February 2011}}}}
'''Enterprise information integration''' ('''EII''') is the ability to support a unified view of data and information for an entire organization.  In a [[data virtualization]] application of EII, a process of [[information integration]], using [[data abstraction]] to provide a unified interface (known as [[uniform data access]]) for viewing all the data within an organization, and a single set of structures and naming conventions (known as [[uniform information representation]]) to represent this data; the goal of EII is to get a large set of [[heterogeneous]] data sources to appear to a user or system as a single, homogeneous data source.

== Overview ==
[[Data]] within an [[Enterprise architecture|enterprise]] can be stored in heterogeneous formats, including [[relational database]]s (which themselves come in a large number of varieties), text files, [[XML]] files, [[spreadsheet]]s and a variety of proprietary [[data storage device|storage]] methods, each with their own [[index (database)|index]]ing and [[data access]] methods.

Standardized data access [[application programming interface|API]]s have emerged, that offer a specific set of commands to retrieve and modify data from a generic data source. Many applications exist that implement these APIs' commands across various data sources, most notably relational databases. Such APIs include [[ODBC]], [[JDBC]], [[XQJ]], [[OLE DB]], and more recently [[ADO.NET]].

There are also standard formats for representing data within a file, that are very important to information integration. The best-known of these is XML, which has emerged as a standard universal representation format. There are also more specific XML "grammars" defined for specific types of data, such as [[Geography Markup Language]] for expressing geographical features, and [[Directory Service Markup Language]], for holding directory-style information. In addition, non-XML standard formats exist, such as [[iCalendar]], for representing calendar information, and [[vCard]], for [[business card]] information.

Enterprise Information Integration (EII) applies [[data integration]] commercially.  Despite the theoretical problems described above, the private sector shows more concern with the problems of data integration as a viable product.&lt;ref name="refthree"&gt;{{cite conference | author=Alon Y. Halevy | title=Enterprise information integration: successes, challenges and controversies | booktitle=SIGMOD 2005 | year=2005 | pages=778–787 | url=http://www.cs.washington.edu/homes/alon/files/eiisigmod05.pdf|display-authors=etal}}&lt;/ref&gt;
EII emphasizes neither on correctness nor tractability, but speed and simplicity. An EII industry has emerged, but many professionals{{Who|date=June 2009}} believe it does not perform to its full potential.  Practitioners cite the following major issues which  EII must address for the industry to become mature:{{Citation needed|date=June 2009}}

; Combining disparate data sets : Each data source is disparate and as such is not designed to support EII.  Therefore, data virtualization as well as [[Federated database system|data federation]] depends upon accidental data commonality to support combining data and information from disparate data sets.  Because of this lack of data value commonality across data sources, the return set may be inaccurate, incomplete, and impossible to validate.

:  One solution is to recast disparate databases to integrate these databases without the need for [[Extract, transform, load|ETL]]. The recast databases support commonality constraints where referential integrity may be enforced between databases.  The recast databases provide designed data access paths with data value commonality across databases.
; Simplicity of understanding : Answering queries with views arouses interest from a theoretical standpoint, but difficulties in understanding how to incorporate it as an "enterprise solution".{{Citation needed|date=June 2009}}  Some developers{{Who|date=June 2009}} believe it should be merged with [[Enterprise application integration|EAI]].  Others{{Who|date=June 2009}} believe it should be incorporated with ETL systems, citing customers' confusion over the differences between the two services.{{Citation needed|date=June 2009}}
; Simplicity of deployment : Even if recognized as a solution to a problem, EII {{as of | 2009 | lc = on}} currently takes time to apply and offers complexities in deployment.  People have proposed a variety of schema-less solutions such as "Lean Middleware",&lt;ref name="reffour"&gt;{{cite conference | author=David A. Maluf | title=Lean middleware | booktitle=SIGMOD 2005 | year=2005 | pages=788–791 | url=http://portal.acm.org/citation.cfm?id=1066157.1066247&amp;coll=portal&amp;dl=ACM&amp;type=series&amp;idx=1066157&amp;part=Proceedings&amp;WantType=Proceedings&amp;title=International%20Conference%20on%20Management%20of%20Data&amp;CFID=15151515&amp;CFTOKEN=6184618|display-authors=etal}}&lt;/ref&gt; but ease-of-use and speed of employment appear inversely proportional to the generality of such systems.{{Citation needed|date=June 2009}}  Others{{Who|date=June 2009}} cite the need for standard data interfaces to speed and simplify the integration process in practice.
; Handling higher-order information : Analysts experience difficulty—even with a functioning information integration system—in determining whether the sources in the database will  satisfy a given application.  Answering these kinds of questions about a set of repositories requires semantic information like [[metadata]] and/or ontologies.  The few commercial tools{{Which|date=June 2009}} that leverage this information remain in their infancy.

== Applications ==
EII products enable [[loose coupling]] between [[wiktionary:Homogeneous|homogeneous]]-data consuming client applications and services and heterogeneous-data stores.  Such client applications and services include Desktop Productivity Tools (spreadsheets, [[word processor]]s, presentation software, etc.), [[Integrated development environment|development environment]]s and [[Software framework|framework]]s ([[Java EE]], [[Microsoft .NET|.NET]], [[Mono (software)|Mono]], [[SOAP]] or [[Representational State Transfer|REST]]ful [[Web service]]s, etc.), [[business intelligence]] (BI), [[business activity monitoring]] (BAM) software, [[enterprise resource planning]] (ERP), [[Customer relationship management]] (CRM), [[business process management]] (BPM and/or BPEL) Software, and [[web content management]] (CMS).

==  Example technology vendors ==
*[[Capsenta]]
*[[Composite Software]]
*[[Denodo]]
*[[MetaMatrix]]
*[[XAware]]

== Data access technologies ==
*[[XQuery]] and [[XQuery API for Java]]
*[[Service Data Objects]] (SDO) for Java, C++ and .Net clients and any type of data source

== See also ==
{{div col|3}}
* [[Business Intelligence 2.0]] (BI 2.0)
* [[Data warehouse]]
* [[Disparate system]]
* [[Enterprise integration]]
* [[Federated database system]]
* [[Resource Description Framework]]
* [[Semantic heterogeneity]]
* [[Semantic integration]]
* [[Semantic Web]]
* [[Web 2.0]]
* [[Web services]]
{{div col end}}

==References==

&lt;references/&gt;

[[Category:Data management]]</text>
      <sha1>9pyzz7i9s3j77airwg93yyzcpcku5ru</sha1>
    </revision>
  </page>
  <page>
    <title>Three-phase commit protocol</title>
    <ns>0</ns>
    <id>2044655</id>
    <revision>
      <id>758801696</id>
      <parentid>708444010</parentid>
      <timestamp>2017-01-07T17:41:47Z</timestamp>
      <contributor>
        <username>Animaala</username>
        <id>30048422</id>
      </contributor>
      <comment>/* Coordinator */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5398" xml:space="preserve">In [[computer networking]] and [[database]]s, the '''three-phase commit protocol''' ('''3PC''')&lt;ref name=3PC&gt;{{cite journal
 | last = Skeen
 | first = Dale
 | title = A Formal Model of Crash Recovery in a Distributed System
 | journal = IEEE Transactions on Software Engineering
 | volume = 9
 | issue = 3
 |date=May 1983
 | pages = 219–228
 | doi = 10.1109/TSE.1983.236608
 | last2 = Stonebraker
 | first2 = M.
}}&lt;/ref&gt; is a [[distributed algorithm]] which lets all nodes in a [[distributed system]] agree to [[Commit (data management)|commit]] a [[database transaction|transaction]].  Unlike the [[two-phase commit protocol]] (2PC) however, 3PC is non-blocking.  Specifically, 3PC places an upper bound on the amount of time required before a transaction either commits or [[Abort (computing)|aborts]].  This property ensures that if a given transaction is attempting to commit via 3PC and holds some [[lock (computer science)|resource locks]], it will release the locks after the timeout.

==Protocol Description==
In describing the protocol, we use terminology similar to that used in the [[two-phase commit protocol]].  Thus we have a single coordinator site leading the transaction and a set of one or more cohorts being directed by the coordinator.

&lt;center&gt;[[Image:Three-phase commit diagram.png]]&lt;/center&gt;

===Coordinator===
#The coordinator receives a transaction request.  If there is a failure at this point, the coordinator aborts the transaction (i.e. upon recovery, it will consider the transaction aborted).  Otherwise, the coordinator sends a canCommit? message to the cohorts and moves to the waiting state.
#If there is a failure, timeout, or if the coordinator receives a No message in the waiting state, the coordinator aborts the transaction and sends an abort message to all cohorts.  Otherwise the coordinator will receive Yes messages from all cohorts within the time window, so it sends preCommit messages to all cohorts and moves to the prepared state.
#If the coordinator succeeds in the prepared state, it will move to the commit state.  However if the coordinator times out while waiting for an acknowledgement from a cohort, it will abort the transaction.  In the case where an acknowledgement is received from the majority of cohorts, the coordinator moves to the commit state as well.

===Cohort===
#The cohort receives a canCommit? message from the coordinator.  If the cohort agrees it sends a Yes message to the coordinator and moves to the prepared state.  Otherwise it sends a No message and aborts.  If there is a failure, it moves to the abort state.
#In the prepared state, if the cohort receives an abort message from the coordinator, fails, or times out waiting for a commit, it aborts.  If the cohort receives a  preCommit message, it sends an '''[[acknowledgement (data networks)|ACK]]''' message back and awaits a final commit or abort.
#If, after a cohort member receives a  preCommit  message, the coordinator fails or times out, the cohort member goes forward with the commit.

==Motivation==
A [[Two-phase commit protocol]] cannot dependably recover from a failure of both the coordinator and a cohort member during the '''Commit phase'''.  If only the coordinator had failed, and no cohort members had received a commit message, it could safely be inferred that
no commit had happened.  If, however, both the coordinator and a cohort member
failed, it is possible that the failed cohort member was the first to be notified, and had
actually done the commit.  Even if a new coordinator is selected, it cannot 
confidently proceed with the operation until it has received an agreement from
all cohort members ... and hence must block until all cohort members respond.

The Three-phase commit protocol eliminates this problem by introducing the Prepared to commit
state.  If the coordinator fails before sending preCommit messages, the cohort will
unanimously agree that the operation was aborted.  The coordinator will not send out a doCommit
message until all cohort members have '''ACK'''ed that they are '''Prepared to commit'''. 
This eliminates the possibility that any cohort member actually completed the 
transaction before all cohort members were aware of the decision to do so 
(an ambiguity that necessitated indefinite blocking in the [[Two-phase commit protocol]]).

==Disadvantages==
The main disadvantage to this algorithm is that it cannot recover in the event the network is segmented in any manner. The original 3PC algorithm assumes a fail-stop model, where processes fail by crashing and crashes can be
accurately detected, and does not work with network partitions or asynchronous communication.

Keidar and Dolev's E3PC&lt;ref name=E3PC&gt;{{cite journal|last=Keidar|first=Idit|author2=Danny Dolev |title=Increasing the Resilience of Distributed and Replicated Database Systems|journal=Journal of Computer and System Sciences (JCSS)|volume=57|issue=3|date=December 1998|pages=309–324|
url=http://webee.technion.ac.il/~idish/Abstracts/jcss.html|doi=10.1006/jcss.1998.1566}}&lt;/ref&gt; algorithm eliminates this disadvantage.

The protocol requires at least 3 round trips to complete, needing a minimum of 3 round trip times (RTTs). This is potentially a long latency to complete each transaction.

==References==
{{Reflist}}

==See also==
*[[Two-phase commit protocol]]

[[Category:Data management]]
[[Category:Transaction processing]]</text>
      <sha1>cp52nbp3b3kv308nle4anq4s4isqktr</sha1>
    </revision>
  </page>
  <page>
    <title>Content format</title>
    <ns>0</ns>
    <id>7428842</id>
    <revision>
      <id>727805879</id>
      <parentid>714242064</parentid>
      <timestamp>2016-07-01T10:36:31Z</timestamp>
      <contributor>
        <ip>189.222.175.222</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5457" xml:space="preserve">[[File:Pcm.svg|200px|thumb|right|Graphical representations of electrical data: analog audio content format (red), 4-bit digital pulse code modulated content format (blue).]][[File:Mi Fu-On Calligraphy.jpg|200px|thumb|right|Chinese calligraphy written in a language content format by [[Song Dynasty]] (A.D. 1051-1108) poet [[Mi Fu]].]][[File:12345678901-2-23456 barcode UPC(A).svg|200px|thumb|A series of numbers encoded in a Universal Product Code digital numeric content format.]]A '''content format''' is an [[encoding|encoded]] format for converting a specific type of [[data]] to displayable [[information]]. [[Content (media and publishing)|Content]] formats are used in [[recording]] and [[Telecommunication|transmission]] to prepare data for [[Information processing|observation]] or [[interpreting|interpretation]].&lt;ref&gt;Bob Boiko, ''Content Management Bible,'' Nov 2004 pp:79, 240, 830&lt;/ref&gt;&lt;ref&gt;[[Ann Rockley]], ''Managing Enterprise Content: A Unified Content Strategy,'' Oct 2002 pp:269, 320, 516&lt;/ref&gt; This includes both [[Analog signal|analog]] and [[digitizing|digitized]] content. Content formats may be recorded and read by either natural or manufactured tools and mechanisms.

In addition to converting data to information, a content format may include the [[encryption]] and/or [[Scrambler|scrambling]] of that information.&lt;ref&gt;Jessica Keyes, ''Technology Trendlines,'' Jul  1995 pp:201&lt;/ref&gt; Multiple content formats may be contained within a single section of a [[storage medium]] (e.g. [[Multitrack recording|track]], [[disk sector]], [[computer file]], [[document]], [[page (paper)|page]], [[Column (typography)|column]]) or transmitted via a single [[Channel (communications)|channel]] (e.g. [[wire]], [[carrier wave]]) of a [[transmission medium]]. With [[multimedia]], multiple tracks containing multiple content formats are presented simultaneously. Content formats may either be recorded in secondary signal processing methods such as a software container format (e.g. [[digital audio]], [[digital video]]) or recorded in the primary format (e.g. [[spectrogram]], [[pictogram]]). 

Observable data is often known as [[raw data]], or raw content.&lt;ref&gt;Oge Marques and Borko Furht, ''Content-Based Image and Video Retrieval,'' April 2002 pp:15&lt;/ref&gt; A primary raw content format may be directly [[information processing|observable]] (e.g. [[image]], [[sound]], [[Motion (physics)|motion]], [[Odor|smell]], [[Haptic perception|sensation]]) or [[physics|physical]] data which only requires hardware to display it, such as a [[phonograph]]ic [[Gramophone needle|needle]] and [[diaphragm (acoustics)|diaphragm]] or a [[Image projector|projector]] [[List of light sources|lamp]] and [[magnifying glass]].

There has been a countless number of content formats throughout history. The following are examples of some common content formats and content format categories (covering: sensory experience, model, and language used for encoding information):
{| width="65%"
|- valign=top
|width="50%"|
*Audio data encoding&lt;ref&gt;David Austerberry, ''The Technology of Video and Audio Streaming, Second Edition,'' Sep 2004 pp: 328&lt;/ref&gt;
**[[Audio coding format]]
**[[Analog signal|Analog]] [[Audio frequency|audio]] data
**[[Stereophonic sound]] formats
**[[Digital audio]] data
**[[Synthesizer]] [[Music sequencer|sequences]]
*Visual data encoding
**[[Art techniques and materials|Hand rendering materials]]
**[[Film speed]] formats
**[[Pixel]] [[coordinates]] data
**[[Color space]] data
**[[Vector graphics|Vector graphic]] [[coordinates]]/[[dimensions]]
**[[Texture mapping]] formats
**[[3D display]] formats
**[[Holographic]] formats
**[[Display resolution]] formatting
*[[Motion graphics]] encoding
**[[Video coding format]]
**[[Frame rate]] data
**[[Video]] data&lt;ref&gt;M. Ghanbari, ''Standard Codecs: Image Compression to Advanced Video Coding,'' Jun 2003 pp:364&lt;/ref&gt;
**[[Computer animation]] formats
*Instruction encoding
**[[Musical notation]]
**[[Computer language]]
**[[Traffic signals]]
|width="50%"|
*[[Language|Natural languages formats]]
**[[Writing system]]s
**[[Phonetic]]
**[[Sign language]]s
*[[Signal (electronics)|Communication signaling formats]]
*[[Code]] formats
*Expert language formats
**[[Graphic organizer]]
**[[Statistical model]]
**[[Table of elements#Standard periodic table|Table of elements]]
**[[DNA sequence]]
**[[Human anatomy]]
**[[Biometrics|Biometric data]]
**[[Chemical formula]]s
**[[Aroma compound]]
**[[Psychoactive drug#Psychoactive drug chart|Drug chart]]
**[[Electromagnetic spectrum]]
**[[Time standard]]
**[[Numerical weather prediction]]
**[[Capital asset pricing model]]
**[[Measures of national income and output|National income and output]]
**[[Celestial coordinate system]]
**[[APP-6a|Military mapping]]
**[[Geographic information system]]
**[[Interstate Highway System]]
|}

==See also==
*[[Communication]]
*[[Representation (arts)]]
*[[Modulation|Content carrier signals]]
*[[Multiplexing|Content multiplexing format]]
*[[Transmission (telecommunications)|Content transmission]]
*[[Wireless|Wireless content transmission]]
*Data storage device
*[[Recording format]]
*[[Encoder]]
*[[Analog television]]: [[NTSC]], [[PAL]] and [[SECAM]]
*[[Information mapping]]

==References==
{{reflist}}

[[Category:Communication]]
[[Category:Media technology]]
[[Category:Data management]]
[[Category:Recording]]
[[Category:Film and video technology]]
[[Category:Sound production technology]]


{{library-stub}}</text>
      <sha1>4yqrpujw4loiubwx4uhsjdj49s703b6</sha1>
    </revision>
  </page>
  <page>
    <title>Content management</title>
    <ns>0</ns>
    <id>223240</id>
    <revision>
      <id>744979862</id>
      <parentid>744971461</parentid>
      <timestamp>2016-10-18T16:20:32Z</timestamp>
      <contributor>
        <username>Biogeographist</username>
        <id>18201938</id>
      </contributor>
      <comment>/* See also */ [[Snippet management]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11862" xml:space="preserve">{{distinguish|Information management|Knowledge management}}
{{Refimprove|date=July 2007}}

{{Business administration}}
'''Content management''' ('''CM'''), is a set of processes and technologies that supports the collection, managing, and publishing of information in any form or medium.  When stored and accessed via computers, this information may be more specifically referred to as [[digital content]], or simply as [[Content (media and publishing)|content]].  Digital content may take the form of text (such as [[electronic document]]s), multimedia files (such as audio or video files), or any other file type that follows a content lifecycle requiring [[Product lifecycle management|management]]. The process is complex enough to manage that several large and small commercial software vendors such as [[Interwoven]] and [[Microsoft]] offer [[Content management system|content management software]] to control and automate significant aspects of the content lifecycle.

==The process of content management==
Content management practices and goals vary by mission and by organizational governance structure.
News organizations, [[e-commerce]] websites, and educational institutions all use content management, but in different ways. This leads to differences in terminology and in the names and number of steps in the process.

For example, some digital content is created by one or more authors. Over time that content may be edited. One or more individuals may provide some editorial oversight, approving the content for publication. Publishing may take many forms: it may be the act of "pushing" content out to others, or simply granting digital access rights to certain content to one or more individuals.  Later that content may be superseded by another version of the content and thus retired or removed from use (as when this wiki page is modified).

Content management is an inherently collaborative process. It often consists of the following basic roles and responsibilities:

* '''Creator''' – responsible for creating and editing content.
* '''Editor''' – responsible for tuning the content message and the style of delivery, including translation and localization.
* '''Publisher''' – responsible for releasing the content for use.
* '''Administrator''' – responsible for managing access permissions to folders and files, usually accomplished by assigning access rights to user groups or roles. Admins may also assist and support users in various ways.
* '''Consumer, viewer or guest''' – the person who reads or otherwise takes in content after it is published or shared.

A critical aspect of content management is the ability to manage versions of content as it evolves (''see also'' [[version control]]). Authors and editors often need to restore older versions of edited products due to a process failure or an undesirable series of edits.

Another equally important aspect of content management involves the creation, maintenance, and application of review standards. Each member of the content creation and review process has a unique role and set of responsibilities in the development or publication of the content. Each review team member requires clear and concise review standards. These must be maintained on an ongoing basis to ensure the long-term consistency and health of the [[knowledge base]].

A content management system is a set of automated processes that may support the following features:

* Import and creation of documents and multimedia material
* Identification of all key users and their roles
* The ability to assign roles and responsibilities to different instances of content categories or types
* Definition of workflow tasks often coupled with messaging so that content managers are alerted to changes in content
* The ability to track and manage multiple versions of a single instance of content
* The ability to publish the content to a repository to support access
* The ability to personalize content based on a set of rules
* 
Increasingly, the repository is an inherent part of the system, and incorporates [[enterprise search]] and retrieval. Content management systems take the following forms:

* [[Web content management system]]—[[software]] for [[web site]] management (often what ''content management'' implicitly means)
* Output of a [[newspaper]] editorial staff organization
* [[Workflow]] for [[essay|article]] publication
* [[Document management system]]
* [[Single source publishing|Single source]] content management system—content stored in chunks within a relational database
* Variant management system—where personnel tag source content (usually text and graphics) to represent variants stored as single source "master" content modules, resolved to the desired variant at publication (for example: automobile owners manual content for 12 model years stored as single master content files and "called" by model year as needed)—often used in concert with database chunk storage (see above) for large content objects

==Governance structures==
Content management expert Marc Feldman defines three primary content management governance structures: localized, centralized, and federated—each having its unique strengths and weaknesses.&lt;ref&gt;http://www.clickz.com/clickz/column/1715089/governance-issues-content-management&lt;/ref&gt;

===Localized governance===
By putting control in the hands of those closest to the content, the context experts, localized governance models empower and unleash creativity.  These benefits come, however, at the cost of a partial-to-total loss of managerial control and oversight.

===Centralized governance===
When the levers of control are strongly centralized, content management systems are capable of delivering an exceptionally clear and unified brand message.  Moreover, centralized content management governance structures allow for a large number of cost-savings opportunities in large enterprises, realized, for example, (1) the avoidance of duplicated efforts in creating, editing, formatting, repurposing and archiving content, (2) through process management and the streamlining of all content related labor, and/or (3) through an orderly deployment or updating of the content management system.

===Federated governance===
Federated governance models potentially realize the benefits of both localized and centralized control while avoiding the weaknesses of both. While content management software systems are inherently structured to enable federated governance models, realizing these benefits can be difficult because it requires, for example, negotiating the boundaries of control with local managers and content creators.  In the case of larger enterprises, in particular, the failure to fully implement or realize a federated governance structure equates to a failure to realize the full return on investment and cost savings that content management systems enable.

==Implementation==
Content management implementations must be able to manage content distributions and digital rights in content life cycle.&lt;ref&gt;{{cite journal |last=White |first=Blake |date=April 2004 |title=A New Era for Content: Protection, Potential, and Profit in the Digital World |url=http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7262562 |journal=SMPTE Motion Imaging Journal |publisher=Society of Motion Picture &amp; Television Engineers |volume=113 |issue=4 |pages=110–120 |doi= |access-date=July 1, 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web | url=http://www.giantstepsmts.com/CM-DRMwhitepaper.pdf  | title=Integrating Content Management with Digital Rights Management   | publisher=GiantSteps  | accessdate=2016-07-01}}&lt;/ref&gt;&lt;ref&gt;{{cite web | url=http://www.locklizard.com/content-distribution-drm/  | title=Content Distribution &amp; DRM - Managing Content Distribution with DRM | publisher=Locklizard.com  | accessdate=2016-07-01}}&lt;/ref&gt;&lt;ref&gt;{{cite book  | title= The World Beyond Digital Rights Management  |url=https://www.amazon.com/World-Beyond-Digital-Rights-Management-ebook/dp/B004GEAEZM  | location = |last=Umeh   |first =Jude | date =October 2007  | publisher = British Computer Society  |page=320 |isbn= 978-1902505879 }}&lt;/ref&gt; Content management systems are usually involved with [[digital rights management]] in order to control user access and digital rights. In this step, the read-only structures of [[digital rights management]] systems force some limitations on content management, as they do not allow authors to change protected content in their life cycle. Creating new content using managed (protected) content is also an issue that gets protected contents out of management controlling systems. A few content management implementations cover all these issues.&lt;ref&gt;{{cite journal |last=White |first=Blake |date=April 2004 |title=A New Era for Content: Protection, Potential, and Profit in the Digital World |url=http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7262562 |journal=SMPTE Motion Imaging Journal |publisher=Society of Motion Picture &amp; Television Engineers |volume=113 |issue=4 |pages=110–120 |doi= |access-date=July 1, 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web | url=http://www.giantstepsmts.com/CM-DRMwhitepaper.pdf  | title=Integrating Content Management with Digital Rights Management   | publisher=GiantSteps  | accessdate=2016-07-01}}&lt;/ref&gt;&lt;ref&gt;{{cite web | url=http://www.locklizard.com/content-distribution-drm/  | title=Content Distribution &amp; DRM - Managing Content Distribution with DRM | publisher=Locklizard.com  | accessdate=2016-07-01}}&lt;/ref&gt;&lt;ref&gt;{{cite book  | title= The World Beyond Digital Rights Management  |url=https://www.amazon.com/World-Beyond-Digital-Rights-Management-ebook/dp/B004GEAEZM  | location = |last=Umeh   |first =Jude | date =October 2007  | publisher = British Computer Society  |page=320 |isbn= 978-1902505879 }}&lt;/ref&gt;

==See also==
{{Div col|colwidth=22em}}
* [[Content delivery]]
* [[Content engineering]]
* [[Content Management Interoperability Services]]
* [[Content management system]]
* [[Digital asset management]]
* [[Enterprise content management]]
* [[Enterprise information management]]
* [[Information architecture]]
* [[List of content management systems]]
* [[Single source publishing]]
* [[Snippet management]]
* [[Web content lifecycle]]
* [[Web design]]
* [[Website architecture]]
* [[Website governance]]
{{Div col end}}

==References==
{{reflist|colwidth=30em}}

==External links==
* {{cite book | last = Boiko | first = Bob | authorlink = | title = Content Management Bible | publisher = Wiley | date = 2004-11-26 | location = | pages = 1176 | url = | doi = | id = | isbn = 0-7645-7371-3 }}
* {{cite book | last = Rockley | first = Ann | authorlink = | title = Managing Enterprise Content: A Unified Content Strategy | publisher = New Riders Press | date = 2002-10-27 | location = | pages = 592 | url = | doi = | id = | isbn = 0-7357-1306-5 }}
* {{cite book | last = Hackos | first = JoAnn T. | authorlink = | title = Content Management for Dynamic Web Delivery | publisher = Wiley | date = 2002-02-14 | location = | pages = 432 | url = | doi = | id = | isbn = 0-471-08586-3 }}
* {{cite book | last = Glushko| first = Robert J. | authorlink = |author2=Tim McGrath| title = Document Engineering: Analyzing and Designing Documents for Business Informatics and Web Services | publisher = MIT Press| year = 2005 | location = | pages = 728| url = | doi = | id = | isbn = 0-262-57245-1 }}
* {{cite book | last = Ferran | first = Núria | authorlink = | author2=Julià Minguillón | title = Content Management for E-Learning | publisher = Springer | date = 2011 | location = | pages = 215 | url = http://www.springer.com/us/book/9781441969583 | doi = | id = | isbn = 978-1-4419-6958-3 }}

{{Content management systems}}
{{WebManTools}}

[[Category:Technical communication]]
[[Category:Data management]]
[[Category:Content management systems]]

[[fr:Système de gestion de contenu]]</text>
      <sha1>6zewsi8cyhjvmvnv865uae3whsjljbb</sha1>
    </revision>
  </page>
  <page>
    <title>Distributed data store</title>
    <ns>0</ns>
    <id>870094</id>
    <revision>
      <id>748872239</id>
      <parentid>733885752</parentid>
      <timestamp>2016-11-10T22:01:10Z</timestamp>
      <contributor>
        <username>Kevinjenscox</username>
        <id>9894519</id>
      </contributor>
      <minor />
      <comment>/* Distributed non-relational databases */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6150" xml:space="preserve">{{Essay-like|date=May 2012}}

A '''distributed data store''' is a [[computer network]] where information is stored on more than one [[Node (networking)|node]], often in a [[Replication (computing)|replicated]] fashion.&lt;ref&gt;{{Citation
|author = Yaniv Pessach
|url = http://openlibrary.org/books/OL25423189M/Distributed_Storage_Concepts_Algorithms_and_Implementations
|title = Distributed Storage
|edition = Distributed Storage: Concepts, Algorithms, and Implementations
}}&lt;/ref&gt; It is usually specifically used to refer to either a [[distributed database]] where users store information on a ''number of nodes'', or a [[computer network]] in which users store information on a ''number of peer network nodes''.

==Distributed databases==
[[Distributed database]]s are usually [[non-relational database]]s that make a quick access to data over a large number of nodes possible. Some distributed databases expose rich query abilities while others are limited to a [[key-value store]] semantics. Examples of limited distributed databases are [[Google]]'s [[BigTable]], which is much more than a [[distributed file system]] or a [[peer-to-peer network]],&lt;ref&gt;{{cite web
| accessdate = 2011-04-05
| location = http://the-paper-trail.org/
| publisher = Paper Trail
| title = BigTable: Google's Distributed Data Store
| quote = Although GFS provides Google with reliable, scalable distributed file storage, it does not provide any facility for structuring the data contained in the files beyond a hierarchical directory structure and meaningful file names. It’s well known that more expressive solutions are required for large data sets. Google’s terabytes upon terabytes of data that they retrieve from web crawlers, amongst many other sources, need organising, so that client applications can quickly perform lookups and updates at a finer granularity than the file level. [...] The very first thing you need to know about BigTable is that it isn’t a relational database. This should come as no surprise: one persistent theme through all of these large scale distributed data store papers is that RDBMSs are hard to do with good performance. There is no hard, fixed schema in a BigTable, no referential integrity between tables (so no foreign keys) and therefore little support for optimised joins.
| url = http://the-paper-trail.org/blog/?p=86}}&lt;/ref&gt; [[Amazon.com|Amazon]]'s [[Dynamo (storage system)|Dynamo]]&lt;ref&gt;{{cite web
| accessdate = 2011-04-05
| author = Sarah Pidcock
| date = 2011-01-31
| location = http://www.cs.uwaterloo.ca/
| page = 2/22
| publisher = WATERLOO – CHERITON SCHOOL OF COMPUTER SCIENCE
| title = Dynamo: Amazon’s Highly Available Key-value Store
| quote = Dynamo: a highly available and scalable distributed data store
| url = http://www.cs.uwaterloo.ca/~kdaudjee/courses/cs848/slides/sarah1.pdf}}&lt;/ref&gt;
and [[Azure Services Platform|Windows Azure Storage]].&lt;ref&gt;{{cite web 
| url= http://www.microsoft.com/windowsazure/features/storage/ |title= Windows Azure Storage |author= |date=2011-09-16 |work= |publisher= |accessdate=6 November 2011}}&lt;/ref&gt;

As the ability of arbitrary querying is not as important as the [[availability]], designers of distributed data stores have increased the latter at an expense of consistency. But the high-speed read/write access results in reduced consistency, as it is not possible to have both [[Consistency (database systems)|consistency]], availability, and partition tolerance of the network, as it has been proven by the [[CAP theorem]].

==Peer network node data stores==
In peer network data stores, the user can usually reciprocate and allow other users to use their computer as a storage node as well. Information may or may not be accessible to other users depending on the design of the network.

Most [[peer-to-peer]] networks do not have distributed data stores in that the user's data is only available when their node is on the network. However, this distinction is somewhat blurred in a system such as [[BitTorrent (protocol)|BitTorrent]], where it is possible for the originating node to go offline but the content to continue to be served. Still, this is only the case for individual files requested by the redistributors, as contrasted with a network such as [[Freenet]] where all computers are made available to serve all files.

Distributed data stores typically use an [[error detection and correction]] technique.
Some distributed data stores (such as [[Parchive]] over NNTP) use [[forward error correction]] techniques to recover the original file when parts of that file are damaged or unavailable.
Others try again to download that file from a different mirror.

==Examples==

===Distributed non-relational databases===
* [[Aerospike database|Aerospike]]
* [[Apache Cassandra]], former data store of [[Facebook]]
* [[BigTable]], the data store of [[Google]]
* [[CrateIO]]
* [[Druid (open-source data store)]], used by [[Netflix]], [[Yahoo]] and others
* [[Dynamo (storage system)|Dynamo]] of [[Amazon.com|Amazon]]
* [[Hazelcast]]
* [[HBase]], current data store of Facebook's Messaging Platform
* [[Couchbase]], data store used by [[LinkedIn]], [[Paypal]], [[Ebay]] and others.
* [[MongoDB]]
* [[Riak]]
* [[Hypertable]], from [[Baidu]]
* [[Voldemort (distributed data store)|Voldemort]], data store used by [[LinkedIn]]

===Peer network node data stores===
* [[BitTorrent (protocol)|BitTorrent]]
* [[Blockchain (database)]]
* [[Chord project]]
* [[GNUnet]]
* [[Freenet]]
* Unity, of the software [[Perfect Dark (P2P)|Perfect Dark]]
* [[Mnet (Computer program)|Mnet]]
* [[Network News Transfer Protocol|NNTP]] (the distributed data storage protocol used for [[Usenet]] news)
* [[Storage@home]]
* [[Tahoe-LAFS]]

==See also==
{{Portal|Computer Science}}
* [[Data store]]
* [[Distributed file system]]
* [[Keyspace (distributed data store)|Keyspace]], the DDS [[Schema (database)|schema]]
* [[Peer-to-peer]]
* [[Distributed hash table]]
* [[Distributed cache]]

==References==
{{Reflist}}

[[Category:Data management]]
[[Category:Distributed data storage| ]]
[[Category:Distributed data stores| ]]

[[ja:分散ファイルシステム#分散データストア]]</text>
      <sha1>k2xfjmp63utf1ytzykhzhzryxkg9wfv</sha1>
    </revision>
  </page>
  <page>
    <title>DAMA</title>
    <ns>0</ns>
    <id>1390371</id>
    <revision>
      <id>710362641</id>
      <parentid>695541045</parentid>
      <timestamp>2016-03-16T14:35:41Z</timestamp>
      <contributor>
        <ip>83.104.230.249</ip>
      </contributor>
      <comment>/* The Data Management Body of Knowledge (DMBOK) */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4767" xml:space="preserve">{{about|the association||Dama (disambiguation)}}

== About DAMA ==

'''DAMA''' (the [http://dama.org Data Management Association]) is a not-for-profit, vendor-independent, international association of technical and business professionals dedicated to advancing the concepts and practices of information resource management (IRM) and [[data resource management]] (DRM).

DAMA's primary purpose is to promote the understanding, development and practice of managing information and data as a key enterprise asset. The group is organized as a set of more than 40 chapters and members-at-large around the world, with an International Conference held every year. 

== Chapters, Chapter Structure, and Central Membership ==

DAMA International is organised through a Chapter structure, with each Chapter being a separate legal entity that formally affiliates with DAMA International. There are over 40 chapters established in over 16 countries around the world. The United States is disproportionately represented in the number of Chapters due to the number of city-based chapters as opposed to country-level in other jurisdictions.

In 2015 DAMA introduced a "Central" membership to help support and develop member services in a more consistent manner internationally and to provide a clear rallying point for members world wide who may lack a local chapter structure.

A full listing of Chapters can be found on the [http://www.dama.org/browse-chapters DAMA International website]. This list does not include "chapters in formation" which have yet to meet the criteria for recognition as Chapters and formal affiliation with DAMA-I.

== The Data Management Body of Knowledge (DMBOK) ==

The DAMA Guide to the [[Data management|Data Management]] Body of Knowledge" (DAMA-DMBOK Guide) was first published in April 5th, 2009.  

It defines ten knowledge domains which are at the core of Information and Data Management. 
* Data Governance (the central knowledge domain that connects all the others) 
* Data Architecture Management 
* Data Development 
* Data Operations Management 
* Data Security Management 
* Reference and Master Data Management 
* Data Warehousing and Business Intelligence 
* Document and Content Management 
* Metadata Management 
* Data Quality Management 
The DMBOK is copyright DAMA International.

== CDMP ==

DAMA International is the owner of the [[Certified Data Management Professional]] certification. This Certification is based on a range of learning objectives derived from the DMBOK.

In October 2015, DAMA International terminated its relationship with ICCP who had provided administrative services for the delivery of the CDMP certification. 

== DAMA Awards ==

From its inception in 1989 through to 2002, The DAMA Individual Achievement Awards have recognized a data professional who has made significant, demonstrable contributions to the information resource management industry consistent with DAMA International's vision.  From 2003 to 2015, DAMA created additional categories to recognize more people who have made special contributions to the world of data management.  The awards categories are:
* Academic Achievement Award: To a member from academia for outstanding research or theoretical contributions in the area of IRM/DRM
* DAMA Community Award : To a member of the DAMA community who has gone beyond the call of volunteer service to enhance the efforts of providing exceptional benefits to the DAMA Membership.
* Government Achievement Award : To a member of the leadership populace for instituting the inclusion and adherence to DRM/IRM principles.
* Professional Achievement Award : To a member from the ‘industry’ (business, discipline, specialist) who has made significant, demonstrable contributions to the IRM/DRM.
* Lifetime Achievement and Contribution Award : This special award has been presented to John Zachman in 2002, Michael Brackett in 2006 and Catherine Nolan in 2015. 

Starting in 2016, DAMA International created the DAMA International Award for Data Management Excellence.  This award will be presented to organizations or individuals who have made contributions to data management principles.  The first awards under this new structure will be given in April, 2016.

=== List of Award Winners ===

A list of award winners can be found on the [https://www.dama.org/content/award-results DAMA website].

=== Speakers Bureau ===
DAMA International provides a speakers bureau service to connect conference and event organisers with internationally regarded expert speakers.

A full listing of speakers can be found [http://www.dama.org/speakers here].

== References ==
&lt;references /&gt;

==External links==
* [http://www.dama.org/ DAMA International]

{{prof-assoc-stub}}
[[Category:Data management]]</text>
      <sha1>g0wf75cuq9whkgvsmpk3s4n8sjmuk07</sha1>
    </revision>
  </page>
  <page>
    <title>Holos</title>
    <ns>0</ns>
    <id>1423557</id>
    <revision>
      <id>570293542</id>
      <parentid>570293506</parentid>
      <timestamp>2013-08-26T18:24:36Z</timestamp>
      <contributor>
        <username>My name is not dave</username>
        <id>19079409</id>
      </contributor>
      <minor />
      <comment>[[Help:Reverting|Reverted]] edits by [[Special:Contributions/39.42.254.201|39.42.254.201]] ([[User talk:39.42.254.201|talk]]) to last version by Mauls</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8914" xml:space="preserve">{{multiple issues|
{{notability|date=May 2010}}
{{Unreferenced|date=May 2010}}
{{Peacock|date=September 2012}}
}}

'''Holos''' is an influential [[OLAP]] (Online Analytical Processing) product of the 1990s. Developed by Holistic Systems in 1987, the product remained in use until around 2004.

==Conception==
The Holos product succeeded an older generation of mainframe products such as [[System-W]]. It was the first to use an industry standard [[SQL]] database (as opposed to a proprietary one), and also the first to use the new GUI PC for the user interface.{{citation needed|date=September 2012}} In physically separating the ''number crunching'' from the user interface, the product was immediately client/server, although the term didn't come into use until some time later. In fact the process was described as cooperative processing at the time as client/server was not a current term at that time. The client/server model used for Holos was initially for a very "light" client as it was not clear at that time (1986/7) that PCs were going to be so commonplace and most were still running MS-DOS.

In fact it was technically possible to run the system using "dumb" terminal with reduced functionality in early versions although save for in Holistic's test environment this was rarely if ever done. In time due to the increased popularly of PCs and their increased power and the available of a stable and more functional version of Microsoft Windows additional functionality was added to the client end mostly in the form of development aids. In addition to data services, the Holos Server supplied business logic and calculation services. It also provided complementary services to the Holos Client which meant the internal processing associated with the report writer, worksheet, etc., was distributed between the two components.

==Architecture==
The core of the Holos Server was a [[business intelligence]] (BI) [[virtual machine]]. The Holos Language (HL) was compiled into a soft instruction code, and executed in this virtual machine (similar in concept to Java in more modern systems). The virtual machine was fully fault-tolerant, using structured [[exception handling]] internally, and provided a debugger interface. The debugger was machine-level until quite late on, after which it also supported source-level access.

OLAP data was handled as a core data type of HL, with specific syntax to accommodate multidimensional data concepts, and complete programmatic freedom to explore and utilise the data. This made it very different from the industry trend of query-based OLAP and SQL engines. On the upside, it allowed amazing flexibility in the applications to which it could be applied. On the downside, it mean that 3-tier configurations were never successfully implemented since the processing had to be close to the data itself. This hindered large-scale deployment to many clients, and the use of OLAP data from other vendors. In reality, its own data access times were probably some of the fastest around—at the individual cell level; they had to be in order to be practical. However, when fetching back bulk data for non-cooperating clients, or data from other vendors, the queries could not be optimised as a whole. Its own data access used a machine-wide shared memory cache.

==Language==
The Holos Language was a very broad language in that it covered a wide range of statements and concepts, including the reporting system, business rules, OLAP data, SQL data (using the Embedded SQL syntax within the hosting HL), device properties, analysis, forecasting, and data mining. It even supported elements to enable self-documentation and self-verification. Placing all these areas on a common footing, and allowing them to co-operate by sharing data, events, etc., was key to the number of possibilities that resulted. For instance, the report writer supported input as well as output, plus interactive graphics, and a comprehensive event mechanism to pass back information about the viewed data to event handlers. Also, reports and data were separate entities, thus allowing the same report to be applied to different data as long as it was described by similar meta-data. This meant that when terms like [[Enterprise Information System|EIS]] and [[Management information systems|MIS]] were first coined, the industry norm was "slideshows", i.e. pre-programmed transitions between views, whereas Holos provided data-driven drill-down, i.e. no pre-programmed views or links. The transitions could be made dependent upon the data values and trends, in conjunction with the available business logic.

==OLAP Storage==
Holos Server provided an array of different, but compatible, storage mechanisms for its multi-cube architecture: memory, disk, SQL. It was therefore the first product to provide "hybrid OLAP" ([[HOLAP]]). It provided a very versatile mechanism for joining cubes, irrespective of their storage technology, dimensionality, or meta-data, and this was eventually given a [[US patent]] (called COA—Compound OLAP Architecture {{US patent|6289352}}{{US patent|6490593}}). One novel aspect of this was a 'stack' feature that allowed read/write cubes to be stacked over read-only cubes. Read operations to the overall virtual cube then visited both 'racks' (top first, and then the bottom), whereas write operations only affected the top. The resulting valve-like mechanism found many applications in data sharing, what-if forecasting, and aggregation of slow SQL-based data. Since the overhead of the joining was small, it was not uncommon to have stacks 7 levels deep, and joining terabytes of real OLAP data. Around about V8.5, Holos Server implemented a hierarchical lock manager, allowing nesting of fine and coarse-grain OLAP locks, and full transaction control.

==Business Rules==
The business logic supported full cross-dimensional calculations, automatic ordering of rules using static data-flow analysis, and the identification and solution of simultaneous equations. The rules treated all dimensions in an orthogonal fashion. The aggregation process did not distinguish between simple summation or average calculations, and more complex non-commutative calculations. Both could be applied to any dimension member. The process allowed aggregation levels (i.e. those calculation levels starting with base data (level 0) and proceeding up to the overall grand total) to be individually pre-stored or left to be calculated on demand.

==Holos Client==
The Holos Client was both a design and delivery vehicle, and this made it quite large. Around about 2000, the Holos Language was made object-oriented (HL++) with a view to allowing the replacement of the Holos Client with a custom Java or VB product. However, the company were never sold on this, and so the project was abandoned.

One of the biggest failures was not to provide a thin-client interface to the Holos Server, and this must have contributed to the product's demise. Although an [[HTML]] toolkit was sold, it was clumsy and restricted. By the time a real thin-client mechanism was developed, it was far too late and it never got to market.

==Deployment==
Before its demise, the Holos Server product ran under Windows NT (Intel and Alpha), VMS (VAX and Alpha), plus about 10 flavours of UNIX, and accessed over half-a-dozen different SQL databases. It was also ported to several different locales, including Japanese.

==Company==
{{main|Crystal Decisions}}
Holistic Systems was purchased by the hardware company [[Seagate Technology]] in 1996. Along with other companies such as [[Crystal Services]], it was used to create a new subsidiary company called [[Seagate Software]]. Only Holistic and Crystal remained, and Seagate Software was renamed to [[Crystal Decisions]]. Holistic and Crystal had very different sales models. The average sale for the Holos Product in the United States was in excess of $250,000 and was sold primarily to Fortune 500 companies by a direct sales force. The Crystal sales model was based upon a "shrink wrapped" product Crystal Reports sold primarily through resellers. As Crystal was acquired prior to Holistic the senior management in the sales and marketing arena were mostly drawn from that organisation. They felt that all the product range should be sold through third parties and over a period of time dismantled the direct sales force culmination in a significant drop in sales for the Holos Product. Subsequently after some in-fighting and argument over product strategy, the main Holos development team finally started to leave around 2000, and Crystal Decisions was finally taken over by [[Business Objects (company)|Business Objects]] in 2004. Following the takeover, support for Holos was outsourced to [[Raspberry Software]], which was set up by former employees of Crystal Decisions.

[[Category:Data management]]
[[Category:Online analytical processing]]</text>
      <sha1>ayngzf4xllfvfznxeu96z5vrd0028l1</sha1>
    </revision>
  </page>
  <page>
    <title>Content re-appropriation</title>
    <ns>0</ns>
    <id>2579709</id>
    <revision>
      <id>552574196</id>
      <parentid>499415140</parentid>
      <timestamp>2013-04-28T14:52:30Z</timestamp>
      <contributor>
        <username>BD2412</username>
        <id>196446</id>
      </contributor>
      <minor />
      <comment>/* See also */fixing links from page moves, replaced: [[Taxonomy]] → [[Taxonomy (general)|Taxonomy]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2383" xml:space="preserve">{{Orphan|date=February 2009}}
Fundamental to modern [[information architecture]]s, and driven by [http://www.webreference.com/internet/semantic/ semantic Web] technologies, '''content re-appropriation''' is the act of searching, filtering, gathering, grouping, and aggregation which allows information to be related, classified and identified.  This is achieved by applying syntactic or semantic meaning though intelligent tagging or artificial interpretation of fragmented content (see [[Resource Description Framework]]).  Hence, all information becomes valuable and interpretable.

==Domain==
Since the domain of Content applies to areas of [[software applications]], [[document]]s, and [[Computer media|media]], these can be processed though a pipeline of generation, aggregation, transform-many, and serialization (see [http://www.w3.org/TR/xml-pipeline/ XML Pipeline]).  The output of this can viewed in a medium most effect for decision making.

The desired outcomes of content re-appropriation are:

*Seamless, Integrated, and Shared User experiences
*[[Software visualization|Visualization]]
*Detection, Analysis &amp; Investigation
*[[Personalization]] unique to the User
*Inbound or Outbound [[web syndication|Syndication]] of Information
*[[Publish]] or [[Subscribe]] to Information
*Dynamically adapted output to Users medium

Essentially to make ''information'' disparities transparent to the [[user (computing)|user]] - getting to the bottom line … quickly.

==Areas of Use==
Content re-appropriation is effective across the [[Content-Tier]], that is places where Content exists:

*Identity &amp; Directory Management e.g. [[Lightweight Directory Access Protocol|LDAP]], [[Security Assertion Markup Language|SAML]] &amp; [[JNDI]]
*Content Management e.g. [http://jakarta.apache.org/slide/ Apache Slide]
*Content Systems e.g. [[File System]]s, [[E-mail]], [[Network share]]s, [[Storage Area Network|SAN]] &amp; [[Database]]
*Business Systems e.g. [[Enterprise resource planning|ERP]] &amp; [[Customer Relationship Management|CRM]]
*Data Warehouse e.g. [[OLAP]]
*Internet &amp; Web Services e.g. [[HyperText Transfer Protocol|HTTP]] &amp; [[Simple Object Access Protocol|SOAP]]
*[[Instant messenger|Presence]] and [[peer-To-Peer]]

== See also ==
* [[Knowledge visualization]]
* [[Web indexing]]
* [[Taxonomy (general)|Taxonomy]]

[[Category:Data management]]
[[Category:Technical communication]]</text>
      <sha1>8c07tgqqoqyt6815izazuhkir8tqo0k</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Semantic desktop</title>
    <ns>14</ns>
    <id>12251078</id>
    <revision>
      <id>547930206</id>
      <parentid>474682861</parentid>
      <timestamp>2013-03-31T04:13:59Z</timestamp>
      <contributor>
        <username>EmausBot</username>
        <id>11292982</id>
      </contributor>
      <minor />
      <comment>Bot: Migrating 1 langlinks, now provided by Wikidata on [[d:Q8728629]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="97" xml:space="preserve">{{Cat main|Semantic desktop}}

[[Category:Knowledge representation]]
[[Category:Data management]]</text>
      <sha1>ru3intp4okx5skv7iqh2ud9q553vtys</sha1>
    </revision>
  </page>
  <page>
    <title>Ontology merging</title>
    <ns>0</ns>
    <id>11270991</id>
    <revision>
      <id>680113476</id>
      <parentid>543030469</parentid>
      <timestamp>2015-09-08T20:09:30Z</timestamp>
      <contributor>
        <username>Faizan</username>
        <id>17823581</id>
      </contributor>
      <minor />
      <comment>clean up, [[WP:AWB/T|typo(s) fixed]]: labour intensive → labour-intensive using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1014" xml:space="preserve">{{Unreferenced|date=March 2013}}

'''Ontology merging''' defines the act of bringing together two conceptually divergent [[ontology (computer science)|ontologies]] or the instance data associated to two ontologies. This is similar to work in database merging ([[schema matching]]). This merging process can be performed in a number of ways, manually, semi automatically, or automatically. Manual ontology merging although ideal is extremely labour-intensive and current research attempts to find semi or entirely automated techniques to merge ontologies. These techniques are statistically driven often taking into account similarity of concepts and raw similarity of instances through textual [[string metrics]] and semantic knowledge. These techniques are similar to those used in [[information integration]] employing [[string metrics]] from [[open source]] similarity libraries.

==See also==
*[[ontology mapping]]
*[[data integration]]

[[Category:Ontology (information science)]]
[[Category:Data management]]</text>
      <sha1>l071ao967f9i89d7g4fg5w4zzfko9qr</sha1>
    </revision>
  </page>
  <page>
    <title>Data governance</title>
    <ns>0</ns>
    <id>6222875</id>
    <revision>
      <id>755902747</id>
      <parentid>755866414</parentid>
      <timestamp>2016-12-20T20:55:31Z</timestamp>
      <contributor>
        <username>RLB2016</username>
        <id>29857789</id>
      </contributor>
      <minor />
      <comment>/* Data governance drivers */ added link to GDPR - a newer external regulation driving data governance</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="13781" xml:space="preserve">{{Governance}}

'''Data governance''' is a [[Control (management)|control]] that ensures that the [[data]] entry by an operations team member or by automated processes meets precise standards, such as a business rule, a data definition and data integrity constraints in the data model. The data governor uses data quality monitoring against production data to communicate errors in data back to operational team members, or to the technical support team, for corrective action.  Data governance is used by organizations to exercise control over processes and methods used by their [[data stewards]] and [[data custodian]]s in order to improve data quality.

Data governance is a set of processes that ensures that important data assets are formally managed throughout the enterprise. Data governance ensures that data can be trusted and that people can be made accountable for any adverse event that happens because of low data quality. It is about putting people in charge of fixing and preventing issues with data so that the enterprise can become more efficient. Data governance also describes an evolutionary process for a company, altering the company’s way of thinking and setting up the processes to handle information so that it may be utilized by the entire organization. It’s about using technology when necessary in many forms to help aid the process. When companies desire, or are required, to gain control of their data, they empower their people, set up processes and get help from technology to do it.&lt;ref name="sarsfield"&gt;Sarsfield, Steve (2009). "The Data Governance Imperative", IT Governance.&lt;/ref&gt;

According to one vendor, data governance is a [[quality control]] discipline for assessing, managing, using, improving, monitoring, maintaining, and protecting organizational information. It is a system of decision rights and accountabilities for information-related processes, executed according to agreed-upon models which describe who can take what actions with what information, and when, under what circumstances, using what methods.&lt;ref name="The DGI Data Governance Framework"&gt;{{cite web|url=http://www.datagovernance.com/wp-content/uploads/2014/11/dgi_framework.pdf|title=The DGI Data Governance Framework}}&lt;/ref&gt;

== Overview ==
Data governance encompasses the people, processes, and [[information technology]] required to create a consistent and proper handling of an organization's data across the business enterprise.  Goals may be defined at all levels of the enterprise and doing so may aid in acceptance of processes by those who will use them.  Some goals include

* Increasing consistency and confidence in [[decision making]]
* Decreasing the risk of regulatory fines
* Improving [[information security|data security]], also defining and verifying the requirements for data distribution policies&lt;ref&gt;Gianni, D., (2015, Jan). Data Policy Definition and Verification for System of Systems Governance, in Modeling and Simulation Support for System of Systems Engineering [http://onlinelibrary.wiley.com/doi/10.1002/9781118501757.ch5/summary]&lt;/ref&gt; 
* Maximizing the income generation potential of data
* Designating accountability for information quality
* Enable better planning by supervisory staff
* Minimizing or eliminating re-work
* Optimize staff effectiveness
* Establish process performance baselines to enable improvement efforts
* Acknowledge and hold all gain

These goals are realized by the implementation of Data governance programs, or initiatives using Change Management techniques

==Data governance drivers==
While data governance initiatives can be driven by a desire to improve data quality, they are more often driven by C-Level leaders responding to external regulations. Examples of these regulations include [[Sarbanes-Oxley]], [[Basel I]], [[Basel II]], [[HIPAA]], [[General Data Protection Regulation|GDPR]] and a number of data privacy regulations. To achieve compliance with these regulations, business processes and controls require formal management processes to govern the data subject to these regulations.&lt;ref&gt;[http://www.rimes.com/rimes-data-governance-handbook 'Rimes Data Governance Handbook'] [[RIMES]]&lt;/ref&gt; Successful programs identify drivers meaningful to both supervisory and executive leadership.

Common themes among the external regulations center on the need to manage risk. The risks can be financial misstatement, inadvertent release of sensitive data, or poor data quality for key decisions. Methods to manage these risks vary from industry to industry. Examples of commonly referenced best practices and guidelines include [[COBIT]], [[ISO/IEC 38500]], and others. The proliferation of regulations and standards creates challenges for data governance professionals, particularly when multiple regulations overlap the data being managed. Organizations often launch data governance initiatives to address these challenges.

== Data governance initiatives (Dimensions)==
Data governance initiatives improve [[data quality]] by assigning a team responsible for data's accuracy, accessibility, consistency, and completeness, among other metrics.  This team usually consists of executive leadership, [[project management]], [[line function|line-of-business managers]], and [[data steward]]s. The team usually employs some form of methodology for tracking and improving enterprise data, such as [[Six Sigma]], and tools for [[data mapping]], [[data profiling|profiling]], cleansing, and monitoring data.

Data governance initiatives may be aimed at achieving a number of objectives including offering better visibility to internal and external customers (such as [[supply chain]] management), compliance with [[compliance (regulation)|regulatory law]], improving operations after rapid company growth or [[mergers and acquisitions|corporate mergers]], or to aid the efficiency of enterprise [[knowledge worker]]s by reducing confusion and error and increasing their scope of knowledge. Many data governance initiatives are also inspired by past attempts to fix information quality at the departmental level, leading to incongruent and redundant data quality processes. Most large companies have many applications and databases that can't easily share information. Therefore, knowledge workers within large organizations often don't have access to the information they need to best do their jobs. When they do have access to the data, the [[data quality]] may be poor. By setting up a data governance practice or [[corporate data|Corporate Data]] Authority, these problems can be mitigated.

The structure of a data governance initiative will vary not only with the size of the organization, but with the desired objectives or the 'focus areas' &lt;ref name="focus areas"&gt;{{cite web|url=http://datagovernance.com/fc_focus_areas_for_data_governance.html |title=Data Governance Focus Areas |deadurl=yes |archiveurl=https://web.archive.org/web/20081006152845/http://www.datagovernance.com/fc_focus_areas_for_data_governance.html |archivedate=2008-10-06 |df= }}&lt;/ref&gt; of the effort.

== Implementation ==
Implementation of a Data Governance initiative may vary in scope as well as origin. Sometimes, an executive mandate will arise to initiate an enterprise wide effort, sometimes the mandate will be to create a pilot project or projects, limited in scope and objectives, aimed at either resolving existing issues or demonstrating value. Sometimes an initiative will originate lower down in the organization’s hierarchy, and will be deployed in a limited scope to demonstrate value to potential sponsors higher up in the organization.  The initial scope of an implementation can vary greatly as well, from review of a one-off IT system, to a cross-organization initiative.

== Data governance tools ==
Leaders of successful data governance programs declared in December 2006 at the Data Governance Conference in Orlando, Fl, that data governance is between 80 and 95 percent communication."&lt;ref&gt;{{cite web
 |url=http://www.dmreview.com/issues/2007_48/10001356-1.html 
 |title=Data Governance: One Size Does Not Fit All 
 |last=Hopwood 
 |first=Peter 
 |authorlink=Peter Hopwood 
 |publisher=[[DM Review Magazine]] 
 |date=June 2008 
 |accessdate=2008-10-02 
 |archiveurl=http://www.webcitation.org/5bGHaz1gA?url=http://www.dmreview.com/issues/2007_48/10001356-1.html 
 |archivedate=2008-10-02 
 |quote=At the inaugural Data Governance Conference in Orlando, Florida, in December 2006, leaders of successful data governance programs declared that in their experience, data governance is between 80 and 95 percent communication. Clearly, data governance is not a typical IT project. 
 |deadurl=yes 
 |df= 
}}&lt;/ref&gt; That stated, it is a given that many of the objectives of a Data Governance program must be accomplished with appropriate tools. Many vendors are now positioning their products as Data Governance tools; due to the different focus areas of various data governance initiatives, any given tool may or may not be appropriate, in addition, many tools that are not marketed as governance tools address governance needs.&lt;ref&gt;{{cite web
 |url=http://www.datagovernancesoftware.com 
 |title=DataGovernanceSoftware.com 
 |publisher=[[The Data Governance Institute]] 
 |accessdate=2008-10-02 
 |archiveurl=http://www.webcitation.org/5bGI3dfHV?url=http://www.datagovernancesoftware.com/ 
 |archivedate=2008-10-02 
 |quote= 
 |deadurl=yes 
 |df= 
}}&lt;/ref&gt;

== Data governance organizations ==
;DAMA International&lt;ref&gt;[http://www.dama.org/i4a/pages/index.cfm?pageid=1 DAMA International]&lt;/ref&gt;
:[[DAMA]] (the Data Management Association) is a not-for-profit, vendor-independent, international association of technical and business professionals dedicated to advancing the concepts and practices of information resource management (IRM) and data resource management (DRM).

;Data Governance Professionals Organization (DGPO)&lt;ref&gt;
[http://www.dgpo.org/ Data Governance Professionals Organization&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;
:The Data Governance Professionals Organization (DGPO) is a non-profit, vendor neutral, association of business, IT and data professionals dedicated to advancing the discipline of data governance.  The objective of the DGPO is to provide a forum that fosters discussion and networking for members and to encourage, develop and advance the skills of members working in the data governance discipline.

;The Data Governance Society &lt;ref&gt;[http://www.datagovernancesociety.org Data Governance Society]&lt;/ref&gt;
:The Data Governance Society, Inc. is dedicated to fostering a new paradigm for the effective use and protection of information in which Data is governed and leveraged as a unique corporate asset.

;The Data Governance Council &lt;ref&gt;[https://www-935.ibm.com/services/uk/cio/pdf/leverage_wp_data_gov_council_maturity_model.pdf Data Governance Council]&lt;/ref&gt; 
:The Data Governance Council is an organization formed by IBM consisting of companies, institutions and technology solution providers with the stated objective to build consistency and quality control in governance, which will help companies better protect critical data."

;IQ International -- the International Association for Information and Data Quality&lt;ref&gt;[http://iaidq.org/ IQ International, the International Association for Information and Data Quality]&lt;/ref&gt;
:IQ International is a not-for-profit, vendor neutral, professional association formed in 2004, dedicated to building the information and data quality profession.

== Data governance conferences ==
A number of major conferences relevant to data governance are held annually:
;Data Governance and Information Quality Conference&lt;ref&gt;[http://dgiq-conference.com/ Data Governance and Information Quality Conference&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; 
:Commercial conferences held each year in the USA

;Data Governance Conference Europe,&lt;ref&gt;[http://www.irmuk.co.uk/ Data Governance Conference Europe&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;
:Commercial conferences held annually in London, England .
;Information and Data Quality Conference&lt;ref&gt;[http://idq-conference.com/ Information and Data Quality Conference]&lt;/ref&gt;
:Not for profit conference run by IQ International in the USA
;Master Data Management &amp; Data Governance Conferences&lt;ref&gt;[http://www.tcdii.com/events/cdimdmsummitseries.html MDM SUMMIT Conference&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; 
:Six major conferences are run annually by the MDM Institute in London, San Francisco, Sydney, Toronto, Madrid, Frankfurt, and New York City.
;Financial Information Summit series of conferences&lt;ref&gt;[http://www.financialinformationsummit.com&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; 
;Hosted by Inside Reference Data magazine in New York, London, Hong Kong, Toronto, Chicago, Frankfurt, Paris and Tokyo.

==See also==
* [[Information Architecture]]
* [[Information technology governance]]
* [[Semantics of Business Vocabulary and Business Rules]]
* [[Master data management]]
* [[COBIT]]
* [[ISO/IEC 38500]]
* [[ISO/TC 215]]
* [[Operational risk management]]
* [[Basel II Accord]]
* [[HIPAA]]
* [[Sarbanes-Oxley Act]]
* [[Information technology controls]]
* [[Data Protection Directive]] (EU)
* [[Universal Data Element Framework]]
* [[Asset Description Metadata Schema]]

==References==
&lt;!--to cite a web resource, use this template
&lt;ref&gt;{{cite web
  | url = MANDATORY
  | title = MANDATORY
  | last =
  | first =
  | authorlink =
  | coauthors =
  | work =
  | publisher =
  | date =
  | format =
  | language=
  | doi =
  | accessdate =  
  | archiveurl = SHOULD BE USED ON PAGES ALLOWING ARCHIVING - USE A SERVICE LIKE webcitation.org or archive.org
  | archivedate = MANDATORY IF archiveurl
  | quote = 
 }}&lt;/ref&gt;
--&gt;
{{reflist}}

[[Category:Information technology governance]]
[[Category:Data management]]</text>
      <sha1>akoh8z7astqpjvcww56lgcnc4zx685g</sha1>
    </revision>
  </page>
  <page>
    <title>Microsoft Office PerformancePoint Server</title>
    <ns>0</ns>
    <id>9562761</id>
    <revision>
      <id>738534516</id>
      <parentid>706312388</parentid>
      <timestamp>2016-09-09T14:54:39Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor />
      <comment>[[User:Green Cardamom/WaybackMedic 2|WaybackMedic 2]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4336" xml:space="preserve">{{multiple issues|
{{more footnotes|date=August 2013}}
{{refimprove|date=August 2013}}
}}

{{Infobox Software
| name = Microsoft Office PerformancePoint Server
| developer = [[Microsoft]]
| released = {{Start date|2007|11}}
| latest_release_version = 1.0 SP2
| latest_release_date = 2008
| operating_system = [[Microsoft Windows]]
| genre = [[Enterprise Performance Management]]
| license = [[Proprietary software|Proprietary]] [[EULA]]
| website = [https://web.archive.org/web/20071016055516/http://www.microsoft.com/business/performancepoint/ www.microsoft.com/business/performancepoint]
}}
'''Microsoft Office PerformancePoint Server''' is a [[business intelligence]] [[Computer software|software]] product released in 2007 by [[Microsoft]]. Although discontinued in 2009, the dashboard, scorecard, and analytics capabilities of PerformancePoint Server were incorporated into [[Sharepoint 2010|SharePoint 2010]] and later versions.

PerformancePoint Server also provided a planning and budgeting component directly integrated with Excel.

==History==

Microsoft offered preview releases of PerformancePoint Server starting in mid-2006. Previews of the product were formed from [[Business Scorecard Manager 2005]] and the Planning Server component. Acquisitions [[ProClarity Corporation|ProClarity]] and [[Great Plains Software|Great Plains]] brought additional analytics and planning/reporting capabilities, as well as companion products ProClarity 6.3 and [[Microsoft FRx|FRx]].

PerformancePoint Server was officially released in November 2007.

Microsoft discontinued PerformancePoint Server as an independent product in 2009 and folded its dashboard, scorecard and analytics capabilities into PerformancePoint Services in [[SharePoint Server 2010]].&lt;ref&gt;{{cite web |url=http://www.informationweek.com/news/business_intelligence/analytics/showArticle.jhtml?articleID=212902915&amp;subSection=Business+Intelligence |title=Microsoft Makes Sweeping Changes To BI Software Strategy |date=January 27, 2009 |last=Weier |first=Mary Hayes |work=[[InformationWeek]]}}&lt;/ref&gt;

==Monitoring Server Component==
Business monitoring capabilities, including dashboards, scorecards &amp; key performance indicators, navigable reports for deeper analysis, strategy maps, and linked filtering, are provided by PerformancePoint's Monitoring Server component. A Dashboard Designer application that is distributed from Monitoring Server enables business analysts or IT Administrators to:

* create &amp; test data source connections
* create views that use those data connections
* assemble the views into a dashboard
* deploy the dashboard as a [[SharePoint]] page

Dashboard Designer saved content and security information back to the Monitoring Server. Data source connections, such as OLAP cubes or relational tables, were also made through Monitoring Server.
 
After a dashboard has been published to the Monitoring Server database, it would be deployed as a SharePoint page and shared with other users as such. When the pages were opened in a web browser, Monitoring Server updated the data in the views by connecting back to the original data sources.

==Planning Server Component==
PerformancePoint's Planning Server component supported maintenance of logical business models, budget &amp; approval workflows, enterprise data sources, and it followed [[Generally Accepted Accounting Principles]].

Planning Server made use of Excel for input and line-of-business reporting, as well as SQL Server for storing and processing business models.

==Management Reporter Component==
The Management Reporter component was designed to perform financial reporting and can read PerformancePoint Planning models directly. A development kit was also available to allow this component to read other models .{{which|date=August 2013}}

==References==
{{reflist}}

==External links==
* [http://msdn2.microsoft.com/en-us/office/bb660518.aspx PerformancePoint Server 2007 Developer Portal]
* [http://blogs.technet.com/datapuzzle Data Puzzle]
* [http://performancepointinsider.com/blogs/default.aspx PerformancePoint Insider]
* [http://alanwhitehouse.wordpress.com/2009/01/26/pps-planning-being-discontinued/ Performance Point Planning being discontinued]

{{Microsoft Office}}

[[Category:Microsoft Office servers]]
[[Category:Business intelligence]]
[[Category:Data management]]</text>
      <sha1>swr63e91wrktyuyert39f6ny54cwk0r</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Data-centric programming languages</title>
    <ns>14</ns>
    <id>925067</id>
    <revision>
      <id>547353776</id>
      <parentid>459422864</parentid>
      <timestamp>2013-03-28T00:10:45Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor />
      <comment>[[User:Addbot|Bot:]] Migrating 3 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q8363819]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="318" xml:space="preserve">{{Cat main|Data-centric programming language}}

This [[Wikipedia:Category|category]] lists those [[programming languages]] that are data-centric, with significant built-in functionality for data storage and manipulation.

[[Category:Data management|Programming languages]]
[[Category:Persistent programming languages]]</text>
      <sha1>pnfo0y5f2cia3c1j913fot0np0xcsjn</sha1>
    </revision>
  </page>
  <page>
    <title>Edge data integration</title>
    <ns>0</ns>
    <id>14124901</id>
    <revision>
      <id>577978011</id>
      <parentid>416996951</parentid>
      <timestamp>2013-10-20T14:16:59Z</timestamp>
      <contributor>
        <username>ChrisGualtieri</username>
        <id>16333418</id>
      </contributor>
      <minor />
      <comment>General Fixes using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1461" xml:space="preserve">{{Refimprove|date=January 2008}}
An '''edge data integration''' is an implementation of [[data integration]] technology undertaken in an ad hoc or tactical fashion. This is also sometimes referred to as point-to-point integration because it connects two types of data directly to serve a narrow purpose. Many edge integrations, and actually the vast majority of all data integration, involves hand-coded scripts. Some may take the form of [[Business Mashups]] (web application hybrids), [[Rich Internet application]]s, or other browser-based models that take advantage of [[Web 2.0]] technologies to combine data in a Web browser.

Examples of edge data integration projects might be:

* extracting a list of customers from a host [[Sales Force Automation]] application and writing the results to an [[Microsoft Excel|Excel]] spreadsheet
* creating a script-driven framework for managing [[RSS]] feeds
* combining data from a weather Web site, a shipping company's Web site, and a company's internal logistics database to track shipments and estimated arrival times of packages

It has been claimed that edge data integration do not typically require large budgets and centrally managed technologies, which is in contrast to a [[core data integration]].

== See also ==
* [[core data integration]]
* [[Business Mashups]]
* [[Rich Internet application]]
* [[Web 2.0]]
* [[Yahoo! Pipes]]
* [[Microsoft Popfly]]
*[[IBM Mashup Center]]

[[Category:Data management]]</text>
      <sha1>jnpgnxqzqb9o5sjdugcvnx8yccslvst</sha1>
    </revision>
  </page>
  <page>
    <title>Reference data</title>
    <ns>0</ns>
    <id>15349103</id>
    <revision>
      <id>752456053</id>
      <parentid>752455472</parentid>
      <timestamp>2016-12-01T10:03:47Z</timestamp>
      <contributor>
        <username>RichardWeiss</username>
        <id>193093</id>
      </contributor>
      <comment>looks like it should be a sectiona nd we need one due to formatting issues</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3104" xml:space="preserve">{{For|use in finance|Reference data (financial markets)}}
'''Reference data''' are [[data]] that define the set of permissible values to be used by other [[data field]]s. Reference data gain in value when they are widely re-used and widely referenced. Typically, they do not change overly much in terms of definition, apart from occasional revisions. Reference data are often defined by standards organizations, such as country codes as defined in [[ISO 3166-1]].&lt;ref&gt;{{Cite web|title = IBM Redbooks {{!}} Reference Data Management|url = http://www.redbooks.ibm.com/abstracts/tips1016.html|website = www.redbooks.ibm.com|date = 2013-05-16|accessdate = 2015-12-09|language = en}}&lt;/ref&gt;&lt;ref&gt;{{Cite web|title = Reference Data Management and Master Data: Are they Related ?   (Oracle Master Data Management)|url = https://blogs.oracle.com/mdm/entry/reference_data_management_and_master|website = blogs.oracle.com|accessdate = 2015-12-09}}&lt;/ref&gt;

Examples of reference data include:
* [[Units of measurement]]
* [[Country code]]s
* Corporate codes
* [[conversion of units|Fixed conversion rates]] e.g., [[weight]], [[temperature]], and [[length]]
* [[Calendar]] structure and constraints

==Differences with master data==
Reference data should be distinguished from [[master data]], which represent key business entities such as customers and materials in all their necessary detail (e.g., for customers: number, name, address, and date of account creation). In contrast, reference data usually consist only of a list of permissible values and attached textual descriptions. A further difference between reference data and master data is that a change to the reference data values may require an associated change in business process to support the change; a change in master data will always be managed as part of existing business processes. For example, adding a new customer or sales product is part of the standard business process. However, adding a new product classification (e.g. restricted sales item) or a new customer type (e.g. gold level customer) will result in a modification to the business processes to manage those items.

==References==
&lt;references /&gt;

==Further reading==
* {{Book reference|title = Managing Reference Data in Enterprise Databases|last = Chisholm|first = Malcolm|publisher = Morgan Kaufmann Publishers|year = 2001|isbn = 1558606971|location = |pages = }}
* {{Book reference|title = Master Data Management for SaaS Applications|last = Whei-Jen|first = Chen|publisher = IBM Redbooks|year = 2014|isbn = 978-0738440040|location = |pages = }}
* {{Book reference|title = Master Data Management and Data Governance|last = Berson|first = Alex|publisher = McGraw-Hill Osborne Media|year = 2011|isbn = 978-0071744584|location = |pages = }}

==See also==
* [[Master Data]]
* [[Data modeling]]
* [[Master Data Management]]
* [[Enterprise bookmarking]]
* [[Data architecture]]
* [[Transaction data]]
* [[Code_(metadata)]]

== External links ==
* [https://msdn.microsoft.com/en-us/library/hh213066.aspx Microsoft MSDN, Reference Data Services in DQS, 2012]

[[Category:Data management]]</text>
      <sha1>qa3v6u3jg1u8zns8ofjkjjvl4vxxpsp</sha1>
    </revision>
  </page>
  <page>
    <title>Operational database</title>
    <ns>0</ns>
    <id>14190258</id>
    <revision>
      <id>759851654</id>
      <parentid>759851590</parentid>
      <timestamp>2017-01-13T14:10:02Z</timestamp>
      <contributor>
        <ip>87.175.207.226</ip>
      </contributor>
      <comment>sorted</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6498" xml:space="preserve">{{more footnotes|date=March 2013}}

Operational database management systems (also referred to as [[OLTP]] On Line Transaction Processing databases), are used to manage dynamic data in real-time. These types of databases allow you to do more than simply view archived data. Operational databases allow you to modify that data (add, change or delete data), doing it in [[Real-time computing|real-time]].

Since the early 90's, the operational database software market has been largely taken over by [[SQL]] engines. Today, the operational [[DBMS]] market (formerly [[OLTP]]) is evolving dramatically, with new, innovative entrants and incumbents supporting the growing use of unstructured data and [[NoSQL]] DBMS engines, as well as [[XML database]]s and [[NewSQL|NewSQL databases]]. Operational databases are increasingly supporting [[distributed database]] architecture that provides [[high availability]] and [[fault tolerance]] through [[replication (computing)|replication]] and scale out ability.

Recognizing the growing role of operational databases in the IT industry that is fast moving from legacy databases to real-time operational databases capable to handle distributed web and mobile demand and to address [[Big data]] challenges, in October 2013 [[Gartner]] started to publish the [[Magic Quadrant]] for Operational Database Management Systems.&lt;ref name="Gartner Magic Quadrant for Operational Database Management Systems"&gt;{{cite web|url=https://www.gartner.com/doc/2610218/magic-quadrant-operational-database-management |title=Gartner Magic Quadrant for Operational Database Management Systems|publisher=Gartner.com}}&lt;/ref&gt;

== List of Operational Databases ==

{| style="text-align: left;" class="wikitable sortable"
|-
! Database platform !! Database model !! [[SQL]] Support !! [[NoSQL]] Support !! Managed objects !! ACID-transactions
|-
| [[Aerospike database|Aerospike]] || Key–Value Store ||  No || '''Yes''' || key-value pairs || None
|-
| [[Altibase]] || Relational database || '''Yes''' || NO || tabular data || Real-time ACID transactions
|-
| [[Apache Cassandra]] || Key-value store || No || '''Yes''' || key-value pairs || None
|-
| [[Cloudant]] || Document-Oriented Database || No || '''Yes''' || JSON || None
|-
| [[Clusterpoint]] || Document-Oriented Database || '''Yes''' (essential SQL)  || '''Yes''' || XML, JSON, text data || Distributed ACID-transactions 
|-
| [[Clustrix]] || Relational Database || '''Yes''' (newSQL) || No || tabular data || ACID-transactions 
|-
| [[Couchbase]] || Document-Oriented Database || '''Yes''' (N1QL) || '''Yes''' || JSON || None
|-
| [[CouchDB]] || Document-Oriented Database || No || '''Yes''' || JSON || None 
|-
| [[EnterpriseDB]] || Relational Database || '''Yes''' || No || tabular data || ACID-transactions 
|-
| [[FoundationDB]] || Key-value store || '''Yes''' || No || key-value pairs || ACID-transactions 
|-
| [[IBM DB2]] || Relational Database || '''Yes''' || No || tabular data || ACID-transactions 
|-
| [[Ingres_(database)|Ingres]] || Relational Database || '''Yes''' || No || tabular data || ACID-transactions 
|-
| [[MarkLogic]] || Document-Oriented Database || No || '''Yes''' (XQuery) || XML || ACID-transactions
|-
| [[Microsoft SQL Server]] || Relational Database || '''Yes''' || No || tabular data || ACID-transactions
|-
| [[MongoDB]] || Document-Oriented Database || No || '''Yes''' || BSON || None
|-
| [[NuoDB]] || Relational Database || '''Yes''' (newSQL) || No || tabular data || ACID-compliant
|-
| [[Oracle Database|Oracle]] || Relational Database || '''Yes''' || No || tabular data || ACID-transactions
|-
| [[OrientDB]] || Document-oriented Database || '''Yes''' || Yes || key-value pairs || ACID-transactions&lt;ref&gt;http://orientdb.com/docs/last/Transactions.html&lt;/ref&gt;
|-
| [[Riak]] || Key-value store || No || '''Yes''' || key-value pairs || None
|-
| [[SAP HANA]] || Relational Database || '''Yes''' || No || tabular data || ACID-transactions 
|-
| [[VoltDB]] || Relational Database || '''Yes''' (newSQL) || No || tabular data || ACID-transactions

|}

== Use in business ==

Operational databases are used to store, manage and track real-time business information. For example, a company might have an operational database used to track warehouse/stock quantities. As customers order products from an online web store, an operational database can be used to keep track of how many items have been sold and when the company will need to reorder stock.  An '''operational database''' stores information about the activities of an [[organization]], for example [[customer relationship management]] transactions or financial operations, in a computer [[database]].

Operational databases allow a business to enter, gather, and retrieve large quantities of specific information, such as company legal data, financial data, call data records, personal employee information, sales data, customer data, data on assets and many other information.  An important feature of storing information in an operational database is the ability to share information across the company and over the Internet.  Operational databases can be used to manage mission-critical business data, to monitor activities, to audit suspicious transactions, or to review the history of dealings with a particular customer.  They can also be part of the actual process of making and fulfilling a purchase, for example in [[e-commerce]].

==Data warehouse terminology==

In [[Data warehouse|data warehousing]], the term is even more specific: the operational database is the one which is accessed by an [[operational system]] (for example a customer-facing website or the application used by the customer service department) to carry out regular operations of an organization. Operational databases usually use an [[online transaction processing]] database which is optimized for faster transaction processing ([[create, read, update and delete]] operations).

== See also ==
* [[Document database|Document-oriented databases]]
* [[NewSQL|NewSQL databases]]
* [[NoSQL|NoSQL databases]]
* [[XML|XML databases]]
* [[SQL|SQL databases]]
* [[Distributed database]]s

== References ==
{{Reflist|33em}}
* O’Brien, Jason., and Marakas, Gorila., (2008).  Management Information Technology Systems.  Computer Software (pp.&amp;nbsp;185). New York, New York:  McGraw-Hill

[[Category:Data warehousing]]
[[Category:Data management]]
[[Category:Information technology management]]
[[Category:Business intelligence]]
[[Category:Types of databases]]</text>
      <sha1>4w0vccaek7mtzvgl6rd2ae46zfm3y57</sha1>
    </revision>
  </page>
  <page>
    <title>Cognos ReportNet</title>
    <ns>0</ns>
    <id>16942788</id>
    <revision>
      <id>754691302</id>
      <parentid>751717308</parentid>
      <timestamp>2016-12-14T00:10:13Z</timestamp>
      <contributor>
        <username>Chris the speller</username>
        <id>525927</id>
      </contributor>
      <minor />
      <comment>punct using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6185" xml:space="preserve">{{Infobox Software
| name = Cognos ReportNet
| logo = 
| screenshot = 
| caption = 
| author = [[Cognos]], an [[IBM]] Company
| developer = 
| released = September, 2003
| latest release version = Cognos ReportNet 1.3
| latest release date = 
| latest preview version = 
| latest preview date = 
| operating system = Multiple
| platform = Multiple
| language = Multi-lingual
| status = '''Inactive'''&lt;ref&gt;[http://www-01.ibm.com/software/analytics/cognos/products/reportnet/ IBM.com, ''Cognos ReportNet : now part of Cognos Enterprise'', consulté le 12 février 2014]&lt;/ref&gt;
| genre = [[Business Intelligence]]
| license = 
| website = [http://www-01.ibm.com/software/data/cognos/products/reportnet/ IBM.com]
}}

'''Cognos ReportNet (CRN)''' is a web-based [[software]] product for creating and managing [[ad hoc]] and custom-made reports. ReportNet is developed by the [[Ottawa]]-based  company [[Cognos]] (formerly Cognos Incorporated), an [[IBM]] company. The web-based reporting tool was launched in September 2003. Since IBM's acquisition of Cognos, ReportNet has been renamed ''IBM Cognos ReportNet'' like all other Cognos products.

ReportNet uses web services standards such as [[XML]] and [[Simple Object Access Protocol]] and also supports dynamic [[HTML]] and [[Java (programming language)|Java]].&lt;ref&gt;[https://web.archive.org/web/20080312025955/http://www.vnunet.com/vnunet/news/2123232/bear-sterns-chooses-cognos-reportnet Cognos ReportNet in news]&lt;/ref&gt; ReportNet is compatible with multiple databases including [[Oracle Database|Oracle]], [[SAP AG|SAP]], [[Teradata]], [[Microsoft SQL server]], [[IBM DB2|DB2]] and [[Sybase]].&lt;ref&gt;[http://www.cognos.com/solutions/data/ibm/advantages.html Data sources]&lt;/ref&gt;&lt;ref&gt;[http://support.cognos.com/en/support/products/crn101_software_environments.html CRN Environment details]&lt;/ref&gt; The product provides interface in over 10 languages,&lt;ref&gt;[http://www.cognos.com/products/business_intelligence/reporting/features.html CRN Features]&lt;/ref&gt; has Web Services architecture to meet the needs of multi-national, diversified enterprises and helps reduce total cost of ownership. Multiple versions of Cognos ReportNet have since been released by the company. Cognos ReportNet was awarded the [[Software and Information Industry Association]] (SIIA) 2005 [[Codie Awards]] for the "Best Business Intelligence or Knowledge Management Solution" category.&lt;ref&gt;[http://www.mywire.com/pubs/PRNewswire/2005/06/08/885642?extID=10051 Cognos ReportNet wins award]&lt;/ref&gt; CRN's capabilities have been further used in [[IBM Cognos 8 Business Intelligence|IBM Cognos 8 BI (2005)]], the latest reporting tool.&lt;ref&gt;[http://www.cognos.com/products/cognos8businessintelligence Cognos 8 BI]&lt;/ref&gt; CRN comes with its own [[software development kit]] (SDK).

==Launch==
Early adopters of Cognos ReportNet for their corporate reporting needs included [[Bear Stearns]], [[BMW]] and [[Alfred Publishing]]. Around this same time of launch, Cognos competitor [[Business Objects]] released version 6.1 of its enterprise reporting tool. Cognos ReportNet has been successful since its launch, raising revenues in 2004 from licensing fees.&lt;ref&gt;[http://www.highbeam.com/doc/1G1-131525446.html Cognos ReportNet delivers $30 Million in License Revenue in one Quarter]&lt;/ref&gt; Subsequently, other major corporations like [[McDonald's]] adopted Cognos ReportNet.&lt;ref&gt;[http://www.ebizq.net/news/5538.html ReportNet and fries]&lt;/ref&gt;

==Controversy==
Cognos rival [[Business Objects (company)|Business Objects]] announced in 2005 that BusinessObjects XI significantly outperformed Cognos ReportNet in benchmark tests conducted by VeriTest, an independent software testing firm. The tests performed showed Cognos ReportNet performed poorly when processing styled reports, complex business reports and combination of both.&lt;ref&gt;[http://www.crm2day.com/news/crm/114773.php BO XI Vs Cognos ReportNet]&lt;/ref&gt; The tests reported a massive 21 times higher report throughput for BusinessObjects XI than Cognos ReportNet at capacity loads.&lt;ref&gt;[http://goliath.ecnext.com/coms2/summary_0199-4404821_ITM BO XI outperforms Cognos ReportNet]&lt;/ref&gt; Cognos soon dismissed the claims by stating Business Objects dictated the environment and testing criteria and Cognos did not provide the software to participate in benchmark test.&lt;ref&gt;[http://www.cognos.com/news/releases/2005/0624_3.html Cognos dismisses the Test results]&lt;/ref&gt; Cognos later performed their own test to demonstrate Cognos ReportNet capabilities.&lt;ref&gt;[http://www.cognos.com/pdfs/whitepapers/wp_cognos_reportnet_scalability_benchmakrs_ms_windows.pdf Cognos scalability results]&lt;/ref&gt;

==Components==
* Cognos Report Studio – A Web-based product for creating complex professional looking reports.&lt;ref&gt;[http://web.princeton.edu/sites/datamall/documents/ug_cr_rptstd.pdf Refer definition in introduction page]&lt;/ref&gt;
* Cognos Query Studio - A Web-based product for creating ad-hoc reports.&lt;ref&gt;[http://web.princeton.edu/sites/datamall/documents/ug_cr_qstd.pdf Refer Introduction page]&lt;/ref&gt;
* Cognos Framework Manager – A [[metadata modeling]] tool to create BI metadata for reporting and dashboard applications.&lt;ref&gt;[http://www.cognos.com/products/framework_services Framework Manager Services] {{webarchive |url=https://web.archive.org/web/20080417030129/http://www.cognos.com/products/framework_services |date=April 17, 2008 }}&lt;/ref&gt;
* Cognos Connection – Main [[Enterprise portal|portal]] used to access reports, schedule reports and perform administrator activities.&lt;ref&gt;[http://web.princeton.edu/sites/datamall/documents/ug_cr_qstd.pdf Refer page9]&lt;/ref&gt;

==Versions==
* Cognos ReportNet 1.1 – [[Java EE]]-style professional web-based authoring tool. (base version)
* Cognos ReportNet IBM Special Edition – comes with an embedded version of [[IBM WebSphere]] as its application server and [[IBM DB2]] as its data store.
* Cognos Linux – for Intel-based [[Linux]] platforms.&lt;ref&gt;[http://www.ebizq.net/news/5688.html ReportNet on Linux]&lt;/ref&gt;

==See also==
*[[IBM Cognos Business Intelligence]]

==References==
{{reflist|30em}}

[[Category:Business intelligence]]
[[Category:Data management]]
[[Category:IBM software]]</text>
      <sha1>5qxneybl8ewy2g5p1b6i0bo36mntnrm</sha1>
    </revision>
  </page>
  <page>
    <title>Holistic Data Management</title>
    <ns>0</ns>
    <id>17377283</id>
    <revision>
      <id>609004781</id>
      <parentid>450897010</parentid>
      <timestamp>2014-05-17T20:25:37Z</timestamp>
      <contributor>
        <username>Hebrides</username>
        <id>1264145</id>
      </contributor>
      <comment>clean up, remove orphan tag using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7548" xml:space="preserve">'''Holistic Data Management''' (HDM) framework is AHISDATA indigenous standard for implementing software implementations within an organization network. This framework extends the existing data management solutions such as [[data quality]], [[data governance]], [[data integration]], [[data processing]], [[master data management]] and [[data validation]] solutions.

The HDM framework specifies that:
*All data objects must exist as a child data object or a parent data object.
*Only one unique parent data object must exist within a data network scope (DNS).
*All child data objects must have a data-mapping link defined within a data network scope.
*A data object relationship must exist at least in one of the following four data management modules:
''Data mapping'', ''data validation'', ''[[data integration]]'',''[[data processing]]''

== HDM framework ==
The following entities are specified in the HDM framework.

*''Data network scope (DNS)''
The data network scope (DNS) is the logical boundary that a software application database system of record (SOR) exists within an enterprise network. There can be multiple DNS within an enterprise network.

*''Data network domain (DND)''
The data network domain (DND) is the logical boundary representing a collection of multiple data network scope (DNS). There can be multiple DND within an enterprise network.

*''System of record (SOR)''
A system of record applies to the master or principal database system that a parent data objects resides on.
There can only be one SOR within a data network scope.

*''Parent data object (PDO)''
A parent data object (PDO) is the system of record schema object name. Only one unique parent data object must exist within a data network scope.

*''Child data object (CDO)''
A child data object (CDO) is a schema object name that derives its data from one or more parent data object(s).

*''Data-mapping link (DML)''
A data-mapping link (DML) is the data requirement specification applied to the relationship between multiple database schema objects where one data object derives its data from one or more data objects. DML is only applicable for a data-mapping data management module.

*''Data–object relationship (DOR)''
The data–object relationship (DOR) is the data requirement, business rule, program function that applies to one or multiple data objects. DOR can be applied on data-mapping links for each data management modules. Only one DOR can exist on a DML within a data management module.

*''Data management modules (DMM)''
Data management modules are the common user interface (UI) programs that defines and manage the data object relationship(s) within a data network scope.

There are four data management modules:

''Data mapping'' – This is the base data management user interface module. The data-mapping module provide the functionalities for managing data-mapping links and data object relationships for all database schemas within a data network scope. A data network scope must have at least one data-mapping design defined.

''Data validation'' – This user interface module provides the functionalities for defining and managing validation events on data object relationships. Validation events include auditing, reporting, scheduler, logger, triggers and DNS health check. Data validation events requires a data-mapping design defined within a data network scope.

''[[Data integration]]'' – This user interface module provides the functionalities for defining and managing interface configurations on data object relationships. The interface configurations include scheduler, transmission mode, listener, interface API and reporting. The interface APIs would allow third-party systems to transfer data using the data object relationship defined within a data network scope. Data Integration interface configuration requires a Data Mapping design defined within a data network scope.

''[[Data processing]]'' – This user interface module provides the functionalities for defining and managing interface configurations and batch runtime engines on data object relationships. The interface configurations include scheduler, transmission mode, multi-batch transmission, user-defined DOR API and reporting. Data Processing interface configuration requires a data-mapping design defined within a data network scope.

== Implementing the HDM framework ==
The HDM framework presents a standard for software implementations within an organization. The objective is to shed visibility, increase efficiency and centralized management of all other software implementations within an organization.

The HDM framework should be implemented as a major organization project that is supervised by the project management office. This would require a project charter developed and a project manager assigned for managing the implementation process. There are several phases involve in implementing the HDM framework:

*''Choose a data management module (DMM)'' – This exercise requires the acquisition of a data management module software application to be used for implementing the rest of the HDM framework. AHISDATA iNTEGRITY software is an integrated solution that provides DMM functionalities.
*''Scrub (inventory of existing applications and data sources)'' – This exercise identifies all applications within an organization and the data sources that they are connected to.
*''Formation (applications and data schema relation)'' – This exercise is to align all applications in relation to the data schemas within the data sources. The applications are grouped in the order of the data schemas that they access.
*''First axe (applications eligible for decommission)'' – This exercise is to identify all applications that are rogue, obsolete and completely redundant. These applications are eligible for removal.
*''Second axe (application eligible for consolidation)'' – This exercise is to identify all applications that have some functional similarities and some uniqueness in the data requirement. These applications are eligible for consolidation. The functionalities that are similar are left intact on one application and turned off or disabled on the other(s).
*''Define data network domain (DND)'' – This exercise is to define the data network domain for all the approved applications within the enterprise network.
*''Define Data Network Scope (DNS)'' – This exercise is to define the data network scope(s) required for each DND.
*''Define system of records (SOR)'' – This exercise is to define the SOR for each DNS.
*''Define parent data objects (PDO)'' – This exercise is to define all PDOs in each DNS.
*''Define child data objects (CDO)'' – This exercise is to define all CDOs in each DNS.
*''Define data mapping links (DML)'' – This exercise is to define all data-mapping links and object relationship in all DNS.
*''Define data object relationships (DOR)'' – This exercise is to define the DOR requirement for each data management module implemented.

==See also==
*[[Reference data]]
*[[Master data]]
*[[Customer data integration]]
*[[Product information management]]
*[[Identity resolution]]

==External links==
* [http://www.ahisdata.com/eHDMS/AHISDATA_HDM_WhitePaper_v35.pdf AHISDATA HDM WhitePaper]
* [http://msdn2.microsoft.com/en-us/library/bb190163.aspx#mdm04_topic4 The What, Why, and How of Master Data Management]

{{Data warehouse}}

[[Category:Business intelligence]]
[[Category:Data management]]
[[Category:Data warehousing products]]
[[Category:Information technology management]]</text>
      <sha1>hf1jfndqe95k7gjtgciaco7xw52d62b</sha1>
    </revision>
  </page>
  <page>
    <title>Sales intelligence</title>
    <ns>0</ns>
    <id>17420819</id>
    <revision>
      <id>737439139</id>
      <parentid>737258376</parentid>
      <timestamp>2016-09-02T19:56:14Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <comment>removed parent category of [[Category:Business intelligence]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3267" xml:space="preserve">'''Sales intelligence''' (SI) refers to technologies, applications and practices for the collection, integration, analysis, and presentation of information to help salespeople keep up to date with clients, [[Prospect research|prospect data]] and drive business. In addition to providing [[Performance metric|metric]]s for win-loss and sales confidence,&lt;ref&gt;[http://chapmanhq.com/solutions/strategic-account-management-sam/metrics-and-measurements Metrics for sales intelligence]&lt;/ref&gt; SI can present contextually relevant customer and product information.

The 2008 survey of 300 companies by the [[Aberdeen Group]]&lt;ref&gt;[http://www.aberdeen.com/Aberdeen-Library/5379/RA-sales-intelligence-nirvana.aspx Sales Intelligence, Aberdeen Group study (2008)]&lt;/ref&gt; show that the recent economic downturn has lengthened traditional sales cycles. As businesses have been forced to reduce spending, sales representatives have been challenged to meet [[Sales quota|quota]]s. Top performing companies have implemented sales intelligence programs to improve the quality and quantity of sales leads. SI contextualizes opportunities by providing relevant industry, corporate and personal information. Frequently SI's fact-based information is integrated or includes [[customer relationship management]] (CRM).

Although some aspects of sales intelligence overlaps business intelligence (BI), SI is specifically designed for the use of salespeople and sales managers. Unlike [[customer relationship management]] (CRM) and traditional [[business intelligence]] (BI) applications, SI provides real-time analysis of current sales data and assists with suggesting and delivering actionable, relevant information.

Sales intelligence solutions are predominantly designed for companies in the [[manufacturing]], [[distribution (business)|distribution]] and [[wholesale]] sectors. These are highly competitive markets, where volumes are high, [[Profit margin|margin]]s are low. (SI) solutions provide unique insight into customer [[buying pattern]]s. By automatically analysing and evaluating these patterns, Sales Intelligence pro-actively identifies and delivers [[up-sell]], [[cross-sell]] and switch-sell opportunities.

==See also==
* [[Analytics]]
* [[Augmented learning]]
* [[Business intelligence tools]]
* [[Dashboards (management information systems)]]
* [[Location intelligence]]
* [[Market intelligence]]
* [[Marketing intelligence]]
* [[Operational Intelligence]]
* [[OODA Loop]]
* [[Predictive analytics]]
* [[Business Intelligence 2.0]]
* [[Process mining]]
* [[Right-time marketing]]
* [[Integrated business planning]]

==References==
{{reflist}}

==External links==
* [http://blog.findable.me/post/52963306183/sales-intelligence-a-short-primer Sales Intelligence A Short Primer]

[[Category:Business intelligence]]
[[Category:Data management]]

[[da:Business intelligence]]
[[de:Business-Intelligence]]
[[es:Inteligencia empresarial]]
[[fr:Informatique décisionnelle]]
[[ko:경영 정보학]]
[[hr:Poslovna inteligencija]]
[[id:Intelijen bisnis]]
[[it:Business intelligence]]
[[lt:Verslo analitika]]
[[nl:Business intelligence]]
[[pl:Business intelligence]]
[[pt:Business intelligence]]
[[ru:Business Intelligence]]
[[fi:Business intelligence]]
[[sv:Business intelligence]]</text>
      <sha1>nwz89ynaa135d2pva4ejfar7dzaxyao</sha1>
    </revision>
  </page>
  <page>
    <title>Retention period</title>
    <ns>0</ns>
    <id>3544624</id>
    <revision>
      <id>754590631</id>
      <parentid>717236511</parentid>
      <timestamp>2016-12-13T13:31:53Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor />
      <comment>Robot - Removing category Administration per [[WP:CFD|CFD]] at [[Wikipedia:Categories for discussion/Log/2016 October 25]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2147" xml:space="preserve">The '''retention period''' of information is an aspect of [[records management|records and information management]] (RIM) and the [[records life cycle]].  It identifies the duration of time for which the information should be maintained or "retained", irrespective of format (paper, electronic, or other).  Retention periods vary on different types of information, based on content and a variety of other factors including: internal organizational need, regulatory requirements for inspection or audit, legal statutes of limitation, involvement in litigation, taxation and financial reporting needs, as well as other factors as defined by local, regional, state, national and/or international governing entities.  

Once an applicable retention period has elapsed for a given type or series of information, and all holds/moratoriums have been released, the information is typically destroyed using an approved and effective destruction method, which renders the information completely and irreversibly unusable via any means.  Information with historical value beyond its "usable value" may be accessioned to the custody of an archive organization for permanent or extended long-term preservation.  

==Defensible retention==
''Defensible retention'' refers to the ability of an identified and applied retention period to effectively provide for the defense of the record, and its eventual destruction or accessioning when scrutinized within a court of law or by other review.

It is commonly advised by [[Records management|Records and Information Management]] (RIM) professionals that any and all retention periods applied to organizational information should be reviewed and approved for use by competent legal counsel, which represents the organization, and is familiar with the specific legal and regulatory requirements of the organization.

==Guidance and education organizations==
*[[ARMA International]]
*[[Information and Records Management Society]]

==See also==
*[[Retention schedule]]

==References==
&lt;references/&gt;

[[Category:Legal documents]]
[[Category:Data management]]
[[Category:Public records]]
[[Category:Records management]]</text>
      <sha1>cjmsfebehr3s7ov4437v51r97iaxgme</sha1>
    </revision>
  </page>
  <page>
    <title>Enterprise data management</title>
    <ns>0</ns>
    <id>2654483</id>
    <revision>
      <id>672129974</id>
      <parentid>672105695</parentid>
      <timestamp>2015-07-19T13:39:14Z</timestamp>
      <contributor>
        <username>Widefox</username>
        <id>1588193</id>
      </contributor>
      <comment>Added {{[[Template:notability|notability]]}} tag to article ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5845" xml:space="preserve">{{notability|date=July 2015}}
{{No footnotes|date=November 2009}}
'''Enterprise Data Management''' ('''EDM''') is:

#A concept – referring to the ability of an organization to precisely define, easily integrate and effectively retrieve data for both internal applications and external communication.
#A business objective – focused on the creation of accurate, consistent and transparent content.  EDM emphasizes data precision, granularity and meaning and is concerned with how the content is integrated into [[business application]]s as well as how it is passed along from one [[business process]] to another.

EDM arose to address circumstances where users within organizations independently source, model, manage and [[Computer data storage|store data]].  Uncoordinated approaches by various segments of the organization can result in [[data conflict]]s and quality inconsistencies – lowering the trustworthiness of the data as it is used for operations and [[Business reporting|reporting]]. 

The goal of EDM is trust and confidence in data assets.  Its components are:

==Strategy and governance==
EDM requires a strategic approach to choosing the right processes, technologies and resources (i.e. data owners, governance, stewardship, [[data analyst]]s and [[data architect]]s).  EDM is a challenge for organizations because it requires alignment among multiple stakeholders (including IT, operations, finance, strategy and [[end-user]]s) and relates to an area (creation and use of common data) that has not traditionally had a clear “owner.”  

The governance challenge can be a big obstacle to the implementation of an effective EDM because of the difficulties associated with providing a [[business case]] on the benefits of data management.  The core of the challenge is due to the fact that data quality has no intrinsic value.  It is an enabler of other processes and the true benefits of effective data management are systematic and intertwined with other processes.  This makes it hard to quantify all the downstream implications or upstream improvements.  

The difficulties associated with quantification of EDM benefits translate into challenges with the positioning of EDM as an organizational priority.  Achieving organizational alignment on the importance of data management (as well as managing data as an ongoing area of focus) is the domain of [[governance]].  In recent years the establishment of an EDM and the EDM governance practice has become commonplace despite these difficulties. 

==Program implementation==
Implementation of an EDM program encompasses many processes – all of which need to be coordinated throughout the organization and managed while maintaining operational continuity.  Below are some of the major components of EDM implementation that should be given consideration:

===Stakeholder requirements===
EDM requires alignment among multiple stakeholders (at the right level of authority) who all need to understand and support the EDM objectives.  EDM begins with a thorough understanding of the requirements of the end users and the organization as a whole.  Managing stakeholder requirements is a critical, and ongoing, process based in an understanding of [[workflow]], data dependencies and the tolerance of the organization for operational disruption.  Many organizations use formal processes such as [[service level agreement]]s to specify requirements and establish EDM program objectives.

===Policies and procedures===
Effective EDM usually includes the creation, documentation and enforcement of operating policies and procedures associated with [[change management]], (i.e. [[data model]], business [[glossary]], master data shared domains, [[data cleansing]] and [[data normalization|normalization]]), data [[stewardship]], security constraints and dependency rules.  In many cases, these policies and procedures are documented for the first time as part of the EDM initiative.

===Data definitions and tagging===

One of the core challenges associated with EDM is the ability to compare data that is obtained from multiple internal and external sources.  In many circumstances, these sources use inconsistent terms and definitions to describe the data content itself – making it hard to compare data, hard to automate business processes, hard to feed complex applications and hard to exchange data.  This frequently results in a difficult process of [[data mapping]] and cross-referencing.  Normalization of all the terms and definitions at the data attribute level is referred to as the [[metadata]] component of EDM and is an essential prerequisite for effective data management.

===Platform requirements===

Even though EDM is fundamentally a data content challenge, there is a core technology dimension that must be addressed.  Organizations need to have a functional storage platform, a comprehensive data model and a robust messaging infrastructure.  They must be able to integrate data into applications and deal with the challenges of the existing (i.e. legacy) technology infrastructure.  Building the platform or partnering with an established technology provider on how the data gets stored and integrated into business applications is an essential component of the EDM process.

Enterprise data management as an essential business requirement has emerged as a priority for many organizations.  The objective is confidence and trust in data as the glue that holds business strategy together.

==See also==
* [[Master data management]]
* [[Master Data]]

==References==
{{Reflist}}

;General
* Enterprise Data Management Council http://www.edmcouncil.org
* [http://www.thegoldensource.com/files/EDM_Finextra_Report_Final.pdf Issues in Enterprise Data Management: A Survey Report, 12/06]

[[Category:Data management]]
[[Category:Product lifecycle management]]</text>
      <sha1>0bzmrtfnp92am5fbv7d09214eadn3zc</sha1>
    </revision>
  </page>
  <page>
    <title>Data exchange</title>
    <ns>0</ns>
    <id>10231058</id>
    <revision>
      <id>748877729</id>
      <parentid>748877673</parentid>
      <timestamp>2016-11-10T22:44:03Z</timestamp>
      <contributor>
        <username>Quercus solaris</username>
        <id>7034620</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10778" xml:space="preserve">{{more footnotes|date=February 2014}}
'''Data exchange''' is the process of taking [[data]] structured under a ''source'' [[Database schema|schema]] and transforming it into data structured under a ''target'' schema, so that the target data is an accurate representation of the source data.&lt;ref&gt;A. Doan, A. Halevy, and Z. Ives. "Principles of data integration", Morgan Kaufmann, 2012 pp. 276&lt;/ref&gt; Data exchange allows data to be [[cross-platform|shared between]] different [[computer program]]s. It is similar to the related concept of [[data integration]] except that data is actually restructured (with possible loss of content) in data exchange. There may be no way to transform an [[Instance (computer science)|instance]] given all of the constraints. Conversely, there may be numerous ways to transform the instance (possibly infinitely many), in which case a "best" choice of solutions has to be identified and justified.

== Single-domain data exchange ==

Often there are a few dozen different source and target schema (proprietary data formats) in some specific domain.
Often people develop an '''exchange format''' or '''interchange format''' for some single domain, and then write a few dozen different routines to (indirectly) translate each and every source schema to each and every target schema by using the interchange format as an intermediate step.
That requires a lot less work than writing and debugging the hundreds of different routines that would be required to directly translate each and every source schema directly to each and every target schema.
(For example,
[[Standard Interchange Format]] for geospatial data,
[[Data Interchange Format]] for spreadsheet data,
[[GPS eXchange Format]] or [[Keyhole Markup Language]] for indicating GPS coordinates on the globe,
[[Quicken Interchange Format]] for financial data,
[[GDSII]] for integrated circuit layout,
etc.){{Citation needed|date=September 2016}}

== Data exchange languages == &lt;!-- redirect target, if you change this, fix the redirect, too! --&gt;
{{merge to|Modeling language|date=May 2016}}
A data exchange language{{citation needed|date=May 2016}} is a language that is domain-independent and can be used for any kind of data. Its semantic expression capabilities and qualities are largely determined by comparison with the capabilities of natural languages. The term is also applied to any [[file format]] that can be read by more than one program, including proprietary formats such as [[Microsoft Office]] documents. However, a file format is not a real language as it lacks a grammar and vocabulary.

Practice has shown that certain types of [[formal language]]s are better suited for this task than others, since their specification is driven by a formal process instead of a particular software implementation needs. For example, [[XML]] is a [[markup language]] that was designed to enable the creation of dialects (the definition of domain-specific sublanguages) and a popular choice now in particular on the internet. However, it does not  contain domain specific dictionaries or fact types. Beneficial to a reliable data exchange is the availability of standard dictionaries-taxonomies and tools libraries such as [[parser]]s, schema [[validator]]s and transformation tools.{{Citation needed|date=September 2016}}

=== Popular languages used for data exchange ===
The following is a partial list of popular generic languages used for data exchange in multiple domains.

&lt;!-- this currently is very rough and ad-hoc - feel free to extend and change it! --&gt;
&lt;!-- Please verify definitions for the column headers of the table! --&gt;
{| class="wikitable sortable" style="font-size: 85%; text-align: center; width: auto;"
!
! Schemas
! Flexible
! Semantic verification
! Dictionary
! Information Model
! Synonyms and homonyms
! Dialecting
! Web standard
! Transformations
! Lightweight
! Human readable
! Compatibility
|-
| {{rh}}|[[Resource Description Framework|RDF]]
| {{yes}}{{Ref label|feat-rdf|1}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{partial}}
| Subset of [[Semantic web]]
|-
| {{rh}}|[[XML]]
| {{yes}}{{Ref label|feat-schema|1}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{yes}}
| subset of [[SGML]], [[HTML]]
|-
| {{rh}}|[[Atom (file format)|Atom]]
| {{yes}}
| {{unk}}
| {{unk}}
| {{unk}}
| {{no}}
| {{unk}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| [[XML]] dialect
|-
| {{rh}}|[[JSON]]
| {{no}}
| {{unk}}
| {{unk}}
| {{unk}}
| {{no}}
| {{unk}}
| {{no}}
| {{yes}}
| {{no}}
| {{yes}}
| {{yes}}
| subset of [[YAML]]
|-
| {{rh}}|[[YAML]]
| {{no}}{{Ref label|feat-ext|2}}
| {{unk}}
| {{unk}}
| {{unk}}
| {{no}}
| {{unk}}
| {{no}}
| {{no}}
| {{no}}{{Ref label|feat-ext|2}}
| {{yes}}
| {{yes}}{{Ref label|feat-yaml-readable|3}}
| superset of [[JSON]]
|-
| {{rh}}|[[REBOL]]
| {{yes}}{{Ref label|feat-rebol-parse|6}}
| {{yes}}
| {{no}}
| {{yes}}
| {{no}}
| {{yes}}
| {{yes}}
| {{no}}
| {{yes}}{{Ref label|feat-rebol-parse|6}}
| {{yes}}
| {{yes}}{{Ref label|feat-rebol-readable|4}}
| 
|-
| {{rh}}|[[Gellish]]
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}{{Ref label|feat-gellish-dict|7}}
| {{no}}
| {{yes}}
| {{yes}}
| ISO
| {{no}}
| {{yes}}
| {{partial}}{{Ref label|feat-gellish-readable|5}}
| SQL, RDF/XML, OWL
|}

'''Nomenclature'''
* Schemas - Whether the language definition is available in a computer interpretable form.
* Flexible - Whether the language enables extension of the semantic expression capabilities without modifying the schema.
* Semantic verification - Whether the language definition enables semantic verification of the correctness of expressions in the language.
* Dictionary-Taxonomy - Whether the language includes a dictionary and a taxonomy (subtype-supertype hierarchy) of concepts with inheritance.
* Synonyms and homonyms - Whether the language includes and supports the use of synonyms and homonyms in the expressions.
* Dialecting - Whether the language definition is available in multiple natural languages or dialects.
* Web or ISO standard - Organization that endorsed the language as a standard.
* Transformations - Whether the language includes a translation to other standards.
* Lightweight - Whether a lightweight version is available, in addition to a full version.
* Human readable - Whether expressions in the language are [[human-readable]]—readable by humans without training.{{Citation needed|date=September 2016}}
* Compatibility - Which other tools are possible or required when using the language.{{Citation needed|date=September 2016}}

'''Notes:'''

# {{note|feat-rdf}} RDF is a schema flexible language.
# {{note|feat-schema}} The schema of XML contains a very limited grammar and vocabulary.
# {{note|feat-ext}} Available as extension.
# {{note|feat-yaml-readable}} in the default format, not the compact syntax.
# {{note|feat-rebol-readable}} the syntax is fairly simple (the language was designed to be human readable); the dialects may require domain knowledge.
# {{note|feat-gellish-readable}} the standardized fact types are denoted by standardized English phrases, which interpretation and use needs some training.
# {{note|feat-rebol-parse}} the [[REBOL#parse|Parse dialect]] is used to specify, validate, and transform dialects.
# {{note|feat-gellish-dict}} the English version includes a Gellish English Dictionary-Taxonomy that also includes standardized fact types (= kinds of relations).

=== XML for data exchange ===
The popularity of [[XML]] for data exchange on the [[World Wide Web]] has several reasons. First of all, it is closely related to the preexisting standards [[Standard Generalized Markup Language]] (SGML) and [[Hypertext Markup Language]] (HTML), and as such a parser written to support these two languages can be easily extended to support XML as well. For example, [[XHTML]] has been defined as a format that is formal XML, but understood correctly by most (if not all) HTML parsers. This led to quick adoption of XML support in web browsers and the toolchains used for generating web pages.{{Citation needed|date=September 2016}}

=== YAML for data exchange ===
[[YAML]] is a language that was designed to be human-readable (and as such to be easy to edit with any standard text editor). Its notion often is similar to [[reStructuredText]] or a Wiki syntax, who also try to be readable both by humans and computers. YAML 1.2 also includes a shorthand notion that is compatible with JSON, and as such any JSON document is also valid YAML; this however does not hold the other way.{{Citation needed|date=September 2016}}

=== REBOL for data exchange ===

[[REBOL]] is a language that was designed to be human-readable and easy to edit using any standard text editor. To achieve that it uses a simple free-form syntax with minimal punctuation, and a rich set of datatypes. REBOL datatypes like URLs, e-mails, date and time values, tuples, strings, tags, etc. respect the common standards. REBOL is designed to not need any additional meta-language, being designed in a metacircular fashion. The metacircularity of the language is the reason why e.g. the Parse dialect used (not exclusively) for definitions and transformations of REBOL dialects is also itself a dialect of REBOL. REBOL was used as a source of inspiration by the designer of JSON.{{Citation needed|date=September 2016}}

=== Gellish for data exchange ===
[[Gellish English]] is a formalized subset of natural English, which includes a simple grammar and a large extensible [[English dictionary|English Dictionary-Taxonomy]] that defines the general and domain specific terminology (terms for concepts), whereas the concepts are arranged in a subtype-supertype hierarchy (a Taxonomy), which supports inheritance of knowledge and requirements. The Dictionary-Taxonomy also includes standardized fact types (also called relation types). The terms and relation types together can be used to create and interpret expressions of facts, knowledge, requirements and other information. Gellish can be used in combination with [[SQL]], [[RDF/XML]], [[Web Ontology Language|OWL]] and various other meta-languages. The Gellish standard is being adopted as ISO 15926-11.{{Citation needed|date=September 2016}}

== See also ==
* [[Atom (file format)]]
* [[Lightweight markup language]]
* [[RSS]]

== References ==

{{reflist}}

{{refbegin}}

*R. Fagin, P. Kolaitis, R. Miller, and L. Popa. "Data exchange: semantics and query answering." Theoretical Computer Science, 336(1):89–124, 2005.
*P. Kolaitis. "Schema mappings, data exchange, and metadata management." Proceedings of the twenty- fourth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems, pages 61–75, 2005
{{refend}}

{{Data Exchange}}

[[Category:Data management]]</text>
      <sha1>gbnjtk7ij0h7y1vcfaj0fm2apzi9gs8</sha1>
    </revision>
  </page>
  <page>
    <title>Electronically stored information (Federal Rules of Civil Procedure)</title>
    <ns>0</ns>
    <id>19675044</id>
    <revision>
      <id>732728082</id>
      <parentid>729551330</parentid>
      <timestamp>2016-08-02T21:11:00Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <minor />
      <comment>cap, bold, simplify heading</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4054" xml:space="preserve">'''Electronically stored information''' ('''ESI'''), for the purpose of the [[Federal Rules of Civil Procedure]] (FRCP) is information created, manipulated, communicated, stored, and best utilized in digital form, requiring the use of computer hardware and software.&lt;ref name="nwjtip"&gt;[http://www.law.northwestern.edu/journals/njtip/v4/n2/3 ''Electronically Stored Information: The December 2006 Amendments to the Federal Rules of Civil Procedure''], Kenneth J. Withers, Northwestern Journal of Technology and Intellectual Property, Vol.4 (2), 171&lt;/ref&gt;

ESI has become a legally defined phrase as the [[Federal government of the United States|U.S. government]] determined for the purposes of the FRCP rules of 2006 that promulgating procedures for maintenance and discovery for electronically stored information was necessary.  References to “electronically stored information” in the Federal Rules of Civil Procedure (FRCP) invoke an expansive approach to what may be discovered during the fact-finding stage of civil litigation.&lt;ref name="Federal Rules of Civil Procedure"&gt;{{cite web|title=Federal Rules of Civil Procedure (FRCP)|url=https://www.law.cornell.edu/rules/frcp/rule_34|website=Legal Information Institute [LII]|publisher=Cornell University Law School|accessdate=October 31, 2015|ref=Rule 34}}&lt;/ref&gt;

Rule 34(a) enables a party in a civil lawsuit to request another party to produce and permit the requesting party or its representative to inspect, copy, test, or sample the following items in the responding party's possession, custody, or control:

&lt;blockquote&gt;any designated documents or electronically stored information—including writings, drawings, graphs, charts, photographs, sound recordings, images, and other data or data compilations—stored in any medium from which information can be obtained either directly or, if necessary, after translation by the responding party into a reasonably usable form...Rule 34(a)(1) is intended to be broad enough to cover all current types of computer-based information, and flexible enough to encompass future changes and developments.&lt;/blockquote&gt;

==Types==

===Native files===
The term ''native files'' refers to user-created documents, which could be in [[Microsoft Office]] or [[Apache OpenOffice|Open Office]] document formats as well as other files stored on computer, but could include video surveillance footage saved on a computer hard drive, [[Computer-aided design]] files such as [[blueprint]]s or maps, [[digital photography|digital photographs]], scanned images, [[archive file]]s, e-mail, and [[digital audio]] files, among other data,

===Logical data===
A judge ruled that [[Random Access Memory|RAM]] is reasonably accessible and retainable for anticipation of litigation.{{Citation needed|date=November 2011}}

In Australia RAM, can be used in litigation post 1996.

==References==
{{reflist}}

==Further reading==
* {{cite book|chapter=Meet the New Rules|title=The Discovery Revolution|author1=George L. Paul |author2=Bruce H. Nearon |publisher=American Bar Association|year=2006|isbn=9781590316054}}
* {{cite book|title=Discovery of Electronically Stored Information|author=Ronald J. Hedges|publisher=BNA Books|year=2007|isbn=9781570186721}}
* {{cite book|title=The Sedona Principles 2007: Best Practices Recommendations &amp;amp; Principles for Addressing Electronic Document Production|author=Jonathan M. Redgrave|publisher=BNA Books|year=2007|isbn=9781570186776}}
* {{cite book|title=Cyber Forensics|author1=Albert J. Marcella |author2=Albert J. Marcella Jr. |author3=Doug Menendez |chapter=Electronically stored information and cyber forensics|publisher=CRC Press|year=2007|isbn=9780849383281}}
* {{cite book|title=Litigating With Electronically Stored Information|author1= Marian K. Riedy |author2=Suman Beros |author3= Kim Sperduto |publisher=Artech House Telecommunications Library|year=2007|isbn=9781596932203}}

[[Category:Computer data]]
[[Category:Data management]]
[[Category:United States discovery law]]
[[Category:Records management]]


{{US-law-stub}}</text>
      <sha1>eajijg8ucj17s6wiz2ajvb4znvjbi36</sha1>
    </revision>
  </page>
  <page>
    <title>Locks with ordered sharing</title>
    <ns>0</ns>
    <id>21064035</id>
    <revision>
      <id>702144234</id>
      <parentid>701929909</parentid>
      <timestamp>2016-01-28T18:54:44Z</timestamp>
      <contributor>
        <username>GünniX</username>
        <id>237572</id>
      </contributor>
      <minor />
      <comment>/* References */ISSN using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="853" xml:space="preserve">In [[databases]] and [[transaction processing]] the term '''Locks with ordered sharing''' comprises several variants of the ''[[Two phase locking]]'' (2PL) [[concurrency control]] protocol generated by changing the blocking semantics of locks upon [[Serializability#View and conflict serializability|conflicts]]. One variant is identical to [[Commitment ordering#Strict CO (SCO)|Strict commitment ordering (SCO)]].

==References==

*D. Agrawal, A. El Abbadi, A. E. Lang: [http://portal.acm.org/citation.cfm?id=627615   ''The Performance of Protocols Based on Locks with Ordered Sharing''], IEEE Transactions on Knowledge and Data Engineering, Volume 6, Issue 5, October 1994, PP. 805 – 818, {{ISSN|1041-4347}}

[[Category:Data management]]
[[Category:Databases]]
[[Category:Concurrency control]]
[[Category:Transaction processing]]


{{database-stub}}</text>
      <sha1>2rqnwxjqg5tmktugripasqjda3xmfqw</sha1>
    </revision>
  </page>
  <page>
    <title>Data verification</title>
    <ns>0</ns>
    <id>22601719</id>
    <revision>
      <id>720573269</id>
      <parentid>720573248</parentid>
      <timestamp>2016-05-16T18:30:17Z</timestamp>
      <contributor>
        <username>Harry-</username>
        <id>4438764</id>
      </contributor>
      <minor />
      <comment>Reverted edits by [[Special:Contributions/168.167.94.5|168.167.94.5]] ([[User talk:168.167.94.5|talk]]): Nonconstructive editing ([[WP:HG|HG]]) (3.1.20)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="951" xml:space="preserve">Data''' Verification''' is a process in which different types of data are checked for accuracy and [[data consistency|inconsistencies]] after [[data migration]] is done.&lt;ref&gt;http://www.datacap.com/products/features/verify/&lt;/ref&gt;

It helps to determine whether data was accurately translated when data is [[data transfer|transferred]] from one source to another, is complete, and supports processes in the new system. During verification, there may be a need for a parallel run of both systems to identify areas of disparity and forestall erroneous [[data loss]].

A type of Data Verification is [[double entry]] and [[proofreading]] data. Proofreading data involves someone checking the data entered against the original document. This is also time consuming and costly.

==References==
{{reflist|2}}

==External links==
* [http://www.pcguide.com/care/bu/howVerification-c.html PC Guide article]

[[Category:Data management]]
[[Category:Data quality]]</text>
      <sha1>f9ton9h4h8m1c75py70ur94gk7ep72y</sha1>
    </revision>
  </page>
  <page>
    <title>Mobile content management system</title>
    <ns>0</ns>
    <id>22464607</id>
    <revision>
      <id>734719946</id>
      <parentid>732744245</parentid>
      <timestamp>2016-08-16T07:55:37Z</timestamp>
      <contributor>
        <username>David Gerard</username>
        <id>36389</id>
      </contributor>
      <comment>/* Examples of Mobile content management systems */ rm section as spam magnet</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4033" xml:space="preserve">A '''Mobile Content Management system''' ('''MCMs''') is a type of [[content management system]] (CMS) capable of storing and delivering content and services to mobile devices, such as mobile phones, smart phones, and PDAs. Mobile content management systems may be discrete systems, or may exist as features, modules or add-ons of larger content management systems capable of multi-channel content delivery. Mobile content delivery has unique, specific constraints including widely variable device capacities, small screen size, limited wireless bandwidth, small storage capacity, and comparatively weak device processors.&lt;ref&gt;[http://www.insight-corp.com/%5CExecSummaries%5Ccontent08ExecSum.pdf Content Management for Wireless Networks, 2008-2013 - Insight Research Report]&lt;/ref&gt;

Demand for mobile content management increased as mobile devices became increasingly ubiquitous and sophisticated. MCMS technology initially focused on the business to consumer (B2C) mobile market place with ringtones, games, text-messaging, news, and other related content. Since, mobile content management systems have also taken root in business-to-business (B2B) and business-to-employee (B2E) situations, allowing companies to provide more timely information and functionality to business partners and mobile workforces in an increasingly efficient manner. A 2008 estimate put global revenue for mobile content management at US$8 billion.&lt;ref&gt;[http://www.wirelessweek.com/Content-Management-Systems-Mobile-Embrace.aspx Content Management Systems’ Mobile Embrace, By Evan Koblentz, WirelessWeek, August 28, 2008]&lt;/ref&gt;

==Key features==

===Multi-channel content delivery===
Multi-channel content delivery capabilities allow users to manage a central content repository while simultaneously delivering that content to mobile devices such as mobile phones, smartphones, tablets and other mobile devices. Content can be stored in a raw format (such as Microsoft Word, Excel, PowerPoint, PDF, Text, HTML etc.) to which device-specific presentation styles can be applied.&lt;ref&gt;[http://www.apoorv.info/2007/05/26/content-management-for-mobile-delivery/ Content Management for Mobile Delivery, Posted by Apoorv, PCM.Blog, May 26, 2007]&lt;/ref&gt;

===Content access control===
Access control includes authorization, authentication, access approval to each content.  In many cases the access control also includes download control, wipe-out for specific user, time specific access.  For the authentication, MCM shall have basic authentication which has user ID and password.  For higher security many MCM supports IP authentication and mobile device authentication.

===Specialized templating system===
While traditional web content management systems handle templates for only a handful of web browsers, mobile CMS templates must be adapted to the very wide range of target devices with different capacities and limitations. There are two approaches to adapting templates: multi-client and multi-site. The multi-client approach makes it possible to see all versions of a site at the same domain (e.g. sitename.com), and templates are presented based on the device client used for viewing. The multi-site approach displays the mobile site on a targeted sub-domain (e.g. mobile.sitename.com).

===Location-based content delivery===
Location-based content delivery provides targeted content, such as information, advertisements, maps, directions, and news, to mobile devices based on current physical location. Currently, GPS (global positioning system)  navigation systems offer the most popular [[location-based service]]s. Navigation systems are specialized systems, but incorporating mobile phone functionality makes greater exploitation of location-aware content delivery possible.

==See also==
*[[Mobile Web]]
*[[Content management]]
*[[Web content management system]]
*[[Enterprise content management]]
*[[Apache Mobile Filter]]

==References==
&lt;references/&gt;

[[Category:Content management systems]]
[[Category:Mobile Web]]
[[Category:Data management]]</text>
      <sha1>bcqzb9s6rh0ifwqulck51jkfkmifxj4</sha1>
    </revision>
  </page>
  <page>
    <title>Isolation (database systems)</title>
    <ns>0</ns>
    <id>325521</id>
    <revision>
      <id>759075398</id>
      <parentid>757215271</parentid>
      <timestamp>2017-01-09T02:47:55Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor />
      <comment>[[WP:CHECKWIKI]] error fix for #61.  Punctuation goes before References. Do [[Wikipedia:GENFIXES|general fixes]] if a problem exists. -</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="21326" xml:space="preserve">{{Refimprove|date=January 2009}}
In [[database]] systems, '''isolation''' determines how transaction integrity is visible to other users and systems. For example, when a user is creating a Purchase Order and has created the header, but not the Purchase Order lines, is the header available for other systems/users, carrying out [[Concurrency (computer science)|concurrent]] operations (such as a report on Purchase Orders), to see?

A lower isolation level increases the ability of many users to access data at the same time, but increases the number of concurrency effects (such as dirty reads or lost updates) users might encounter. Conversely, a higher isolation level reduces the types of concurrency effects that users may encounter, but requires more system resources and increases the chances that one transaction will block another.&lt;ref&gt;"Isolation Levels in the Database Engine", Technet, Microsoft, http://technet.microsoft.com/en-us/library/ms189122(v=SQL.105).aspx&lt;/ref&gt;

Isolation is typically defined at database level as a property that defines how/when the changes made by one operation become visible to other. On older systems, it may be implemented systemically, for example through the use of temporary tables. In two-tier systems, a Transaction Processing (TP) manager is required to maintain isolation. In n-tier systems (such as multiple websites attempting to book the last seat on a flight), a combination of stored procedures and transaction management is required to commit the booking and send confirmation to the customer.&lt;ref&gt;"The Architecture of Transaction Processing Systems", Chapter 23, Evolution of Processing Systems, Department of Computer Science, Stony Brook University, retrieved 20 March 2014, http://www.cs.sunysb.edu/~liu/cse315/23.pdf&lt;/ref&gt;

Isolation is one of the [[ACID]] (Atomicity, Consistency, Isolation, Durability) properties.

==Concurrency control==
[[Concurrency control]] comprises the underlying mechanisms in a [[DBMS]] which handles isolation and guarantees related correctness. It is heavily utilized by the database and storage engines (see above) both to guarantee the correct execution of concurrent transactions, and (different mechanisms) the correctness of other DBMS processes. The transaction-related mechanisms typically constrain the database data access operations' timing ([[Schedule (computer science)|transaction schedules]]) to certain orders characterized as the [[serializability]] and [[recoverability]] schedule properties. Constraining database access operation execution typically means reduced performance (rates of execution), and thus concurrency control mechanisms are typically designed to provide the best performance possible under the constraints. Often, when possible without harming correctness, the serializability property is compromised for better performance. However, recoverability cannot be compromised, since such typically results in a quick database integrity violation.

[[Two-phase locking]] is the most common transaction concurrency control method in DBMSs, used to provide both serializability and recoverability for correctness. In order to access a database object a transaction first needs to acquire a [[Lock (database)|lock]] for this object. Depending on the access operation type (e.g., reading or writing an object) and on the lock type, acquiring the lock may be blocked and postponed, if another transaction is holding a lock for that object.

==Isolation levels==
Of the four [[ACID]] properties in a [[Database management system|DBMS]] (Database Management System), the isolation property is the one most often relaxed.  When attempting to maintain the highest level of isolation, a DBMS usually acquires [[Lock (database)|locks]] on data or implements [[multiversion concurrency control]], which may result in a loss of [[concurrency (computer science)|concurrency]].  This requires adding logic for the [[software application|application]] to function correctly.

Most DBMSs offer a number of ''transaction isolation levels'', which control the degree of locking that occurs when selecting data. For many database applications, the majority of database transactions can be constructed to avoid requiring high isolation levels (e.g. SERIALIZABLE level), thus reducing the locking overhead for the system.  The programmer must carefully analyze database access code to ensure that any relaxation of isolation does not cause software bugs that are difficult to find. Conversely, if higher isolation levels are used, the possibility of [[deadlock]] is increased, which also requires careful analysis and programming techniques to avoid.

The isolation levels defined by the [[American National Standards Institute|ANSI]]/[[International Organization for Standardization|ISO]] [[SQL]] standard are listed as follows.

===Serializable===
This is the ''highest'' isolation level.

With a lock-based [[concurrency control]] DBMS implementation, [[serializability]] requires read and write locks (acquired on selected data) to be released at the end of the transaction.  Also ''range-locks'' must be acquired when a [[Select (SQL)|SELECT]] query uses a ranged ''WHERE'' clause, especially to avoid the ''phantom reads'' phenomenon (see below).

When using non-lock based concurrency control, no locks are acquired; however, if the system detects a ''write collision'' among several concurrent transactions, only one of them is allowed to commit.  See ''[[snapshot isolation]]'' for more details on this topic.

From : (Second Informal Review Draft) ISO/IEC 9075:1992, Database Language SQL- July 30, 1992:
''The execution of concurrent SQL-transactions at isolation level SERIALIZABLE is guaranteed to be serializable. A serializable execution is defined to be an execution of the operations of concurrently executing SQL-transactions that produces the same effect as some serial execution of those same SQL-transactions. A serial execution is one in which each SQL-transaction executes to completion before the next SQL-transaction begins.''

===Repeatable reads===
In this isolation level, a lock-based [[concurrency control]] DBMS implementation keeps read and write locks (acquired on selected data) until the end of the transaction.  However, ''range-locks'' are not managed, so '''''[[Isolation (database systems)#Phantom reads|phantom reads]]''''' can occur.

Also, write skew is possible when one transaction updates column to some color whereas competing transactions updates the same column to some other color(s). In serial execution of the transactions, you should end up with the whole column unicolored whereas repeatable read admits a mixture of colors.&lt;ref&gt;[https://wiki.postgresql.org/wiki/SSI#Simple_Write_Skew Postgresql wiki - SSI]&lt;/ref&gt;

===Read committed===
In this isolation level, a lock-based [[concurrency control]] DBMS implementation keeps write locks (acquired on selected data) until the end of the transaction, but read locks are released as soon as the [[Select (SQL)|SELECT]] operation is performed (so the ''non-repeatable reads'' phenomenon can occur in this isolation level, as discussed below). As in the previous level, ''range-locks'' are not managed.

Putting it in simpler words, read committed is an isolation level that guarantees that any data read is committed at the moment it is read. It simply restricts the reader from seeing any intermediate, uncommitted, 'dirty' read. It makes no promise whatsoever that if the transaction re-issues the read, it will find the same data; data is free to change after it is read.

===Read uncommitted===
This is the ''lowest'' isolation level. In this level, '''''[[Isolation (database systems)#Dirty reads|dirty reads]]''''' are allowed, so one transaction may see ''not-yet-committed'' changes made by other transactions.

Since each isolation level is stronger than those below, in that no higher isolation level allows an action forbidden by a lower one, the standard permits a DBMS to run a transaction at an isolation level stronger than that requested (e.g., a "Read committed" transaction may actually be performed at a "Repeatable read" isolation level).

==Default isolation level==
The ''default isolation level'' of different [[Database management system|DBMS]]'s varies quite widely. Most databases that feature transactions allow the user to set any isolation level. Some DBMS's also require additional syntax when performing a SELECT statement to acquire locks (e.g. ''SELECT ... FOR UPDATE'' to acquire exclusive write locks on accessed rows).

However, the definitions above have been criticized as being ambiguous, and as not accurately reflecting the isolation provided by many databases:

:This paper shows a number of weaknesses in the anomaly approach to defining isolation levels. The three ANSI phenomena are ambiguous, and even in their loosest interpretations do not exclude some anomalous behavior ... This leads to some counter-intuitive results. In particular, lock-based isolation levels have different characteristics than their ANSI equivalents. This is disconcerting because commercial database systems typically use locking implementations. Additionally, the ANSI phenomena do not distinguish between a number of types of isolation level behavior that are popular in commercial systems.&lt;ref name="sql-isolation"&gt;
{{cite web
| url = http://www.cs.umb.edu/~poneil/iso.pdf
| title = A Critique of ANSI SQL Isolation Levels
| accessdate = 29 July 2012 }}
&lt;/ref&gt;

There are also other criticisms concerning ANSI SQL's isolation definition, in that it encourages implementors to do "bad things":

:... it relies in subtle ways on an assumption that a locking schema is used for concurrency control, as opposed to an optimistic or multi-version concurrency scheme. This implies that the proposed semantics are ''ill-defined''.&lt;ref&gt;{{cite web
| accessdate = 2010-03-09
| publisher = DataStax
| location = www.DataStax.com
| title = Customer testimonials (SimpleGeo, CLOUDSTOCK 2010)
| author = salesforce
| date = 2010-12-06
| url = https://www.youtube.com/v/7J61pPG9j90?version=3
| quote = (see above at about 13:30 minutes of the webcast!)}}&lt;/ref&gt;

==Read phenomena==
The ANSI/ISO standard SQL 92 refers to three different ''read phenomena'' when Transaction 1 reads data that Transaction 2 might have changed.

In the following examples, two transactions take place. In the first, Query 1 is performed. Then, in the second transaction, Query 2 is performed and committed. Finally, in the first transaction, Query 1 is performed again.

The queries use the following data table:

{|class="wikitable"
|+ users
! id !! name !! age
|-
| 1  || Joe  || 20
|-
| 2  || Jill || 25
|}

===Dirty reads===
A ''dirty read'' (aka ''uncommitted dependency'') occurs when a transaction is allowed to read data from a row that has been modified by another running transaction and not yet committed.

Dirty reads work similarly to [[Isolation (database systems)#Non-repeatable reads|non-repeatable reads]]; however, the second transaction would not need to be committed for the first query to return a different result. The only thing that may be prevented in the READ UNCOMMITTED isolation level is updates appearing out of order in the results; that is, earlier updates will always appear in a result set before later updates.

In our example, Transaction 2 changes a row, but does not commit the changes.  Transaction 1 then reads the uncommitted data.  Now if Transaction 2 rolls back its changes (already read by Transaction 1) or updates different changes to the database, then the view of the data may be wrong in the records of Transaction 1.

{|style="font-size: 94%;"
|-
! Transaction 1
! Transaction 2
|-
|&lt;source lang="sql"&gt;
/* Query 1 */
SELECT age FROM users WHERE id = 1;
/* will read 20 */
&lt;/source&gt;
|
|-
|
|&lt;source lang="sql"&gt;
/* Query 2 */
UPDATE users SET age = 21 WHERE id = 1;
/* No commit here */
&lt;/source&gt;
|-
|&lt;source lang="sql"&gt;
/* Query 1 */
SELECT age FROM users WHERE id = 1;
/* will read 21 */
&lt;/source&gt;
|
|-
|
|&lt;source lang="sql"&gt;
ROLLBACK; /* lock-based DIRTY READ */
&lt;/source&gt;
|}

But in this case no row exists that has an id of 1 and an age of 21.

===Non-repeatable reads===
A ''non-repeatable read'' occurs, when during the course of a transaction, a row is retrieved twice and the values within the row differ between reads.

''Non-repeatable reads'' phenomenon may occur in a lock-based concurrency control method when read locks are not acquired when performing a [[Select (SQL)|SELECT]], or when the acquired locks on affected rows are released as soon as the SELECT operation is performed.  Under the [[multiversion concurrency control]] method, ''non-repeatable reads'' may occur when the requirement that a transaction affected by a [[commit conflict]] must roll back is relaxed.

{|style="font-size: 94%;"
|-
! Transaction 1
! Transaction 2
|-
|&lt;source lang="sql"&gt;
/* Query 1 */
SELECT * FROM users WHERE id = 1;
&lt;/source&gt;
|
|-
|
|&lt;source lang="sql"&gt;
/* Query 2 */
UPDATE users SET age = 21 WHERE id = 1;
COMMIT; /* in multiversion concurrency
   control, or lock-based READ COMMITTED */
&lt;/source&gt;
|-
|&lt;source lang="sql"&gt;
/* Query 1 */
SELECT * FROM users WHERE id = 1;
COMMIT; /* lock-based REPEATABLE READ */
&lt;/source&gt;
|}

In this example, Transaction 2 commits successfully, which means that its changes to the row with id 1 should become visible. However, Transaction 1 has already seen a different value for ''age'' in that row. At the SERIALIZABLE and REPEATABLE READ isolation levels, the DBMS must return the old value for the second SELECT. At READ COMMITTED and READ UNCOMMITTED, the DBMS may return the updated value; this is a non-repeatable read.

There are two basic strategies used to prevent non-repeatable reads. The first is to delay the execution of Transaction 2 until Transaction 1 has committed or rolled back. This method is used when locking is used, and produces the serial [[Schedule (computer science)|schedule]] '''T1, T2'''. A serial schedule exhibits ''repeatable reads'' behaviour.

In the other strategy, as used in ''[[multiversion concurrency control]]'', Transaction 2 is permitted to commit first, which provides for better concurrency. However, Transaction 1, which commenced prior to Transaction 2, must continue to operate on a past version of the database&amp;nbsp;— a snapshot of the moment it was started. When Transaction 1 eventually tries to commit, the DBMS checks if the result of committing Transaction 1 would be equivalent to the schedule '''T1, T2'''. If it is, then Transaction 1 can proceed. If it cannot be seen to be equivalent, however, Transaction 1 must roll back with a serialization failure.

Using a lock-based concurrency control method, at the REPEATABLE READ isolation mode, the row with ID = 1 would be locked, thus blocking Query 2 until the first transaction was committed or rolled back. In READ COMMITTED mode, the second time Query 1 was executed, the age would have changed.

Under multiversion concurrency control, at the SERIALIZABLE isolation level, both SELECT queries see a snapshot of the database taken at the start of Transaction 1. Therefore, they return the same data. However, if Transaction 1 then attempted to UPDATE that row as well, a serialization failure would occur and Transaction 1 would be forced to roll back.

At the READ COMMITTED isolation level, each query sees a snapshot of the database taken at the start of each query. Therefore, they each see different data for the updated row. No serialization failure is possible in this mode (because no promise of serializability is made), and Transaction 1 will not have to be retried.

===Phantom reads===
A ''phantom read'' occurs when, in the course of a transaction, two identical queries are executed, and the collection of rows returned by the second query is different from the first.

This can occur when ''[[range locks]]'' are not acquired on performing a ''[[Select (SQL)|SELECT]] ... WHERE'' operation.
The ''phantom reads'' anomaly is a special case of ''Non-repeatable reads'' when Transaction 1 repeats a ranged ''SELECT ... WHERE'' query and, between both operations, Transaction 2 creates (i.e. [[INSERT]]) new rows (in the target table) which fulfill that ''WHERE'' clause.

{|style="font-size: 95%;"
|-
! Transaction 1
! Transaction 2
|-
|&lt;source lang="sql"&gt;
/* Query 1 */
SELECT * FROM users
WHERE age BETWEEN 10 AND 30;
&lt;/source&gt;
|
|-
|
|&lt;source lang="sql"&gt;
/* Query 2 */
INSERT INTO users(id,name,age) VALUES ( 3, 'Bob', 27 );
COMMIT;
&lt;/source&gt;
|-
|&lt;source lang="sql"&gt;
/* Query 1 */
SELECT * FROM users
WHERE age BETWEEN 10 AND 30;
COMMIT;
&lt;/source&gt;
|
|}

Note that Transaction 1 executed the same query twice. If the highest level of isolation were maintained, the same set of rows should be returned both times, and indeed that is what is mandated to occur in a database operating at the SQL SERIALIZABLE isolation level. However, at the lesser isolation levels, a different set of rows may be returned the second time.

In the SERIALIZABLE isolation mode, Query 1 would result in all records with age in the range 10 to 30 being locked, thus Query 2 would block until the first transaction was committed. In REPEATABLE READ mode, the range would not be locked, allowing the record to be inserted and the second execution of Query 1 to include the new row in its results.

==Isolation Levels, Read Phenomena and Locks==

===Isolation Levels vs Read Phenomena===
{|class="wikitable"
! Isolation level !! Dirty reads !! Non-repeatable reads !! Phantoms
|-
| Read Uncommitted  || may occur || may occur || may occur
|-
| Read Committed  || - || may occur || may occur
|-
| Repeatable Read || - || - || may occur
|-
| Serializable || - || - || -
|}

Anomaly Serializable is not the same as Serializable. That is, it is necessary, but not sufficient that a Serializable schedule should be free of all three phenomena types.&lt;ref name="sql-isolation"/&gt;

"may occur" means that the isolation level suffers that phenomenon, while "-" means that it does not suffer it.

===Isolation Levels vs Lock Duration ===
{{Citation needed|reason=This is a still a mess. Write operations always place locks until commit, whatever isolation level is used. Queries (selects/reads) never place any kind of lock under Read Uncommitted isolation level. Locks placed by read operations (the only operations that are affected by the isolation level used) should not be referred as Read and Range, but as Data Locks and Predicate Locks. Write operations also place Data and Predicate locks, always until commit, whatever isolation level is used.|date=January 2014}}

In lock-based concurrency control, isolation level determines the duration that locks are held.&lt;br&gt;  '''"C"''' - Denotes that locks are held until the transaction commits.&lt;br&gt;
'''"S"''' - Denotes that the locks are held only during the currently executing statement.  Note that if locks are released after a statement, the underlying data could be changed by another transaction before the current transaction commits, thus creating a violation.

{|class="wikitable"
! Isolation level !! Write Operation !! Read Operation !! Range Operation (...where...)
|-
| Read Uncommitted  || S || S || S
|-
| Read Committed  || C || S || S
|-
| Repeatable Read || C || C || S
|-
| Serializable || C || C || C
|}

==See also==
* [[Atomicity (database systems)|Atomicity]]
* [[Consistency (database systems)|Consistency]]
* [[Durability (database systems)|Durability]]
* [[Lock (database)]]
* [[Optimistic concurrency control]]
* [[Relational Database Management System]]
* [[Snapshot isolation]]

==References==
{{Reflist}}

==External links==
* [http://docs.oracle.com/cd/B12037_01/server.101/b10743/toc.htm  Oracle® Database Concepts], [http://docs.oracle.com/cd/B12037_01/server.101/b10743/consist.htm#sthref1919 chapter 13 Data Concurrency and Consistency, Preventable Phenomena and Transaction Isolation Levels]
* [http://docs.oracle.com/cd/B19306_01/server.102/b14200/toc.htm Oracle® Database SQL Reference], [http://docs.oracle.com/cd/B19306_01/server.102/b14200/statements_10.htm#i2068385 chapter 19 SQL Statements: SAVEPOINT to UPDATE], [http://docs.oracle.com/cd/B19306_01/server.102/b14200/statements_10005.htm#i2067247 SET TRANSACTION]
&lt;!-- representations in api: java --&gt;
* in [[Java Database Connectivity|JDBC]]: [http://docs.oracle.com/javase/7/docs/api/java/sql/Connection.html#field_summary Connection constant fields], [http://docs.oracle.com/javase/7/docs/api/java/sql/Connection.html#getTransactionIsolation() Connection.getTransactionIsolation()], [http://docs.oracle.com/javase/7/docs/api/java/sql/Connection.html#setTransactionIsolation(int) Connection.setTransactionIsolation(int)]
* in [[Spring Framework]]: [http://static.springsource.org/spring/docs/current/javadoc-api/org/springframework/transaction/annotation/Transactional.html @Transactional], [http://static.springsource.org/spring/docs/current/javadoc-api/org/springframework/transaction/annotation/Isolation.html Isolation]
&lt;!-- representations in api: .NET_Framework --&gt;
* [http://www.bailis.org/blog/when-is-acid-acid-rarely/ P.Bailis. When is "ACID" ACID? Rarely]

{{Authority control}}

{{DEFAULTSORT:Isolation (Database Systems)}}
[[Category:Data management]]
[[Category:Transaction processing]]</text>
      <sha1>1tifb0noy4j1ylnpqhiecd8jyhmvewh</sha1>
    </revision>
  </page>
  <page>
    <title>Vocabulary-based transformation</title>
    <ns>0</ns>
    <id>3219147</id>
    <revision>
      <id>624461101</id>
      <parentid>618660500</parentid>
      <timestamp>2014-09-06T21:05:40Z</timestamp>
      <contributor>
        <username>Brenont</username>
        <id>4034676</id>
      </contributor>
      <minor />
      <comment>typo</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2089" xml:space="preserve">{{unreferenced|date=July 2013}}
In [[metadata]], a '''vocabulary-based transformation (VBT)''' is a transformation aided by the use of a [[semantic equivalence]] statements within a [[controlled vocabulary]].

Many organizations today require communication between two or more computers.  Although many standards exist to exchange data between computers such as [[HTML]] or [[email]], there is still much structured information that needs to be exchanged between computers that is not standardized.  The process of mapping one source of data into another is often a slow and labor-intensive process.

VBT is a possible way to avoid much of the time and cost of manual data mapping using traditional [[Extract, transform, load]] technologies.

== History ==

The term ''vocabulary-based transformation'' was first defined by Roy Shulte of the [[Gartner Group]] around May 2003 and appeared in annual "[[hype cycle|hype]]-cycle" for [[data integration|integration]].

== Application ==
VBT allows computer systems integrators to more automatically "look up" the definitions of data elements in a centralized [[data dictionary]] and use that definition and the equivalent mappings to transform that data element into a foreign [[namespace]].

The [[Web Ontology Language]] (OWL) language also support three [[semantic equivalence]] statements.

== Companies or products ==
* [[IONA Technologies]]
* [http://liaison.com/products/transform Contivo and Delta] by [http://liaison.com/ Liaison Technologies]
* enLeague Systems
* ItemField
* Unicorn Solutions
* Vitria Technology
* Zonar

== See also ==
* [[metadata]]
* [[Controlled vocabulary]]
* [[Data dictionary]]
* [[Semantic spectrum]]
* [[Semantic equivalence]]
* [[XSLT]]
* [[Enterprise Application Integration]]

==External links==
* [http://www.gartner.com/6_help/glossary/GlossaryV.jsp Gartner Glossary of Terms] Gartner definition Vocabulary-based transformation
* [http://www.sun.com/service/openwork/analyst/Gartner_Hype_Cycle.pdf Gartner Hype Cycle 2003]

{{DEFAULTSORT:Vocabulary-Based Transformation}}
[[Category:Data management]]</text>
      <sha1>ndf9nng75ldqegmz2fn6oul1cd4olir</sha1>
    </revision>
  </page>
  <page>
    <title>Enterprise information system</title>
    <ns>0</ns>
    <id>1010494</id>
    <revision>
      <id>744923318</id>
      <parentid>744923314</parentid>
      <timestamp>2016-10-18T08:01:50Z</timestamp>
      <contributor>
        <username>ClueBot NG</username>
        <id>13286072</id>
      </contributor>
      <minor />
      <comment>Reverting possible vandalism by [[Special:Contribs/43.245.120.161|43.245.120.161]] to version by Mild Bill Hiccup. [[WP:CBFP|Report False Positive?]] Thanks, [[WP:CBNG|ClueBot NG]]. (2800771) (Bot)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4454" xml:space="preserve">An '''enterprise information system''' ('''EIS''') is any kind of [[information system]] which improves the functions of an enterprise business processes by integration. This means typically offering high quality of service, dealing with large volumes of [[data]] and capable of supporting some large and possibly complex [[organization]] or enterprise. An EIS must be able to be used by all parts and all levels of an enterprise.&lt;ref name="eidvtai" /&gt;

The word ''enterprise'' can have various connotations. Frequently the term is used only to refer to very large organizations such as multi-national companies or public-sector organizations. However, the term may be used to mean virtually anything, by virtue of it having become the latest corporate-speak [[buzzword]].{{Citation needed|date=June 2016}}

==Purpose==
Enterprise information systems provide a technology platform that enables organizations to [[Enterprise integration|integrate]] and coordinate their [[business processes]] on a robust foundation. An EIS is currently used in conjunction with [[customer relationship management]] and [[supply chain management]] to automate business processes.&lt;ref name="eidvtai" /&gt;  An enterprise information system provides a single system that is central to the organization that ensures information can be shared across all functional levels and management [[hierarchies]].

An EIS can be used to increase business [[productivity]] and reduce service cycles, [[product development]] cycles and marketing life cycles.&lt;ref name="eidvtai"&gt;{{cite book |title=Enterprise Information Systems: Contemporary Trends and Issues |last=Olson |first=David L. |author2=Subodh Kesharwani |year=2010 |publisher=World Scientific |isbn=9814273163 |pages=2, 13–16 |url=https://books.google.com/books?id=-AwDAp7Fe2UC |accessdate=20 August 2013}}&lt;/ref&gt;  It may be used to amalgamate existing applications. Other outcomes include higher [[operational efficiency]] and cost savings.&lt;ref name="eidvtai" /&gt;

Financial value is not usually a direct outcome from the implementation of an enterprise information system.&lt;ref name="eiscmta"&gt;{{cite book |title=Enterprise Information Systems: Concepts, Methodologies, Tools and Applications |author=Information Resources Management Association |year=2010 |publisher=Idea Group Inc |isbn=1616928530 |pages=38, 43 |url=https://books.google.com/books?id=hpc6-SfS2scC |accessdate=20 August 2013}}&lt;/ref&gt;

==Design stage==
At the design stage the main characteristic of EIS efficiency evaluation is the probability of timely delivery of various messages such as command, service, etc.&lt;ref&gt;{{cite journal |title=ОЦЕНКА ХАРАКТЕРИСТИК ФУНКЦИОНИРОВАНИЯ КОРПОРАТИВНЫХ ИНФОРМАЦИОННЫХ СИСТЕМ С НЕОДНОРОДНОЙ НАГРУЗКОЙ |trans-title=Efficiency Evaluation of Enterprise Information Systems with Non-uniform Load |language=ru |url=http://ntv.ifmo.ru/en/article/13881/ocenka_harakteristik_funkcionirovaniya_korporativnyhinformacionnyh_sistem_s_neodnorodnoy_nagruzkoy.htm |author1=Kalinin I.V. |author2=Maharevs E. |author3=Muravyeva-Vitkovskaya L.A. |journal=Scientific and Technical Journal of Information Technologies, Mechanics and Optics |volume=15 |issue=5 |pages=863–868 |year=2015 |doi=10.17586/2226-1494-2015-15-5-863-868}}&lt;/ref&gt;

==Information systems==
{{main|Information systems}}
Enterprise systems create a standard [[data structure]] and are invaluable in eliminating the problem of information fragmentation caused by multiple information systems within an organization.  An EIS differentiates itself from [[legacy system]]s in that it self-transactional, self-helping and adaptable to general and specialist conditions.&lt;ref name="eidvtai" /&gt;  Unlike an enterprise information system, legacy systems are limited to department wide communications.&lt;ref name="eiscmta" /&gt;

A typical enterprise information system would be housed in one or more [[data center]]s, would run [[enterprise software]], and could include applications that typically cross organizational borders such as [[content management systems]].

==See also==
{{Portal|Computing|Business}}
*[[Executive information system]]
*[[Management information system]]
*[[Enterprise planning systems]]
*[[Enterprise software]]

==References==
{{Reflist}}

[[Category:Data management]]
[[Category:Enterprise architecture]]
[[Category:Enterprise modelling]]
[[Category:Website management]]</text>
      <sha1>6pcqoeu98ktpsd3ip3zjpo27xztzest</sha1>
    </revision>
  </page>
  <page>
    <title>Operational data store</title>
    <ns>0</ns>
    <id>1740486</id>
    <revision>
      <id>742578922</id>
      <parentid>742571257</parentid>
      <timestamp>2016-10-04T14:51:51Z</timestamp>
      <contributor>
        <username>Mindmatrix</username>
        <id>160367</id>
      </contributor>
      <minor />
      <comment>Reverted edits by [[Special:Contribs/202.74.243.46|202.74.243.46]] ([[User talk:202.74.243.46|talk]]) to last version by Mindmatrix</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2507" xml:space="preserve">An '''operational data store''' (or "'''ODS'''") is a [[database]] designed to [[data integration|integrate data]] from multiple sources for additional operations on the data. Unlike a master data store, the data is not passed back to [[operational system]]s. It may be passed for further operations and to the [[data warehouse]] for reporting. 

Because the [[data]] originate from multiple sources, the integration often involves [[data cleaning|cleaning]], resolving redundancy and checking against [[business rule]]s for [[data integrity|integrity]].  An ODS is usually designed to contain low-level or atomic (indivisible) data (such as transactions and prices) with limited history that is captured "real time" or "near real time" as opposed to the much greater volumes of data stored in the data warehouse generally on a less-frequent basis.

==General Use==
The general purpose of an ODS is to integrate data from disparate source systems in a single structure, using [[data integration]] technologies like [[Data Virtualization|data virtualization]], [[Federated database system|data federation]], or [[Extract, transform, load|extract, transform, and load]]. This will allow operational access to the data for operational reporting, [[Master Data|master data or reference data management]].

An ODS is not a replacement or substitute for a [[data warehouse]] but in turn could become a source.

==See also==
* Some examples of ODS Architecture Patterns can be found in the article [[Architectural pattern (computer science)#Examples|Architecture Patterns]].

==Publications==
* {{cite book |last1=Inmon |first1=William |author1-link=Bill Inmon |title=Building the Operational Data Store |edition=2nd |location=New York |publisher=[[John Wiley &amp; Sons]] |year=1999 |isbn=0-471-32888-X}}

==External links==
*[[Architectural pattern (computer science)#Examples|ODS Architecture Patterns (EA Reference Architecture)]]
* [http://www.dmreview.com/issues/19980701/469-1.html Bill Inmon Information Management article on ODS]
* [http://www.information-management.com/issues/20000101/1749-1.html Bill Inmon Information Management article on the five classes of ODS]
* [http://www.intelsols.com/documents/Imhoff_10-02.pdf Claudia Imhoff Information Management article on ODS] PDF

{{Data warehouse}}

== See also ==
{{Wikipedia books|Enterprise Architecture}}
* [[Enterprise architecture]]

{{DEFAULTSORT:Operational Data Store}}
[[Category:Data management]]
[[Category:Data warehousing]]

{{database-stub}}</text>
      <sha1>m9slhhsw4itn7u6gobmj4y1grkatoud</sha1>
    </revision>
  </page>
  <page>
    <title>Disaster recovery</title>
    <ns>0</ns>
    <id>640655</id>
    <revision>
      <id>762835821</id>
      <parentid>754651773</parentid>
      <timestamp>2017-01-31T00:30:06Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor />
      <comment>Reformat 1 archive link. [[User:Green Cardamom/WaybackMedic_2.1|Wayback Medic 2.1]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="14597" xml:space="preserve">{{About|1=[[business continuity planning]]|2=societal disaster recovery|3=emergency management}}
{{Other uses|DR (disambiguation)}}

{{merge to|business continuity|date=June 2015}}

'''Disaster recovery''' (DR) involves a set of policies and procedures to enable the recovery or continuation of vital technology infrastructure and systems following a [[natural disaster|natural]] or [[man-made hazards|human-induced]] [[disaster]]. Disaster recovery focuses on the IT or [[technology systems]] supporting critical business functions,&lt;ref&gt;[http://continuity.georgetown.edu/dr/ ''Systems and Operations Continuity: Disaster Recovery.''] Georgetown University. University Information Services. Retrieved 3 August 2012.&lt;/ref&gt; as opposed to [[business continuity]], which involves keeping all essential aspects of a business functioning despite significant disruptive events.  Disaster recovery is therefore a subset of business continuity.&lt;ref&gt;[http://www-304.ibm.com/partnerworld/gsd/solutiondetails.do?solution=44832&amp;expand=true&amp;lc=en ''Disaster Recovery and Business Continuity, version 2011.''] {{webarchive |url=https://web.archive.org/web/20130111203921/http://www-304.ibm.com/partnerworld/gsd/solutiondetails.do?solution=44832&amp;expand=true&amp;lc=en |date=January 11, 2013 }} IBM. Retrieved 3 August 2012.&lt;/ref&gt;

==History==
Disaster recovery developed in the mid- to late 1970s as computer center managers began to recognize the dependence of their organizations on their computer systems. At that time, most systems were [[Batch processing|batch]]-oriented [[Mainframe computer|mainframe]]s which in many cases could be [[downtime|down]] for a number of days before significant damage would be done to the organization.

As awareness of the potential business disruption that would follow an IT-related disaster, the disaster recovery industry developed to provide backup computer centers, with Sun Information Systems (which later became Sungard Availability Services) becoming the first major US commercial hot site vendor, established in 1978 in Philadelphia.

During the 1980s and 90s, customer awareness and industry both grew rapidly, driven by the advent of open systems and [[Real-time computing|real-time processing]] which increased the dependence of organizations on their IT systems. Regulations mandating business continuity and disaster recovery plans for organizations in various sectors of the economy, imposed by the authorities and by business partners, increased the demand and led to the availability of commercial disaster recovery services, including mobile data centers delivered to a suitable recovery location by truck.

With the rapid growth of the [[Internet]] through the late 1990s and into the 2000s, organizations of all sizes became further dependent on the continuous [[availability]] of their IT systems, with some organizations setting objectives of 2, 3, 4 or 5 nines (99.999%) availability of critical systems.{{citation needed|date=March 2016}} This increasing dependence on IT systems, as well as increased awareness from large-scale disasters such as tsunami, earthquake, flood, and volcanic eruption, spawned disaster recovery-related products and services, ranging from [[high-availability]] solutions to [[hot-site]] facilities.  Improved networking meant critical IT services could be served remotely, hence on-site recovery became less important.

The rise of cloud computing since 2010 continues that trend: nowadays, it matters even less where computing services are physically served, just so long as the network itself is sufficiently reliable (a separate issue, and less of a concern since modern networks are highly resilient by design).  'Recovery as a Service' (RaaS) is one of the security features or benefits of cloud computing being promoted by the Cloud Security Alliance.&lt;ref&gt;[https://cloudsecurityalliance.org/download/secaas-category-9-bcdr-implementation-guidance/ ''SecaaS Category 9 // BCDR Implementation Guidance''] CSA, retrieved 14 July 2014.&lt;/ref&gt;

===Classification of disasters===
Disasters can be classified into two broad categories. The first is natural disasters such as floods, hurricanes, tornadoes or earthquakes. While preventing a natural disaster is impossible, risk management measures such as avoiding disaster-prone situations and good planning can help. The second category is man made disasters, such as hazardous material spills, infrastructure failure, bio-terrorism, and disastrous IT bugs or failed change implementations. In these instances, surveillance, testing and mitigation planning are invaluable.

==Importance of disaster recovery planning==
Recent research supports the idea that implementing a more holistic pre-disaster planning approach is more cost-effective in the long run. Every $1 spent on hazard mitigation(such as a [[disaster recovery plan]]) saves society $4 in response and recovery costs.&lt;ref&gt;{{cite web|first=Partnership for Disaster Resilience|title=Post-Disaster Recovery Planning Forum: How-To Guide|url=http://nthmp.tsunami.gov/Minutes/oct-nov07/post-disaster_recovery_planning_forum_uo-csc-2.pdf|publisher=University of Oregon's Community Service Center|accessdate=2013-05-23}}&lt;/ref&gt;

As [[Information technology|IT systems]] have become increasingly critical to the smooth operation of a company, and arguably the economy as a whole, the importance of ensuring the continued operation of those systems, and their rapid recovery, has increased. For example, of companies that had a major loss of business data, 43% never reopen and 29% close within two years. As a result, preparation for continuation or recovery of systems needs to be taken very seriously. This involves a significant investment of time and money with the aim of ensuring minimal losses in the event of a disruptive event.&lt;ref&gt;{{cite web|url=http://www.ready.gov/business/implementation/IT|title=IT Disaster Recovery Plan|date=25 October 2012|publisher=FEMA|accessdate=11 May 2013}}&lt;/ref&gt;

==Control measures==
Control measures are steps or mechanisms that can reduce or eliminate various threats for organizations. Different types of measures can be included in disaster recovery plan (DRP).

Disaster recovery planning is a subset of a larger process known as business continuity planning and includes planning for resumption of applications, data, hardware, electronic communications (such as networking) and other IT infrastructure. A business continuity plan (BCP) includes planning for non-IT related aspects such as key personnel, facilities, crisis communication and reputation protection, and should refer to the disaster recovery plan (DRP) for IT related infrastructure recovery / continuity.

IT disaster recovery control measures can be classified into the following three types:
# Preventive measures - Controls aimed at preventing an event from occurring.
# Detective measures - Controls aimed at detecting or discovering unwanted events.
# Corrective measures - Controls aimed at correcting or restoring the system after a disaster or an event.

Good disaster recovery plan measures dictate that these three types of controls be documented and exercised regularly using so-called "DR tests".

==Strategies==
Prior to selecting a disaster recovery strategy, a disaster recovery planner first refers to their organization's business continuity plan which should indicate the key metrics of [[recovery point objective]] (RPO) and [[recovery time objective]] (RTO) for various business processes (such as the process to run payroll, generate an order, etc.). The metrics specified for the business processes are then mapped to the underlying IT systems and infrastructure that support those processes.&lt;ref&gt;Gregory, Peter. CISA Certified Information Systems Auditor All-in-One Exam Guide, 2009. ISBN 978-0-07-148755-9. Page 480.&lt;/ref&gt;

Incomplete RTOs and RPOs can quickly derail a disaster recovery plan. Every item in the DR plan requires a defined recovery point and time objective, as failure to create them may lead to significant problems that can extend the disaster’s impact.&lt;ref&gt;{{cite web|url=http://content.dell.com/us/en/enterprise/d/large-business/mistakes-that-kill-disaster.aspx |title=Five Mistakes That Can Kill a Disaster Recovery Plan |publisher=Dell.com |accessdate=2012-06-22 |deadurl=yes |archiveurl=https://web.archive.org/web/20130116112225/http://content.dell.com/us/en/enterprise/d/large-business/mistakes-that-kill-disaster.aspx |archivedate=2013-01-16 |df= }}&lt;/ref&gt; Once the RTO and RPO metrics have been mapped to IT infrastructure, the DR planner can determine the most suitable recovery strategy for each system.  The organization ultimately sets the IT budget and therefore the RTO and RPO metrics need to fit with the available budget. While most business unit heads would like zero data loss and zero time loss, the cost associated with that level of protection may make the desired high availability solutions impractical. A [[cost-benefit analysis]] often dictates which disaster recovery measures are implemented.

Traditionally, a disaster recovery system involved cutover or switch-over recovery systems.{{citation needed|date=April 2016}} Such measures would allow an organization to preserve its technology and information, by having a remote disaster recovery location that produced backups on a regular basis. However, this strategy proved to be expensive and time-consuming. Therefore, more affordable and effective cloud-based systems were introduced.

Some of the most common strategies for [[Data recovery|data protection]] include: 
* backups made to tape and sent off-site at regular intervals
* backups made to disk on-site and automatically copied to off-site disk, or made directly to off-site disk
* replication of data to an off-site location, which overcomes the need to restore the data (only the systems then need to be restored or synchronized), often making use of [[storage area network]] (SAN) technology
* Private Cloud solutions which replicate the management data (VMs, Templates and disks) into the storage domains which are part of the private cloud setup. These management data are configured as an xml representation called OVF (Open Virtualization Format), and can be restored once a disaster occurs.
* Hybrid Cloud solutions that replicate both on-site and to off-site data centers.  These solutions provide the ability to instantly fail-over to local on-site hardware, but in the event of a physical disaster, servers can be brought up in the cloud data centers as well.
* the use of high availability systems which keep both the data and system replicated off-site, enabling continuous access to systems and data, even after a disaster (often associated with [[cloud storage]])&lt;ref&gt;{{cite web|url=http://www.inc.com/guides/201106/how-to-use-the-cloud-as-a-disaster-recovery-strategy.html|title=How to Use the Cloud as a Disaster Recovery Strategy|last=Brandon|first=John|date=23 June 2011|publisher=Inc. |accessdate=11 May 2013}}&lt;/ref&gt;

In many cases, an organization may elect to use an outsourced disaster recovery provider to provide a stand-by site and systems rather than using their own remote facilities, increasingly via [[cloud computing]].

In addition to preparing for the need to recover systems, organizations also implement precautionary measures with the objective of preventing a disaster in the first place. These may include: 
* local mirrors of systems and/or data and use of disk protection technology such as [[RAID]]
* surge protectors — to minimize the effect of power surges on delicate electronic equipment
* use of an [[uninterruptible power supply]] (UPS) and/or backup generator to keep systems going in the event of a power failure
* fire prevention/mitigation systems such as alarms and fire extinguishers
* anti-virus software and other security measures

==See also==
* [[Backup site]]
* [[High availability]]
* [[Continuous data protection]]
* [[Data recovery]]
* [[Emergency management]]
* [[IT service continuity]]
* [[Remote backup service]]
* [[Seven tiers of disaster recovery]]
* [[Virtual tape library]]

==References==
{{reflist}}

==Further reading==
* ISO/IEC 22301:2012 (replacement of BS-25999:2007) Societal Security - Business Continuity Management Systems - Requirements
* ISO/IEC 27001:2013 (replacement of ISO/IEC 27001:2005 [formerly BS 7799-2:2002]) Information Security Management System
* ISO/IEC 27002:2013 (replacement of ISO/IEC 27002:2005 [renumbered ISO17799:2005]) Information Security Management - Code of Practice
* ISO/IEC 22399:2007 Guideline for incident preparedness and operational continuity management
* ISO/IEC 24762:2008 Guidelines for information and communications technology disaster recovery services
* IWA 5:2006 Emergency Preparedness—British Standards Institution --
* BS 25999-1:2006 Business Continuity Management Part 1: Code of practice
* BS 25999-2:2007 Business Continuity Management Part 2: Specification
* BS 25777:2008 Information and communications technology continuity management - Code of practice—Others --
* "A Guide to Business Continuity Planning" by James C. Barnes
* "Business Continuity Planning", A Step-by-Step Guide with Planning Forms on CDROM by Kenneth L Fulmer
* "Disaster Survival Planning: A Practical Guide for Businesses" by Judy Bell
* ICE Data Management (In Case of Emergency) made simple - by MyriadOptima.com
* Harney, J.(2004). Business continuity and disaster recovery: Back up or shut down.
* AIIM E-Doc Magazine, 18(4), 42-48.
* Dimattia, S. (November 15, 2001).Planning for Continuity. Library Journal,32-34.

==External links==
* [https://www.ready.gov/business/implementation/IT IT Disaster Recovery Plan from Ready.gov]
&lt;!--========================({{No More Links}})============================
 | PLEASE BE CAUTIOUS IN ADDING MORE LINKS TO THIS ARTICLE. WIKIPEDIA |
 | IS NOT A COLLECTION OF LINKS NOR SHOULD IT BE USED FOR ADVERTISING. |
 | |
 | Excessive or inappropriate links WILL BE DELETED. |
 | See [[Wikipedia:External links]] &amp; [[Wikipedia:Spam]] for details. |
 | |
 | If there are already plentiful links, please propose additions or |
 | replacements on this article's discussion page, or submit your link |
 | to the relevant category at the Open Directory Project (dmoz.org) |
 | and link back to that category using the {{dmoz}} template. |
 =======================({{No More Links}})=============================--&gt;

{{Disasters}}

{{DEFAULTSORT:Disaster Recovery}}
[[Category:Disaster recovery]]
[[Category:Data management]]
[[Category:Backup]]
[[Category:IT risk management]]</text>
      <sha1>0l2r2dhyomkgwomfpdehhr5szvbngap</sha1>
    </revision>
  </page>
  <page>
    <title>Concurrency control</title>
    <ns>0</ns>
    <id>217356</id>
    <revision>
      <id>730186339</id>
      <parentid>711854345</parentid>
      <timestamp>2016-07-17T09:23:28Z</timestamp>
      <contributor>
        <username>Ewen</username>
        <id>3610</id>
      </contributor>
      <minor />
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="32811" xml:space="preserve">{{pp-pc1}}
In [[information technology]] and [[computer science]],  especially in the fields of [[computer programming]], [[operating systems]], [[multiprocessor]]s, and [[database]]s, '''concurrency control''' ensures that correct results for [[Concurrent computing|concurrent]] operations are generated, while getting those results as quickly as possible.

Computer systems, both [[software]] and [[computer hardware|hardware]], consist of modules, or components. Each component is designed to operate correctly, i.e., to obey or to meet certain consistency rules. When components that operate concurrently interact by messaging or by sharing accessed data (in [[Computer memory|memory]] or [[Computer data storage|storage]]), a certain component's consistency may be violated by another component. The general area of concurrency control provides rules, methods, design methodologies, and [[Scientific theory|theories]] to maintain the consistency of components operating concurrently while interacting, and thus the consistency and correctness of the whole system. Introducing concurrency control into a system means applying operation constraints which typically result in some performance reduction. Operation consistency and correctness should be achieved with as good as possible efficiency, without reducing performance below reasonable levels. Concurrency control can require significant additional complexity and overhead in a [[concurrent algorithm]] compared to the simpler [[sequential algorithm]].

For example, a failure in concurrency control can result in [[data corruption]] from [[Torn data-access operation|torn read or write operations]].

==Concurrency control in databases==
'''Comments:'''
# This section is applicable to all transactional systems, i.e., to all systems that use ''[[database transaction]]s'' (''atomic transactions''; e.g., transactional objects in [[Systems management]] and in networks of [[smartphone]]s which typically implement private, dedicated database systems), not only general-purpose [[database management system]]s (DBMSs).
# DBMSs need to deal also with concurrency control issues not typical just to database transactions but rather to operating systems in general. These issues (e.g., see ''[[Concurrency control#Concurrency control in operating systems|Concurrency control in operating systems]]'' below) are out of the scope of this section.

Concurrency control in [[Database management system]]s (DBMS; e.g., [[#Bern87|Bernstein et al. 1987]], [[#Weikum01|Weikum and Vossen 2001]]), other [[database transaction|transactional]] objects, and related distributed applications (e.g., [[Grid computing]] and [[Cloud computing]]) ensures that ''[[database transaction]]s'' are performed [[Concurrency (computer science)|concurrently]] without violating the [[data integrity]] of the respective [[database]]s. Thus concurrency control is an essential element for correctness in any system where two database transactions or more, executed with time overlap, can access the same data, e.g., virtually in any general-purpose database system. Consequently, a vast body of related research has been accumulated since database systems emerged in the early 1970s. A well established concurrency control [[Scientific theory|theory]] for database systems is outlined in the references mentioned above: [[Serializability|serializability theory]], which allows to effectively design and analyze concurrency control methods and mechanisms. An alternative theory for concurrency control of atomic transactions over [[abstract data type]]s is presented in ([[#Lynch1993|Lynch et al. 1993]]), and not utilized below. This theory is more refined, complex, with a wider scope, and has been less utilized in the Database literature than the classical theory above. Each theory has its pros and cons, emphasis and [[insight]]. To some extent they are complementary, and their merging may be useful.

To ensure correctness, a DBMS usually guarantees that only ''[[Serializability|serializable]]'' transaction [[Schedule (computer science)|schedule]]s are generated, unless ''serializability'' is [[Serializability#Relaxing serializability|intentionally relaxed]] to increase performance, but only in cases where application correctness is not harmed. For maintaining correctness in cases of failed (aborted) transactions (which can always happen for many reasons) schedules also need to have the ''[[Serializability#Correctness - recoverability|recoverability]]'' (from abort) property. A DBMS also guarantees that no effect of ''committed'' transactions is lost, and no effect of ''aborted'' ([[Rollback (data management)|rolled back]]) transactions remains in the related database. Overall transaction characterization is usually summarized by the [[ACID]] rules below. As databases have become [[Distributed database|distributed]], or needed to cooperate in distributed environments (e.g., [[Federated database]]s in the early 1990, and [[Cloud computing]] currently), the effective distribution of concurrency control mechanisms has received special attention.

===Database transaction and the ACID rules===
{{main|Database transaction|ACID}}
The concept of a ''database transaction'' (or ''atomic transaction'') has evolved in order to enable both a well understood database system behavior in a faulty environment where crashes can happen any time, and ''recovery'' from a crash to a well understood database state. A database transaction is a unit of work, typically encapsulating a number of operations over a database (e.g., reading a database object, writing, acquiring lock, etc.), an abstraction supported in database and also other systems. Each transaction has well defined boundaries in terms of which program/code executions are included in that transaction (determined by the transaction's programmer via special transaction commands). Every database transaction obeys the following rules (by support in the database system; i.e., a database system is designed to guarantee them for the transactions it runs):
*'''[[Atomicity (database systems)|Atomicity]]''' - Either the effects of all or none of its operations remain ("all or nothing" semantics) when a [[database transaction|transaction]] is completed (''committed'' or ''aborted'' respectively). In other words, to the outside world a committed transaction appears (by its effects on the database) to be indivisible (atomic), and an aborted transaction does not affect the database at all.
*'''[[Consistency (database systems)|Consistency]]''' - Every transaction must leave the database in a consistent (correct) state, i.e., maintain the predetermined integrity rules of the database (constraints upon and among the database's objects). A transaction must transform a database from one consistent state to another consistent state (however, it is the responsibility of the transaction's programmer to make sure that the transaction itself is correct, i.e., performs correctly what it intends to perform (from the application's point of view) while the predefined integrity rules are enforced by the DBMS). Thus since a database can be normally changed only by transactions, all the database's states are consistent.
*'''[[Isolation (database systems)|Isolation]]''' - Transactions cannot interfere with each other (as an end result of their executions). Moreover, usually (depending on concurrency control method) the effects of an incomplete transaction are not even visible to another transaction. Providing isolation is the main goal of concurrency control.
*'''[[Durability (database systems)|Durability]]''' - Effects of successful (committed) transactions must persist through [[Crash (computing)|crash]]es (typically by recording the transaction's effects and its commit event in a [[non-volatile memory]]).

The concept of atomic transaction has been extended during the years to what has become [[Business transaction management|Business transactions]] which actually implement types of [[Workflow]] and are not atomic. However also such enhanced transactions typically utilize atomic transactions as components.

===Why is concurrency control needed?===
If transactions are executed ''serially'', i.e., sequentially with no overlap in time, no transaction concurrency exists. However, if concurrent transactions with interleaving operations are allowed in an uncontrolled manner, some unexpected, undesirable result may occur, such as:
# The lost update problem: A second transaction writes a second value of a data-item (datum) on top of a first value written by a first concurrent transaction, and the first value is lost to other transactions running concurrently which need, by their precedence, to read the first value. The transactions that have read the wrong value end with incorrect results.
# The dirty read problem: Transactions read a value written by a transaction that has been later aborted. This value disappears from the database upon abort, and should not have been read by any transaction ("dirty read"). The reading transactions end with incorrect results.
# The incorrect summary problem: While one transaction takes a summary over the values of all the instances of a repeated data-item, a second transaction updates some instances of that data-item. The resulting summary does not reflect a correct result for any (usually needed for correctness) precedence order between the two transactions (if one is executed before the other), but rather some random result, depending on the timing of the updates, and whether certain update results have been included in the summary or not.

Most high-performance transactional systems need to run transactions concurrently to meet their performance requirements. Thus, without concurrency control such systems can neither provide correct results nor maintain their databases consistent.

===Concurrency control mechanisms===

====Categories====
The main categories of concurrency control mechanisms are:
* '''[[Optimistic concurrency control|Optimistic]]''' - Delay the checking of whether a transaction meets the isolation and other integrity rules (e.g., [[serializability]] and [[Serializability#Correctness - recoverability|recoverability]]) until its end, without blocking any of its (read, write) operations ("...and be optimistic about the rules being met..."), and then abort a transaction to prevent the violation, if the desired rules are to be violated upon its commit. An aborted transaction is immediately restarted and re-executed, which incurs an obvious overhead (versus executing it to the end only once). If not too many transactions are aborted, then being optimistic is usually a good strategy.
* '''Pessimistic''' - Block an operation of a transaction, if it may cause violation of the rules, until the possibility of violation disappears. Blocking operations is typically involved with performance reduction.
*'''Semi-optimistic''' - Block operations in some situations,  if they may cause violation of some rules, and do not block in other situations while delaying rules checking (if needed) to transaction's end, as done with optimistic.

Different categories provide different performance, i.e., different average transaction completion rates (''throughput''), depending on transaction types mix, computing level of parallelism, and other factors. If selection and knowledge about trade-offs are available, then category and method should be chosen to provide the highest performance.

The mutual blocking between two transactions (where each one blocks the other) or more results in a [[deadlock]], where the transactions involved are stalled and cannot reach completion. Most non-optimistic mechanisms (with blocking) are prone to deadlocks which are resolved by an intentional abort of a stalled transaction (which releases the other transactions in that deadlock), and its immediate restart and re-execution. The likelihood of a deadlock is typically low.

Blocking, deadlocks, and aborts all result in performance reduction, and hence the trade-offs between the categories.

====Methods====
Many methods for concurrency control exist. Most of them can be implemented within either main category above. The major methods,&lt;ref name=Bern2009&gt;[[Phil Bernstein|Philip A. Bernstein]], Eric Newcomer (2009): [http://www.elsevierdirect.com/product.jsp?isbn=9781558606234 ''Principles of Transaction Processing'', 2nd Edition],  Morgan Kaufmann (Elsevier), June 2009, ISBN 978-1-55860-623-4 (page 145)&lt;/ref&gt; which have each many variants, and in some cases may overlap or be combined, are:
#Locking (e.g., '''[[Two-phase locking]]''' - 2PL) - Controlling access to data by [[Lock (computer science)|locks]] assigned to the data. Access of a transaction to a data item (database object) locked by another transaction may be blocked (depending on lock type and access operation type) until lock release.
#'''Serialization [[Serializability#Testing conflict serializability|graph checking]]''' (also called Serializability, or Conflict, or Precedence graph checking) - Checking for [[Cycle (graph theory)|cycles]] in the schedule's [[Directed graph|graph]] and breaking them by aborts.
#'''[[Timestamp-based concurrency control|Timestamp ordering]]''' (TO) - Assigning timestamps to transactions, and controlling or checking access to data by timestamp order.
#'''[[Commitment ordering]]''' (or Commit ordering; CO) - Controlling or checking transactions' chronological order of commit events to be compatible with their respective [[Serializability#Testing conflict serializability|precedence order]].

Other major concurrency control types that are utilized in conjunction with the methods above include:

* '''[[Multiversion concurrency control]]''' (MVCC) - Increasing concurrency and performance by generating a new version of a database object each time the object is written, and allowing transactions' read operations of several last relevant versions (of each object) depending on scheduling method.
* '''[[Index locking|Index concurrency control]]''' - Synchronizing access operations to [[Index (database)|index]]es, rather than to user data. Specialized methods provide substantial performance gains.
* '''Private workspace model''' ('''Deferred update''') - Each transaction maintains a private workspace for its accessed data, and its changed data become visible outside the transaction only upon its commit (e.g., [[#Weikum01|Weikum and Vossen 2001]]). This model provides a different concurrency control behavior with benefits in many cases.

The most common mechanism type in database systems since their early days in the 1970s has been ''[[Two-phase locking|Strong strict Two-phase locking]]'' (SS2PL; also called ''Rigorous scheduling'' or ''Rigorous 2PL'') which is a special case (variant) of both [[Two-phase locking]] (2PL) and [[Commitment ordering]] (CO). It is pessimistic. In spite of its long name (for historical reasons) the idea of the '''SS2PL''' mechanism is simple: "Release all locks applied by a transaction only after the transaction has ended." SS2PL (or Rigorousness) is also the name of the set of all schedules that can be generated by this mechanism, i.e., these are SS2PL (or Rigorous) schedules, have the SS2PL (or Rigorousness) property.

===Major goals of concurrency control mechanisms===
Concurrency control mechanisms firstly need to operate correctly, i.e., to maintain each transaction's integrity rules (as related to concurrency; application-specific integrity rule are out of the scope here) while transactions are running concurrently, and thus the integrity of the entire transactional system. Correctness needs to be achieved with as good performance as possible. In addition, increasingly a need exists to operate effectively while transactions are [[Distributed transaction|distributed]] over [[Process (computing)|processes]], [[computer]]s, and [[computer network]]s. Other subjects that may affect concurrency control are [[Data recovery|recovery]] and [[Replication (computer science)|replication]].

====Correctness====

=====Serializability=====
{{Main|Serializability}}

For correctness, a common major goal of most concurrency control mechanisms is generating [[Schedule (computer science)|schedule]]s with the ''[[Serializability]]'' property. Without serializability undesirable phenomena may occur, e.g., money may disappear from accounts, or be generated from nowhere. '''Serializability''' of a schedule means equivalence (in the resulting database values) to some ''serial'' schedule with the same transactions (i.e., in which transactions are sequential with no overlap in time, and thus completely isolated from each other: No concurrent access by any two transactions to the same data is possible). Serializability is considered the highest level of [[isolation (database systems)|isolation]] among [[database transaction]]s, and the major correctness criterion for concurrent transactions. In some cases compromised, [[serializability#Relaxing serializability|relaxed forms]] of serializability are allowed for better performance (e.g., the popular ''[[Snapshot isolation]]'' mechanism) or to meet [[availability]] requirements in highly distributed systems (see ''[[Eventual consistency]]''), but only if application's correctness is not violated by the relaxation (e.g., no relaxation is allowed for [[money]] transactions, since by relaxation money can disappear, or appear from nowhere).

Almost all implemented concurrency control mechanisms achieve serializability by providing ''[[Serializability#View and conflict serializability|Conflict serializablity]]'', a broad special case of serializability (i.e., it covers, enables most serializable schedules, and does not impose significant additional delay-causing constraints) which can be implemented efficiently.

=====Recoverability=====
:See ''[[Serializability#Correctness - recoverability|Recoverability]]'' in ''[[Serializability]]''

'''Comment:''' While in the general area of systems the term "recoverability" may refer to the ability of a system to recover from failure or from an incorrect/forbidden state, within concurrency control of database systems this term has received a specific meaning.

Concurrency control typically also ensures the ''[[Serializability#Correctness - recoverability|Recoverability]]'' property of schedules for maintaining correctness in cases of aborted transactions (which can always happen for many reasons). '''Recoverability''' (from abort) means that no committed transaction in a schedule has read data written by an aborted transaction. Such data disappear from the database (upon the abort) and are parts of an incorrect database state. Reading such data violates the consistency rule of ACID. Unlike Serializability, Recoverability cannot be compromised, relaxed at any case, since any relaxation results in quick database integrity violation upon aborts. The major methods listed above provide serializability mechanisms. None of them in its general form automatically provides recoverability, and special considerations and mechanism enhancements are needed to support recoverability. A commonly utilized special case of recoverability is ''[[Schedule (computer science)#Strict|Strictness]]'', which allows efficient database recovery from failure (but excludes optimistic implementations; e.g., [[Commitment ordering#Strict CO (SCO)|Strict CO (SCO)]] cannot have an optimistic implementation, but [[The History of Commitment Ordering#Semi-optimistic database scheduler|has semi-optimistic ones]]).

'''Comment:''' Note that the ''Recoverability'' property is needed even if no database failure occurs and no database ''recovery'' from failure is needed. It is rather needed to correctly automatically handle transaction aborts, which may be unrelated to database failure and recovery from it.

====Distribution====
With the fast technological development of computing the difference between local and distributed computing over low latency [[Computer network|networks]] or [[Bus (computing)|buses]] is blurring. Thus the quite effective utilization of local techniques in such distributed environments is common, e.g., in [[computer cluster]]s and [[multi-core processor]]s. However the local techniques have their limitations and use multi-processes (or threads) supported by multi-processors (or multi-cores) to scale. This often turns transactions into distributed ones, if they themselves need to span multi-processes. In these cases most local concurrency control techniques do not scale well.

=====Distributed serializability and Commitment ordering=====
{{POV-section|Commitment ordering|date=November 2011}}
:See ''[[Serializability#Distributed serializability|Distributed serializability]]'' in ''[[Serializability]]''
{{Main|Global serializability}} {{Main|Commitment ordering}}
As database systems have become [[Distributed database|distributed]], or started to cooperate in distributed environments (e.g., [[Federated database]]s in the early 1990s, and nowadays [[Grid computing]], [[Cloud computing]], and networks with [[smartphone]]s), some transactions have become distributed. A [[distributed transaction]] means that the transaction spans [[Process (computing)|processes]], and may span [[computer]]s and geographical sites. This generates a need in effective [[distributed concurrency control]] mechanisms. Achieving the Serializability property of a distributed system's schedule (see ''[[Serializability#Distributed serializability|Distributed serializability]]'' and ''[[Global serializability]]'' (''Modular serializability'')) effectively poses special challenges typically not met by most of the regular serializability mechanisms, originally designed to operate locally. This is especially due to a need in costly distribution of concurrency control information amid communication and computer [[latency (engineering)|latency]]. The only known general effective technique for distribution is Commitment ordering, which was disclosed publicly in 1991 (after being [[patent]]ed). '''[[Commitment ordering]]''' (Commit ordering, CO; [[#Raz92|Raz 1992]]) means that transactions' chronological order of commit events is kept compatible with their respective [[Serializability#Testing conflict serializability|precedence order]]. CO does not require the distribution of concurrency control information and provides a general effective solution ([[Reliability engineering|reliable]], high-performance, and [[Scalability|scalable]]) for both distributed and global serializability, also in a heterogeneous environment with database systems (or other transactional objects) with different (any) concurrency control mechanisms.&lt;ref name=Bern2009/&gt; CO is indifferent to which mechanism is utilized, since it does not interfere with any transaction operation scheduling (which most mechanisms control), and only determines the order of commit events. Thus, CO enables the efficient distribution of all other mechanisms, and also the distribution of a mix of different (any) local mechanisms, for achieving distributed and global serializability. The existence of such a solution has been considered "unlikely" until 1991, and by many experts also later, due to misunderstanding of the [[Commitment ordering#Summary|CO solution]] (see [[Global serializability#Quotations|Quotations]] in ''[[Global serializability]]''). An important side-benefit of CO is [[Commitment ordering#Exact characterization of voting-deadlocks by global cycles|automatic distributed deadlock resolution]]. Contrary to CO, virtually all other techniques (when not combined with CO) are prone to [[Deadlock#Distributed deadlock|distributed deadlocks]] (also called global deadlocks) which need special handling. CO is also the name of the resulting schedule property: A schedule has the CO property if the chronological order of its transactions' commit events is compatible with the respective transactions' [[Serializability#Testing conflict serializability|precedence (partial) order]].

[[Two-phase locking|SS2PL]] mentioned above is a variant (special case) of CO and thus also effective to achieve distributed and global serializability. It also provides automatic distributed deadlock resolution (a fact overlooked in the research literature even after CO's publication), as well as Strictness and thus Recoverability. Possessing these desired properties together with known efficient locking based implementations explains SS2PL's popularity. SS2PL has been utilized to efficiently achieve Distributed and Global serializability since the 1980, and has become the [[de facto standard]] for it. However, SS2PL is blocking and constraining (pessimistic), and with the proliferation of distribution and utilization of systems different from traditional database systems (e.g., as in [[Cloud computing]]), less constraining types of CO (e.g., [[Commitment ordering#Distributed optimistic CO (DOCO)|Optimistic CO]]) may be needed for better performance.

'''Comments:'''
# The ''Distributed conflict serializability'' property in its general form is difficult to achieve efficiently, but it is achieved efficiently via its special case ''Distributed CO'': Each local component (e.g., a local DBMS) needs both to provide some form of CO, and enforce a special ''vote ordering strategy'' for the ''[[Two-phase commit protocol]]'' (2PC: utilized to commit [[distributed transaction]]s). Differently from the general Distributed CO, ''Distributed SS2PL'' [[Commitment ordering#Strong strict two phase locking (SS2PL)|exists automatically when all local components are SS2PL based]] (in each component CO exists, implied, and the vote ordering strategy is now met automatically). This fact has been known and utilized since the 1980s (i.e., that SS2PL exists globally, without knowing about CO) for efficient Distributed SS2PL, which implies Distributed serializability and strictness (e.g., see [[#Raz92|Raz 1992]], page 293; it is also implied in [[#Bern87|Bernstein et al. 1987]], page 78). Less constrained Distributed serializability and strictness can be efficiently achieved by Distributed [[Commitment ordering#Strict CO (SCO)|Strict CO (SCO)]], or by a mix of SS2PL based and SCO based local components.
# About the references and Commitment ordering: ([[#Bern87|Bernstein et al. 1987]]) was published before the discovery of CO in 1990. The CO schedule property is called ''[[The History of Commitment Ordering#Dynamic atomicity|Dynamic atomicity]]'' in ([[#Lynch1993|Lynch et al. 1993]], page 201). CO is described in ([[#Weikum2001|Weikum and Vossen 2001]], pages 102, 700), but the description is partial and misses [[Commitment ordering#Summary|CO's essence]]. ([[#Raz92|Raz 1992]]) was the first refereed and accepted for publication article about CO algorithms (however, publications about an equivalent Dynamic atomicity property can be traced to 1988). Other [[Commitment ordering#References|CO articles]] followed. (Bernstein and Newcomer 2009)&lt;ref name=Bern2009/&gt; note CO as one of the four major concurrency control methods, and CO's ability to provide interoperability among other methods.

=====Distributed recoverability=====
Unlike Serializability, ''Distributed recoverability'' and ''Distributed strictness'' can be achieved efficiently in a straightforward way, similarly to the way Distributed CO is achieved: In each database system they have to be applied locally, and employ a vote ordering strategy for the [[Two-phase commit protocol]] (2PC; [[#Raz92|Raz 1992]], page 307).

As has been mentioned above, Distributed [[Two-phase locking|SS2PL]], including Distributed strictness (recoverability) and Distributed [[commitment ordering]] (serializability), automatically employs the needed vote ordering strategy, and is achieved (globally) when employed locally in each (local) database system (as has been known and utilized for many years; as a matter of fact locality is defined by the boundary of a 2PC participant ([[#Raz92|Raz 1992]]) ).

====Other major subjects of attention====
The design of concurrency control mechanisms is often influenced by the following subjects:

=====Recovery=====
{{Main|Data recovery}}
All systems are prone to failures, and handling ''[[Data recovery|recovery]]'' from failure is a must. The properties of the generated schedules, which are dictated by the concurrency control mechanism, may affect the effectiveness and efficiency of recovery. For example, the Strictness property (mentioned in the section [[Concurrency control#Recoverability|Recoverability]] above) is often desirable for an efficient recovery.

=====Replication=====
{{Main|Replication (computer science)}}
For high availability database objects are often ''[[Replication (computer science)|replicated]]''. Updates of replicas of a same database object need to be kept synchronized. This may affect the way concurrency control is done (e.g., Gray et al. 1996&lt;ref name=Gray1996&gt;{{cite conference
 | author = [[Jim Gray (computer scientist)|Gray, J.]]
 |author2=Helland, P. |author3=[[Patrick O'Neil|O'Neil, P.]] |author4=[[Dennis Shasha|Shasha, D.]]
 | year = 1996
 | conference = The dangers of replication and a solution
 | title = Proceedings of the 1996 [[ACM SIGMOD International Conference on Management of Data]]
 | pages = 173–182
 | conference-url = ftp://ftp.research.microsoft.com/pub/tr/tr-96-17.pdf
 | doi = 10.1145/233269.233330
 }}&lt;/ref&gt;).

=== See also ===
* [[Schedule (computer science)|Schedule]]
* [[Isolation (computer science)]]
* [[Distributed concurrency control]]
* [[Global concurrency control]]

===References===
*&lt;cite id=Bern87&gt;[[Phil Bernstein|Philip A. Bernstein]], Vassos Hadzilacos, Nathan Goodman (1987): [http://research.microsoft.com/en-us/people/philbe/ccontrol.aspx  ''Concurrency Control and Recovery in Database Systems''] (free PDF download), Addison Wesley Publishing Company, 1987, ISBN 0-201-10715-5 &lt;/cite&gt;
*&lt;cite id=Weikum01&gt;[[Gerhard Weikum]], Gottfried Vossen (2001): [http://www.elsevier.com/wps/find/bookdescription.cws_home/677937/description#description  ''Transactional Information Systems''], Elsevier, ISBN 1-55860-508-8 &lt;/cite&gt;
*&lt;cite id=Lynch1993&gt;[[Nancy Lynch]], Michael Merritt, William Weihl, Alan Fekete (1993): [http://www.elsevier.com/wps/find/bookdescription.cws_home/680521/description#description ''Atomic Transactions in Concurrent and Distributed Systems ''], Morgan Kauffman (Elsevier), August 1993, ISBN 978-1-55860-104-8, ISBN 1-55860-104-X &lt;/cite&gt;
*&lt;cite id=Raz92&gt;[[Yoav Raz]] (1992): [http://www.informatik.uni-trier.de/~ley/db/conf/vldb/Raz92.html  "The Principle of Commitment Ordering, or Guaranteeing Serializability in a Heterogeneous Environment of Multiple Autonomous Resource Managers Using Atomic Commitment."]  ([http://www.vldb.org/conf/1992/P292.PDF  PDF]), ''Proceedings of the Eighteenth International Conference on Very Large Data Bases'' (VLDB), pp. 292-312, Vancouver, Canada, August 1992. (also DEC-TR 841, [[Digital Equipment Corporation]], November 1990) &lt;/cite&gt;

===Footnotes===
{{Reflist}}

== Concurrency control in operating systems ==
{{Expand section|date=December 2010}}
[[Computer multitasking|Multitasking]] operating systems, especially [[real-time operating system]]s, need to maintain the illusion that all tasks running on top of them are all running at the same time, even though only one or a few tasks really are running at any given moment due to the limitations of the hardware the operating system is running on. Such multitasking is fairly simple when all tasks are independent from each other. However, when several tasks try to use the same resource, or when tasks try to share information, it can lead to confusion and inconsistency. The task of [[concurrent computing]] is to solve that problem. Some solutions involve "locks" similar to the locks used in databases, but they risk causing problems of their own such as [[deadlock]]. Other solutions are [[Non-blocking algorithm]]s and [[Read-copy-update]].

=== See also ===
* [[Linearizability]]
* [[Mutual exclusion]]
* [[Semaphore (programming)]]
* [[Lock (computer science)]]
* [[Software transactional memory]]
* [[Transactional Synchronization Extensions]]

===References===
*  Andrew S. Tanenbaum, Albert S Woodhull (2006): ''Operating Systems Design and Implementation, 3rd Edition'', [[Prentice Hall]], ISBN 0-13-142938-8
* {{cite book | last = Silberschatz | first = Avi |author2=Galvin, Peter |author3=Gagne, Greg | title = Operating Systems Concepts, 8th edition | publisher = [[John Wiley &amp; Sons]] | year = 2008 | isbn = 0-470-12872-0 }}

{{Databases}}

{{DEFAULTSORT:Concurrency Control}}
[[Category:Concurrency control| ]]
[[Category:Data management]]
[[Category:Databases]]
[[Category:Transaction processing]]</text>
      <sha1>n6eq33kdj9uuwigzvrqtq2adn579g2r</sha1>
    </revision>
  </page>
  <page>
    <title>Database engine</title>
    <ns>0</ns>
    <id>209503</id>
    <revision>
      <id>745928263</id>
      <parentid>723318473</parentid>
      <timestamp>2016-10-24T06:38:13Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* External links */http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8036" xml:space="preserve">A '''database engine''' (or '''storage engine''') is the underlying software component that a [[database management system]] (DBMS) uses to [[create, read, update and delete]] (CRUD) [[data]] from a [[database]]. Most database management systems include their own [[application programming interface]] (API) that allows the user to interact with their underlying engine without going through the user interface of the DBMS.

The term "database engine" is frequently used interchangeably with "[[database server]]" or "database management system". A 'database instance' refers to the processes and memory structures of the running '''database engine'''.

==Storage engines==
Many of the modern DBMS support multiple storage engines within the same database. For example, [[MySQL]] supports [[InnoDB]] as well as [[MyISAM]].

Some storage engines are [[Database transaction|transactional]].

{| class="wikitable"
|-
! Name !! License !! Transactional
|-
| [[Aria (storage engine)|Aria]] || GPL || {{No}}
|-
| BlitzDB || GPL || {{No}}
|-
| [[Falcon (storage engine)|Falcon]] || GPL || {{Yes}}
|-
| [[InnoDB]] || GPL || {{Yes}}
|-
| [[MyISAM]] || GPL || {{No}}
|-
| [[InfiniDB]] || CPL || {{No}}
|-
| [[TokuDB]] || GPL || {{Yes}}
|-
| [[WiredTiger]] || GPL || {{Yes}}
|-
| [[XtraDB]] || GPL || {{Yes}}
|}

Additional engine types include:
*[[Embedded database]] engines
*[[In-memory database]] engines

==Design considerations==
Database bits are laid out in storage in data structures and groupings that can take advantage of both known effective algorithms to retrieve and manipulate them and the storage own properties. Typically the storage itself is designed to meet requirements of various areas that extensively utilize storage, including databases. A DBMS in operation always simultaneously utilizes several storage types (e.g., memory, and external storage), with respective layout methods.

In principle the database storage can be viewed as a [[linear address space]], where every bit of data has its unique address in this address space. In practice, only a very small percentage of addresses are kept as initial reference points (which also requires storage); most data is accessed by indirection using displacement calculations (distance in bits from the reference points) and data structures which define access paths (using pointers) to all needed data in an effective manner, optimized for the needed data access operations.

===Database storage hierarchy===
A database, while in operation, resides simultaneously in several types of storage, forming a [[storage hierarchy]]. By the nature of contemporary computers most of the database part inside a computer that hosts the DBMS resides (partially replicated) in volatile storage. Data (pieces of the database) that are being processed/manipulated reside inside a processor, possibly in [[CPU cache|processor's caches]]. These data are being read from/written to memory, typically through a computer [[Bus (computing)|bus]] (so far typically volatile storage components). Computer memory is communicating data (transferred to/from) external storage, typically through standard storage interfaces or networks (e.g., [[fibre channel]], [[iSCSI]]). A [[Disk array|storage array]], a common external storage unit, typically has storage hierarchy of its own, from a fast cache, typically consisting of (volatile and fast) [[DRAM]], which is connected (again via standard interfaces) to drives, possibly with different speeds, like [[USB flash drive|flash drives]] and magnetic [[disk drive]]s (non-volatile). The drives may be connected to [[magnetic tape]]s, on which typically the least active parts of a large database may reside, or database backup generations.

Typically a correlation exists currently between storage speed and price, while the faster storage is typically volatile.

===Data structures===
{{Main|Database storage structures}}
A data structure is an abstract construct that embeds data in a well defined manner. An efficient data structure allows to manipulate the data in efficient ways. The data manipulation may include data insertion, deletion, updating and retrieval in various modes. A certain data structure type may be very effective in certain operations, and very ineffective in others. A data structure type is selected upon DBMS development to best meet the operations needed for the types of data it contains. Type of data structure selected for a certain task typically also takes into consideration the type of storage it resides in (e.g., speed of access, minimal size of storage chunk accessed, etc.). In some DBMSs database administrators have the flexibility to select among options of data structures to contain user data for performance reasons. Sometimes the data structures have selectable parameters to tune the database performance.

Databases may store data in many data structure types.&lt;ref name="Physical Database Design"&gt;{{harvnb|Lightstone|Teorey|Nadeau|2007}}&lt;/ref&gt; Common examples are the following:
*ordered/unordered [[flat file database|flat files]]
*[[hash table]]s
*[[B+ tree]]s
*[[ISAM]]
*[[heap (data structure)|heaps]]

===Data orientation and clustering===
In contrast to conventional row-orientation, relational databases can also be [[Column-oriented DBMS|column-oriented]] or [[Correlational database|correlational]] in the way they store data in any particular structure.

In general, substantial performance improvement is gained if different types of database objects that are usually utilized together are laid in storage in proximity, being "clustered". This usually allows to retrieve needed related objects from storage in minimum number of input operations (each sometimes substantially time consuming). Even for in-memory databases clustering provides performance advantage due to common utilization of large caches for input-output operations in memory, with similar resulting behavior.

For example, it may be beneficial to cluster a record of an "item" in stock with all its respective "order" records. The decision of whether to cluster certain objects or not depends on the objects' utilization statistics, object sizes, caches sizes, storage types, etc.

===Database indexing===
{{Main|Database index}}
Indexing is a technique some storage engines use for improving database performance. The many types of indexes share the common property that they reduce the need to examine every entry when running a query. In large databases, this can reduce query time/cost by orders of magnitude. The simplest form of index is a sorted list of values that can be searched using a [[binary search]] with an adjacent reference to the location of the entry, analogous to the index in the back of a book. The same data can have multiple indexes (an employee database could be indexed by last name and hire date).

Indexes affect performance, but not results. Database designers can add or remove indexes without changing application logic, reducing maintenance costs as the database grows and database usage evolves.  Indexes can speed up data access, but they consume space in the database, and must be updated each time the data is altered. Indexes therefore can speed data access but slow data maintenance. These two properties determine whether a given index is worth the cost.

==See also==
{{cleanup-merge}} &lt;!--Into chart --&gt;
*[[Architecture of Btrieve#Micro-Kernel Database Engine|Btrieve's Micro-Kernel Database Engine]]
*[[Berkeley DB]]
*[[c-treeACE|c-treeACE Database Engine]]
*[[FLAIM Database Engine]]
*[[Microsoft Jet Database Engine]]
*[[MySQL Cluster]], on the NDB storage engine of [[MySQL]]
*[[NuoDB]]

==References==
{{Reflist}}

==External links==
*http://dev.mysql.com/tech-resources/articles/storage-engine/part_3.html
*[https://books.google.com/books?id=PqZ6QytCemcC&amp;pg=PT287&amp;dq=storage+engines MySQL Administrator's Bible] Chapter 11 "Storage Engines"

{{DEFAULTSORT:Database Engine}}
[[Category:Data management]]
[[Category:Database engines| ]]
[[Category:Database management systems]]</text>
      <sha1>qxtupdiiuy6oik3o6jzdr8wkljvfidn</sha1>
    </revision>
  </page>
  <page>
    <title>Network transparency</title>
    <ns>0</ns>
    <id>1786529</id>
    <revision>
      <id>732099751</id>
      <parentid>705601954</parentid>
      <timestamp>2016-07-29T16:34:00Z</timestamp>
      <contributor>
        <username>Kvng</username>
        <id>910180</id>
      </contributor>
      <comment>separate section for X as discussed on talk page</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3718" xml:space="preserve">{{Multiple issues|
{{Unreferenced|date=December 2009}}
{{Lead rewrite|date=July 2015}}
}}

'''Network transparency''' in its most general sense refers to the ability of a protocol to transmit data over the [[computer network|network]] in a manner which is [[Transparency (human–computer interaction)|transparent]] (invisible) to those using the applications that are using the protocol.

==X Window==
The term is often partially correctly applied in the context of the [[X Window System]], which is able to transmit graphical data over the network and integrate it seamlessly with applications running and displaying locally; however, certain extensions of the X Window System are not capable of working over the network.&lt;ref&gt;{{cite web |url=https://lwn.net/Articles/553415/ |title=The Wayland Situation: Facts About X vs. Wayland (Phoronix) |publisher=[[LWN.net]] |date=23 June 2013}}&lt;/ref&gt;

==Databases==
In a [[centralized database system]], the only available resource that needs to be shielded from the user is the data (that is, the [[storage system]]). In a [[Distributed Database Management System|distributed DBMS]], a second resource needs to be managed in much the same manner: the [[computer network|network]]. Preferably, the user should be protected from the network operational details. Then there would be no difference between database applications that would run on the centralized database and those that would run on a distributed one. This kind of transparency is referred to as '''network transparency''' or '''distribution transparency'''. From a [[database management system]] (DBMS) perspective, distribution transparency requires that users do not have to specify where data is located.

Some have separated distribution transparency into location transparency and naming transparency.

Location transparency in commands used to perform a task is independent both of the locations of the data, and of the system on which an operation is carried out.

Naming transparency means that a unique name is provided for each object in the database.

==Firewalls==
{{See also|Proxy server#Transparent proxy}}

Transparency in firewall technology can be defined at the networking (IP or [[Internet Layer|Internet layer]]) or at the [[Internet Layer|application layer]].

Transparency at the IP layer means the client targets the real IP address of the server. If a connection is non-transparent, then the client targets an intermediate host (address), which could be a proxy or a caching server. IP layer transparency could be also defined from the point of server's view. If the connection is transparent, the server sees the real client IP. If it is non-transparent, the server sees the IP of the intermediate host.

Transparency at the application layer means the client application uses the protocol in a different way. An example of a transparent HTTP request for a server:

&lt;syntaxhighlight lang="text"&gt;
GET / HTTP/1.1
Host: example.org
Connection: Keep-Alive
&lt;/syntaxhighlight&gt;

An example non-transparent HTTP request for a proxy (cache):

&lt;syntaxhighlight lang="text"&gt;
GET http://foo.bar/ HTTP/1.1
Proxy-Connection: Keep-Alive
&lt;/syntaxhighlight&gt;

Application layer transparency is symmetric when the same working mode is used on both the sides. The transparency is asymmetric when the firewall (usually a proxy) converts server type requests to proxy type or vice versa. 

Transparency at the IP layer does not mean automatically application layer transparency.

== See also ==
{{Portal|Computer networking}}

* [[Data independence]]
* [[Replication transparency]]

==References==
{{Reflist}}

{{DEFAULTSORT:Network Transparency}}
[[Category:Telecommunications]]
[[Category:Data management]]</text>
      <sha1>c3s8z1z7xf9eor96ujg8165t0gob27x</sha1>
    </revision>
  </page>
  <page>
    <title>Uniform data access</title>
    <ns>0</ns>
    <id>1610890</id>
    <revision>
      <id>578107650</id>
      <parentid>578107434</parentid>
      <timestamp>2013-10-21T12:57:02Z</timestamp>
      <contributor>
        <ip>142.240.200.7</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="765" xml:space="preserve">{{Unreferenced stub|auto=yes|date=December 2009}}
'''Uniform data access''' is a computational concept describing an even-ness of connectivity and controllability across numerous target data sources.  

Necessary to fields such as [[Enterprise Information Integration]] (EII) and [[Electronic Data Interchange]] (EDI), it is most often used regarding analysis of disparate data types and data sources, which must be rendered into a [[uniform information representation]], and generally must appear [[wiktionary:Homogenous|homogenous]] to the analysis tools—when the data being analyzed is typically [[heterogeneous]] and widely varying in size, type, and original representation.{{DEFAULTSORT:Uniform Data Access}}
[[Category:Data management]]


{{Comp-sci-stub}}</text>
      <sha1>dqnsnfnf9ezgfpdbbhtb89g3e1fu5vu</sha1>
    </revision>
  </page>
  <page>
    <title>Recording format</title>
    <ns>0</ns>
    <id>793548</id>
    <revision>
      <id>694556608</id>
      <parentid>563133697</parentid>
      <timestamp>2015-12-10T00:32:33Z</timestamp>
      <contributor>
        <username>SJ Defender</username>
        <id>19403234</id>
      </contributor>
      <minor />
      <comment>Disambiguated: [[drawer]] → [[drawer (furniture)]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3894" xml:space="preserve">{{Unreferenced|date=December 2009}}
[[File:Cylinder Head Sector.svg|thumb|300px|right|A cylinder, head, and sector of a hard drive. The sectors are a recording container format. The digital data on the disks may be both secondary [[Container format (digital)|container file formats]] and raw digital data content formats such as digital audio or ASCII encoded text.]]
[[File:WorldMapLongLat-eq-circles-tropics-non.png|thumb|440px|A map of Earth showing lines of latitude (horizontally) and longitude (vertically). The lines are a grid, a method for dividing and containing recorded [[cartographical]] data. The land masses and oceans are cartographical data in a raw content ([[pictorial]] graphical) format. The text is in an [[alphanumeric]]al symbolic raw content format.]]
A '''recording format ''' is a [[content format|format]] for [[encoder|encoding]] data for storage on a [[storage medium]]. The format can be container information such as [[Cylinder-head-sector|sectors]] on a disk, or user/audience information ([[Content (media and publishing)|content]]) such as [[analog signal|analog]] [[stereo]] [[Sound recording and reproduction|audio]].  Multiple levels of encoding may be achieved in one format. For example, a text encoded page may contain [[HTML]] and [[XML]] encoding, combined in a [[plain text]] file format, using either [[EBCDIC]] or [[ASCII]] character encoding, on a [[Universal Disk Format|UDF]] [[Digital data|digital]]ly formatted disk.  

In [[electronic media]], the primary format is the encoding that requires hardware to interpret (decode) data; while secondary encoding is interpreted by secondary [[signal processing]] methods, usually [[computer software]]. 

==Recording container formats==
A container format is a system for dividing physical storage space or virtual space for data. Data space can be divided evenly by a [[systems of measurement|system of measurement]], or divided unevenly with [[meta data]]. A grid may divide physical or virtual space with physical or virtual (dividers) borders, evenly or unevenly.  Just as a physical container (such as a [[file cabinet]]) is divided by physical borders (such as [[drawer (furniture)|drawer]]s and [[file folder]]s), data space is divided by virtual borders. Meta data such as a [[unit of measurement]], [[Address (geography)|address]], or [[meta tags]] act as virtual borders in a container format. A template may be considered an abstract format for containing a solution as well as the content itself. 

* Systems of measurement
**[[Metric system]]
** [[Geographic coordinate system]]
**[[Grid (page layout)|Page grid]]
* [[Film formats]]
* [[Audio format|Audio data format]]
* [[Video tape|Video tape format]]
* [[Disk format]]
* [[File format]]
* [[Meta data]]
** [[Formatted text|Text formatting]]
** [[Template (file format)|Template]]
** [[Data structure]]

==Raw content formats==
{{Main|content format}}

A raw content format is a system of converting data to displayable [[information]].  Raw content formats may either be recorded in secondary signal processing methods such as a software container format (e.g. [[digital audio]], [[digital video]]) or recorded in the primary format. A primary raw content format may be directly [[information processing|observable]] (e.g. [[image]], [[sound]], [[Motion (physics)|motion]], [[Odor|smell]], [[Haptic perception|sensation]]) or [[physics|physical]] data which only requires hardware to display it, such as a [[phonograph]]ic [[Gramophone needle|needle]] and [[diaphragm (acoustics)|diaphragm]] or a [[Image projector|projector]] [[List of light sources|lamp]] and [[magnifying glass]].

{{Audio format}}{{Homevid}}

{{DEFAULTSORT:Recording Format}}
[[Category:Communication]]
[[Category:Information science]]
[[Category:Data management]]
[[Category:Film and video technology]]
[[Category:Computer storage media]]
[[Category:Recording]]</text>
      <sha1>41djunpvjeye4mntbzr0yj5z86kf64j</sha1>
    </revision>
  </page>
  <page>
    <title>Data virtualization</title>
    <ns>0</ns>
    <id>26041421</id>
    <revision>
      <id>752147765</id>
      <parentid>751181827</parentid>
      <timestamp>2016-11-29T19:30:21Z</timestamp>
      <contributor>
        <ip>70.114.204.101</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7311" xml:space="preserve">'''Data virtualization''' is any approach to data management that allows an application to retrieve and manipulate data without requiring technical details about the data, such as how it is formatted at source, or where it is physically located.&lt;ref&gt;[http://searchdatamanagement.techtarget.com/definition/data-virtualization "What is Data Virtualization?"], Margaret Rouse, TechTarget.com, retrieved 19 August 2013&lt;/ref&gt;

Unlike the traditional [[extract, transform, load]] ("ETL") process, the data remains in place, and real-time access is given to the source system for the data, thus reducing the risk of data errors and reducing the workload of moving data around that may never be used.

Unlike a [[federated database system]], it does not attempt to impose a single data model on the data (heterogeneous data). The technology also supports the writing of transaction data updates back to the source systems.&lt;ref name="morgan"&gt;[http://www.computerweekly.com/feature/Data-virtualisation-on-rise-as-ETL-alternative-for-data-integration "Data virtualisation on rise as ETL alternative for data integration"] Gareth Morgan, Computer Weekly, retrieved 19 August 2013&lt;/ref&gt;

To resolve differences in source and consumer formats and semantics, various abstraction and transformation techniques are used.
This concept and software is a subset of [[data integration]] and is commonly used within [[business intelligence]], [[service-oriented architecture]] data services, [[cloud computing]], [[enterprise search]], and [[master data management]].

==Examples ==
* The Phone House—the trading name for the European operations of UK-based mobile phone retail chain [[Carphone Warehouse]]—implemented [[Denodo]]’s data virtualization technology between its Spanish subsidiary’s transactional systems and the Web-based systems of mobile operators.&lt;ref name="morgan"/&gt;
* [[Novartis]], which implemented a data virtualization tool from [[Composite Software]] to enable its researchers to quickly combine data from both internal and external sources into a searchable virtual data store.&lt;ref name="morgan"/&gt;
* The storage-agnostic [http://primarydata.com/ Primary Data] data virtualization platform enables applications, servers, and clients to transparently access data while it is intelligently migrated between direct-attached, network-attached, private and public cloud storage. Server flash memory pioneer [[Fusion-io]] co-founder David Flynn, now Primary Data CTO, saw the need to move data across storage types to maximize efficiency with data virtualization.
* [[Linked Data]] can use a single hyperlink-based [[Data Source Name]] ([[Data Source Name|DSN]]) to provide a connection to a virtual database layer that is internally connected to a variety of back-end data sources using [[ODBC]], [[JDBC]], [[OLE DB]], [[ADO.NET]], [[Service-oriented architecture|SOA]]-style services, and/or [[REST]] patterns.
* [[Database virtualization]] may use a single ODBC-based DSN to provide a connection to a similar virtual database layer.

==Functionality==

Data Virtualization software provides some or all of the following capabilities:

* '''Abstraction''' –  Abstract the technical aspects of stored data, such as location, storage structure, API, access language, and storage technology. 
* '''Virtualized Data Access''' – Connect to different data sources and make them accessible from a common logical data access point.
* '''Transformation''' – Transform, improve quality, reformat, etc. source data for consumer use. 
* '''Data Federation''' – Combine result sets from across multiple source systems. 
* '''Data Delivery''' – Publish result sets as views and/or data services executed by client application or users when requested.

Data virtualization software may include functions for development, operation, and/or management.

Benefits include:
* Reduce risk of data errors
* Reduce systems workload through not moving data around
* Increase speed of access to data on a real-time basis
* Significantly reduce development and support time
* Increase governance and reduce risk through the use of policies&lt;ref&gt;[http://www.informatica.com/us/products/data-virtualization/data-services/ "Rapid Access to Disparate Data Across Projects Without Rework"] Informatica, retrieved 19 August 2013&lt;/ref&gt;
* Reduce data storage required&lt;ref&gt;[http://www.zdnet.com/blog/service-oriented/data-virtualization-6-best-practices-to-help-the-business-get-it/7897 Data virtualization: 6 best practices to help the business 'get it'] Joe McKendrick, ZDNet, 27 October 2011&lt;/ref&gt;

Drawbacks include:
* May impact Operational systems response time, particularly if under-scaled to cope with unanticipated user queries or not tuned early on&lt;ref&gt;[http://searchdatamanagement.techtarget.com/news/2240165242/IT-pros-reveal-the-benefits-drawbacks-of-data-virtualization-software|IT pros reveal benefits, drawbacks of data virtualization software"] Mark Brunelli, SearchDataManagement, 11 October 2012&lt;/ref&gt;
* Does not impose a heterogeneous data model, meaning the user has to interpret the data, unless combined with [[Federated database system|Data Federation]] and business understanding of the data&lt;ref name="lawson"&gt;[http://www.itbusinessedge.com/cm/blogs/lawson/the-pros-and-cons-of-data-virtualization/?cs=48794 "The Pros and Cons of Data Virtualization"] Loraine Lawson, BusinessEdge, 7 October 2011&lt;/ref&gt;
* Requires a defined Governance approach to avoid budgeting issues with the shared services
* Not suitable for recording the historic snapshots of data - data warehouse is better for this&lt;ref name="lawson"/&gt;
* Change management "is a huge overhead, as any changes need to be accepted by all applications and users sharing the same virtualization kit"&lt;ref name="lawson"/&gt;

== Technology ==

Some data virtualization technologies include:

*  [[Actifio]] Copy Data Virtualization
*  [[Capsenta]]'s Ultrawrap Platform &lt;ref&gt;https://capsenta.com/&lt;/ref&gt;
*  [[Cisco]] Data Virtualization (formerly [[Composite Software]])
*  [[Denodo|Denodo Platform]]
* DataVirtuality
* Data Virtualization Platform
*  HiperFabric Data Virtualization and Integration
* Stonebond Enterprise Enabler Data Virtualization Platform
* [[Red Hat]] [[JBoss Enterprise Application Platform]] Data Virtualization
* [[XAware]] Data Services

==History==
[[Enterprise information integration]] (EII), first coined by Metamatrix, now known as Red Hat JBoss Data Virtualization, and [[federated database system]]s are terms used by some vendors to describe a core element of data virtualization: the capability to create relational JOINs in a federated VIEW.

==See also==

* [[Data integration]]
* [[Enterprise information integration]] (EII)
* [[Master data management]]
* [[Database virtualization]]
* [[Federated database system|Data Federation]]
* [[Disparate system]]

==References==
{{reflist}}

==Further reading==
* '''Data Virtualization: Going Beyond Traditional Data Integration to Achieve Business Agility''', Judith R. Davis and Robert Eve
* '''Data Virtualization for Business Intelligence Systems: Revolutionizing Data Integration for Data Warehouses''' Rick van der Lans
* '''Data Integration Blueprint and Modeling: Techniques for a Scalable and Sustainable Architecture ''' Anthony Giordano

[[Category:Data management]]</text>
      <sha1>8yq3c8q2a2zntfpfpfhnje3wfl7vb2k</sha1>
    </revision>
  </page>
  <page>
    <title>Photo recovery</title>
    <ns>0</ns>
    <id>24862923</id>
    <revision>
      <id>757502706</id>
      <parentid>684745204</parentid>
      <timestamp>2016-12-31T01:34:16Z</timestamp>
      <contributor>
        <username>Fixuture</username>
        <id>19796795</id>
      </contributor>
      <comment>added [[Category:Photography]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6292" xml:space="preserve">{{Advert|date=August 2010}}

'''Photo recovery''' is the process of salvaging digital photographs from damaged, failed, corrupted, or inaccessible [[Computer data storage#Secondary storage|secondary storage]] media when it cannot be accessed normally. Photo Recovery can be considered a subset of
the overall [[Data Recovery]] field. 

Photo loss or deletion failures may be due to both hardware or software failures.

==Recovering data after hardware failure==
An excellent explanation of hardware failures is provided in the section for [[Data Recovery|data recovery]]. Typically, if your
drive or card is so badly damaged that your computer can not recognize that a drive/card has been connected, you
will need to consult a data recovery service provider.

==Recovering data after logical failure==
Logical Damage or the inability to view photos can occur due to many reasons. The most common reasons are:

# Deletion of photos.
# Corruption of boot sector of media.
# Corruption of [[file system]].
# [[Disk formatting]].
# Move or Copy errors.

=== Photo Recovery Using File Carving ===
The majority of photo recovery programs work by using a technique called [[file carving|file carving (data carving)]].
There are many different file carving techniques that are used to recover photos. Most of these techniques
fail in the presence of [[file system fragmentation]]. Simson Garfinkel showed that on average 16% of [[JPEG]]s are fragmented,&lt;ref name=garfinkel_dfrws2007&gt;[[Simson Garfinkel]], ''Carving Contiguous and Fragmented Files with Fast Object Validation'', in Proceedings of the 2007 digital forensics research workshop, DFRWS, Pittsburgh, PA, August 2007&lt;/ref&gt; which
means on average 16% of jpegs are recovered partially or appear corrupt when recovered using techniques that
can't handle fragmented photos.

==== Header-Footer Carving ====
In Header-Footer Carving, a recovery program attempts to recover photos based on the standard starting and ending byte
signature of the photo format. To take an example, all [[JPEG]]s always begin with the hex sequence "FFD8" and they must
end with the hex sequence "FFD9".

Header-Footer Carving cannot be used to recover fragmented photos, and fragmented
photos will appear to be partially recovered or corrupt if incorrect data is added. Header-Footer Carving, along
with Header-Size Carving, are by far the most common techniques for photo recovery. One of the first non-gui/console
based programs to use this technique is [[PhotoRec]].
Use of footers can often truncate a photo, as many JPEGs contain thumbnails as an embedded object.  If a file is terminated with a FFD9 it will be corrupted, unless nested FFD8/FFD9s are counted.

==== Header-Size Carving ====
In Header-Size Carving, a recovery program attempts to recover photos based on the standard starting byte signature of
the photo format, along with the size of the photo that is either derived or explicitly stated in the photo format.
To take an example all 24-bit Windows Bitmaps (*.bmp), begin with the letters "BM", and store the size of the file in
the header. Header-Size Carving cannot be used to recover fragmented photos, and fragmented photos will appear to be
partially recovered or corrupt if incorrect data is added.

==== File-Structure Based Carving ====
A more advanced form of carving, a recovery program attempts to recover photos based on detailed knowledge of the
structure rules of the photo format. This will enable a recovery program to identify when a photo is not complete or
fragmented, but more needs to be done to see if a fragmented photo can be recovered. This technique is rarely
used by most photo recovery programs.

==== Validated Carving ====
In validated carving, a decoder is used to detect any errors in recovery of a photo. More advanced forms of validated
carving occur when each part of the recovered photo is compared against the rest of the photo to see if it "fits"
visually. Validated carving is superb at detecting photos that are either fragmented or have parts over-written or
missing. Validated carving alone cannot be used to recover fragmented photos.&lt;ref name=pal_ieee_ip&gt;A. Pal and N. Memon, [http://digital-assembly.com/technology/research/pubs/ieee-trans-2006.pdf "Automated reassembly of file fragmented images using greedy algorithms"] in IEEE Transactions on Image processing, February 2006, pp 385393&lt;/ref&gt;

==== Log Carving ====
Log Carving occurs when a recovery program uses information left over in either file system structures or the log
to recover a deleted photo. For example, occasionally NTFS will store in the logs the exact location of where the
file was located prior to its deletion. A program using Log Carving will be able to then recover the photo. To be
sure about the quality of recovery, Validated Carving or File-Structure based carving should also be used to
validate the recovered photo.

==== Bi-Fragment Gap Carving ====
A fragmented photo recovery technique where a header and footer are identified and then all combinations of blocks
between the header and footer are validated to determine which combination results in the correct recovery of the
photo.&lt;ref name="garfinkel_dfrws2007"/&gt; This technique will only work if the file is fragmented into two parts.

==== SmartCarving ====
A process by which fragmented photos are recovered by looking at blocks on the disk and determining which block
is the best visual match for the photo being recovered. This is done in parallel for all blocks that are not part
of a recovered file.&lt;ref name=pal_dfrws2008&gt;A. Pal, T. Sencar, N. Memon, [http://digital-assembly.com/technology/research/pubs/dfrws2008.pdf "Detecting File Fragmentation Point Using Sequential Hypothesis Testing"] Digital Forensic Research Workshop, August 2008&lt;/ref&gt;

==References==
&lt;references /&gt;

==Further reading==
* Tanenbaum, A. &amp; Woodhull, A. S. (1997). ''Operating Systems: Design And Implementation,'' 2nd ed. New York: Prentice Hall.
*[http://www.informationweek.com/news/windows/showArticle.jhtml?articleID=200000329 What To Do When Windows Vista Crashes: Little-Known Recovery Strategies], from Information Week

[[Category:Data recovery|photo]]
[[Category:Computer data]]
[[Category:Data management]]
[[Category:Hard disk software|*]]
[[Category:Photography]]</text>
      <sha1>tfwvf3aj3nf2dhb4qdf2thtzug9721g</sha1>
    </revision>
  </page>
  <page>
    <title>BBC Archives</title>
    <ns>0</ns>
    <id>23998233</id>
    <revision>
      <id>762519177</id>
      <parentid>754698446</parentid>
      <timestamp>2017-01-29T09:13:28Z</timestamp>
      <contributor>
        <username>Tim!</username>
        <id>203786</id>
      </contributor>
      <comment>removed [[Category:History of television]]; added [[Category:History of television in the United Kingdom]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="35335" xml:space="preserve">{{for|the Iron Maiden album|BBC Archives (album)}}
{{distinguish|BBC Motion Gallery}}
{{EngvarB|date=September 2013}}
{{Use dmy dates|date=September 2013}}
[[File:BBC Information and Archives Logo.svg|thumb|300px|BBC Information and Archives logo]]

'''BBC Information and Archives''' (sometimes known just as '''BBC Archives''') are collections documenting the [[BBC]]'s broadcasting history, including copies of [[BBC Television|television]] and [[BBC Radio|radio]] broadcasts, internal documents, photographs, [[BBC Online|online]] content, [[sheet music]], commercially available music, press cuttings and historic equipment.&lt;ref name="BBCArchive TV 1"/&gt; The original copies of these collections are permanently retained but are now in the process of being digitised, estimated to take until approximately 2015. Some collections are now being uploaded onto the BBC Archives website on [[BBC Online]] for viewers to see. The archive is one of the largest broadcast archives in the world with over 12 million items.&lt;ref name=Perivale1&gt;{{cite web|last=Hayes|first=Sarah|title=The new BBC Archive Centre in Perivale|url=http://www.bbc.co.uk/blogs/aboutthebbc/2011/10/the-new-bbc-archive-centre-at.shtml|work=About the BBC Blog|publisher=BBC Online|accessdate=17 January 2012}}&lt;/ref&gt;

==Overview==
The BBC Archives encompass numerous different archives containing different materials produced or acquired by the BBC. The earliest material dates back to 1890 and now consists of 1 million hours of playable material, in addition to documents, photographs and equipment.&lt;ref name="gutechweekly"&gt;{{cite news|last=Kiss|first=Jemima|title=In The BBC Archive|url=https://www.theguardian.com/technology/blog/audio/2010/aug/18/bbc-archive-roly-keating-windmill-road|work=Tech Weekly|publisher=Guardian News &amp; Media Ltd|accessdate=21 August 2010 | location=London | date=18 August 2010| archiveurl= https://web.archive.org/web/20100821165828/http://www.guardian.co.uk/technology/blog/audio/2010/aug/18/bbc-archive-roly-keating-windmill-road?| archivedate= 21 August 2010 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt; The archives contain 12 million items on 66 miles of shelving spread over several sites.&lt;ref name=gutechweekly /&gt; The stock is managed using a [[bar code]] system, which help to locate material on the shelves and also track material that has been lent out.&lt;ref name=gutechweekly /&gt; The BBC says that the budget for managing, protecting and digitising the archive accounts for only a small part of the BBC's overall spend.&lt;ref name=gutechweekly /&gt;

The BBC is engaging in an ongoing project to [[Digital reformatting|digitise]] archived programme material, converting recordings made on older [[Analog recording|analogue formats]] such as audio tape, [[videotape]] and film to electronic formats which are compatible with modern computer systems. Much of the audio-visual material was originally recorded on formats which are now obsolete and incompatible with modern broadcast equipment due to the fact that the machines used to reproduce many formats are no longer being manufactured. Additionally, some film and audio formats are slowly disintegrating, and digitisation also serves as a [[digital preservation]] programme. As of summer 2010 BBC Archive staff have spent approximately ten years digitising half of the media content&lt;ref name=gutechweekly /&gt;&lt;ref name="BBC Archive BBCInternetblog"/&gt; and due to improving work practices expect to complete the other half in five years. Current estimates suggest the digitised archive would comprise approximately 52 [[petabyte]]s of information,&lt;ref name=gutechweekly /&gt; with one programme minute of video requiring 1.4 [[gigabyte]]s of storage.&lt;ref name=gutechweekly /&gt; The BBC uses the [[Material Exchange Format]] (MXF)&lt;ref name=gutechweekly /&gt; which is an uncompressed, non-proprietary format which the BBC has been publicising to mitigate the threat of the format becoming obsolete (as digital formats can and do).&lt;ref name=gutechweekly /&gt;

The Archive digitisation a key part of the BBC's programme to engineer a fully digital and [[tapeless|tapeless production workflow]] across the entire Corporation. It was closely tied in with the ill-fated [[Digital Media Initiative]] (DMI), a scheme which ran from 2008 to 2013 and attempted to create a unified online archive search and programme production system.&lt;ref name=bbc-dmi&gt;{{cite web|title=Digital Media Initiative|url=http://www.bbc.co.uk/careers/divisions/digital-media-initiative|publisher=BBC|accessdate=15 February 2012|archiveurl=https://web.archive.org/web/20120310041000/http://www.bbc.co.uk/careers/divisions/digital-media-initiative|archivedate=10 March 2012|deadurl=yes}}&lt;/ref&gt; After spiralling development costs and project delays, the problems with DMI came to public attention during coverage of the [[death and funeral of Margaret Thatcher]] in April 2013, when it was reported that the lack of digital ingest facilities provided for [[BBC News]] staff meant that tapes had to be sent by taxi from the Perivale centre to be digitised by independent companies in central London.&lt;ref&gt;{{cite news|title=BBC's Thatcher coverage highlights problems with non-digital archives|url=https://www.theguardian.com/media/2013/apr/11/bbc-thatcher-coverage|accessdate=3 May 2013|newspaper=The Guardian|date=11 February 2012|location=London|first=Tara|last=Conlan}}&lt;/ref&gt; DMI was cancelled in 2013.&lt;ref name=bbc_abandons&gt;{{cite news|title=BBC abandons £100m digital project|url=http://www.bbc.co.uk/news/entertainment-arts-22651126|accessdate=25 May 2013|newspaper=BBC News|date=24 May 2013}}&lt;/ref&gt;

The BBC Archive website was relaunched online in 2008 and has provided newly released historical material regularly since then.&lt;ref&gt;{{cite web|last=Sangster|first=Jim|title=A new homepage for BBC Archive|url=http://www.bbc.co.uk/blogs/bbcinternet/2010/05/a_new_homepage_for_bbc_archive.html|work=BBC Internet Blog|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; The BBC works in partnership with the [[British Film Institute]] (BFI), [[The National Archives]] and other partners in working with and using the materials.&lt;ref name=gutechweekly /&gt; A related project called "Genome" is expected to complete in 2011 and will make programme listings dating back to 1923, sourced from ''[[The Radio Times]]'', available to search online.&lt;ref name=gutechweekly /&gt;

In July 2008, [[Roly Keating]] was appointed Director of Archive Content,&lt;ref&gt;{{cite web|url=http://www.bbc.co.uk/pressoffice/pressreleases/stories/2008/07_july/22/archive.shtml |title=Roly Keating appointed as Director of Archive Content |publisher=BBC Press Office|date=22 July 2008 |accessdate=1 July 2011}}&lt;/ref&gt; with responsibility for increasing public access to the BBC’s archives. In October 2008, Keating appointed [[Tony Ageh]] Controller of Archive Development with "specific responsibility for developing ways of making the archive easily understandable and accessible to users".&lt;ref&gt;{{cite web|url=http://www.bbc.co.uk/pressoffice/pressreleases/stories/2008/10_october/10/ageh.shtml|publisher=BBC Press Office|date=10 October 2008|accessdate=1 July 2011|title=Tony Ageh appointed Controller of Archive Development}}&lt;/ref&gt;

In 2012, BBC Archive Development produced a book - primarily aimed as BBC staff - titled 'BBC Archive Collections: What's In The Archive And How To Use Them'.&lt;ref&gt;'BBC Archive Collections: What's In The Archives, And How To Use Them' Edited by Jake Berger https://www.dropbox.com/s/rz1o57nzlsf1v04/BBC%20Archive%20Collections%20Guide%202012.pdf?dl=0&lt;/ref&gt;  This book describes the BBC's archive collections and offers guidance around on how items from the collections can be reused online.  The book's references to 'Fabric', a system due to be delivered by the [[Digital Media Initiative]] are no longer accurate as the project was cancelled.

==Buildings==
From 1968 to 2010 the BBC Archive was housed at the Archive centre in Windmill Road, [[Brentford]], in [[W postcode area|west London]].&lt;ref name=Perivale1/&gt;&lt;ref name="BBC Archive BBCInternetblog"/&gt;&lt;ref name="Perivale 2 AtBBCblog"/&gt; The condition of the building deteriorated over the years and suffered occasional flooding incidents, and eventually the Archive was relocated to a new centre at Perivale Park, [[Perivale]], three miles north of the old site.&lt;ref name="Perivale 2 AtBBCblog"/&gt;&lt;ref name="Perivale Centre R4 Blog"/&gt; The new BBC Archive Centre was opened in Summer 2010 and all material was successfully moved by March 2011.&lt;ref name="Perivale 2 AtBBCblog"/&gt;&lt;ref name="Perivale 3 S&amp;Pblog"/&gt; The cost of the refurbishment and of the move was approximately £16.6 million.&lt;ref name=Perivale1/&gt;&lt;ref name="Perivale 2 AtBBCblog"&gt;{{cite news|last=Skinner|first=Peter|title=A new home for the BBC Archive|url=http://www.bbc.co.uk/blogs/aboutthebbc/2010/08/a-warm-balmy-afternoon-in.shtml|accessdate=19 January 2012|newspaper=BBC About the BBC Blog|date=20 August 2010}}&lt;/ref&gt;&lt;ref name="Perivale Centre R4 Blog"&gt;{{cite news|last=Bolton|first=Roger|title=Tears in Perivale – Feedback in the archives|url=http://www.bbc.co.uk/blogs/radio4/2011/09/tears_in_perivale_feedback_in_the_archives.html|accessdate=19 January 2012|newspaper=BBC Radio 4 and 4 Extra Blog|date=23 September 2011}}&lt;/ref&gt;&lt;ref name="Perivale 3 S&amp;Pblog"&gt;{{cite news|last=Kane|first=Chris|title=Preserving the past at Perivale|url=http://www.bbc.co.uk/blogs/spacesandplaces/2011/03/preserving_the_past_at_perival.shtml|accessdate=19 January 2012|newspaper=BBC Spaces &amp; Places Blog|date=9 March 2011}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://downloads.bbc.co.uk/foi/classes/disclosure_logs/rfi20111170_cost_of_new_archive_centre.pdf|title=Freedom of Information Act 2000 – RFI20111170|last=Jupe|first=Steve|date=20 October 2011|work=Freedom of Information Request|publisher=BBC|accessdate=7 February 2012}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=The BBC Archive Centre has moved|url=http://www.bbc.co.uk/commissioning/news/the-bbc-archive-centre-has-moved.shtml|work=BBC Commissioning|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt;

Material is stored in thirteen vaults,&lt;ref name="Perivale 2 AtBBCblog"/&gt; controlled to match the best climate for the material inside them,&lt;ref name=Perivale1/&gt;&lt;ref name=gutechweekly /&gt;&lt;ref name="BBC Archive BBCInternetblog"/&gt;&lt;ref name="Perivale 2 AtBBCblog"/&gt; and named after a different BBC personality depending on the content contained in them.&lt;ref name="Perivale 2 AtBBCblog"/&gt; In addition to the vaults, new editing and workrooms have been added so that the material can easily be transferred between formats as well as viewed and restored.&lt;ref name="Perivale 2 AtBBCblog"/&gt; The building has also been fitted with fire suppression systems to protect the archive in the event of an incident at the centre, so the total loss of the archive is avoided.&lt;ref name="Perivale Centre R4 Blog"/&gt;

==Television Archive==
The '''BBC Television Archive''' contains over 600,000 hours of television broadcast material&lt;ref name="BBCArchive TV 1"&gt;{{cite web|last=Lee|first=Adam|title=BBC Television Archive – What's in the BBC Archive|url=http://www.bbc.co.uk/archive/tv_archive.shtml|work=BBC Archive – Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; located on 600,000/650,000 film reels&lt;ref name="Windmill Road"&gt;{{cite web|title=A Tour of the BBC Archive at Windmill Road|url=https://www.youtube.com/watch?v=S3Z2djrAW2M|publisher=[[BBC]]|accessdate=23 July 2015|date=13 Aug 2010}}&lt;/ref&gt;&lt;ref name="BBC Archive BBCInternetblog"&gt;{{cite news|last=Williams|first=Adrian|title=Safeguarding the BBC's archive|url=http://www.bbc.co.uk/blogs/bbcinternet/2010/08/safeguarding_the_bbcs_archive.html|accessdate=19 January 2012|newspaper=BBC Internet Blog|date=18 August 2010}}&lt;/ref&gt;&lt;ref name="BBCArchive PTV 4"&gt;{{cite web|last=Williams|first=Adrian|title=Preserving the Television Archive – Film|url=http://www.bbc.co.uk/archive/preserving.shtml?chapter=4|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; and 2.4/2.7 million videotapes.&lt;ref name="BBC Archive BBCInternetblog"/&gt;&lt;ref name="Windmill Road"/&gt; The archive itself holds extensive material from approximately the mid-1970s onwards, when important recordings at the broadcaster were retained for the future.&lt;ref name="BBCArchives TV 6"&gt;{{cite web|last=Lee|first=Adam|title=BBC Television Archive – When did the BBC start to ensure that important broadcasts were not destroyed|url=http://www.bbc.co.uk/archive/tv_archive.shtml?chapter=6|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt;

Recordings from before this date are less comprehensively preserved; the process of [[Kinescope|telerecording]] was originally invented in 1947&lt;ref name="BBCArchive TV 2"&gt;{{cite web|last=Lee|first=Adam|title=BBC Television Archive – Why aren't there many recordings from the early days of television|url=http://www.bbc.co.uk/archive/tv_archive.shtml?chapter=2|work=BBC Archive – Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; while videotape recording was gradually introduced from the late 1950s onwards,&lt;ref name="BBC Archive TV 4"&gt;{{cite web|last=Lee|first=Adam|title=BBC Television Archive – When did the BBC start recording programmes regularly|url=http://www.bbc.co.uk/archive/tv_archive.shtml?chapter=4|work=BBC Archive – Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; but due to the expense of the tapes,&lt;ref name="BBCArchives PTV 8"&gt;{{cite web|last=Williams|first=Adrian|title=Preserving the Television Archive – Why was videotape invented|url=http://www.bbc.co.uk/archive/preserving.shtml?chapter=8|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; recording was seen for production use only with recordings subsequently being [[wiping|wiped]].&lt;ref name="BBC Archive TV 4"/&gt; or telerecordings being junked. The exceptions in the early years were usually occasions of great importance, such as the [[coronation of Queen Elizabeth II]].&lt;ref name="BBCArchive TV 2"/&gt; In addition, numerous programmes at the time were broadcast 'live' and so utilised no recording procedure in the production process.&lt;ref name="BBCArchive TV 2"/&gt; The earliest item in the collection is from 1936.&lt;ref name="BBC Archive BBCInternetblog"/&gt;&lt;ref name="Perivale 3 S&amp;Pblog"/&gt;&lt;ref name="BBCArchives PTV 6"&gt;{{cite web|last=Williams|first=Adrian|title=Preserving the Television Archive – The oldest BBC Television film clip|url=http://www.bbc.co.uk/archive/preserving.shtml?chapter=6|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt;

In 2013 there were 340,000 D3 Tapes but the hardware they have could only transfer up to 130,000 D3 tapes.&lt;ref name=Digitising&gt;{{cite web|title=Digitising the BBC archive|url=http://www.bbc.co.uk/academy/technology/article/art20130704121742520|publisher=[[BBC]]|accessdate=23 July 2015|date=2013}}&lt;/ref&gt; The BBC has had to be very selective of what they are transferring.&lt;ref name=Digitising/&gt;

Before anything is put into the archive a team of Digitisation Operators watch and listen to programs looking for problems with the tapes or transfers.&lt;ref name="Sarah Bello"&gt;{{cite web|title=Sarah Bello, BBC Archive|url=https://www.youtube.com/watch?v=MyG1lSdsKQs|publisher=[[BBC]]|accessdate=23 July 2015|date=11 March 2013}}&lt;/ref&gt; 

Today, the majority of programmes are kept, including news, entertainment, drama and a selection of other long-running programmes such as quiz shows.&lt;ref name="BBCArchive TV 7"&gt;{{cite web|last=Lee|first=Adam|title=BBC Television Archive – How does the BBC decide what to keep in its archive today|url=http://www.bbc.co.uk/archive/tv_archive.shtml?chapter=7|work=BBC Archive – Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; The remaining material from the television archive is offered to the [[British Film Institute]] prior to being disposed of.&lt;ref name="BBCArchive TV 8"&gt;{{cite web|last=Lee|first=Adam|title=BBC Television Archive – Does the BBC offer recordings it's not keeping for the archive to anyone else|url=http://www.bbc.co.uk/archive/tv_archive.shtml?chapter=8|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt;

==Sound Archive==
{{main|BBC Sound Archive}}

The '''BBC Sound Archive''' contains the archived output from the BBC's radio output. Widespread recordings exist in the archive from the mid-1930s, when recording of programmes and speeches were kept for rebroadcast; the catalyst for this was the launch of the [[BBC Empire Service]] in 1932 and the subsequent rebroadcast of speeches from political leaders at a time convenient in the different time zones.&lt;ref name="BBCArchive - Radio 3"&gt;{{cite web|last=Rooks|first=Simon|title=BBC Sound Archive – Why did the BBC start making recordings|url=http://www.bbc.co.uk/archive/sound_archive.shtml?chapter=3|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; Prior to this, the broadcast of recordings was seen as being false to the listener and was avoided.&lt;ref name="BBCArchives Radio 2"&gt;{{cite web|last=Rooks|first=Simon|title=BBC Sound Archive – Why aren't there many recordings from the early days of radio|url=http://www.bbc.co.uk/archive/sound_archive.shtml?chapter=2|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; Any recordings made were frequently disposed of and it was the efforts of [[Marie Slocombe]], who founded the Sound Archive in 1937 when she retained recordings of prominent figures in the country, that the archive became into being officially when she was appointed the Sounds Recording Librarian in 1941.&lt;ref name="BBCArchives - PRadio 6"&gt;{{cite web|last=Weaver|first=Julia|title=Preserving the Sound Archive – How did the Sound Archive begin|url=http://www.bbc.co.uk/archive/preserving_sound.shtml?chapter=6|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; Today, all of the BBC's radio output is recorded for re-use,&lt;ref name="BBCArchive - Radio 8"&gt;{{cite web|last=Rooks|first=Simon|title=BBC Sound Archive – Does the BBC keep copies of all programmes today|url=http://www.bbc.co.uk/archive/sound_archive.shtml?chapter=8|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; with approximately 66% of output being preserved in the Archives;&lt;ref name="BBCArchive - Radio 8"/&gt; programmes involving guests or live performances from artists are kept&lt;ref name="BBCArchive - Radio 8"/&gt; whereas programmes in which the DJ plays commercially available music are only sampled and not kept entirely.&lt;ref name="BBCArchive - Radio 8"/&gt; Prior to any material being disposed of, the material is offered to the [[British Library Sound Archive]].&lt;ref name="BBCArchive TV 8"/&gt;

The archive consists of a number of different formats including 200 [[Phonograph cylinder|wax cylinders]],&lt;ref name="BBCArchive - PRadio 3"&gt;{{cite web|last=Weaver|first=Julia|title=Preserving the Sound Archive – What are the earliest sound recordings|url=http://www.bbc.co.uk/archive/preserving_sound.shtml?chapter=3|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; numerous [[gramophone record]]s made from both [[shellac]] and [[vinyl]]&lt;ref name="BBCArchives - PRadio 4"&gt;{{cite web|last=Weaver|first=Julia|title=Preserving the Sound Archive – Discs|url=http://www.bbc.co.uk/archive/preserving_sound.shtml?chapter=4|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; as well as numerous more recordings on [[Reel-to-reel audio tape recording|tape]], CD and on [[digital audio tape]] (DAT).&lt;ref name="BBCArchive PRadio 5"&gt;{{cite web|last=Weaver|first=Julia|title=Preserving the Sound Archive – Tape|url=http://www.bbc.co.uk/archive/preserving_sound.shtml?chapter=5|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; The difficulty of these different formats is the availability of the machines required to play them; some of the vinyl records in the archive are 16 inches in size and require large [[phonograph]] units to play,&lt;ref name="BBCArchives - PRadio 4"/&gt; while the players for the wax cylinders and DATs are no longer in production.&lt;ref name="BBCArchive PRadio 5"/&gt; There are 700,00 vinyl records, 180,000 78's records, 400,000 [[LP record]] and 350,000 [[Compact disc|Cd's]] in the archive.&lt;ref name="Windmill Road"/&gt;

The oldest item is a wax cylinder containing a recording made by [[Florence Nightingale]], recorded on 30 July 1890.&lt;ref name="BBCArchive - PRadio 3"/&gt; Another unique item is the gramophone record from [[Mary of Teck|Queen Mary]]'s [[doll house]], which is approximately an inch in size and had the [[God Save the Queen|national anthem]] on it.&lt;ref name="BBCArchives - PRadio 4"/&gt;

The Sound Archive is based at the new BBC Archive Centre in Perivale, along with the television archive,&lt;ref name=Perivale1/&gt;&lt;ref name="Perivale 3 S&amp;Pblog"/&gt; and was previously based at Windmill Road, Brentford.

==Written Archives==
The '''BBC Written Archive''' contains all the internal written documents and communications from the corporation from the launch in 1922 to the present day.&lt;ref name="Written Archives BBC Story"&gt;{{cite web|title=The Written Archives|url=http://www.bbc.co.uk/historyofthebbc/contacts/wac.shtml|work=The BBC Story|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt;&lt;ref name="BBCArchive Written 1"&gt;{{cite web|last=Kavanagh|first=Jacquie|title=BBC Written Archives – What are the BBC Written Archives|url=http://www.bbc.co.uk/archive/written.shtml|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; Its collections shed light into the behind the scenes workings of the corporation and also elaborate on the difficulties of getting a television or radio programme to or off the air as the case may be.&lt;ref name="BBC Archive Written 3"&gt;{{cite web|last=Kavanagh|first=Jacquie|title=BBC Written Archives – What do the documents reveal|url=http://www.bbc.co.uk/archive/written.shtml?chapter=3|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; The archive guidelines state that access to files post-1980 is restricted due to the current nature of the files; the general exception to this rule are documents such as scripts and Programme as Broadcast records.

The Written Archives are located at the BBC Written Archive Centre in [[Caversham, Berkshire]], near [[Reading, Berkshire|Reading]].&lt;ref name="Written Archives BBC Story"/&gt; The centre houses the archive on four and a half miles of shelving along with reading rooms. The centre is different from the other BBC Archives in that the centre opens for writers and academic researchers in higher education.&lt;ref name="Written Archives BBC Story"/&gt;

==Photographic Library==
The '''BBC Photographic Library''' is responsible for approximately 10 million images,&lt;ref name="BBCArchive Photo 1"&gt;{{cite web|last=Dewar|first=Natalie|title=Photographic Library – What's in the BBC Photo Library|url=http://www.bbc.co.uk/archive/photo_library.shtml|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; dating back to 1922,&lt;ref&gt;{{cite web|title=BBC Pictures|url=http://www.bbc.co.uk/mediacentre/pictures/index.html|work=BBC Media Centre|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; created for publicity purposes and subsequently kept for future use.&lt;ref name="BBCArchive Photo 2"&gt;{{cite web|last=Dewar|first=Natalie|title=Photographic Library – Why does the BBC have photographs|url=http://www.bbc.co.uk/archive/photo_library.shtml?chapter=2|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; In addition to programme promotion, a large number of images are of historic events which are often incorporate into the daily news bulletins; as a result, half the photographic library team work specifically with these images.&lt;ref name="BBCArchive Photo 4"&gt;{{cite web|last=Dewar|first=Natalie|title=Photographic Library – The Team|url=http://www.bbc.co.uk/archive/photo_library.shtml?chapter=4|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; The images themselves are kept as originals in the archive, with digitisation only utilised when a specific image is required for use, when the image is sent in a digital format.&lt;ref name="BBCArchive Photo 5"&gt;{{cite web|last=Dewar|first=Natalie|title=Photographic Library – What format are the images stored on|url=http://www.bbc.co.uk/archive/photo_library.shtml?chapter=5|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; Copies of images are also used in case any images are damaged, notable due to [[vinegar syndrome]].&lt;ref name="BBCArchive Photo 6"&gt;{{cite web|last=Dewar|first=Natalie|title=Photographic Library – Preservation|url=http://www.bbc.co.uk/archive/photo_library.shtml?chapter=6|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; The BBC Photographic library itself is based within [[BBC Television Centre]], London.

The most popular images from the Archive include [[Colin Firth]] in ''[[Pride and Prejudice (1995 TV series)|Pride and Prejudice]]'', [[Michael Parkinson]] interviewing [[Muhammad Ali]], [[Jimmy Savile]] presenting the first ''[[Top of the Pops]]'', [[Martin Bashir]] interviewing [[Diana, Princess of Wales]] and a picture of [[Delia Derbyshire]] at work in the Radiophonic workshop at the BBC.&lt;ref name="BBCArchive Photo 8"&gt;{{cite web|last=Dewar|first=Natalie|title=Photographic Library – Our Top 10|url=http://www.bbc.co.uk/archive/photo_library.shtml?chapter=8|work=BBC Archive – Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt;

==Heritage Collection==
The '''BBC Heritage Collection''' is the newest of the BBC Archives and holds a variety of historic broadcast technology, art, props and merchandise.&lt;ref name="BBCArchives Heritage 2"&gt;{{cite web|last=O'Connell|first=Rory|title=BBC Heritage Collection – Where do the items come from|url=http://www.bbc.co.uk/archive/heritage.shtml?chapter=2|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; The collection was created out of personal collections and bequeaths by former staff members, as the BBC had no formal policy on the heritage collection until c.2003.&lt;ref name="BBCArchives Heritage 2"/&gt;

The collection includes, amongst other items, the BBC One Noddy Globe and clock,&lt;ref name="BBCArchive Heritage 3"&gt;{{cite web|last=O'Connell|first=Rory|title=BBC Heritage Collection – Broadcast technology|url=http://www.bbc.co.uk/archive/heritage.shtml?chapter=3|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; a [[BBC-Marconi Type A]] microphone,&lt;ref name="BBCArchive Heritage 3"/&gt; an early [[crystal radio]] made by the [[British Broadcasting Company]],&lt;ref name="BBCArchive Heritage 3"/&gt; a [[405-line television system|Marconi/EMI camera]] used in the early [[BBC Television]] experiments,&lt;ref name="BBCArchive Heritage 3"/&gt; a [[BBC Micro]] computer&lt;ref name="BBCArchive Heritage 3"/&gt; and a selection of items used to create [[Foley (filmmaking)|Foley]].&lt;ref name="BBCArchive Heritage 3"/&gt; In addition to all the broadcast technology, art is also kept, namely the portraits of all the BBC [[Director-General of the BBC|Director General]]s,&lt;ref name="BBCArchive Heritage 4"&gt;{{cite web|last=O'Connell|first=Rory|title=BBC Heritage Collection – Art|url=http://www.bbc.co.uk/archive/heritage.shtml?chapter=4|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; as well as props including an original [[TARDIS]] from ''[[Doctor Who]]''&lt;ref name="BBCArchive Heritage 5"&gt;{{cite web|last=O'Connell|first=Rory|title=BBC Heritage Collection – Costumes and Props|url=http://www.bbc.co.uk/archive/heritage.shtml?chapter=5|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt; and the children's television puppet [[Gordon the Gopher]].&lt;ref name="BBCArchive Heritage 5"/&gt;

The heritage collection itself has no one permanent home, as the majority of objects are on display, either around BBC properties or on loan to museums or other collections; the most notable museum housing the collection is the [[National Media Museum]] in [[Bradford]].&lt;ref name="BBCArchive Heritage 8"&gt;{{cite web|last=O'Connell|first=Rory|title=BBC Heritage Collection – Where can I see items from the collection|url=http://www.bbc.co.uk/archive/heritage.shtml?chapter=8|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}&lt;/ref&gt;

==Archive Treasure Hunt==
At the turn of the millennium, the BBC launched the '''BBC Archive Treasure Hunt''', a public appeal to recover pre-1980s lost BBC radio and television productions.&lt;ref&gt;{{cite web|url=http://www.bbc.co.uk/cult/treasurehunt/about/about.shtml |title=BBC Online – Cult – Treasure Hunt – About the Campaign |publisher=Bbc.co.uk |date= |accessdate=30 July 2010| archiveurl= https://web.archive.org/web/20100721235531/http://www.bbc.co.uk/cult/treasurehunt/about/about.shtml| archivedate= 21 July 2010 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt; Original material, featuring several popular programmes were lost due to the practice of [[wiping]], because of copyright issues and for technological reasons.&lt;ref&gt;{{cite web|url=http://www.bbc.co.uk/cult/treasurehunt/about/lost.shtml |title=BBC Online – Cult – Treasure Hunt – About the Campaign |publisher=Bbc.co.uk |date= |accessdate=30 July 2010}}&lt;/ref&gt;&lt;ref&gt;{{cite web|author=Stuart Douglas - www.thiswaydown.org |url=http://www.btinternet.com/~m.brown1/bbchunt.htm |title=missing episodes articles |publisher=Btinternet.com |date=7 July 1965 |accessdate=30 July 2010| archiveurl= https://web.archive.org/web/20100814103420/http://www.btinternet.com/~m.brown1/bbchunt.htm| archivedate= 14 August 2010 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;

The resolution of this appeal was that over one hundred productions were recovered&lt;ref&gt;{{cite web|url=http://fiatifta.org/aboutfiat/news/old/2001/2001-04/03.light.html |title=No 4 2001 – Missing Believed Wiped |publisher=Fiat/Ifta |date= |accessdate=30 July 2010| archiveurl= https://web.archive.org/web/20100716032923/http://fiatifta.org/aboutfiat/news/old/2001/2001-04/03.light.html| archivedate= 16 July 2010 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt; including ''[[The Men from the Ministry]]'', ''[[Something to Shout About (film)|Something To Shout About]]'', ''[[Man and Superman]]'', ''[[The Doctor's Dilemma (play)|The Doctor's Dilemma]]'', ''[[I'm Sorry, I'll Read That Again]]'', ''[[Hancock's Half Hour]]'', ''[[I'm Sorry, I Haven't A Clue]]'' and ''[[The Ronnie Corbett Thing]]'' in addition to recording sessions with [[Elton John]], [[Ringo Starr]] and [[Paul Simon]].&lt;ref name="BBCTH"&gt;{{cite web|url=http://www.bbc.co.uk/cult/treasurehunt/about/listoffinds.shtml |title=BBC Online – Cult – Treasure Hunt – List of Finds |publisher=Bbc.co.uk |date= |accessdate=30 July 2010}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.allbusiness.com/services/motion-pictures/4848337-1.html |title='hunt' Unearths BBC Treasures From Radio, Tv &amp;#124; Business solutions from |publisher=AllBusiness.com |date=9 November 2001 |accessdate=30 July 2010}}&lt;/ref&gt; Also, the Peter Sellers Estate Collection donated numerous recordings featuring [[Peter Sellers]].&lt;ref name="BBCTH"/&gt;

==Creative Archive Licence==
The BBC together with the [[British Film Institute]], the [[Open University]], [[Channel 4]] and [[Teachers' TV]] formed a collaboration, named the Creative Archive Licence Group, to create a copyright licence for the re-release of archived material.&lt;ref name=CAL&gt;{{cite web|title=Creative Archive pilot|url=http://www.bbc.co.uk/creativearchive/|publisher=BBC|accessdate=31 March 2016}}&lt;/ref&gt;

The Licence was a trial, launched in 2005, and notable for the re-release of part of the [[BBC News]]' archive and programmes made by the [[BBC Natural History Unit]] for creative use by the public. While artists and teachers were encouraged to use the content to create works of their own, the terms of the licence were restrictive compared to [[copyleft]] licences. Use of Creative Archive content for commercial, "endorsement, campaigning, defamatory or derogatory purposes" was forbidden, any derivative works were to be released under the same licence, and content was only  to be used within the UK.&lt;ref name=CAL/&gt;&lt;ref&gt;{{cite web|title=Creative Archive License|url=http://news.bbc.co.uk/1/hi/help/4527506.stm|publisher=BBC|accessdate=17 January 2012}}&lt;/ref&gt; The trial ended in 2006 following a review by the [[BBC Trust]] and works released under the licence were withdrawn.&lt;ref name=CAL/&gt;

==Voices from the archives==
Voices from the Archives is a former [[BBC]] project, launched in partnership with [[BBC Four]] that provided free access to audio interviews with various notable people and professions from a variety of political, religious and social backgrounds. The website ceased to be updated in June 2005, and the concept was instead adopted by [[BBC Radio 4]] as a collection of film interviews from various programmes.

==Programme catalog==
{{main|BBC Programme Catalogue}}
Over the years there the BBC has used various Programme catalog databases to keep a record of the programmes in the archives. Internal databases include [[Infax]] and [[BBC Fabric|Fabric]], and publicly accessible databases include [[BBC Genome]] and [http://www.bbc.co.uk/programmes BBC Programmes].

==See also==
{{portal|BBC}}
* [[BBC Genome Project]]
* [[Lost film]]
* [[Film preservation]]
* [[Missing Believed Wiped]]
* [[Telerecording]]
* [[Doctor Who missing episodes|''Doctor Who'' missing episodes]]
* [[Timeline of the BBC]]

==References==
{{reflist|2}}

==External links==

===BBC Archives===
*{{bbc.co.uk|id=archive|title=BBC Archives}}
*{{bbc.co.uk|id=bbcfour/collections|title=BBC Four – Collections}}
*{{bbc.co.uk|id=archive/archive_pioneers|title=BBC Archive collection – Archive Pioneers: Saviours of sound at the BBC}}
*{{bbc.co.uk|id=programmes|title=BBC Programmes}}
*{{bbc.co.uk|id=informationandarchives|title=BBC Information and Archives}}
* [https://www.theguardian.com/technology/blog/audio/2010/aug/18/bbc-archive-roly-keating-windmill-road Tech Weekly podcast: In the BBC archives] from ''[[The Guardian]]'' website.
* [https://www.dropbox.com/s/rz1o57nzlsf1v04/BBC%20Archive%20Collections%20Guide%202012.pdf?dl=0 BBC Archive Collections: What's In The Archives, And How To Use Them]

===Wiped Material===
* [http://www.missing-episodes.com/ British TV Missing Episodes Index]
* [http://www.wipednews.com/ Wiped News.Com – A news and features website devoted to missing TV, Film &amp; Radio]

{{BBC}}

[[Category:BBC]]
[[Category:BBC New Media|Archives]]
[[Category:Data management]]
[[Category:Broadcasting websites]]
[[Category:British websites]]
[[Category:BBC offices, studios and buildings|Archives]]
[[Category:Organisations based in Reading, Berkshire]]
[[Category:History of television in the United Kingdom]]
[[Category:History of radio]]
[[Category:BBC history]]
[[Category:Year of establishment missing]]
[[Category:Archives in Berkshire]]
[[Category:Television archives]]</text>
      <sha1>1j5euqw83ymiltdatnus7k813c0zcws</sha1>
    </revision>
  </page>
  <page>
    <title>Data independence</title>
    <ns>0</ns>
    <id>1786411</id>
    <revision>
      <id>750567967</id>
      <parentid>750555610</parentid>
      <timestamp>2016-11-20T15:31:01Z</timestamp>
      <contributor>
        <username>KylieTastic</username>
        <id>2790592</id>
      </contributor>
      <comment>Reverted [[WP:AGF|good faith]] edits by [[Special:Contributions/182.64.133.11|182.64.133.11]] ([[User talk:182.64.133.11|talk]]): Revert asserting National varieties of English (see [[WP:ENGVAR]]). ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5481" xml:space="preserve">{{multiple issues|
{{Cleanup|date=January 2008}}
{{Unreferenced|date=December 2009}}
}}

'''Data independence''' is the type of [[data]] transparency that matters for a centralised [[Database management system|DBMS]]. It refers to the immunity of user [[application software|applications]] to changes made in the definition and organization of data.

Physical data independence deals with hiding the details of the storage structure from user applications. The application should not be involved with these issues, since there is no difference in the operation carried out against the data.

The data independence and operation independence together gives the feature of [[data abstraction]]. There are two levels of data independence.

==First Level of Data Independence==
The [[logical]] structure of the data is known as the 'schema definition'. In general, if a user application operates on a subset of the [[Attribute (computing)|attributes]] of a [[Relation (database)|relation]], it should not be affected later when new attributes are added to the same relation.
Logical data independence indicates that the conceptual schema can be changed without affecting the existing schemas.

==Second Level of Data Independence==
The physical structure of the data is referred to as "physical data description". Physical data independence deals with hiding the details of the storage structure from user applications. The application should not be involved with these issues since, conceptually, there is no difference in the operations carried out against the data. There are three types of data independence:
# Logical data independence: The ability to change the logical (conceptual) schema without changing the External schema (User View) is called logical data independence. For example, the addition or removal of new entities, attributes, or relationships to the conceptual schema should be possible without having to change existing external schemas or having to rewrite existing application programs.
# Physical data independence: The ability to change the physical schema without changing the logical schema is called physical data independence. For example, a change to the internal schema, such as using different file organization or storage structures, storage devices, or indexing strategy, should be possible without having to change the conceptual or external schemas.
#View level data independence: always independent no effect, because there doesn't exist any other level above view level.

===Data Independence===

Data independence can be explained as follows: Each higher level of the data architecture is immune to changes of the next lower level of the architecture.

The logical scheme stays unchanged even though the storage space or type of some data is changed for reasons of optimization or reorganization. In this external schema does not change. In this internal schema changes may be required due to some physical schema were reorganized here.  Physical data independence is present in most databases and file environment in which hardware storage of encoding, exact location of data on disk,merging of records, so on this are hidden from user.

One of the biggest advantage of databases is data independence. It means we can change the conceptual schema at one level without affecting the data at another level. It also means we can change the structure of a database without affecting the data required by users and programs. This feature was not available in the file oriented approach.

==Data Independence Types==

The ability to modify schema definition in one level without affecting schema definition in the next higher level is called data independence. There are two levels of data independence, they are Physical data independence and Logical data independence.

# Physical data independence is the ability to modify the physical schema without causing application programs to be rewritten. Modifications at the physical level are occasionally necessary to improve performance. It means we change the physical storage/level without affecting the conceptual or external view of the data. The new changes are absorbed by mapping techniques.
# Logical data independence is the ability to modify the logical schema without causing application program to be rewritten. Modifications at the logical level are necessary whenever the logical structure of the database is altered (for example, when money-market accounts are added to banking system).  Logical Data independence means if we add some new columns or remove some columns from table then the user view and programs should not change. For example: consider two users A &amp; B. Both are selecting the fields "EmployeeNumber" and "EmployeeName". If user B adds a new column (e.g. salary) to his table, it will not effect the external view for user A, though the internal schema of the database has been changed for both users A &amp; B. 

Logical data independence is more difficult to achieve than physical data independence, since application programs are heavily dependent on the logical structure of the data that they access.

Physical data independence means we change the physical storage/level without affecting the conceptual or external view of the data. Mapping techniques absorbs the new changes.

==See also==
* [[Network transparency]]
* [[Replication transparency]]
* [[Codd's 12 rules]]
* [[ANSI-SPARC_Architecture]]


{{DEFAULTSORT:Data Independence}}
[[Category:Data management]]</text>
      <sha1>3ywft3bvsfhrckpsdzvtxmy0di1jztg</sha1>
    </revision>
  </page>
  <page>
    <title>Distributed transaction</title>
    <ns>0</ns>
    <id>619053</id>
    <revision>
      <id>729608163</id>
      <parentid>729608081</parentid>
      <timestamp>2016-07-13T11:01:02Z</timestamp>
      <contributor>
        <ip>59.163.27.11</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4129" xml:space="preserve">{{POV|Commitment ordering|date=November 2011}}
A '''distributed transaction''' is a [[database transaction]] in which two or more network hosts are involved. Usually, hosts provide '''transactional resources''', while the '''transaction manager''' is responsible for creating and managing a global transaction that encompasses all operations against such resources. Distributed transactions, as any other [[Database transaction|transactions]], must have all four [[ACID|ACID (atomicity, consistency, isolation, durability)]] properties, where atomicity guarantees all-or-nothing outcomes for the unit of work (operations bundle).

Open Group, a vendor consortium, proposed the [[X/Open XA|X/Open Distributed Transaction Processing (DTP) Model]] (X/Open XA), which became a de facto standard for behavior of transaction model components.

Database are common transactional resources and, often, transactions span a couple of such databases. In this case, a distributed transaction can be seen as a [[database transaction]] that must be [[Synchronization|synchronized]] (or provide [[ACID]] properties) among multiple participating [[database]]s which are [[distributed computing|distributed]] among different physical locations. The [[isolation (computer science)|isolation]] property (the I of ACID) poses a special challenge for multi database transactions, since the (global) [[serializability]] property could be violated, even if each database provides it (see also [[global serializability]]). In practice most commercial database systems use [[Two phase locking|strong strict two phase locking (SS2PL)]] for [[concurrency control]], which ensures global serializability, if all the participating databases employ it. (see also [[commitment ordering]] for multidatabases.)

A common [[algorithm]] for ensuring [[correctness (computer science)|correct]] completion of a distributed transaction is the [[two-phase commit]] (2PC). This algorithm is usually applied for updates able to [[commit (data management)|commit]] in a short period of time, ranging from couple of milliseconds to couple of minutes.

There are also long-lived distributed transactions, for example a transaction to book a trip, which consists of booking a flight, a rental car and a hotel. Since booking the flight might take up to a day to get a confirmation, two-phase commit is not applicable here, it will lock the resources for this long. In this case more sophisticated techniques that involve multiple undo levels are used. The way you can undo the hotel booking by calling a desk and cancelling the reservation, a system can be designed to undo certain operations (unless they are irreversibly finished).

In practice, long-lived distributed transactions are implemented in systems based on [[Web Services]]. Usually these transactions utilize principles of [[Compensating transaction]]s, Optimism and Isolation Without Locking. X/Open standard does not cover long-lived DTP.

Several modern technologies, including [[Enterprise Java Beans]] (EJBs) and [[Microsoft Transaction Server]] (MTS) fully support distributed transaction standards.

==See also==
* [[Java Transaction API|Java Transaction API (JTA)]]
* [[Enduro/X|Enduro/X Open source X/Open XA and XATMI implementation]]

==References==
* {{cite web | title=Web-Services Transactions | work=Web-Services Transactions | url=http://xml.sys-con.com/read/43755.htm | accessdate=May 2, 2005 }}
* {{cite web | title=Nuts And Bolts Of Transaction Processing | work=Article about Transaction Management | url=http://www.subbu.org/articles/transactions/NutsAndBoltsOfTP.html
| accessdate=May 3, 2005 }}
* {{cite web | title=A Detailed Comparison of Enterprise JavaBeans (EJB) &amp; The Microsoft Transaction Server (MTS) Models
 | url=http://gsraj.tripod.com/misc/ejbmtscomp.html }}

==Further reading==
* Gerhard Weikum, Gottfried Vossen, ''Transactional information systems: theory, algorithms, and the practice of concurrency control and recovery'', Morgan Kaufmann, 2002, ISBN 1-55860-508-8

{{DEFAULTSORT:Distributed Transaction}}
[[Category:Data management]]
[[Category:Transaction processing]]</text>
      <sha1>623uoin2ix7w9v0uywdhsi9ebvmjwx6</sha1>
    </revision>
  </page>
  <page>
    <title>SQL programming tool</title>
    <ns>0</ns>
    <id>12821559</id>
    <revision>
      <id>693449642</id>
      <parentid>667028191</parentid>
      <timestamp>2015-12-02T17:25:30Z</timestamp>
      <contributor>
        <username>Lfstevens</username>
        <id>1686264</id>
      </contributor>
      <minor />
      <comment>/* Debugging */clean up using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3884" xml:space="preserve">{{unreferenced|date=April 2010}}
In the field of [[software]], '''[[SQL]] programming tools''' provide platforms for [[database administrator]]s (DBAs) and [[application software|application]] developers to perform daily tasks efficiently and accurately.

Database administrators and application developers often face constantly changing environments  which they rarely completely control. Many changes result from new development projects or from modifications to existing code, which, when deployed to production, do not always produce the expected result.

For organizations to better manage development projects and the [[team]]s that develop code, suppliers of SQL programming tools normally provide more than facility to the database administrator or application developer to aid in database management and in quality [[Software deployment|code-deployment]] practices.

==Features==

SQL programming tools may include the following features:

===SQL editing===

SQL editors allow users to edit and execute SQL statements. They may support the following features:

* cut, copy, paste, undo, redo, find (and replace), bookmarks
* block indent, print, save file, uppercase/lowercase
* keyword highlighting
* auto-completion
* access to frequently used files
* output of query result
* editing query-results
* committing and rolling-back transactions
* inside cut paper

===Object browsing===

Tools may display information about [[database object]]s relevant to developers or to database administrators. Users may:

* view object descriptions
* view object [[Data Definition Language|definition]]s (DDL)
* create database objects
* enable and disable [[database trigger|trigger]]s and [[database constraints|constraint]]s
* recompile valid or invalid objects
* query or edit [[Table (database)|table]]s and [[View (database)|view]]s

Some tools also provide features to display dependencies among objects, and allow users to expand these dependent objects recursively (for example: packages may reference views, views generally reference tables, super/subtypes, and so on).

===Session browsing===

Database administrators and application developers can use session browsing tools to view the current activities of each user in the database. They can check the resource-usage of individual users, statistics information, locked objects and the current running SQL of each individual session.

===User-security management===

DBAs can create, edit, delete, disable or enable user-accounts in the database using security-management tools. DBAs can also assign [[database role|role]]s, system [[Privilege (computing)|privilege]]s, object privileges, and [[database storage|storage]]-quotas to users.

===Debugging===

Some tools offer features for the debugging of [[stored procedure]]s: [[program animation|Step In]], Step Over, Step Out, Run Until Exception, [[Breakpoint]]s, View &amp; Set Variables, View Call Stack, and so on. Users can debug any program-unit without making any modification to it, including triggers and [[object type]]s.

===Performance monitoring===

Monitoring tools may show the database resources — usage summary, service time summary, recent activities, top sessions, session history or top SQL — in easy-to-read graphs. Database administrators can easily monitor the health of various components in the monitoring instance. Application developers may also make use of such tools to diagnose and correct application-performance problems as well as improve SQL server performance.

===Test Data===

Test data generation tools can populate the database by realistic test data for server or client side testing purposes. Also, this kind of software can upload sample BLOB files to database.

==See also==
* [[Comparison of database tools]]

{{DEFAULTSORT:Sql Programming Tool}}
[[Category:Data management]]
[[Category:Relational database management systems]]</text>
      <sha1>hc4fhvthkd8taqtnydbfe6vniepcyac</sha1>
    </revision>
  </page>
  <page>
    <title>Association rule learning</title>
    <ns>0</ns>
    <id>577053</id>
    <revision>
      <id>763087018</id>
      <parentid>761499698</parentid>
      <timestamp>2017-02-01T07:01:13Z</timestamp>
      <contributor>
        <username>Monitor333</username>
        <id>16957266</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="33758" xml:space="preserve">{{Redirect|OneR|filmmaking technique|Long take}}
{{machine learning bar}}
'''Association rule learning''' is a [[rule-based machine learning]] method for discovering interesting relations between variables in large databases.  It is intended to identify strong rules discovered in databases using some measures of interestingness.&lt;ref name="piatetsky"&gt;Piatetsky-Shapiro, Gregory (1991), ''Discovery, analysis, and presentation of strong rules'', in Piatetsky-Shapiro, Gregory; and Frawley, William J.; eds., ''Knowledge Discovery in Databases'', AAAI/MIT Press, Cambridge, MA.&lt;/ref&gt;  Based on the concept of strong rules, [[Rakesh Agrawal (computer scientist)|Rakesh Agrawal]], [[Tomasz Imieliński]] and Arun Swami &lt;ref name="mining"&gt;{{Cite book | last1 = Agrawal | first1 = R. | last2 = Imieliński | first2 = T. | last3 = Swami | first3 = A. | doi = 10.1145/170035.170072 | chapter = Mining association rules between sets of items in large databases | title = Proceedings of the 1993 ACM SIGMOD international conference on Management of data  - SIGMOD '93 | pages = 207 | year = 1993 | isbn = 0897915925 | pmid =  | pmc = }}&lt;/ref&gt; introduced association rules for discovering regularities between products in large-scale transaction data recorded by [[point-of-sale]] (POS) systems in supermarkets. For example, the rule &lt;math&gt;\{\mathrm{onions, potatoes}\} \Rightarrow \{\mathrm{burger}\}&lt;/math&gt; found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as, e.g., promotional [[pricing]] or [[product placement]]s. In addition to the above example from [[market basket analysis]] association rules are employed today in many application areas including [[Web usage mining]], [[intrusion detection]], [[Continuous production]], and [[bioinformatics]]. In contrast with [[sequence mining]], association rule learning typically does not consider the order of items either within a transaction or across transactions.

== Definition ==
{|class="wikitable" style="float: right; margin-left: 1em;"
|+ Example database with 5 transactions and 5 items
|-
! transaction ID !! milk !! bread !! butter !! beer !! diapers
|-
| 1 || 1 || 1 || 0 || 0 || 0
|-
| 2 || 0 || 0 || 1 || 0 || 0
|-
| 3 || 0 || 0 || 0 || 1 || 1
|-
| 4 || 1 || 1 || 1 || 0 || 0
|-
| 5 || 0 || 1 || 0 || 0 || 0
|-
|}

Following the original definition by Agrawal et al.&lt;ref name="mining" /&gt; the problem of association rule mining is defined as:

Let &lt;math&gt;I=\{i_1, i_2,\ldots,i_n\}&lt;/math&gt; be a set of &lt;math&gt;n&lt;/math&gt; binary attributes called ''items''.

Let &lt;math&gt;D = \{t_1, t_2, \ldots, t_m\}&lt;/math&gt; be a set of transactions called the ''database''.

Each ''transaction'' in &lt;math&gt;D&lt;/math&gt; has a unique transaction ID and contains a subset of the items in &lt;math&gt;I&lt;/math&gt;.

A ''rule'' is defined as an implication of the form:

&lt;math&gt;X \Rightarrow Y&lt;/math&gt;, where &lt;math&gt;X, Y \subseteq I&lt;/math&gt;.

In Agrawal et al.,&lt;ref name="mining" /&gt; a ''rule'' is defined only between a set and a single item, &lt;math&gt;X \Rightarrow i_j&lt;/math&gt; for &lt;math&gt;i_j \in I&lt;/math&gt;.

Every rule is composed by two different sets of items, also known as ''itemsets'', &lt;math&gt;X&lt;/math&gt; and &lt;math&gt;Y&lt;/math&gt;, where &lt;math&gt;X&lt;/math&gt; is called ''antecedent'' or left-hand-side (LHS) and &lt;math&gt;Y&lt;/math&gt; ''consequent'' or right-hand-side (RHS).

To illustrate the concepts, we use a small example from the supermarket domain. The set of items is &lt;math&gt;I= \{\mathrm{milk, bread, butter, beer, diapers}\}&lt;/math&gt; and in the table is shown a small database containing the items, where, in each entry, the value 1 means the presence of the item in the corresponding transaction, and the value 0 represents the absence of an item in that transaction.

An example rule for the supermarket could be &lt;math&gt;\{\mathrm{butter, bread}\} \Rightarrow \{\mathrm{milk}\}&lt;/math&gt; meaning that if butter and bread are bought, customers also buy milk.

Note: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant{{Citation needed|date=February 2015}}, and datasets often contain thousands or millions of transactions.

== Useful Concepts ==
In order to select interesting rules from the set of all possible rules, constraints on various measures of significance and interest are used. The best-known constraints are minimum thresholds on support and confidence.

Let &lt;math&gt;X&lt;/math&gt; be an itemset, &lt;math&gt;X \Rightarrow Y&lt;/math&gt; an association rule and &lt;math&gt;T&lt;/math&gt; a set of transactions of a given database.

=== Support ===
Support is an indication of how frequently the itemset appears in the database.

The support of &lt;math&gt;X&lt;/math&gt; with respect to &lt;math&gt;T&lt;/math&gt; is defined as the proportion of transactions &lt;math&gt;t&lt;/math&gt; in the database which contains itemset &lt;math&gt;X&lt;/math&gt;.

&lt;math&gt;\mathrm{supp}(X) = \frac{|\{t \in T; X \subseteq t\}|}{|T|}&lt;/math&gt;

In the example database, the itemset &lt;math&gt;X=\{\mathrm{beer, diapers}\}&lt;/math&gt; has a support of &lt;math&gt;1/5=0.2&lt;/math&gt; since it occurs in 20% of all transactions (1 out of 5 transactions). The argument of &lt;math&gt;\mathrm{supp}()&lt;/math&gt; is a set of preconditions, and thus becomes more restrictive as it grows (instead of more inclusive).&lt;ref name=":0"&gt;{{Cite journal|last=Hahsler|first=Michael|date=2005|title=Introduction to arules – A computational environment for mining association rules and frequent item sets|url=https://mran.revolutionanalytics.com/web/packages/arules/vignettes/arules.pdf|journal=Journal of Statistical Software|doi=|pmid=|access-date=}}&lt;/ref&gt;

=== Confidence ===
Confidence is an indication of how often the rule has been found to be true.

The ''confidence'' value of a rule, &lt;math&gt;X \Rightarrow Y&lt;/math&gt; , with respect to a set of transactions &lt;math&gt;T&lt;/math&gt;, is the proportion of the transactions that contains &lt;math&gt;X&lt;/math&gt; which also contains &lt;math&gt;Y&lt;/math&gt;.

Confidence is defined as:

&lt;math&gt;\mathrm{conf}(X \Rightarrow Y) = \mathrm{supp}(X \cup Y) / \mathrm{supp}(X)&lt;/math&gt;.

For example, the rule &lt;math&gt;\{\mathrm{butter,  bread}\} \Rightarrow \{\mathrm{milk}\}&lt;/math&gt; has a confidence of &lt;math&gt;0.2/0.2=1.0&lt;/math&gt; in the database, which means that for 100% of the transactions containing butter and bread the rule is correct (100% of the times a customer buys butter and bread, milk is bought as well).

Note that &lt;math&gt;\mathrm{supp}(X \cup Y)&lt;/math&gt; means the support of the union of the items in X and Y. This is somewhat confusing since we normally think in terms of probabilities of [[Event (probability theory)|events]] and not sets of items. We can rewrite &lt;math&gt;\mathrm{supp}(X \cup Y)&lt;/math&gt; as the joint probability &lt;math&gt;P(E_X \cup E_Y)&lt;/math&gt;, where &lt;math&gt;E_X&lt;/math&gt; and &lt;math&gt;E_Y&lt;/math&gt; are the events that a transaction contains itemset &lt;math&gt;X&lt;/math&gt; or &lt;math&gt;Y&lt;/math&gt;, respectively.&lt;ref name="michael.hahsler.net"&gt;Michael Hahsler (2015).  A Probabilistic Comparison of Commonly Used Interest Measures for Association Rules. http://michael.hahsler.net/research/association_rules/measures.html&lt;/ref&gt;

Thus confidence can be interpreted as an estimate of the conditional probability &lt;math&gt;P(E_Y | E_X)&lt;/math&gt;, the probability of finding the RHS of the rule in transactions under the condition that these transactions also contain the LHS.&lt;ref name=":0" /&gt;&lt;ref name="hipp"&gt;{{Cite journal | last1 = Hipp | first1 = J. | last2 = Güntzer | first2 = U. | last3 = Nakhaeizadeh | first3 = G. | title = Algorithms for association rule mining --- a general survey and comparison | doi = 10.1145/360402.360421 | journal = ACM SIGKDD Explorations Newsletter | volume = 2 | pages = 58 | year = 2000 | pmid =  | pmc = }}&lt;/ref&gt;

=== Lift ===
The ''[[lift (data mining)|lift]]'' of a rule is defined as:

&lt;math&gt; \mathrm{lift}(X\Rightarrow Y) = \frac{ \mathrm{supp}(X \cup Y)}{ \mathrm{supp}(X) \times \mathrm{supp}(Y) } &lt;/math&gt;

or the ratio of the observed support to that expected if X and Y were [[Independence (probability theory)|independent]].{{citation needed|reason=I couldn't find this in 'Witten: Data Mining - Practical Machine Learning Tools and Techniques'|date=May 2016}}

For example, the rule &lt;math&gt;\{\mathrm{milk, bread}\} \Rightarrow \{\mathrm{butter}\}&lt;/math&gt; has a lift of &lt;math&gt;\frac{0.2}{0.4 \times 0.4} = 1.25 &lt;/math&gt;.

If the rule had a lift of 1, it would imply that the probability of occurrence of the antecedent and that of the consequent are independent of each other. When two events are independent of each other, no rule can be drawn involving those two events.

If the lift is &gt; 1, that lets us know the degree to which those two occurrences are dependent on one another, and makes those rules potentially useful for predicting the consequent in future data sets.

The value of lift is that it considers both the confidence of the rule and the overall data set.&lt;ref name=":0" /&gt;

=== Conviction ===
The ''conviction'' of a rule is defined as &lt;math&gt; \mathrm{conv}(X\Rightarrow Y) =\frac{ 1 - \mathrm{supp}(Y) }{ 1 - \mathrm{conf}(X\Rightarrow Y)}&lt;/math&gt;.

For example, the rule &lt;math&gt;\{\mathrm{milk, bread}\} \Rightarrow \{\mathrm{butter}\}&lt;/math&gt; has a conviction of &lt;math&gt;\frac{1 - 0.4}{1 - 0.5} = 1.2 &lt;/math&gt;, and can be interpreted as the ratio of the expected frequency that X occurs without Y (that is to say, the frequency that the rule makes an incorrect prediction) if X and Y were independent divided by the observed frequency of incorrect predictions. In this example, the conviction value of 1.2 shows that the rule &lt;math&gt;\{\mathrm{milk, bread}\} \Rightarrow \{\mathrm{butter}\}&lt;/math&gt; would be incorrect 20% more often (1.2 times as often) if the association between X and Y was purely random chance.

== Process ==
[[File:FrequentItems.png|thumb|Frequent itemset lattice, where the color of the box indicates how many transactions contain the combination of items. Note that lower levels of the lattice can contain at most the minimum number of their parents' items; e.g. {ac} can have only at most &lt;math&gt;min(a,c)&lt;/math&gt; items. This is called the ''downward-closure property''.&lt;ref name="mining" /&gt;]] Association rules are usually required to satisfy a user-specified minimum support and a user-specified minimum confidence at the same time. Association rule generation is usually split up into two separate steps:
# A minimum support threshold is applied to find all ''frequent itemsets'' in a database.
# A minimum confidence constraint is applied to these frequent itemsets in order to form rules.
While the second step is straightforward, the first step needs more attention.

Finding all frequent itemsets in a database is difficult since it involves searching all possible itemsets (item combinations).  The set of possible itemsets is the [[power set]] over &lt;math&gt;I&lt;/math&gt; and has size &lt;math&gt;2^n-1&lt;/math&gt; (excluding the empty set which is not a valid itemset). Although the size of the power-set grows exponentially in the number of items &lt;math&gt;n&lt;/math&gt; in &lt;math&gt;I&lt;/math&gt;, efficient search is possible using the '''''downward-closure property''''' of support&lt;ref name="mining" /&gt;&lt;ref&gt;{{cite book |last1=Tan |first1=Pang-Ning |last2=Michael |first2=Steinbach |last3=Kumar |first3=Vipin |title=Introduction to Data Mining |publisher=[[Addison-Wesley]] |year=2005 |isbn=0-321-32136-7 |chapter=Chapter 6. Association Analysis: Basic Concepts and Algorithms |chapterurl=http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf }}&lt;/ref&gt; (also called ''anti-monotonicity''&lt;ref name="pei"&gt;Pei, Jian; Han, Jiawei; and Lakshmanan, Laks V. S.; ''Mining frequent itemsets with convertible constraints'', in ''Proceedings of the 17th International Conference on Data Engineering, April 2–6, 2001, Heidelberg, Germany'', 2001, pages 433-442&lt;/ref&gt;) which guarantees that for a frequent itemset, all its subsets are also frequent and thus no infrequent itemset can be a subset of a frequent itemset. Exploiting this property, efficient algorithms (e.g., Apriori&lt;ref name="apriori"&gt;Agrawal, Rakesh; and Srikant, Ramakrishnan; [http://rakesh.agrawal-family.com/papers/vldb94apriori.pdf ''Fast algorithms for mining association rules in large databases''], in Bocca, Jorge B.; Jarke, Matthias; and Zaniolo, Carlo; editors, ''Proceedings of the 20th International Conference on Very Large Data Bases (VLDB), Santiago, Chile, September 1994'', pages 487-499&lt;/ref&gt; and Eclat&lt;ref name="eclat"&gt;{{Cite journal | last1 = Zaki | first1 = M. J. | title = Scalable algorithms for association mining | doi = 10.1109/69.846291 | journal = IEEE Transactions on Knowledge and Data Engineering | volume = 12 | issue = 3 | pages = 372–390 | year = 2000 | pmid =  | pmc = }}&lt;/ref&gt;) can find all frequent itemsets.

==History==
The concept of association rules was popularised particularly due to the 1993 article of Agrawal et al.,&lt;ref name="mining" /&gt; which has acquired more than 18,000 citations according to Google Scholar, as of August 2015, and is thus one of the most cited papers in the Data Mining field. However, it is possible that what is now called "association rules" is similar to what appears in  the 1966 paper&lt;ref name="guha_oldest"&gt;Hájek, Petr; Havel, Ivan; Chytil, Metoděj; ''The GUHA method of automatic hypotheses determination'', Computing 1 (1966) 293-308&lt;/ref&gt; on GUHA, a general data mining method developed by [[Petr Hájek]] et al.&lt;ref name="pospaper"&gt;Hájek, Petr; Feglar, Tomas; Rauch, Jan; and Coufal, David; ''The GUHA method, data preprocessing and mining'', Database Support for Data Mining Applications, Springer, 2004, ISBN 978-3-540-22479-2&lt;/ref&gt;

An early (circa 1989) use of minimum support and confidence to find all association rules is the Feature Based Modeling framework, which found all rules with &lt;math&gt;\mathrm{supp}(X)&lt;/math&gt; and &lt;math&gt;\mathrm{conf}(X \Rightarrow Y)&lt;/math&gt; greater than user defined constraints.&lt;ref&gt;{{cite journal|last1=Webb|first1=Geoffrey|title=A Machine Learning Approach to Student Modelling|journal=Proceedings of the Third Australian Joint Conference on Artificial Intelligence (AI 89)|date=1989|pages=195–205}}&lt;/ref&gt;

== Alternative measures of interestingness ==
&lt;!-- would be nice to explain each measure --&gt;
In addition to confidence, other measures of ''interestingness'' for rules have been proposed. Some popular measures are:

*  All-confidence&lt;ref name="allconfidence"&gt;Omiecinski, Edward R.; ''Alternative interest measures for mining associations in databases'', IEEE Transactions on Knowledge and Data Engineering, 15(1):57-69, Jan/Feb 2003&lt;/ref&gt;
* Collective strength&lt;ref name="collectivestrength"&gt;Aggarwal, Charu C.; and Yu, Philip S.; ''A new framework for itemset generation'', in ''PODS 98, Symposium on Principles of Database Systems, Seattle, WA, USA, 1998'', pages 18-24&lt;/ref&gt;
*  Conviction&lt;ref name="brin-dynamic-itemset1"&gt;Brin, Sergey; Motwani, Rajeev; Ullman, Jeffrey D.; and Tsur, Shalom; ''Dynamic itemset counting and implication rules for market basket data'', in ''SIGMOD 1997, Proceedings of the ACM SIGMOD International Conference on Management of Data (SIGMOD 1997), Tucson, Arizona, USA, May 1997'', pp. 255-264&lt;/ref&gt;
*  Leverage&lt;ref name="leverage"&gt;Piatetsky-Shapiro, Gregory; ''Discovery, analysis, and presentation of strong rules'', Knowledge Discovery in Databases, 1991, pp. 229-248&lt;/ref&gt;
*  Lift (originally called interest)&lt;ref name="brin-dynamic-itemset2"&gt;Brin, Sergey; Motwani, Rajeev; Ullman, Jeffrey D.; and Tsur, Shalom; ''Dynamic itemset counting and implication rules for market basket data'', in ''SIGMOD 1997, Proceedings of the ACM SIGMOD International Conference on Management of Data (SIGMOD 1997), Tucson, Arizona, USA, May 1997'', pp. 265-276&lt;/ref&gt;

Several more measures are presented and compared by Tan et al.&lt;ref name="measurescomp"&gt;Tan, Pang-Ning; Kumar, Vipin; and Srivastava, Jaideep; ''Selecting the right objective measure for association analysis'', Information Systems, 29(4):293-313, 2004&lt;/ref&gt; and by Hahsler.&lt;ref name="michael.hahsler.net"/&gt; Looking for techniques that can model what the user has known  (and using these models as interestingness measures) is currently an active research trend under the name of "Subjective Interestingness."

== Statistically sound associations ==

One limitation of the standard approach to discovering associations is that by searching massive numbers of possible associations to look for collections of items that appear to be associated, there is a large risk of finding many spurious associations.  These are collections of items that co-occur with unexpected frequency in the data, but only do so by chance.  For example, suppose we are considering a collection of 10,000 items and looking for rules containing two items in the left-hand-side and 1 item in the right-hand-side.  There are approximately 1,000,000,000,000 such rules.  If we apply a statistical test for independence with a significance level of 0.05 it means there is only a 5% chance of accepting a rule if there is no association.  If we assume there are no associations, we should nonetheless expect to find 50,000,000,000 rules.  Statistically sound association discovery&lt;ref&gt;Webb, Geoffrey I. (2007); ''Discovering Significant Patterns'', Machine Learning 68(1), Netherlands: Springer, pp. 1-33 [http://link.springer.com/article/10.1007%2Fs10994-007-5006-x online access]&lt;/ref&gt;&lt;ref&gt;Gionis, Aristides; [[Heikki Mannila|Mannila, Heikki]]; Mielikäinen, Taneli; and Tsaparas, Panayiotis; ''Assessing Data Mining Results via Swap Randomization'', ACM Transactions on Knowledge Discovery from Data (TKDD), Volume 1, Issue 3 (December 2007), Article No. 14&lt;/ref&gt; controls this risk, in most cases reducing the risk of finding ''any'' spurious associations to a user-specified significance levels.

== Algorithms ==

Many algorithms for generating association rules were presented over time.

Some well-known algorithms are [[Apriori algorithm|Apriori]], Eclat and FP-Growth, but they only do half the job, since they are algorithms for mining frequent itemsets. Another step needs to be done after to generate rules from frequent itemsets found in a database.

=== Apriori algorithm ===
{{Main article|Apriori algorithm}}

Apriori&lt;ref name="apriori" /&gt; uses a breadth-first search strategy to count the support of itemsets and uses a candidate generation function which exploits the downward closure property of support.

=== Eclat algorithm ===

Eclat&lt;ref name="eclat" /&gt; (alt. ECLAT, stands for Equivalence Class Transformation) is a depth-first search algorithm using set intersection. It is a naturally elegant algorithm suitable for both sequential as well as parallel execution with locality-enhancing properties. It was first introduced by Zaki, Parthasarathy, Li and Ogihara in a series of papers written in 1997.

Mohammed Javeed Zaki, Srinivasan Parthasarathy, M. Ogihara, Wei Li:
New Algorithms for Fast Discovery of Association Rules. KDD 1997.

Mohammed Javeed Zaki, Srinivasan Parthasarathy, Mitsunori Ogihara, Wei Li:
Parallel Algorithms for Discovery of Association Rules. Data Min. Knowl. Discov. 1(4): 343-373 (1997)

=== FP-growth algorithm ===

FP stands for frequent pattern.&lt;ref&gt;{{cite journal|last1=Han|title=Mining Frequent Patterns Without Candidate Generation|journal=Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data|date=2000|volume=SIGMOD '00|pages=1–12|doi=10.1145/342009.335372}}&lt;/ref&gt;

In the first pass, the algorithm counts occurrence of items (attribute-value pairs) in the dataset, and stores them to 'header table'. In the second pass, it builds the FP-tree structure by inserting instances.
Items in each instance have to be sorted by descending order of their frequency in the dataset, so that the tree can be processed quickly.
Items in each instance that do not meet minimum coverage threshold are discarded.
If many instances share most frequent items, FP-tree provides high compression close to tree root.

Recursive processing of this compressed version of main dataset grows large item sets directly, instead of generating candidate items and testing them against the entire database.
Growth starts from the bottom of the header table (having longest branches), by finding all instances matching given condition.
New tree is created, with counts projected from the original tree corresponding to the set of instances that are conditional on the attribute, with each node getting sum of its children counts.
Recursive growth ends when no individual items conditional on the attribute meet minimum support threshold, and processing continues on the remaining header items of the original FP-tree.

Once the recursive process has completed, all large item sets with minimum coverage have been found, and association rule creation begins.&lt;ref&gt;Witten, Frank, Hall: Data mining practical machine learning tools and techniques, 3rd edition&lt;/ref&gt;

=== Others ===

==== AprioriDP ====
AprioriDP&lt;ref name="dharmesh2013" /&gt; utilizes [[Dynamic Programming]]  in Frequent itemset mining. The working principle is to eliminate the candidate generation like FP-tree, but it stores support count in specialized data structure instead of tree.

==== Context Based Association Rule Mining Algorithm ====
{{Main article|Context Based Association Rules}}

CBPNARM is an algorithm, developed in 2013, to mine association rules on the basis of context. It uses context variable on the basis of which the support of an itemset is changed on the basis of which the rules are finally populated to the rule set.

==== Node-set-based algorithms ====
FIN,&lt;ref name="deng2014" /&gt; PrePost &lt;ref name="deng2012" /&gt; and PPV &lt;ref name="deng2010" /&gt; are three algorithms based on node sets. They use nodes in a coding FP-tree to represent itemsets, and employ a depth-first search strategy to discovery frequent itemsets using "intersection" of node sets.

==== GUHA procedure ASSOC ====

[[GUHA]] is a general method for exploratory data analysis that has theoretical foundations in [[observational calculi]].&lt;ref name="ObservationalCalculi"&gt;Rauch, Jan; ''Logical calculi for knowledge discovery in databases'', in ''Proceedings of the First European Symposium on Principles of Data Mining and Knowledge Discovery'', Springer, 1997, pp. 47-57&lt;/ref&gt;

The ASSOC procedure&lt;ref&gt;{{cite book |last=Hájek |first=Petr |author2=Havránek, Tomáš |title=Mechanizing Hypothesis Formation: Mathematical Foundations for a General Theory |publisher=Springer-Verlag |year=1978 |isbn=3-540-08738-9 |url=http://www.cs.cas.cz/hajek/guhabook/ }}&lt;/ref&gt; is a GUHA method which mines for generalized association rules using fast [[bitstring]]s operations. The association rules mined by this method are more general than those output by apriori, for example "items" can be connected both with conjunction and disjunctions and the relation between antecedent and consequent of the rule is not restricted to setting minimum support and confidence as in apriori: an arbitrary combination of supported interest measures can be used.

==== OPUS search ====

OPUS is an efficient algorithm for rule discovery    that, in contrast to most alternatives, does not require either monotone or anti-monotone constraints such as minimum support.&lt;ref name=OPUS&gt;Webb, Geoffrey I. (1995); ''OPUS: An Efficient Admissible Algorithm for Unordered Search'', Journal of Artificial Intelligence Research 3, Menlo Park, CA: AAAI Press, pp. 431-465 [http://www.cs.washington.edu/research/jair/abstracts/webb95a.html online access]{{dead link|date=October 2016 |bot=InternetArchiveBot |fix-attempted=yes }}&lt;/ref&gt; Initially used to find rules for a fixed consequent&lt;ref name="OPUS" /&gt;&lt;ref name="Bayardo"&gt;{{Cite journal |doi=10.1023/A:1009895914772 |last1=Bayardo |first1=Roberto J., Jr. |last2=Agrawal |first2=Rakesh |last3=Gunopulos |first3=Dimitrios |year=2000 |title=Constraint-based rule mining in large, dense databases |journal=Data Mining and Knowledge Discovery |volume=4 |issue=2 |pages=217–240 }}&lt;/ref&gt; it has subsequently been extended to find rules with any item as a consequent.&lt;ref name="webb"&gt;Webb, Geoffrey I. (2000); ''Efficient Search for Association Rules'', in Ramakrishnan, Raghu; and Stolfo, Sal; eds.; ''Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2000), Boston, MA'', New York, NY: The Association for Computing Machinery, pp. 99-107 [http://www.csse.monash.edu/~webb/Files/Webb00b.pdf online access]&lt;/ref&gt; OPUS search is the core technology in the popular Magnum Opus association discovery system.

== Lore ==
A famous story about association rule mining is the "beer and diaper" story.  A purported survey of behavior of supermarket shoppers discovered that customers (presumably young men) who buy diapers tend also to buy beer. This anecdote became popular as an example of how unexpected association rules might be found from everyday data. There are varying opinions as to how much of the story is true.&lt;ref name="dss"&gt;http://www.dssresources.com/newsletters/66.php&lt;/ref&gt; Daniel Powers says:&lt;ref name="dss" /&gt;

&lt;blockquote&gt;In 1992, Thomas Blischok, manager of a retail consulting group at [[Teradata]], and his staff prepared an analysis of 1.2 million market baskets from about 25 Osco Drug stores. Database queries were developed to identify affinities. The analysis "did discover that between 5:00 and 7:00 p.m. that consumers bought beer and diapers". Osco managers did NOT exploit the beer and diapers relationship by moving the products closer together on the shelves.&lt;/blockquote&gt;

== Other types of association mining ==

'''Multi-Relation Association Rules''': Multi-Relation Association Rules (MRAR) is a new class of association rules which in contrast to primitive, simple and even multi-relational association rules (that are usually extracted from multi-relational databases), each rule item consists of one entity but several relations. These relations indicate indirect relationship between the entities. Consider the following MRAR where the first item consists of three relations ''live in'', ''nearby'' and ''humid'': “Those who ''live in'' a place which is ''near by'' a city with ''humid'' climate type and also are ''younger'' than 20 -&gt; their ''health condition'' is good”. Such association rules are extractable from RDBMS data or semantic web data.&lt;ref name="MRAR: Mining Multi-Relation Association Rules"&gt;Ramezani, Reza, Mohamad Saraee, and Mohammad Ali Nematbakhsh; ''MRAR: Mining Multi-Relation Association Rules'', Journal of Computing and Security, 1, no. 2 (2014)&lt;/ref&gt;

'''[[Context Based Association Rules]]''' is a form of association rule. '''Context Based Association Rules''' claims more accuracy in association rule mining by considering a hidden variable named context variable which changes the final set of association rules depending upon the value of context variables. For example the baskets orientation in market basket analysis reflects an odd pattern in the early days of month.This might be because of abnormal context i.e. salary is drawn at the start of the month &lt;ref name="Context Based Positive and Negative Spatio Temporal Association Rule Mining"&gt;Shaheen, M; Shahbaz, M; and Guergachi, A; ''Context Based Positive and Negative Spatio Temporal Association Rule Mining'', Elsevier Knowledge-Based Systems, Jan 2013, pp. 261-273&lt;/ref&gt;

'''[[Contrast set learning]]''' is a form of associative learning. '''Contrast set learners''' use rules that differ meaningfully in their distribution across subsets.&lt;ref name="webb03"&gt;{{cite conference
 | author = GI Webb and S. Butler and D. Newlands
 | year = 2003
 | title = On Detecting Differences Between Groups
 | conference = KDD'03 Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
| url= http://portal.acm.org/citation.cfm?id=956781
 }}
&lt;/ref&gt;&lt;ref name="busy"&gt;Menzies, Tim; and Hu, Ying; ''Data Mining for Very Busy People'', IEEE Computer, October 2003, pp. 18-25&lt;/ref&gt;

'''Weighted class learning''' is another form of associative learning in which weight may be assigned to classes to give focus to a particular issue of concern for the consumer of the data mining results.

'''High-order pattern discovery''' facilitate the capture of high-order (polythetic) patterns or event associations that are intrinsic to complex real-world data.
&lt;ref name="discovere"&gt;{{cite journal |last=Wong |first=Andrew K.C. |author2=Wang, Yang |title=High-order pattern discovery from discrete-valued data |journal=IEEE Transactions on Knowledge and Data Engineering (TKDE) |year=1997 |pages=877–893 }}&lt;/ref&gt;

'''[[K-optimal pattern discovery]]''' provides an alternative to the standard approach to association rule learning that requires that each pattern appear frequently in the data.

'''Approximate Frequent Itemset''' mining is a relaxed version of Frequent Itemset mining that allows some of the items in some of the rows to be 0.&lt;ref&gt;Jinze Liu, Susan Paulsen, Xing Sun, Wei Wang, Andrew Nobel, J. P. (2006). Mining approximate frequent itemsets in the presence of noise: Algorithm and analysis. Retrieved from http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.62.3805{{dead link|date=October 2016 |bot=InternetArchiveBot |fix-attempted=yes }}&lt;/ref&gt;

'''Generalized Association Rules''' hierarchical taxonomy (concept hierarchy)

'''Quantitative Association Rules''' categorical and quantitative data
&lt;ref name="quantminer"&gt;{{cite journal |last=Salleb-Aouissi |first=Ansaf |author2=Vrain, Christel|author3= Nortet, Cyril |title=QuantMiner: A Genetic Algorithm for Mining Quantitative Association Rules |journal=International Joint Conference on Artificial Intelligence (IJCAI) |year=2007 |pages=1035–1040 }}&lt;/ref&gt;

'''Interval Data Association Rules''' e.g. partition the age into 5-year-increment ranged

'''Maximal Association Rules'''

'''Sequential pattern mining ''' discovers subsequences that are common to more than minsup sequences in a sequence database, where minsup is set by the user. A sequence is an ordered list of transactions.&lt;ref name="sequence"&gt;Zaki, Mohammed J. (2001); ''SPADE: An Efficient Algorithm for Mining Frequent Sequences'', Machine Learning Journal, 42, pp. 31–60&lt;/ref&gt;

'''Sequential Rules''' discovering relationships between items while considering the time ordering. It is generally applied on a sequence database. For example, a sequential rule found in database of sequences of customer transactions can be that customers who bought a computer and CD-Roms, later bought a webcam, with a given confidence and support.

'''Subspace Clustering''', a specific type of [[Clustering high-dimensional data]], is in many variants also based on the downward-closure property for specific clustering models.&lt;ref name="ZimekAssent2014"&gt;{{cite journal|last1=Zimek|first1=Arthur|last2=Assent|first2=Ira|last3=Vreeken|first3=Jilles|title=Frequent Pattern Mining Algorithms for Data Clustering|year=2014|pages=403–423|doi=10.1007/978-3-319-07821-2_16}}&lt;/ref&gt;

'''Warmr '''is shipped as part of the ACE data mining suite. It allows association rule learning for first order relational rules.&lt;ref&gt;{{cite journal | pmid = 11272703 | volume=15 | issue=2 | title=Warmr: a data mining tool for chemical data. | date=Feb 2001 | journal=J Comput Aided Mol Des | pages=173–81}}&lt;/ref&gt;

==See also==
* [[Sequence mining]]
* [[Production system (computer science)]]
* [[Learning classifier system]]
* [[Rule-based machine learning]]

==References==
{{reflist|3|refs=
&lt;ref name="deng2014"&gt;Z. H. Deng and S. L. Lv. Fast mining frequent itemsets using Nodesets.[http://www.sciencedirect.com/science/article/pii/S0957417414000463]. Expert Systems with Applications, 41(10): 4505–4512, 2014.&lt;/ref&gt;
&lt;ref name="deng2012"&gt;Z. H. Deng, Z. Wang，and J. Jiang. A New Algorithm for Fast Mining Frequent Itemsets Using N-Lists [http://info.scichina.com:8084/sciFe/EN/abstract/abstract508369.shtml]. SCIENCE CHINA Information Sciences, 55 (9): 2008 - 2030, 2012.&lt;/ref&gt;
&lt;ref name="deng2010"&gt;Z. H. Deng and Z. Wang.   A New Fast Vertical Method for Mining Frequent Patterns [http://www.tandfonline.com/doi/abs/10.1080/18756891.2010.9727736]. International Journal of Computational Intelligence Systems, 3(6): 733 - 744, 2010.&lt;/ref&gt;
&lt;ref name="dharmesh2013"&gt;D. Bhalodiya, K. M. Patel and C. Patel. An Efficient way to Find Frequent Pattern with Dynamic Programming Approach [http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=6780102&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D6780102]. NIRMA UNIVERSITY INTERNATIONAL CONFERENCE ON ENGINEERING, NUiCONE-2013, 28-30 NOVEMBER, 2013.&lt;/ref&gt;
}}

==External links==

===Bibliographies===
* [http://www.uco.es/grupos/kdis/ARMBibliography Extensive Bibliography on Association Rules] by J.M. Luna
* [http://michael.hahsler.net/research/bib/association_rules/ Annotated Bibliography on Association Rules] by M. Hahsler
* [http://www.statsoft.com/textbook/association-rules/ Statsoft Electronic Statistics Textbook: Association Rules] by [[Dell]] Software

{{Prone to spam|date=February 2016}}
{{Z148}}&lt;!--     {{No more links}}

       Please be cautious adding more external links.

Wikipedia is not a collection of links and should not be used for advertising.

     Excessive or inappropriate links will be removed.

 See [[Wikipedia:External links]] and [[Wikipedia:Spam]] for details.

If there are already suitable links, propose additions or replacements on
the article's talk page, or submit your link to the relevant category at
DMOZ (dmoz.org) and link there using {{Dmoz}}.

--&gt;

{{DEFAULTSORT:Association Rule Learning}}
[[Category:Data management]]
[[Category:Data mining]]</text>
      <sha1>abkqiap84goeq7rscp0uefd8ygwja4u</sha1>
    </revision>
  </page>
  <page>
    <title>Novell File Reporter</title>
    <ns>0</ns>
    <id>28208309</id>
    <revision>
      <id>722194301</id>
      <parentid>721088348</parentid>
      <timestamp>2016-05-26T15:09:09Z</timestamp>
      <contributor>
        <ip>137.65.45.116</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3329" xml:space="preserve">{{Infobox software
|name                       = Novell File Reporter
|logo                       =
|screenshot                 =
|caption                    =
|collapsible                =
|author                     =
|developer                  = [[Novell]]
|released                   = {{Start date|2010|01}}
|discontinued               =
|latest release version     = 2.6.1
|latest release date        = {{Start date|2015|10|02}}
|latest preview version     =
|latest preview date        = &lt;!-- {{Start date|YYYY|MM|DD}} --&gt;
|frequently updated         =
|programming language       =
|operating system           =
|platform                   =
|size                       =
|language                   =
|status                     =
|genre                      = [[System Software]]
|license                    =
|website                    = [http://www.novell.com/products/file-reporter/ Novell File Reporter]
}}

'''Novell File Reporter''' (a.k.a. '''NFR''') is software that allows network administrators to identify files stored on the network and generates reports regarding the size of individual files, [[File format | file type]], when files were last accessed, and where duplicates exist. Additionally, the File Reporter tracks storage volume capacity and usage. It is a component of the [[Novell File Management Suite]].

==How It Works==

Novell File Reporter examines and reports on terabytes of data via a central reporting engine (NFR Engine) and distributed agents (NFR Agents). &lt;ref&gt;{{Citation| title = Novell File Reporter Reports on Terabytes of Data | url= http://www.novell.com/products/file-reporter/terabytes_data.html | accessdate = 31 July 2010}}&lt;/ref&gt; The NFR Engine schedules the scans of file instances conducted by NFR Agents, processes and compiles the scans for reporting purposes, and provides report information to the user interface. 

In addition to the standard reports &lt;ref&gt;{{Citation| title = Novell File Report Standard Reports | url= http://www.novell.com/products/file-reporter/standard_reports.html | accessdate = 31 July 2010}}&lt;/ref&gt; it can generate, the NFR Engine can also produce "trigger reports" in response to specific events (a server volume crossing a capacity threshold, for example). Accordingly, the NFR Engine monitors the data gathered by the NFR Agents in order to identify these "triggers."

The NFR Engine when working in either [[Novell eDirectory | eDirectory]] or [[Active Directory]] connects to the directory via a Directory Services Interface (DSI) and thus can monitor and check file permissions.&lt;ref&gt;{{Citation | last = Huber | first= Matthias | journal= Linux Magazine | title= Novell File Management Suite Optimizes Storage | date= 25 January 2010| url=http://www.linux-magazine.com/Online/News/Novell-File-Management-Suite-Optimizes-Storage | accessdate= 31 July 2010}}&lt;/ref&gt;

==References==
{{Reflist}}

==External links==
*[http://www.novell.com/products/file-reporter/technicalinfo/ Novell File Reporter: Product page] Overview, features, and technical information
*[http://www.novell.com/documentation/filereporter2/ Novell File Reporter: Documentation]
*[http://www.filereportersupport.com/nfr/ Novell File Reporter: Support]

{{Novell}}

[[Category:Novell]]
[[Category:Novell software]]
[[Category:Storage software]]
[[Category:Data management]]</text>
      <sha1>a8z2brbq58opkdwvulpism9emblrn19</sha1>
    </revision>
  </page>
  <page>
    <title>Virtual data room</title>
    <ns>0</ns>
    <id>17879652</id>
    <revision>
      <id>755590300</id>
      <parentid>755583202</parentid>
      <timestamp>2016-12-19T00:34:06Z</timestamp>
      <contributor>
        <username>Grayfell</username>
        <id>6603956</id>
      </contributor>
      <comment>Undid revision 755583202 by [[Special:Contributions/Qianding|Qianding]] ([[User talk:Qianding|talk]]) Again, no explanation or reason given for arbitrary list of companies. This sure looks like aggressive spamming.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2855" xml:space="preserve">A '''virtual data room''' (sometimes called a '''VDR''') is an online repository of information that is used for the storing and distribution of documents.  In many cases, a virtual data room is used to facilitate the [[due diligence]] process during an [[M&amp;A]] transaction, [[loan syndication]], or private equity and venture capital transactions.  This due diligence process has traditionally used a physical [[data room]] to accomplish the disclosure of documents. For reasons of cost, efficiency and security, virtual data rooms have widely replaced the more traditional physical data room.&lt;ref&gt;http://www.inc.com/best-industries-2013/jeremy-quittner/virtual-data-rooms.html&lt;/ref&gt;&lt;ref&gt;http://www.forbes.com/pictures/fghj45fjl/2-virtual-data-rooms/&lt;/ref&gt;

A virtual data room is an [[extranet]] to which the bidders and their advisers are given access via the internet. An extranet is essentially a website with limited controlled access, using a secure log-on supplied by the vendor, which can be disabled at any time, by the vendor, if a bidder withdraws. Much of the information released is confidential and restrictions are applied to the viewer’s ability to release this to third parties (by means of forwarding, copying or printing). This can be effectively applied to protect the data using digital rights management.&lt;ref&gt;http://www.divestopedia.com/definition/836/virtual-data-room-vdr&lt;/ref&gt; 

In the process of [[mergers and acquisitions]] the data room is set up as part of the central repository of data relating to companies or divisions being acquired or sold. The data room enables the interested parties to view information relating to the business in a controlled environment where confidentiality can be preserved. Conventionally this was achieved by establishing a supervised, physical data room in secure premises with controlled access. In most cases, with a physical data room, only one bidder team can access the room at a time. A virtual data room is designed to have the same advantages as a conventional data room (controlling access, viewing, copying and printing, etc.) with fewer disadvantages. Due to their increased efficiency, many businesses and industries have moved to using virtual data rooms instead of physical data rooms. In 2006, a spokesperson for a company which sets up virtual deal rooms was reported claiming that the process reduced the bidding process by about thirty days compared to physical data rooms.&lt;ref&gt;{{cite news|last1=Buckler|first1=Grant|title=A virtual smoke-filled room|url=http://www.theglobeandmail.com/technology/a-virtual-smoke-filled-room/article1110056/|accessdate=4 July 2016|work=The Globe and Mail|date=21 November 2006}}&lt;/ref&gt;

==References==
{{Reflist}}


{{DEFAULTSORT:Virtual Data Room}}
[[Category:Data management]]
[[Category:Disclosure]]
[[Category:Mergers and acquisitions]]</text>
      <sha1>7z2and6wsytwr8fyo4oatlw8xm0it4g</sha1>
    </revision>
  </page>
  <page>
    <title>ISO 8000</title>
    <ns>0</ns>
    <id>20375252</id>
    <revision>
      <id>754083379</id>
      <parentid>754082984</parentid>
      <timestamp>2016-12-10T19:39:33Z</timestamp>
      <contributor>
        <ip>82.137.56.69</ip>
      </contributor>
      <comment>Fix previously added item.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5595" xml:space="preserve">'''[[International Organization for Standardization|ISO]] 8000''', the global standard for ''[[Data Quality and Enterprise Master Data]]'', describes the features and defines the requirements for the Data Quality and Portability of Enterprise Master Data.  Master Data is typically "internal" business information about clients, products and operations.  The standard is currently under development, but is quickly being adopted by many Fortune 500 corporations and certain public agencies involved in the regulation and supervision of financial markets around the world. ISO 8000 is one of the emerging technology standards that large and complex organizations are turning to in order to improve business processes and control operational costs.  The standard will be published as a number of separate documents, which [[International Organization for Standardization|ISO]] calls "parts".

ISO 8000 is being developed by [[ISO TC 184/SC 4|ISO technical committee TC 184, ''Automation systems and integration'', sub-committee SC 4, ''Industrial data'']]. Like other [[International Organization for Standardization|ISO]] and [[International Electrotechnical Commission|IEC]] standards, ISO 8000 is copyrighted and is not freely available.&lt;ref&gt;[http://www.iso.org/iso/copyright.htm ISO copyright policy]&lt;/ref&gt;

== Published parts ==

The following part has already been published:

* ISO/TS 8000-1:2011, ''Data quality &amp;mdash; Part 1: Overview''&lt;ref&gt;[http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=50798 ISO catalogue page for ISO/TS 8000-1:2011]&lt;/ref&gt;
* ISO 8000-2:2012, ''Data quality &amp;mdash; Part 2: Vocabulary''&lt;ref&gt;[http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=57436 ISO catalogue page for ISO 8000-2:2012]&lt;/ref&gt;
* ISO 8000-61:2016, ''Data quality &amp;mdash; Part 61: Data quality management: Process reference model''&lt;ref&gt;[http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=63086 ISO catalogue page for ISO-61:2016]&lt;/ref&gt;
* ISO/TS 8000-100:2009, ''Data quality &amp;mdash; Part 100: Master data: Exchange of characteristic data: Overview''&lt;ref&gt;[http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=52129 ISO catalogue page for ISO/TS 8000-100:2009]&lt;/ref&gt;
* ISO 8000-102:2009, ''Data quality &amp;mdash; Part 102: Master data: Exchange of characteristic data: Vocabulary''&lt;ref&gt;[http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=50799 ISO catalogue page for ISO 8000-102:2009]&lt;/ref&gt;
* ISO 8000-110:2009, ''Data quality — Part 110: Master data: Exchange of characteristic data: Syntax, semantic encoding, and conformance to data specification''&lt;ref&gt;[http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=51653 ISO catalogue page for ISO 8000-110:2009]&lt;/ref&gt;
* ISO/TS 8000-120:2009, ''Data quality &amp;mdash; Part 120: Master data: Exchange of characteristic data: Provenance''&lt;ref&gt;[http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=50801 ISO catalogue page for ISO/TS 8000-120:2009]&lt;/ref&gt;
* ISO/TS 8000-130:2009, ''Data quality &amp;mdash; Part 130: Master data: Exchange of characteristic data: Accuracy''&lt;ref&gt;[http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=50802 ISO catalogue page for ISO/TS 8000-130:2009]&lt;/ref&gt;
* ISO/TS 8000-140:2009, ''Data quality &amp;mdash; Part 140: Master data: Exchange of characteristic data: Completeness''&lt;ref&gt;[http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=53589 ISO catalogue page for ISO/TS 8000-140:2009]&lt;/ref&gt;
* ISO/TS 8000-150:2011, ''Data quality &amp;mdash; Part 150: Master data: Quality management framework''&lt;ref&gt;[http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=53589 ISO catalogue page for ISO/TS 8000-150:2011]&lt;/ref&gt;

== Further reading ==

{{Citation | last=Benson | first=Peter | journal=Real-World Decision Support (RWDS) Journal | year=2009 | volume=3 | issue=4 | title=ISO 8000 Data Quality — The Fundamentals, Part 1 | url=http://www.ewsolutions.com/resource-center/rwds_folder/rwds-archives/issue.2009-10-12.0790666855/document.2009-10-12.3367922336/view?searchterm=ISO%208000}}

{{Citation | last=Benson | first=Peter | title=NATO Codification System as the foundation for ISO 8000, the International Standard for data quality (Oil IT Journal)| year=2008 | url=http://www.oilit.com/papers/Benson.pdf}}

{{Citation | last=Benson | first=Peter | title=ISO 8000 &amp;mdash; A new international standard for data quality | year=2009 | url=http://www.dataqualitypro.com/data-quality-home/iso-8000-a-new-international-standard-for-data-quality.html}}

{{Citation | last=Benson | first=Peter | title=Peter Benson discusses the certification options of ISO 8000 (Live Recording) | year=2010 | url=http://www.dataqualitypro.com/data-quality-home/peter-benson-discusses-the-certification-options-of-iso-8000.html}}

{{Citation | last=Grantner | first=Emily | title=ISO 8000 &amp;mdash; A Standard for data quality | year=2007 | issue=Oct.-Dec. | journal=Logistics Spectrum | url=http://www.highbeam.com/doc/1P3-1518467381.html}}

{{Citation | last=West | first=Matthew | title=ISO 8000 &amp;mdash; the Emerging Standard for Data Quality | year=2009 | journal=IAIDQ's Information and Data Quality Newsletter | volume=5 | issue=3 | url=http://iaidq.org/publications/west-2009-07.shtml}} (full article requires no cost registration to access)

== References ==
&lt;references/&gt;

{{ISO standards}}

{{DEFAULTSORT:Iso 8000}}
[[Category:ISO standards|#08000]]
[[Category:Data management]]</text>
      <sha1>4d2ca6v0pjcvkesyyuxaqa9sh2x2q13</sha1>
    </revision>
  </page>
  <page>
    <title>Document capture software</title>
    <ns>0</ns>
    <id>23392007</id>
    <revision>
      <id>736154514</id>
      <parentid>736153291</parentid>
      <timestamp>2016-08-25T15:07:44Z</timestamp>
      <contributor>
        <username>MrOllie</username>
        <id>6908984</id>
      </contributor>
      <comment>/* Document Capture Software */ rm link farm per [[WP:ELNO]], [[WP:NOT]] a link directory</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6176" xml:space="preserve">{{primary sources|date=August 2009}}
'''Document Capture Software''' refers to applications that provide the ability and feature set to automate the process of [[Image scanner|scanning]] paper documents. Most scanning [[Personal computer hardware|hardware]], both scanners and [[copier]]s, provides the basic ability to scan to any number of [[image file formats]], including: [[PDF]], [[TIFF]], [[JPG]], [[BMP file format|BMP]], etc. This basic functionality is augmented by document capture software, which can add efficiency and standardization to the process.

==Typical features==
Typical features of Document Capture Software include:
* [[Barcode]] recognition
* Patch Code recognition
* Separation
* [[Optical character recognition|Optical Character Recognition (OCR)]]
* [[Optical mark recognition|Optical Mark Recognition (OMR)]]
* Quality Assurance
* Indexing
* Migration

===Goal for Implementation of a Document Capture Solution===
The goal for implementing a document capture solution is to reduce the amount of time spent in the scanning and capture process, and produce metadata along with an image file, and/or OCR text. This information is then migrated to a [[Document management system|Document Management]] or [[Enterprise content management|Enterprise Content Management]] system. These systems often provide a search function, allowing search of the assets based on the produced [[metadata]], and then viewed using [[document imaging]] software.

== Document Capture System Solutions - General ==

===Integration with Document Management System===
{{main|Enterprise content management}}
ECM (Enterprise Content management) and their DMS component (Document Management System) are being adopted by many organizations as a corporate document management system for all types of electronic files, e.g. MS word, PDF ... However, much of the information held by organisations is on paper and this needs to be integrated within the same document repository.

By converting paper documents into digital format through scanning companies can convert paper into image formats such as TIF and JPG and also extract valuable index information or business data from the document using OCR technology. Digital documents and associated metadata can easily be stored in the ECM in a variety of formats. The most popular of these formats is PDF which not only provides an accurate representation of the document but also allows all the OCR text in the document to be stored behind the PDF image. This format is known as PDF with hidden text or text-searchable PDF. This allows users to search for documents by using keywords in the metadata fields or by searching the content of PDF files across the repository.

====Advantages of scanning documents into a ECM/DMS====

Information held on paper is usually just as valuable to organisations as the electronic documents that are generated internally. Often this information represents a large proportion of the day to day correspondence with suppliers and customers. Having the ability to manage and share this information internally through a document management system such as [[SharePoint]] can improve collaboration between departments or employees and also eliminate the risk of losing this information through disasters such as floods or fire.

Organisations adopting an ECM/DMS  often implement electronic workflow which allows the information held on paper to be included as part of an electronic business process and incorporated into a customer record file along with other associated office documents and emails.
For business critical documents, such as purchase orders and supplier invoices, digitising documents can help speed up business transactions as well as reduce manual effort involved in keying data into business systems, such as CRM, ERP and Accounting. Scanned invoices can also be routed to managers for payment approval via email or an electronic workflow.

==Distributed Capture Solutions==

Distributed document capture is a technology which allows the scanning of documents into a central server through the use of individual capture stations. A variation of distributed capture is thin-client document capture in which documents are scanned into a central server through the use of web browser.  One of these web-based products was reviewed by AIIM.  They said, "(this product) is a thin-client distributed capture system that streamlines the process of acquiring and creating documents."&lt;ref&gt;Association for Information and Image Management [http://www.aiim.org/community/product-guide/Capture/Prevalent-Software-Quillix "Prevalent Software - Quillix"], accessed August 29, 2011.&lt;/ref&gt;  The streamlining is a result of several factors including the lack of software which needs to be installed at every scanning station and the variety of input sources from which documents can be captured.  This includes things like email, fax, or a watched folder.

Jeff Shuey, Director of Business Development at Kodak, makes a distinction between distributed capture and what he calls "remote" capture.  In an article publishing in [[AIIM]], he said that the key difference between the two is whether or not the information that is captured from scanning needs to be sent to the centralized server. If, as he points out in his article, the document just needs to be scanned and committed to a [[SharePoint]] system and doesn't need to be sent to some other centralized server, this is just a remote capture situation.&lt;ref&gt;Association for Information and Image Management [http://www.aiim.org/community/blogs/expert/Remote-or-Distributed-Scanning-Are-they-Different "Remote or Distributed Scanning - Are They Different?"], accessed August 29, 2011.&lt;/ref&gt;

There are Document Capture Software comparisons available, featuring some of the most relevant products (EMC Captiva, IBM Datacap, Artsyl Technologies or Ephesoft) and extracting performance facts and their most relevant features.

==References==
&lt;references/&gt;

{{DEFAULTSORT:Document Capture Software}}
[[Category:Artificial intelligence applications]]
[[Category:Optical character recognition]]
[[Category:Data management]]
[[Category:SharePoint]]</text>
      <sha1>24wdyg3kn2dyxodclklvntyct0pcbo0</sha1>
    </revision>
  </page>
  <page>
    <title>Data binding</title>
    <ns>0</ns>
    <id>15592339</id>
    <revision>
      <id>758400077</id>
      <parentid>758300916</parentid>
      <timestamp>2017-01-05T05:20:15Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor />
      <comment>v1.41b - [[WP:WCW]] project (Link equal to linktext)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2708" xml:space="preserve">'''Data binding''' is a general technique that binds data sources from the provider and consumer together and [[data synchronization|synchronizes]] them. This is usually done with two data/information sources with different languages as in [[XML data binding]]. In [[UI data binding]], data and information objects of the same language but different logic function are bound together (e.g. [[Java (programming language)|Java]] [[user interface|UI]] elements to Java objects).&lt;ref&gt;{{cite web |url=https://www.techopedia.com/definition/15652/data-binding|title=What is Data Binding? |work=Techopedia.com |accessdate=30 December 2015}}&lt;/ref&gt;

In a data binding process, each data change is reflected automatically by the elements that are bound to the data. The term data binding is also used in cases where an outer representation of data in an element changes, and the underlying data is automatically updated to reflect this change. As an example, a change in a [[text box|&lt;code&gt;TextBox&lt;/code&gt;]] element could modify the underlying data value.&lt;ref&gt;{{cite web |url=https://msdn.microsoft.com/en-us/library/ms752347(v=vs.110).aspx |title=Data Binding Overview |work=Microsoft Developer Network |publisher=Microsoft |access-date=29 December 2016}}&lt;/ref&gt;

== Data binding frameworks and tools ==

=== [[Embarcadero Delphi|Delphi]] ===
* DSharp 3rd-party Data Binding tool
* [[OpenWire (library)|OpenWire]] Visual Live Binding - 3rd-party Visual Data Binding tool
* LiveBindings

=== [[C Sharp (programming language)|C#]] ===
* [[Windows Presentation Foundation]]

=== [[JavaScript]] ===
* [[AngularJS]]
* [[Backbone.js]]
* BindingJS
* Datum.js&lt;ref&gt;{{cite web |url=http://datumjs.com|title=Datum.js|accessdate=7 November 2016}}&lt;/ref&gt;
* [[EmberJS]]
* Generic Data Binder
* [[KnockoutJS]]
* [[React (JavaScript library)]]
* SAP/OPEN UI5
* [[Vue.js]]

=== [[Java_(programming_language)|Java]] ===
* [[Google Web Toolkit]]

=== [[Objective-C]] ===
* AKABeacon iOS Data Binding framework

=== [[Scala (programming language)|Scala]] ===
* Binding.scala&lt;ref&gt;{{cite web |url=https://github.com/ThoughtWorksInc/Binding.scala|title=Binding.scala|accessdate=30 December 2016}}&lt;/ref&gt; Reactive data-binding for Scala

==See also==
* [[Windows Presentation Foundation]]
* [[XML data binding]]
* [[UI data binding]]
* [[Bound property]]

==References==
&lt;references/&gt;

==Further reading==
*{{cite book |last=Noyes |first=Brian |title=Data Binding with Windows Forms 2.0: Programming Smart Client Data Applications with .NET |url=https://books.google.com/books?id=RxptHgJ5W2cC |date=12 January 2006 |publisher=Pearson Education |isbn=978-0-321-63010-0}}

{{DEFAULTSORT:Data Binding}}
&lt;!--Categories--&gt;
[[Category:Data management]]</text>
      <sha1>bukuen0dau6i8ggd19mopseiv126nlp</sha1>
    </revision>
  </page>
  <page>
    <title>Learning object metadata</title>
    <ns>0</ns>
    <id>1955471</id>
    <revision>
      <id>734815271</id>
      <parentid>727590586</parentid>
      <timestamp>2016-08-16T22:02:48Z</timestamp>
      <contributor>
        <username>David Gerard</username>
        <id>36389</id>
      </contributor>
      <comment>Removing link(s) to "IMS Global": rm redlink (deleted). ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="15686" xml:space="preserve">[[Image:LOM base schema.svg|340px|right|thumb|A schematic representation of the hierarchy of elements in the LOM data model]]

'''Learning Object Metadata''' is a data model, usually encoded in XML, used to describe a [[learning object]] and similar digital resources used to support learning. The purpose of learning object metadata is to support the reusability of learning objects, to aid discoverability, and to facilitate their interoperability, usually in the context of online [[learning management systems]] (LMS).

The IEEE 1484.12.1 – 2002 Standard for Learning Object Metadata is an internationally recognised open standard (published by the [[Institute of Electrical and Electronics Engineers]] Standards Association, New York) for the description of “[[learning object]]s”. Relevant attributes of learning objects to be described include: type of object; author; owner; terms of distribution; format; and [[pedagogy|pedagogical]] attributes, such as teaching or interaction style.

== IEEE 1484.12.1 – 2002 Standard for Learning Object Metadata ==

=== In brief ===
The IEEE working group that developed the standard defined learning objects, ''for the purposes of the standard,'' as being “any entity, digital or non-digital, that may be used for learning, education or training." This definition has struck many commentators as being rather broad in its scope, but the definition was intended to provide a broad class of objects to which LOM metadata might usefully be associated rather than to give an instructional or pedagogic definition of a learning object. ''IEEE 1484.12.1'' is the first part of a multipart standard, and describes the LOM data model. The LOM data model specifies which aspects of a learning object should be described and what vocabularies may be used for these descriptions; it also defines how this data model can be amended by additions or constraints. Other parts of the standard are being drafted to define bindings of the LOM data model, i.e. define how LOM records should be represented in [[XML]] and [[Resource Description Framework|RDF]] (''IEEE 1484.12.3'' and ''IEEE 1484.12.4'' respectively). This article focuses on the LOM data model rather than issues relating to XML or other bindings.

IMS Global Learning Consortium is an international consortium that contributed to the drafting of the IEEE Learning Object Metadata (together with the ARIADNE Foundation) and endorsed early drafts of the data model as part of the IMS Learning Resource Meta-data specification (IMS LRM, versions 1.0 – 1.2.2). Feedback and suggestions from the implementers of IMS LRM fed into the further development of the LOM, resulting in some drift between version 1.2 of the IMS LRM specification and what was finally published at the LOM standard. Version 1.3 of the IMS LRM specification realigns the IMS LRM data model with the IEEE LOM data model and specifies that the IEEE XML binding should be used. Thus, we can now use the term 'LOM' in referring to both the IEEE standard and version 1.3 of the IMS specification. The IMS LRM specification also provides an extensive ''Best Practice and Implementation Guide'', and an ''XSL transform'' that can be used to migrate metadata instances from the older versions of the IMS LRM XML binding to the IEEE LOM XML binding.

== Technical details ==

=== How the data model works ===
The LOM comprises a '''hierarchy of elements'''&lt;!--, as shown in the diagram (top right)--&gt;. At the first level, there are nine categories, each of which contains sub-elements; these sub-elements may be simple elements that hold data, or may themselves be aggregate elements, which contain further sub-elements. The semantics of an element are determined by its context: they are affected by the parent or container element in the hierarchy and by other elements in the same container. For example, the various ''Description'' elements (1.4, 5.10, 6.3, 7.2.2, 8.3 and 9.3) each derive their context from their parent element. In addition, description element 9.3 also takes its context from the value of element 9.1 ''Purpose'' in the same instance of ''Classification''.

The data model specifies that some elements may be repeated either individually or as a group; for example, although the elements 9.2 (''Description'') and 9.1 (''Purpose'') can only occur once within each instance of the ''Classification'' container element, the ''Classification'' element may be repeated - thus allowing many descriptions for different purposes.

The data model also specifies the '''value space''' and '''datatype''' for each of the simple data elements. The value space defines the restrictions, if any, on the data that can be entered for that element. For many elements, the value space allows any string of [[Unicode]] character to be entered, whereas other elements entries must be drawn from a declared list (i.e. a [[controlled vocabulary]]) or must be in a specified format (e.g. date and language codes). Some element datatypes simply allow a string of characters to be entered, and others comprise two parts, as described below:
* '''LangString''' items contain Language and String parts, allowing the same information to be recorded in multiple languages
* '''Vocabulary''' items are constrained in such a way that their entries have to be chosen from a controlled list of terms - composed of Source-Value pairs - with the Source containing the name of the list of terms being used and the Value containing the chosen term
* '''DateTime''' and '''Duration''' items contain one part that allows the date or duration to be given in a machine readable format, and a second that allows a description of the date or duration (for example “mid summer, 1968”).

When implementing the LOM as a data or service provider, it is not necessary to support all the elements in the data model, nor need the LOM data model limit the information which may be provided. The creation of an [[application profile]] allows a community of users to specify which elements and vocabularies they will use. Elements from the LOM may be dropped and elements from other metadata schemas may be brought in; likewise, the vocabularies in the LOM may be supplemented with values appropriate to that community.

=== Requirements ===
The key requirements for exploiting the LOM as a data or service provider are to:
* Understand user/community needs and to express these as an application profile
* Have a strategy for creating high quality metadata
* Store this metadata in a form which can be exported as LOM records
* Agree a binding for LOM instances when they are exchanged
* Be able to exchange records with other systems either as single instances or ''en masse''.

=== Related specifications ===
There are many metadata specifications; of particular interest is the [[Dublin Core]] Metadata Element Set (commonly known as Simple Dublin Core, standardised as ''ANSI/NISO Z39.85 – 2001''). Simple Dublin Core (DC) provides a non-complex, loosely defined set of elements which is useful for sharing metadata across a wide range of disparate services. Since the LOM standard used Dublin Core as a starting point, refining the Simple DC schema with qualifiers relevant to learning objects, there is some overlap between the LOM and DC standards.&lt;ref&gt;{{cite book|last1=Miller|first1=Steven J.|title=Metadata for Digital Collections: A How-To-Do-It Manual|date=2011|publisher=ALA Neal-Schuman|location=Chicago|isbn=978-1-55570-746-0|pages=56}}&lt;/ref&gt; The Dublin Core Metadata Initiative is also working on a set of terms which allow the Dublin Core Element Set to be used with greater semantic precision (Qualified Dublin Core). The Dublin Education Working Group aims to provide refinements of [[Dublin Core]] for the specific needs of the education community.

Many other education-related specifications allow for LO metadata to be embedded within XML instances, such as: describing the resources in an IMS Content Package or Resource List; describing the vocabularies and terms in an [[IMS VDEX]] (Vocabulary Definition and Exchange) file; and describing the question items in an IMS QTI (Question and Test Interoperability) file.

The [[IMS VDEX|IMS Vocabulary Definition and Exchange (VDEX) specification]] has a double relation with the LOM, since not only can the LOM provide metadata on the vocabularies in a VDEX instance, but VDEX can be used to describe the controlled vocabularies which are the value space for many LOM elements.

LOM records can be transported between systems using a variety of protocols, perhaps the most widely used being [[OAI-PMH]].

=== Application profiles ===

==== UK LOM Core ====
For UK Further and Higher Education, the most relevant family of application profiles are those based around the ''UK LOM Core''.&lt;ref&gt;http://zope.cetis.ac.uk/profiles/uklomcore/&lt;/ref&gt; The UK LOM Core is currently a draft schema researched by a community of practitioners to identify common UK practice in learning object content, by comparing 12 metadata schemas. UK LOM is currently legacy work, it is not in active development.

==== CanCore ====
''CanCore'' provides detailed guidance for the interpretation and implementation of each data element in the LOM standard.&lt;ref name="CanCore"&gt;{{cite web | url = http://cancore.tru.ca/en/guidelines.html| title = CanCore Guidelines: Introduction | author = [[Norm Friesen]]| publisher = Athabasca University| date = 2003-01-20 | accessdate = 2009-02-23 |display-authors=etal}}&lt;/ref&gt; These guidelines (2004) constitute a 250-page document, and have been developed over three years under the leadership of [[Norm Friesen]], and through consultation with experts across Canada and throughout the world. These guidelines are also available at no charge from the CanCore Website.

==== ANZ-LOM ====
ANZ-LOM is a metadata profile developed for the education sector in Australia and New Zealand. The profile sets obligations for elements and illustrates how to apply controlled vocabularies, including example regional vocabularies used in the "classification" element. The ANZ-LOM profile was first published by The Le@rning Federation (TLF) in January, 2008.

==== Vetadata ====
The Australian Vocational Training and Education (VET) sector uses an application profile of the IEEE LOM called Vetadata. The profile contains five mandatory elements, and makes use of a number of vocabularies specific to the Australian VET sector. This application profile was first published in 2005. The Vetadata and ANZ-LOM profiles are closely aligned.

==== NORLOM ====
NORLOM is the Norwegian LOM profile.
The profile is managed by NSSL (The Norwegian Secretariat for Standardization of Learning Technologies)

==== ISRACore ====
ISRACORE is the Israeli LOM profile.
The Israel Internet Association (ISOC-IL) and Inter University Computational Center (IUCC) have teamed up to manage and establish an e-learning objects database.

====SWE-LOM====
SWE-LOM is the Swedish LOM profile that is managed by IML at [[Umeå University]] as a part of the work with the national standardization group TK450 at [[Swedish Standards Institute]].

====TWLOM====
TWLOM is the Taiwanese LOM profile that is managed by Industrial Development and Promotion of Archives and e-Learning Project

====LOM-FR====
LOM-FR is a metadata profile developed for the education sector in France. This application profile was first published in 2006.

====NL LOM====
NL LOM is the Dutch metadata profile for educational resources in the Netherlands. This application profile was the result of merging the Dutch higher education LOM profile with the one used in primary and secondary Dutch education. The final version was released in 2011.

====LOM-CH====
LOM-CH is a metadata profile developed for the education sector in Switzerland. It is currently available in French and German. This application profile was published in July 2014.

====LOM-ES====
LOM-ES is a metadata profile developed for the education sector in Spain. It is available in Spanish.

====LOM-GR====
LOM-GR, also known as "LOM-GR ''Photodentro''" is the Greek LOM application profile for educational resources, currently being used for resources related to school education. It was published in 2012 and is currently available in Greek and English.&lt;ref&gt;https://git.dschool.edu.gr/photodentro/LOM-GR&lt;/ref&gt; It is maintained by [[CTI DIOPHANTUS]] as part of the "[[Photodentro]] Federated Architecture for Educational Content for Schools" that includes a number of educational content repositories (for Learning Objects, Educational Video, and User Generated Content) and the Greek National Aggregator of Educational Content accumulating metadata from collections stored in repositories of other organizations.&lt;ref name="Photodentro LOR"&gt;{{cite journal|last1=Megalou|first1=Elina|last2=Kaklamanis|first2=Christos|title=PHOTODENTRO LOR, THE GREEK NATIONAL LEARNING OBJECT REPOSITORY|journal=INTED2014 Proceedings|date=10–12 March 2014|pages=309–319|url=https://library.iated.org/view/MEGALOU2014PHO|accessdate=7 April 2016|series=8th International Technology, Education and Development Conference|publisher=IATED|location=Valencia, Spain|issn=2340-1079}}&lt;/ref&gt; LOM-GR is a working specification of the TC48/WG3 working group of the [[Hellenic Organization for Standardization]].

==== Others ====
Other application profiles are those developed by the Celebrate project&lt;ref&gt;European Schoolnet, [http://web.archive.org/web/20071225053548/http://www.eun.org/ww/en/pub/celebrate_help/application_profile.htm CELEBRATE Application Profile] (2003).&lt;/ref&gt; and the metadata profile that is part of the SCORM reference model.&lt;ref&gt;ADL, [http://www.adlnet.gov/capabilities/scorm#tab-learn SCORM].&lt;/ref&gt;

== See also ==
* [[Application profile]]
* [[Content package]]
* [[Dublin Core]]
* IMS Global
* [[Learning object]]
* [http://dublincore.org/dcx/lrmi-terms/1.1/ LRMI (Learning Resource Metadata Initiative)]
* [[Metadata]]
* [[Metadata standards|Metadata Standards]]
* [[OAI-PMH]]
* [[SCORM]]
* [[XML]]
* [[:m:Learning Object Metadata]]

==References==
{{Reflist}}

== External links ==
{{wikiversity|Introduction to Learning Objects}}
* [http://cancore.athabascau.ca/en/ cancore.athabascau.ca] is a thorough element-by-element guide to implementing the IEEE LOM.
* [http://www.imsglobal.org/metadata/ www.imsglobal.org: IMS Global Learning Consortium Learning resource meta-data specification].
* [http://ltsc.ieee.org/wg12/files/IEEE_1484_12_03_d8_submitted.pdf ltsc.ieee.org: XML Binding Specification].
* [http://www.intrallect.com/support/metadata/ims2lom_metadata_mapping.htm www.intrallect.com: A mapping between the IEEE LOM and IMS Learning Resource Metadata]
* [http://www.ontopia.net/topicmaps/materials/tm-vs-thesauri.html www.ontopia.net: Metadata? Thesauri? Taxonomies? Topic Maps! Making sense of it all], 2004.
{{Prone to spam|date=October 2014}}
{{Z148}}&lt;!--     {{No more links}}

       Please be cautious adding more external links.

Wikipedia is not a collection of links and should not be used for advertising.

     Excessive or inappropriate links will be removed.

 See [[Wikipedia:External links]] and [[Wikipedia:Spam]] for details.

If there are already suitable links, propose additions or replacements on
the article's talk page, or submit your link to the relevant category at
DMOZ (dmoz.org) and link there using {{Dmoz}}.

--&gt;

{{Use dmy dates|date=October 2010}}

{{DEFAULTSORT:Learning Object Metadata}}
[[Category:Data management]]
[[Category:Educational technology]]
[[Category:Knowledge representation]]
[[Category:Library science]]
[[Category:Metadata]]
[[Category:Standards]]
[[Category:Technical communication]]</text>
      <sha1>e336juo1bmbvyc8i6nqaimy1zgo74ue</sha1>
    </revision>
  </page>
  <page>
    <title>Grid-oriented storage</title>
    <ns>0</ns>
    <id>11962687</id>
    <revision>
      <id>749335160</id>
      <parentid>742871218</parentid>
      <timestamp>2016-11-13T20:34:42Z</timestamp>
      <contributor>
        <username>TGCP</username>
        <id>11261013</id>
      </contributor>
      <comment>/* top */ [[grid storage]] more used for energy</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6044" xml:space="preserve">{{primary sources|article|date=March 2009}}

'''Grid-oriented Storage''' ('''GOS''') was a term used for data storage by a university project during the era when the term [[grid computing]] was popular.

== Description ==
GOS was a successor of the term [[network-attached storage]] (NAS). GOS systems contained hard disks, often [[RAID]]s (redundant arrays of independent disks), like traditional file servers. 
[[Image:gosongrid.jpg |thumb |upright=1.4]]

GOS was designed to deal with long-distance, cross-domain and single-image file operations, which is typical in Grid environments. GOS behaves like a file server via the file-based GOS-FS protocol to any entity on the grid. Similar to [[Advanced Resource Connector|GridFTP]], GOS-FS integrates a parallel stream engine and [[Grid Security Infrastructure]] (GSI). 

Conforming to the universal VFS (Virtual Filesystem Switch), GOS-FS can be pervasively used as an underlying platform to best utilize the increased transfer bandwidth and accelerate the [[Network File System (protocol)|NFS]]/[[CIFS]]-based applications. GOS can also run over [[SCSI]], [[Fibre Channel]] or [[iSCSI]], which does not affect the acceleration performance, offering both file level protocols and block level protocols for [[storage area network]] (SAN) from the same system.

In a grid infrastructure, resources may be geographically distant from each other, produced by differing manufacturers, and have differing access control policies. This makes access to grid resources dynamic and conditional upon local constraints. Centralized management techniques for these resources are limited in their scalability both in terms of execution efficiency and fault tolerance. Provision of services across such platforms requires a distributed resource management mechanism and the peer-to-peer clustered GOS appliances allow a single storage image to continue to expand, even if a single GOS appliance reaches its capacity limitations. The cluster shares a common, aggregate presentation of the data stored on all participating GOS appliances. Each GOS appliance manages its own internal storage space. The major benefit of this aggregation is that clustered GOS storage can be accessed by users as a single mount point. 

GOS products fit the thin-server categorization. Compared with traditional “fat server”-based storage architectures, thin-server GOS appliances deliver numerous advantages, such as the alleviation of potential network/grid bottle-necks, CPU and OS optimized for I/O only, ease of installation, remote management and minimal maintenance, low cost and Plug and Play, etc. Examples of similar innovations include NAS, printers, fax machines, routers and switches.

An [[Apache server]] has been installed in the GOS operating system, ensuring an HTTPS-based communication between the GOS server and an administrator via a Web browser. Remote management and monitoring makes it easy to set up, manage, and monitor GOS systems.

== History ==
[[Frank Zhigang Wang]] and Na Helian proposed a funding proposal to the UK government titled “Grid-Oriented Storage (GOS): Next Generation Data Storage System Architecture for the Grid Computing Era” in 2003. The proposal was approved and granted one million pounds{{citation needed|date=March 2009}} in 2004. The first prototype was constructed in 2005 at Centre for Grid Computing, Cambridge-Cranfield High Performance Computing Facility. The first conference presentation was at IEEE Symposium on Cluster Computing and Grid (CCGrid), 9–12 May 2005, Cardiff, UK. As one of the five best work-in-progress, it was included in the IEEE Distributed Systems Online. In 2006, the GOS architecture and its implementations was published in IEEE Transactions on Computers, titled “Grid-oriented Storage: A Single-Image, Cross-Domain, High-Bandwidth Architecture”.  
Starting in January 2007, demonstrations were presented at [[Princeton University]], Cambridge University Computer Lab and others.
By 2013, the Cranfield Centre still used future tense for the project.&lt;ref name="Cranfield CGC"&gt;{{cite web |url= http://www.cranfield.ac.uk/soe/departments/appliedmaths/gridcomputing/index.html |title= Centre for Grid Computing |accessdate= June 14, 2013 |publisher=Cranfield University}} &lt;!--  --&gt;&lt;/ref&gt;

[[Peer-to-peer file sharing]]s use similar techniques.

==Notes==
{{reflist}}

==Further reading==
* Frank Wang, Na Helian, Sining Wu, Yuhui Deng, Yike Guo, Steve Thompson, Ian Johnson, Dave Milward &amp; Robert Maddock, Grid-Oriented Storage, IEEE Distributed Systems Online,  Volume 6,  Issue 9, Sept. 2005.
* Frank Wang, Sining Wu, Na Helian, Andy Parker, Yike Guo, Yuhui Deng, Vineet Khare, Grid-oriented Storage: A Single-Image, Cross-Domain, High-Bandwidth Architecture, IEEE Transaction on Computers, Vol.56, No.4, pp.&amp;nbsp;474–487, 2007.
* Frank Zhigang Wang, Sining Wu, Na Helian, An Underlying Data-Transporting Protocol for Accelerating Web Communications, International Journal of Computer Networks, Elsevier, 2007.
* Frank Zhigang Wang, Sining Wu, Na Helian, Yuhui Deng, Vineet Khare, Chris Thompson and Michael Parker, Grid-based Data Access to Nucleotide Sequence Database with 6x Improvement in Response Times, New Generation Computing, No.2, Vol.25, 2007.
* Frank Wang, Yuhui Deng, Na Helian, Evolutionary Storage: Speeding up a Magnetic Disk by Clustering Frequent Data, IEEE Transactions on Magnetics, Issue.6, Vol.43, 2007.
* Frank Zhigang Wang, Na Helian, Sining Wu, Yuhui Deng, Vineet Khare, Chris Thompson and Michael Parker, Grid-based Storage Architecture for Accelerating Bioinformatics Computing, Journal of VLSI Signal Processing Systems, No.1, Vol.48, 2007.
* Yuhui Deng and   Frank Wang, A Heterogeneous Storage Grid Enabled by Grid Service, ACM Operating System Review, No.1, Vol.41, 2007.
* Yuhui Deng &amp; Frank Wang, Optimal Clustering Size of Small File Access in Network Attached Storage Device, Parallel Processing Letters, No.1, Vol.17, 2007.

{{DEFAULTSORT:Grid-Oriented Storage}}
[[Category:Data management]]</text>
      <sha1>76wfrkipp9t7s77qjknrxusi2eg9f9f</sha1>
    </revision>
  </page>
  <page>
    <title>Change data capture</title>
    <ns>0</ns>
    <id>3557729</id>
    <revision>
      <id>760392514</id>
      <parentid>760392298</parentid>
      <timestamp>2017-01-16T18:34:53Z</timestamp>
      <contributor>
        <ip>187.39.108.160</ip>
      </contributor>
      <comment>/* Push versus pull */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11133" xml:space="preserve">{{No footnotes|date=March 2016}}
In [[database]]s, '''change data capture''' (CDC) is a set of software [[Design pattern (computer science)|design patterns]] used to determine (and track) the data that has changed so that action can be taken using the changed data. Also, Change data capture (CDC) is an approach to data integration that is based on the identification, capture and delivery of the changes made to enterprise data sources.

CDC solutions occur most often in [[data warehouse|data-warehouse]] environments since capturing and preserving the state of data across time is one of the core functions of a data warehouse, but CDC can be utilized in any database or data repository system.

== Methodology ==

System developers can set up CDC mechanisms in a number of ways and in any one or a combination of system layers from application logic down to physical storage.

In a simplified CDC context, one computer system has data believed to have changed from a previous point in time, and a second computer system needs to take action based on that changed data.  The former is the source, the latter is the target.  It is possible that the source and target are the same system physically, but that would not change the design pattern logically.

Not uncommonly, multiple CDC solutions can exist in a single system.

=== Timestamps on rows ===
Tables whose changes must be captured may have a column that represents the time of '''last''' change.  Names such as LAST_UPDATE, etc. are common.  Any row in any table that has a timestamp in that column that is more recent than the last time data was captured is considered to have changed.

=== Version Numbers on rows ===
Database designers give tables whose changes must be captured a column that contains a version number.  Names such as VERSION_NUMBER, etc. are common.  When data in a row changes, its version number is updated to the current version.  A supporting construct such as a reference table with the current version in it is needed.  When a change capture occurs, all data with the latest version number is considered to have changed.  When the change capture is complete, the reference table is updated with a new version number.

Three or four major techniques exist for doing CDC with version numbers, the above paragraph is just one.

==== Use in Optimistic Locking ====
Version numbers can be useful with [[optimistic locking]] in ACID transactional or [[Relational database management system|relational database management systems (RDMBS)]]. For an example in read-then-update scenarios for [[Create, read, update and delete|CRUD]] applications in [[relational database management system]]s, a row is first read along with the state of its version number; in a separate transaction, a [[Update (SQL)|SQL UPDATE]] statement is executed along with an additional [[Where (SQL)|WHERE clause]] that includes the version number found from the initial read. If no record was updated, it usually means that the version numbers didn't match because some other action/transaction had already updated the row and consequently its version number. Several [[Object relational mapper|object relational mapping]] tools use this method to detect for optimistic locking scenarios (including [[Hibernate (Java)|Hibernate]]).

=== Status indicators on rows ===
This technique can either supplement or complement timestamps and versioning. It can configure an alternative if, for example, a status column is set up on a table row indicating that the row has changed (e.g. a boolean column that, when set to true, indicates that the row has changed). Otherwise, it can act as a complement to the previous methods, indicating that a row, despite having a new version number or an earlier date, still shouldn't be updated on the target (for example, the data may require human validation).

=== Time/Version/Status on rows ===
This approach combines the three previously discussed methods.  As noted, it is not uncommon to see multiple CDC solutions at work in a single system, however, the combination of time, version, and status provides a particularly powerful mechanism and programmers should utilize them as a trio where possible.  The three elements are not redundant or superfluous.  Using them together allows for such logic as, "Capture all data for version 2.1 that changed between 6/1/2005 12:00 a.m. and 7/1/2005 12:00 a.m. where the status code indicates it is ready for production."

=== Triggers on tables ===
May include a [[observer pattern|publish/subscribe]] pattern to communicate the changed data to multiple targets.  In this approach, [[Database trigger|triggers]] log events that happen to the transactional table into another queue table that can later be "played back".  For example, imagine an Accounts table, when transactions are taken against this table, triggers would fire that would then store a history of the event or even the deltas into a separate queue table.  The queue table might have schema with the following fields: Id, TableName, RowId, TimeStamp, Operation.  The data inserted for our Account sample might be:  1, Accounts, 76, 11/02/2008 12:15am, Update.
More complicated designs might log the actual data that changed.  This queue table could then be "played back" to replicate the data from the source system to a target.

[More discussion needed]

An example of this technique is the pattern known as the [[log trigger]].

=== Event Programming ===
Coding a change into an application at appropriate points is another method that can give  intelligent discernment that data changed.  Although this method involves programming vs. more easily implemented "dumb" triggers, it may provide more accurate and desirable CDC, such as only after a COMMIT, or only after certain columns changed to certain values - just what the target system is looking for.

=== Log scanners on databases ===
Most database management systems manage a [[transaction log]] that records changes made to the database contents and to [[metadata]]. By scanning and interpreting the contents of the database transaction log one can capture the changes made to the database in a non-intrusive manner.

Using transaction logs for change data capture offers a challenge in that the structure, contents and use of a transaction log is specific to a database management system. Unlike data access, no standard exists for transaction logs. Most database management systems do not document the internal format of their transaction logs, although some provide programmatic interfaces to their transaction logs (for example: Oracle, DB2, SQL/MP, SQL/MX and SQL Server 2008).

Other challenges in using transaction logs for change data capture include:

* Coordinating the reading of the transaction logs and the archiving of log files (database management software typically archives log files off-line on a regular basis).
* Translation between physical storage formats that are recorded in the transaction logs and the logical formats typically expected by database users (e.g., some transaction logs save only minimal buffer differences that are not directly useful for change consumers).
* Dealing with changes to the format of the transaction logs between versions of the database management system.
* Eliminating uncommitted changes that the database wrote to the transaction log and later [[Rollback (data management)|rolled back]].
* Dealing with changes to the metadata of tables in the database.

CDC solutions based on transaction log files have distinct advantages that include:

* minimal impact on the database (even more so if one uses [[log shipping]] to process the logs on a dedicated host).
* no need for programmatic changes to the applications that use the database.
* low [[Latency (engineering)|latency]] in acquiring changes.
* [[data integrity|transactional integrity]]: log scanning can produce a change stream that replays the original transactions in the order they were committed. Such a change stream include changes made to all tables participating in the captured transaction.
* no need to change the database schema

== Confounding factors ==
As often occurs in complex domains, the final solution to a CDC problem  may have to balance many competing concerns.

=== Unsuitable source systems ===

Change data capture both increases in complexity and reduces in value if the source system saves [[metadata]] changes when the data itself is not modified.  For example, some [[Data model]]s track the user who last looked at but did not change the data in the same structure as the data.  This results in [[Noise (signal processing)|noise]] in the Change Data Capture.

=== Tracking the capture ===

Actually tracking the changes depends on the data source.  If the data is being persisted in a modern [[database]] then Change Data Capture is a simple matter of permissions.  Two techniques are in common use:
# Tracking changes using [[Database Trigger]]s
# Reading the [[transaction log]] as, or shortly after, it is written.

If the data is not in a modern database, Change Data Capture becomes a programming challenge.

=== Push versus pull ===
* '''Push''': the source process creates a snapshot of changes within its own process and delivers rows downstream. The downstream process uses the snapshot, creates its own subset and delivers them to the next process.
* '''Pull''': the target that is immediately downstream from the source, prepares a request for data from the source. The downstream target delivers the snapshot to the next target, as in the push model.

=== Alternatives ===

Sometimes the [[Slowly changing dimension]] is used as a method, this is an example:
[[File:Scd model.png|frame|center|Scd model]]

==See also==

* [[Slowly Changing Dimension]]
* [[Referential integrity]]

== References ==
{{reflist}}

==External links==
* [https://github.com/linkedin/databus/wiki LinkedIn Databus]
* [http://www.informaticacloud.com/images/whitepapers/data%20replication%20best%20practices.pdf Data Replication as a Service Best Practices]
* [https://web.archive.org/web/20110902084451/http://www.attunity.com:80/attunity_stream Attunity Change Data Capture (CDC)]
* [http://www-01.ibm.com/software/data/infosphere/change-data-capture/  IBM Infosphere CDC]
* [https://web.archive.org/web/20060523023144/http://www.oracle.com:80/technology/oramag/oracle/03-nov/o63tech_bi.html Tutorial on setting up CDC in Oracle 9i]
* [http://social.technet.microsoft.com/wiki/contents/articles/how-to-enable-sql-azure-change-data-capture.aspx Tutorial on setting up SQL Azure Change Data Capture]
* [http://msdn2.microsoft.com/en-us/library/bb522489(SQL.100).aspx Details of the CDC facility included in Microsoft Sql Server 2008 Feb '08 CTP]
* [http://www.jumpmind.com/products/symmetricds/features SymmetricDS - Heterogeneous, Cross Platform CDC]
* [http://www.gamma-soft.com/wp/index.php?page_id=30 Gamma-Soft CDC Technology]
* [http://www.talend.com/download/talend-open-studio?qt-product_tos_download=3#qt-product_tos_download Talend]

{{DEFAULTSORT:Change Data Capture}}
[[Category:Computer data]]
[[Category:Data management]]</text>
      <sha1>lr4yj0hsc0qbibwkv666wl89h700hkg</sha1>
    </revision>
  </page>
  <page>
    <title>Commitment ordering</title>
    <ns>0</ns>
    <id>4379212</id>
    <revision>
      <id>749431938</id>
      <parentid>748688337</parentid>
      <timestamp>2016-11-14T08:23:42Z</timestamp>
      <contributor>
        <username>Pintoch</username>
        <id>16990030</id>
      </contributor>
      <minor />
      <comment>/* Strict CO (SCO) */change |id={{citeseerx}} to |citeseerx= using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="94670" xml:space="preserve">{{multiple issues|
{{expert subject|computer science|date=October 2012|reason=it is impossible to copy edit the article in its current state}}
{{notability|date=December 2011}}
{{more footnotes|date=November 2011}}
{{technical|date=November 2011}}
{{essay-like|date=November 2011}}
}}

'''Commitment ordering''' ('''CO''') is a class of interoperable ''[[serializability]]'' techniques in [[concurrency control]] of [[database]]s, [[transaction processing]], and related applications. It allows [[Serializability#Optimistic versus pessimistic techniques|optimistic]] (non-blocking) implementations. With the proliferation of [[multi-core processor]]s, CO has been also increasingly utilized in [[Concurrent computing|concurrent programming]], [[transactional memory]], and especially in [[software transactional memory]] (STM) for achieving serializability [[Optimistic concurrency control|optimistically]]. CO is also the name of the resulting transaction [[Schedule (computer science)|schedule]] (history) property, which was originally defined in 1988 with the name ''dynamic atomicity''.&lt;ref name=Fekete1988&gt;Alan Fekete, [[Nancy Lynch]], Michael Merritt, William Weihl (1988): [http://www.dtic.mil/cgi-bin/GetTRDoc?AD=ADA200980&amp;Location=U2&amp;doc=GetTRDoc.pdf ''Commutativity-based locking for nested transactions'' (PDF)] MIT, LCS lab, Technical report MIT/LCS/TM-370, August 1988.&lt;/ref&gt; In a CO compliant schedule the chronological order of commitment events of transactions is compatible with the [[Serializability#Testing conflict serializability|precedence]] order of the respective transactions. CO is a broad special case of ''[[serializability#View and conflict serializability|conflict serializability]]'', and effective means ([[Reliability engineering|reliable]], high-performance, distributed, and [[Scalability|scalable]]) to achieve [[global serializability]] (modular serializability) across any collection of database systems that possibly use different concurrency control mechanisms (CO also makes each system serializability compliant, if not already).

Each not-CO-compliant database system is augmented with a CO component (the commitment order coordinator—COCO) which orders the commitment events for CO compliance, with neither data-access nor any other transaction operation interference. As such CO provides a low overhead, general solution for global serializability (and distributed serializability), instrumental for [[global concurrency control]] (and [[distributed concurrency control]]) of multi database systems and other [[transactional object]]s, possibly highly distributed (e.g., within [[cloud computing]], [[grid computing]], and networks of [[smartphone]]s). An [[atomic commitment protocol]] (ACP; of any type) is a fundamental part of the solution, utilized to break global cycles in the conflict (precedence, serializability) graph. CO is the most general property (a [[necessary condition]]) that guarantees global serializability, if the database systems involved do not share concurrency control information beyond atomic commitment protocol (unmodified) messages, and have no knowledge whether transactions are global or local (the database systems are ''autonomous''). Thus CO (with its variants) is the only general technique that does not require the typically costly distribution of local concurrency control information (e.g., local precedence relations, locks, timestamps, or tickets). It generalizes the popular ''[[Two-phase locking|strong strict two-phase locking]]'' (SS2PL) property, which in conjunction with the ''[[two-phase commit protocol]]'' (2PC) is the [[de facto standard]] to achieve global serializability across (SS2PL based) database systems. As a result, CO compliant database systems (with any, different concurrency control types) can transparently join such SS2PL based solutions for global serializability.

In addition, locking based ''global deadlocks'' are resolved automatically in a CO based multi-database environment, an important side-benefit (including the special case of a completely SS2PL based environment; a previously unnoticed fact for SS2PL).

Furthermore, '''strict commitment ordering''' (SCO; [[#Raz1991c|Raz 1991c]]), the intersection of ''[[Schedule (computer science)#Strict|Strictness]]'' and CO, provides better performance (shorter average transaction completion time and resulting better transaction [[throughput]]) than SS2PL whenever read-write conflicts are present (identical blocking behavior for write-read and write-write conflicts; comparable locking overhead). The advantage of SCO is especially significant during lock contention. Strictness allows both SS2PL and SCO to use the same effective ''database recovery'' mechanisms.

Two major generalizing variants of CO exist, '''extended CO''' (ECO; [[#Raz1993a|Raz 1993a]]) and '''multi-version CO''' (MVCO; [[#Raz1993b|Raz 1993b]]). They as well provide global serializability without local concurrency control information distribution, can be combined with any relevant concurrency control, and allow optimistic (non-blocking) implementations. Both use additional information for relaxing CO constraints and achieving better concurrency and performance. '''Vote ordering''' (VO or Generalized CO (GCO); [[#Raz2009|Raz 2009]]) is a container schedule set (property) and technique for CO and all its variants. Local VO is a necessary condition for guaranteeing global serializability, if the atomic commitment protocol (ACP) participants do not share concurrency control information (have the ''generalized autonomy'' property). CO and its variants inter-operate transparently, guaranteeing global serializability and automatic global deadlock resolution also together in a mixed, heterogeneous environment with different variants.

==Overview==

The ''Commitment ordering'' (CO; [[#Raz1990|Raz 1990]], [[#Raz1992|1992]], [[#Raz1994|1994]], [[#Raz2009|2009]]) schedule property has been referred to also as ''Dynamic atomicity'' (since 1988&lt;ref name=Fekete1988/&gt;), ''commit ordering'', ''commit order serializability'',  and ''strong recoverability'' (since 1991). The latter is a misleading name since CO is incomparable with ''[[serializability#Correctness - recoverability|recoverability]]'', and the term "strong" implies a special case. This means that a schedule with a strong recoverability property does not necessarily have the CO property, and vice versa.

In 2009 CO has been characterized as a major concurrency control method, together with the previously known (since the 1980s) three major methods: ''Locking'', ''Time-stamp ordering'', and ''Serialization graph testing'', and as an enabler for the interoperability of systems using different concurrency control mechanisms.&lt;ref name=Bern2009&gt;[[Phil Bernstein|Philip A. Bernstein]], Eric Newcomer (2009): [http://www.elsevierdirect.com/product.jsp?isbn=9781558606234 ''Principles of Transaction Processing'', 2nd Edition],  Morgan Kaufmann (Elsevier), June 2009, ISBN 978-1-55860-623-4 (pages 145, 360)&lt;/ref&gt;

In a [[federated database system]] or any other more loosely defined multidatabase system, which are typically distributed in a communication network, transactions span multiple and possibly [[Distributed database]]s. Enforcing [[global serializability]] in such system is problematic. Even if every local schedule of a single database is serializable, still, the global schedule of a whole system is not necessarily serializable. The massive communication exchanges of conflict information needed between databases to reach conflict serializability would lead to unacceptable performance, primarily due to computer and communication [[latency (engineering)|latency]]. The problem of achieving global serializability effectively had been characterized as [[open problem|open]] until the public disclosure of CO in 1991 by its [[Invention|inventor]] [[Yoav Raz]] ([[#Raz1991a|Raz 1991a]]; see also [[Global serializability]]).

Enforcing CO is an effective way to enforce conflict serializability globally in a distributed system, since enforcing CO locally in each database (or other transactional object) also enforces it globally. Each database may use any, possibly different, type of concurrency control mechanism. With a local mechanism that already provides conflict serializability, enforcing CO locally does not cause any additional aborts, since enforcing CO locally does not affect the data access scheduling strategy of the mechanism (this scheduling determines the serializability related aborts; such a mechanism typically does not consider the commitment events or their order). The CO solution requires no communication overhead, since it uses (unmodified) ''[[atomic commitment]]'' protocol messages only, already needed by each distributed transaction to reach atomicity. An atomic commitment protocol plays a central role in the distributed CO algorithm, which enforces CO globally, by breaking global cycles (cycles that span two or more databases) in the global conflict graph.
CO, its special cases, and its generalizations are interoperable, and achieve global serializability while transparently being utilized together in a single heterogeneous distributed environment comprising objects with possibly different concurrency control mechanisms. As such, ''Commitment ordering'', including its special cases, and together with its generalizations (see CO variants below), provides a general, high performance, fully distributed solution (no central processing component or central data structure are needed) for guaranteeing global serializability in heterogeneous environments of multidatabase systems and other multiple transactional objects (objects with states accessed and modified only by transactions; e.g., in the framework of [[transactional processes]], and within Cloud computing and Grid computing). The CO solution scales up with network size and the number of databases without any negative impact on performance (assuming the statistics of a single distributed transaction, e.g., the average number of databases involved with a single transaction, are unchanged).

With the proliferation of [[Multi-core processor]]s, Optimistic CO (OCO) has been also increasingly utilized to achieve serializability in software transactional memory, and numerous STM articles and patents utilizing "commit order" have already been published (e.g., Zhang et al. 2006&lt;ref name=Zhang2006/&gt;).

==The commitment ordering solution for global serializability==

===General characterization of CO===

''Commitment ordering'' (CO) is a special case of conflict serializability. CO can be enforced with ''non-blocking'' mechanisms (each transaction can complete its task without having its data-access blocked, which allows [[optimistic concurrency control]]; however, commitment could be blocked). In a CO schedule the commitment events' ([[partial order|partial]]) precedence order of the transactions corresponds to the precedence (partial) order of the respective transactions in the ([[directed graph|directed]]) conflict graph (precedence graph, serializability graph), as induced by their conflicting access operations (usually read and write (insert/modify/delete) operations; CO also applies to higher level operations, where they are conflicting if [[noncommutative]], as well as to conflicts between operations upon multi-version data).

;Definition{{colon}} commitment ordering: Let &lt;math&gt;T_{1}, T_{2}&lt;/math&gt; be two ''committed'' transactions in a schedule, such that &lt;math&gt;T_{2}&lt;/math&gt; is ''in a conflict'' with &lt;math&gt;T_{1}&lt;/math&gt; (&lt;math&gt;T_{1}&lt;/math&gt; ''precedes'' &lt;math&gt;T_{2}&lt;/math&gt;). The schedule has the '''Commitment ordering''' (CO) property, if for every two such transactions &lt;math&gt;T_{1}&lt;/math&gt; commits before &lt;math&gt;T_{2}&lt;/math&gt; commits.

The commitment decision events are generated by either a local commitment mechanism, or an atomic commitment protocol, if different processes need to reach consensus on whether to commit or abort. The protocol may be distributed or centralized. Transactions may be committed concurrently, if the commit partial order allows (if they do not have conflicting operations). If different conflicting operations induce different partial orders of same transactions, then the conflict graph has [[cycle (graph theory)|cycles]], and the schedule will violate serializability when all the transactions on a cycle are committed. In this case no partial order for commitment events can be found. Thus, cycles in the conflict graph need to be broken by aborting transactions. However, any conflict serializable schedule can be made CO without aborting any transaction, by properly delaying commit events to comply with the transactions' precedence partial order.

CO enforcement by itself is not sufficient as a concurrency control mechanism, since CO lacks the recoverability property, which should be supported as well.

===The distributed CO algorithm===

A fully distributed ''Global commitment ordering'' enforcement algorithm exists, that uses local CO of each participating database, and needs only (unmodified) Atomic commitment protocol messages with no further communication. The distributed algorithm is the combination of local (to each database) CO algorithm processes, and an atomic commitment protocol (which can be fully distributed).
Atomic commitment protocol is essential to enforce atomicity of each distributed transaction (to decide whether to commit or abort it; this procedure is always carried out for distributed transactions, independently of concurrency control and CO). A common example of an atomic commitment protocol is the ''[[two-phase commit protocol]]'', which is resilient to many types of system failure. In a reliable environment, or when processes usually fail together (e.g., in the same [[integrated circuit]]), a simpler protocol for atomic commitment may be used (e.g., a simple handshake of distributed transaction's participating processes with some arbitrary but known special participant, the transaction's coordinator, i.e., a type of ''one-phase commit'' protocol). An atomic commitment protocol reaches consensus among participants on whether to ''commit'' or ''abort'' a distributed (global) transaction that spans these participants. An essential stage in each such protocol is the '''YES vote''' (either explicit, or implicit) by each participant, which means an obligation of the voting participant to obey the decision of the protocol, either commit or abort. Otherwise a participant can unilaterally abort the transaction by an explicit NO vote. The protocol commits the transaction only if YES votes have been received from ''all'' participants, and thus typically a missing YES vote of a participant is considered a NO vote by this participant. Otherwise the protocol aborts the transaction. The various atomic commit protocols only differ in their abilities to handle different computing environment failure situations, and the amounts of work and other computing resources needed in different situations.

The entire CO solution for global serializability is based on the fact that in case of a missing vote for a distributed transaction, the atomic commitment protocol eventually aborts this transaction.

====Enforcing global CO====

In each database system a local CO algorithm determines the needed commitment order for that database. By the characterization of CO above, this order depends on the local precedence order of transactions, which results from the local data access scheduling mechanisms. Accordingly, YES votes in the atomic commitment protocol are scheduled for each (unaborted) distributed transaction (in what follows "a vote" means a YES vote). If a precedence relation (conflict) exists between two transactions, then the second will not be voted on before the first is completed (either committed or aborted), to prevent possible commit order violation by the atomic commitment protocol. Such can happen since the commit order by the protocol is not necessarily the same as the voting order. If no precedence relation exists, both can be voted on concurrently. This ''vote ordering strategy'' ensures that also the atomic commitment protocol maintains commitment order, and it is a ''necessary condition'' for guaranteeing Global CO (and the local CO of a database; without it both Global CO and Local CO (a property meaning that each database is CO compliant) may be violated).

However, since database systems schedule their transactions independently, it is possible that the transactions' precedence orders in two databases or more are not compatible (no global partial order exists that can [[Embedding|embed]] the respective local partial orders together). With CO precedence orders are also the commitment orders. When participating databases in a same distributed transaction do not have compatible local precedence orders for that transaction (without "knowing" it; typically no coordination between database systems exists on conflicts, since the needed communication is massive and unacceptably degrades performance) it means that the transaction resides on a global cycle (involving two or more databases) in the global conflict graph. In this case the atomic commitment protocol will fail to collect all the votes needed to commit that transaction: By the ''vote ordering strategy'' above at least one database will delay its vote for that transaction indefinitely, to comply with its own commitment (precedence) order, since it will be waiting to the completion of another, preceding transaction on that global cycle, delayed indefinitely by another database with a different order. This means a '''''voting-[[deadlock]]''''' situation involving the databases on that cycle.
As a result, the protocol will eventually abort some deadlocked transaction on this global cycle, since each such transaction is missing at least one participant's vote. Selection of the specific transaction on the cycle to be aborted depends on the atomic commitment protocol's abort policies (a [[timeout (telecommunication)|timeout]] mechanism is common, but it may result in more than one needed abort per cycle; both preventing unnecessary aborts and abort time shortening can be achieved by a dedicated abort mechanism for CO). Such abort will break the global cycle involving that distributed transaction. Both deadlocked transactions and possibly other in conflict with the deadlocked (and thus blocked) will be free to be voted on. It is worthwhile noting that each database involved with the voting-deadlock continues to vote regularly on transactions that are not in conflict with its deadlocked transaction, typically almost all the outstanding transactions. Thus, in case of incompatible local (partial) commitment orders, no action is needed since the atomic commitment protocol resolves it automatically by aborting a transaction that is a cause of incompatibility. This means that the above ''vote ordering strategy'' is also a ''sufficient condition'' for guaranteeing Global CO.

The following is concluded:

*'''The Vote ordering strategy for Global CO Enforcing [[Theorem]]'''

:Let &lt;math&gt;T_{1}, T_{2}&lt;/math&gt; be undecided (neither committed nor aborted) transactions in a database system that enforces CO for local transactions, such that &lt;math&gt;T_{2}&lt;/math&gt; is ''global'' and ''in conflict'' with &lt;math&gt;T_{1}&lt;/math&gt; (&lt;math&gt;T_{1}&lt;/math&gt; ''precedes'' &lt;math&gt;T_{2}&lt;/math&gt;). Then, having &lt;math&gt;T_{1}&lt;/math&gt; ended (either committed or aborted) before &lt;math&gt;T_{2}&lt;/math&gt; is voted on to be committed (the ''vote ordering strategy''), in each such database system in a multidatabase environment, is a [[necessary and sufficient condition]] for guaranteeing Global CO (the condition guarantees Global CO, which may be violated without it).

:'''Comments:'''
# The ''vote ordering strategy'' that enforces global CO is referred to as &lt;math&gt;CD^3C&lt;/math&gt; in ([[#Raz1992|Raz 1992]]).
#The Local CO property of a global schedule means that each database is CO compliant. From the necessity discussion part above it directly follows that the theorem is true also when replacing "Global CO" with "Local CO" when global transactions are present. Together it means that Global CO is guaranteed [[if and only if]] Local CO is guaranteed (which is untrue for Global conflict serializability and Local conflict serializability: Global implies Local, but not the opposite).

Global CO implies Global serializability.

The '''Global CO algorithm''' comprises enforcing (local) CO in each participating database system by ordering commits of local transactions (see [[Commitment ordering#Enforcing CO locally|Enforcing CO locally]] below) and enforcing the ''vote ordering strategy'' in the theorem above (for global transactions).

====Exact characterization of voting-deadlocks by global cycles====

The above global cycle elimination process by a '''voting deadlock''' can be explained in detail by the following observation:

First it is assumed, for simplicity, that every transaction reaches the ready-to-commit state and is voted on by at least one database (this implies that no blocking by locks occurs).
Define a ''"wait for vote to commit" graph'' as a directed graph with transactions as nodes, and a directed edge from any first transaction to a second transaction if the first transaction blocks the vote to commit of the second transaction (opposite to conventional edge direction in a [[wait-for graph]]). Such blocking happens only if the second transaction is in a conflict with the first transaction (see above). Thus this "wait for vote to commit" graph is identical to the global conflict graph. A cycle in the "wait for vote to commit" graph means a deadlock in voting. Hence there is a deadlock in voting if and only if there is a cycle in the conflict graph. Local cycles (confined to a single database) are eliminated by the local serializability mechanisms. Consequently, only global cycles are left, which are then eliminated by the atomic commitment protocol when it aborts deadlocked transactions with missing (blocked) respective votes.

Secondly, also local commits are dealt with: Note that when enforcing CO also waiting for a regular local commit of a local transaction can block local commits and votes of other transactions upon conflicts, and the situation for global transactions does not change also without the simplifying assumption above: The final result is the same also with local commitment for local transactions, without voting in atomic commitment for them.

Finally, blocking by a lock (which has been excluded so far) needs to be considered: A lock blocks a conflicting operation and prevents a conflict from being materialized. If the lock is released only after transaction end, it may block indirectly either a vote or a local commit of another transaction (which now cannot get to ready state), with the same effect as of a direct blocking of a vote or a local commit. In this case a cycle is generated in the conflict graph only if such a blocking by a lock is also represented by an edge. With such added edges representing events of blocking-by-a-lock, the conflict graph is becoming an ''augmented conflict graph''.

*'''Definition: augmented conflict graph'''

:An '''augmented conflict graph''' is a [[serializability#Testing conflict serializability|conflict graph]] with added edges: In addition to the original edges a directed edge exists from transaction &lt;math&gt;T_{1}&lt;/math&gt; to transaction &lt;math&gt;T_{2}&lt;/math&gt; if two conditions are met:
# &lt;math&gt;T_{2}&lt;/math&gt; is blocked by a data-access lock applied by &lt;math&gt;T_{1}&lt;/math&gt; (the blocking prevents the conflict of &lt;math&gt;T_{2}&lt;/math&gt; with &lt;math&gt;T_{1}&lt;/math&gt; from being materialized and have an edge in the regular conflict graph), and
# This blocking will not stop before &lt;math&gt;T_{1}&lt;/math&gt; ends (commits or aborts; true for any locking-based CO)

:The graph can also be defined as the [[Union (set theory)|union]] of the (regular) ''conflict graph'' with the (reversed edge, regular) ''wait-for graph''

:'''Comments:'''
# Here, unlike the regular conflict graph, which has edges only for materialized conflicts, all conflicts, both materialized and non-materialized, are represented by edges.
# Note that all the new edges are all the (reversed to the conventional) edges of the ''wait-for graph''. The ''wait-for graph'' can be defined also as the graph of non-materialized conflicts. By the common conventions edge direction in a ''conflict graph'' defines time order between conflicting operations which is opposite to the time order defined by an edge in a ''wait-for graph''.
# Note that such global graph contains (has embedded) all the (reversed edge) regular local ''wait-for'' graphs, and also may include locking based global cycles (which cannot exist in the local graphs). For example, if all the databases on a global cycle are SS2PL based, then all the related vote blocking situations are caused by locks (this is the classical, and probably the only global deadlock situation dealt with in the database research literature). This is a global deadlock case where each related database creates a portion of the cycle, but the complete cycle does not reside in any local wait-for graph.

In the presence of CO the ''augmented conflict graph'' is in fact a (reversed edge) ''local-commit and voting wait-for graph'': An edge exists from a first transaction, either local or global, to a second, if the second is waiting for the first to end in order to be either voted on (if global), or locally committed (if local). All ''global cycles'' (across two or more databases) in this graph generate voting-deadlocks. The graph's global cycles provide complete characterization for voting deadlocks and may include any combination of materialized and non-materialized conflicts. Only cycles of (only) materialized conflicts are also cycles of the regular conflict graph and affect serializability. One or more (lock related) non-materialized conflicts on a cycle prevent it from being a cycle in the regular conflict graph, and make it a locking related deadlock. All the global cycles (voting-deadlocks) need to be broken (resolved) to both maintain global serializability and resolve global deadlocks involving data access locking, and indeed they are all broken by the atomic commitment protocol due to missing votes upon a voting deadlock.

'''Comment:''' This observation also explains the correctness of ''[[Commitment ordering#Extended CO (ECO)|Extended CO (ECO)]]'' below: Global transactions' voting order must follow the conflict graph order with vote blocking when order relation (graph path) exists between two global transactions. Local transactions are not voted on, and their (local) commits are not blocked upon conflicts. This results in same voting-deadlock situations and resulting global cycle elimination process for ECO.

The ''voting-deadlock'' situation can be summarized as follows:

*'''The CO Voting-Deadlock Theorem'''

:Let a multidatabase environment comprise CO compliant (which eliminates ''local cycles'') database systems that enforce, each, ''Global CO'' (using the condition in the theorem above). Then a ''voting-deadlock'' occurs if and only if a ''global cycle'' (spans two or more databases) exists in the ''Global augmented conflict graph'' (also blocking by a data-access lock is represented by an edge). If the cycle does not break by any abort, then all the ''global transactions'' on it are involved with the respective voting-deadlock, and eventually each has its vote blocked (either directly, or indirectly by a data-access lock); if a local transaction resides on the cycle, eventually it has its (local) commit blocked.

:'''Comment:''' A rare situation of a voting deadlock (by missing blocked votes) can happen, with no voting for any transaction on the related cycle by any of the database systems involved with these transactions. This can occur when local sub-transactions are [[Thread (computer science)|multi-threaded]]. The highest probability instance of such rare event involves two transactions on two simultaneous opposite cycles. Such global cycles (deadlocks) overlap with local cycles which are resolved locally, and thus typically resolved by local mechanisms without involving atomic commitment. Formally it is also a global cycle, but practically it is local (portions of local cycles generate a global one; to see this, split each global transaction (node) to local sub-transactions (its portions confined each to a single database); a directed edge exists between transactions if an edge exists between any respective local sub-transactions; a cycle is local if all its edges originate from a cycle among sub-transactions of the same database, and global if not; global and local can overlap: a same cycle among transactions can result from several different cycles among sub-transactions, and be both local and global).

Also the following locking based special case is concluded:

*'''The CO Locking-based Global-Deadlock Theorem'''

:In a CO compliant multidatabase system a locking-based global-deadlock, involving at least one data-access lock (non-materialized conflict), and two or more database systems, is a reflection of a global cycle in the ''Global augmented conflict graph'', which results in a voting-deadlock. Such cycle is not a cycle in the (regular) ''Global conflict graph'' (which reflects only materialized conflicts, and thus such cycle does not affect ''[[serializability]]'').

:'''Comments:'''
# Any blocking (edge) in the cycle that is not by a data-access lock is a direct blocking of either voting or local commit. All voting-deadlocks are resolved (almost all by ''Atomic commitment''; see comment above), including this locking-based type.
# Locking-based global-deadlocks can be generated also in a completely SS2PL-based distributed environment (special case of CO based), where all the vote blocking (and voting-deadlocks) are caused by data-access locks. Many research articles have dealt for years with resolving such global deadlocks, but none (except the CO articles) is known (as of 2009) to notice that ''atomic commitment'' automatically resolves them. Such automatic resolutions are regularly occurring unnoticed in all existing SS2PL based multidatabase systems, often bypassing dedicated resolution mechanisms.

Voting-deadlocks are the key for the operation of distributed CO.

Global cycle elimination (here voting-deadlock resolution by ''atomic commitment'') and resulting aborted transactions' re-executions are time consuming, regardless of concurrency control used. If databases schedule transactions independently, global cycles are unavoidable (in a complete analogy to cycles/deadlocks generated in local SS2PL; with distribution, any transaction or operation scheduling coordination results in autonomy violation, and typically also in substantial performance penalty). However, in many cases their likelihood can be made very low by implementing database and transaction design guidelines that reduce the number of conflicts involving a global transaction. This, primarily by properly handling hot spots (database objects with frequent access), and avoiding conflicts by using commutativity when possible (e.g., when extensively using counters, as in finances, and especially multi-transaction ''accumulation counters'', which are typically hot spots).

Atomic commitment protocols are intended and designed to achieve atomicity without considering database concurrency control. They abort upon detecting or [[Heuristic algorithm|heuristically]] finding (e.g., by timeout; sometimes mistakenly, unnecessarily) missing votes, and typically unaware of global cycles. These protocols can be specially enhanced for CO (including CO's variants below) both to prevent unnecessary aborts, and to accelerate aborts used for breaking global cycles in the global augmented conflict graph (for better performance by earlier release upon transaction-end of computing resources and typically locked data). For example, existing locking based global deadlock detection methods, other than timeout, can be generalized to consider also local commit and vote direct blocking, besides data access blocking. A possible compromise in such mechanisms is effectively detecting and breaking the most frequent and relatively simple to handle length-2 global cycles, and using timeout for undetected, much less frequent, longer cycles.

===Enforcing CO locally===

''Commitment ordering'' can be enforced locally (in a single database) by a dedicated CO algorithm, or by any algorithm/protocol that provides any special case of CO. An important such protocol, being utilized extensively in database systems, which generates a CO schedule, is the ''strong strict [[two phase locking]]'' protocol (SS2PL: "release transaction's locks only after the transaction has been either committed or aborted"; see below). SS2PL is a [[proper subset]] of the intersection of [[Two-phase locking|2PL]] and strictness.

====A generic local CO algorithm====

A '''generic local CO algorithm''' ([[#Raz1992|Raz 1992]]; Algorithm 4.1) is an algorithm independent of implementation details, that enforces exactly the CO property. It does not block data access (nonblocking), and consists of aborting a certain set of transactions (only if needed) upon committing a transaction. It aborts a (uniquely determined at any given time) minimal set of other undecided (neither committed, nor aborted) transactions that run locally and can cause serializability violation in the future (can later generate cycles of committed transactions in the conflict graph; this is the ABORT set of a committed transaction T; after committing T no transaction in ABORT at commit time can be committed, and all of them are doomed to be aborted). This set consists of all undecided transactions with directed edges in the conflict graph to the committed transaction. The size of this set cannot increase when that transaction is waiting to be committed (in ready state: processing has ended), and typically decreases in time as its transactions are being decided. Thus, unless [[Real-time computing|real-time]] constraints exist to complete that transaction, it is preferred to wait with committing that transaction and let this set decrease in size. If another serializability mechanism exists locally (which eliminates cycles in the local conflict graph), or if no cycle involving that transaction exists, the set will be empty eventually, and no abort of set member is needed. Otherwise the set will stabilize with transactions on local cycles, and aborting set members will have to occur to break the cycles. Since in the case of CO conflicts generate blocking on commit, local cycles in the ''augments conflict graph'' (see above) indicate local commit-deadlocks, and deadlock resolution techniques as in [[Serializability#Common mechanism - SS2PL|SS2PL]] can be used (e.g., like ''timeout'' and ''wait-for graph''). A local cycle in the ''augmented conflict graph'' with at least one non-materialized conflict reflects a locking-based deadlock. The local algorithm above, applied to the local augmented conflict graph rather than the regular local conflict graph, comprises the '''generic enhanced local CO algorithm''', a single local cycle elimination mechanism, for both guaranteeing local serializability and handling locking based local deadlocks. Practically an additional concurrency control mechanism is always utilized, even solely to enforce recoverability. The generic CO algorithm does not affect local data access scheduling strategy, when it runs alongside of any other local concurrency control mechanism. It affects only the commit order, and for this reason it does not need to abort more transactions than those needed to be aborted for serializability violation prevention by any combined local concurrency control mechanism. The net effect of CO may be, at most, a delay of commit events (or voting in a distributed environment), to comply with the needed commit order (but not more delay than its special cases, for example, SS2PL, and on the average significantly less).

The following theorem is concluded:

*'''The Generic Local CO Algorithm Theorem'''
:When running alone or alongside any concurrency control mechanism in a database system then
#The ''Generic local CO algorithm'' guarantees (local) CO (a CO compliant schedule).
#The ''Generic enhanced local CO algorithm'' guarantees both (local) CO and (local) locking based deadlock resolution.
: and (when not using ''timeout'', and no ''real-time'' transaction completion constraints are applied) neither algorithm aborts more transactions than the minimum needed (which is determined by the transactions' operations scheduling, out of the scope of the algorithms).

====Example: Concurrent programming and Transactional memory====
:See also ''[[The History of Commitment Ordering#Concurrent programming and Transactional memory|Concurrent programming and Transactional memory]]''

With the proliferation of Multi-core processors, variants of the Generic local CO algorithm have been also increasingly utilized in Concurrent programming, [[Transactional memory]], and especially in Software transactional memory for achieving serializability optimistically by "commit order" (e.g., Ramadan et al. 2009,&lt;ref name=Ramadan2009&gt;Hany E. Ramadan, Indrajit Roy, Maurice Herlihy, Emmett Witchel (2009): [http://portal.acm.org/citation.cfm?id=1504201 "Committing conflicting transactions in an STM"] ([http://www.cs.utexas.edu/~indrajit/pubs/ppopp121-ramadan.pdf PDF]) ''Proceedings of the 14th ACM SIGPLAN symposium on Principles and practice of parallel programming'' (PPoPP '09), ISBN 978-1-60558-397-6&lt;/ref&gt; Zhang et al. 2006,&lt;ref name=Zhang2006&gt;Lingli Zhang, Vinod K.Grover, Michael M. Magruder, David Detlefs, John Joseph Duffy, Goetz Graefe (2006): [http://www.freepatentsonline.com/7711678.html  Software transaction commit order and conflict management] United States Patent 7711678, Granted 05/04/2010.&lt;/ref&gt; von Parun et al. 2007&lt;ref name=vonParun2007&gt;Christoph von Praun, Luis Ceze, Calin Cascaval (2007) [http://portal.acm.org/citation.cfm?id=1229443 "Implicit Parallelism with Ordered Transactions"] ([http://www.cs.washington.edu/homes/luisceze/publications/ipot_ppopp07.pdf PDF]), ''Proceedings of the 12th ACM SIGPLAN symposium on Principles and practice of parallel programming'' (PPoPP '07), ACM New York ©2007, ISBN 978-1-59593-602-8 doi 10.1145/1229428.1229443&lt;/ref&gt;). Numerous related articles and patents utilizing CO have already been published.

====Implementation considerations: The Commitment Order Coordinator (COCO)====

A database system in a multidatabase environment is assumed. From a [[software architecture]] point of view a CO component that implements the generic CO algorithm locally, the ''Commitment Order Coordinator'' (COCO), can be designed in a straightforward way as a [[mediator pattern|mediator]] between a (single) database system and an atomic commitment protocol component ([[#Raz1991b|Raz 1991b]]). However, the COCO is typically an integral part of the database system. The COCO's functions are to vote to commit on ready global transactions (processing has ended) according to the local commitment order, to vote to abort on transactions for which the database system has initiated an abort (the database system can initiate abort for any transaction, for many reasons), and to pass the atomic commitment decision to the database system. For local transactions (when can be identified) no voting is needed. For determining the commitment order the COCO maintains an updated representation of the local conflict graph (or local augmented conflict graph for capturing also locking deadlocks) of the undecided (neither committed nor aborted) transactions as a data structure (e.g., utilizing mechanisms similar to [[lock (computer science)|locking]] for capturing conflicts, but with no data-access blocking). The COCO component has an [[interface (computer science)|interface]] with its database system to receive "conflict," "ready" (processing has ended; readiness to vote on a global transaction or commit a local one), and "abort" notifications from the database system. It also interfaces with the atomic commitment protocol to vote and to receive the atomic commitment protocol's decision on each global transaction. The decisions are delivered from the COCO to the database system through their interface, as well as local transactions' commit notifications, at a proper commit order. The COCO, including its interfaces, can be enhanced, if it implements another variant of CO (see below), or plays a role in the database's concurrency control mechanism beyond voting in atomic commitment.

The COCO also guarantees CO locally in a single, isolated database system with no interface with an atomic commitment protocol.

===CO is a necessary condition for global serializability across autonomous database systems===

If the databases that participate in distributed transactions (i.e., transactions that span more than a single database) do not use any shared concurrency control information and use unmodified atomic commitment protocol messages (for reaching atomicity), then maintaining (local) ''commitment ordering'' or one of its generalizing variants (see below) is a [[necessary condition]] for guaranteeing global serializability (a proof technique can be found in ([[#Raz1992|Raz 1992]]), and a different proof method for this in ([[#Raz1993a|Raz 1993a]])); it is also a [[sufficient condition]]. This is a mathematical fact derived from the definitions of ''serializability'' and a ''[[Database transaction|transaction]]''. It means that if not complying with CO, then global serializability cannot be guaranteed under this condition (the condition of no local concurrency control information sharing between databases beyond atomic commit protocol messages). Atomic commitment is a minimal requirement for a distributed transaction since it is always needed, which is implied by the definition of transaction.

([[#Raz1992|Raz 1992]]) defines ''database autonomy'' and ''independence'' as complying with this requirement without using any additional local knowledge:
*'''Definition:''' (concurrency control based) '''autonomous database system'''
:A database system is '''Autonomous''', if it does not share with any other entity any concurrency control information beyond unmodified [[atomic commitment protocol]] messages. In addition it does not use for concurrency control any additional local information beyond conflicts (the last sentence does not appear explicitly but rather implied by further discussion in [[#Raz1992|Raz 1992]]).

Using this definition the following is concluded:

*'''The CO and Global serializability Theorem'''

#CO compliance of every ''autonomous'' database system (or transactional object) in a multidatabase environment is a ''necessary condition'' for guaranteeing Global serializability (without CO Global serializability may be violated).
#CO compliance of every database system is a ''sufficient condition'' for guaranteeing Global serializability.

However, the definition of autonomy above implies, for example, that transactions are scheduled in a way that local transactions (confined to a single database) cannot be identified as such by an autonomous database system. This is realistic for some transactional objects, but too restrictive and less realistic for general purpose database systems. If autonomy is augmented with the ability to identify local transactions, then compliance with a more general property, ''Extended commitment ordering'' (ECO, see below), makes ECO the necessary condition.

Only in ([[#Raz2009|Raz 2009]]) the notion of ''Generalized autonomy'' captures the intended notion of autonomy:
*'''Definition: generalized autonomy'''
:A database system has the ''Generalized autonomy'' property, if it does not share with any other database system any local concurrency information beyond (unmodified) atomic commit protocol messages (however any local information can be utilized).

This definition is probably the broadest such definition possible in the context of database concurrency control, and it makes CO together with any of its (useful: No concurrency control information distribution) generalizing variants (Vote ordering (VO); see CO variants below) the necessary condition for Global serializability (i.e., the union of CO and its generalizing variants is the necessary set VO, which may include also new unknown useful generalizing variants).

===Summary===

The ''Commitment ordering'' (CO) solution (technique) for global serializability can be summarized as follows:

If each ''database'' (or any other ''transactional object'') in a multidatabase environment complies with CO, i.e., arranges its local transactions' commitments and its votes on (global, distributed) transactions to the ''[[atomic commitment]]'' protocol according to the local (to the database) [[partial order]] induced by the local conflict graph (serializability graph) for the respective transactions, then ''Global CO'' and ''Global serializability'' are guaranteed. A database's CO compliance can be achieved effectively with any local [[Serializability#View serializability and conflict serializability|conflict serializability]] based concurrency control mechanism, with neither affecting any transaction's execution process or scheduling, nor aborting it. Also the database's autonomy is not violated. The only low overhead incurred is detecting conflicts (e.g., as with locking, but with no data-access blocking; if not already detected for other purposes), and ordering votes and local transactions' commits according to the conflicts.

[[Image:CO-ScheduleClasses.jpg|thumb|350px| '''Schedule classes containment:''' An arrow from class A to class B indicates that class A strictly contains B; a lack of a directed path between classes means that the classes are incomparable.

A property is '''inherently blocking''', if it can be enforced only by blocking transaction’s data access operations until certain events occur in other transactions. ([[#Raz1992|Raz 1992]])]]

In case of incompatible partial orders of two or more databases (no global partial order can [[Embedding|embed]] the respective local partial orders together), a global cycle (spans two databases or more) in the global conflict graph is generated. This, together with CO, results in a cycle of blocked votes, and a ''voting-[[deadlock]]'' occurs for the databases on that cycle (however, allowed concurrent voting in each database, typically for almost all the outstanding votes, continue to execute). In this case the atomic commitment protocol fails to collect all the votes needed for the blocked transactions on that global cycle, and consequently the protocol aborts some transaction with a missing vote. This breaks the global cycle, the voting-deadlock is resolved, and the related blocked votes are free to be executed. Breaking the global cycle in the global conflict graph ensures that both global CO and global serializability are maintained. Thus, in case of incompatible local (partial) commitment orders no action is needed since the atomic commitment protocol resolves it automatically by aborting a transaction that is a cause for the incompatibility. Furthermore, also global deadlocks due to locking (global cycles in the ''augmented conflict graph'' with at least one data access blocking) result in voting deadlocks and are resolved automatically by the same mechanism.

''Local CO'' is a necessary condition for guaranteeing ''Global serializability,'' if the databases involved do not share any concurrency control information beyond (unmodified) atomic commitment protocol messages, i.e., if the databases are ''autonomous'' in the context of concurrency control. This means that every global serializability solution for autonomous databases must comply with CO. Otherwise global serializability may be violated (and thus, is likely to be violated very quickly in a high-performance environment).

The CO solution [[Scalability|scales up]] with network size and the number of databases without performance penalty when it utilizes [[Two-phase commit protocol#Common architecture|common distributed atomic commitment architecture]].

==Distributed serializability and CO==

===Distributed CO===

A distinguishing characteristic of the CO solution to distributed serializability from other techniques is the fact that it requires no conflict information distributed (e.g., local precedence relations, locks, [[Timestamp-based concurrency control|timestamps]], tickets), which makes it uniquely effective. It utilizes (unmodified) atomic commitment protocol messages (which are already used) instead.

A common way to achieve distributed serializability in a [[Distributed system|(distributed) system]] is by a [[distributed lock manager]] (DLM). DLMs, which communicate lock (non-materialized conflict) information in a distributed environment, typically suffer from computer and communication [[Latency (engineering)|latency]], which reduces the performance of the system. CO allows to achieve distributed serializability under very general conditions, without a distributed lock manager, exhibiting the benefits already explored above for multidatabase environments; in particular: reliability, high performance, scalability, possibility of using ''optimistic concurrency control'' when desired, no conflict information related communications over the network (which have incurred overhead and delays), and automatic distributed deadlock resolution.

All ''distributed transactional systems'' rely on some atomic commitment protocol to coordinate atomicity (whether to commit or abort) among processes in a [[distributed transaction]]. Also, typically ''recoverable data'' (i.e., data under transactions' control, e.g., database data; not to be confused with the ''recoverability'' property of a schedule) are directly accessed by a single ''transactional data manager'' component (also referred to as a ''resource manager'') that handles local sub-transactions (the distributed transaction's portion in a single location, e.g., network node), even if these data are accessed indirectly by other entities in the distributed system during a transaction (i.e., indirect access requires a direct access through a local sub-transaction). Thus recoverable data in a distributed transactional system are typically partitioned among transactional data managers. In such system these transactional data managers typically comprise the participants in the system's atomic commitment protocol. If each participant complies with CO (e.g., by using SS2PL, or COCOs, or a combination; see above), then the entire distributed system provides CO (by the theorems above; each participant can be considered a separate transactional object), and thus (distributed) serializability. Furthermore: When CO is utilized together with an atomic commitment protocol also ''distributed deadlocks'' (i.e., deadlocks that span two or more data managers) caused by data-access locking are resolved automatically. Thus the following corollary is concluded:

*'''The CO Based Distributed Serializability Theorem'''

:Let a ''distributed transactional system'' (e.g., a [[distributed database]] system) comprise ''transactional data managers'' (also called ''resource managers'') that manage all the system's ''recoverable data''. The data managers meet three conditions:
# '''Data partition:''' Recoverable data are partitioned among the data managers, i.e., each recoverable datum (data item) is controlled by a single data manager (e.g., as common in a [[Shared nothing architecture]]; even copies of a same datum under different data managers are physically distinct, ''replicated'').
# '''Participants in atomic commitment protocol:''' These data managers are the participants in the system's atomic commitment protocol for coordinating distributed transactions' atomicity.
# '''CO compliance:''' Each such data manager is CO compliant (or some CO variant compliant; see below).
:Then
# The entire distributed system guarantees (distributed CO and) ''serializability'', and
# Data-access based ''distributed deadlocks'' (deadlocks involving two or more data managers with at least one non-materialized conflict) are resolved automatically.

:Furthermore: The data managers being CO compliant is a ''necessary condition'' for (distributed) serializability in a system meeting conditions 1, 2 above, when the data managers are ''autonomous'', i.e., do not share concurrency control information beyond unmodified messages of atomic commitment protocol.

This theorem also means that when SS2PL (or any other CO variant) is used locally in each transactional data manager, and each data manager has exclusive control of its data, no distributed lock manager (which is often utilized to enforce distributed SS2PL) is needed for distributed SS2PL and serializability. It is relevant to a wide range of distributed transactional applications, which can be easily designed to meet the theorem's conditions.

===Distributed optimistic CO (DOCO)===

For implementing Distributed Optimistic CO (DOCO) the generic local CO algorithm is utilized in all the atomic commitment protocol participants in the system with no data access blocking and thus with no local deadlocks. The previous theorem has the following corollary:

*'''The Distributed optimistic CO (DOCO) Theorem'''

:If DOCO is utilized, then:
:# No local deadlocks occur, and
:# Global (voting) deadlocks are resolved automatically (and all are serializability related (with non-blocking conflicts) rather than locking related (with blocking and possibly also non-blocking conflicts)).

:Thus, no deadlock handling is needed.

===Examples===

====Distributed SS2PL====

A distributed database system that utilizes [[Two-phase locking#Strong strict two-phase locking|SS2PL]] resides on two remote nodes, A and B. The database system has two ''transactional data managers'' (''resource managers''), one on each node, and the database data are partitioned between the two data managers in a way that each has an exclusive control of its own (local to the node) portion of data: Each handles its own data and locks without any knowledge on the other manager's. For each distributed transaction such data managers need to execute the available atomic commitment protocol.

Two distributed transactions, &lt;math&gt;T_{1}&lt;/math&gt; and &lt;math&gt;T_{2}&lt;/math&gt;, are running concurrently, and both access data x and y. x is under the exclusive control of the data manager on A (B's manager cannot access x), and y under that on B.

:&lt;math&gt;T_{1}&lt;/math&gt; reads x on A and writes y on B, i.e., &lt;math&gt;T_{1} = R_{1A}(x)&lt;/math&gt; &lt;math&gt;W_{1B}(y)&lt;/math&gt; when using notation common for concurrency control.
:&lt;math&gt;T_{2}&lt;/math&gt; reads y on B and writes x on A, i.e., &lt;math&gt;T_{2} = R_{2B}(y)&lt;/math&gt; &lt;math&gt;W_{2A}(x)&lt;/math&gt;

The respective ''local sub-transactions'' on A and B (the portions of &lt;math&gt;T_{1}&lt;/math&gt; and &lt;math&gt;T_{2}&lt;/math&gt; on each of the nodes) are the following:

:{| class="wikitable" style="text-align:center;"
|+Local sub-transactions
|-
! Transaction \ Node !! A  !! B
|-
! &lt;math&gt;T_{1}&lt;/math&gt;
|  &lt;math&gt;T_{1A}=R_{1A}(x)&lt;/math&gt; || &lt;math&gt;T_{1B}=W_{1B}(y)&lt;/math&gt;
|-
! &lt;math&gt;T_{2}&lt;/math&gt;
| &lt;math&gt;T_{2A}=W_{2A}(x)&lt;/math&gt; || &lt;math&gt;T_{2B}=R_{2B}(y)&lt;/math&gt;
|}

The database system's [[Schedule (computer science)|schedule]] at a certain point in time is the following:

:&lt;math&gt;R_{1A}(x)&lt;/math&gt; &lt;math&gt;R_{2B}(y)&lt;/math&gt;
:(also &lt;math&gt;R_{2B}(y)&lt;/math&gt; &lt;math&gt;R_{1A}(x)&lt;/math&gt; is possible)

&lt;math&gt;T_{1}&lt;/math&gt; holds a read-lock on x and &lt;math&gt;T_{2}&lt;/math&gt; holds read-locks on y. Thus &lt;math&gt;W_{1B}(y)&lt;/math&gt; and &lt;math&gt;W_{2A}(x)&lt;/math&gt; are blocked by the [[Two-phase locking#Data-access locks|lock compatibility]] rules of SS2PL and cannot be executed. This is a distributed deadlock situation, which is also a voting-deadlock (see below) with a distributed (global) cycle of length 2 (number of edges, conflicts; 2 is the most frequent length). The local sub-transactions are in the following states:

:&lt;math&gt;T_{1A}&lt;/math&gt; is ''ready'' (execution has ended) and ''voted'' (in atomic commitment)
:&lt;math&gt;T_{1B}&lt;/math&gt; is ''running'' and blocked (a non-materialized conflict situation; no vote on it can occur)
:&lt;math&gt;T_{2B}&lt;/math&gt; is ''ready'' and ''voted''
:&lt;math&gt;T_{2A}&lt;/math&gt; is ''running'' and blocked (a non-materialized conflict; no vote).

Since the atomic commitment protocol cannot receive votes for blocked sub-transactions (a voting-deadlock), it will eventually abort some transaction with a missing vote(s) by [[Timeout (computing)|timeout]], either &lt;math&gt;T_{1}&lt;/math&gt;, or &lt;math&gt;T_{2}&lt;/math&gt;, (or both, if the timeouts fall very close). This will resolve the global deadlock. The remaining transaction will complete running, be voted on, and committed. An aborted transaction is immediately ''restarted'' and re-executed.

'''Comments:'''
# The data partition (x on A; y on B) is important since without it, for example, x can be accessed directly from B. If a transaction &lt;math&gt;T_{3}&lt;/math&gt; is running on B concurrently with &lt;math&gt;T_{1}&lt;/math&gt; and &lt;math&gt;T_{2}&lt;/math&gt; and directly writes x, then, without a distributed lock manager the read-lock for x held by &lt;math&gt;T_{1}&lt;/math&gt; on A is not visible on B and cannot block the write of &lt;math&gt;T_{3}&lt;/math&gt; (or signal a materialized conflict for a non-blocking CO variant; see below). Thus serializability can be violated.
# Due to data partition, x cannot be accessed directly from B. However, functionality is not limited, and a transaction running on B still can issue a write or read request of x (not common). This request is communicated to the transaction's local sub-transaction on A (which is generated, if does not exist already) which issues this request to the local data manager on A.

====Variations====

In the scenario above both conflicts are ''non-materialized'', and the global voting-deadlock is reflected as a cycle in the global ''wait-for graph'' (but not in the global ''conflict graph''; see [[Commitment ordering#Exact characterization of voting-deadlocks by global cycles|Exact characterization of voting-deadlocks by global cycles]] above). However the database system can utilize any CO variant with exactly the same conflicts and voting-deadlock situation, and same resolution. Conflicts can be either ''materialized'' or ''non-materialized'', depending on CO variant used. For example, if [[Commitment ordering#Strict CO (SCO)|SCO]] (below) is used by the distributed database system instead of SS2PL, then the two conflicts in the example are ''materialized'', all local sub-transactions are in ''ready'' states, and vote blocking occurs in the two transactions, one on each node, because of the CO voting rule applied independently on both A and B: due to conflicts &lt;math&gt;T_{2A}=W_{2A}(x)&lt;/math&gt; is not voted on before &lt;math&gt;T_{1A}=R_{1A}(x)&lt;/math&gt; ends, and &lt;math&gt;T_{1B}=W_{1B}(y)&lt;/math&gt; is not voted on before &lt;math&gt;T_{2B}=R_{2B}(y)&lt;/math&gt; ends, which is a voting-deadlock. Now the ''conflict graph'' has the global cycle (all conflicts are materialized), and again it is resolved by the atomic commitment protocol, and distributed serializability is maintained. Unlikely for a distributed database system, but possible in principle (and occurs in a multi-database), A can employ SS2PL while B employs SCO. In this case the global cycle is neither in the wait-for graph nor in the serializability graph, but still in the ''augmented conflict graph'' (the union of the two). The various combinations are summarized in the following table:

{| class="wikitable" style="text-align:center;"
|+Voting-deadlock situations
|-
!Case!! Node&lt;br&gt;A  !! Node&lt;br&gt;B !!Possible schedule!!Materialized&lt;br&gt;conflicts&lt;br&gt;on cycle!!Non-&lt;br&gt;materialized&lt;br&gt;conflicts!!&lt;math&gt;T_{1A}&lt;/math&gt; =&lt;br&gt;&lt;math&gt;R_{1A}(x)&lt;/math&gt;!!&lt;math&gt;T_{1B}&lt;/math&gt; =&lt;br&gt;&lt;math&gt;W_{1B}(y)&lt;/math&gt;!!&lt;math&gt;T_{2A}&lt;/math&gt; =&lt;br&gt;&lt;math&gt;W_{2A}(x)&lt;/math&gt;!!&lt;math&gt;T_{2B}&lt;/math&gt; =&lt;br&gt;&lt;math&gt;R_{2B}(y)&lt;/math&gt;
|-
! 1
|SS2PL||SS2PL||&lt;math&gt;R_{1A}(x)&lt;/math&gt; &lt;math&gt;R_{2B}(y)&lt;/math&gt;|| 0 || 2 ||Ready&lt;br&gt;Voted||Running&lt;br&gt;(Blocked)||Running&lt;br&gt;(Blocked)||Ready&lt;br&gt;Voted
|-
! 2
|SS2PL|| SCO ||&lt;math&gt;R_{1A}(x)&lt;/math&gt; &lt;math&gt;R_{2B}(y)&lt;/math&gt; &lt;math&gt;W_{1B}(y)&lt;/math&gt;|| 1 || 1 ||Ready&lt;br&gt;Voted ||Ready&lt;br&gt;Vote blocked||Running&lt;br&gt;(Blocked)||Ready&lt;br&gt;Voted
|-
! 3
|SCO||SS2PL|| &lt;math&gt;R_{1A}(x)&lt;/math&gt; &lt;math&gt;R_{2B}(y)&lt;/math&gt; &lt;math&gt;W_{2A}(x)&lt;/math&gt; || 1 || 1 ||Ready&lt;br&gt;Voted||Running&lt;br&gt;(Blocked)||Ready&lt;br&gt;Vote blocked||Ready&lt;br&gt;Voted
|-
! 4
|SCO||SCO||&lt;math&gt;R_{1A}(x)&lt;/math&gt; &lt;math&gt;R_{2B}(y)&lt;/math&gt; &lt;math&gt;W_{1B}(y)&lt;/math&gt; &lt;math&gt;W_{2A}(x)&lt;/math&gt;|| 2 || 0 ||Ready&lt;br&gt;Voted||Ready&lt;br&gt;Vote blocked ||Ready&lt;br&gt;Vote blocked||Ready&lt;br&gt;Voted
|}

:'''Comments:'''
# Conflicts and thus cycles in the ''augmented conflict graph'' are determined by the transactions and their initial scheduling only, independently of the concurrency control utilized. With any variant of CO, any ''global cycle'' (i.e., spans two databases or more) causes a ''voting deadlock''. Different CO variants may differ on whether a certain conflict is ''materialized'' or ''non-materialized''.
# Some limited operation order changes in the schedules above are possible, constrained by the orders inside the transactions, but such changes do not change the rest of the table.
# As noted above, only case 4 describes a cycle in the (regular) conflict graph which affects serializability. Cases 1-3 describe cycles of locking based global deadlocks (at least one lock blocking exists). All cycle types are equally resolved by the atomic commitment protocol. Case 1 is the common Distributed SS2PL, utilized since the 1980s. However, no research article, except the CO articles, is known to notice this automatic locking global deadlock resolution as of 2009. Such global deadlocks typically have been dealt with by dedicated mechanisms.
# Case 4 above is also an example for a typical voting-deadlock when [[Commitment ordering#Distributed optimistic CO (DOCO)|Distributed optimistic CO (DOCO)]] is used (i.e., Case 4 is unchanged when Optimistic CO (OCO; see below) replaces SCO on both A and B): No data-access blocking occurs, and only materialized conflicts exist.

====Hypothetical Multi Single-Threaded Core (MuSiC) environment====

'''Comment:''' While the examples above describe real, recommended utilization of CO, this example is hypothetical, for demonstration only.

Certain experimental distributed memory-resident databases advocate multi single-threaded core (MuSiC) transactional environments. "Single-threaded" refers to transaction [[Thread (computer science)|threads]] only, and to ''serial'' execution of transactions. The purpose is possible orders of magnitude gain in performance (e.g., [[Michael Stonebraker#H-Store and VoltDB|H-Store]]&lt;ref name=Stone08&gt;Robert Kallman, Hideaki Kimura, Jonathan Natkins, Andrew Pavlo, Alex Rasin, [[Stanley Zdonik]], Evan Jones, Yang Zhang, Samuel Madden, [[Michael Stonebraker]], John Hugg, Daniel Abadi (2008): [http://portal.acm.org/citation.cfm?id=1454211  "H-Store: A High-Performance, Distributed Main Memory Transaction Processing System"], ''Proceedings of the 2008 VLDB'', pages 1496 - 1499, Auckland, New-Zealand, August 2008.&lt;/ref&gt; and [[VoltDB]]) relatively to conventional transaction execution in multiple threads on a same core. In what described below MuSiC is independent of the way the cores are distributed. They may reside in one [[integrated circuit]] (chip), or in many chips, possibly distributed geographically in many computers. In such an environment, if recoverable (transactional) data are partitioned among threads (cores), and it is implemented in the conventional way for distributed CO, as described in previous sections, then DOCO and Strictness exist automatically. However, downsides exist with this straightforward implementation of such environment, and its practicality as a general-purpose solution is questionable. On the other hand, tremendous performance gain can be achieved in applications that can bypass these downsides in most situations.

'''Comment:''' The MuSiC straightforward implementation described here (which uses, for example, as usual in distributed CO, voting (and transaction thread) blocking in atomic commitment protocol when needed) is for demonstration only, and has '''no connection''' to the implementation in H-Store or any other project.

In a MuSiC environment local schedules are ''serial''. Thus both local Optimistic CO (OCO; see below) and the ''Global CO enforcement vote ordering strategy'' condition for the atomic commitment protocol are met automatically. This results in both distributed CO compliance (and thus distributed serializability) and automatic global (voting) deadlock resolution.

Furthermore, also local ''Strictness'' follows automatically in a serial schedule. By Theorem 5.2 in ([[#Raz1992|Raz 1992]]; page  307), when the CO vote ordering strategy is applied, also Global Strictness is guaranteed. Note that ''serial'' locally is the only mode that allows strictness and "optimistic" (no data access blocking) together.

The following is concluded:

* '''The MuSiC Theorem'''
:In MuSiC environments, if recoverable (transactional) data are partitioned among cores (threads), then both
:#''OCO'' (and implied ''Serializability''; i.e., DOCO and Distributed serializability)
:#''Strictness'' (allowing effective recovery; 1 and 2 implying Strict CO—see SCO below) and
:#(voting) ''deadlock resolution''
:automatically exist globally with unbounded scalability in number of cores used.

:'''Comment:''' However, two major downsides, which need special handling, may exist:
#Local sub-transactions of a global transaction are blocked until commit, which makes the respective cores idle. This reduces core utilization substantially, even if scheduling of the local sub-transactions attempts to execute all of them in time proximity, almost together. It can be overcome by detaching execution from commit (with some atomic commitment protocol) for global transactions, at the cost of possible cascading aborts.
#increasing the number of cores for a given amount of recoverable data (database size) decreases the average amount of (partitioned) data per core. This may make some cores idle, while others very busy, depending on data utilization distribution. Also a local (to a core) transaction may become global (multi-core) to reach its needed data, with additional incurred overhead. Thus, as the number of cores increases, the amount and type of data assigned to each core should be balanced according to data usage, so a core is neither overwhelmed to become a bottleneck, nor becoming idle too frequently and underutilized in a busy system. Another consideration is putting in a same core partition all the data that are usually accessed by a same transaction (if possible), to maximize the number of local transactions (and minimize the number of global, distributed transactions). This may be achieved by occasional data re-partition among cores based on load balancing (data access balancing) and patterns of data usage by transactions. Another way to considerably mitigate this downside is by proper physical data replication among some core partitions in a way that read-only global transactions are possibly (depending on usage patterns) completely avoided, and replication changes are synchronized by a dedicated commit mechanism.

==CO variants: Interesting special cases and generalizations==

Special case schedule property classes (e.g., SS2PL and SCO below) are strictly contained in the CO class. The generalizing classes (ECO and MVCO) strictly contain the CO class (i.e., include also schedules that are not CO compliant). The generalizing variants also guarantee global serializability without distributing local concurrency control information (each database has the ''generalized autonomy'' property: it uses only local information), while relaxing CO constraints and utilizing additional (local) information for better concurrency and performance: ECO uses knowledge about transactions being local (i.e., confined to a single database), and MVCO uses availability of data versions values. Like CO, both generalizing variants are ''non-blocking'', do not interfere with any transaction's operation scheduling, and can be seamlessly combined with any relevant concurrency control mechanism.

The term '''CO variant''' refers in general to CO, ECO, MVCO, or a combination of each of them with any relevant concurrency control mechanism or property (including Multi-version based ECO, MVECO). No other interesting generalizing variants (which guarantee global serializability with no local concurrency control information distribution) are known, but may be discovered.

===Strong strict two phase locking (SS2PL)===
{{main|Two-phase locking}}

'''Strong Strict Two Phase Locking''' (SS2PL;  also referred to as ''Rigorousness'' or ''Rigorous scheduling'') means that both read and write locks of a transaction are released only after the transaction has ended (either committed or aborted). The set of SS2PL schedules is a [[proper subset]] of the set of CO schedules.
This property is widely utilized in database systems, and since it implies CO, databases that use it and participate in global transactions generate together a serializable global schedule (when using any atomic commitment protocol, which is needed for atomicity in a multi-database environment). No database modification or addition is needed in this case to participate in a CO distributed solution: The set of undecided transactions to be aborted before committing in the [[Commitment ordering#The algorithm|local generic CO algorithm]] above is empty because of the locks, and hence such an algorithm is unnecessary in this case. A transaction can be voted on by a database system immediately after entering a "ready" state, i.e., completing running its task locally. Its locks are released by the database system only after it is decided by the atomic commitment protocol, and thus the condition in the ''Global CO enforcing theorem'' above is kept automatically. Interestingly, if a local timeout mechanism is used by a database system to resolve (local) SS2PL deadlocks, then aborting blocked transactions breaks not only potential local cycles in the global conflict graph (real cycles in the augmented conflict graph), but also database system's potential global cycles as a side effect, if the [[atomic commitment]] protocol's abort mechanism is relatively slow. Such independent aborts by several entities typically may result in unnecessary aborts for more than one transaction per global cycle. The situation is different for a local ''wait-for graph'' based mechanisms: Such cannot identify global cycles, and the atomic commitment protocol will break the global cycle, if the resulting voting deadlock is not resolved earlier in another database.

Local SS2PL together with atomic commitment implying global serializability can also be deduced directly: All transactions, including distributed, obey the [[Two-phase locking|2PL]] (SS2PL) rules. The atomic commitment protocol mechanism is not needed here for consensus on commit, but rather for the end of phase-two synchronization point. Probably for this reason, without considering the atomic commitment voting mechanism, automatic global deadlock resolution has not been noticed before CO.

===Strict CO (SCO)===

[[Image:SCO-VS-SS2PL.jpg|thumb|450px|'''Read-write conflict: SCO Vs. SS2PL'''. Duration of transaction T2 is longer with SS2PL than with SCO.

SS2PL delays write operation w2[x] of T2 until T1 commits, due to a lock on x by T1 following read operation r1[x]. If t time units are needed for transaction T2 after starting write operation w2[x] in order to reach ready state, than T2 commits t time units after T1 commits. However, SCO does not block w2[x], and T2 can commit immediately after T1 commits. ([[#Raz1991c|Raz 1991c]])]]

'''Strict Commitment Ordering''' (SCO; ([[#Raz1991c|Raz 1991c]])) is the intersection of [[Schedule (computer science)#Strict|strictness]] (a special case of recoverability) and CO, and provides an upper bound for a schedule's concurrency when both properties exist. It can be implemented using blocking mechanisms (locking) similar to those used for the popular SS2PL with similar overheads.

Unlike SS2PL, SCO does not block on a read-write conflict but possibly blocks on commit instead. SCO and SS2PL have identical blocking behavior for the other two conflict types: write-read, and write-write. As a result, SCO has shorter average blocking periods, and more concurrency (e.g., performance simulations of a single database for the most significant variant of ''[[locks with ordered sharing]],'' which is identical to SCO, clearly show this, with approximately 100% gain for some transaction loads; also for identical transaction loads SCO can reach higher transaction rates than SS2PL before ''lock [[Thrashing (computer science)|thrashing]]'' occurs). More concurrency means that with given computing resources more transactions are completed in time unit (higher transaction rate, [[throughput]]), and the average duration of a transaction is shorter (faster completion; see chart). The advantage of SCO is especially significant during lock contention.

*'''The SCO Vs. SS2PL Performance Theorem'''
:SCO provides shorter average transaction completion time than SS2PL, if read-write conflicts exist. SCO and SS2PL are identical otherwise (have identical blocking behavior with write-read and write-write conflicts).

SCO is as practical as SS2PL since as SS2PL it provides besides serializability also strictness, which is widely utilized as a basis for efficient recovery of databases from failure. An SS2PL mechanism can be converted to an SCO one for better performance in a straightforward way without changing recovery methods. A description of a SCO implementation can be found in (Perrizo and Tatarinov 1998).&lt;ref&gt;{{cite conference | first1 = William | last1 = Perrizo | first2 = Igor | last2 = Tatarinov | title =  A Semi-Optimistic Database Scheduler Based on Commit Ordering | citeseerx = 10.1.1.53.7318 | conference = 1998 Int'l Conference on Computer Applications in Industry and Engineering | pages = 75–79 | location = Las Vegas | date = November 11, 1998 }}&lt;/ref&gt; See also  ''[[The History of Commitment Ordering#Semi-optimistic database scheduler|Semi-optimistic database scheduler]]''.

SS2PL is a proper subset of SCO (which is another explanation why SCO is less constraining and provides more concurrency than SS2PL).

===Optimistic CO (OCO)===

For implementing '''Optimistic commitment ordering''' (OCO) the generic local CO algorithm is utilized without data access blocking, and thus without local deadlocks. OCO without transaction or operation scheduling constraints covers the entire CO class, and is not a special case of the CO class, but rather a useful CO variant and mechanism characterization.

===Extended CO (ECO)===

====General characterization of ECO====

'''Extended Commitment Ordering''' (ECO; ([[#Raz1993a|Raz 1993a]])) generalizes CO. When local transactions (transactions confined to a single database) can be distinguished from global (distributed) transactions (transactions that span two databases or more), commitment order is applied to global transactions only. Thus, for a local (to a database) schedule to have the ECO property, the chronological (partial) order of commit events of global transactions only (unimportant for local transactions) is consistent with their order on the respective local conflict graph.

*'''Definition: extended commitment ordering'''

:Let &lt;math&gt;T_{1}, T_{2}&lt;/math&gt; be two committed ''global'' transactions in a schedule, such that a ''directed path'' of unaborted transactions exists in the ''conflict graph'' ([[precedence graph]]) from &lt;math&gt;T_{1}&lt;/math&gt; to &lt;math&gt;T_{2}&lt;/math&gt; (&lt;math&gt;T_{1}&lt;/math&gt; precedes &lt;math&gt;T_{2}&lt;/math&gt;, possibly [[transitive relation|transitively]], indirectly). The schedule has the '''Extended commitment ordering''' (ECO) property, if for every two such transactions &lt;math&gt;T_{1}&lt;/math&gt; commits before &lt;math&gt;T_{2}&lt;/math&gt; commits.

A distributed algorithm to guarantee global ECO exists. As for CO, the algorithm needs only (unmodified) atomic commitment protocol messages. In order to guarantee global serializability, each database needs to guarantee also the conflict serializability of its own transactions by any (local) concurrency control mechanism.

* '''The ECO and Global Serializability Theorem'''

#(Local, which implies global) ECO together with local conflict serializability, is a sufficient condition to guarantee global conflict serializability.
#When no concurrency control information beyond atomic commitment messages is shared outside a database (autonomy), and local transactions can be identified, it is also a necessary condition.

:See a necessity proof in ([[#Raz1993a|Raz 1993a]]).

This condition (ECO with local serializability) is weaker than CO, and allows more concurrency at the cost of a little more complicated local algorithm (however, no practical overhead difference with CO exists).

When all the transactions are assumed to be global (e.g., if no information is available about transactions being local), ECO reduces to CO.

====The ECO algorithm====

Before a global transaction is committed, a generic local (to a database) ECO algorithm aborts a minimal set of undecided transactions (neither committed, nor aborted; either local transactions, or global that run locally), that can cause later a cycle in the conflict graph. This set of aborted transactions (not unique, contrary to CO) can be optimized, if each transaction is assigned with a weight (that can be determined by transaction's importance and by the computing resources already invested in the running transaction; optimization can be carried out, for example, by a reduction from the ''[[Max flow in networks]]'' problem ([[#Raz1993a|Raz 1993a]])). Like for CO such a set is time dependent, and becomes empty eventually. Practically, almost in all needed implementations a transaction should be committed only when the set is empty (and no set optimization is applicable). The local (to the database) concurrency control mechanism (separate from the ECO algorithm) ensures that local cycles are eliminated (unlike with CO, which implies serializability by itself; however, practically also for CO a local concurrency mechanism is utilized, at least to ensure Recoverability). Local transactions can be always committed concurrently (even if a precedence relation exists, unlike CO). When the overall transactions' local partial order (which is determined by the local conflict graph, now only with possible temporary local cycles, since cycles are eliminated by a local serializability mechanism) allows, also global transactions can be voted on to be committed concurrently (when all their transitively (indirect) preceding (via conflict) ''global'' transactions are committed, while transitively preceding local transactions can be at any state. This in analogy to the distributed CO algorithm's stronger concurrent voting condition, where all the transitively preceding transactions need to be committed).

The condition for guaranteeing ''Global ECO'' can be summarized similarly to CO:

*'''The Global ECO Enforcing Vote ordering strategy Theorem'''

:Let &lt;math&gt;T_{1}, T_{2}&lt;/math&gt; be undecided (neither committed nor aborted) ''global transactions'' in a database system that ensures serializability locally, such that a ''directed path'' of unaborted transactions exists in the ''local conflict graph'' (that of the database itself) from &lt;math&gt;T_{1}&lt;/math&gt; to &lt;math&gt;T_{2}&lt;/math&gt;. Then, having &lt;math&gt;T_{1}&lt;/math&gt; ended (either committed or aborted) before &lt;math&gt;T_{2}&lt;/math&gt; is voted on to be committed, in every such database system in a multidatabase environment, is a [[necessary and sufficient condition]] for guaranteeing Global ECO (the condition guarantees Global ECO, which may be violated without it).

Global ECO (all global cycles in the global conflict graph are eliminated by atomic commitment) together with Local serializability (i.e., each database system maintains serializability locally; all local cycles are eliminated) imply Global serializability (all cycles are eliminated). This means that if each database system in a multidatabase environment provides local serializability (by ''any'' mechanism) and enforces the ''vote ordering strategy'' in the theorem above (a generalization of CO's vote ordering strategy), then ''Global serializability'' is guaranteed (no local CO is needed anymore).

Similarly to CO as well, the ECO ''voting-deadlock'' situation can be summarized as follows:

*'''The ECO Voting-Deadlock Theorem'''

:Let a multidatabase environment comprise database systems that enforce, each, both ''Global ECO'' (using the condition in the theorem above) and ''local conflict serializability'' (which eliminates local cycles in the global conflict graph). Then, a ''voting-deadlock'' occurs if and only if a ''global cycle'' (spans two or more databases) exists in the ''Global augmented conflict graph'' (also blocking by a data-access lock is represented by an edge). If the cycle does not break by any abort, then all the ''global transactions'' on it are involved with the respective voting-deadlock, and eventually each has its vote blocked (either directly, or indirectly by a data-access lock). If a local transaction resides on the cycle, it may be in any unaborted state (running, ready, or committed; unlike CO no local commit blocking is needed).

As with CO this means that also global deadlocks due to data-access locking (with at least one lock blocking) are voting deadlocks, and are automatically resolved by atomic commitment.

===Multi-version CO (MVCO)===

'''Multi-version Commitment Ordering''' (MVCO; ([[#Raz1993b|Raz 1993b]])) is a generalization of CO for databases with [[Multiversion concurrency control|multi-version resources]]. With such resources ''read-only transactions'' do not block or being blocked for better performance. Utilizing such resources is a common way nowadays to increase concurrency and performance by generating a new version of a database object each time the object is written, and allowing transactions' read operations of several last relevant versions (of each object). MVCO implies ''One-copy-serializability'' (1SER or 1SR) which is the generalization of [[serializability]] for multi-version resources. Like CO, MVCO is non-blocking, and can be combined with any relevant multi-version concurrency control mechanism without interfering with it. In the introduced underlying theory for MVCO conflicts are generalized for different versions of a same resource (differently from earlier multi-version theories). For different versions conflict chronological order is replaced by version order, and possibly reversed, while keeping the usual definitions for conflicting operations. Results for the regular and augmented conflict graphs remain unchanged, and similarly to CO a distributed MVCO enforcing algorithm exists, now for a mixed environment with both single-version and multi-version resources (now single-version is a special case of multi-version). As for CO, the MVCO algorithm needs only (unmodified) [[atomic commitment]] protocol messages with no additional communication overhead. Locking-based global deadlocks translate to voting deadlocks and are resolved automatically. In analogy to CO the following holds:

*'''The MVCO and Global one-copy-serializability Theorem'''

#MVCO compliance of every ''autonomous'' database system (or transactional object) in a mixed multidatabase environment of single-version and multi-version databases is a ''necessary condition'' for guaranteeing Global one-copy-serializability (1SER).
#MVCO compliance of every database system is a ''sufficient condition'' for guaranteeing Global 1SER.
#Locking-based global deadlocks are resolved automatically.

:'''Comment''': Now a CO compliant single-version database system is automatically also MVCO compliant.

MVCO can be further generalized to employ the generalization of ECO (MVECO).

====Example: CO based snapshot isolation (COSI)====

'''CO based snapshot isolation''' (COSI) is the intersection of ''[[Snapshot isolation]]'' (SI) with MVCO. SI is a [[multiversion concurrency control]] method widely utilized due to good performance and similarity to serializability (1SER) in several aspects. The theory in (Raz 1993b) for MVCO described above is utilized later in (Fekete et al. 2005) and other articles on SI, e.g., (Cahill et al. 2008);&lt;ref name=Cahill08&gt;Michael J. Cahill, Uwe Röhm, Alan D. Fekete (2008): [http://portal.acm.org/citation.cfm?id=1376690  "Serializable isolation for snapshot databases"], ''Proceedings of the 2008 ACM SIGMOD international conference on Management of data'', pp. 729-738, Vancouver, Canada, June 2008, ISBN 978-1-60558-102-6 (SIGMOD 2008 best paper award&lt;/ref&gt; see also [[Snapshot isolation#Making Snapshot Isolation Serializable|Making snapshot isolation serializable]] and the references there), for analyzing conflicts in SI in order to make it serializable. The method presented in (Cahill et al. 2008), ''Serializable snapshot isolation'' (SerializableSI), a low overhead modification of SI, provides good performance results versus SI, with only small penalty for enforcing serializability. A different method, by combining SI with MVCO (COSI), makes SI serializable as well, with a relatively low overhead, similarly to combining the generic CO algorithm with single-version mechanisms. Furthermore, the resulting combination, COSI, being MVCO compliant, allows COSI compliant database systems to inter-operate and transparently participate in a CO solution for distributed/global serializability (see below). Besides overheads also protocols' behaviors need to be compared quantitatively. On one hand, all serializable SI schedules can be made MVCO by COSI (by possible commit delays when needed) without aborting transactions. On the other hand, SerializableSI is known to unnecessarily abort and restart certain percentages of transactions also in serializable SI schedules.

===CO and its variants are transparently interoperable for global serializability===

With CO and its variants (e.g., SS2PL, SCO, OCO, ECO, and MVCO above) global serializability is achieved via ''atomic commitment'' protocol based distributed algorithms. For CO and all its variants atomic commitment protocol is the instrument to eliminate global cycles (cycles that span two or more databases) in the ''global augmented'' (and thus also regular) ''conflict graph'' (implicitly; no global data structure implementation is needed). In cases of either incompatible local commitment orders in two or more databases (when no global [[partial order]] can [[Embedding|embed]] the respective local partial orders together), or a data-access locking related voting deadlock, both implying a global cycle in the global augmented conflict graph and missing votes, the atomic commitment protocol breaks such cycle by aborting an undecided transaction on it (see [[commitment ordering#The distributed CO algorithm|The distributed CO algorithm]] above). Differences between the various variants exist at the local level only (within the participating database systems). Each local CO instance of any variant has the same role, to determine the position of every global transaction (a transaction that spans two or more databases) within the local commitment order, i.e., to determine when it is the transaction's turn to be voted on locally in the atomic commitment protocol. Thus, all the CO variants exhibit the same behavior in regard to atomic commitment. This means that they are all interoperable via atomic commitment (using the same software interfaces, typically provided as [[Service (systems architecture)|service]]s, some already [[international standard|standardized]] for atomic commitment, primarily for the [[two phase commit]] protocol, e.g., [[X/Open XA]]) and transparently can be utilized together in any distributed environment (while each CO variant instance is possibly associated with any relevant local concurrency control mechanism type).

In summary, any single global transaction can participate simultaneously in databases that may employ each any, possibly different, CO variant (while concurrently running processes in each such database, and running concurrently with local and other global transactions in each such database). The atomic commitment protocol is indifferent to CO, and does not distinguish between the various CO variants. Any ''global cycle'' generated in the augmented global conflict graph may span databases of different CO variants, and generate (if not broken by any local abort) a voting deadlock that is resolved by atomic commitment exactly the same way as in a single CO variant environment. ''local cycles'' (now possibly with mixed materialized and non-materialized conflicts, both serializability and data-access-locking deadlock related, e.g., SCO) are resolved locally (each by its respective variant instance's own local mechanisms).

'''Vote ordering''' (VO or Generalized CO (GCO); [[#Raz2009|Raz 2009]]), the union of CO and all its above variants, is a useful concept and global serializability technique. To comply with VO, local serializability (in it most general form, commutativity based, and including multi-versioning) and the ''vote order strategy'' (voting by local precedence order) are needed.

Combining results for CO and its variants, the following is concluded:

*'''The CO Variants Interoperability Theorem'''
#In a multi-database environment, where each database system (transactional object) is compliant with some CO variant property (VO compliant), any global transaction can participate simultaneously in databases of possibly different CO variants, and Global serializability is guaranteed (''sufficient condition'' for Global serializability; and Global one-copy-serializability (1SER), for a case when a multi-version database exists).
#If only local (to a database system) concurrency control information is utilized by every database system (each has the ''generalized autonomy'' property, a generalization of ''autonomy''), then compliance of each with some (any) CO variant property (VO compliance) is a ''necessary condition'' for guaranteeing Global serializability (and Global 1SER; otherwise they may be violated).
#Furthermore, in such environment data-access-locking related global deadlocks are resolved automatically (each such deadlock is generated by a global cycle in the ''augmented conflict graph'' (i.e., a ''voting deadlock''; see above), involving at least one data-access lock (non-materialized conflict) and two database systems; thus, not a cycle in the regular conflict graph and does not affect serializability).

==References==

*{{citation|first=Yoav|last=Raz|url=http://www.vldb.org/conf/1992/P292.PDF|title=The Principle of Commitment Ordering, or Guaranteeing Serializability in a Heterogeneous Environment of Multiple Autonomous Resource Managers Using Atomic Commitment|work=Proceedings of the Eighteenth International Conference on Very Large Data Bases|pages=292–312|place=Vancouver, Canada|date=August 1992}} (also DEC-TR 841, [[Digital Equipment Corporation]], November 1990)
*{{citation|first=Yoav|last=Raz|title=Serializability by Commitment Ordering|work=Information Processing Letters|volume=51|number=5|pages=257–264|date=September 1994|doi=10.1016/0020-0190(94)90005-1}}
*{{citation|first=Yoav|last=Raz|url=http://sites.google.com/site/yoavraz2/home/theory-of-commitment-ordering|title=Theory of Commitment Ordering: Summary|date=June 2009|accessdate=November 11, 2011}}
*{{citation|first=Yoav|last=Raz|url=http://yoavraz.googlepages.com/DEC-CO-MEMO-90-11-16.pdf|title=On the Significance of Commitment Ordering|publisher=Digital Equipment Corporation|date=November 1990}}
*&lt;cite id=Raz1991a&gt;Yoav Raz (1991a): US patents [http://patft1.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=3&amp;f=G&amp;l=50&amp;co1=AND&amp;d=PTXT&amp;s1=%22commitment+ordering%22.TI.&amp;OS=TTL/ 5,504,899 (ECO)] [http://patft1.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=2&amp;f=G&amp;l=50&amp;co1=AND&amp;d=PTXT&amp;s1=%22commitment+ordering%22.TI.&amp;OS=TTL/ 5,504,900 (CO)]  [http://patft1.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;co1=AND&amp;d=PTXT&amp;s1=%22commitment+ordering%22.TI.&amp;OS=TTL/ 5,701,480 (MVCO)] &lt;/cite&gt;
*&lt;cite id=Raz1991b&gt;Yoav Raz (1991b): "The Commitment Order Coordinator (COCO) of a Resource Manager, or Architecture for Distributed Commitment Ordering Based Concurrency Control", DEC-TR 843, Digital Equipment Corporation, December 1991. &lt;/cite&gt;
*&lt;cite id=Raz1991c&gt;Yoav Raz (1991c): "Locking Based Strict Commitment Ordering, or How to improve Concurrency in Locking Based Resource Managers", DEC-TR 844, December 1991. &lt;/cite&gt;
*&lt;cite id=Raz1993a&gt;Yoav Raz (1993a): [http://portal.acm.org/citation.cfm?id=153858 "Extended Commitment Ordering or Guaranteeing Global Serializability by Applying Commitment Order Selectivity to Global Transactions."] ''Proceedings of the Twelfth ACM Symposium on Principles of Database Systems'' (PODS), Washington, DC, pp. 83-96, May 1993. (also DEC-TR 842, November 1991) &lt;/cite&gt;
*&lt;cite id=Raz1993b&gt;Yoav Raz (1993b): [http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=281924  "Commitment Ordering Based Distributed Concurrency Control for Bridging Single and Multi Version Resources."] ''Proceedings of the Third IEEE International Workshop on Research Issues on Data Engineering: Interoperability in Multidatabase Systems'' (RIDE-IMS), Vienna, Austria, pp. 189-198, April 1993. (also DEC-TR 853, July 1992)  &lt;/cite&gt;

==Footnotes==
{{reflist}}

==External links==
*[http://sites.google.com/site/yoavraz2/the_principle_of_co Yoav Raz's Commitment ordering page]

{{DEFAULTSORT:Commitment Ordering}}
[[Category:Data management]]
[[Category:Databases]]
[[Category:Transaction processing]]
[[Category:Concurrency control]]
[[Category:Distributed algorithms]]</text>
      <sha1>6gdw5whuoe507ttnv5ly7xfzxoqbyp2</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Storage area networks</title>
    <ns>14</ns>
    <id>30304657</id>
    <revision>
      <id>734498895</id>
      <parentid>588991844</parentid>
      <timestamp>2016-08-14T19:27:50Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor />
      <comment>Robot - Speedily moving category Computer storage to [[:Category:Computer data storage]] per [[WP:CFDS|CFDS]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="255" xml:space="preserve">{{Commons category|Storage area networks}}
{{See also category|Computer storage buses}}
{{cat main|Storage area network}}

[[Category:Computer data storage]]
[[Category:Local area networks]]
[[Category:Data management]]
[[Category:Storage virtualization]]</text>
      <sha1>q4bq0jgbdtwjasu3nufy7cov6mwuklc</sha1>
    </revision>
  </page>
  <page>
    <title>Operational historian</title>
    <ns>0</ns>
    <id>8829912</id>
    <revision>
      <id>761769230</id>
      <parentid>749378592</parentid>
      <timestamp>2017-01-24T18:41:35Z</timestamp>
      <contributor>
        <ip>106.220.132.175</ip>
      </contributor>
      <comment>/* Commercial */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7056" xml:space="preserve">{{Refimprove|date=October 2013}}
{{Use British English|date=March 2014}}
'''Operational historian''' refers to a database software application that logs or historizes time-based process data.&lt;ref&gt;{{cite web|title=A Practical Guide to Process Data Historians and Process Information Systems|url=http://www.tappi.org/Downloads/unsorted/UNTITLED---PCEI99339pdf.aspx|publisher=TAPPI|accessdate=14 September 2012|author=R. H. (Rick) Meeker, Jr.|format=PDF|date=13 January 1999}}&lt;/ref&gt; Historian software is used to record trends and historical information about industrial processes for future reference. It captures plant management information about production status, performance monitoring, quality assurance, tracking and genealogy, and product delivery with enhanced data capture, data compression, and data presentation capabilities.&lt;ref name=GlobalspecArticle  &gt;{{cite web |url= http://www.globalspec.com/learnmore/industrial_engineering_software/industrial_controls_software/trending_historian_software |title= Globalspec Historian Article| accessdate=12 Jul 2012}}&lt;/ref&gt;

Operational historians are like enterprise historians but differ in that they are used by engineers on the plant floor rather than by business processes.{{citation needed|date=February 2015}} They are typically cheaper, lighter in weight, and easier to use and reconfigure than enterprise historians. Having an operational historian enables "at the source" analysis of the historical data that is not typically possible with enterprise historians.
Typically, these applications offer two layers of data access: through a dedicated SDK(Standard Development Kit)(sometimes in two different flavours: full administration API(Application Programming Interface) and high-speed read/write API), as well as user front-end tools (for instance, administration panels, engineering consoles or portal-like web clients).

Because these applications are designed to fulfil specific operation time requirements, their marketing materials often indicate that these are real-time database systems.&lt;ref name="example"&gt;[http://software.schneider-electric.com/products/wonderware/production-information-management/historian/ Wonderware Historian - Example of naming the operational historian the real-time database]&lt;/ref&gt; However, since such performance measurements are often executed for atomic operations (especially write operations), not necessarily whole transactions, not all of the operational historians must be in fact real-time databases.

Usual challenges the operational historians must address are as follows:
* data collection from real-time external systems,
* storage and archiving of very large volumes of data,
* tag organisation (typically [[Time series database|time series]], where a single sample contains the information about the time stamp, the value and the sample quality),
* basic data limit monitoring (alarms) and user prompts (messages),
* performance of read and write operations.

== Data access ==
As opposed to enterprise historians, the data access layer in the operational historian is designed to offer sophisticated data fetching modes without complex information analysis facilities. The following settings are typically available for data access operations:
* Data scope (single point, history based on time range, history based on sample count),
* Request modes (raw data, last-known value, aggregation, interpolation),
* Sampling (single point, all points without sampling, all points with interval sampling),
* Data omission (based on the sample quality, based on the sample value, based on the count).

Even though the operational historians are rarely [[relational database management system]]s, they often offer [[SQL]]-based interfaces to query the database. In most of such implementations, the dialect does not follow the SQL standard in order to provide syntax for specifying data access operations parameters.

== Notable software ==
=== Commercial ===
* [[ABB]] Decathlon History
* [[Aspen Technology]] InfoPlus.21 &lt;ref&gt;{{Citation | url = http://www.aspentech.com/aspenONE_MES_Brochure.pdf | format = PDF | publisher = Aspen Technology, Inc. | title = aspenONE MES Brochure | page = 2 | year = 2014 | accessdate = 30 July 2014 }}&lt;/ref&gt;
* [[ENEA_AB|Enea]]'s Polyhedra Historian, a module of [[Polyhedra DBMS]]&lt;ref&gt;{{Citation | url = http://developer.polyhedra.com/polyhedra-features/historian | publisher = Enea AB |  title = Handling time-series data in Polyhedra IMDB | date = 11 May 2012 | accessdate = 30 July 2014 }}&lt;/ref&gt;
* [[GE Intelligent Platforms]] Proficy Historian&lt;ref&gt;{{Citation | url = http://www.ge-ip.com/account/prepsend/file/Proficy_Historian_5-5.pdf | format = PDF | publisher = GE Intelligent Platforms, Inc. | title = Datasheet: Proficy Historian 5.5 | year = 2013 | accessdate = 30 July 2014 }}&lt;/ref&gt;
* [[Honeywell]] Uniformance PHD&lt;ref&gt;{{Citation | url = http://www.honeywellprocess.com/library/marketing/notes/uniformance-phd-pin.pdf | publisher = Honeywell International Inc. | title = Uniformance PHD Product Information Note | format = PDF | year = 2013 | accessdate = 30 July 2014 }}&lt;/ref&gt;
* [[Iconics ]] [http://www.iconics.com/Home/Products/Historians/Hyper-Historian.aspx Hyper Historian]
* [[Inductive Automation]] [[Ignition SCADA#SQL Bridge| SQL Bridge]] module of [[Ignition SCADA]]&lt;ref&gt;{{Citation | url = http://inductiveautomation.com/scada-software/scada-modules/sqlbridge | publisher = Inductive Automation | title = High-Powered Data Acquisition | year = 2014 | accessdate = 30 July 2014 }}&lt;/ref&gt;
* [[National Instruments]] Citadel, used in [[LabVIEW]] DSC and other products&lt;ref&gt;{{Citation | url = http://www.ni.com/white-paper/6579/en/ | publisher = National Instruments Corp. |  title = Logging Data with National Instruments Citadel | date = 19 July 2012 | accessdate = 30 July 2014 }}&lt;/ref&gt;
* [[OSIsoft]] - PI System
* [[Schneider Electric]] InStep Software [http://www.instepsoftware.com/instep-software-products/edna-enterprise-data-historian eDNA Real-Time Historian]
* [[Schneider Electric]] Wonderware Historian&lt;ref&gt;{{Citation | url =  http://global.wonderware.com/EN/PDF%20Library/Datasheet_Wonderware_Historian.pdf | publisher = Invensys Systems, Inc. | title = Wonderware Historian Software Datasheet | format = PDF | year = 2013 | accessdate = 30 July 2014 }}&lt;/ref&gt;
* [[Yokogawa]] Exaquantum Historian&lt;ref&gt;{{Citation | url = http://www.yokogawa.com/eu/pims/pdf/BU%20GMSCS0102-02E%20Exaquantum%20Bulletin%2072ppi.pdf | publisher = Yokogawa Marex Limited |  title = Exaquantum delivers Production Excellence | format = PDF | year = 2013 | accessdate = 30 July 2014 }}&lt;/ref&gt;
* [[Jaaji Technologies]] inSis Historian&lt;ref&gt;{{Citation | url = http://www.jaajitech.com/Infoview | publisher = Jaaji Software Technologies Private Limited |  title = Find, View and Analyze your process data from everywhere | year = 2014 | accessdate = 30 July 2014 }}&lt;/ref&gt;

==See also==
* [[Time series database]]
* [[Relational database management system]]

== References ==
{{reflist}}

[[Category:Data management]]</text>
      <sha1>nn6i2b58bfobkmfkg00n5tsziuarvov</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Database theory</title>
    <ns>14</ns>
    <id>10221974</id>
    <revision>
      <id>547489250</id>
      <parentid>460799758</parentid>
      <timestamp>2013-03-28T19:13:37Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor />
      <comment>[[User:Addbot|Bot:]] Migrating 5 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q8363916]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="134" xml:space="preserve">{{Cat main|database theory}}

[[Category:Areas of computer science]]
[[Category:Databases|Theory]]
[[Category:Data management|Theory]]</text>
      <sha1>n7vxpzshmvizcohpoadxlv240xz1rfp</sha1>
    </revision>
  </page>
  <page>
    <title>Technical data management system</title>
    <ns>0</ns>
    <id>20092666</id>
    <revision>
      <id>754896932</id>
      <parentid>753285847</parentid>
      <timestamp>2016-12-15T02:58:47Z</timestamp>
      <contributor>
        <username>Just a guy from the KP</username>
        <id>9994896</id>
      </contributor>
      <minor />
      <comment>softwares → software</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="22913" xml:space="preserve">{{Orphan|date=February 2009}}
A '''Technical Data Management System''' (TDMS) is essentially a [[Document management system]] (DMS) pertaining to the management of technical and [[engineering drawing]]s and documents. Often the data are contained in 'records' of various forms, such as on paper, microfilms or digital media. Hence technical [[data management]] is also concerned with record management involving technical data. Proper Technical Document [[Management system|Management Systems]] are essential for executions within large organisations with large scale projects involving engineering. For example, TDMS is a vital function for the successful management of Integrated Steel Plants (ISP), Automobile factories, Aero-space facilities, Infrastructure companies, City Corporations, Research Organisations, etc. In such organisations, Technical Archives or Technical Documentation Centres are created as central facilities for effective management of technical data and records.
[[File:Information processing system (english).svg|alt= A simplified example of information flow within a Technical Data Management System|thumb|A simplified example of information flow within a Technical Data Management System]]
TDMS functions are similar to that of conventional archive functions in concepts, except that the archived materials in this case are essentially engineering drawings, survey maps, [[Specification|technical specifications]], plant and equipment data sheets, feasibility reports, project reports, operation and maintenance manuals, standards, etc.

Document registration, indexing, repository management, reprography, etc. are parts of TDMS.  Various kinds of sophisticated technologies such as document scanners, microfilming and digitization camera units, wide format printers, digital plotters, software, etc. are available now, making TDMS functions an easier process than previous times.

== Crucial Constituents of a Technical Data Management System ==
Technical data refers to both scientific and technical information recorded and presented in any form or manner (excluding financial and management information).&lt;ref&gt;{{Cite web|url = http://www.businessdictionary.com/definition/technical-data.html|title = What is technical data? Definition and meaning|date = 2015-11-03|accessdate = 2015-11-03|website = BusinessDictionary.com|publisher = WebFinance, Inc|last = |first = }}&lt;/ref&gt; A Technical Data Management System is created within an organisation for archiving and sharing information such as [[technical specifications]], datasheets and drawings. Similar to other types of data management system, a Technical Data Management System consists of the 4 crucial constituents mentioned below.

=== Data planning ===
Data plans (long-term or short-term) are constructed as the first essential step of a proper and complete TDMS. It is created to ultimately help with the 3 other constituents, Data Acquisition, Data Management and Data sharing. A proper data plan should not exceed 2 pages and should address the following basics:&lt;ref&gt;{{Cite web|url = https://www.libraries.psu.edu/psul/pubcur/what_is_dm.html#data-planning|title = Data planning|date = 2015-11-03|accessdate = 2015-11-03|website = Data Curation|publisher = Penn State University Libraries|last = |first = }}&lt;/ref&gt;
* Types of data (samples, experiment results, reports, drawings, etc.) and [[Metadata]] (Data that summarizes and describes other data. In this case, it refers to details such as sample sizes, experiment conditions and procedures, dates of reports, explanations of drawings, etc.)&lt;ref&gt;{{Cite web|url = http://whatis.techtarget.com/definition/metadata|title = metadata|date = July 2014|accessdate = 2015-11-03|website = WhatIs.com|publisher = Search engine optimization (SEO)|last = Rouse|first = Margaret}}&lt;/ref&gt; 
* Means of researches and collections of data (field works, experiments in production lines, etc.)
* Costs of researches
* Policies for access, sharing (re-use within the organisation and re-distribution to the public)
* Proposals for archiving data and maintaining access to it

=== Data Acquisition ===
Raw Data is collected from Primary Sites of the organisations through the use of modern Technologies.&lt;ref name=":0"&gt;{{Cite web|url = http://sine.ni.com/cs/app/doc/p/id/cs-13019#|title = By using powerful default components, TDM, NI DataFinder, and DIAdem, and without using a database, we considerably reduced our creation and maintenance costs.|date = 2015-11-03|accessdate = 2015-11-03|website = National Instruments|publisher = a-solution GmbH|last = Finkl|first = Karl}}&lt;/ref&gt; Please reference the table below for examples.&lt;ref name=":0" /&gt;
{| class="wikitable"
!Organisations
!Raw Data
!Primary Sites
!Technologies
|-
|Integrated steel plants, Automobile factories
|Feasibility reports, Equipment datasheets, etc.
|Test rigs and Controls
|Transiting software to digitize data and Input software for recording report results and details on datasheets
|-
|Aero-space facilities
|Engineering drawings, Operation manuals, maintenance logs, etc.
|Engineering labs
|Scanners for engineering drawings, Input software for maintenance logs
|-
|City corporations
|Survey maps, Population reports, etc.
|City to be mapped and City that involves the research
|Digital cameras for survey maps, Input software for statistics of population
|}
The data collected is then transferred to Technical Data Centres for Data Management.

=== Data Management ===
After Data Acquisition, data is sorted out, whilst useful data is archived, unwanted data is disposed. When managing and archiving data, the features below of the data are considered.&lt;ref&gt;{{Cite web|url = https://www.libraries.psu.edu/psul/pubcur/what_is_dm.html#data-management|title = Data Management|date = 2015-11-03|accessdate = 2015-11-03|website = Data Curation|publisher = Penn State University Libraries|last = |first = }}&lt;/ref&gt;
* Names, labels, values and descriptions for variables and records. (In the case of TDMS, one example is names of equipments on an equipment datasheet)
* Derived data from the original data, with code, algorithm or command file used to create them. (In the case of TDMS, one example is an expectation report derived from the analysis of an equipment datasheet)
* [[Metadata]] associates with the data being archived

=== Data Sharing ===
Archived and managed data are accessible to rightful entities. A proper and complete TDMS should share data to a suitable extent, under suitable security, in order to achieve optimal usage of data within the organisation. It aims for easy access when reused by other researchers and hence it enhances other research processes. Data is often referred in other tests and [[Specification (technical standard)|technical specifications]], where new analysis is generated, managed and archived again. As a result, data is flowing within the organisation under effective management through the use of TDMS.&lt;ref&gt;{{Cite web|url = https://www.libraries.psu.edu/psul/pubcur/what_is_dm.html#data-sharing|title = Data Sharing|date = 2015-11-03|accessdate = 2015-11-03|website = Data Curation|publisher = Penn State University Libraries|last = |first = }}&lt;/ref&gt;

== Advantages and disadvantages of usage of Technical Data Management Systems ==
There are strengths and weakness when using Technical Data Management Systems (TDMS) to archive data. Some of the advantages and disadvantages are listed below.&lt;ref&gt;{{Cite web|url = https://razorleaf.com/solutions/technologies/product-data-management/|title = Product Data Management / Technical Data Management (PDM/TDM)|date = 2015-11-03|accessdate = 2015-11-03|website = Razorleaf Solutions|publisher = Razorleaf Corporation|last = |first = }}&lt;/ref&gt;&lt;ref&gt;{{Cite web|url = http://arxiv.org/ftp/arxiv/papers/1008/1008.1321.pdf|title = Contributions of PDM Systems in Organiza- tional Technical Data Management|date = 2015-11-03|accessdate = 2015-11-03|website = |publisher = Mechanical Engineering Informatics and Virtual Product Development Division (MIVP),  Vienna University of Technology|last1 = Ahmed|first1 = Zeeshan|last2 = Gerhard|first2 = Detlef}}&lt;/ref&gt;&lt;ref&gt;{{Cite web|url = http://www.flosim.com/calcium.aspx|title = Calcium - technical data management|date = 2015-11-03|accessdate = 2015-11-03|website = Flow Simulation|publisher = Flow Simulation Ltd.|last = |first = }}&lt;/ref&gt;

=== Advantages ===

==== 1. Faster and easier data management ====

Since TDMS is integrated into the organisation's systems, whenever workers develop data files (SolidWorks, AutoCAD, Microsoft Word, etc.), they can also archive and manage data, linking what they need to their current work, at the same time they can also update the archives with useful data. This speeds up working processes and makes them more efficient.

==== 2. Increased security ====

All data files are centralized, hence internal and external data leakages are less likely to happen, and the data flow is more closely monitored. As a result, data in the organisation is more secured.

==== 3. Increased collaboration within the organisation ====

Since the data files are centralized and the data flow within the organisation increases, researchers and workers within the organisation are able to work on joint projects. More complex tasks can be performed for higher yields.

==== 4. Compatible to various formats of data ====

TDMS is compatible to many formats of data, from basic data like Microsoft Words to complex data like voice data. This enhances the quality of the management of data archived.

=== Disadvantages ===

==== 1. Higher financial costs ====

Implementing TDMS into the organisation's systems involves monetary costs. Maintenance costs certain amount of human resources and money as well. These resources involve opportunity costs as they can be utilized in other aspects.

==== 2. Lower stability ====

Since TDMS manages and centralizes all the data the organisation processes, it links the working processes within the whole organisation together. It also increases the vulnerability of the organisation data network. If TDMS is not stable enough or when it is exposed to hacker and virus attacks, the organisation's data flow might shut down completely, affecting the work in an organisation-wide scale and leading to a lower stability as results.

== Comparison between Traditional Data Management Approaches and Technical Data Management Systems ==
Test engineers and researchers are facing great challenges in turning complex test results and simulation data into usable information for higher yields of firms. These challenges are listed below.&lt;ref&gt;{{Cite web|url = http://www.ni.com/white-paper/7389/en/|title = From Raw Data to Engineering Results: The NI Technical Data Management Solution|date = 2015-10-13|accessdate = 2015-11-03|website = |publisher = National Instruments|last = |first = }}&lt;/ref&gt;
* Increase in complication of designs
* Reduced in time and budgets available
* Higher quality is demanded
[[File:Logo oracle.jpg|alt= A company logo for Oracle|thumb|A company logo for Oracle]]

=== Traditional Data Management Approaches ===
Many organisations are still applying the conventional file management systems, due to the difficulty in building a proper and complete archives for data management.

The first approach is the simple file-folder system. This costs the problem of ineffectiveness as workers and researchers have to manually go through numerous layers of systems and files for the target data. Moreover, the target data may contain files with different formats and these files may not be stored in the same machine. These files are also easily lost if renamed or moved to another location.

The second approach is conventional databases such as Oracle. These databases are capable of enabling easy search and access of data. However, a great drawback is that huge effort for preparing and modeling the data is required. For large-scale projects, huge monetary costs are induced, and extra IT human resources must be employed for constant handling, expanding and maintaining the inflexible system, which is custom for specific tasks, instead of all tasks. In the long-term, it is not cost-effective.

=== Technical Data Management Systems(TDMS) ===
TDMS is developed based on 3 principles, flexible and organized file storage, self-scaling hybrid data index, and an interactive post-processing environment. The system in practical, mainly consists of 3 components, data files with essential and relevant [[Metadata]], data finders for organizing and managing data regardless of files formats, and, a software of searching, analyzing and reporting. With [[Metadata]] attached to original data files, the data finder can identify different related data files during searches, even if they are in different file formats. TDMS hence allows researchers to search for data like browsing the Internet. Last but not least, it can adapt to changes and update itself according to the changes, unlike databases.

== Comparison between Strong Information Systems and Weak Information Systems ==
Complex organizations may need large amounts of technical information, which can be distributed among several independent archives. Existing approaches span from “no integration” to “strong integration”, that is based on a common database or product model. The so-called “Weak Information Systems” (WIS)&lt;ref&gt;{{cite conference |url=http://www.marcolazzari.it/publications/weak-information-systems-for-technical-data-management-preprint.pdf |title=Weak information systems for technical data management |first= |last1=Salvaneschi |first1=Paolo |last2=Lazzari |first2=Marco |year=1997 |conference=Worldwide ECCE Symposium on computers in the practice of building and civil engineering |location=Lahti, Finland |pages=310–314 |access-date=2015-11-29 }}&lt;/ref&gt; lie somewhere in the middle. Their basic concept is to add to the pre-existing information a new layer of multiple partial models of products and processes, so that it is possible to reuse existing databases, to reduce the development from scratch, and to provide evolutionary paths relevant for the development of the WIS. Each partial model may include specific knowledge and it acts as a way to structure and access the information according to a specific user view.
The comparison between strong and weak information systems may be summarized as follows:
{| class="wikitable"
!Strong information systems
!Weak information systems
|-
|Common data model
|Multiple specific integration models
|-
|Database oriented architecture
|Integration of multiple data sources by adding integration layers
|-
|One shot design
|Growing process
|-
|Redesign of legacy systems
|Integration of legacy systems
|}
The architecture of a weak information  system is composed of:
* information sources (databases, computational programs, ...);
* the integration layer.
The integration layer comprises the following sub-layers:
* abstraction layer (information models);
* communication layer between models and information sources;
* communication layer between models and humans (human-computer interface).

== Technical Data Management Systems in terms of regulations in different countries ==
In some countries, such as in the US, record and document management are considered very vital functions, and much stress is given in the management of Technical Archives. Records and documents coming under the public domain are governed by appropriate laws.&lt;ref&gt;{{Cite web|url = http://apps.americanbar.org/lpm/lpt/articles/tch01093.shtml|title = Document Management in the Digital Law Office|date = January 2009|accessdate = 2015-11-03|website = Law Practice Today|publisher = American Bar Association|last1 = Best|last2 = Foster|first1 = Steven J.|first2 = Debbie}}&lt;/ref&gt; However, this has not been so in many underdeveloped and [[Developing country|developing nations]]. For example, India enacted the ' Public Records Act'&lt;ref&gt;{{Cite web|url = http://nationalarchives.nic.in/writereaddata/html_en_files/html/public_records93.html|title = THE PUBLIC RECORDS ACT, 1993 (India)|date = 1993-12-22|accessdate = 2015-11-03|website = |publisher = Government of India|last = MOHANPURIA|first = K.L.}}&lt;/ref&gt; in 1993. However, many in the country are not aware of the existence of such a law or its importance.

== Applications and Examples of Technical Data Management Systems ==
Technical Data Management Systems (TDMS) are widely applied across the globe, in different sectors. Some of the examples are listed below.
* Voith Hydro tests models of the power plant turbines, including 4 main program parts, engine characteristics values, oscillation and cavitation, and transfer data from 1 program part to the next one using TDMS.&lt;ref name=":0" /&gt;
* Danburykline created a knowledge and data platform, SOROS, which is following the wiki based approach. It aims to represent data in accessible and simple forms.&lt;ref&gt;{{Cite web|url = http://danburykline.co.uk/DKWP/?page_id=967|title = Knowledge &amp; Technical Data Management|date = 2015-11-03|accessdate = 2015-11-03|website = |publisher = Danburykline|last = |first = }}&lt;/ref&gt;
* Berghof develops and provides a TDMS to simplify and manage data for development of firms including automobile firms. This TDMS enables reserve of data, centralization of data volumes on an online server. It is also compatible to Windows PC and many other systems.&lt;ref&gt;{{Cite web|url = http://www.berghof.com/en/products/test-engineering/technical-data-management/|title = Data availability|date = 2015-11-03|accessdate = 2015-11-03|website = Test engineering Technical data management|publisher = Berghof|last = |first = }}&lt;/ref&gt;
* This journal proposes the use of [[Cloud database|Cloud]] TDMS in third world countries for higher education purposes. Republic of Sudan is the model in this journal. Some of the solutions mentioned include online course delivery and online assignments and tests for greater class participation. Weaknesses mentioned include high financial costs and the fact that underdeveloped countries have not enough infrastructure to support such proposal.&lt;ref&gt;{{Cite journal|url = http://airccse.org/journal/ijdms/papers/7315ijdms02.pdf|title = Cloud Computing Architecture for higher education in the third world countries (Republic of the Sudan as model)|last = Adrees|first = Mohmed Sirelkhtem|date = June 2015|journal = International Journal of Database Management Systems ( IJDMS )|doi = 10.5121/ijdms.2015.7302|pmid = |access-date = 2015-11-03|volume = 7|last3 = Sheta|last2 = Omer|first2 = Majzoob Kamal Aldein|first3 = Osama E.|issue = 3}}&lt;/ref&gt;
* This journal is about [[text simplification]]. The purpose of this text simplification project in the journal is to simplify high level knowledge in English, so that students in high level studies who do not have sufficient English foundations can learn about these knowledge and data more easily. The method to do so suggested by the journal is to introduce a TDMS that can transform complicated English words into easier words. A problem with this project is that the Internet is flooded with useless information and it is very difficult to sort out useful information for simplification.&lt;ref&gt;{{Cite journal|url = http://airccse.org/journal/ijdms/papers/7415ijdms01.pdf|title = Software feasibility study to transform complex scientific written knowledge to a clear, rationale and simple language|last = Khandelwal|first = Manoj|date = August 2015|journal = International Journal of Database Management System (IJDMS)|doi = 10.5121/ijdms.2015.7401|pmid = |access-date = 2015-11-03|last2 = Jafarabad|first2 = Mohammad|issue = 4|volume = 7}}&lt;/ref&gt;
* This journal mentions about [[River basin|River Basin]] Information System (RBIS), which monitors data of different parts of a river basin, in order to identify which parts of the basin are gauging. Data is dynamic and lots of information has to be taken, which is impossible to do it manually. RBIS can help with this but one current weakness is that there are only 2 synoptic stations working (Kara and Niamtougou), whilst the rest are out of order.&lt;ref&gt;{{Cite journal|url = http://airccse.org/journal/ijdms/papers/7115ijdms02.pdf|title = An information for integrated land and water resources management in the Kara River Basin (Togo and Benin)|last = BADJANA|first = Hèou Maléki|date = February 2015|journal = International Journal of Database Management System (IJDMS)|doi = 10.5121/ijdms.2015.7102|pmid = |access-date = 2015-11-03|last2 = ZANDER|first2 = Franziska|issue = 1|volume = 7|last3 = KRALISCH|first3 = Sven|last4 = HELMSCHROT|first4 = Jörg|last5 = FLÜGEL|first5 = Wolfgang-Albert}}&lt;/ref&gt;

== Data mining ==
Data mining is an important criteria in constructing a technical Data Management System. For example, in building a E-commence platform, TDMS is needed to search and display information about the products.&lt;ref&gt;{{Cite journal|url = http://airccse.org/journal/ijdms/papers/7115ijdms01.pdf|title = Web-mining on Indonesia E-commerce site: Lazada and Rakuten|last = Simanjuntak|first = Humasak|date = February 2015|journal = International Journal of Database Management System (IJDMS)|doi = 10.5121/ijdms.2015.7101|pmid = |access-date = 2015-11-03|last2 = Sibarani|first2 = Novitasari|issue = 1|volume = 7|last3 = inaga|first3 = Bambang|last4 = Hutabarat|first4 = Novalina}}&lt;/ref&gt; It indicates that it is essential to gather information from other systems, to archive and manage it properly, and finally, to share it to users.

== See also ==
[[Data management system|Data Management System]]

[[Data mining]]

[[Database]]

[[Information Systems Research]]

== Further reading ==
http://airccse.org/journal/ijdms/papers/7115ijdms03.pdf&lt;ref&gt;{{Cite journal|url = http://airccse.org/journal/ijdms/papers/7115ijdms03.pdf|title = Mining closed sequential patterns in large sequence databases|last = Raju|first = V. Purushothama|date = February 2015|journal = International Journal of Database Management System (IJDMS)|doi = 10.5121/ijdms.2015.7103|pmid = |access-date = 2015-11-03|last2 = Varma|first2 = G.P. Saradhi|issue = 1|volume = 7}}&lt;/ref&gt;

https://seer.lcc.ufmg.br/index.php/jidm&lt;ref&gt;{{Cite journal|url = https://seer.lcc.ufmg.br/index.php/jidm|title = JOURNAL OF INFORMATION AND DATA MANAGEMENT|date = February 2015|journal = JOURNAL OF INFORMATION AND DATA MANAGEMENT|doi = |pmid = |access-date = 2015-11-03|volume = 6|issue = 1|editor-last = Traina Junior|editor-first = Caetano|editor2-last = Cordeiro|editor2-first = Robson L. F.|editor3-last = Amo|editor3-first = Sandra de|display-editors = 3 |editor4-last = Davis|editor4-first = Clodoveu|issn = 2178-7107}}&lt;/ref&gt;

== External links ==
* http://airccse.org/journal/ijdms/Editorialboard.html

==References==

&lt;references&gt;
&lt;/references&gt;

&lt;nowiki/&gt;

[[Category:Data management]]
[[Category:Document management systems]]
[[Category:Systems engineering]]</text>
      <sha1>p0ares3jmtb5a40qkkuuq9ygkcmoivf</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Data warehousing</title>
    <ns>14</ns>
    <id>5611387</id>
    <revision>
      <id>548512320</id>
      <parentid>535192175</parentid>
      <timestamp>2013-04-03T16:55:39Z</timestamp>
      <contributor>
        <username>Chobot</username>
        <id>259798</id>
      </contributor>
      <minor />
      <comment>Bot: Migrating 6interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:Q8363893|Q8363893]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="122" xml:space="preserve">[[Category:Information technology management]]
[[Category:Business intelligence]]
[[Category:Data management|Warehousing]]</text>
      <sha1>mtus0ip5whglz3uxpwdlmx3ekvuh4fs</sha1>
    </revision>
  </page>
  <page>
    <title>Record linkage</title>
    <ns>0</ns>
    <id>978951</id>
    <revision>
      <id>758742592</id>
      <parentid>756022920</parentid>
      <timestamp>2017-01-07T08:35:25Z</timestamp>
      <contributor>
        <username>Rjwilmsi</username>
        <id>203434</id>
      </contributor>
      <minor />
      <comment>/* Data preprocessing */Journal cites, set missing volume/pages parameter,  using [[Project:AWB|AWB]] (12142)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="30872" xml:space="preserve">'''Record linkage''' (RL) refers to the task of finding [[Record (database)|records]] in a data set that refer to the same [[entity]] across different data sources (e.g., data files, books, websites, databases).  Record linkage is necessary when [[Join (SQL)|joining]] data sets based on entities that may or may not share a common identifier (e.g., [[Relational model|database key]], [[Uniform Resource Identifier|URI]], [[National identification number]]), as may be the case due to differences in record shape, storage location, and/or curator style or preference.  A data set that has undergone RL-oriented reconciliation may be referred to as being ''cross-linked''. 
Record Linkage is called Data Linkage in many jurisdictions, but is the same process.

== History ==
The initial idea of record linkage goes back to [[Halbert L. Dunn]] in his 1946 article titled "Record Linkage" published in the ''[[American Journal of Public Health]]''.&lt;ref&gt;{{cite journal
 | first = Halbert L. | last = Dunn | authorlink = Halbert L. Dunn
 | title = Record Linkage
 | journal = [[American Journal of Public Health]]
 |date=December 1946 | volume = 36 | issue = 12 | pages = ''pp.'' 1412&amp;ndash;1416
 | url = http://www.ajph.org/cgi/reprint/36/12/1412
 | format = PDF
 | accessdate = 2008-05-31
 | doi = 10.2105/AJPH.36.12.1412
}}&lt;/ref&gt;  Howard Borden Newcombe laid the probabilistic foundations of modern record linkage theory in a 1959 article in ''[[Science (journal)|Science]]'',&lt;ref&gt;{{cite journal|last=Newcombe|first=H. B. |author2=J.M. Kennedy |author3=S.J. Axford |author4=A. P. James|title=Automatic Linkage of Vital Records|journal=Science|date=October 1959|volume=130|issue=3381|pages=954–959|doi=10.1126/science.130.3381.954|pmid=14426783}}&lt;/ref&gt; which were then formalized in 1969 by [[Ivan Fellegi]] and Alan Sunter who proved that the probabilistic decision rule they described was optimal when the comparison attributes were conditionally independent.  Their pioneering work "A Theory For Record Linkage"&lt;ref name=FellegiSunter&gt;{{cite journal 
 | first = Ivan  | last = Fellegi | authorlink = Ivan Fellegi 
 |author2=Sunter, Alan 
  | title = A Theory for Record Linkage 
 | journal = [[Journal of the American Statistical Association]] 
 |date=December 1969 | volume = 64 | issue = 328 | pages = ''pp.'' 1183&amp;ndash;1210  
 | jstor = 2286061
 | doi = 10.2307/2286061 
| url = http://courses.cs.washington.edu/courses/cse590q/04au/papers/Felligi69.pdf | format = PDF
 }}&lt;/ref&gt; remains the mathematical foundation for many record linkage applications even today.

Since the late 1990s, various machine learning techniques have been developed that can, under favorable conditions, be used to estimate the conditional probabilities required by the Fellegi-Sunter (FS) theory.  Several researchers have reported that the conditional independence assumption of the FS algorithm is often violated in practice; however, published efforts to explicitly model the conditional dependencies among the comparison attributes have not resulted in an improvement in record linkage quality.
{{Citation needed|date=May 2007}} On the other hand, machine learning or neural network algorithms that do not rely on these assumptions often provide far higher accuracy, when sufficient labeled training data is available.&lt;ref name="ReferenceA"&gt;{{cite conference | first = D. Randall | last = Wilson, D. Randall | title = Beyond Probabilistic Record Linkage: Using Neural Networks and Complex Features to Improve Genealogical Record Linkage | conference = Proceedings of International Joint Conference on Neural Networks | location = San Jose, California, USA | date = July 31 – August 5, 2011 | url = http://axon.cs.byu.edu/~randy/pubs/wilson.ijcnn2011.beyondprl.pdf}}&lt;/ref&gt;

Record linkage can be done entirely without the aid of a computer, but the primary reasons computers are often used for record linkage are to reduce or eliminate manual review and to make results more easily reproducible.  Computer matching has the advantages of allowing central supervision of processing, better quality control, speed, consistency, and better reproducibility of results.&lt;ref&gt;{{cite web|last=Winkler|first=William E.|title=Matching and Record Linkage|url=http://www.census.gov/srd/papers/pdf/rr93-8.pdf|publisher=U.S. Bureau of the Census|accessdate=12 November 2011}}&lt;/ref&gt;

== Naming conventions ==
"Record linkage" is the term used by statisticians, epidemiologists, and historians, among others, to describe the process of joining records from one data source with another that describe the same entity.  Commercial mail and database applications refer to it as "merge/purge processing" or "list washing".  [[computer science|Computer scientists]] often refer to it as "data matching" or as the "object identity problem".  Other names used to describe the same concept include: "coreference/entity/identity/name/record resolution", "entity disambiguation/linking", "duplicate detection", "deduplication", "record matching", "(reference) reconciliation", "object identification", "data/information integration" and "conflation".&lt;ref&gt;http://homes.cs.washington.edu/~pedrod/papers/icdm06.pdf&lt;/ref&gt;  This profusion of terminology has led to few cross-references between these research communities.&lt;ref&gt;[http://datamining.anu.edu.au/linkage.html Cristen, P &amp; T: Febrl - Freely extensible biomedical record linkage (Manual, release 0.3) p.9]&lt;/ref&gt;&lt;ref&gt;
{{cite journal
 | first = Ahmed  | last = Elmagarmid 
 |author2=Panagiotis G. Ipeirotis |author3=Vassilios Verykios 
  | title = Duplicate Record Detection: A Survey 
 | journal = IEEE Transactions on Knowledge and Data Engineering 
 |date=January 2007  | volume = 19  | issue = 1  | pages = ''pp.'' 1&amp;ndash;16 
 | url = http://www.cs.purdue.edu/homes/ake/pub/TKDE-0240-0605-1.pdf | format = PDF  | accessdate = 2009-03-30
 | doi = 10.1109/TKDE.2007.9
}}
&lt;/ref&gt;

While they share similar names, record linkage and [[Linked Data]] are two separate concepts.  Whereas record linkage focuses on the more narrow task of identifying matching entities across different data sets, Linked Data focuses on the broader methods of structuring and publishing data to facilitate the discovery of related information.

== Methods ==

=== Data preprocessing ===
Record linkage is highly sensitive to the quality of the data being linked, so all data sets under consideration (particularly their key identifier fields) should ideally undergo a [[data quality assessment]] prior to record linkage.  Many key identifiers for the same entity can be presented quite differently between (and even within) data sets, which can greatly complicate record linkage unless understood ahead of time.  For example, key identifiers for a man named William J. Smith might appear in three different data sets as so:

{| class="wikitable"
|-
! Data set !! Name !! Date of birth !! City of residence
|-
| Data set 1 || William J. Smith || 1/2/73 || Berkeley, California
|-
| Data set 2 || Smith, W. J. || 1973.1.2 || Berkeley, CA
|-
| Data set 3 || Bill Smith || Jan 2, 1973 || Berkeley, Calif.
|}

In this example, the different formatting styles lead to records that look different but in fact all refer to the same entity with the same logical identifier values.  Most, if not all, record linkage strategies would result in more accurate linkage if these values were first ''normalized'' or ''standardized'' into a consistent format (e.g., all names are "Surname, Given name", and all dates are "YYYY/MM/DD").  Standardization can be accomplished through simple rule-based [[data transformation]]s or more complex procedures such as lexicon-based [[Tokenization (lexical analysis)|tokenization]] and probabilistic hidden Markov models.&lt;ref&gt;{{cite journal|last=Churches|first=Tim|author2=Peter Christen |author3=Kim Lim |author4=Justin Xi Zhu |title=Preparation of name and address data for record linkage using hidden Markov models|journal=BMC Medical Informatics and Decision Making|date=13 December 2002|volume=2|doi=10.1186/1472-6947-2-9|url=http://www.biomedcentral.com/1472-6947/2/9 |pages=9}}&lt;/ref&gt;  Several of the packages listed in the ''Software Implementations'' section provide some of these features to simplify the process of data standardization.

===Entity resolution===
'''Entity resolution''' is an operational [[intelligence]] process, typically powered by an entity resolution engine or [[middleware]], whereby organizations can connect disparate data sources with a [[Opinion|view]] to understanding possible entity matches and non-obvious relationships across multiple [[data silos]]. It analyzes all of the [[information]] relating to individuals and/or entities from multiple sources of data, and then applies likelihood and probability scoring to determine which identities are a match and what, if any, non-obvious relationships exist between those identities.

Entity resolution engines are typically used to uncover [[risk]], [[fraud]], and conflicts of interest, but are also useful tools for use within [[Customer Data Integration]] (CDI) and [[Master Data Management]] (MDM) requirements. Typical uses for entity resolution engines include terrorist screening, insurance fraud detection, [[USA Patriot Act]] compliance, [[Organized retail crime]] ring detection and applicant screening.

For example: Across different data silos - employee records, vendor data, watch lists, etc. - an organization may have several variations of an entity named ABC, which may or may not be the same individual. These entries may, in fact, appear as ABC1, ABC2, or ABC3 within those data sources. By comparing similarities between underlying attributes such as [[Address (geography)|address]], [[date of birth]], or [[social security number]], the user can eliminate some possible matches and confirm others as very likely matches.

Entity resolution engines then apply rules, based on common sense logic, to identify hidden relationships across the data. In the example above, perhaps ABC1 and ABC2 are not the same individual, but rather two distinct people who share common attributes such as address or phone number.

====Data Matching====
While entity resolution solutions include data matching technology, many data matching offerings do not fit the definition of entity resolution. Here are four factors that distinguish entity resolution from data matching, according to John Talburt, director of the [[Ualr|UALR]] Center for Advanced Research in Entity Resolution and Information Quality:

* Works with both structured and unstructured records, and it entails the process of extracting references when the sources are unstructured or semi-structured
* Uses elaborate business rules and concept models to deal with missing, conflicting, and corrupted information
* Utilizes non-matching, asserted linking (associate) information in addition to direct matching
* Uncovers non-obvious relationships and association networks (i.e. who's associated with whom)

In contrast to data quality products, more powerful identity resolution engines also include a rules engine and workflow process, which apply business intelligence to the resolved identities and their relationships. These advanced technologies make automated decisions and impact business processes in real time, limiting the need for human intervention.

=== Deterministic record linkage ===
The simplest kind of record linkage, called ''deterministic'' or ''rules-based record linkage'', generates links based on the number of individual identifiers that match among the available data sets.&lt;ref&gt;{{cite journal|last=Roos|first=LL|author2=Wajda A |title=Record linkage strategies. Part I: Estimating information and evaluating approaches.|journal=Methods of Information in Medicine|date=April 1991|volume=30|issue=2|pages=117–123|pmid=1857246}}&lt;/ref&gt;  Two records are said to match via a deterministic record linkage procedure if all or some identifiers (above a certain threshold) are identical.  Deterministic record linkage is a good option when the entities in the data sets are identified by a common identifier, or when there are several representative identifiers (e.g., name, date of birth, and sex when identifying a person) whose quality of data is relatively high.

As an example, consider two standardized data sets, Set A and Set B, that contain different bits of information about patients in a hospital system.  The two data sets identify patients using a variety of identifiers: [[Social Security Number]] (SSN), name, date of birth (DOB), sex, and [[ZIP code]] (ZIP).  The records in two data sets (identified by the "#" column) are shown below:

{| class="wikitable"
|-
! Data Set !! # !! SSN !! Name !! DOB !! Sex !! ZIP
|-
| rowspan="4" | Set A || 1 || 000956723 || Smith, William || 1973/01/02 || Male || 94701
|- style="background:#f0f0f0;"
| 2 || 000956723 || Smith, William || 1973/01/02 || Male || 94703
|-
| 3 || 000005555 || Jones, Robert || 1942/08/14 || Male || 94701
|- style="background:#f0f0f0;"
| 4 || 123001234 || Sue, Mary || 1972/11/19 || Female || 94109
|-
| rowspan="2" | Set B || 1 || 000005555 ||Jones, Bob || 1942/08/14 || ||
|- style="background:#f0f0f0;"
| 2 || || Smith, Bill || 1973/01/02 || Male || 94701
|}

The most simple deterministic record linkage strategy would be to pick a single identifier that is assumed to be uniquely identifying, say SSN, and declare that records sharing the same value identify the same person while records not sharing the same value identify different people.  In this example, deterministic linkage based on SSN would create entities based on A1 and A2; A3 and B1; and A4.  While A1, A2, and B2 appear to represent the same entity, B2 would not be included into the match because it is missing a value for SSN.

Handling exceptions such as missing identifiers involves the creation of additional record linkage rules.  One such rule in the case of missing SSN might be to compare name, date of birth, sex, and ZIP code with other records in hopes of finding a match.  In the above example, this rule would still not match A1/A2 with B2 because the names are still slightly different: standardization put the names into the proper (Surname, Given name) format but could not discern "Bill" as a nickname for "William".  Running names through a [[phonetic algorithm]] such as [[Soundex]], [[NYSIIS]], or [[metaphone]], can help to resolve these types of problems (though it may still stumble over surname changes as the result of marriage or divorce), but then B2 would be matched only with A1 since the ZIP code in A2 is different.  Thus, another rule would need to be created to determine whether differences in particular identifiers are acceptable (such as ZIP code) and which are not (such as date of birth).

As this example demonstrates, even a small decrease in data quality or small increase in the complexity of the data can result in a very large increase in the number of rules necessary to link records properly.  Eventually, these linkage rules will become too numerous and interrelated to build without the aid of specialized software tools.  In addition, linkage rules are often specific to the nature of the data sets they are designed to link together.  One study was able to link the Social Security [[Death Master File]] with two hospital registries from the [[Midwestern United States]] using SSN, NYSIIS-encoded first name, birth month, and sex, but these rules may not work as well with data sets from other geographic regions or with data collected on younger populations.&lt;ref&gt;{{cite journal|last=Grannis|first=SJ|author2=Overhage JM |author3=McDonald CJ |title=Analysis of identifier performance using a deterministic linkage algorithm|journal=Proc AMIA Symp.|year=2002|pages=305–9|pmid=12463836|pmc=2244404}}&lt;/ref&gt;  Thus, continuous maintenance testing of these rules is necessary to ensure they continue to function as expected as new data enter the system and need to be linked.  New data that exhibit different characteristics than was initially expected could require a complete rebuilding of the record linkage rule set, which could be a very time-consuming and expensive endeavor.

=== Probabilistic record linkage ===
''Probabilistic record linkage'', sometimes called ''fuzzy matching'' (also ''probabilistic merging'' or ''fuzzy merging'' in the context of merging of databases), takes a different approach to the record linkage problem by taking into account a wider range of potential identifiers, computing weights for each identifier based on its estimated ability to correctly identify a match or a non-match, and using these weights to calculate the probability that two given records refer to the same entity.  Record pairs with probabilities above a certain threshold are considered to be matches, while pairs with probabilities below another threshold are considered to be non-matches; pairs that fall between these two thresholds are considered to be "possible matches" and can be dealt with accordingly (e.g., human reviewed, linked, or not linked, depending on the requirements).  Whereas deterministic record linkage requires a series of potentially complex rules to be programmed ahead of time, probabilistic record linkage methods can be "trained" to perform well with much less human intervention.

Many probabilistic record linkage algorithms assign match/non-match weights to identifiers by means of two probabilities called ''u'' and ''m''. The ''u'' probability is the probability that an identifier in two ''non-matching'' records will agree purely by chance.  For example, the ''u'' probability for birth month (where there are twelve values that are approximately uniformly distributed) is 1/12 ≈ 0.083; identifiers with values that are not uniformly distributed will have different ''u'' probabilities for different values (possibly including missing values).  The ''m'' probability is the probability that an identifier in ''matching'' pairs will agree (or be sufficiently similar, such as strings with high [[Jaro-Winkler distance]] or low [[Levenshtein distance]]).  This value would be 1.0 in the case of perfect data, but given that this is rarely (if ever) true, it can instead be estimated.  This estimation may be done based on prior knowledge of the data sets, by manually identifying a large number of matching and non-matching pairs to "train" the probabilistic record linkage algorithm, or by iteratively running the algorithm to obtain closer estimations of the ''m'' probability.  If a value of 0.95 were to be estimated for the ''m'' probability, then the match/non-match weights for the birth month identifier would be:

{| class="wikitable"
|-
! Outcome !! Proportion of links !! Proportion of non-links !! Frequency ratio !! Weight
|-
| Match || ''m'' = 0.95 || ''u'' ≈ 0.083 || ''m''/''u'' ≈ 11.4 || ln(''m''/''u'')/ln(2) ≈ 3.51
|-
| Non-match || 1−''m'' = 0.05 || 1-''u'' ≈ 0.917 || (1-''m'')/(1-''u'') ≈ 0.0545 || ln((1-''m'')/(1-''u''))/ln(2) ≈ -4.20
|}

The same calculations would be done for all other identifiers under consideration to find their match/non-match weights.  Then, every identifier of one record would be compared with the corresponding identifier of another record to compute the total weight of the pair: the ''match'' weight is added to the running total whenever a pair of identifiers agree, while the ''non-match'' weight is added (i.e. the running total decreases) whenever the pair of identifiers disagrees.  The resulting total weight is then compared to the aforementioned thresholds to determine whether the pair should be linked, non-linked, or set aside for special consideration (e.g. manual validation).&lt;ref name="prl"&gt;{{cite journal|last=Blakely|first=Tony|author2=Salmond, Clare |title=Probabilistic record linkage and a method to calculate the positive predictive value|journal=International Journal of Epidemiology|date=December 2002|volume=31|issue=6|pages=1246–1252|doi=10.1093/ije/31.6.1246|pmid=12540730|url=http://ije.oxfordjournals.org/content/31/6/1246.full}}&lt;/ref&gt;

Determining where to set the match/non-match thresholds is a balancing act between obtaining an acceptable [[Sensitivity and specificity#Sensitivity|sensitivity]] (or ''recall'', the proportion of truly matching records that are linked by the algorithm) and [[positive predictive value]] (or ''precision'', the proportion of records linked by the algorithm that truly do match).  Various manual and automated methods are available to predict the best thresholds, and some record linkage software packages have built-in tools to help the user find the most acceptable values.  Because this can be a very computationally demanding task, particularly for large data sets, a technique known as ''blocking'' is often used to improve efficiency.  Blocking attempts to restrict comparisons to just those records for which one or more particularly discriminating identifiers agree, which has the effect of increasing the positive predictive value (precision) at the expense of sensitivity (recall).&lt;ref name=prl /&gt;  For example, blocking based on a phonetically coded surname and ZIP code would reduce the total number of comparisons required and would improve the chances that linked records would be correct (since two identifiers already agree), but would potentially miss records referring to the same person whose surname or ZIP code was different (due to marriage or relocation, for instance).  Blocking based on birth month, a more stable identifier that would be expected to change only in the case of data error, would provide a more modest gain in positive predictive value and loss in sensitivity, but would create only twelve distinct groups which, for extremely large data sets, may not provide much net improvement in computation speed.  Thus, robust record linkage systems often use multiple blocking passes to group data in various ways in order to come up with groups of records that should be compared to each other.

===Machine learning===
In recent years, a variety of machine learning techniques have been used in record linkage.  It has been recognized&lt;ref name="ReferenceA"/&gt; that a classic algorithm for probabilistic record linkage is equivalent to the [[Naive Bayes]] algorithm in the field of machine learning,&lt;ref&gt;Quass, Dallan, and Starkey, Paul. “Record Linkage for Genealogical Databases,” ACM SIGKDD ’03 Workshop on Data Cleaning, Record Linkage, and Object Consolidation, August 24–27, 2003, Washington, D.C.&lt;/ref&gt; and suffers from the same assumption of the independence of its features (an assumption that is typically not true).&lt;ref&gt;Langley, Pat, Wayne Iba, and Kevin Thompson. “An Analysis of Bayesian Classifiers,” In Proceedings of the 10th National Conference on Artificial Intelligence, (AAAI-92), AAAI Press/MIT Press, Cambridge, MA, pp. 223-228, 1992.&lt;/ref&gt;&lt;ref&gt;Michie, D., D. Spiegelhalter, and C. Taylor. Machine Learning, Neural and Statistical Classification, Ellis Horwood, Hertfordshire, England. Book 19, 1994.&lt;/ref&gt;  Higher accuracy can often be achieved by using various other machine learning techniques, including a single-layer [[perceptron]].&lt;ref name="ReferenceA"/&gt;

== Mathematical model ==
In an application with two files, A and B, denote the rows (''records'') by &lt;math&gt;\alpha (a)&lt;/math&gt; in file A and &lt;math&gt;\beta (b)&lt;/math&gt; in file B. Assign &lt;math&gt;K&lt;/math&gt; ''characteristics'' to each record. The set of records that represent identical entities is defined by

&lt;math&gt; M = \left\{ (a,b); a=b; a \in A; b \in B \right\} &lt;/math&gt;

and the complement of set &lt;math&gt;M&lt;/math&gt;, namely set &lt;math&gt;U&lt;/math&gt; representing different entities is defined as

&lt;math&gt; U = \{ (a,b); a \neq b; a \in A, b \in B \} &lt;/math&gt;.

A vector, &lt;math&gt;\gamma&lt;/math&gt; is defined, that contains the coded agreements and disagreements on each characteristic:

&lt;math&gt; \gamma \left[ \alpha ( a ), \beta ( b ) \right] = \{ \gamma^{1} \left[ \alpha ( a ) , \beta ( b ) \right] ,...,	\gamma^{K} \left[ \alpha ( a ), \beta ( b ) \right] \} &lt;/math&gt;

where &lt;math&gt;K&lt;/math&gt; is a subscript for the characteristics (sex, age, marital status, etc.) in the files. The conditional probabilities of observing a specific vector &lt;math&gt;\gamma&lt;/math&gt; given &lt;math&gt;(a, b) \in M&lt;/math&gt;, &lt;math&gt;(a, b) \in U&lt;/math&gt; are defined as

&lt;math&gt;
 m(\gamma) = P \left\{ \gamma \left[ \alpha (a), \beta (b) \right] | (a,b) \in M \right\} =
 \sum_{(a, b) \in M} P \left\{\gamma\left[ \alpha(a), \beta(b) \right] \right\} \cdot
                 P \left[ (a, b) | M\right]
&lt;/math&gt;

and

&lt;math&gt;
 u(\gamma) = P \left\{ \gamma \left[ \alpha (a), \beta (b) \right] | (a,b) \in U \right\} =
 \sum_{(a, b) \in U} P \left\{\gamma\left[ \alpha(a), \beta(b) \right] \right\} \cdot
                 P \left[ (a, b) | U\right],
&lt;/math&gt;
respectively.&lt;ref name=FellegiSunter /&gt;

== Applications ==

===Master data management===
Most [[Master data management]] (MDM) products use a record linkage process to identify records from different sources representing the same real-world entity. This linkage is used to create a "golden master record" containing the cleaned, reconciled data about the entity. The techniques used in MDM are the same as for record linkage generally. MDM expands this matching not only to create a "golden master record" but to infer relationships also. (i.e. a person has a same/similar surname and same/similar address, this might imply they share a household relationship).

=== Data warehousing and business intelligence ===
Record linkage plays a key role in [[data warehousing]] and [[business intelligence]].  Data warehouses serve to combine data from many different operational source systems into one [[logical data model]], which can then be subsequently fed into a business intelligence system for reporting and analytics.  Each operational source system may have its own method of identifying the same entities used in the logical data model, so record linkage between the different sources becomes necessary to ensure that the information about a particular entity in one source system can be seamlessly compared with information about the same entity from another source system.  Data standardization and subsequent record linkage often occur in the "transform" portion of the [[extract, transform, load]] (ETL) process.

=== Historical research ===
Record linkage is important to social history research since most data sets, such as [[census|census records]] and parish registers were recorded long before the invention of [[National identification number]]s.  When old sources are digitized, linking of data sets is a prerequisite for [[longitudinal study]].  This process is often further complicated by lack of standard spelling of names, family names that change according to place of dwelling, changing of administrative boundaries, and problems of checking the data against other sources.  Record linkage was among the most prominent themes in the [[History and computing]] field in the 1980s, but has since been subject to less attention in research.{{Citation needed|date=November 2011}}

=== Medical practice and research ===
&lt;!-- any experts out there? --&gt;
Record linkage is an important tool in creating data required for examining the health of the public and of the health care system itself. It can be used to improve data holdings, data collection, quality assessment, and the dissemination of information. Data sources can be examined to eliminate duplicate records, to identify under-reporting and missing cases (e.g., census population counts), to create person-oriented health statistics, and to generate disease registries and health surveillance systems. Some cancer registries link various data sources (e.g., hospital admissions, pathology and clinical reports, and death registrations) to generate their registries. Record linkage is also used to create health indicators. For example, fetal and infant mortality is a general indicator of a country's socioeconomic development, public health, and maternal and child services. If infant death records are matched to birth records, it is possible to use birth variables, such as birth weight and gestational age, along with mortality data, such as cause of death, in analyzing the data.  Linkages can help in follow-up studies of cohorts or other groups to determine factors such as vital status, residential status, or health outcomes. Tracing is often needed for follow-up of industrial cohorts, clinical trials, and longitudinal surveys to obtain the cause of death and/or cancer.  An example of a successful and long-standing record linkage system allowing for population-based medical research is the [[Rochester Epidemiology Project]] based in [[Rochester, Minnesota]].&lt;ref name="data resource profile"&gt;{{cite journal | author1=St. Sauver JL  | author2=Grossardt BR | author3=Yawn BP | author4=Melton LJ 3rd | author5=Pankratz JJ |author6=Brue SM | author7=Rocca WA. | title = Data Resource Profile: The Rochester Epidemiology Project (REP) medical records-linkage system | journal = Int J Epidemiol | volume=41 | issue=6 | pages=1614–24 | year = 2012 | pmid = 23159830 | doi=10.1093/ije/dys195 | url=http://ije.oxfordjournals.org/content/41/6/1614.long}}&lt;/ref&gt;

== Criticism of existing software implementations==
The main reasons cited are:
* '''Project costs''': costs typically in the hundreds of thousands of dollars
* '''Time''': lack of enough time to deal with large-scale [[data cleansing]] software
* '''Security''': concerns over sharing information, giving an application access across systems, and effects on legacy systems

== See also ==
* [[Capacity optimization]]
* [[Content-addressable storage]]
* [[Data deduplication]]
* [[Delta encoding]]
* [[Entity linking]]
* [[Entity-attribute-value model]]
* [[Identity resolution]]
* [[Linked data]]
* [[Named-entity recognition]]
* [[Open data]]
* [[Schema matching]]
* [[Single-instance storage]]

== Notes and references ==
{{Reflist|2}}

== External links ==
* [http://pike.psu.edu/linkage/ Data Linkage Project at Penn State, USA]
* [http://www.datadecision.com Datadecision - Data matching online tool]
* [http://www.nameapi.org/en/demos/name-matcher/ NameAPI - Name Matcher]
* [http://sourceforge.net/projects/oysterer/ OYSTER Entity Resolution]
* [http://datamining.anu.edu.au/ Febrl - Freely Extensible Biomedical Record Linkage]
* [http://infolab.stanford.edu/serf/ Stanford Entity Resolution Framework]
* [http://dbs.uni-leipzig.de/de/research/projects/large_scale_object_matching/ Dedoop - Deduplication with Hadoop]
* [https://sourceforge.net/projects/erframework/ BlockingFramework A framework for blocking-based Entity Resolution]
* [http://www.ipdln.org/ International Population Data Linkage Network]
* [https://github.com/yahoo/FEL Yahoo Fast Entity Linker Core]

{{DEFAULTSORT:Record Linkage}}
[[Category:Data management]]</text>
      <sha1>acbyzgoc9m1uqnmjuvn18yscshm2o4p</sha1>
    </revision>
  </page>
  <page>
    <title>Customer data management</title>
    <ns>0</ns>
    <id>31501606</id>
    <revision>
      <id>695948693</id>
      <parentid>669195217</parentid>
      <timestamp>2015-12-19T21:19:32Z</timestamp>
      <contributor>
        <username>NickPenguin</username>
        <id>703140</id>
      </contributor>
      <comment>rm merge tag, no consensus to merge</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4170" xml:space="preserve">'''Customer data management (CDM)''' is the ways in which businesses keep track of their customer information and survey their [[customer base]] in order to obtain feedback. CDM embraces a range of software or [[cloud computing]] applications designed to give large organizations rapid and efficient access to customer data. Surveys and data can be centrally located and widely accessible within a company, as opposed to being warehoused in separate departments. CDM encompasses the collection, analysis, organizing, reporting and sharing of customer information throughout an organization. Businesses need a thorough understanding of their customers’ needs if they are to retain and increase their customer base. Efficient CDM solutions provide companies with the ability to deal instantly with customer issues and obtain immediate feedback. As a result, [[customer retention]] and [[customer satisfaction]] can show dramatic improvement. According to a recent study by [[Aberdeen Group]] inc.: "Above-average and best-in-class companies... attain greater than 20% annual improvement in retention rates, revenues, data accuracy and partner/customer satisfaction rates."&lt;ref&gt;Smalltree, Hannah (2006) [http://searchcrm.techtarget.com/news/1212337/Best-practices-in-managing-customer-data]&lt;/ref&gt;

== Customer data management and cloud computing ==

Cloud computing offers an attractive choice for CDM in many companies due to its accessibility and [[Cost efficiency|cost-effectiveness]]. Businesses can decide who, within their company, should have the ability to create, adjust, analyze or share customer information. In December 2010, 52% of [[Information technology|Information Technology]] (IT) professionals worldwide were deploying, or planning to deploy, cloud computing;&lt;ref&gt;Cisco.com (December 2010) [http://newsroom.cisco.com/dlls/2010/prod_120810.html]&lt;/ref&gt; this percentage is far higher in many countries.

== Uses for management ==

'''Customer data management'''
* should provide a cost-effective, user-friendly solution for [[marketing]], research, sales, [[human resources]] and IT departments
* enables companies to create and email online surveys, reports and newsletters
* encompasses and simplifies [[customer relationship management]] (CRM) and [[Customer feedback management services|customer feedback management]] (CFM)

== Background ==

Customer data management, as a term, was coined in the 1990s, pre-dating the alternative term [[enterprise feedback management]] (EFM). Customer data management (CDM) was introduced as a software solution that would replace earlier disc-based or paper-based surveys and [[spreadsheet]] data. Initially, CDM solutions were marketed to businesses as software, specific to one company, and often to one department within that company. This was superseded by [[application service  provider]]s (ASPs) where software was hosted for [[end user]] organizations, thus avoiding the necessity for IT professionals to deploy and support software. However, ASPs with their single-tenancy architecture were, in turn, superseded by [[software as a service]] (SaaS), engineered for multi-tenancy. By 2007 SaaS applications, giving businesses on-demand access to their customer information, were rapidly gaining popularity compared with ASPs. Cloud computing now includes SaaS and many prominent CDM providers offer cloud-based applications to their clients.

In recent years, there has been a push away from the term EFM, with many of those working in this area advocating the slightly updated use of CDM. The return to the term CDM is largely based on the greater need for clarity around the solutions offered by companies, and on the desire to retire terminology veering on techno-jargon that customers may have a hard time understanding.&lt;ref&gt;InSiteSystems.com (December, 2010) [http://www.insitesystems.com/systems/blogs/the-problem-with-efm.html]&lt;/ref&gt;

== References ==
&lt;!--- See [[Wikipedia:Footnotes]] on how to create references using &lt;ref&gt;&lt;/ref&gt; tags which will then appear here automatically --&gt;
{{Reflist}}

&lt;!--- Categories ---&gt;
[[Category:Articles created via the Article Wizard]]
[[Category:Data management]]</text>
      <sha1>f691y9d2juj5vjogm01pv3foi6ldjuf</sha1>
    </revision>
  </page>
  <page>
    <title>Australian National Data Service</title>
    <ns>0</ns>
    <id>31960097</id>
    <revision>
      <id>750314629</id>
      <parentid>745574692</parentid>
      <timestamp>2016-11-19T01:07:08Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor />
      <comment>1 archive template merged to {{[[template:webarchive|webarchive]]}} ([[User:Green_Cardamom/Webarchive_template_merge|WAM]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2761" xml:space="preserve">The '''Australian National Data Service''' (ANDS) was established in 2008 to help address the challenges of storing and managing Australia's research data, and making it discoverable and accessible for validation and reuse. It is a joint collaboration between [[Monash University]], [[The Australian National University]] and [[CSIRO]].

==Background==
ANDS is funded by the [[Australian Department of Education]]. The funding has been provided through Australian Government's National Collaborative Research Infrastructure Strategy (NCRIS) as part of the Platforms for Collaboration Investment Plan.&lt;ref&gt;{{cite web|url=http://ncris.innovation.gov.au/Capabilities/Pages/PfC.aspx#ANDS |title=Platforms for Collaboration |accessdate=2011-06-02 |deadurl=yes |archiveurl=https://web.archive.org/web/20110702094824/http://ncris.innovation.gov.au:80/Capabilities/Pages/PfC.aspx |archivedate=2011-07-02 |df= }}&lt;/ref&gt; The NCRIS roadmap emphasized the vital importance of eResearch Infrastructure to Australian future research competitiveness.&lt;ref&gt;{{cite web|title=eResearch Infrastructure |url=http://www.pfc.org.au/bin/view/Main |publisher=NCRIS |accessdate=25 June 2011 }}{{dead link|date=October 2016 |bot=InternetArchiveBot |fix-attempted=yes }}&lt;/ref&gt; In mid-2009 ANDS was further funded by the Education Investment Fund (EIF) for the establishment of the Australian Research Data Commons under the Australian Government’s Super Science Initiative.&lt;ref&gt;{{cite web|title=Super Science Initiative |url=http://www.innovation.gov.au/SCIENCE/RESEARCHINFRASTRUCTURE/Pages/SuperScience.aspx |publisher=DIISR |accessdate=25 June 2011 |deadurl=yes |archiveurl=https://web.archive.org/web/20110601193908/http://www.innovation.gov.au/Science/ResearchInfrastructure/Pages/SuperScience.aspx |archivedate=1 June 2011 |df= }}&lt;/ref&gt;

==Research Data Australia==
''Research Data Australia'' (formerly the ''ANDS Collections Registry'') is an online discovery service run by ANDS.&lt;ref&gt;[http://researchdata.ands.org.au/home/about Research Data Australia]&lt;/ref&gt;&lt;ref&gt;[http://www.ands.org.au/resource/registry.html ANDS Collections Registry] {{webarchive |url=https://web.archive.org/web/20140302113241/http://www.ands.org.au/resource/registry.html |date=March 2, 2014 }}&lt;/ref&gt; It allows researchers to publicise the existence of their research data and enable prospective users of that data to find it.

''Research Data Australia'' makes use of the [[ISO 2146]]-based [[RIF-CS]] metadata standard.&lt;ref&gt;[http://ands.org.au/guides/cpguide/cpgrifcs.html About RIF-CS]&lt;/ref&gt;

== External links ==
* {{Official website|http://www.ands.org.au}}
* [http://researchdata.ands.org.au Research Data Australia]

==References==
&lt;references /&gt;

[[Category:Data management]]


{{Australia-org-stub}}</text>
      <sha1>awj1rh15yoacuzkznnsp5hxiuj8a8mb</sha1>
    </revision>
  </page>
  <page>
    <title>Super column</title>
    <ns>0</ns>
    <id>31220085</id>
    <revision>
      <id>721058477</id>
      <parentid>577671349</parentid>
      <timestamp>2016-05-19T15:03:52Z</timestamp>
      <contributor>
        <username>Vrettos</username>
        <id>169378</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2183" xml:space="preserve">[[Image:SuperColumn (data store).png|300px|thumb|The super column consists of a (unique) super column name, and a number of columns.]]
A '''super column''' is a [[tuple]] (a pair) with a binary super column name and a value that maps it to many columns.&lt;ref&gt;{{cite web
| accessdate = 2011-03-18
| author = Arin Sarkissian
| date = 2009-09-01
| location = http://arin.me/post/40054651676/wtf-is-a-supercolumn-cassandra-data-model
| publisher = Arin Sarkissian
| title = WTF is a SuperColumn? An Intro to the Cassandra Data Model
| quote = A SuperColumn is a tuple with a binary name &amp; a value which is a map containing an unbounded number of Columns – keyed by the Column‘s name.
| url = }}&lt;/ref&gt; They consist of a key-value pairs, where the values are columns. Theoretically speaking, super columns are ([[Sorting algorithm|sorted]]) [[associative array]] of columns.&lt;ref&gt;{{cite web
| accessdate = 2011-03-18
| location = http://wiki.apache.org/cassandra/DataModel
| publisher = Apache Cassandra
| title = Cassandra wiki: Data Model: Super columns
| url = http://wiki.apache.org/cassandra/DataModel}}&lt;/ref&gt; Similar to a regular [[column family]] where a row is a sorted map of column names and column values, a row in a super column family is a sorted map of super column names that maps to column names and column values. 

A super column is part of a [[keyspace (data model)]] together with other super columns and column families, and columns.

==Code example==
Written in the [[JSON]]-like syntax, a super column definition can be like this:

&lt;source lang="SQL"&gt;
 {
   "mccv": {
     "Tags": {
       "cassandra": {
         "incubator": {"url": "http://incubator.apache.org/cassandra/"},
         "jira": {"url": "http://issues.apache.org/jira/browse/CASSANDRA"}
       },
       "thrift": {
         "jira": {"url": "http://issues.apache.org/jira/browse/THRIFT"}
       }
     }
   }
 }
&lt;/source&gt;

==See also==
* [[Column (data store)]]
* [[Keyspace (NoSQL)]]

==References==
{{reflist}}

==External links==
* [http://wiki.apache.org/cassandra/DataModel The Apache Cassandra data model]

&lt;!--Interwikies--&gt;
[[Category:Data_management]]
&lt;!--Categories--&gt;


{{database-stub}}</text>
      <sha1>lh17mds6cufklvwkhdwd88sgi2a6tln</sha1>
    </revision>
  </page>
  <page>
    <title>PureXML</title>
    <ns>0</ns>
    <id>19470961</id>
    <revision>
      <id>738923456</id>
      <parentid>615294716</parentid>
      <timestamp>2016-09-11T20:07:55Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor />
      <comment>[[User:Green Cardamom/WaybackMedic 2|WaybackMedic 2]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7621" xml:space="preserve">{{Lowercase title}}
'''pureXML''' is the native [[XML]] storage feature in the [[IBM DB2]] data server.  pureXML provides [[query language]]s, storage technologies, indexing technologies, and other features to support XML data.  The word ''pure'' in pureXML was chosen to indicate that DB2 natively stores and natively processes XML data in its inherent hierarchical structure, as opposed to treating XML data as plain text or converting it into a relational format.&lt;ref&gt;http://www.ibm.com/developerworks/blogs/page/datastudioteam?entry=purexml_and_purequery_what_s&lt;/ref&gt;

== Technical information ==
DB2 includes two distinct storage mechanisms: one for efficiently managing traditional SQL data types, and another for managing XML data.  The underlying storage mechanism is transparent to users and applications; they simply use SQL (including SQL with XML extensions or [[SQL/XML]]) or [[XQuery]] to work with the data.

XML data is stored in columns of DB2 tables that have the XML data type.  XML data is stored in a parsed format that reflects the hierarchical nature of the original XML data.  As such, pureXML uses trees and nodes as its model for storing and processing XML data.  If you instruct DB2 to validate XML data against an XML schema prior to storage, DB2 annotates all nodes in the XML hierarchy with information about the schema types; otherwise, it will annotate the nodes with default type information.  Upon storage, DB2 preserves the internal structure of XML data, converting its tag names and other information into integer values. Doing so helps conserve disk space and also improves the performance of queries that use navigational expressions. However, users aren't aware of this internal representation.  Finally, DB2 automatically splits XML nodes across multiple database pages, as needed.

XML schemas specify which XML elements are valid, in what order these elements should appear in XML data, which XML data types are associated with each element, and so on.  pureXML allows you to validate the cells in a column of XML data against no schema, one schema, or multiple schemas.  pureXML also provides tools to support evolving XML schemas.

IBM has enhanced its [[programming language]] interfaces to support access to its XML data. These enhancements span [[Java (programming language)|Java]] ([[JDBC]]), [[C (programming language)|C]] (embedded SQL and call-level interface), [[COBOL]] (embedded SQL), [[PHP]], and [[Microsoft]]'s [[.NET Framework]] (through the DB2.NET provider).

== History ==
pureXML was first included in the DB2 9 for [[Linux]], [[Unix]], and [[Microsoft Windows]] release, which was codenamed Viper, in June 2006.&lt;ref&gt;{{cite web| url= http://www-03.ibm.com/press/us/en/pressrelease/19781.wss | title = IBM News room - 2006-06-08 IBM Transforms Database Market With Introduction of DB2 - United States | archiveurl= https://web.archive.org/web/20121011235127/http://www-03.ibm.com/press/us/en/pressrelease/19781.wss | archivedate= 2012-10-11 }}&lt;/ref&gt;  It was available on DB2 9 for [[z/OS]] in March 2007.&lt;ref&gt;{{cite web| url= http://www-03.ibm.com/press/us/en/pressrelease/21189.wss | title = IBM News room - 2007-03-06 IBM Unveils DB2 Viper for the Mainframe - United States | archiveurl= https://web.archive.org/web/20121011235143/http://www-03.ibm.com/press/us/en/pressrelease/21189.wss | archivedate= 2012-10-11 }}&lt;/ref&gt;  In October 2007, IBM released DB2 9.5 with improved XML data transaction performance and improved storage savings.&lt;ref&gt;{{cite web| url= http://www-03.ibm.com/press/us/en/pressrelease/22455.wss | title = IBM News room - 2007-10-15 IBM Extends Data Server Technology Lead With Introduction of DB2 &amp;quot;Viper 2&amp;quot; - United States | archiveurl= https://web.archive.org/web/20121011235149/http://www-03.ibm.com/press/us/en/pressrelease/22455.wss | archivedate= 2012-10-11 }}&lt;/ref&gt; In June 2009, IBM released DB2 9.7 with XML supported for database-partitioned, range-partitioned, and multi-dimensionally clustered tables as well as compression of XML data and indices.&lt;ref&gt;{{cite web| url= http://www-03.ibm.com/press/us/en/pressrelease/27279.wss | title = IBM News room - 2009-04-22 IBM Database Software Improves Operational Efficiency and Cuts Storage Costs by Up to 75% - United States | archiveurl= https://web.archive.org/web/20121121014600/http://www-03.ibm.com/press/us/en/pressrelease/27279.wss | archivedate= 2012-11-21 }}&lt;/ref&gt;

== Competition ==
{{See also|XML database}}
DB2 is a hybrid data server—it offers data management for traditional relational data, as well as providing native XML data management.  Other vendors that offer data management for both relational data and native XML storage include [[Oracle Corporation|Oracle]] with its [[Oracle Database|11g]] product and Microsoft with its [[Microsoft SQL Server|SQL Server]] product.

pureXML also competes with native XML databases like [[BaseX (database)|BaseX]], [[eXist]], [[MarkLogic]] or [[Sedna (database)|Sedna]].

== User groups ==
The International DB2 Users Group (IDUG) is an independent, not-for-profit association of IT professionals who use IBM DB2.  IDUG provides education, technical resources, peer networking opportunities, online resources and other programs for DB2 users.

== Books ==
IBM International Technical Support Organization (ITSO) has published the following books, which are available in print or as free e-books:
* [http://www.redbooks.ibm.com/abstracts/sg247298.html?Open DB2 9: pureXML Overview and Fast Start]
* [http://www.redbooks.ibm.com/abstracts/sg247315.html?Open DB2 9 pureXML Guide]

The following books are also available for purchase:
* [http://www.amazon.com/DB2-pureXML-Cookbook-Master-Hybrid/dp/0138150478/ DB2 pureXML Cookbook: Master the Power of IBM Hybrid Data Server]

== Education and training ==
The following pureXML classroom and online courses are available from IBM Education:
* [http://www-304.ibm.com/jct03001c/services/learning/ites.wss/us/en?pageType=course_description&amp;courseCode=CG130 Query and Manage XML Data with DB2 9].  IBM course CG130.  Classroom.  Duration: 4 days.
* [http://www-304.ibm.com/jct03001c/services/learning/ites.wss/us/en?pageType=course_description&amp;courseCode=CG100 Query XML Data with DB2 9].  IBM course CG100.  Classroom.  Duration: 2 days (first 2 days of CG130).
* Managing XML Data in DB2 9.  IBM course CG160.  Classroom.  Duration: 2 days (last 2 days of CG130).
* [http://www-304.ibm.com/jct03001c/services/learning/ites.wss/us/en?pageType=course_search&amp;sortBy=5&amp;searchType=1&amp;sortDirection=9&amp;includeNotScheduled=15&amp;rowStart=0&amp;rowsToReturn=20&amp;maxSearchResults=200&amp;searchString=CT140&amp;language=en&amp;country=us DB2 pureXML].  IBM Course CT140.  Self-paced study plus Live Virtual Classroom.

== See also ==
* [[IBM DB2]]
* [[XML database]]

== References ==
{{Reflist}}

== External links ==
* {{official website|http://www.ibm.com/software/data/db2/xml}}
* [http://www.ibm.com/developerworks/wikis/display/db2xml/Home pureXML Wiki]
* [http://www.ibm.com/developerworks/forums/forum.jspa?forumID=1423 pureXML Forum]
* [http://www.ibm.com/developerworks/blogs/page/purexml pureXML Team Blog]
* [http://www.nativexmldatabase.com Native XML Database Blog]
* [http://blog.4loeser.net Blog with pureXML Topics]

=== Online communities ===
Online communities allow pureXML users to network with fellow professionals.
* [http://www.linkedin.com/groups?gid=129185 pureXML Group on LinkedIn]

{{IBM DB2 product family}}

[[Category:XML software]]
[[Category:Data management]]
[[Category:Data modeling]]
[[Category:IBM DB2]]
[[Category:IBM software]]
[[Category:XML databases]]</text>
      <sha1>bvlgeblt5dlbnhx2wgo56lkwiep87yx</sha1>
    </revision>
  </page>
  <page>
    <title>H-Store</title>
    <ns>0</ns>
    <id>32670994</id>
    <revision>
      <id>739312237</id>
      <parentid>727316634</parentid>
      <timestamp>2016-09-13T23:59:03Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor />
      <comment>[[User:Green Cardamom/WaybackMedic 2|WaybackMedic 2]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7274" xml:space="preserve">{{Infobox software
| name                   = H-Store
| logo                   = [[File:H-Store-logo.png|80px|H-Store logo]]
| screenshot             =
| caption                =
| developer              = [[Brown University|Brown]], [[Carnegie Mellon University|CMU]], [[Massachusetts Institute of Technology|MIT]], [[Yale University|Yale]]
| latest release version = June 2016
| latest release date    = {{Start date and age|2016|06|03}}
| programming language   = [[C++]], [[Java (programming language)|Java]]
| operating system       = [[Linux]], [[Mac OS X]]
| genre                  = [[Database Management System]]
| license                = [[BSD License]], [[GPL]]
| website                = {{URL|hstore.cs.brown.edu}}
}}

'''H-Store''' is an experimental [[database management system]] (DBMS) designed for [[online transaction processing]] applications that is being developed by a team at [[Brown University]], [[Carnegie Mellon University]], the [[Massachusetts Institute of Technology]], and [[Yale University]].&lt;ref&gt;
{{cite web 
| url = http://hstore.cs.brown.edu
| title = H-Store - Next Generation OLTP DBMS Research
| accessdate = 2011-08-07
}}
&lt;/ref&gt;&lt;ref&gt;
{{cite web 
| url = http://www.dbms2.com/2008/02/18/mike-stonebraker-calls-for-the-complete-destruction-of-the-old-dbms-order/
| title = Stonebraker's H-Store: There's something happenin' here
| first = David
| last = Van Couvering

| date = 2008-02-18
| &lt;!-- is this one just plain wrong?:--&gt; publication-date = 2011-03-11
| accessdate = 2012-07-18
}}
&lt;/ref&gt;
The system's design was developed in 2007 by database researchers [[Michael Stonebraker]], [[Samuel Madden (MIT)|Sam Madden]], Andy Pavlo and Daniel Abadi.&lt;ref&gt;
{{cite conference
| authorlink = Michael Stonebraker
| first = Mike | last = Stonebraker
| title = The end of an architectural era: (it's time for a complete rewrite)
| booktitle = VLDB '07: Proceedings of the 33rd international conference on Very large data bases
| location = Vienna, Austria
| year = 2007
| url = http://hstore.cs.brown.edu/papers/hstore-endofera.pdf
| format = PDF |display-authors=etal}}&lt;/ref&gt;&lt;ref&gt;
{{cite journal
| last1 = Kallman
| first1 = Robert
| last2 = Kimura
| first2 = Hideaki
| last3 = Natkins
| first3 = Jonathan
| last4 = Pavlo
| first4 = Andrew
| last5 = Rasin
| first5 = Alexander
| last6 = Zdonik
| first6 = Stanley
| authorlink6 = Stan Zdonik
| last7 = Jones
| first7 = Evan P. C.
| last8 = Madden
| first8 = Samuel
| authorlink8 = Samuel Madden (MIT)
| last9 = Stonebraker
| first9 = Michael
| authorlink9 = Michael Stonebraker
| last10 = Zhang
| first10 = Yang
| last11 = Hugg
| first11 = John
| last12 = Abadi
| first12 = Daniel J.
| title = H-Store: a high-performance, distributed main memory transaction processing system
| journal = Proc. VLDB Endowment
| year = 2008
| volume = 1
| series = 2
| pages = 1496–1499
| url = http://hstore.cs.brown.edu/papers/hstore-demo.pdf
| issn = 2150-8097
}}&lt;/ref&gt;&lt;ref&gt;
{{cite web 
| url = http://www.dbms2.com/2008/02/18/mike-stonebraker-calls-for-the-complete-destruction-of-the-old-dbms-order/
| title = Mike Stonebraker calls for the complete destruction of the old DBMS order
| first = Curt
| last = Monash
| year = 2008
| publication-date = 2008-02-18
| accessdate  = 2012-07-18
}}
&lt;/ref&gt;

==Architecture==
The significance of the H-Store is that it is the first implementation of a new class of [[Parallel database|parallel database management systems]], called [[NewSQL]],&lt;ref&gt;{{cite web|url=http://www.cs.brown.edu/courses/cs227/papers/newsql/aslett-newsql.pdf |title=How Will The Database Incumbents Respond To NoSQL And NewSQL? |first=Matthew |last=Aslett |publisher=451 Group |publication-date=2011-04-04 |year=2010 |accessdate=2012-07-06 |deadurl=yes |archiveurl=https://web.archive.org/web/20120127202623/http://www.cs.brown.edu/courses/cs227/papers/newsql/aslett-newsql.pdf |archivedate=January 27, 2012 }}
&lt;/ref&gt;&lt;!-- good link, just not supporting H-Store directly, is supporting [[VoltDB]] that is related, but doesn not state the connection: &lt;ref&gt;
{{cite web 
| url = http://cacm.acm.org/blogs/blog-cacm/109710-new-sql-an-alternative-to-nosql-and-old-sql-for-new-oltp-apps/fulltext
| title = NewSQL: An Alternative to NoSQL and Old SQL for New OLTP Apps
| first = Michael
| last = Stonebraker
| publisher = Communications of the ACM
| publication-date = 2011-06-16
| accessdate = 2012-07-06
}}
&lt;/ref&gt; --&gt;that provide the high-throughput and high-availability of [[NoSQL]] systems, but without giving up the [[ACID|transactional guarantees]] of a traditional DBMS.&lt;ref&gt;
{{cite web 
| url = http://preferisco.blogspot.com/2008/03/h-store-new-architectural-era-or-just.html
| title = H-Store - a new architectural era, or just a toy? 
| first = Nigel
| last = Thomas

| date = 2008-03-01
| accessdate = 2012-07-05
}}
&lt;/ref&gt;
Such systems are able to scale out horizontally across multiple machines to improve throughput, as opposed to moving to a more powerful, more expensive machine for a single-node system.&lt;ref&gt;
{{cite web 
| url = http://blogs.the451group.com/information_management/2008/03/04/is-h-store-the-future-of-database-management-systems/
| title = Is H-Store the future of database management systems?
| first = Matthew
| last = Aslett

| date = 2008-03-04
| accessdate  = 2012-07-05
}}
&lt;/ref&gt;

H-Store is able to execute [[transaction processing]] with high throughput by forgoing much of legacy architecture of [[IBM System R|System R]]-like systems. For example, H-Store was designed as a [[Parallel database|parallel]], row-storage relational DBMS that runs on a cluster of [[Shared nothing architecture|shared-nothing]], main memory executor nodes.&lt;ref&gt;
{{cite web 
| url = http://hstore.cs.brown.edu/documentation/architecture-overview/
| title = H-Store - Architecture Overview
| accessdate  = 2011-08-07
}}
&lt;/ref&gt;
The database is [[Partition (database)|partitioned]] into disjoint subsets that are assigned to a single-threaded execution engine assigned to one and only one [[Multi-core processor|core]] on a node. Each engine has exclusive access to all of the data at its partition. Because it is single-threaded, only one transaction at a time is able to access the data stored at its partition. Thus, there are no physical locks or latches in the system, and no transaction will stall waiting for another transaction once it is started.&lt;ref&gt;
{{cite web 
| url = http://www.zdnet.com/blog/btl/h-store-complete-destruction-of-the-old-dbms-order/8055
| title = H-Store: Complete destruction of the old DBMS order?
| first = Larry
| last = Dignan
| year = 2008
| accessdate  = 2012-07-05
}}
&lt;/ref&gt;

==Licensing==
H-Store is licensed under the [[BSD license]] and [[GPL]] licenses. The commercial version of H-Store's design is [[VoltDB]].&lt;ref&gt;
{{cite web
| url         = http://www.dbms2.com/2009/06/22/h-store-horizontica-voltdb/
| title       = H-Store is now VoltDB
| first       = Curt
| last        = Monash
| year        = 2009
| accessdate  = 2011-07-14
| postscript  = 
}}
&lt;/ref&gt;

==See also==
{{Portal|Free software}}
*[[VoltDB]]
*[[C-Store]]
*[[Transaction processing]]

==References==
{{Reflist}}

==External links==

[[Category:Data management]]
[[Category:Distributed data stores]]
[[Category:Free database management systems]]
[[Category:NewSQL]]</text>
      <sha1>lq1itcnsejsxvpi3snd9901bl7lu1pv</sha1>
    </revision>
  </page>
  <page>
    <title>Data monetization</title>
    <ns>0</ns>
    <id>32714470</id>
    <revision>
      <id>761647211</id>
      <parentid>759776788</parentid>
      <timestamp>2017-01-24T01:18:27Z</timestamp>
      <contributor>
        <username>EdwardUK</username>
        <id>28516722</id>
      </contributor>
      <comment>Undid revision 756473370 by [[Special:Contributions/Hitesh kapasia|Hitesh kapasia]] ([[User talk:Hitesh kapasia|talk]]) test edit?</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="29588" xml:space="preserve">'''Data monetization''', a form of [[monetization]], is generating [[revenue]] from available data sources or real time streamed data by instituting the discovery, capture, storage, analysis, dissemination, and use of that data.  Said differently, it is the process by which data producers, data aggregators and data consumers, large and small, exchange sell or trade data. Data monetization leverages data generated through business operations as well as data associated with individual actors and with electronic devices and sensors participating in the [[internet of things]].  The ubiquity of the [[internet of things]] is generating [[location data]] and other data from sensors and [[mobile devices]] at an ever increasing rate. When this data is collated against traditional databases, the value and utility of both sources of data increases, leading to tremendous potential to mine data for social good, research and discovery, and achievement of business objectives.  Closely associated with data monetization are the emerging [[data as a service]] models for transactions involving data by the data item.

There are three [[ethical]] and regulatory vectors involved in data monetization due to the sometimes conflicting interests of actors involved in the [[data supply chain]].  The individual data creator who generates files and records through his own efforts or owns a device such as a sensor or a mobile phone that generates data has a claim to ownership of data.  The business entity that generates data in the course of its operations, such as its transactions with financial institutions or [[risk factors]] discovered through feedback from customers also has a claim on data captured through their systems and platforms. However, the person that contributed the data may also have a legitimate claim on the data.  Internet platforms and service providers, such as [[Google]] or [[Facebook]] that require a user to forgo some ownership interest in their data in exchange for use of the platform also have a legitimate claim on the data.  Thus the practice of data monetization, although common since 2000, is now getting increasing attention from regulators.  The [[European Union]] and the [[United States Congress]] have begun to address these issues.  For instance, in the financial services industry, regulations involving data are included in the [[Gramm–Leach–Bliley Act]] and [[Dodd-Frank]].  Some individual creators of data are shifting to using [[personal data vaults]]&lt;ref&gt;http://www.freepatentsonline.com/y2014/0032267.html&lt;/ref&gt; and implementing [[vendor relationship management]]&lt;ref&gt;[[Vendor Relationship Management]]&lt;/ref&gt; concepts as a reflection of an increasing resistance to their data being federated or aggregated and resold without compensation.  Groups such as the [[Personal Data Ecosystem Consortium]],&lt;ref&gt;http://personaldataecosystem.org&lt;/ref&gt; [[Patient Privacy Rights]],&lt;ref&gt;http://patientprivacyrights.org/&lt;/ref&gt; and others are also challenging corporate cooptation of data without compensation.

[[Financial services]] companies are a relatively good example of an industry focused on generating revenue by leveraging data.  [[Credit card]] issuers and [[retail banks]] use customer transaction data to improve targeting of [[cross-sell]] offers.  Partners are increasingly promoting merchant based [[reward programs]] which leverage a bank’s data and provide discounts to customers at the same time.

==Steps==
# Identification of available data sources – this includes data currently available for monetization as well as other external data sources that may enhance the value of what’s currently available.
# Connect, aggregate, attribute, validate, authenticate, and exchange data - this allows data to be converted directly into actionable or revenue generating insight or services.
# Set terms and prices and facilitate data trading - methods for data vetting, storage, and access. For example, many global corporations have locked and siloed data storage infrastructures, which stymies efficient access to data and cooperative and real time exchange. 
# Perform [[Research]] and [[analytics]] – draw predictive insights from existing data as a basis for using data for to reduce [[risk]], enhance product development or performance, or improve [[customer experience]] or business outcomes.
# Action and leveraging – the last phase of monetizing data includes determining alternative or improved datacentric products, ideas, or services.  Examples may include real time actionable triggered notifications or enhanced channels such as web or mobile response mechanisms.

==Pricing Variables and Factors==
*  A fee for use of a platform to connect buyers and sellers
*  A fee for use of a platform to configure, organize, and otherwise process data included in a data trade 
*  A fee for connecting or including a device or sensor into a data supply chain
*  A fee for connecting and credentialing a creator of a data source and a data buyer - often through a [[federated identity]]
*  A fee for connecting a data source to other data sources to be included into a data supply chain
*  A fee for use of an internet service or other transmission service for uploading and downloading data - sometimes, for an individual, through a [[personal cloud]]
*  A price or exchange or other trade value assigned by a data creator or generator to a data item or a data source 
*  A price or exchange or other trade value offered by a data buyer to a data creator 
*  A price or exchange or other trade value assigned by a data buyer for a data item or a data source formatted according to criteria set by a data buyer
*  An incremental fee assigned by a data buyer for a data item or a data set scaled to the reputation of the data creator
*  A fee for use of encrypted keys to achieve secure data transfer
*  A fee for use of a search algorithm specifically designed to tag data sources that contain data points of value to the data buyer
*  A fee for linking a data creator or generator to a data collection protocol or form
*  A fee for server actions - such as a notification - triggered by an update to a data item or data source included into a data supply chain

==Benefits==
*  Improved decision-making that leads to [[real time (media)|real time]] [[crowd sourced]] research, improved profits, decreased costs, reduced risk and improved compliance
*  More impactful decisions (e.g., make real time decisions)
*  More timely (lower latency) decisions (e.g., a vendor making purchase recommendations while the customer is still on the phone or in the store, a customer connecting with multiple vendors to discover a best price, triggered notifications when thresholds are reached for data values )
*  More granular decisions (e.g., localized pricing decisions at an individual or device or sensor level versus larger aggregates).

==Frameworks==
There are a wide variety of industries, firms and business models related to data monetization.  The following frameworks have been offered to help understand the types of business models that are used:

Doug Laney of [[Gartner]], a leading IT research and advisory firm, has posited a model for a range of data monetization methods:

* Indirect Data Monetization
**Using data to improve efficiencies
**Using data to measurably reduce risks
**Using data to develop new products, markets
**Using data to build and solidify partner relationships
**Publishing Branded indices
* Direct Data Monetization
**Bartering or trading with information
**Information-enhanced products or services
**Selling raw data through brokers
**Offering data/report subscriptions

He also suggests a set of feasibility tests and questions for any data monetization ideas being considered:

{| class="wikitable"
|-
! Type of Feasibility !! Feasibility Question 
|-
| Practical || Is the idea utilitarian, or merely interesting/cool? Is it usable?
|-
| Marketable || Would the idea have sufficiently broad appeal, internally or externally?
|-
| Scalable || Can the idea be developed and implemented to the extent required or intended?
|-
| Manageable || Do you have the skills to oversee the development &amp; implementation of the idea?
|-
| Technological || Do you have the tools, information and skills to develop and rollout the idea?
|-
| Economical || Will the idea require too much investment or generate sufficient return on investment?
|-
| Legal || Does the idea conform to local laws where it will be used or implemented?
|-
| Ethical || Will the idea be something that has the potential for customer/user/public backlash?
|-
| Example || Will the idea cause significant positive vs. negative impact on the environment?
|}

Roger Ehrenberg of IA Ventures, a VC firm that invests in this space has defined three basic types of data product firms:
:"'''Contributory databases'''. The magic of these businesses is that a customer provides their own data in exchange for receiving a more robust set of aggregated data back that provides insight into the broader marketplace, or provides a vehicle for expressing a view. Give a little, get a lot back in return –  a pretty compelling value proposition, and one that frequently results in a payment from the data contributor in exchange for receiving enriched, aggregated data. Once these contributory databases are developed and customers become reliant on their insights, they become extremely valuable and persistent data assets.
:
:'''Data processing platforms'''. These businesses create barriers through a combination of complex data architectures, proprietary algorithms and rich analytics to help customers consume data in whatever form they please. Often these businesses have special relationships with key data providers, that when combined with other data and processed as a whole create valuable differentiation and competitive barriers. Bloomberg is an example of a powerful data processing platform. They pull in data from a wide array of sources (including their own home grown data), integrate it into a unified stream, make it consumable via a dashboard or through an API, and offer a robust analytics suite for a staggering number of use cases. Needless to say, their scale and profitability is the envy of the industry.
:
:'''Data creation platforms'''. These businesses solve vexing problems for large numbers of users, and by their nature capture a broad swath of data from their customers. As these data sets grow, they become increasingly valuable in enabling companies to better tailor their products and features, and to target customers with highly contextual and relevant offers. Customers don’t sign up to directly benefit from the data asset; the product is so valuable that they simply want the features offered out-of-the-box. As the product gets better over time, it just cements the lock-in of what is already a successful platform. Mint was an example of this kind of business. People saw value in the core product. But the product continued to get better as more customer data was collected and analyzed. There weren’t network effects, per se, but the sheer scale of the data asset that was created was an essential element of improving the product over time."&lt;ref&gt;{{cite web|last=Ehrenberg |first=Roger |title=Creating competitive advantage through data |url=http://www.iaventures.com/creating-competitive-advantage-through-data |publisher=IA Ventures' blog |accessdate=23 November 2013 |deadurl=yes |archiveurl=https://web.archive.org/web/20131203023719/http://www.iaventures.com/creating-competitive-advantage-through-data |archivedate=3 December 2013 |df= }}&lt;/ref&gt;

Selvanathan and Zuk &lt;ref&gt;Big Data Realized: Developing New Data-Driven Products and Services to Drive Growth Perspective&lt;/ref&gt; offer a framework that includes "monetization methods that are outside the bounds of the
traditional value capture systems employed by an enterprise... tuned to match the context and consumption models for the target customer."  They offer examples of "four distinct approaches: platforms, applications, data-as-a-service, and professional services."

Ethan McCallum and Ken Gleason published an O'Rielly eBook titled ''Business Models for the Data Economy''
:Collect/Supply
:Store/Host
:Filter/Refine
:Enhance/Enrich
:Simplify Access
:Analyze
:Obscure
:Consult/Advise&lt;ref&gt;{{cite book|last=Gleason|first=Ken|title=Business Models for the Data Economy|year=2013|publisher=O'Reilly|isbn=978-1-449-37223-1|url=http://www.oreilly.com/data/free/business-models-for-the-data-economy.csp}}&lt;/ref&gt;

==Examples==
*  Packaging of data (with analytics) to be resold to customers for things such as wallet share, [[market share]] and [[benchmarking]]
*  Integration of data (with analytics) into new products as a value-added differentiator such as [[On-Star]] for [[General Motors]] cars
* [[GPS]] enabled [[smartphones]]
* [[Geolocation]]-based offers and location discounts, such as those offered by [[Facebook]]&lt;ref&gt;https://www.theguardian.com/technology/2011/jan/31/facebook-places-deals-uk-europe&lt;/ref&gt; and [[Groupon]]&lt;ref&gt;http://mashable.com/2011/05/10/groupon-now-launches/&lt;/ref&gt; are other prime examples of data monetization leveraging new emerging channels
* CRM based ad targeting and media attribution, such as those offered by Circulate

==Intellectual property landscape==
Some of the patents issued since 2010 by the [[USPTO]] for monetizing data generated by individuals include; 8,271,346, 8,612,307, 8,560,464, 8,510,176, and 7,860,760.  These are usually in the class 705 related to electronic commerce, data processing, and cost and price determination. Some of these patents use the term, the [[data supply chain]] to reflect emerging technology to federate and aggregate data in real time from many individuals and devices linked together through the [[internet of things]].  Another emerging term is [[information banking]].

An unexplored but potentially disruptive arena for data monetization is the use of [[Bitcoin micropayments]] for data transactions.  Because Bitcoins are emerging as competitors with payment services like Visa or PayPal that can readily enable and reduce or eliminate transaction costs, transactions for as little as a single data item can be facilitated. Consumers as well as enterprises who desire to monetize their participation in a data supply chain may soon be able to access social network enabled Bitcoin exchanges and platforms.&lt;ref&gt;Lomas, Natasha, Techcrunch, August 18, 2014&lt;/ref&gt;  [[Clickbait]] and data hijacking may wither as micropayments for data are ubiquitous and enabled. Potentially, even the current need to build out data broker managed data trading exchanges may be bypassed.  Stanley Smith,&lt;ref&gt;http://www.linkedin.com/pub/stan-smith/9/3ab/b37/&lt;/ref&gt; who introduced the notion of the data supply chain, has said that simple micropayments for data monetization are the key to evolution of ubiquitous implementation of user configurable data supply schemata, enabling data monetization on a universal scale for all data creators, including the burgeoning internet of things.

==Presentations and Publications ==

2016
* [https://www.gartner.com/doc/3267517 How CIOs and CDOs Can Use Infonomics to Identify, Justify and Fund Initiatives], Douglas Laney and Michael Smith, [[Gartner]] 29 March 2016
* [http://www.wsj.com/articles/accountings-21st-century-challenge-how-to-value-intangible-assets-1458605126 Accounting's 21st Century Challenge: How to Value Intangible Assets], [[WSJ]] CFO Journal, 22 March 2016 
* [https://s3.amazonaws.com/files.technologyreview.com/whitepapers/MIT_Oracle+Report-The_Rise_of_Data_Capital.pdf The Rise of Data Capital], [[Oracle Corporation|Oracle]] and [[MIT]] Technology Review Custom, 2016
* [http://www.gartner.com/smarterwithgartner/treating-information-as-an-asset/ Treating Information as an Asset], Christy Pettey, Douglas Laney and Michael M. Moran, Smarter With [[Gartner]], 17 February 2016
* [http://www.reuters.com/article/us-europe-data-competition-idUSKCN0VF0KV German competition watchdog wants 'big data' hoards considered in merger probes], [[Reuters]], 6 Feb 2016
* [https://www.gartner.com/doc/3188917 Shift From a Project to an Asset Perspective to Properly Value and Fund IT Investments], Michael Smith and Douglas Laney, [[Gartner]], 16 January 2016
* [http://www.iri.com/blog/iri/business/infonomics-and-you/ Infonomics and You], Eric Leohner, [[IRI CoSort]], January 2016
* [http://www.nowozin.net/sebastian/blog/the-fair-price-to-pay-a-spy-an-introduction-to-the-value-of-information.html The Fair Price to Pay a Spy: An Introduction to the Value of Information], Sebastian Nowozin, Nowozin.net blog, 9 January 2016 
2015
* [https://www.gartner.com/doc/3173343 Measure Your Information Yield to Maximize Return on Information and Analytics Investments], Frank Buytendijk, Andrew White, Douglas Laney and Thomas W. Oestreich, [[Gartner]], 1 December 2015
* [https://www.gartner.com/doc/3162520 IBM Storms Information, IoT Markets by Buying The Weather Company], Douglas Laney, [[Gartner]], 4 November 2015
* [https://www.gartner.com/doc/3158117 How to Adopt Open Data for Business Data and Analytics — And Why You Should], Alan D. Duncan &amp; Douglas Laney, [[Gartner]], 28 October 2015
* [https://www.gartner.com/doc/3151321 Seven Steps to Monetizing Your Information Assets], Douglas Laney, [[Gartner]], 15 October 2015
* [http://www.gartner.com/smarterwithgartner/why-and-how-to-value-your-information-as-an-asset/ Why and How to Value Your Information as an Asset] Heather Levy &amp; Douglas Laney, Smarter With [[Gartner]] blog, 3 September 2015
* [https://www.gartner.com/doc/3106721 Hackers Know the Value of Health Information, So Why Don't HDOs Appreciate Healthcare Infonomics?], Laura Craft &amp; Douglas Laney, [[Gartner]], 5 August 2015
* In August 20, 2015 [[Gartner]] Analyst Doug Laney gave a publicly-available webinar (with replay available) on [http://www.gartner.com/webinar/3098518 Methods for Monetizing Your Data]. This is a reprise of the presentation he has given at various [[Gartner]] summits and symposia around the world. 
* [https://www.gartner.com/doc/3106721 Hackers Know the Value of Health Information, So Why Don't HDOs Appreciate Healthcare Infonomics?], Laura Craft &amp; Douglas Laney, [[Gartner]], 5 August 2015
* [https://www.gartner.com/doc/3106719 Why and How to Measure the Value of Your Information Assets], Douglas Laney, [[Gartner]], 5 August 2015
* [http://prezi.com/xirqf54fix68/?utm_campaign=share&amp;utm_medium=copy&amp;rc=ex0share Applied Infonomics: Measuring the Economic Value of Information Assets], [http://www.mitcdoiq.org/ MIT Chief Data Officer Symposium], Doug Laney, [[Gartner]], 22 July 2015
* [http://www.kpmg.com/US/en/topics/data-analytics/Documents/kpmg-d-a-main-report-for-web-28-june-2015.pdf Data and Analytics: A New Driver of Performance and Valuation], [[Institutional Investor Research]] and [[KPMG]], 28 June 2015
* [http://www.thesummits.org/watch.htm?id=132356411 The Convergence of Information Economics and Economic Information] [[Corp Development Summit]] presentation replay, Doug Laney, [[Gartner]], 1 July 2015
* [http://smartdatacollective.com/rk-paleru/319941/data-opportunity-are-you-monetizing-information Data = Opportunity: But Are You Monetizing Information?] [[Smart Data Collective]], RK Paleru, 28 May 2015
* [http://blogs.gartner.com/doug-laney/keeping-busy-with-data-strategy/ Keeping Busy with Data Strategy], [[Gartner]] Blog Network, Doug Laney, 26 May 2015
* [http://blogs.wsj.com/cio/2015/05/20/dollar-value-of-data-radioshack-other-bankrupt-firms-auction-customer-data-to-pay-debt/ Dollar Value of Data: RadioShack, Other Bankrupt Firms Auction Customer Data to Pay Debt], [[Wall Street Journal]], Kim Nash, 20 May 2015
* [https://www.gartner.com/doc/3024417 The Benefits and Risks of Using Open Data], Doug Laney, [[Gartner]], 8 April 2015
* [http://goodstrat.com/2015/01/30/consider-this-does-all-data-have-value/ Consider this: Does all data have value?] Good Strategy blog, Martyn Jones, 30 January 2015
* [http://www.rsd.com/en/resources/white-papers/theory-infonomics-valuating-corporate-information-assets The Theory of Infonomics: Valuating Corporate Information Assets - white paper], [[RSD (company)|RSD]], January 2015
* [http://www.firstpost.com/business/customer-data-valuable-asset-treat-way-2046119.html Customer data is a valuable asset. Why not treat it that way?], [[F.Business]], Ajay Kelkar, 14 January 2015
* [https://www.youtube.com/watch?v=du4YVpu4VHE The Rise of Data Capital] (video), [[Oracle Corporation|Oracle]], 8 January 2015

2014
* [http://www.cmswire.com/cms/information-management/quantifying-the-value-of-your-data-026674.php Quantifying the Value of Your Data], [[CMS Wire]], Bassam Zarkout, 30 September 2014
* [http://www.rsd.com/en/blog/201409/what-infonomics What is Infonomics?], Ed Hallock, [[RSD (company)|RSD]] blog, 9 September 2014
* [http://cisr.mit.edu/blog/documents/2014/08/21/2014_0801_datamonetization_wixom.pdf/ Cashing In on Your Data], [[MIT Sloan]] Center for Information Systems Research, Barbara H. Wixom, Volume XIV, Number 8, August 2014
* [https://www.gartner.com/doc/2813227 Increase the Return on Your Information Investments With the Information Yield Curve], [[Gartner]], Andrew White and Douglas Laney, 31 July 2014
* [http://www.forbes.com/sites/gartnergroup/2014/07/21/the-hidden-shareholder-boost-from-information-assets/ The Hidden Shareholder Boost from Information Assets], [[Forbes]], Doug Laney, 21 July 2014
* [http://searchcio.bitpipe.com/data/demandEngage.action?resId=1401817861_376 CIO Decisions: The new infonomics reality: Determining the value of data], [[TechTarget]] SearchCIO, June 2014
* [http://searchcio.techtarget.com/opinion/Putting-a-price-on-information-The-nascent-field-of-infonomics Putting a price on information: The nascent field of infonomics], [[TechTarget]] SearchCIO, Linda Tucci, 13 May 2014
* [http://searchcio.techtarget.com/feature/Six-ways-to-measure-the-value-of-your-information-assets Six ways to measure the value of your information assets], [[TechTarget]] SearchCIO, Nicole Laskowski, 13 May 2014
* [http://searchcio.techtarget.com/feature/Infonomics-treats-data-as-a-business-asset Infonomics treats data as a business asset], [[TechTarget]] SearchCIO, Nicole Laskowski, 13 May 2014
* [http://pv.tl/blog/2014/04/13/the-economics-of-information-management/?utm_content=buffer29c67&amp;utm_medium=social&amp;utm_source=twitter.com&amp;utm_campaign=buffer, The economics of information management], PVTL Blog, Felix Barbalet, 13 April 2014
* [http://www.forbes.com/sites/gartnergroup/2014/03/27/the-hidden-tax-advantage-of-monetizing-your-data/ The Hidden Tax Advantage of Monetizing Your Data], [[Forbes]], Doug Laney, 27 March 2014
* [http://blogs.teradata.com/anz/the-chief-data-officer-managing-the-value-of-data/#comment-2147 The Chief Data Officer – Managing the Value of Data], [[Teradata]] ANZ Blog, Renato Manongdo, March 2014
* [https://www.gartner.com/doc/2677518 How Organizations Can Monetize Customer Data], [[Gartner]], Olive Huang, Doug Laney, 6 March 2014
* [https://www.gartner.com/doc/2677515?ref=QuickSearch&amp;sthkw=infonomics Improving the Value of Customer Data Through Applied Infonomics], [[Gartner]] Research Publication, Douglas Laney, Olive Huang, 6 March 2014
* [http://blogs.gartner.com/andrew_white/2014/02/14/information-value-accrual-and-its-asymmetry/ Information Value Accrual and Its Asymmetry], [[Gartner]] Blog Network, Andrew White, 14 February 2014
* [http://blogs.gartner.com/andrew_white/2014/01/29/does-information-utility-suffer-a-half-life/ Does Information Utility Suffer a Half Life?], [[Gartner]] Blog Network, Andrew White, 29 January 2014

2013
* [http://www.rsd.com/en/blog/201312/what-information-information-governance What is the "Information" in "Information Governance"?], [[RSD (company)|RSD]] Blog, James Amsler, 30 December 2013
* [http://blogs.gartner.com/doug-laney/to-twitter-youre-worth-101-70/ To Twitter You're Worth $101.70] [[Gartner]] Blog Network, by Douglas Laney, 12 November 2013
* [http://www.economistgroup.com/leanback/big-data-2/treat-data-like-money Treat data like money. CMO's Advice: Marketers must develop an investment strategy for data], [[The Economist Group]], Jim Davis, SVP &amp; CMO, SAS, October 2013
* [http://www.ft.com/intl/cms/s/0/205ddf5c-1bf0-11e3-b678-00144feab7de.html#axzz2g8PCOrV3 Infonomics: The New Economics of Information], [[The Financial Times]], Doug Laney, VP Research, Gartner, September 2013
* [https://www.youtube.com/watch?v=mQk_5Q3VJv4 Value of Information], GigaOM presentation by Dave McCrory, SVP at [[Warner Music Group]], July 2013   
* [http://www.bankingtech.com/147432/accounting-for-the-value-of-big-data/ Accounting for the value of (big) data], [[Banking Technology Magazine]], David Bannister, 11 June 2013
* [http://searchcio.techtarget.com/opinion/Putting-a-price-on-information-The-nascent-field-of-infonomics Putting a price on information: The nascent field of infonomics], [[SearchCIO]] Journal, Linda Tucci, May 2013
* On March 19, 2013 the Chicago Chapter of the [[Product Development and Management Association]] (PDMA) held an event titled "Monetizing Data: An Evening with Eight of Chicago's Data Product Management Leaders"&lt;ref&gt;http://www.builtinchicago.org/blog/check-out-ppt-deck-monetizing-data-evening-eight-chicagos-data-product-management-leaders&lt;/ref&gt;

2012
* [http://www.informationweek.com/big-data/news/big-data-analytics/whats-your-big-data-worth/240144449 What's Your Big Data Worth], [[InformationWeek]], Ellis Booker, 17 December 2012
* [https://www.gartner.com/doc/2278915 Future of Money: Infonomics Monetizing Value in Big Data Information Assets], Mary Knox, [[Gartner]], 14 December 2012
* [http://www.information-age.com/technology/information-management/2134803/an-introduction-to-infonomics An Introduction to Infonomics], [[InformationAge]], Pete Swabey, 26 November 2012
* [https://www.gartner.com/doc/2186116 The Birth of Infonomics: the New Economics of Information], [[Gartner]] research publication, Douglas Laney, 3 October 2012 (public summary, full text available to Gartner clients)
* [http://blogs.gartner.com/doug-laney/tobins-q-a-evidence-of-informations-real-market-value-2/ Tobin’s Q &amp; A: Evidence of Information’s Real Market Value], [[Gartner]] Blog Network, Douglas Laney, 14 Aug 2012
* [http://www.ft.com/intl/cms/s/0/27476ad4-a6a5-11e1-968b-00144feabdc0.html#axzz1vzOCxVYw Extracting Value from Information], [[Financial Times]], interview with Douglas Laney by Paul Taylor, 25 May 2012 (free registration required)
* [http://www.forbes.com/sites/gartnergroup/2012/05/22/infonomics-the-practice-of-information-economics/ Infonomics: The Practice of Information Economics], [[Forbes]], by Douglas Laney, 22 May 2012
* [http://blogs.wsj.com/cio/2012/05/03/to-facebook-youre-worth-80-95/?mod=wsjcio_hps_cioreport# To Facebook You're Worth $80.95], [[Wall Street Journal]], by Douglas Laney, 3 May 2012
* [https://www.gartner.com/doc/1958016 Introducing Infonomics: Valuing Information as a Corporate Asset], [[Gartner]] research publication, Douglas Laney, 21 March 2012 (public summary, full text available to Gartner clients)
* [http://www.ijikm.org/Volume7/IJIKMv7p177-199Evans0650.pdf Barriers to the Effective Deployment of Information Assets: An Executive Management Perspective], [[Interdisciplinary Journal of Information, Knowledge, and Management]], Nina Evans and James Price, Volume 7, 2012

Older
* [http://imcue.com/wp-content/uploads/2011/06/What-is-EIM.pdf/ What is Enterprise Information Management (EIM)] by John Ladley, Morgan Kaufmann, 2010
* [http://blogs.informatica.com/perspectives/2010/01/26/data-as-an-asset/#comment-1072 Data as an Asset] blog series by John Schmidt, 2010
* [http://www.amazon.com%2FInformation-Driven-Business-Information-Maximum-Advantage%2Fdp%2F0470625775%2Fref%3Dsr_1_1%3Fie%3DUTF8%26s%3Dbooks%26qid%3D1267263302%26sr%3D8-1/ Information Driven Business: How to Manage Data and Information for Maximum Advantage] by Rob Hillard, Wiley 2010
* [http://www.amazon.com%2FHow-Measure-Anything-Intangibles-Business%2Fdp%2F0470539399%2Fref%3Dsr_1_1%3Fs%3Dbooks%26ie%3DUTF8%26qid%3D1316207185%26sr%3D1-1/ How to Measure Anything: Finding the Value of Intangibles in Business] by Douglas W. Hubbard, Wiley 2010
* [http://www.amazon.com%2FIntangible-Assets-Valuation-Economic-Benefit%2Fdp%2F0471671312%2Fref%3Dsr_1_3%3Fs%3Dbooks%26ie%3DUTF8%26qid%3D1316206576%26sr%3D1-3/ Intangible Assets: Valuation and Economic Benefit] by Jeffrey A. Cohen, Wiley 2005
* [http://www.amazon.com%2FValue-Driven-Intellectual-Capital-Intangible%2Fdp%2F0471351040%2Fref%3Dsr_1_1%3Fs%3Dbooks%26ie%3DUTF8%26qid%3D1316206774%26sr%3D1-1/ Value Driven Intellectual Capital: How to Convert Intangible Corporate Assets Into Market Value] by Patrick H. Sullivan, Wiley, 2000
* [http://www.vldb.org/conf/1998/p641.pdf Bank of America Case Study: The Information Currency Advantage], [[Teradata]], Felipe Carino and Mark Jahnke, Proceedings of the 24th VLDB Conference, New York, NY, 1998
* [http://www.amazon.com/Information-Payoff-Transformation-Work-Electronic/dp/0029317207/ Information Payoff: The Transformation of Work in the Electronic Age] by Paul A. Strassmann, The Free Press, 1985

== See also ==
*[[Infonomics]]
*[[Monetization]]
*[[Business intelligence]]
*[[Analytics]]
*[[Bitcoin]]
*[[Data as a service]]

== References ==
{{Reflist|33em}}

[[Category:Articles created via the Article Wizard]]
[[Category:Data management]]</text>
      <sha1>fw61ylki6p9q12t2rqxpb4fqbwl051t</sha1>
    </revision>
  </page>
  <page>
    <title>Contrast set learning</title>
    <ns>0</ns>
    <id>34055690</id>
    <revision>
      <id>722587737</id>
      <parentid>721153417</parentid>
      <timestamp>2016-05-29T03:21:41Z</timestamp>
      <contributor>
        <username>Dcirovic</username>
        <id>11795905</id>
      </contributor>
      <minor />
      <comment>clean up using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="16164" xml:space="preserve">'''Contrast set learning''' is a form of [[association rule learning]] that seeks to identify meaningful differences between separate groups by reverse-engineering the key predictors that identify for each particular group. For example, given a set of attributes for a pool of students (labeled by degree type), a contrast set learner would identify the ''contrasting'' features between students seeking bachelor's degrees and those working toward PhD degrees.

== Overview ==

A common practice in [[data mining]] is to [[Statistical classification|classify]], to look at the attributes of an object or situation and make a guess at what category the observed item belongs to. As new evidence is examined (typically by feeding a ''training set'' to a learning [[algorithm]]), these guesses are reﬁned and improved. Contrast set learning works in the opposite direction. While classiﬁers read a collection of data and collect information that is used to place new data into a series of discrete categories, contrast set learning takes the category that an item belongs to and attempts to reverse engineer the statistical evidence that identifies an item as a member of a class. That is, contrast set learners seek rules associating attribute values with changes to the class distribution.&lt;ref name="bay01"&gt;{{cite journal
 |author1=Stephen Bay |author2=Michael Pazzani | year = 2001
 | title = Detecting group differences: Mining contrast sets
 | journal = Data Mining and Knowledge Discovery
 | volume = 5
 | issue= 3
 | pages= 213–246
 | url= http://wotan.liu.edu/docis/lib/musl/rclis/dbl/dmiknd/(2001)5%253A3%253C213%253ADGDMCS%253E/www.isle.org%252F~sbay%252Fpapers%252Fstucco.dmkd.pdf
 }}
&lt;/ref&gt; They seek to identify the key predictors that contrast one classification from another.

For example, an aerospace engineer might record data on test launches of a new rocket. Measurements would be taken at regular intervals throughout the launch, noting factors such as the trajectory of the rocket, operating temperatures, external pressures, and so on. If the rocket launch fails after a number of successful tests, the engineer could use contrast set learning to distinguish between the successful and failed tests. A contrast set learner will produce a set of association rules that, when applied, will indicate the key predictors of each failed tests versus the successful ones (the temperature was too high, the wind pressure was too high, etc.).

Contrast set learning is a form of [[association rule learning]].&lt;ref name="webb03"&gt;{{cite conference
 |author1=GI Webb |author2=S. Butler |author3=D. Newlands | year = 2003
 | title = On Detecting Differences Between Groups
 | conference = KDD'03 Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
| url= http://portal.acm.org/citation.cfm?id=956781
 }}
&lt;/ref&gt; Association rule learners typically offer rules linking attributes commonly occurring together in a training set (for instance, people who are enrolled in four-year programs and take a full course load tend to also live near campus). Instead of ﬁnding rules that describe the current situation, contrast set learners seek rules that differ meaningfully in their distribution across groups (and thus, can be used as predictors for those groups).&lt;ref name="bay99"&gt;{{cite conference
 |author1=Stephen Bay |author2=Michael Pazzani | year = 1999
 | title = Detecting change in categorical data: mining contrast sets
 | conference = KDD '99 Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining
 }}
&lt;/ref&gt; For example, a contrast set learner could ask, “What are the key identifiers of a person with a bachelor's degree or a person with a PhD, and how do people with PhD's and bachelor’s degrees differ?”

Standard [[Classification in machine learning|classifier]] algorithms, such as [[C4.5]], have no concept of class importance (that is, they do not know if a class is "good" or "bad"). Such learners cannot bias or filter their predictions towards certain desired classes. As the goal of contrast set learning is to discover meaningful differences between groups, it is useful to be able to target the learned rules towards certain classifications. Several contrast set learners, such as MINWAL&lt;ref name="cai98"&gt;{{cite conference
 |author1=C.H. Cai |author2=A.W.C. Fu |author3=C.H. Cheng |author4=W.W. Kwong | year = 1998
 | title = Mining association rules with weighted items
 | conference = Proceedings of International Database Engineering and Applications Symposium (IDEAS 98)
 | url= http://appsrv.cse.cuhk.edu.hk/~kdd/assoc_rule/paper_chcai.pdf
 }}
&lt;/ref&gt; or the family of TAR algorithms,&lt;ref name="hu03"/&gt;&lt;ref name="burlet07"&gt;{{cite conference
 |author1=K. Gundy-Burlet |author2=J. Schumann |author3=T. Barrett |author4=T. Menzies | year = 2007
 | title = Parametric analysis of ANTARES re-entry guidance algorithms using advanced test generation and data analysis
 | conference = In 9th International Symposium on Artiﬁcial Intelligence, Robotics and Automation in Space
 }}
&lt;/ref&gt;&lt;ref name="gay10"&gt;{{cite journal
 |author1=Gregory Gay |author2=Tim Menzies |author3=Misty Davies |author4=Karen Gundy-Burlet | year = 2010
 | title = Automatically Finding the Control Variables for Complex System Behavior
 | journal = Automated Software Engineering
 | volume = 17
 | issue= 4
 | url= http://www.greggay.com/pdf/10tar3.pdf 
 }}
&lt;/ref&gt; assign weights to each class in order to focus the learned theories toward outcomes that are of interest to a particular audience. Thus, contrast set learning can be though of as a form of weighted class learning.&lt;ref name="menzies03"&gt;{{cite journal
 |author1=T. Menzies |author2=Y. Hu | year = 2003
 | title = Data Mining for Very Busy People
 | journal = IEEE Computer
 | volume = 36
 | issue= 11
 | pages= 22–29
 | url= http://menzies.us/pdf/03tar2.pdf
 | doi=10.1109/mc.2003.1244531
 }}
&lt;/ref&gt;

=== Example: Supermarket Purchases ===

The differences between standard classification, association rule learning, and contrast set learning can be illustrated with a simple supermarket metaphor. In the following small dataset, each row is a supermarket transaction and each "1" indicates that the item was purchased (a "0" indicates that the item was not purchased):

{| class="wikitable"
|-
! ''Hamburger'' !! ''Potatoes'' !! ''Foie Gras'' !! ''Onions'' !! ''Champagne'' !! ''Purpose of Purchases''
|-
| 1 || 1 || 0 || 1 || 0 || Cookout
|-
| 1 || 1 || 0 || 1 || 0 || Cookout
|-
| 0 || 0 || 1 || 0 || 1 || Anniversary
|-
| 1 || 1 || 0 || 1 || 0 || Cookout
|-
| 1 || 1 || 0 || 0 || 1 || Frat Party
|}

Given this data,
* Association rule learning may discover that customers that buy onions and potatoes together are likely to also purchase hamburger meat.
* Classification may discover that customers that bought onions, potatoes, and hamburger meats were purchasing items for a cookout.
* Contrast set learning may discover that the major difference between customers shopping for a cookout and those shopping for an anniversary dinner are that customers acquiring items for a cookout purchase onions, potatoes, and hamburger meat (and ''do not purchase'' foie gras or champagne).

== Treatment learning ==

&lt;!--[[File:TreatmentLearningExample.png|thumb|200px|Example of a treatment produced based on data collected while riding a bicycle. This treatment states that an optimal riding speed can be obtained while the hill slope is constrained to between −10 and 0% and the cadence is between 1.4 and 2.5. {{deletable image-caption|date=December 2011}}]]--&gt;

Treatment learning is a form of weighted contrast-set learning that takes a single ''desirable'' group and contrasts it against the remaining ''undesirable'' groups (the level of desirability is represented by weighted classes).&lt;ref name="hu03"&gt;{{cite book
 | author = Y. Hu
 | year = 2003
 | title = Treatment learning: Implementation and application
 | type= Master's thesis
 | publisher=Department of Electrical Engineering, University of British Columbia
 }}
&lt;/ref&gt; The resulting "treatment" suggests a set of rules that, when applied, will lead to the desired outcome.

Treatment learning differs from standard contrast set learning through the following constraints:
* Rather than seeking the differences between all groups, treatment learning specifies a particular group to focus on, applies a weight to this desired grouping, and lumps the remaining groups into one "undesired" category.
* Treatment learning has a stated focus on minimal theories. In practice, treatment are limited to a maximum of four constraints (i.e., rather than stating all of the reasons that a rocket differs from a skateboard, a treatment learner will state one to four major differences that predict for rockets at a high level of statistical significance).

This focus on simplicity is an important goal for treatment learners. Treatment learning seeks the ''smallest'' change that has the ''greatest'' impact on the class distribution.&lt;ref name="menzies03"/&gt;

Conceptually, treatment learners explore all possible subsets of the range of values for all attributes. Such a search is often infeasible in practice, so treatment learning often focuses instead on quickly pruning and ignoring attribute ranges that, when applied, lead to a class distribution where the desired class is in the minority.&lt;ref name="gay10"/&gt;

=== Example: Boston housing data ===

The following example demonstrates the output of the treatment learner TAR3 on a dataset of housing data from the city of [[Boston]] (a nontrivial public dataset with over 500 examples). In this dataset, a number of factors are collected for each house, and each house is classified according to its quality (low, medium-low, medium-high, and high). The ''desired'' class is set to "high", and all other classes are lumped together as undesirable.

The output of the treatment learner is as follows:

&lt;code&gt;
 Baseline class distribution:
 low: 29%
 medlow: 29%
 medhigh: 21%
 high: 21%

 Suggested Treatment: [PTRATIO=[12.6..16), RM=[6.7..9.78)]

 New class distribution:
 low: 0%
 medlow: 0%
 medhigh: 3%
 high: 97%
&lt;/code&gt;

With no applied treatments (rules), the desired class represents only 21% of the class distribution. However, if one filters the data set for houses with 6.7 to 9.78 rooms and a neighborhood parent-teacher ratio of 12.6 to 16, then 97% of the remaining examples fall into the desired class (high-quality houses).

== Algorithms ==

There are a number of algorithms that perform contrast set learning. The following subsections describe two examples.

=== STUCCO ===

The STUCCO contrast set learner&lt;ref name="bay01"/&gt;&lt;ref name="bay99"/&gt; treats the task of learning from contrast sets as a [[Tree traversal|tree search]] problem where the root node of the tree is an empty contrast set. Children are added by specializing the set with additional items picked through a canonical ordering of attributes (to avoid visiting the same nodes twice). Children are formed by appending terms that follow all existing terms in a given ordering. The formed tree is searched in a breadth-first manner. Given the nodes at each level, the dataset is scanned and the support is counted for each group. Each node is then examined to determine if it is significant and large, if it should be pruned, and if new children should be generated. After all significant contrast sets are located, a post-processor selects a subset to show to the user - the low order, simpler results are shown first, followed by the higher order results which are "surprising and significantly different.&lt;ref name="bay99"/&gt;"

The support calculation comes from testing a null hypothesis that the contrast set support is equal across all groups (i.e., that contrast set support is ''independent of group membership''). The support count for each group is a frequency value that can be analyzed in a contingency table where each row represents the truth value of the contrast set and each column variable indicates the group membership frequency. If there is a difference in proportions between the contrast set frequencies and those of the null hypothesis, the algorithm must then determine if the differences in proportions represent a relation between variables or if it can be attributed to random causes. This can be determined through a [[Chi-squared test|chi-square test]] comparing the observed frequency count to the expected count.

Nodes are pruned from the tree when all specializations of the node can never lead to a significant and large contrast set. The decision to prune is based on:
* The minimum deviation size: The maximum difference between the support of any two groups bust be greater than a user-specified threshold.
* Expected cell frequencies: The expected cell frequencies of a contingency table can only decrease as the contrast set is specialized. When these frequencies are too small, the validity of the chi-square test is violated.
* &lt;math&gt;\chi^2&lt;/math&gt; bounds: An upper bound is kept on the distribution of a statistic calculated when the null hypothesis is true. Nodes are pruned when it is no longer possible to meet this cutoff.

=== TAR3 ===

The TAR3&lt;ref name="burlet07"/&gt;&lt;ref name="schumann09"&gt;{{cite conference
 |author1=J. Schumann |author2=K. Gundy-Burlet |author3=C. Pasareanu |author4=T. Menzies |author5=A. Barrett | year = 2009
 | title = Software V&amp;V support by parametric analysis of large software simulation systems
 | conference = Proceedings of the 2009 IEEE Aerospace Conference
 }}
&lt;/ref&gt; weighted contrast set learner is based on two fundamental concepts - the '''lift''' and '''support''' of a rule set.

The lift of a set of rules is the change that some decision makes to a set of examples after imposing that decision (i.e., how the class distribution shifts in response to the imposition of a rule). TAR3 seeks the smallest set of rules which induces the biggest changes in the sum of the weights attached to each class multiplied by the frequency at which each class occurs. The lift is calculated by dividing the score of the set in which the set of rules is imposed by the score of the baseline set (i.e., no rules are applied). Note that by reversing the lift scoring function, the TAR3 learner can also select for the remaining classes and reject the target class.

It is problematic to rely on the lift of a rule set alone. Incorrect or misleading data noise, if correlated with failing examples, may result in an overfitted rule set. Such an overfitted model may have a large lift score, but it does not accurately reﬂect the prevailing conditions within the dataset. To avoid overfitting, TAR3 utilizes a support threshold and rejects all rules that fall on the wrong side of this threshold. Given a target class, the support threshold is a user-supplied value (usually 0.2) which is compared to the ratio of the frequency of the target class when the rule set has been applied to the frequency of that class in the overall dataset. TAR3 rejects all sets of rules with support lower than this threshold.

By requiring both a high lift and a high support value, TAR3 not only returns ideal rule sets, but also favors smaller sets of rules. The fewer rules adopted, the more evidence that will exist supporting those rules.

The TAR3 algorithm only builds sets of rules from attribute value ranges with a high heuristic value. The algorithm determines which ranges to use by ﬁrst determining the lift score of each attribute’s value ranges. These individual scores are then sorted and converted into a cumulative probability distribution. TAR3 randomly selects values from this distribution, meaning that low-scoring ranges are unlikely to be selected. To build a candidate rule set, several ranges are selected and combined. These candidate rule sets are then scored and sorted. If no improvement is seen after a user-defined number of rounds, the algorithm terminates and returns the top-scoring rule sets.

== References ==
{{Reflist}}

{{DEFAULTSORT:Contrast Set Learning}}
[[Category:Data management]]
[[Category:Data mining]]</text>
      <sha1>ej44g5zxcmej82tux2g7tgeneull81r</sha1>
    </revision>
  </page>
  <page>
    <title>Machine-readable data</title>
    <ns>0</ns>
    <id>34578263</id>
    <revision>
      <id>738689561</id>
      <parentid>738451281</parentid>
      <timestamp>2016-09-10T13:56:49Z</timestamp>
      <contributor>
        <username>Owen Ambur</username>
        <id>3035628</id>
      </contributor>
      <comment>/* See also */ Added link to Machine-Readable Documents</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3212" xml:space="preserve">'''Machine-readable data''' is [[data]] (or [[metadata]]) which is in a format that can be understood by a [[computer]].

There are two types; human-readable data that is [[markup language|marked up]] so that it can also be read by machines (examples; [[microformat]]s, [[RDFa]], [[HTML]]) or data file formats intended principally for processing by machines ([[Resource Description Framework|RDF]], [[XML]], [[JSON]]).

''Machine readable'' is not synonymous with ''digitally accessible''.  A digitally accessible document may be online, making it easier for a human to access it via a computer, but unless the relevant data is available in a machine readable format, it will be much harder to use the computer to extract, transform and process that data.&lt;ref&gt;{{cite web
 | url=https://www.data.gov/developers/blog/primer-machine-readability-online-documents-and-data
 | title=A Primer on Machine Readability for Online Documents and Data
 | work=Data.gov
 | date=2012-09-24
 | accessdate=2015-02-27 }}
&lt;/ref&gt;

For purposes of implementation of the [[Government Performance and Results Act]] (GPRA) Modernization Act, the [[Office of Management and Budget]] (OMB) defines "machine readable" as follows:  "Format in a standard computer language (not English text) that can be read automatically by a web browser or computer system. (e.g.; xml). Traditional word processing documents and portable document format (PDF) files are easily read by humans but typically are difficult for machines to interpret. Other formats such as extensible markup language (XML), (JSON), or spreadsheets with header columns that can be exported as comma separated values (CSV) are machine readable formats. As HTML is a structural markup language, discreetly labeling parts of the document, computers are able to gather document components to assemble Tables of Content, outlines, literature search bibliographies, etc. It is possible to make traditional word processing documents and other formats machine readable but the documents must include enhanced structural elements."&lt;ref&gt;[http://www.whitehouse.gov/sites/default/files/omb/assets/a11_current_year/s200.pdf OMB Circular A-11, Part 6], Preparation and Submission of Strategic Plans, Annual Performance Plans, and Annual Program Performance Reports&lt;/ref&gt;

==References==
{{reflist}}

==See also==

* [[Open data]]
* [[Linked data]]
* [[Machine-Readable Documents]]
* [[Human-readable medium]]
* [http://xml.fido.gov/stratml/references/PL111-532StratML.htm#SEC10 Section 10] of the [[Government Performance and Results Act|GPRA]] Modernization Act (GPRAMA), which requires U.S. federal agencies to publish their strategic and performance plans and reports in machine-readable format, like [[Strategy Markup Language]] (StratML)
* President Obama's [http://xml.fido.gov/stratml/carmel/EOOMRDwStyle.xml Executive Order] Making Open and Machine Readable the New Default for Government Information
* [http://xml.fido.gov/stratml/carmel/M-13-13wStyle.xml#_78e85ef4-b91c-11e2-bf2b-79d279ad226c OMB M-13-13], Open Data Policy: Managing Information as an Asset, which requires agencies to use open, machine-readable, data format standards
[[Category:Data management]]


{{Comp-stub}}</text>
      <sha1>ccg3hblttqhhqn9beqcvzh1of5zzqrk</sha1>
    </revision>
  </page>
  <page>
    <title>National Information Governance Board for Health and Social Care</title>
    <ns>0</ns>
    <id>34419120</id>
    <revision>
      <id>654169819</id>
      <parentid>653808128</parentid>
      <timestamp>2015-03-30T12:08:55Z</timestamp>
      <contributor>
        <username>PanchoS</username>
        <id>343908</id>
      </contributor>
      <comment>removed [[Category:Health informatics]]; added [[Category:Medical privacy]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6611" xml:space="preserve">The '''National Information Governance Board for Health and Social Care''' (NIGB) was established in the [[United Kingdom]] under section 157&lt;ref&gt;[http://www.legislation.gov.uk/ukpga/2008/14/section/157 Section 157 of the Health and Social Care Act 2008]&lt;/ref&gt; of the Health and Social Care Act 2008, with effect from October 2008, with a range of advisory functions relating to [[information governance]].  From January 2009, the NIGB also gained functions under section 251&lt;ref&gt;[http://www.legislation.gov.uk/ukpga/2006/41/section/251 Section 251 of the NHS Act 2006]&lt;/ref&gt; of the NHS Act 2006 which had previously been held by the [[Patient Information Advisory Group]] (PIAG) until its abolition. These functions were to advise the [[Secretary of State for Health]] on the use of powers to set aside the common law duty of confidentiality in [[England]] where identifiable patient information is needed and where consent is not practicable. From 1 April 2013, the NIGB's functions for monitoring and improving information governance practice have transferred to the [[Care Quality Commission]], which established a National Information Governance Committee to oversee this work. Functions relating to section 251 of the [[National Health Service Act 2006|NHS Act 2006]] (access to people’s personal and confidential information for research purposes) were transferred to the [[Health Research Authority]]'s Confidentiality Advisory Group.&lt;ref&gt;[http://webarchive.nationalarchives.gov.uk/20130513181011/http://www.nigb.nhs.uk/ Webarchive page of the National Information Governance Board for Health and Social Care]&lt;/ref&gt;

==Terms of reference==

The key functions of the NIGB (excerpted from the legislation) were:
&lt;ol type="a"&gt;
&lt;li&gt; to monitor the practice followed by relevant bodies in relation to the processing of relevant information;&lt;/li&gt;
&lt;li&gt; to keep the [[Secretary of State for Health]], and such bodies as the [[Secretary of State for Health]] may designate by direction, informed about the practice being followed by relevant bodies in relation to the processing of relevant information;&lt;/li&gt;
&lt;li&gt; to publish guidance on the practice to be followed in relation to the processing of relevant information;&lt;/li&gt;
&lt;li&gt; to advise the [[Secretary of State for Health]] on particular matters relating to the processing of relevant information by any person; and&lt;/li&gt;
&lt;li&gt; to advise persons who process relevant information on such matters relating to the processing of relevant information by them as the [[Secretary of State for Health]] may from time to time designate by direction.&lt;/li&gt;
&lt;/ol&gt;

The definition of “relevant information” in the legislation covers patient information, any other information obtained or generated in the course of the provision of the health service, and any information obtained or generated in the course of the exercise by a local social services authority in [[England]] of its adult social services functions.

==Ethics and Confidentiality Committee==

Some areas of NIGB functions (d) and (e) above had been delegated to the NIGB’s Ethics and Confidentiality Committee (ECC).  These functions primarily related to applications to use identifiable patient information without consent, in specific circumstances within the bounds of section 251 of the NHS Act 2006.  These applications, which had been considered by PIAG before the NIGB, passed on to the [[Health Research Authority]]'s Confidentiality Advisory Group (CAG) on 1 April 2013.

==Care Record Development Board==

The NIGB had also replaced the Care Record Development Board (CRDB),&lt;ref&gt;[http://www.connectingforhealth.nhs.uk/crdb Care Record Development Board archive page hosted by NHS Connecting for Health]&lt;/ref&gt; which had closed in September 2007. The NIGB had subsequently maintained the NHS Care Record Guarantee which was originally developed by the CRDB and developed a companion Social Care Record Guarantee.

==Members==

The NIGB had consisted of a Chair, a number of Public Members appointed by the NHS Appointments Commission, and a number of Representative Members appointed by the [[Secretary of State for Health]] from a range of stakeholder organisations.  Representatives of several other stakeholder organisations had served as Corresponding Advisers to the NIGB but had not typically attended meetings.  Regular observers at meetings had included representatives from the [[Information Commissioner's Office]] and the devolved UK administrations.

The ECC had consisted of a Chair and a number of Members, all of whom had been appointed by the NIGB with advice from an NHS Appointments Commission approved independent assessor.  The ECC Chair and two ECC Members had also been NIGB Members.

Between 1 June 2011 and 31 March 2013 Dame [[Fiona Caldicott]]&lt;ref&gt;[http://www.connectingforhealth.nhs.uk/newsroom/news/nigbchair Appointment of Fiona Caldicott as new NIGB Chair - press release on NHS Connecting for Health website, June 2011]&lt;/ref&gt; had been Chair of the NIGB, succeeding [[Harry Cayton]] who had chaired the NIGB since its inception.

==Geography==

Members of the NIGB and ECC had been widely distributed nationally but had attended meetings at the NIGB office.  Since September 2011, this had been based at [[Skipton House]], London SE1.  The NIGB’s staff team had been predominantly based at this office.

==Abolition==

As a result of the [[Health and Social Care Act 2012]] the NIGB was abolished with effect from 1 April 2013. The functions delegated to the ECC with respect to research transferred to the [[Health Research Authority]].&lt;ref&gt;[http://www.hra.nhs.uk/news/2012/12/17/further-update-on-transfer-of-s251-function-from-nigb-to-hra/ Transfer of s251 function from NIGB to HRA]&lt;/ref&gt; The [[NHS Commissioning Board]] is now responsible for providing advice and guidance to NHS bodies. Other functions were transferred to the National Information Governance Committee hosted by the [[Care Quality Commission]].

==References==

{{Reflist}}

==External links==
*[http://webarchive.nationalarchives.gov.uk/20130513181011/http://www.nigb.nhs.uk/ NIGB website (archived)]
*[http://www.hra.nhs.uk/ NHS Health Research Authority]
*[http://www.cqc.org.uk/ Care Quality Commission]
*[http://www.nres.nhs.uk National Research Ethics Service]
*[http://www.commissioningboard.nhs.uk/ NHS Commissioning Board]


[[Category:Data management]]
[[Category:Medical privacy]]
[[Category:2008 establishments in the United Kingdom]]
[[Category:Organizations established in 2008]]
[[Category:Governance in the United Kingdom]]
[[Category:Social care in the United Kingdom]]</text>
      <sha1>syiywe3ai2sjykaj74ia2rp6zc0qg0n</sha1>
    </revision>
  </page>
  <page>
    <title>Data grid</title>
    <ns>0</ns>
    <id>35951900</id>
    <revision>
      <id>753458812</id>
      <parentid>748820489</parentid>
      <timestamp>2016-12-07T08:01:35Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 6 sources and tagging 3 as dead. #IABot (v1.2.7.1)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="42762" xml:space="preserve">[[File:High Level View Data Grid V1.jpg|200px|right|High Level View Data Grid Topology]]

A '''data grid''' is an [[architecture]] or set of services that gives individuals or groups of users the ability to access, modify and transfer extremely large amounts of geographically distributed [[data]] for research purposes.&lt;ref&gt;Allcock, Bill; Chervenak, Ann; Foster, Ian; et al. Data Grid tools: enabling science on big distributed data&lt;/ref&gt; Data grids make this possible through a host of [[middleware]] [[Application software|applications]] and [[Service (systems architecture)|services]] that pull together data and [[Resource (computer science)|resources]] from multiple [[administrative domain]]s and then present it to users upon request. The data in a data grid can be located at a single site or multiple sites where each site can be its own administrative domain governed by a set of security restrictions as to who may access the data.&lt;ref&gt;Venugopal, Srikumar; Buyya, Rajkumar; Ramamohanarao, Kotagiri. A taxonomy of data grids for distributed data sharing - management and processing p.37&lt;/ref&gt; Likewise, multiple [[replica]]s of the data may be distributed throughout the grid outside their original administrative domain and the security restrictions placed on the original data for who may access it must be equally applied to the replicas.&lt;ref&gt;Shorfuzzaman, Mohammad; Graham, Peter; Eskicioglu, Rasit. Adaptive replica placement in hierarchical data grids. p.15&lt;/ref&gt; Specifically developed data grid middleware is what handles the integration between users and the data they request by controlling access while making it available as efficiently as possible. The diagram to the right depicts a high level view of a data grid.

==Middleware==
Middleware provides all the services and applications necessary for efficient management of [[dataset]]s and [[Computer file|files]] within the data grid while providing users quick access to the datasets and files.&lt;ref&gt;Padala, Pradeep. A survey of data middleware for Grid systems p.1&lt;/ref&gt; There are a number of concepts and tools that must be available to make a data grid operationally viable. However, at the same time not all data grids require the same capabilities and services because of differences in access requirements, security and location of resources in comparison to users. In any case, most data grids will have similar middleware services that provide for a universal [[namespace|name space]], data transport service, data access service, data replication and resource management service. When taken together, they are key to the data grids functional capabilities.

===Universal namespace===
Since sources of data within the data grid will consist of data from multiple separate systems and [[Computer network|networks]] using different file [[naming convention]]s, it would be difficult for a user to locate data within the data grid and know they retrieved what they needed based solely on existing physical file names (PFNs). A universal or unified name space makes it possible to create logical file names (LFNs) that can be referenced within the data grid that map to PFNs.&lt;ref&gt;Padala, Pradeep. A survey of data middleware for Grid systems&lt;/ref&gt; When an LFN is requested or queried, all matching PFNs are returned to include possible replicas of the requested data. The end user can then choose from the returned results the most appropriate replica to use. This service is usually provided as part of a management system known as a [[Storage Resource Broker]] (SRB).&lt;ref&gt;Arcot, Rajasekar; Wan, Michael; Moore, Reagan; Schroeder, Wayne; Kremenek. Storage resource broker – managing distributed data in a grid&lt;/ref&gt; Information about the locations of files and mappings between the LFNs and PFNs may be stored in a [[metadata]] or replica catalogue.&lt;ref&gt;Venugopal, Srikumar; Buyya, Rajkumar; Ramamohanarao, Kotagiri. A taxonomy of data grids for distributed data sharing - management and processing p.11&lt;/ref&gt; The replica catalogue would contain information about LFNs that map to multiple replica PFNs.

===Data transport service===
Another middleware service is that of providing for data transport or data transfer. Data transport will encompass multiple functions that are not just limited to the transfer of bits, to include such items as fault tolerance and data access.&lt;ref&gt;Coetzee, Serena. Reference model for a data grid approach to address data in a dynamic SDI p.16&lt;/ref&gt; Fault tolerance can be achieved in a data grid by providing mechanisms that ensures data transfer will resume after each interruption until all requested data is received.&lt;ref&gt;Venugopal, Srikumar; Buyya, Rajkumar; Ramamohanarao, Kotagiri. A taxonomy of data grids for distributed data sharing - management and processing p.21&lt;/ref&gt; There are multiple possible methods that might be used to include starting the entire transmission over from the beginning of the data to resuming from where the transfer was interrupted. As an example, [[GridFTP]] provides for fault tolerance by sending data from the last acknowledged byte without starting the entire transfer from the beginning.

The data transport service also provides for the low-level access and connections between [[Host (network)|hosts]] for file transfer.&lt;ref&gt;Allcock, Bill; Foster,Ian; Nefedova, Veronika; Chervenak, Ann; Deelman, Ewa; Kesselman, Carl. High-performance remote access to climate simulation data: A challenge problem for data grid technologies.&lt;/ref&gt; The data transport service may use any number of modes to implement the transfer to include parallel data transfer where two or more data streams are used over the same [[Channel (communications)|channel]] or striped data transfer where two or more steams access different blocks of the file for simultaneous transfer to also using the underlying built-in capabilities of the network hardware or specifically developed [[Protocol (object-oriented programming)|protocols]] to support faster transfer speeds.&lt;ref&gt;Izmailov, Rauf; Ganguly, Samrat; Tu, Nan. Fast parallel file replication in data grid p.2&lt;/ref&gt; The data transport service might optionally include a [[network overlay]] function to facilitate the routing and transfer of data as well as file [[I/O]] functions that allow users to see remote files as if they were local to their system. The data transport service hides the complexity of access and transfer between the different systems to the user so it appears as one unified data source.

===Data access service===
Data access services work hand in hand with the data transfer service to provide security, access controls and management of any data transfers within the data grid.&lt;ref&gt;Raman, Vijayshankar; Narang, Inderpal; Crone, chris; Hass, Laura; Malaika, Susan. Services for data access and data processing on grids&lt;/ref&gt; Security services provide mechanisms for authentication of users to ensure they are properly identified. Common forms of security for authentication can include the use of passwords or [[Kerberos (protocol)]]. Authorization services are the mechanisms that control what the user is able to access after being identified through authentication. Common forms of authorization mechanisms can be as simple as file permissions. However, need for more stringent controlled access to data is done using [[Access Control List]]s (ACLs), [[Role-Based Access Control]] (RBAC) and Tasked-Based Authorization Controls (TBAC).&lt;ref&gt;Thomas, R. K. and Sandhu R. S. Task-based authorization controls (tbac): a family of models for active and enterprise-oriented authorization management&lt;/ref&gt; These types of controls can be used to provide granular access to files to include limits on access times, duration of access to granular controls that determine which files can be read or written to. The final data access service that might be present to protect the confidentiality of the data transport is encryption.&lt;ref&gt;Sreelatha, Malempati. Grid based approach for data confidentiality. p.1&lt;/ref&gt; The most common form of encryption for this task has been the use of [[Transport Layer Security|SSL]] while in transport. While all of these access services operate within the data grid, access services within the various administrative domains that host the datasets will still stay in place to enforce access rules. The data grid access services must be in step with the administrative domains access services for this to work.

===Data replication service===
To meet the needs for scalability, fast access and user collaboration, most data grids support replication of datasets to points within the distributed storage architecture.&lt;ref&gt;Chervenak, Ann; Schuler, Robert; Kesselman, Carl; Koranda, Scott; Moe, Brian. Wide area data replication for scientific collaborations&lt;/ref&gt; The use of replicas allows multiple users faster access to datasets and the preservation of bandwidth since replicas can often be placed strategically close to or within sites where users need them. However, replication of datasets and creation of replicas is bound by the availability of storage within sites and bandwidth between sites. The replication and creation of replica datasets is controlled by a replica management system. The replica management system determines user needs for replicas based on input requests and creates them based on availability of storage and bandwidth.&lt;ref&gt;Lamehamedi, Houda; Szymanski, Boleslaw; Shentu, Zujun; Deelman, Ewa. Data replication strategies in grid environments&lt;/ref&gt; All replicas are then cataloged or added to a directory based on the data grid as to their location for query by users. In order to perform the tasks undertaken by the replica management system, it needs to be able to manage the underlying storage infrastructure. The data management system will also ensure the timely updates of changes to replicas are propagated to all nodes.

====Replication update strategy====
There are a number of ways the replication management system can handle the updates of replicas. The updates may be designed around a centralized model where a single master replica updates all others, or a decentralized model, where all peers update each other.&lt;ref&gt;Lamehamedi, Houda; Szymanski, Boleslaw; Shentu, Zujun; Deelman, Ewa. Data replication strategies in grid environments&lt;/ref&gt; The topology of node placement may also influence the updates of replicas. If a hierarchy topology is used then updates would flow in a tree like structure through specific paths. In a flat topology it is entirely a matter of the peer relationships between nodes as to how updates take place. In a hybrid topology consisting of both flat and hierarchy topologies updates may take place through specific paths and between peers.

====Replication placement strategy====
There are a number of ways the replication management system can handle the creation and placement of replicas to best serve the user community. If the storage architecture supports replica placement with sufficient site storage, then it becomes a matter of the needs of the users who access the datasets and a strategy for placement of replicas.&lt;ref&gt;Padala, Pradeep. A survey of data middleware for Grid systems&lt;/ref&gt; There have been numerous strategies proposed and tested on how to best manage replica placement of datasets within the data grid to meet user requirements. There is not one universal strategy that fits every requirement the best. It is a matter of the type of data grid and user community requirements for access that will determine the best strategy to use. Replicas can even be created where the files are encrypted for confidentiality that would be useful in a research project dealing with medical files.&lt;ref&gt;Kranthi, G. and Rekha, D. Shashi. Protected data objects replication in data grid p.40&lt;/ref&gt; The following section contains several strategies for replica placement.

=====Dynamic replication=====
Dynamic replication is an approach to placement of replicas based on popularity of the data.&lt;ref&gt;Belalem, Ghalem and Meroufel, Bakhta. Management and placement of replicas in a hierarchical data grid&lt;/ref&gt; The method has been designed around a hierarchical replication model. The data management system keeps track of available storage on all nodes. It also keeps track of requests (hits) for which data clients (users) in a site are requesting. When the number of hits for a specific dataset exceeds the replication threshold it triggers the creation of a replica on the server that directly services the user’s client. If the direct servicing server known as a father does not have sufficient space, then the father’s father in the hierarchy is then the target to receive a replica and so on up the chain until it is exhausted. The data management system algorithm also allows for the dynamic deletion of replicas that have a null access value or a value lower than the frequency of the data to be stored to free up space. This improves system performance in terms of response time, number of replicas and helps load balance across the data grid. This method can also use dynamic algorithms that determine whether the cost of creating the replica is truly worth the expected gains given the location.&lt;ref&gt;Lamehamedi, Houda; Szymanski, Boleslaw; Shentu, Zujun; Deelman, Ewa. Data replication strategies in grid environments&lt;/ref&gt;

=====Adaptive replication=====
This method of replication like the one for dynamic replication has been designed around a hierarchical replication model found in most data grids. It works on a similar algorithm to dynamic replication with file access requests being a prime factor in determining which files should be replicated. A key difference, however, is the number and frequency of replica creations is keyed to a dynamic threshold that is computed based on request arrival rates from clients over a period of time.&lt;ref&gt;Shorfuzzaman, Mohammad; Graham, Peter; Eskicioglu, Rasit. Adaptive replica placement in hierarchical data grids&lt;/ref&gt; If the number of requests on average exceeds the previous threshold and shows an upward trend, and storage utilization rates indicate capacity to create more replicas, more replicas may be created. As with dynamic replication, the removal of replicas that have a lower threshold that were not created in the current replication interval can be removed to make space for the new replicas.

=====Fair-share replication=====
Like the adaptive and dynamic replication methods before, fair-share replication is based on a hierarchical replication model. Also, like the two before, the popularity of files play a key role in determining which files will be replicated. The difference with this method is the placement of the replicas is based on access load and storage load of candidate servers.&lt;ref&gt;Rasool, Qaisar; Li, Jianzhong; Oreku, George S.; Munir, Ehsan Ullah. Fair-share replication in data grid&lt;/ref&gt; A candidate server may have sufficient storage space but be servicing many clients for access to stored files. Placing a replicate on this candidate could degrade performance for all clients accessing this candidate server. Therefore, placement of replicas with this method is done by evaluating each candidate node for access load to find a suitable node for the placement of the replica. If all candidate nodes are equivalently rated for access load, none or less accessed than the other, then the candidate node with the lowest storage load will be chosen to host the replicas. Similar methods to the other described replication methods are used to remove unused or lower requested replicates if needed. Replicas that are removed might be moved to a parent node for later reuse should they become popular again.

=====Other replication=====
The above three replica strategies are but three of many possible replication strategies that may be used to place replicas within the data grid where they will improve performance and access. Below are some others that have been proposed and tested along with the previously described replication strategies.&lt;ref&gt;Ranganathan, Kavitha and Foster, Ian. Identifying dynamic replication strategies for a high performance data grid&lt;/ref&gt; 
* '''Static''' – uses a fixed replica set of nodes with no dynamic changes to the files being replicated.
* '''Best Client''' – Each node records number of requests per file received during a preset time interval; if the request number exceeds the set threshold for a file a replica is created on the best client, one that requested the file the most; stale replicas are removed based on another algorithm. 
* '''Cascading''' – Is used in a hierarchical node structure where requests per file received during a preset time interval is compared against a threshold. If the threshold is exceeded a replica is created at the first tier down from the root, if the threshold is exceeded again a replica is added to the next tier down and so on like a waterfall effect until a replica is placed at the client itself.
* '''Plain Caching''' – If the client requests a file it is stored as a copy on the client.
* '''Caching plus Cascading''' – Combines two strategies of caching and cascading.
* '''Fast Spread''' – Also used in a hierarchical node structure this strategy automatically populates all nodes in the path of the client that requests a file.

===Tasks scheduling and resource allocation===
Such characteristics of the data grid systems as large scale and heterogeneity require specific methods of tasks scheduling and resource allocation. To resolve the problem, majority of systems use extended classic methods of scheduling.&lt;ref&gt;Epimakhov, Igor; Hameurlain, Abdelkader ; Dillon, Tharam; Morvan, Franck. Resource Scheduling Methods for Query Optimization in Data Grid Systems&lt;/ref&gt; Others invite fundamentally different methods based on incentives for autonomous nodes, like virtual money or reputation of a node.
Another specificity of data grids, dynamics, consists in the continuous process of connecting and disconnecting of nodes and local load imbalance during an execution of tasks. That can make obsolete or non-optimal results of initial resource allocation for a task. As a result, much of the data grids utilize execution-time adaptation techniques that permit the systems to reflect to the dynamic changes: balance the load, replace disconnecting nodes, use the profit of newly connected nodes, recover a task execution after faults.

===Resource management system (RMS)===
The resource management system represents the core functionality of the data grid. It is the heart of the system that manages all actions related to storage resources. In some data grids it may be necessary to create a federated RMS architecture because of different administrative policies and a diversity of possibilities found within the data grid in place of using a single RMS. In such a case the RMSs in the federation will employ an architecture that allows for interoperability based on an agreed upon set of protocols for actions related to storage resources.&lt;ref&gt;Krauter, Klaus; Buyya, Rajkumar; Maheswaran, Muthucumaru. A taxonomy and survey of grid resource management systems for distributed computing&lt;/ref&gt;

====RMS functional capabilities====
* Fulfillment of user and application requests for data resources based on type of request and policies; RMS will be able to support multiple policies and multiple requests concurrently
* Scheduling, timing and creation of replicas
* Policy and security enforcement within the data grid resources to include authentication, authorization and access 
* Support systems with different administrative policies to inter-operate while preserving site autonomy
* Support quality of service (QoS) when requested if feature available
* Enforce system fault tolerance and stability requirements
* Manage resources, i.e. disk storage, network bandwidth and any other resources that interact directly or as part of the data grid 
* Manage trusts concerning resources in administrative domains, some domains may place additional restrictions on how they participate requiring adaptation of the RMS or federation.
* Supports adaptability, extensibility, and scalability in relation to the data grid.

==Topology==
[[File:Data Grid Multiple Topologies 1.jpg|right|Possible Data Grid Topologies]]
Data grids have been designed with multiple topologies in mind to meet the needs of the scientific community. On the right are four diagrams of various topologies that have been used in data grids.&lt;ref&gt;Zhu, Lichun. Metadata management in grid database federation&lt;/ref&gt; Each topology has a specific purpose in mind for where it will be best utilized. Each of these topologies is further explained below.

'''Federation topology''' is the choice for institutions that wish to share data from already existing systems. It allows each institution control over their data. When an institution with proper authorization requests data from another institution it is up to the institution receiving the request to determine if the data will go to the requesting institution. The federation can be loosely integrated between institutions, tightly integrated or a combination of both.

'''Monadic topology''' has a central repository that all collected data is fed into. The central repository then responds to all queries for data. There are no replicas in this topology as compared to others. Data is only accessed from the central repository which could be by way of a web portal. One project that uses this data grid topology is the [[Network for Earthquake Engineering Simulation| Network for Earthquake Engineering Simulation (NEES)]] in the United States.&lt;ref&gt;Venugopal, Srikumar; Buyya, Rajkumar; Ramamohanarao, Kotagiri. A taxonomy of data grids for distributed data sharing - management and processing p.16&lt;/ref&gt; This works well when all access to the data is local or within a single region with high speed connectivity.

'''Hierarchical topology''' lends itself to collaboration where there is a single source for the data and it needs to be distributed to multiple locations around the world. One such project that will benefit from this topology would be [[CERN]] that runs the [[Large Hadron Collider]] that generates enormous amounts of data. This data is located at one source and needs to be distributed around the world to organizations that are collaborating in the project.

'''Hybrid Topology''' is simply a configuration that contains an architecture consisting of any combination of the previous mentioned topologies. It is used mostly in situations where researchers working on projects want to share their results to further research by making it readily available for collaboration.

==History==
The need for data grids was first recognized by the [[scientific community]] concerning [[climate modeling]], where [[terabyte]] and [[petabyte]] sized [[data set]]s were becoming the norm for transport between sites.&lt;ref&gt;Allcock, Bill; Foster,Ian; Nefedova, Veronika; Chervenak, Ann; Deelman, Ewa; Kesselman, Carl. High-performance remote access to climate simulation data: A challenge problem for data grid technologies.&lt;/ref&gt; More recent research requirements for data grids have been driven by the [[Large Hadron Collider]] (LHC) at [[CERN]], the [[LIGO|Laser Interferometer Gravitational Wave Observatory (LIGO)]], and the [[Sloan Digital Sky Survey|Sloan Digital Sky Survey (SDSS)]]. These examples of scientific instruments produce large amounts of data that need to be accessible by large groups of geographically dispersed researchers.&lt;ref&gt;Allcock, Bill; Chervenak, Ann; Foster, Ian; et al. p.571&lt;/ref&gt;&lt;ref&gt;Tierney, Brian L. Data grids and data grid performance issues. p.7&lt;/ref&gt; Other uses for data grids involve governments, hospitals, schools and businesses where efforts are taking place to improve services and reduce costs by providing access to dispersed and separate data systems through the use of data grids.&lt;ref&gt;Thibodeau, P. Governments plan data grid projects&lt;/ref&gt;
 
From its earliest beginnings, the concept of a Data Grid to support the scientific community was thought of as a specialized extension of the “grid” which itself was first envisioned as a way to link super computers into meta-computers.&lt;ref&gt;Heingartner, douglas. The grid: the next-gen internet&lt;/ref&gt; However, that was short lived and the grid evolved into meaning the ability to connect computers anywhere on the web to get access to any desired files and resources, similar to the way electricity is delivered over a grid by simply plugging in a device. The device gets electricity through its connection and the connection is not limited to a specific outlet. From this the data grid was proposed as an integrating architecture that would be capable of delivering resources for distributed computations. It would also be able to service numerous to thousands of queries at the same time while delivering gigabytes to terabytes of data for each query. The data grid would include its own management infrastructure capable of managing all aspects of the data grids performance and operation across multiple wide area networks while working within the existing framework known as the web.&lt;ref&gt;Heingartner, douglas. The grid: the next-gen internet&lt;/ref&gt; 
 
The data grid has also been defined more recently in terms of usability; what must a data grid be able to do in order for it to be useful to the scientific community. Proponents of this theory arrived at several criteria.&lt;ref&gt;Venugopal, Srikumar; Buyya, Rajkumar; Ramamohanarao, Kotagiri. A taxonomy of data grids for distributed data sharing - management and processing p.1&lt;/ref&gt; One, users should be able to search and discover applicable resources within the data grid from amongst its many datasets. Two, users should be able to locate datasets within the data grid that are most suitable for their requirement from amongst numerous replicas. Three, users should be able to transfer and move large datasets between points in a short amount of time. Four, the data grid should provide a means to manage multiple copies of datasets within the data grid. And finally, the data grid should provide security with user access controls within the data grid, i.e. which users are allowed to access which data.

The data grid is an evolving technology that continues to change and grow to meet the needs of an expanding community. One of the earliest programs begun to make data grids a reality was funded by the [[DARPA|Defense Advanced Research Projects Agency (DARPA)]] in 1997 at the [[University of Chicago]].&lt;ref&gt;Globus. About the globus toolkit&lt;/ref&gt; This research spawned by DARPA has continued down the path to creating open source tools that make data grids possible. As new requirements for data grids emerge projects like the [[Globus Toolkit]] will emerge or expand to meet the gap. Data grids along with the "Grid" will continue to evolve.

== Notes ==
{{Reflist}}

== References ==
*{{cite journal
|last1= Allcock
|first1= Bill |last2= Chervenak |first2= Ann |last3= Foster |first3= Ian |last4= Kesselman |first4= Carl |last5= Livny |first5= Miron 
|year= 2005
|title= Data Grid tools: enabling science on big distributed data
|journal= Journal of Physics: Conference Series
|volume= 16
|pages= 571–575
|publisher= Institute of Physics Publishing
|doi= 10.1088/1742-6596/16/1/079
|url= http://iopscience.iop.org/1742-6596/16/1/079
|accessdate= April 15, 2012}}

*{{cite journal
|last1=Allcock
|first1=Bill |last2=Foster |first2=Ian |last3=Nefedova  |first3= Veronika  l|last4= Chervenak |first4= Ann |last5= Deelman |first5= Ewa |last6= Kesselman |first6= Carl |last7= Lee |first7= Jason |last8= Sim |first8= Alex |last9= Shoshani |first9= Arie |last10= Drach |first10=Bob |last11= Williams |first11= Dean    
|title= High-performance remote access to climate simulation data: A challenge problem for data grid technologies
|work =
|publisher =  [[ACM Press]]
|year = 2001
|citeseerx = 10.1.1.64.6603
|format =
|doi =
|accessdate = &lt;!-- April 20, 2012 --&gt; }}

*{{cite web
 |last1=Arcot 
 |first1=Rajasekar 
 |last2=Wan 
 |first2=Michael 
 |last3=Moore 
 |first3=Reagan 
 |last4=Schroeder 
 |first4=Wayne 
 |last5=Kremenek 
 |first5=George 
 |title=Storage resource broker – managing distributed data in a grid 
 |work= 
 |publisher= 
 |date= 
 |url=http://www.npaci.edu/DICE/Pubs/CSI-paper-sent.doc 
 |format= 
 |doi= 
 |accessdate=April 28, 2012 
 |deadurl=yes 
 |archiveurl=https://web.archive.org/web/20060507193028/http://www.npaci.edu:80/DICE/Pubs/CSI-paper-sent.doc 
 |archivedate=May 7, 2006 
 |df= 
}}

*{{cite journal
|last1= Belalem
|first1= Ghalem |last2= Meroufel |first2= Bakhta 
|year= 2011
|title= Management and placement of replicas in a hierarchical data grid 
|journal= International Journal of Distributed and Parallel Systems (IJDPS)
|volume= 2
|issue= 6
|pages= 23–30
|location =
|publisher=
|doi= 10.5121/ijdps.2011.2603
|url= http://www.scribd.com/doc/75105419/Management-and-Placement-of-Replicas-in-a-Hierarchical-Data-Grid
|accessdate= April 28, 2012}}

*{{cite journal
|last1= Chervenak
|first1= A.|last2= Foster |first2= I. |last3= Kesselman |first3= C.|last4= Salisbury |first4= C. |last5= Tuecke |first5= S.
|year= 2001
|title= The data grid: towards an architecture for the distributed management and analysis of large scientific datasets 
|journal= Journal of Network and Computer Applications
|volume= 23
|issue= 
|pages= 187–200
|location =
|publisher=
|doi= 10.1006/jnca.2000.0110
|url= http://www.globus.org/alliance/publications/papers/JNCApaper.pdf
|accessdate= April 11, 2012}}

*{{cite web
|last1= Chervenak
|first1= Ann |last2= Schuler |first2= Robert |last3= Kesselman | first3= Carl |last4= Koranda |first4= Scott |last5= Moe |first5= Brian
|title= Wide area data replication for scientific collaborations
|work= |publisher = [[IEEE]]
|date = November 14, 2005
|url= http://www.globus.org/alliance/publications/papers/chervenakGrid2005.pdf
|format=
|doi=
| accessdate = April 25, 2012 }}

*{{cite journal
 |last1=Coetzee 
 |first1=Serena 
 |year=2012 
 |title=Reference model for a data grid approach to address data in a dynamic SDI 
 |journal=Geoinformatica 
 |volume=16 
 |issue=1 
 |pages=111–129 
 |location= 
 |publisher= 
 |doi=10.1007/s10707-011-0129-4 
 |url=http://web.up.ac.za/sitefiles/file/48/16053/Coetzee_2011_ReferenceModelForDataGridApproach(2).pdf 
 |accessdate=April 28, 2012 
}}{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}

*{{cite conference
|last1= Epimakhov
|first1= Igor 
|last2= Hameurlain
|first2= Abdelkader 
|last3= Dillon
|first3= Tharam 
|last4= Morvan
|first4= Franck 
|title= Resource Scheduling Methods for Query Optimization in Data Grid Systems 
|booktitle = Advances in Databases and Information Systems. 15th International Conference, ADBIS 2011
|pages = 185–199
|publisher = Springer Berlin Heidelberg
|year = 2011
|location = Vienna, Austria
|url = http://link.springer.com/chapter/10.1007%2F978-3-642-23737-9_14
|doi = 10.1007/978-3-642-23737-9_14
|id = 
|accessdate = September 20, 2011 }}

*{{cite web
|last1= Globus
|first1= 
|title= About the globus toolkit
|work=
|publisher= [[Globus Alliance|Globus]]
|year= 2012
|url= http://www.globus.org/toolkit/about.html
|doi=
|accessdate = May 27, 2012 }}

*{{cite news
 |last1=Heingartner 
 |first1=Douglas 
 |title=The Grid: The Next-Gen Internet 
 |work=Wired 
 |publisher= 
 |date=March 8, 2001 
 |url=http://www.wired.com/science/discoveries/news/2001/03/42230 
 |format= 
 |doi= 
 |accessdate=May 13, 2012 
 |deadurl=yes 
 |archiveurl=https://web.archive.org/web/20120504035536/http://www.wired.com:80/science/discoveries/news/2001/03/42230 
 |archivedate=May 4, 2012 
 |df= 
}}

*{{cite web
 |last1=Izmailov 
 |first1=Rauf 
 |last2=Ganguly 
 |first2=Samrat 
 |last3=Tu 
 |first3=Nan 
 |title=Fast parallel file replication in data grid 
 |work= 
 |publisher= 
 |year=2004 
 |url=http://www.cs.huji.ac.il/labs/danss/p2p/resources/fast-parallel-file-replication-on-data-grid.pdf 
 |format= 
 |doi= 
 |accessdate=May 10, 2012 
 |deadurl=yes 
 |archiveurl=https://web.archive.org/web/20120421081052/http://www.cs.huji.ac.il/labs/danss/p2p/resources/fast-parallel-file-replication-on-data-grid.pdf 
 |archivedate=April 21, 2012 
 |df= 
}}

*{{cite journal
 |last1=Kranthi 
 |first1=G. Aruna 
 |last2=Rekha 
 |first2=D. Shashi 
 |year=2012 
 |title=Protected data objects replication in data grid 
 |journal=International Journal of Network Security &amp; Its Applications (IJNSA) 
 |volume=4 
 |issue=1 
 |pages=29–41 
 |location= 
 |publisher= 
 |doi=10.5121/ijnsa.2012.4103 
 |url=http://journaldatabase.org/articles/protected_data_objects_replication.html 
 |issn=0975-2307 
 |accessdate=April 1, 2012 
 |deadurl=yes 
 |archiveurl=https://web.archive.org/web/20131008004646/http://journaldatabase.org/articles/protected_data_objects_replication.html 
 |archivedate=October 8, 2013 
 |df= 
}}

*{{cite journal
|last1= Krauter
|first1= Klaus |last2= Buyya |first2= Rajkumar |last3= Maheswaran |first3= Muthucumaru
|year= 2002
|title= A taxonomy and survey of grid resource management systems for distributed computing
|journal= Software Practice and Experience (SPE)
|volume= 32
|issue= 2
|pages= 135–164 
|location = 
|publisher= 
|doi=10.1002/spe.432
|citeseerx = 10.1.1.38.2122
|accessdate= &lt;!-- April 17, 2012 --&gt;}}


*{{cite conference
|last1= Lamehamedi
|first1= Houda |last2= Szymanski |first2= Boleslaw |last3= Shentu |first3= Zujun |last4= Deelman |first4= Ewa  
|title = Data replication strategies in grid environments
|booktitle = Fifth International Conference on Algorithms and Architectures for Parallel Processing (ICA3PP’02)
|pages = 378–383
|publisher = Press
|year = 2002
|location =
|citeseerx = 10.1.1.11.5473
|doi =
|accessdate = &lt;!-- April 5, 2012 --&gt; }}

*{{cite journal
|last1= Padala
|first1= Pradeep
|title= A survey of data middleware for Grid systems
|work =
|publisher = 
|date = 
|citeseerx = 10.1.1.114.1901
|format =
|doi =
|accessdate = &lt;!-- April 28, 2012 --&gt; }}

*{{cite web
|last1= Raman 
|first1= Vijayshankar |last2= Narang |first2= Inderpal |last3= Crone |first3= Chris |last4= Hass |first4= Laura |last5= Malaika |first5= Susan 
|title= Services for data access and data processing on grids
|work =
|publisher = 
|date = February 9, 2003
|url = http://www.ogf.org/documents/GFD.14.pdf
|format =
|doi =
|accessdate = May 10, 2012 }}

*{{cite conference
|last1= Ranganathan
|first1= Kavitha | last2= Foster |first2= Ian
|title= Identifying dynamic replication strategies for a high performance data grid
|booktitle= In Proc. of the International Grid Computing Workshop
|pages= 75–86
|publisher = 
|year= 2001
|location=
|citeseerx = 10.1.1.20.6836
|format=
|doi= 10.1007/3-540-45644-9_8
|accessdate = &lt;!-- May 15, 2012 --&gt; }}

*{{cite journal
|last1= Rasool
|first1= Qaisar |last2= Li |first2= Jianzhong |last3= Oreku| first3= George S.|last4= Munir |first4= Ehsan Ullah
|year= 2008
|title= Fair-share replication in data grid
|journal= Information Technology Journal
|volume= 7
|issue= 5
|pages= 776–782
|publisher=
|doi= 10.3923/itj.2008.776.782
|url= http://scialert.net/abstract/?doi=itj.2008.776.782
|accessdate= April 27, 2012 }}

*{{cite journal
|last1= Shorfuzzaman
|first1= Mohammad |last2= Graham |first2= Peter |last3= Eskicioglu |first3= Rasit 
|year= 2010
|title= Adaptive replica placement in hierarchical data grids
|journal= Journal of Physics: Conference Series
|volume= 256
|issue= 1
|pages= 1–18
|location =
|publisher= [[IOP Publishing Ltd]]
|doi= 10.1088/1742-6596/256/1/012020
|url= http://iopscience.iop.org/1742-6596/256/1/012020
|accessdate= April 15, 2012}}

*{{cite journal
|last1= Sreelatha
|first1= Malempati
|year= 2011
|title= Grid based approach for data confidentiality 
|journal= International Journal of Computer Applications
|volume= 25
|issue= 9
|pages= 1–5
|location =
|publisher=
|doi= 10.5120/3063-4186
|issn = 0975-8887
|url= http://www.ijcaonline.org/volume25/number9/pxc3874186.pdf
|accessdate= April 28, 2012}}

*{{cite journal
|last1= Thibodeau
|first1=P.   
|title= Governments plan data grid projects
|journal= Computerworld
|volume= 39
|issue= 42
|pages= 14
|location= United States
|publisher= Computerworld
|date = May 30, 2005
|url = http://www.computerworld.com/s/article/102119/Governments_Plan_Data_Grid_Projects
|format =
|doi=
|issn= 0010-4841
|accessdate = April 28, 2012 }}

*{{cite web
|last1= Thomas
|first1= R. K. |last2= Sandhu |first2= R. S. 
|title= Task-based authorization controls (tbac): a family of models for active and enterprise-oriented authorization management
|work =
|publisher = 
|year = 1997
|url = http://profsandhu.com/confrnc/ifip/i97tbac.pdf
|format =
|doi =
|accessdate = April 28, 2012 }}

*{{cite web
|last1=Tierney
|first1=Brian L.   
|title= Data grids and data grid performance issues
|work =
|publisher = 
|year = 2000
|url = http://www-didc.lbl.gov/presentations/CSC2000-tierney.pdf
|format =
|doi =
|accessdate = April 28, 2012 }}

*{{cite journal
|last1= Venugopal
|first1= Srikumar |last2= Buyya |first2= Rajkumar |last3= Ramamohanarao |first3= Kotagiri
|year= 2006
|title= A taxonomy of data grids for distributed data sharing, management and processing
|journal= ACM Computing Surveys
|volume= 38
|issue= 1
|pages= 1–60 
|location = New York
|publisher= [[Association for Computing Machinery]]
|doi=
|url= http://www.cloudbus.org/reports/DataGridTaxonomy.pdf
|accessdate= April 10, 2012}}

*{{cite web
 |last1=Zhu 
 |first1=Lichun 
 |title=Metadata management in grid database federation 
 |work= 
 |publisher= 
 |date= 
 |url=http://cs.uwindsor.ca/richard/cs510/lichun_zhu_survey.pdf 
 |format= 
 |doi= 
 |accessdate=May 15, 2012 
}}{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}

==Further reading==

*{{cite web
|last1= Allcock
|first1= W.
|authorlink = W. Allcock
|title= Gridftp: protocol extensions to ftp for the grid
|work=
|publisher = [[Argonne National Laboratory]]
|date = April 2003
|url = http://www.globus.org/alliance/publications/papers/GFD-R.0201.pdf
|format = 
|doi =
| accessdate = April 20, 2012 }}

*{{cite web
|last1= Allcock
|first1= W.|last2= Bresnahan |first2= J. |last3= Kettimuthu |first3= R.|last4= Link |first4= M.|last5= Dumitrescu |first5= C.|last6= Raicu |first6= I.|last7= Foster |first7= I.
|title= The globus striped gridftp framework and server
|work=
|publisher= [[ACM Press]]
|date= November 2005
|url= http://www.globus.org/alliance/publications/papers/gridftp_final.pdf
|format=
|doi=
|accessdate = April 20, 2012 }}

*{{cite journal
|last1= Foster
|first1= Ian |last2= Kesselman |first2= Carl |last3= Tuecke |first3= Steven
|year= 2001
|title= The anatomy of the grid enabling scalable virtual organizations
|journal= [[International Journal of High Performance Computing Applications]]
|volume= 15
|issue= 3
|pages= 200–222 
|location = Thousand Oaks
|publisher= [[Sage Publications]]
|doi=10.1177/109434200101500302
|url= http://www.globus.org/alliance/publications/papers/anatomy.pdf
|accessdate= April 10, 2012}}

*{{cite web
 |last1=Foster 
 |first1=Ian 
 |last2=Kesselman 
 |first2=Carl 
 |last3=Nick 
 |first3=Jeffrey M. 
 |last4=Tuecke 
 |first4=Steven 
 |title=The physiology of the grid: an open grid services architecture for distributed systems integration 
 |work= 
 |publisher= 
 |date=June 22, 2002 
 |url=http://forge.gridforum.org/sf/go/doc13483?nav=1 
 |format= 
 |doi= 
 |accessdate=May 10, 2012 
 |deadurl=yes 
 |archiveurl=https://web.archive.org/web/20080322035911/http://forge.gridforum.org:80/sf/go/doc13483?nav=1 
 |archivedate=March 22, 2008 
 |df= 
}}

*{{cite journal
|last1= Hancock
|first1= B.
|year= 2009
|title= A simple data grid using the inferno operating system
|journal= Library Hi Tech
|volume= 27
|issue= 3
|pages= 382–392 
|location =
|publisher= [[Emerald Group Publishing Limited]]
|doi= 10.1108/07378830910988513
|url= }}&lt;!--|accessdate= April 10, 2012--&gt;

*{{cite web
 |last1=Hoschek 
 |first1=W. 
 |last2=McCance 
 |first2=G. 
 |title=Grid enabled relational database middleware 
 |work= 
 |publisher=[[Global Grid Forum]] 
 |date=October 10, 2001 
 |url=http://ppewww.ph.gla.ac.uk/preprints/2001/11/GGF3Rome2001.pdf 
 |format= 
 |doi= 
 |accessdate=April 22, 2012 
 |deadurl=yes 
 |archiveurl=https://web.archive.org/web/20060128234459/http://ppewww.ph.gla.ac.uk:80/preprints/2001/11/GGF3Rome2001.pdf 
 |archivedate=January 28, 2006 
 |df= 
}}

*{{cite web
|last1= Kunszt
|first1= Peter Z.|last2= Guy |first2= Leanne P.
|title= The open grid services architecture and data grids
|work =
|publisher = 
|date = July 7, 2002
|url = http://www.computing.surrey.ac.uk/courses/csm23/Papers/data_grid.pdf
|format =
|doi =
|accessdate = May 10, 2012 }}

*{{cite web
|last1= Moore
|first1= Reagan W. 
|title= Evolution of data grid concepts
|work =
|publisher = 
|date = 
|url = http://www.nesc.ac.uk/events/GGF10-DA/programme/papers/06-Moore-Grid-evolution.pdf
|format =
|doi =
|accessdate = May 10, 2012 }}

*{{cite conference
|last1= Rajkumar
|first1= Kettimuthu |last2= Allcock |first2= William |last3= Liming |first3= Lee |last4= Navarro |first4= John-Paul |last5= Foster |first5= Ian
| title = GridCopy moving data fast on the grid
| booktitle = International parallel and distributed processing symposium (IPDPS 2007)
| pages = 1–6
| publisher = IEEE International
| date = March 30, 2007
| location =  Long Beach
| url = http://www.globus.org/alliance/publications/papers/GridCopy.pdf
| doi =
| id =
| accessdate = April 29, 2012 }}

*{{cite journal
|last1= Thenmozhi
|first1= N. |last2= Madheswaran |first2= M. 
|year= 2011
|title= Content based data transfer mechanism for efficient bulk data transfer in grid computing environment
|journal= International Journal of Grid Computing &amp; Applications (IJGCA)
|volume= 2
|issue= 4
|pages= 49–62
|location =
|publisher=
|doi= 10.5121/ijgca.2011.2405
|issn= 2229-3949
|url= http://www.scribd.com/doc/78611092/Content-Based-Data-Transfer-Mechanism-for-Efficient-Bulk-Data-Transfer-in-Grid-Computing-Environment
|accessdate= April 28, 2012}}

*{{cite journal
 |last1=Tu 
 |first1=Manghui 
 |last2=Li 
 |first2=Peng 
 |last3=I-Ling 
 |first3=Yen 
 |last4=Thuraisingham 
 |first4=Bhavani 
 |last5=Khan 
 |first5=Latifur 
 |year=2010 
 |title=Secure data objects replication in data grid 
 |journal=IEEE Transactions on Dependable and Secure Computing 
 |volume=7 
 |issue=1 
 |pages=50–64 
 |publisher=[[IEEE]] 
 |doi=10.1109/tdsc.2008.19 
 |url=http://www.utdallas.edu/~lkhan/papers/Secure_Data_Objects_Replication_in_Data_Grid.pdf 
 |accessdate=April 26, 2012 
}}{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}

[[Category:Data management]]</text>
      <sha1>3tu5no0jl5rdox5zurrzsxdfzlupkxp</sha1>
    </revision>
  </page>
  <page>
    <title>Classora</title>
    <ns>0</ns>
    <id>35915041</id>
    <revision>
      <id>576035764</id>
      <parentid>542703533</parentid>
      <timestamp>2013-10-06T19:45:14Z</timestamp>
      <contributor>
        <username>Lemnaminor</username>
        <id>17971031</id>
      </contributor>
      <comment>Disambiguated: [[ETL]] → [[Extract, transform, load]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7236" xml:space="preserve">{{Refimprove|date=June 2012}}

'''Classora''' is a [[knowledge base]] for the [[Internet]] oriented to [[data]] analysis. From a practical point of view, Classora is a [[digital repository]] that stores structured [[information]] and allows it to be displayed in multiple formats: analytically, graphically, geographically (through maps); as well as carry out [[OLAP]] analysis. The information contained in Classora comes from public sources&lt;ref&gt;[http://revista.mundo-r.com/contido/%E2%80%9Cclassora-evita-tener-que-bucear-entre-resultados-google-o-wikipedia%E2%80%9D Interview in R Technological Magazine (Spanish)]&lt;/ref&gt; and is uploaded into the system through bots and [[Extract, transform, load|ETL]] processes. The [[Knowledge Base]] has a '''commercial API'''&lt;ref&gt;[http://blog.classora.com/2012/03/05/api-de-classora-para-desarrolladores/ Classora API in Official Weblog]&lt;/ref&gt; for semantic enhancement, and an '''open web'''&lt;ref&gt;[http://www.classora.com Open Web of Classora Knowledge Base]&lt;/ref&gt; through which any user can access to part of the information collected (it also allows users to complete data and share opinions).

Internally, Classora is organized into '''Knowledge Units''' and '''Reports'''. A «Knowledge Unit» is any element of the World about which information may be stored and presented in the form of a data sheet (a person, a company, a country, etc.) A «Report» is a group of Knowledge Units: a ranking of companies, a sport classification table, a survey about people, etc. In fact, one of the technical capabilities of Classora is that it allows the comparison of reports and knowledge units gathered from different sources, thereby generating an added value for the media in which this information is published: digital media, interactive TV, etc.

== Key definitions ==

=== Knowledge unit ===
The '''units of knowledge''' (also known as ''entries'') in Classora are data sheets that have a certain semantic equivalence with the articles on the Wikipedia: they store information about any element of the world, be it a film, a country, a company or an animal. However, they differ from Wikipedia in that Classora stores structured information, enriched with a metadata layer; and therefore it is able to automatically interpret the meaning of each unit of knowledge.

=== Data report ===
A '''report''' is a group of units of knowledge in which the repetition of elements is not allowed. This definition includes any list, poll, ranking, etc.; and, in general, any consultation that involves more than one unit of knowledge. Classora excels at the reports management due to its visualization capabilities, being able to display data in the form of tables, graphs and maps. 

Types of reports:

* '''Sports scores''': Sports competitions results sanctioned by the competent institution.
* '''Rankings and lists''': All types of interesting and curious lists, whether they have an implicit order or not.
* '''Polls''': Units of knowledge that are ranked according to users’ votes.
* '''Queries to the Knowledge Base''': Questions from users using [[Contextual Query Language|CQL]].
* '''Networks of connections''': automatically calculated from the reports and the taxonomy of each Knowledge Unit.

=== Organizational taxonomy ===
An '''organizational taxonomy''' (also referred to as '''entry type''') is a data sheet that brings together the common attributes of a set of units of knowledge. For instance, the organizational taxonomy ''F1 Driver'' displays attributes such as date of debut, team, etc.; and the organizational taxonomy ''Football Club'' presents attributes such as city, stadium, etc.

In Classora, taxonomies are hierarchically organized, so that they inherit attributes from their parent taxonomies. For instance, ''F1 Driver'' is a subsidiary taxonomy of ''Sportsperson'', which is a subsidiary taxonomy of ''Person'', which in turn is a subsidiary taxonomy of ''Organism''.

The simplest type of entry in Classora is '''Classora Object'''. All the other taxonomies are its subsidiaries and inherit its attributes. In fact, the only attribute Classora Object possesses is ''name'' (all units of knowledge are required to have one name at least).

== Architecture of Classora ==

=== Data Extraction Module ===
The Data Extraction Module consists of a set of robots coordinated by software that also manages the potential incidents. Most of the information available in Classora is automatically uploaded through those robots, which connect to the main online public sources to gather all types of data. There are three categories of robots:
* '''Extraction robots''': responsible for the massive uploading of reports from official public sources (FIFA, CIA, IMF, Eurostat...). They are used for either absolute or incremental data uploading.
* '''Data scanner robots''': responsible for looking for and updating the data of a unit of knowledge. They use specific sources to perform this task: Wikipedia, IMDB, World Bank, etc.
* '''Content aggregators''':  they don’t connect to external sources. Instead, they generate new information using Classora’s internal database.

=== Participatory Module ===
In Classora’s Open Website, Internet users may participate providing their knowledge as they would on the Wikipedia. There are different ways to participate: adding or correcting data in the Knowledge Base, voting in surveys (participatory rankings) and creating new Knowledge Units and Data Reports.

=== Connectivity Module ===
The Knowledge Base is designed to be embedded in multi-platform, multi-channel systems, thus enabling its integration into mobile devices, tablets, interactive TV, etc. This integration may be carried out through specific plugins (for [[navigators]] or other devices) or an [[API]] [[REST]] that provides content in [[XML]] or [[JSON]] formats. The API is divided into three blocks of operations. The first one is the block of '''general utility tools''' (ranging from autosuggest components about geographical hierarchies to operations to obtain the list of today’s celebrity birthdays, using [[Contextual Query Language|CQL]]). The second one is the block of '''operations for widget generation''' (graphs, maps, rankings) using information from the knowledge base. Finally, there is a block of '''operations designed for the publication of free-source content'''.&lt;ref&gt;[http://blog.classora.com/2012/03/05/api-de-classora-para-desarrolladores/ Post about API in Classora official weblog]&lt;/ref&gt;

== Project statistics ==
As of April 2012, 2,000,000 Knowledge Units, 15,000 Reports, around 10,000 Maps and several million potential Comparative Analyses had been added to Classora. According to the site of web metrics Alexa, Classora Open Website is ranked at 100,557 globally and at 2,880 in the Spanish traffic ranking.&lt;ref&gt;[http://www.alexa.com/siteinfo/http%3A%2F%2Fwww.classora.com Alexa metrics for Classora Open Web]&lt;/ref&gt; Users spend an average of 9 ½ minutes in Classora.

== External links ==
* [http://www.classora.com Open Website of Classora Knowledge Base]

== References ==
&lt;references/&gt;

[[Category:Knowledge bases]]
[[Category:Data management]]
[[Category:Semantic Web]]
[[Category:Knowledge representation software]]</text>
      <sha1>t8d4ctshnw8bjrtv5egnrk4bijjix2k</sha1>
    </revision>
  </page>
  <page>
    <title>Single customer view</title>
    <ns>0</ns>
    <id>37040021</id>
    <revision>
      <id>761320533</id>
      <parentid>760820213</parentid>
      <timestamp>2017-01-22T08:33:09Z</timestamp>
      <contributor>
        <username>RichardWeiss</username>
        <id>193093</id>
      </contributor>
      <comment>more detail</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2779" xml:space="preserve">A '''Single Customer View''' is an aggregated, consistent and holistic representation of the [[data]] known by an organisation about its customers&lt;ref&gt;[http://www.experian.co.uk/assets/about-us/white-papers/single-customer-view-whitepaper.pdf Exploiting the Single Customer  View to Maximise the Value of Customer Relationships]&lt;/ref&gt;&lt;ref&gt;[http://www.marketingweek.co.uk/driving-value-from-the-single-customer-view/3015497.article Driving value from the single customer view]&lt;/ref&gt; that can be viewed in one place, such as a single page.&lt;ref&gt;[https://spotlessdata.com/blog/data-driven-marketing Data-driven marketing]&lt;/ref&gt; The advantage to an organisation of attaining this unified view comes from the ability it gives to analyse past behaviour in order to better target and personalise future customer interactions.&lt;ref&gt;[http://www.atominsight.com/about-us/blog/single-customer-view-essential Why a single customer view is essential]&lt;/ref&gt; A single customer view is also considered especially relevant where organisations engage with customers through [[multichannel marketing]], since customers expect those interactions to reflect a consistent understanding of their history and preferences.&lt;ref&gt;[http://econsultancy.com/uk/blog/9612-the-impact-of-a-single-customer-view-on-consumer-behaviour-infographic The impact of a single customer view on consumer behaviour: infographic]&lt;/ref&gt; However, some commentators have challenged the idea that a single view of customers across an entire organisation is either natural or meaningful, proposing that the priority should instead be consistency between the multiple views that arise in different contexts.

Where representations of a customer are held in more than one [[data set]], achieving a single customer view can be difficult: firstly because customer identity must be traceable between the records held in those systems, and secondly because anomalies or discrepancies in the customer data must be [[data cleansing|data cleansed]].&lt;ref&gt;[http://www.atominsight.com/about-us/blog/single-customer-view-hard Why building a single customer view isn’t as easy as you might think]&lt;/ref&gt; As such, the acquisition by an organisation of a single customer view is one potential outcome of successful [[master data management]]. Since 31 December, 2010, maintaining a single customer view has become mandatory for [[United Kingdom]] banks and other deposit takers due to new rules introduced by the [[Financial Services Compensation Scheme]].&lt;ref&gt;[https://www.fscs.org.uk/industry/single-customer-view/ Single Customer View]&lt;/ref&gt;

==References==
{{reflist}}

[[Category:Identity management]]
[[Category:Business intelligence]]
[[Category:Data management]]
[[Category:Data warehousing]]
[[Category:Information technology management]]</text>
      <sha1>7t0lkfmjp3x794vl7a8241cmvuwqzsn</sha1>
    </revision>
  </page>
  <page>
    <title>Data flow diagram</title>
    <ns>0</ns>
    <id>1344164</id>
    <revision>
      <id>762717677</id>
      <parentid>762717389</parentid>
      <timestamp>2017-01-30T10:08:43Z</timestamp>
      <contributor>
        <username>Triptothecottage</username>
        <id>29005906</id>
      </contributor>
      <comment>/* Physical vs. logical DFD */ cleanup after merge</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6758" xml:space="preserve">{{context|date=July 2014}}
[[File:Data Flow Diagram Example.jpg|thumb|360px|Data flow diagram example.&lt;ref&gt;John Azzolini (2000). [http://ses.gsfc.nasa.gov/ses_data_2000/000712_Azzolini.ppt Introduction to Systems Engineering Practices]. July 2001.&lt;/ref&gt;]]

A '''data flow diagram''' ('''DFD''') is a graphical representation of the "flow" of data through an [[information system]], modelling its ''process'' aspects. A DFD is often used as a preliminary step to create an overview of the system, which can later be elaborated.&lt;ref&gt;Bruza, P. D., Van der Weide, Th. P., "The Semantics of Data Flow Diagrams", University of Nijmegen, 1993.&lt;/ref&gt; DFDs can also be used for the [[Data visualization|visualization]] of [[data processing]] (structured design).

A DFD shows what kind of information will be input to and output from the system, where the data will come from and go to, and where the data will be stored. It does not show information about the timing of process or information about whether processes will operate in sequence or in parallel (which is shown on a [[flowchart]]).

==History==
[[Larry Constantine]], the original developer of structured design,&lt;ref&gt;W. Stevens, G. Myers, L. Constantine, [http://domino.watson.ibm.com/tchjr/journalindex.nsf/d9f0a910ab8b637485256bc80066a393/a801ae3750be70ac85256bfa00685ded!OpenDocument "Structured Design"], IBM Systems Journal, 13 (2), 115-139, 1974.&lt;/ref&gt; based on Martin and Estrin's "Data Flow Graph"  model of computation.

Starting in the 1970s, data flow diagrams (DFD) became a popular way to visualize the major steps and data involved in software system processes. DFDs were usually used to show data flow in a computer system, although they could in theory be applied to [[business process modeling]]. DFD were useful to document the major data flows or to explore a new high-level design in terms of data flow.&lt;ref&gt;Craig Larman, "Applying UML and Patterns", Pearson Education, ISBN 978-81-7758-979-5&lt;/ref&gt;

==Theory==
[[File:DataFlowDiagram Example.png|thumb|360px|Data flow diagram example]]
[[File:Data-flow-diagram-notation.svg|thumb|160px|Data flow diagram - [[Edward Yourdon|Yourdon]]/[[Tom DeMarco|DeMarco]] notation]]

Data flow diagrams are also known as bubble charts.&lt;ref&gt;[http://www.orm.net/pdf/jcm13.pdf Introduced by Clive Finkelstein in Australia,  CACI in the UK, and later writers such as James Martin]&lt;/ref&gt; DFD is a designing tool used in the top-down approach to Systems Design. This context-level DFD is next "exploded", to produce a Level 1 DFD that shows some of the detail of the system being modeled. The Level 1 DFD shows how the system is divided into sub-systems (processes), each of which deals with one or more of the data flows to or from an external agent, and which together provide all of the functionality of the system as a whole. It also identifies internal data stores that must be present in order for the system to do its job, and shows the flow of data between the various parts of the system.

Data flow diagrams are one of the three essential perspectives of the structured-systems analysis and design method [[SSADM]]. The sponsor of a project and the end users will need to be briefed and consulted throughout all stages of a system's evolution.  With a data flow diagram, users are able to visualize how the system will operate, what the system will accomplish, and how the system will be implemented.  The old system's dataflow diagrams can be drawn up and compared with the new system's data flow diagrams to draw comparisons to implement a more efficient system. Data flow diagrams can be used to provide the end user with a physical idea of where the data they input ultimately has an effect upon the structure of the whole system from order to dispatch to report. How any system is developed can be determined through a data flow diagram model. 

In the course of developing a set of ''levelled'' data flow diagrams the analyst/designer is forced to address how the system may be decomposed into component sub-systems, and to identify the [[transaction data]] in the [[data model]].

Data flow diagrams can be used in both Analysis and Design phase of the [[Systems development life cycle|SDLC]].

There are  different notations to draw data flow diagrams (Yourdon &amp; Coad and [[Chris Gane (computer scientist)|Gane]] &amp; [[Trish Sarson|Sarson]]&lt;ref&gt;[[Chris Gane (computer scientist)|Chris Gane]] and [[Trish Sarson]]. ''Structured Systems Analysis: Tools and Techniques.'' McDonnell Douglas Systems Integration Company, 1977&lt;/ref&gt;), defining different visual representations for processes, data stores, data flow, and external entities.&lt;ref&gt;[http://www.smartdraw.com/tutorials/software/dfd/tutorial_01.htm How to draw Data Flow Diagrams]&lt;/ref&gt;

===Physical vs. logical DFD=== 
A logical DFD captures the data flows that are necessary for a system to operate. It describes the processes that are undertaken, the data required and produced by each process, and the stores needed to hold the data. On the other hand, a physical DFD shows how the system is actually implemented, either at the moment (Current Physical DFD), or how the designer intends it to be in the future (Required Physical DFD). Thus, a Physical DFD may be used to describe the set of data items that appear on each piece of paper that move around an office, and the fact that a particular set of pieces of paper are stored together in a filing cabinet. It is quite possible that a Physical DFD will include references to data that are duplicated, or redundant, and that the data stores, if implemented as a set of [[database table]]s, would constitute an un-normalised (or de-normalised) relational database. In contrast, a Logical DFD attempts to capture the data flow aspects of a system in a form that has neither redundancy nor duplication.

==See also==
* [[Activity diagram]]
* [[Business Process Model and Notation]]
* [[Control flow diagram]]
* [[Data island]]
* [[Dataflow]]
* [[Directed acyclic graph]]
* [[DRAKON|Drakon-chart]]
* [[Functional flow block diagram]]
* [[Function model]]
* [[IDEF0]]
* [[Logical Data Flow]]
* [[Pipeline (software)|Pipeline]]
* [[Structured Analysis and Design Technique]]
* [[Structure chart]]
* [[System context diagram]]
* [[Value stream mapping]]
* [[Workflow]]

==References==
{{Reflist}}

==Further reading==
*[[Scott W. Ambler]]. [http://www.agilemodeling.com/artifacts/dataFlowDiagram.htm The Object Primer 3rd Edition Agile Model Driven Development with UML 2]

==External links==
*{{Commons-inline}}

{{Data model}}
{{Authority control}}

{{DEFAULTSORT:Data Flow Diagram1}}
[[Category:Information systems]]
[[Category:Data management]]
[[Category:Diagrams]]
[[Category:Visualization (graphic)]]
[[Category:Systems analysis]]</text>
      <sha1>3hks82uqqw2psvpr5hfmptomozxhicz</sha1>
    </revision>
  </page>
  <page>
    <title>PL/Perl</title>
    <ns>0</ns>
    <id>2537690</id>
    <revision>
      <id>588743260</id>
      <parentid>568227670</parentid>
      <timestamp>2014-01-02T00:56:19Z</timestamp>
      <contributor>
        <username>Chirlu</username>
        <id>55863</id>
      </contributor>
      <comment>Improve links for database terms</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1731" xml:space="preserve">'''PL/Perl (Procedural Language/Perl)''' is a procedural language supported by the [[PostgreSQL]] [[RDBMS]].

PL/Perl, as an [[imperative programming language]], allows more control than the [[relational algebra]] of [[SQL]].
Programs created in the PL/Perl language are called functions and can use most of the features that the [[Perl|Perl programming language]] provides, including common flow control structures and syntax that has incorporated [[regular expressions]] directly.
These functions can be evaluated as part of a SQL statement, or in response to a [[Database trigger|trigger]] or [[Constraint (database)|rule]].

The design goals of PL/Perl were to create a loadable procedural language that:

* can be used to create functions and trigger procedures,
* adds control structures to the SQL language,
* can perform complex computations,
* can be defined to be either [http://www.postgresql.org/docs/current/static/plperl-trusted.html trusted or untrusted] by the server,
* is easy to use.

PL/Perl is one of many "PL" languages available for PostgreSQL
[[PL/pgSQL]]
[http://gborg.postgresql.org/project/pljava/projdisplay.php PL/Java], 
[http://plphp.commandprompt.com/ plPHP], 
[http://www.postgresql.org/docs/current/interactive/plpython.html PL/Python], 
[http://www.joeconway.com/plr/ PL/R], 
[http://raa.ruby-lang.org/list.rhtml?name=pl-ruby PL/Ruby], 
[http://plsh.projects.postgresql.org/ PL/sh], 
and [http://www.postgresql.org/docs/current/interactive/pltcl.html PL/Tcl].

==References==
* [http://www.postgresql.org/docs/current/static/plperl.html PostgreSQL PL/Perl documentation]

{{DEFAULTSORT:PL Perl}}
[[Category:Data management]]
[[Category:PostgreSQL]]
[[Category:Data-centric programming languages]]</text>
      <sha1>fx057wzvptc3gu3w8ubmbtvi2bq0yzj</sha1>
    </revision>
  </page>
  <page>
    <title>XML database</title>
    <ns>0</ns>
    <id>1442351</id>
    <revision>
      <id>757893342</id>
      <parentid>745145991</parentid>
      <timestamp>2017-01-02T09:30:12Z</timestamp>
      <contributor>
        <ip>2600:100C:B204:22AB:99C3:C902:5C54:31B1</ip>
      </contributor>
      <comment>grammar</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11211" xml:space="preserve">{{multiple issues|
{{refimprove|date=August 2011}}
{{update|date=March 2015}}
}}

An '''XML database''' is a [[data persistence]] software system that allows data to be specified, and sometimes stored, in [[XML]] format. This data can be [[XQuery|queried]], transformed, exported and returned to a calling system. XML databases are a flavor of [[document-oriented database]]s which are in turn a category of [[NoSQL]] database.

== Rationale for XML in databases ==
There are a number of reasons to directly specify data in XML or other document formats such as JSON. For XML in particular, they include:&lt;ref name=nicola2010&gt;{{cite web|last1=Nicola|first1=Matthias|title=5 Reasons for Storing XML in a Database|url=http://nativexmldatabase.com/2010/09/28/5-reasons-for-storing-xml-in-a-database/|website=Native XML Database|accessdate=17 March 2015|date=28 September 2010}}&lt;/ref&gt;
&lt;ref name=feldman2013&gt;{{cite conference|last1=Feldman|first1=Damon|title=Moving from Relational Modeling to XML and MarkLogic Data Models|url=http://www.marklogic.com/resources/slides-moving-from-relational-modeling-to-xml-and-marklogic-data-models/resource_download/presentations/|conference=MarkLogic World|conferenceurl=http://world.marklogic.com/|date=11 April 2013|accessdate=17 March 2015}}&lt;/ref&gt;
* An enterprise may have a lot of XML in an existing standard format
* Data may need to be exposed or ingested as XML, so using another format such as relational forces double-modeling of the data
* XML is very well suited to sparse data, deeply nested data and mixed content (such as text with embedded markup tags)
* XML is human readable whereas relational tables require expertise to access
* Metadata is often available as XML
* Semantic web data is available as RDF/XML

Steve O'Connell gives one reason for the use of XML in databases: the increasingly common use of XML for [[transport layer|data transport]], which has meant that "data is extracted from databases and put into XML documents and vice-versa".&lt;ref name=oconnell2005&gt;{{cite report|author=O'Connell, Steve|work=Advanced Databases Course Notes|title="Section 9.2"|type=Syllabus|date=2005|publisher=[[University of Southampton]]|location=Southampton, England}}&lt;/ref&gt;{{update inline|date=March 2015}} It may prove more efficient (in terms of conversion costs) and easier to store the data in XML format.  In content-based applications, the ability of the native XML database also minimizes the need for extraction or entry of metadata to support searching and navigation.

== XML Enabled databases ==
XML enabled databases typically offer one or more of the following approaches to storing XML within the traditional relational structure:
#XML is stored into a CLOB ([[Character large object]])
#XML is `shredded` into a series of Tables based on a Schema&lt;ref name=oracle&gt;{{cite book|title=Oracle XML DB Developer's Guide, 10''g'' Release 2|date=August 2005|publisher=Oracle Corporation|chapter-url=http://docs.oracle.com/cd/B19306_01/appdev.102/b14259/xdb05sto.htm|accessdate=17 March 2015|chapter=XML Schema Storage and Query: Basic}}. Section [http://docs.oracle.com/cd/B19306_01/appdev.102/b14259/xdb05sto.htm#i1042421 Creating XMLType Tables and Columns Based on XML Schema]&lt;/ref&gt;
#XML is stored into a native XML Type as defined by ISO Standard 9075-14&lt;ref name=iso9075-2011&gt;{{cite web|title=ISO/IEC 9075-14:2011: Information technology -- Database languages -- SQL -- Part 14: XML-Related Specifications (SQL/XML)|url=http://www.iso.org/iso/home/store/catalogue_ics/catalogue_detail_ics.htm?csnumber=53686|publisher=[[International Organization for Standardization]]|accessdate=17 March 2015|date=2011}}&lt;/ref&gt;

RDBMS that support the ISO XML Type are:
#IBM DB2 (pureXML&lt;ref name=db2purexml&gt;{{cite web|title=pureXML overview -- DB2 as an XML database|url=http://www-01.ibm.com/support/knowledgecenter/SSEPGG_10.1.0/com.ibm.db2.luw.xml.doc/doc/c0022308.html|website=IBM Knowledge Center|publisher=[[IBM]]|accessdate=17 March 2015}}&lt;/ref&gt;)
#Microsoft SQL Server&lt;ref name=sqlserver2005&gt;{{cite web|title=Using XML in SQL Server|url=https://msdn.microsoft.com/en-us/library/ms190936.aspx|website=Microsoft Developer Network|publisher=[[Microsoft Corporation]]|accessdate=17 March 2015}}&lt;/ref&gt;
#Oracle Database&lt;ref name=oracle2&gt;{{cite book|title=Oracle XML DB Developer's Guide, 10''g'' Release 2|date=August 2005|publisher=Oracle Corporation|chapter-url=http://docs.oracle.com/cd/B19306_01/appdev.102/b14259/xdb04cre.htm|accessdate=17 March 2015|chapter=XMLType Operations}}&lt;/ref&gt;
#PostgreSQL&lt;ref name=postgresql&gt;{{cite book|title=PostgreSQL 9.0.19 Documentation|chapter-url=http://www.postgresql.org/docs/9.0/static/datatype-xml.html|accessdate=17 March 2015|chapter=8.13. XML Type}}&lt;/ref&gt; &lt;ref&gt;[http://www.postgresql.org/docs/9.0/static/datatype-xml.html PostgreSQL - Data Types - XML Type]&lt;/ref&gt;

Typically an XML enabled database is best suited where the majority of data are non-XML. For datasets where the majority of data are XML, a [[#Native XML databases|native XML database]] is better suited.

=== Example of XML Type Query in IBM DB2 SQL ===
&lt;source lang="sql"&gt;
select
   id, vol, xmlquery('$j/name', passing journal as "j") as name
from
   journals
where 
   xmlexists('$j[licence="CreativeCommons"]', passing journal as "j")
&lt;/source&gt;

== Native XML databases ==
These databases are typically better when much of the data is in XML or other non-relational formats.{{fact|date=August 2015}}
* [[BaseX]]
* [[Berkeley DB]] XML Edition 
* [[eXist]]
* [[MarkLogic Server]]
* [[Qizx]]
* [[Sedna (database)|Sedna]]

All the above databases uses XML as an interface to specify documents as tree structured data that may contain unstructured text, but on disk the data is stored as "optimized binary files." This makes query and retrieval faster. For MarkLogic it also allows XML and JSON to co-exist in one binary format.&lt;ref&gt;{{cite book|last1=Siegel|first1=Erik|last2=Retter|first2=Adam|title=eXist|date=December 2014|publisher=O'Reilly &amp; Associates|isbn=978-1-4493-3710-0|url=https://www.safaribooksonline.com/library/view/exist/9781449337094/ch04.html|accessdate=18 March 2015|chapter=4. Architecture}}&lt;/ref&gt;

Key features of native XML databases include:

* Has an [[XML]] document as at least one fundamental unit of (logical) storage, just as a [[relational database]] has a row in a table as a fundamental unit of (logical) storage.
* Need not have any particular underlying physical storage model. For example, NXDs can use optimized, proprietary storage formats. This is a key aspect of XML databases. Managing XML as large strings is inefficient due to the extra markup in XML. Compressing and indexing XML allows the illusion of directly accessing, querying and transforming XML while gaining the performance advantages of working with optimized binary tree structures.&lt;ref name=kellogg2010&gt;{{cite web|last1=Kellogg|first1=Dave|title=Yes, Virginia, MarkLogic is a NoSQL System|url=http://kellblog.com/2010/04/11/yes-virginia-marklogic-is-a-nosql-system/|website=Kellblog|accessdate=18 March 2015|date=11 April 2010}}&lt;/ref&gt;

The standards for XML querying per W3C recommendation are [[XQuery]] 1.0 and XQuery 3.0.{{citation needed|date=March 2015}} XQuery includes [[XPath]] as a sub-language and XML itself is a valid sub-syntax of XQuery.

In addition to XPath, XML databases support [[XSLT]] as a method of transforming documents or query-results retrieved from the database. XSLT provides a [[declarative language]] written using an XML grammar. It aims to define a set of XPath [[Filter (software)|filter]]s that can transform documents (in part or in whole) into other formats including [[plain text]], XML, or [[HTML]].

But big picture, XML persistence describes only one format in the larger, faster moving [[NoSQL]] movement at this time. Many databases support XML plus other formats, even if XML is internally stored as an optimized, high-performance format and is a first-class citizen within the database. (see Google Trends Link above to see relative popularity of terms).

=== Language features  ===
{| class="wikitable sortable"
|-
! Name
! License
! Native Language
! XQuery 3.0
! XQuery Update
! XQuery Full Text
! EXPath Extensions
! EXQuery Extensions
! XSLT 2.0
|-
| BaseX
| [[BSD License]]
| Java
| {{Yes}} || {{Yes}} || {{Yes}} || {{Yes}} || {{Yes}} || {{Yes}}
|-
| eXist
| [[LGPL|LGPL License]]
| Java
| {{Partial}} || {{Proprietary}} || {{Proprietary}} ||  {{No}} || {{Yes}} || {{Yes}}
|-
| MarkLogic Server
| [[Commercial software|Commercial]]
| C++
| {{Partial}} ||  {{Proprietary}} || {{Proprietary}} || {{No}}  || {{No}} || {{Yes}}
|-
| Qizx
| [[Commercial software|Commercial]]
| Java
| {{Yes}} || {{Yes}} || {{Yes}} || {{No}} || {{No}} || {{Yes}}
|}

=== Supported APIs ===
{| class="wikitable sortable"
|-
! Name
! [[XQuery API for Java|XQJ]]
! XML:DB
! [[Representational State Transfer|RESTful]]
! RESTXQ
! WebDAV
|-
| BaseX
| {{Yes}} || {{Yes}} || {{Yes}} || {{Yes}} || {{Yes}}
|-
| eXist
| {{Yes}} || {{Yes}} || {{Yes}} || {{Yes}} || {{Yes}}
|-
| MarkLogic Server
| {{Yes}} || {{No}} || {{Yes}} || {{Yes}} || {{Yes}}
|-
| Qizx
| {{No}} || {{No}} || {{Yes}} || {{No}} || {{No}}
|-
| Sedna
| {{Yes}} || {{Yes}} || {{No}} || {{No}} || {{No}}
|}

== References ==
{{reflist}}
{{Refbegin}}

== External links ==
* [https://web.archive.org/web/20150906171257/http://db-engines.com/en/ranking/native+xml+dbms DB-Engines Ranking of Native XML DBMS] by popularity, updated monthly
* [http://www.cfoster.net/articles/xmldb-business-case XML Databases - The Business Case, Charles Foster, June 2008] - Talks about the current state of Databases and data persistence, how the current Relational Database model is starting to crack at the seams and gives an insight into a strong alternative for today's requirements.
* [http://urn.kb.se/resolve?urn=urn:nbn:se:liu:diva-3717 An XML-based Database of Molecular Pathways (2005-06-02)] Speed / Performance comparisons of eXist, X-Hive, Sedna and Qizx/open
* [https://web.archive.org/web/20070922082133/http://swing.felk.cvut.cz/index.php?option=com_docman&amp;task=doc_view&amp;gid=5&amp;Itemid=62 XML Native Database Systems: Review of Sedna, Ozone, NeoCoreXMS] 2006
* [http://csdl2.computer.org/persagen/DLAbsToc.jsp?resourcePath=/dl/mags/ic/&amp;toc=comp/mags/ic/2005/02/w2toc.xml&amp;DOI=10.1109/MIC.2005.48 XML Data Stores: Emerging Practices]
* Bhargava, P.; Rajamani, H.; Thaker, S.; Agarwal, A. (2005) ''XML Enabled Relational Databases'', Texas, The University of Texas at Austin.
* [https://web.archive.org/web/20070113224941/http://xmldb-org.sourceforge.net/ Initiative for XML Databases]
* [http://www.rpbourret.com/xml/XMLAndDatabases.htm  XML and Databases, Ronald Bourret, September 2005]
* [https://web.archive.org/web/20071011101718/http://cafe.elharo.com/xml/the-state-of-native-xml-databases/  The State of Native XML Databases, Elliotte Rusty Harold, August 13, 2007]
* {{Official website|https://www.qualcomm.com/qizx|Qualcomm Qizx official website}}{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }}
{{Refend}}

{{Database models}}
{{Databases}}

[[Category:XML]]
[[Category:Data management]]
[[Category:Data modeling]]
[[Category:XML databases| ]]</text>
      <sha1>oibm0ruo4cwpyvkz8vxmq00955tkcsv</sha1>
    </revision>
  </page>
  <page>
    <title>EU Open Data Portal</title>
    <ns>0</ns>
    <id>38138335</id>
    <revision>
      <id>725255762</id>
      <parentid>723334317</parentid>
      <timestamp>2016-06-14T14:42:01Z</timestamp>
      <contributor>
        <ip>158.169.150.5</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6045" xml:space="preserve">{{Infobox Website
| logo = |250px
| url = {{URL|http://data.europa.eu/euodp/}}
| commercial = No
| type = [[public services|Public service]] [[Web portal|portal]] and &lt;br /&gt;institutional information
| registration = Not required
| language = 24 official languages of the EU 
| owner = {{Flag|European Union}}
| content license = Open
| author = [[Publications Office (European Union)|EU Publications Office]]
| launch date = December 2012
}}

The '''EU Open Data Portal''' is the single point of access to a wide range of data held by EU institutions, agencies and other bodies. The portal is a key element of EU open data strategy.

== Legal basis and launch date ==

Launched in December 2012 in beta mode, the portal was formally established by Commission Decision of 12 December 2011 (2011/833/EU) on the reuse of Commission documents to promote accessibility and reuse.&lt;ref name="r1"&gt;{{cite news|url=http://eur-lex.europa.eu/legal-content/en/TXT/?uri=CELEX:32011D0833|title=Commission Decision of 12 December 2011 (2011/833/EU)}}&lt;/ref&gt; 

While the operational management of the portal is the task of the [[Publications Office of the European Union]], implementation of EU open data policy is the responsibility of the [[Directorate General for Communications Networks, Content and Technology]] of the European Commission.

== Features ==

The portal allows anyone to easily search, explore, link, download and reuse the data for commercial or non-commercial purposes, through a catalogue of common metadata. Through this catalogue, users access data stored on the websites of the EU institutions, agencies and other bodies.

Semantic technologies offer new functionalities. The metadata catalogue can be searched via an interactive search engine (Data tab) and through [[SPARQL]] queries (Linked data tab). There is also a showcase of visualisation applications from various EU institutions, agencies and other bodies.

Users can suggest data they would like the portal to be linked to, give feedback on the quality of data obtainable and share information with other users about how they have used it.

The interface is in 24 EU official languages, while most [[metadata]] are currently available in a limited number of languages (English, French and German). Some of the metadata (e.g. names of the data providers, geographical coverage) are in 24 languages following the translation of [[controlled vocabulary]] lists that are used by the portal.&lt;ref name="r3"&gt;{{cite news|url=http://publications.europa.eu/mdr/authority/index.html|title=EU controlled vocabularies}}&lt;/ref&gt;

== Terms of reuse ==

Most data accessible via the EU Open Data Portal are covered by the Europa Legal Notice &lt;ref name="r6"&gt;{{cite news|url=http://ec.europa.eu/geninfo/legal_notices_en.htm|title=Europa Legal Notice}}&lt;/ref&gt; and can be reused free of charge, for commercial and non-commercial purposes, provided that the source is acknowledged. Specific conditions on reuse, related mostly to the protection of third-party intellectual property rights, apply for a very limited amount of data.

== Data available ==

The portal contains a very wide variety of high-value open data across EU policy domains, as also more recently identified by the G8 Open Data Charter. These include the economy, employment, science, environment and education. The number of data providers — which include [[Eurostat]], the [[European Environment Agency]] and the [[Joint Research Centre]] — continues to grow.

So far, around 56 EU institutions, bodies or departments (e.g. Eurostat, the European Environment Agency, the Joint Research Centre and other European Commission Directorates General and EU Agencies) have made datasets available, making a total of over 7,800.

In addition to giving access to datasets, the portal also is an easy entry point to a whole range of visualisation applications using EU data. The applications are displayed as much for their information value as for giving examples of what applications can be made using the data.

== Architecture of the portal ==

The portal is built using open source solutions such as the [[Drupal]] content management system and [[CKAN]], the data catalogue software developed by the [[Open Knowledge Foundation]]. It uses Virtuoso as an [[Resource Description Framework|RDF]] database and has a [[SPARQL]] endpoint.

Its metadata catalogue is built on the basis of international standards such as [[Dublin Core]], the data catalogue vocabulary DCAT and the asset description metadata schema [[ADMS]].&lt;ref name="r5"&gt;{{cite news|url=http://ec.europa.eu/digital-agenda/en/open-data-portals|title=Open Data Portals in Europe}}&lt;/ref&gt;

==See also==

*[[Open data]]
*[[Institutions of the European Union]] 
*[[Agencies of the European Union]]
*[[Bodies of the European Union]]
*[[European Data Portal]]

==References==
{{reflist}}

==External links==
* [http://ec.europa.eu/europe2020/index_en.htm Europe 2020 – Official EU Site]
* [http://ec.europa.eu/digital-agenda/ Digital Agenda for Europe]
* [http://ec.europa.eu/digital-agenda/en/open-data-0 Open Data section of above site]
* [https://joinup.ec.europa.eu/community/ods/description Joinup community on EU open data]
* [http://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:52011DC0882 Communication ‘Open data — An engine for innovation, growth and transparent governance’]
* [https://ec.europa.eu/digital-agenda/en/legislative-measures Legal rules on public services information]
* [http://okfn.org/ Open Knowledge Foundation]
* [http://dublincore.org/ Dublin Core]
* [http://latc-project.eu/ Publication and usage of linked data on the Web]
* [http://datacatalogs.org/group/eu-official Data catalogues]
* [http://5stardata.info/ Open data classification by [[Tim Berners Lee]]] 
* [http://opendatachallenge.org/ Open Data Challenge (now over)]

[[Category:European Commission]]
[[Category:Open data]]
[[Category:Transparency (behavior)]]
[[Category:Open government]]
[[Category:Semantic Web]]
[[Category:Data management]]
[[Category:Creative Commons]]</text>
      <sha1>lzt69kb0mi6wqgg3jtfdl5iox0h7cue</sha1>
    </revision>
  </page>
  <page>
    <title>Content-oriented workflow models</title>
    <ns>0</ns>
    <id>38907061</id>
    <revision>
      <id>738697827</id>
      <parentid>716152100</parentid>
      <timestamp>2016-09-10T14:50:06Z</timestamp>
      <contributor>
        <username>Bender235</username>
        <id>88026</id>
      </contributor>
      <minor />
      <comment>/* Distributed Document-oriented */clean up; HTTP&amp;rarr;HTTPS for [[Github]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="18553" xml:space="preserve">{{Orphan|date=May 2014}}

The goal of '''content-oriented workflow models''' is to articulate workflow progression by the presence of content units (like data-records/objects/documents).
Most content-oriented workflow approaches provide a life-cycle model for content units, such that workflow progression can be qualified by conditions on the state of the units.
Most approaches are research and work in progress and the content models and life-cycle models are more or less formalized.

The term ''content-oriented workflows'' is an umbrella term for several scientific workflow approaches, namely "data-driven", "resource-driven", "artifact-centric", "object-aware", and "document-oriented". Thus, the meaning of "content" ranges from simple data attributes to self-contained documents; the term "content-oriented workflows" appeared at first in &lt;ref name="Neumann2010" /&gt; as an umbrella term. Such general term, independent from a specific approach, is necessary to contrast the content-oriented modelling principle with traditional activity-oriented workflow models (like [[Petri net]]s or [[Business Process Model and Notation|BPMN]]) where a workflow is driven by a control flow and where the content production perspective is neglected or even missing.

The term "content" was chosen to subsume the different levels in granularity of the content units in the respective workflow models; it was also chosen to make associations with [[content management]]. Both terms "artifact-centric" and "data-driven" would also be good candidates for an umbrella term, but each is closely related to a specific approach of a single working group. The "artifact-centric" group itself (i.e. IBM Research) has generalized the characteristics of their approach and has used "information-centric" as an umbrella term in.&lt;ref name="Kumaran2008" /&gt; Yet, the term [[information]] is too unspecific in the context of computer science, thus, "content-orientated workflows" is considered as good compromise.

== Workflow Model Approaches ==

=== Data-driven ===

The data-driven process structures provides a sophisticated workflow model being specialized on hierarchical write-and-review-processes.
The approach provides interleaved synchronization of sub-processes and extends activity diagrams.
Unfortunately, the COREPRO prototype implementation is not publicly available.

Research on the project had been ceased. The general idea has been continued by Reichert in form of the [[#Object-aware]] approach.

; Synonyms
: data-driven process structures / data-driven modeling and coordination
;Protagonists
: Dr. Dominic Müller (University of Twente), Joachim Herbst (DaimlerChrysler Research), and Manfred Reichert (at this time [http://wwwhome.cs.utwente.nl/~reichertm/index01.htm Assoc. Prof. at Univ. of Twente], currently [http://www.uni-ulm.de/en/in/institute-of-databases-and-information-systems/staff/manfred-reichert.html Prof. at Ulm Univ.])
;Organization(s)
: University of Twente, DaimlerChrysler
;Period
: 2005 - 2007
;Selected publications
:&lt;ref name="Mueller2006" /&gt;&lt;ref name="Mueller2007" /&gt;
;Implementation
: [http://www.utwente.nl/ewi/is/research/completed_projects/completed_projects/corepro.doc/ COREPRO]

=== Resource-driven ===

The resource-driven workflow system is an early approach that considered workflows from a content-oriented perspective and emphasizes on the missing support for plain document-driven processes by traditional activity-oriented workflow engines.
The resource-driven approach demonstrated the application of database triggers for handling workflow events.
Still the system implementation is centralized and the workflow schema is statically defined.
The project appeared in 2005 but many aspects are considered future work by the authors.

Research did not continue on the project. Wang completed his PhD thesis in 2009, yet, his thesis does not mention the resource-driven approach to workflow modelling but is about discrete event simulation.

;Synonyms
: Resource-based Workflows / Document-Driven Workflow Systems
;Protagonists
: Jianrui Wang and [http://www.personal.psu.edu/axk41/ Prof. Akhil Kumar]
;Organization
: Pennsylvania State University
;Period
: 2005 - today
;Selected publications
:&lt;ref name="Wang2005" /&gt;&lt;ref name="Kumar2010" /&gt;
;Implementation
: N/A

=== Artifact-centric ===
{{See also|Artifact-centric business process model}}

The artifact-centric approach appears as a mature framework for general purpose content-oriented workflows.
The distribution of the enterprise application landscape with its business services is considered, yet, the workflow engine itself seems to be centralized.
The process enactment seems to be tightly coupled with a technically pre-integrated database management system infrastructure.
The latter makes it most suitable for manufacturing process or for organizational processes within a well-defined institutional scope.
The approach remains work in progress, still, it is a relatively old and established project on content-oriented workflows.
Funded by IBM, it has comparably high number of developers.
It is a promising approach.

;Synonyms
: artifact-centric business process models / artifact-based business process (ACP) / artifact-centric workflows
;Protagonists
: [http://domino.research.ibm.com/comm/research_people.nsf/pages/hull.index.html Richard Hull] and Dr. Kamal Bhattacharya as well as Cagdas E. Gerede and Jianwen Su
;Organization
: IBM (T.J. Watson Research Center, NY)
;Period
: 2007 - today
;Selected publications
:&lt;ref name="Bhattacharya2007" /&gt;&lt;ref name="Calvanese2009" /&gt;
;Implementation
: [http://domino.research.ibm.com/comm/research_projects.nsf/pages/artifact.index.html ArtiFact]

=== Object-aware ===

The object-aware approach manages a set of object types and generates forms for creating object instances.
The form completion flow is controlled by transitions between object configurations each describing a progressing set of mandatory attributes.
Each object configuration is named by an object state.
The data production flow is user-shifting and it is discrete by defining a sequence of object states.
The discussion is currently limited to a centralized system, without any workflows across different organizations.
However, the approach is of great relevance to many domains like concurrent engineering.
Finally, the object-aware approach and its PHILharmonicFlows system are going to provide general-purpose workflow systems for generic enactment of data production processes.

;Synonyms
: object-aware process management / datenorientiertes Prozess-Management-System
;Protagonists
: [http://www.uni-ulm.de/en/in/institute-of-databases-and-information-systems/staff/vera-kuenzle.html Vera Künzle] and [http://www.uni-ulm.de/en/in/institute-of-databases-and-information-systems/staff/manfred-reichert.html Prof. Manfred Reichert]
;Organization
: Ulm University
;Period
: 2009 - today
;Selected publications
:&lt;ref name="Kuenzle2009" /&gt;&lt;ref name="Kuenzle2010" /&gt;
;Implementation
: [http://www.uni-ulm.de/en/in/institute-of-databases-and-information-systems/research/projects/philharmonic-flows.html PHILharmonicFlows]

=== Distributed Document-oriented ===

Distributed document-oriented process management (dDPM) enables distributed case handling in heterogeneous system environments and it is based on document-oriented [[semantic integration|integration]].
The workflow model reflects the paper-based working practice in inter-institutional healthcare scenarios.
It targets distributed knowledge-driven ad-hoc workflows, wherein distributed information systems are required to coordinate work with initially unknown sets of actors and activities.

The distributed workflow engine supports process planning &amp; process history as well as participant management and process template creation with import/export.
The workflow engine embeds a functional fusion of 1) group-based instant messaging 2) with a shared work list editor 3) with version control.
The software implementation of dDPM is α-Flow which is available as open source.
dDPM and α-Flow provide a content-oriented approach to schema-less workflows.

The complete distributed case handling application is provided in form of a single active Document ("&amp;alpha;-Doc").
The α-Doc is a case file (as information carrier) with an embedded workflow engine (in form of active properties).
Inviting process participants is equivalent to providing them with a copy of an α-Doc, copying it like an ordinary desktop file.
All α-Docs that belong to the same case can synchronize each other, based on the participant management, electronic postboxes, store-and-forward messaging, and an offline-capable synchronization protocol.

;Synonyms
: distributed document-oriented process management (dDPM), distributed case handling via active documents
;Protagonists
: [http://www6.informatik.uni-erlangen.de/people/cpn/ Christoph P. Neumann] and [http://www6.informatik.uni-erlangen.de/people/lenz/ Prof. Richard Lenz]
;Organization
: Friedrich-Alexander-Universit&amp;auml;t Erlangen-N&amp;uuml;rnberg
;Period
: 2009 - 2012
;Selected Publications
:&lt;ref name="Neumann2011" /&gt;&lt;ref name="Neumann2012" /&gt; and a PhD thesis &lt;ref name="DissNeumann2012" /&gt;
;Implementation
: [https://github.com/cpnatwork/alphaflow_dev &amp;alpha;-Flow (open source)]

== Related Concepts ==

=== Content Management ===

The bandwidth of [[Content management system]]s (CMS) reaches from [[Web content management system]]s (WCMS) and [[Document management system]] (DMS) to [[Enterprise Content Management]] (ECM). Mature DMS products support document production workflows in a basic form, primarily focusing on review cycle workflows concerning a single document. Market leaders are [[Alfresco (software)|Alfresco]] , [[eXo Platform]] and EMC with [[Documentum]].

=== Groupware and Computer-Supported Cooperative Work ===
[[Groupware]] focuses on messaging (like E-Mail, Chat, and Instant Messaging), shared calendars (e.g. Lotus Notes, Microsoft Outlook with Exchange Server), and conferencing (e.g. Skype).
Groupware overlaps with [[Computer-supported cooperative work]] (CSCW), that originated from shared multimedia editors (for live drawing/sketching) and synchronous multi-user applications like [[desktop sharing]]. The extensive conceptual claim of CSWC must be put into perspective by its actual solution scope, that is available as the [[CSCW#CSCW Matrix|CSCW Matrix]].

=== Case Handling ===

The case handling paradigm stems from Prof. van der Aalst and gained momentum in 2005. The core features are:
(a) provide all information available, i.e. present the case as a whole rather than showing bits and pieces,
(b) decide about activities on the basis of the information available rather than the activities already executed,
(c) separate work distribution from authorization and allow for additional types of roles, not just the execute role, and
(d) allow workers to view and add/modify data before or after the corresponding activities have been executed.

In healthcare, the flow of a patient between healthcare professionals is considered as a workflow - with activities that include all kinds of diagnostic or therapeutic treatments. The workflow is considered as a case, and workflow management in healthcare is to handle these cases.

Case handling is orthogonal to content-oriented workflows. Some content-oriented workflow approaches are not related to case handling, but, for example, to automated manufacturing. In contrast, systems that are considered to be case handling systems (CHS) but which do not apply a content-oriented workflow model are, for example, BPMone (formerly PROTOS and FLOWer) from Pallas Athena, ECHO from Digital, CMDT from ICL, and Vectus from London Bridge Group. In conclusion, those content-oriented workflow approaches that are tightly related to case handling are the [[#Resource-driven]] workflow model and the [[#Distributed Document-oriented]] workflow model.

;Protagonists
: [http://wwwis.win.tue.nl/~wvdaalst/ Prof. Wil van der Aalst] and Associate Professor Dr. [http://reijers.com/ Hajo Reijers] (with focus on healthcare)
;Organization
: Univ. of Technology, Eindhoven
;Period
: 2001 - today
;Selected publication
:&lt;ref name="Reijers2003" /&gt;&lt;ref name="Aalst2005" /&gt;

== See also ==
* [[Process philosophy]]
* [[Workflow]]
* [[Adam Smith]]
* [[Process control]]
* [[Process management]]
* [[Business process]]
* [[Business process automation]]
* [[Business process management]]
* [[Business Process Model and Notation]]
* [[Advanced case management]]
* [[Content management system]]
* [[Web content management system]]
* [[Document management system]]
* [[Enterprise Content Management]]

== References ==
&lt;references&gt;
&lt;ref name="Neumann2010"&gt;Christoph P. Neumann and Richard Lenz: [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5541988 The alpha-Flow Use-Case of Breast Cancer Treatment - Modeling Inter-Institutional Healthcare Workflows by Active Documents]. In: Proc of the 8th Int'l Workshop on Agent-based Computing for Enterprise Collaboration (ACEC) at the 19th Int'l Workshops on Enabling Technologies: Infrastructures for Collaborative Enterprises (WETICE 2010), Larissa, Greece, June 2010. ([http://www6.informatik.uni-erlangen.de/publications/public/2010/acec2010_neumann_alphaUC.pdf PDF])&lt;/ref&gt;
&lt;ref name="Kumaran2008"&gt;Kumaran, S., R. Liu, and F. Wu. [http://www.springerlink.com/content/a837212173812011/ On the Duality of Information-Centric and Activity-Centric Models of Business Processes]. In: Advanced Information Systems Engineering. 2008. p. 32-47.&lt;/ref&gt;
&lt;ref name="Mueller2006"&gt;Dominic Müller, Manfred Reichert und Joachim Herbst. [http://www.springerlink.com/content/kpm9454157514825/ Flexibility of Data-Driven Process Structures]. In: Business Process Management Workshops Lecture Notes in Computer Science, 2006, Volume 4103/2006, 181-192. ([http://dbis.eprints.uni-ulm.de/111/2/Mueller06-DPM.pdf PDF])&lt;/ref&gt;
&lt;ref name="Mueller2007"&gt;Dominic Müller, Manfred Reichert und Joachim Herbst. [http://www.springerlink.com/content/2771670210653747/ Data-Driven Modeling and Coordination of Large Process Structures]. On the Move to Meaningful Internet Systems 2007: CoopIS, DOA, ODBASE, GADA, and IS Lecture Notes in Computer Science, 2007, Volume 4803/2007, 131-149. ([http://dbis.eprints.uni-ulm.de/116/1/Mueller07-CoopIS.pdf PDF])&lt;/ref&gt;
&lt;ref name="Bhattacharya2007"&gt;Kamal Bhattacharya, Cagdas Gerede, Richard Hull, Rong Liu, and Jianwen Su. 2007. [http://dl.acm.org/citation.cfm?id=1793141 Towards formal analysis of artifact-centric business process models]. In Proceedings of the 5th international conference on Business process management (BPM'07), Gustavo Alonso, Peter Dadam, and Michael Rosemann (Eds.). Springer-Verlag, Berlin, Heidelberg, 288-304. (pages 289ff in: [http://alumni.cs.ucsb.edu/~gerede/research/papers/bghls-bpm07-Artifact.pdf PDF])&lt;/ref&gt;
&lt;ref name="Calvanese2009"&gt;Diego Calvanese, Giuseppe De Giacomo, Richard Hull und Jianwen Su. [http://www.springerlink.com/content/t58342lj86807111/ Artifact-Centric Workflow Dominance]. Lecture Notes in Computer Science, 2009, Volume 5900/2009, 130-143. ([http://www.dis.uniroma1.it/~degiacom/papers/2009/ICSOC09.pdf PDF])&lt;/ref&gt;
&lt;ref name="Kuenzle2009"&gt;Vera Künzle und Manfred Reichert. [http://www.springerlink.com/content/n6448q47g0474242/ Towards Object-Aware Process Management Systems: Issues, Challenges, Benefits]. In: Enterprise, Business-Process and Information Systems Modeling Lecture Notes in Business Information Processing, 2009, Volume 29, Part 1, Part 5, 197-210, DOI: 10.1007/978-3-642-01862-6_17 ([http://dbis.eprints.uni-ulm.de/526/1/BPMDS09_Kuenzle_Reichert.pdf PDF])&lt;/ref&gt;
&lt;ref name="Kuenzle2010"&gt;Künzle, Vera and Reichert, Manfred. 2010. [http://dbis.eprints.uni-ulm.de/647/ Herausforderungen bei der Integration von Benutzern in Datenorientierten Prozess-Management-Systemen]. EMISA Forum, 30 (1). pp. 11-28. ISSN 1610-3351 ([http://dbis.eprints.uni-ulm.de/647/2/KuRe10.pdf PDF])&lt;/ref&gt;
&lt;ref name="Wang2005"&gt;Wang, J. and A. Kumar. [http://www.springerlink.com/content/79k8v7terwchn5ct/ A Framework for Document-Driven Workflow Systems]. In: Business Process Management. 2005. p. 285-301. ([http://php.scripts.psu.edu/faculty/a/x/axk41/BPM05-jerry-reprint.pdf PDF])&lt;/ref&gt;
&lt;ref name="Kumar2010"&gt;Akhil Kumar und Jianrui Wang. [http://www.springerlink.com/content/n218t085521q1347/ A Framework for Designing Resource-Driven Workflows]. In: Handbook on Business Process Management 1, International Handbooks on Information Systems, 2010, Part III, 419-440.&lt;/ref&gt;
&lt;ref name="Neumann2011"&gt;Christoph P. Neumann, Peter K. Schwab, Andreas M. Wahl and Richard Lenz. alpha-Adaptive: Evolutionary Workflow Metadata in Distributed Document-Oriented Process Management. In: Proc of the 4th Int'l Workshop on Process-oriented Information Systems in Healthcare (ProHealth'11) in conjunction with the 9th Int'l Conf on Business Process Management (BPM'11), Clermont-Ferrand, France, August 2011. ([http://www6.informatik.uni-erlangen.de/publications/public/2011/prohealth2011_neumann.pdf PDF])&lt;/ref&gt;
&lt;ref name="Neumann2012"&gt;Christoph P. Neumann and Richard Lenz. The alpha-Flow Approach to Inter-Institutional Process Support in Healthcare. International Journal of Knowledge-Based Organizations. IGI Global, 2012.&lt;/ref&gt;
&lt;ref name="DissNeumann2012"&gt;Christoph P. Neumann. [http://www.dr.hut-verlag.de/978-3-8439-0919-8.html Distributed Case Handling]. PhD thesis (German 'Dissertation'). Friedrich-Alexander-Universit&amp;auml;t Erlangen-N&amp;uuml;rnberg. 2012.&lt;/ref&gt;
&lt;ref name="Reijers2003"&gt;Hajo Reijers , Jaap Rigter , Wil Van Der Aalst. [http://www.worldscinet.com/ijcis/12/1203/S0218843003000784.html The Case Handling Case]. International Journal of Cooperative Information Systems (IJCIS), Volume: 12, Issue: 3(2003) pp. 365-391. ([http://is.tm.tue.nl/staff/hreijers/H.A.%20Reijers%20Bestanden/chc.pdf PDF])&lt;/ref&gt;
&lt;ref name="Aalst2005"&gt;[[Wil M.P. van der Aalst]], [[Mathias Weske]], Dolf Grünbauer. [http://www.sciencedirect.com/science/article/pii/S0169023X04001296 Case handling: a new paradigm for business process support]. In: Data &amp;amp; Knowledge Engineering, Volume 53, Issue 2, May 2005, Pages 129-162, ISSN 0169-023X, 10.1016/j.datak.2004.07.003. ([http://www.imamu.edu.sa/Scientific_selections/abstracts/AbstratctIT1/Case%20handling%20a%20new%20paradigm%20for%20business%20process%20support.pdf PDF])&lt;/ref&gt;
&lt;/references&gt;

{{DEFAULTSORT:Content-oriented Workflows}}
&lt;!--Categories--&gt;
[[Category:Workflow technology]]
[[Category:Data management]]</text>
      <sha1>oyyp9598ggb3s69ce7is6nr7s7jf40e</sha1>
    </revision>
  </page>
  <page>
    <title>Read–write conflict</title>
    <ns>0</ns>
    <id>217827</id>
    <revision>
      <id>731657231</id>
      <parentid>731653028</parentid>
      <timestamp>2016-07-26T18:30:03Z</timestamp>
      <contributor>
        <username>Chris the speller</username>
        <id>525927</id>
      </contributor>
      <minor />
      <comment>cap, punct</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1516" xml:space="preserve">{{Unreferenced|date=June 2008}}
In [[computer science]], in the field of [[database]]s, '''read–write conflict''', also known as '''unrepeatable reads''', is a computational anomaly associated with interleaved execution of transactions. 

Given a schedule S

:&lt;math&gt;S = \begin{bmatrix}
T1 &amp; T2 \\
R(A) &amp;  \\
 &amp; R(A) \\
 &amp; W(A)\\
 &amp; Com. \\
R(A) &amp; \\
W(A) &amp; \\
Com. &amp; \end{bmatrix}&lt;/math&gt;

In this example, T1 has read the original value of A, and is waiting for T2 to finish. T2 also reads the original value of A, overwrites A, and commits.

However, when T1 reads to A, it discovers two different versions of A, and T1 would be forced to [[Abort (computing)|abort]], because T1 would not know what to do. This is an unrepeatable read. This could never occur in a serial schedule. [[Strict two-phase locking]] (Strict 2PL) prevents this conflict.

== Real-world example==
[[Alice and Bob]] are using a website to book tickets for a specific show. Only one ticket is left for the specific show. Alice signs on first to see that only one ticket is left, and finds it expensive. Alice takes time to decide. Bob signs on and also finds one ticket left, and orders it instantly. Bob purchases and logs off. Alice decides to buy a ticket, to find there are no tickets. This is a typical read-write conflict situation.

== See also ==

* [[Concurrency control]]
* [[Write–read conflict]]
* [[Write–write conflict]]

{{DEFAULTSORT:Read-write conflict}}
[[Category:Data management]]
[[Category:Transaction processing]]</text>
      <sha1>lg2gxepwav2zv5ck5za7kpv3g1mccms</sha1>
    </revision>
  </page>
  <page>
    <title>Address space</title>
    <ns>0</ns>
    <id>507144</id>
    <revision>
      <id>759279035</id>
      <parentid>759276544</parentid>
      <timestamp>2017-01-10T06:49:50Z</timestamp>
      <contributor>
        <username>Flyer22 Reborn</username>
        <id>4293477</id>
      </contributor>
      <minor />
      <comment>Reverted 1 edit by [[Special:Contributions/41.189.162.22|41.189.162.22]] identified as test/vandalism using [[WP:STiki|STiki]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5148" xml:space="preserve">{{About|a concept used universally in computing|addressing specifically the main memory|Memory address}}
{{refimprove|date=December 2011}}

In [[computing]], an '''address space''' defines a range of discrete addresses, each of which may correspond to a [[network host]], [[peripheral device]], [[disk sector]], a [[computer data storage|memory]] cell or other logical or physical entity.

For software programs to save and retrieve stored data, each unit of data must have an address where it can be individually located or else the program will be unable to find and manipulate the data. The number of address spaces available will depend on the underlying address structure and these will usually be limited by the computer architecture being used.

Address spaces are created by combining enough uniquely identified qualifiers to make an address unambiguous (within a particular address space). For a person's physical address, the ''address space'' would be a combination of locations, such as a neighborhood, town, city, or country. Some elements of an address space may be the same– but if any element in the address is different than addresses in said space will reference different entities. An example could be that there are multiple buildings at the same address of "32 Main Street" but in different towns, demonstrating that different towns have different, although similarly arranged, [[street address]] spaces.

An address space usually provides (or allows) a partitioning to several regions according to the [[mathematical structure]] it has. In the case of [[total order]], as for [[memory address]]es, these are simply [[interval (mathematics)|chunks]]. Some nested domains hierarchy appears in the case of [[arborescence (graph theory)|directed ordered tree]] as for the [[Domain Name System]] or a [[directory structure]]; this is similar to the hierarchical design of [[postal address]]es. In the [[Internet]], for example, the [[Internet Assigned Numbers Authority]] (IANA) allocates ranges of [[IP address]]es to various registries in order to enable them to each manage their parts of the global Internet address space.&lt;ref&gt;{{cite web|url=http://www.iana.org/assignments/ipv4-address-space/ |title= IPv4 Address Space Registry |date=March 11, 2009 |publisher=Internet Assigned Numbers Authority (IANA) |accessdate= September 1, 2011}}&lt;/ref&gt;

==Examples==
Uses of addresses include, but are not limited to the following:
* [[Memory address]]es for [[main memory]], [[memory-mapped I/O]], as well as for [[virtual memory]];
* Device addresses on an [[expansion bus]];
* [[disk sector|Sector]] addressing for [[disk drive]]s;
* [[File name]]s on a particular [[volume (computing)|volume]];
* Various kinds of network host addresses in [[computer network]]s;
* [[Uniform resource locator]]s in the Internet.

== Address mapping and translation ==
[[File:CNFTL9.JPG|thumb|Illustration of translation from logical block addressing to physical geometry]]
Another common feature of address spaces are [[map (mathematics)|mappings and translations]], often forming numerous layers. This usually means that some higher-level address must be translated to lower-level ones in some way. 
For example, [[file system]] on a [[logical disk]] operates [[one-dimensional array|linear]] sector numbers, which have to be translated to ''absolute'' [[Logical block addressing|LBA]] sector addresses, in simple cases, via [[addition]] of the partition's first sector address. Then, for a disk drive connected via [[Parallel ATA]], each of them must be converted to ''logical'' (means fake) [[cylinder-head-sector]] address due to the interface historical shortcomings. It is converted back to LBA by the disk [[controller (computing)|controller]] and then, finally, to ''physical'' [[cylinder (disk drive)|cylinder]], [[disk head|head]] and [[track (disk drive)|sector]] numbers.

The [[Domain Name System]] maps its names to (and from) network-specific addresses (usually IP addresses), which in turn may be mapped to [[link layer]] network addresses via [[Address Resolution Protocol]]. Also, [[network address translation]] may occur on the edge of ''different'' IP spaces, such as a [[local area network]] and the Internet.

[[File:Virtual address space and physical address space relationship.svg|thumb|Virtual address space and physical address space relationship]]
An iconic example of virtual-to-physical address translation is [[virtual memory]], where different [[Page (computer memory)|pages]] of [[virtual address space]] map either to [[paging|page file]] or to main memory [[physical address]] space. It is possible that several numerically different virtual addresses all refer to one physical address and hence to the same physical byte of [[Random access memory|RAM]]. It is also possible that a single virtual address maps to zero, one, [[CPU cache#Homonym and synonym problems|or more than one]] physical address.

== See also ==
* [[Linear address space]]
* [[Name space]]
* [[Virtualization]]

== References ==
{{Reflist}}

[[Category:Computing terminology]]
[[Category:Data management]]
[[Category:Computer architecture]]</text>
      <sha1>ccs2vb2l5zp8050k06eumey469b764m</sha1>
    </revision>
  </page>
  <page>
    <title>Content repository</title>
    <ns>0</ns>
    <id>18877735</id>
    <revision>
      <id>750173701</id>
      <parentid>677710153</parentid>
      <timestamp>2016-11-18T04:50:43Z</timestamp>
      <contributor>
        <ip>61.82.109.244</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2343" xml:space="preserve">A '''content  repository''' or '''content store''' is a database of digital content with an associated set of data management, search and access methods allowing application-independent access to the content, rather like a digital library, but with the ability to store and modify content in addition to searching and retrieving. The content repository acts as the storage engine for a larger application such as a [[Content Management System]] or a [[Document Management System]], which adds a [[user interface]] on top of the repository's [[application programming interface]].&lt;ref&gt;[http://openacs.org/doc/acs-content-repository/design.html Content Repository Design], [http://openacs.org/doc/acs-content-repository/ ACS Content Repository], [http://openacs.org/ OpenACS.org].&lt;/ref&gt;

==Advantages provided by repositories==

*Common rules for data access allow many applications to work with the same content without interrupting the data.
*They give out signals when changes happen, letting other applications using the repository know that something has been modified, which enables collaborative data management.
*Developers can deal with data using programs that are more compatible with the desktop programming environment.
*The data model is scriptable when users use a content repository.

== Content repository features ==
A content repository may provide functionality such as:
* Add/edit/delete content
* Hierarchy and sort order management
* Query / search
* Versioning
* Access control
* Import / export
* Locking
* Life-cycle management
* Retention and holding / records management

== Examples ==

* [[Apache Jackrabbit]]
* ModeShape

==Applications==
*[[Content management]]
*[[Document management system|Document management]]
*[[Digital asset management]]
*[[Records management]]
*[[Revision control]]
*[[Social collaboration]]
*[[Web content management system|Web content management]]

== Standards and specification ==
*[[Content repository API for Java]]
*[[WebDAV]]
*[[Content Management Interoperability Services]]

== See also ==
* [[Information repository]]
* [[Content (media)]]

== References ==
{{reflist}}

==External links==
* [http://db-engines.com/en/ranking/content+store DB-Engines Ranking of Content Stores] by popularity, updated monthly

[[Category:Data management]]
[[Category:Content management systems]]</text>
      <sha1>bbh79vfx07038mk4sprn7q3xf4pjpj7</sha1>
    </revision>
  </page>
  <page>
    <title>Abstraction (software engineering)</title>
    <ns>0</ns>
    <id>60491</id>
    <revision>
      <id>754117328</id>
      <parentid>754115292</parentid>
      <timestamp>2016-12-10T23:04:57Z</timestamp>
      <contributor>
        <username>Greenrd</username>
        <id>15476</id>
      </contributor>
      <comment>/* Specification languages */ Specification phase is specific to waterfall projects, agile projects do not have a specification phase as such</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="30265" xml:space="preserve">{{Refimprove|date=June 2011}}
{{Quote box|quote=The essence of abstractions is preserving information that is relevant in a given context, and forgetting information that is irrelevant in that context.|source=– John V. Guttag&lt;ref&gt;{{Cite book | edition = Spring 2013 | publisher = The MIT Press | isbn = 9780262519632 | last = Guttag | first = John V. | title = Introduction to Computation and Programming Using Python | location = Cambridge, Massachusetts | date = 2013-01-18}}&lt;/ref&gt;|width=25%}}

In [[software engineering]] and [[computer science]], '''abstraction''' is a technique for arranging complexity of computer systems. It works by establishing a level of complexity on which a person interacts with the system, suppressing the more complex details below the current level. The programmer works with an idealized interface (usually well defined) and can add additional levels of functionality that would otherwise be too complex to handle. For example, a programmer writing code that involves numerical operations may not be interested in the way numbers are represented in the underlying hardware (e.g. whether they're ''16 bit'' or ''32 bit [[Integer (computer science)|integers]]''), and where those details have been suppressed it can be said that they were ''abstracted away'', leaving simply ''numbers'' with which the programmer can work. In addition, a task of sending an email message across continents would be extremely complex if the programmer had to start with a piece of fiber optic cable and basic hardware components. By using layers of complexity that have been created to abstract away the physical cables and network layout, and presenting the programmer with a virtual data channel, this task is manageable.

Abstraction can apply to control or to data: '''Control abstraction''' is the abstraction of actions while '''data abstraction''' is that of [[data structures]].

* Control abstraction involves the use of [[subroutine]]s and [[control flow]] abstractions
* Data abstraction allows handling pieces of data in meaningful ways. For example, it is the basic motivation behind the [[datatype]].

The notion of an [[object (computer science)|object]] in object-oriented programming can be viewed as a way to combine abstractions of data and code.

The same abstract definition can be used as a common [[Interface (computer science)|interface]] for a family of objects with different implementations and behaviors but which share the same meaning. The [[Inheritance (computer science)|inheritance]] mechanism in object-oriented programming can be used to define an [[Class (computer science)#Abstract|abstract class]] as the common interface.

The recommendation that programmers use abstractions whenever suitable in order to avoid duplication (usually [[code duplication|of code]]) is known as the [[Abstraction principle (computer programming)|abstraction principle]]. The requirement that a programming language provide suitable abstractions is also called the abstraction principle.

==Rationale==
Computing mostly operates independently of the concrete world: The hardware implements a [[model of computation]] that is interchangeable with others. The software is structured in [[software architecture|architecture]]s to enable humans to create the enormous systems by concentrating on a few issues at a time. These architectures are made of specific choices of abstractions. [[Greenspun's Tenth Rule]] is an [[aphorism]] on how such an architecture is both inevitable and complex.

A central form of abstraction in computing is language abstraction: new artificial languages are developed to express specific aspects of a system. ''[[Modeling languages]]'' help in planning. ''[[Computer language]]s'' can be processed with a computer. An example of this abstraction process is the generational development of [[programming language]]s from the [[First-generation programming language|machine language]] to the [[Second-generation programming language|assembly language]] and the [[Third-generation programming language|high-level language]]. Each stage can be used as a stepping stone for the next stage. The language abstraction continues for example in [[scripting language]]s and [[domain-specific programming language]]s.

Within a programming language, some features let the programmer create new abstractions. These include [[subroutine]]s, [[module (programming)|modules]], [[polymorphism (computer science)|polymorphism]], and [[software component]]s. Some other abstractions such as [[software design pattern]]s and [[software architecture#Architecture examples|architectural styles]] remain invisible to a [[translator (computing)|translator]] and operate only in the design of a system.

Some abstractions try to limit the range of concepts a programmer needs to be aware of, by completely hiding the abstractions that they in turn are built on. The software engineer and writer [[Joel Spolsky]] has criticised these efforts by claiming that all abstractions are ''[[leaky abstraction|leaky]]'' — that they can never completely hide the details below;&lt;ref&gt;{{cite web|last1=Spolsky|first1=Joel|title=The Law of Leaky Abstractions|url=http://www.joelonsoftware.com/articles/LeakyAbstractions.html}}&lt;/ref&gt; however, this does not negate the usefulness of abstraction.

Some abstractions are designed to interoperate with other abstractions - for example, a programming language may contain a [[foreign function interface]] for making calls to the lower-level language.

==Language features==

===Programming languages===
{{Main|Programming language}}

Different programming languages provide different types of abstraction, depending on the intended applications for the language. For example:

* In [[object-oriented programming language]]s such as [[C++]], [[Object Pascal]], or [[Java (programming language)|Java]], the concept of '''abstraction''' has itself become a declarative statement – using the [[keyword (computer programming)|keyword]]s ''&lt;code&gt;virtual&lt;/code&gt;'' (in [[C++]]) or ''&lt;code&gt;abstract&lt;/code&gt;''&lt;ref name="Oracle Java abstract"&gt;{{cite web|title=Abstract Methods and Classes|url=http://docs.oracle.com/javase/tutorial/java/IandI/abstract.html|website=The Java™ Tutorials|publisher=Oracle|accessdate=4 September 2014}}&lt;/ref&gt; and ''&lt;code&gt;interface&lt;/code&gt;''&lt;ref name="Oracle Java interface"&gt;{{cite web|title=Using an Interface as a Type|url=http://docs.oracle.com/javase/tutorial/java/IandI/interfaceAsType.html|website=The Java™ Tutorials|publisher=Oracle|accessdate=4 September 2014}}&lt;/ref&gt; (in [[Java (programming language)|Java]]). After such a declaration, it is the responsibility of the programmer to implement a [[Class (computer science)|class]] to instantiate the [[Object (computer science)|object]] of the declaration.
* [[Functional programming language]]s commonly exhibit abstractions related to functions, such as [[lambda abstraction]]s (making a term into a function of some variable) and [[higher-order function]]s (parameters are functions). &lt;!-- This has to be merged in the following sections. --&gt;
* Modern members of the Lisp programming language family such as [[Clojure]], [[Scheme (programming language)|Scheme]] and [[Common Lisp]] support [[Macro (computer science)#Syntactic macros|macro systems]] to allow syntactic abstraction. Other programming languages such as [[Scala (programming language)|Scala]] also have macros, or very similar [[metaprogramming]] features (for example, [[Haskell (programming language)|Haskell]] has [[Template Haskell]], and [[OCaml]] has [[MetaOCaml]]). These can allow a programmer to eliminate [[boilerplate code]], abstract away tedious function call sequences, implement new [[Control flow|control flow structures]], and implement [[Domain-specific language|Domain Specific Languages (DSLs)]], which allow domain-specific concepts to be expressed in concise and elegant ways. All of these, when used correctly, improve both the programmer's efficiency and the clarity of the code by making the intended purpose more explicit. A consequence of syntactic abstraction is also that any Lisp dialect and in fact almost any programming language can, in principle, be implemented in any modern Lisp with significantly reduced (but still non-trivial in some cases) effort when compared to "more traditional" programming languages such as [[Python (programming language)|Python]], [[C (programming language)|C]] or [[Java (programming language)|Java]].

===Specification methods===
{{Main|Formal specification}}

Analysts have developed various methods to formally specify software systems.  Some known methods include:

* Abstract-model based method (VDM, Z);
* Algebraic techniques (Larch, CLEAR, OBJ, ACT ONE, CASL);
* Process-based techniques (LOTOS, SDL, Estelle);
* Trace-based techniques (SPECIAL, TAM);
* Knowledge-based techniques (Refine, Gist).

===Specification languages===
{{Main|Specification language}}

Specification languages generally rely on abstractions of one kind or another, since specifications are typically defined earlier in a project, (and at a more abstract level) than an eventual implementation. The [[Unified Modeling Language|UML]] specification language, for example, allows the definition of ''abstract'' classes, which in a waterfall project, remain abstract during the architecture and specification phase of the project.

==Control abstraction==
{{Main|Control flow}}

Programming languages offer control abstraction as one of the main purposes of their use. Computer machines understand operations at the very low level such as moving some bits from one location of the memory to another location and producing the sum of two sequences of bits. Programming languages allow this to be done in the higher level. For example, consider this statement written in a [[Pascal (programming language)|Pascal]]-like fashion:

:&lt;code&gt;a := (1 + 2) * 5&lt;/code&gt;

To a human, this seems a fairly simple and obvious calculation (''"one plus two is three, times five is fifteen"''). However, the low-level steps necessary to carry out this evaluation, and return the value "15", and then assign that value to the variable "a", are actually quite subtle and complex. The values need to be converted to binary representation (often a much more complicated task than one would think) and the calculations decomposed (by the compiler or interpreter) into assembly instructions (again, which are much less intuitive to the programmer: operations such as shifting a binary register left, or adding the binary complement of the contents of one register to another, are simply not how humans think about the abstract arithmetical operations of addition or multiplication). Finally, assigning the resulting value of "15" to the variable labeled "a", so that "a" can be used later, involves additional 'behind-the-scenes' steps of looking up a variable's label and the resultant location in physical or virtual memory, storing the binary representation of "15" to that memory location, etc.

Without control abstraction, a programmer would need to specify ''all'' the register/binary-level steps each time they simply wanted to add or multiply a couple of numbers and assign the result to a variable. Such duplication of effort has two serious negative consequences:

# it forces the programmer to constantly repeat fairly common tasks every time a similar operation is needed
# it forces the programmer to program for the particular hardware and instruction set

===Structured programming===
{{Main|Structured programming}}

Structured programming involves the splitting of complex program tasks into smaller pieces with clear flow-control and interfaces between components, with reduction of the complexity potential for side-effects.

In a simple program, this may aim to ensure that loops have single or obvious exit points and (where possible) to have single exit points from functions and procedures.

In a larger system, it may involve breaking down complex tasks into many different modules. Consider a system which handles payroll on ships and at shore offices:

* The uppermost level may feature a menu of typical end-user operations.
* Within that could be standalone executables or libraries for tasks such as signing on and off employees or printing checks.
* Within each of those standalone components there could be many different source files, each containing the program code to handle a part of the problem, with only selected interfaces available to other parts of the program. A sign on program could have source files for each data entry screen and the database interface (which may itself be a standalone third party library or a statically linked set of library routines).
*Either the database or the payroll application also has to initiate the process of exchanging data with between ship and shore, and that data transfer task will often contain many other components.

These layers produce the effect of isolating the implementation details of one component and its assorted internal methods from the others. Object-oriented programming embraces and extends this concept.

==Data abstraction==
{{Main|Abstract data type}}

Data abstraction enforces a clear separation between the ''abstract'' properties of a [[data type]] and the ''concrete'' details of its implementation. The abstract properties are those that are visible to client code that makes use of the data type—the ''interface'' to the data type—while the concrete implementation is kept entirely private, and indeed can change, for example to incorporate efficiency improvements over time. The idea is that such changes are not supposed to have any impact on client code, since they involve no difference in the abstract behaviour.

For example, one could define an [[abstract data type]] called ''lookup table'' which uniquely associates ''keys'' with ''values'', and in which values may be retrieved by specifying their corresponding keys. Such a lookup table may be implemented in various ways: as a [[hash table]], a [[binary search tree]], or even a simple linear [[List (computing)|list]] of (key:value) pairs. As far as client code is concerned, the abstract properties of the type are the same in each case.

Of course, this all relies on getting the details of the interface right in the first place, since any changes there can have major impacts on client code. As one way to look at this: the interface forms a ''contract'' on agreed behaviour between the data type and client code; anything not spelled out in the contract is subject to change without notice.

&lt;!-- This makes no sense to me. [[User:TakuyaMurata|Taku]] 07:13, 19 June 2005 (UTC) --&gt;
Languages that implement data abstraction include [[Ada programming language|Ada]] and [[Modula-2]]. [[Object-oriented]] languages are commonly claimed{{By whom|date=March 2009}} to offer data abstraction; however, their [[Inheritance (computer science)|inheritance]] concept tends to put information in the interface that more properly belongs in the implementation; thus, changes to such information ends up impacting client code, leading directly to the [[Fragile binary interface problem]].

==Abstraction in object oriented programming==
{{Main|Object (computer science)}}

In [[object-oriented programming]] theory, '''abstraction''' involves the facility to define objects that represent abstract "actors" that can perform work, report on and change their state, and "communicate" with other objects in the system. The term [[Encapsulation (object-oriented programming)|encapsulation]] refers to the hiding of [[state (computer science)|state]] details, but extending the concept of ''data type'' from earlier programming languages to associate ''behavior'' most strongly with the data, and standardizing the way that different data types interact, is the beginning of '''abstraction'''.  When abstraction proceeds into the operations defined, enabling objects of different types to be substituted, it is called [[polymorphism (computer science)|polymorphism]]. When it proceeds in the opposite direction, inside the types or classes, structuring them to simplify a complex set of relationships, it is called [[Delegation (object-oriented programming)|delegation]] or [[Inheritance (computer science)|inheritance]].

Various object-oriented programming languages offer similar facilities for abstraction, all to support a general strategy of [[polymorphism (computer science)|polymorphism]] in object-oriented programming, which includes the substitution of one [[type in object-oriented programming|type]] for another in the same or similar role. Although not as generally supported, a [[configuration in object-oriented programming|configuration]] or image or package may predetermine a great many of these [[name binding|bindings]] at [[compile-time]], [[link-time]], or [[loadtime]]. This would leave only a minimum of such bindings to change at [[Run time (program lifecycle phase)|run-time]].

[[Common Lisp Object System]] or [[Self (programming language)|Self]], for example, feature less of a class-instance distinction and more use of delegation for [[polymorphism in object-oriented programming|polymorphism]]. Individual objects and functions are abstracted more flexibly to better fit with a shared functional heritage from [[Lisp programming language|Lisp]].

C++ exemplifies another extreme: it relies heavily on [[generic programming|templates]] and [[method overloading|overloading]] and other static bindings at compile-time, which in turn has certain flexibility problems.

Although these examples offer alternate strategies for achieving the same abstraction, they do not fundamentally alter the need to support abstract nouns in code - all programming relies on an ability to abstract verbs as functions, nouns as data structures, and either as processes.

Consider for example a sample [[Java (programming language)|Java]] fragment to represent some common farm "animals" to a level of abstraction suitable to model simple aspects of their hunger and feeding. It defines an &lt;code&gt;Animal&lt;/code&gt; class to represent both the state of the animal and its functions:

&lt;source lang=java&gt;
public class Animal extends LivingThing
{
     private Location loc;
     private double energyReserves;

     public boolean isHungry() {
         return energyReserves &lt; 2.5;
     }
     public void eat(Food food) {
         // Consume food
         energyReserves += food.getCalories();
     }
     public void moveTo(Location location) {
         // Move to new location
         this.loc = location;
     }
}
&lt;/source&gt;
With the above definition, one could create objects of type &lt;tt&gt;Animal&lt;/tt&gt; and call their methods like this:

&lt;source lang=java&gt;
thePig = new Animal();
theCow = new Animal();
if (thePig.isHungry()) {
    thePig.eat(tableScraps);
}
if (theCow.isHungry()) {
    theCow.eat(grass);
}
theCow.moveTo(theBarn);
&lt;/source&gt;
In the above example, the class ''&lt;code&gt;Animal&lt;/code&gt;'' is an abstraction used in place of an actual animal, ''&lt;code&gt;LivingThing&lt;/code&gt;'' is a further abstraction (in this case a generalisation) of ''&lt;code&gt;Animal&lt;/code&gt;''.

If one requires a more differentiated hierarchy of animals — to differentiate, say, those who provide milk from those who provide nothing except meat at the end of their lives — that is an intermediary level of abstraction, probably DairyAnimal (cows, goats) who would eat foods suitable to giving good milk, and MeatAnimal (pigs, steers) who would eat foods to give the best meat-quality.

Such an abstraction could remove the need for the application coder to specify the type of food, so s/he could concentrate instead on the feeding schedule. The two classes could be related using [[Inheritance (computer science)|inheritance]] or stand alone, and the programmer could define varying degrees of [[polymorphism (computer science)|polymorphism]] between the two types. These facilities tend to vary drastically between languages, but in general each can achieve anything that is possible with any of the others. A great many operation overloads, data type by data type, can have the same effect at compile-time as any degree of inheritance or other means to achieve polymorphism. The class notation is simply a coder's convenience.

===Object-oriented design===
{{Main|Object-oriented design}}

Decisions regarding what to abstract and what to keep under the control of the coder become the major concern of object-oriented design and [[domain analysis]]&amp;mdash;actually determining the relevant relationships in the real world is the concern of [[object-oriented analysis]] or [[legacy analysis]].

In general, to determine appropriate abstraction, one must make many small decisions about scope (domain analysis), determine what other systems one must cooperate with (legacy analysis), then perform a detailed object-oriented analysis which is expressed within project time and budget constraints as an object-oriented design. In our simple example, the domain is the barnyard, the live pigs and cows and their eating habits are the legacy constraints, the detailed analysis is that coders must have the flexibility to feed the animals what is available and thus there is no reason to code the type of food into the class itself, and the design is a single simple Animal class of which pigs and cows are instances with the same functions. A decision to differentiate DairyAnimal would change the detailed analysis but the domain and legacy analysis would be unchanged&amp;mdash;thus it is entirely under the control of the programmer, and we refer to abstraction in object-oriented programming as distinct from abstraction in domain or legacy analysis.

==Considerations==
When discussing [[formal semantics of programming languages]], [[formal methods]] or [[abstract interpretation]], '''abstraction''' refers to the act of considering a less detailed, but safe, definition of the observed program behaviors. For instance, one may observe only the final result of program executions instead of considering all the intermediate steps of executions. Abstraction is defined to a '''concrete''' (more precise) model of execution.

Abstraction may be '''exact''' or '''faithful''' with respect to a property if one can answer a question about the property equally well on the concrete or abstract model. For instance, if we wish to know what the result of the evaluation of a mathematical expression involving only integers +, -, ×, is worth [[modular arithmetic|modulo]] ''n'', we need only perform all operations modulo ''n'' (a familiar form of this abstraction is [[casting out nines]]).

Abstractions, however, though not necessarily '''exact''', should be '''sound'''. That is, it should be possible to get sound answers from them&amp;mdash;even though the abstraction may simply yield a result of [[undecidable problem|undecidability]]. For instance, we may abstract the students in a class by their minimal and maximal ages; if one asks whether a certain person belongs to that class, one may simply compare that person's age with the minimal and maximal ages; if his age lies outside the range, one may safely answer that the person does not belong to the class; if it does not, one may only answer "I don't know".

The level of abstraction included in a programming language can influence its overall [[usability]]. The [[Cognitive dimensions]] framework includes the concept of ''abstraction gradient'' in a formalism. This framework allows the designer of a programming language to study the trade-offs between abstraction and other characteristics of the design, and how changes in abstraction influence the language usability.

Abstractions can prove useful when dealing with computer programs, because non-trivial properties of computer programs are essentially [[undecidable problem|undecidable]] (see [[Rice's theorem]]). As a consequence, automatic methods for deriving information on the behavior of computer programs either have to drop termination (on some occasions, they may fail, crash or never yield out a result), soundness (they may provide false information), or precision (they may answer "I don't know" to some questions).

Abstraction is the core concept of [[abstract interpretation]]. [[Model checking]] generally takes place on abstract versions of the studied systems.

==Levels of abstraction==
{{Main|Abstraction layer}}

Computer science commonly presents ''levels'' (or, less commonly, ''layers'') of abstraction, wherein each level represents a different model of the same information and processes, but uses a system of expression involving a unique set of objects and compositions that apply only to a particular domain.
&lt;ref&gt;[[Luciano Floridi]], [http://www.cs.ox.ac.uk/activities/ieg/research_reports/ieg_rr221104.pdf ''Levellism and the Method of Abstraction'']
IEG – Research Report 22.11.04&lt;/ref&gt;
Each relatively abstract, "higher" level builds on a relatively concrete, "lower" level, which tends to provide an increasingly "granular" representation. For example, gates build on electronic circuits, binary on gates, machine language on binary, programming language on machine language, applications and operating systems on programming languages. Each level is embodied, but not determined, by the level beneath it, making it a language of description that is somewhat self-contained.

===Database systems===
{{Main|Database management system}}

Since many users of database systems lack in-depth familiarity with computer data-structures, database developers often hide complexity through the following levels:

[[Image:Data abstraction levels.png|thumb|Data abstraction levels of a database system]]

'''Physical level:''' The lowest level of abstraction describes ''how'' a system actually stores data. The physical level describes complex low-level data structures in detail.

'''Logical level:''' The next higher level of abstraction describes ''what'' data the database stores, and what relationships exist among those data. The logical level thus describes an entire database in terms of a small number of relatively simple structures. Although implementation of the simple structures at the logical level may involve complex physical level structures, the user of the logical level does not need to be aware of this complexity. This referred to as [[physical data independence]]. [[Database administrator]]s, who must decide what information to keep in a database, use the logical level of abstraction.

'''View level:''' The highest level of abstraction describes only part of the entire database. Even though the logical level uses simpler structures, complexity remains because of the variety of information stored in a large database. Many users of a database system do not need all this information; instead, they need to access only a part of the database. The view level of abstraction exists to simplify their interaction with the system. The system may provide many [[view (database)|view]]s for the same database.

===Layered architecture===
{{Main|Abstraction layer}}
The ability to provide a [[design]] of different levels of abstraction can

* simplify the design considerably
* enable different role players to effectively work at various levels of abstraction
* support the portability of [[software artifact]]s (model-based ideally)

[[Systems design]] and [[Business process modeling|business process design]] can both use this. Some [[Software modeling|design processes]] specifically generate designs that contain various levels of abstraction.

Layered architecture partitions the concerns of the application into stacked groups (layers).
It is a technique used in designing computer software, hardware, and communications in which system or network components are isolated in layers so that changes can be made in one layer without affecting the others.

==See also==
* [[Abstraction principle (computer programming)]]
* [[Abstraction inversion]] for an anti-pattern of one danger in abstraction
* [[Abstract data type]] for an abstract description of a set of data
* [[Algorithm]] for an abstract description of a computational procedure
* [[Bracket abstraction]] for making a term into a function of a variable
* [[Data modeling]] for structuring data independent of the processes that use it
* [[Encapsulation (object-oriented programming)|Encapsulation]] for abstractions that hide implementation details
* [[Greenspun's Tenth Rule]] for an aphorism about an (the?) optimum point in the space of abstractions
* [[Higher-order function]] for abstraction where functions produce or consume other functions
* [[Lambda abstraction]] for making a term into a function of some variable
* [[List of abstractions (computer science)]]
* [[Program refinement|Refinement]] for the opposite of abstraction in computing

==References==
{{Reflist}}
{{FOLDOC}}

==Further reading==
{{refbegin}}
* {{cite book|author1=Harold Abelson|author2=Gerald Jay Sussman|author3=Julie Sussman|title=Structure and Interpretation of Computer Programs|url=http://mitpress.mit.edu/sicp/full-text/book/book-Z-H-10.html|accessdate=22 June 2012|edition=2|date=25 July 1996|publisher=MIT Press|isbn=978-0-262-01153-2}}
* {{cite web|last=Spolsky|first=Joel|title=The Law of Leaky Abstractions|url=http://www.joelonsoftware.com/articles/LeakyAbstractions.html|work=Joel on Software|date=11 November 2002}}
* [http://www.cs.cornell.edu/courses/cs211/2006sp/Lectures/L08-abstraction/08_abstraction.html Abstraction/information hiding] - CS211 course, Cornell University.
* {{cite book|author=Eric S. Roberts|title=Programming Abstractions in C A Second Course in Computer Science|date=1997}}
* {{cite web|last=Palermo|first=Jeffrey|title=The Onion Architecture|url=http://jeffreypalermo.com/blog/the-onion-architecture-part-1/|work=Jeffrey Palermo|date=29 July 2008}} 
{{refend}}

==External links==
* [https://sites.google.com/site/simulationarchitecture/ SimArch] example of layered architecture for distributed simulation systems.

{{Use dmy dates|date=June 2011}}

{{DEFAULTSORT:Abstraction (computer science)}}
[[Category:Data management]]
[[Category:Programming paradigms]]
[[Category:Articles with example Java code]]
[[Category:Abstraction]]</text>
      <sha1>02reazahfcduvc7xhkos91cwkicknpt</sha1>
    </revision>
  </page>
  <page>
    <title>Datafication</title>
    <ns>0</ns>
    <id>41731546</id>
    <revision>
      <id>759548351</id>
      <parentid>702953646</parentid>
      <timestamp>2017-01-11T21:00:17Z</timestamp>
      <contributor>
        <username>Fixuture</username>
        <id>19796795</id>
      </contributor>
      <comment>added [[Category:Information society]] using [[WP:HC|HotCat]], +see also to [[Big data]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2038" xml:space="preserve">'''Datafication''' is a modern technological trend turning many aspects of our life into computerised data &lt;ref name="CukierMayer-Schoenberger2013"&gt;{{cite journal | last = Cukier | first =Kenneth | last2 = Mayer-Schoenberger | first2 = Viktor  | title =The Rise of Big Data | journal =Foreign Affairs | issue =May/June | pages = 28–40. | date =2013  | url = http://www.foreignaffairs.com/articles/139104/kenneth-neil-cukier-and-viktor-mayer-schoenberger/the-rise-of-big-data | accessdate = 24 January 2014}}&lt;/ref&gt; and transforming this information into new forms of value. 
&lt;ref name="SchuttOneil2014"&gt;
{{cite book
 | last = O'Neil | first =Cathy
 | last2 =Schutt
 | first2 = Rachel
| title =Doing Data Science
 | publisher =O’Reilly Media
 | date =2013
 | pages =406
  | isbn =978-1-4493-5865-5
 }}
&lt;/ref&gt;
Examples of datafication as applied to social and communication media are how [[Twitter]] datafies stray thoughts or datafication of [[Human resource management|HR]] by [[LinkedIn]] and others.  Alternative examples are diverse and include aspects of the built environment, and design via engineering and or other tools that tie data to formal, functional or other physical media outcomes of which [[Formsolver]]&lt;ref&gt;https://www.formsolver.com&lt;/ref&gt; is an example.

[[File:Shape optimization for buildings by formsolver.jpg|thumbnail|right|Example: Datafication of the skin and form of a building to assist engineers, designers and architects determine the performance of particular building geometries. Example provided courtesy of Formsolver.com]]
[[File:Emerging Shape Optimization Families for buildings by formsolver.jpg|thumbnail|right|Example: Shape families resulting from differing goals when data is used for the purposes of shape optimization. Example provided courtesy of Formsolver.com]]

==See also==
* [[Big data]]

==References==
{{Reflist}}

[[Category:Information science]]
[[Category:Technology forecasting]]
[[Category:Data management]]
[[Category:Big data]]
[[Category:Information society]]


{{Tech-stub}}</text>
      <sha1>l0hqjo4931c42rxj7oc1fqznwqdcrua</sha1>
    </revision>
  </page>
  <page>
    <title>PhUSE</title>
    <ns>0</ns>
    <id>43000868</id>
    <revision>
      <id>760307557</id>
      <parentid>751624586</parentid>
      <timestamp>2017-01-16T05:10:27Z</timestamp>
      <contributor>
        <username>Syced</username>
        <id>223682</id>
      </contributor>
      <comment>illustration</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1538" xml:space="preserve">{{unreferenced|date=June 2014}}
[[File:PhUSE Computational Science Symposium 2016 (26133831630).jpg|thumb|PhUSE Computational Science Symposium 2016]]
'''PhUSE''', or '''Pharmaceutical Users Software Exchange''' is an independent, [[not-for-profit]] organization, that started in Europe, but now which serves as forum and global platform for [[clinical data management]], [[biostatistics]], and [[eClinical]] [[information technology]] professionals. It provides three collaboration platforms for members, a set online suits which implements worldwide collaboration tools, a [[wiki]], a repository of videos (PhUSE Tube), a [[blog]], a [[webforum]] and an [[archive]].

PhUSE also publishes '''[[Pharmaceutical Programming]]''',  an [[academic journal]] focusing on programming for [[Drug regulation|drug regulation environments]] of the [[pharmaceutical]] industry, a quarterly [[newsletter]], '''PhUSE News'''. In addition, it organizes an annual [[Meeting|conference]].

==External links==

* [http://www.phuse.eu/ Official web site]
* [http://www.phusewiki.org/wiki/index.php?title=PhUSE_Wiki PhUSE Wiki]
* [http://www.phuse.eu/blog/blog.aspx Blog]
* [http://www.phuse.eu/Society-Newsletters.aspx Newsletter]
* [http://www.phuse.eu/forum.aspx Forum]
* [http://www.phuse.eu/archive.aspx Archive]
* [http://www.phuse.eu/phusetube.aspx PhUSE Tube]
* [http://www.phuse.eu/publications.aspx Publications]

[[Category:Pharmacy organizations]]
[[Category:Regulation|Therapeutic goods]]
[[Category:Biostatistics]]
[[Category:Data management]]</text>
      <sha1>ntpzz02eu5tii83k85te9i2ebb8f8rd</sha1>
    </revision>
  </page>
  <page>
    <title>ANSI 834 Enrollment Implementation Format</title>
    <ns>0</ns>
    <id>42783850</id>
    <revision>
      <id>760868602</id>
      <parentid>760742706</parentid>
      <timestamp>2017-01-19T15:16:18Z</timestamp>
      <contributor>
        <username>MrOllie</username>
        <id>6908984</id>
      </contributor>
      <comment>/* See also */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2401" xml:space="preserve">{{Orphan|date=June 2014}}
Edi 834 files.
[[American National Standards Institute|ANSI]] 834 EDI Enrollment Implementation [[File format|Format]] is a standard format for electronically exchanging health plan enrollment data between employers and [[health insurance]] carriers. An 834 file contains a string of data elements and each data element represents a fact, such as a subscriber’s name, hire date, etc. The entire string is called a data segment. [[Health Insurance Portability and Accountability Act|The Health Insurance Portability and Accountability Act (HIPAA)]] requires that all health plans or health insurance carriers accept a standard enrollment format, ANSI 834A Version 5010. The ANSI 834A is the national standard for electronic enrollment and maintenance health plan.

The 834 is used to transfer enrollment information from the sponsor of the [[insurance]] coverage, benefits, or policy to a payer. The intent of this implementation guide is to meet the [[health care]] industry's specific need for the initial enrollment and subsequent maintenance of individuals who are enrolled in insurance products. This implementation guide specifically addresses the enrollment and maintenance of health care products only. One or more separate guides may be developed for life, flexible spending, and retirement products.

An example layout of an ANSI 834A Version 5010 file is shown below.

'''Sample File Output'''&lt;br /&gt;
INS*Y*18*030*XN*A*E**FT~&lt;br /&gt;
REF*OF*152239999~&lt;br /&gt;
REF*1L*Blue~&lt;br /&gt;
DTP*336*D8*20070101~&lt;br /&gt;
NM1*IL*1*BLUTH*LUCILLE****34*152239999~&lt;br /&gt;
N3*224 N DES PLAINES*6TH FLOOR~&lt;br /&gt;
N4*CHICAGO*IL*60661*USA~&lt;br /&gt;
DMG*D8*19720121*F*M~&lt;br /&gt;
HD*030**VIS**EMP~&lt;br /&gt;
DTP*348*D8*20111016~&lt;br /&gt;
INS*N*19*030*XN*A*E***N*N~&lt;br /&gt;
REF*OF*152239999~&lt;br /&gt;
REF*1L*Blue~&lt;br /&gt;
DTP*357*D8*20111015~&lt;br /&gt;
NM1*IL*1*BLUTH*BUSTER~&lt;br /&gt;
N3*224 N DES PLAINES*6TH FLOOR~&lt;br /&gt;
N4*CHICAGO*IL*60661*USA~&lt;br /&gt;
DMG*D**19911015*M-HD*030**VIS~&lt;br /&gt;
DTP*348*D8*20110101~&lt;br /&gt;
DTP*349*D8*20111015~

==See also==
* [[X12 Document List]]

==References==
{{reflist}}
* [http://getworkforce.com/ansi-834-file-layout/ "ANSI 834 File Layout"]
* [http://getworkforce.com/ansi-834-file-layout/ "Guardian Electronic User Guide 834 Enrollment and Maintenance"]
* [http://www.1edisource.com/transaction-sets?tset=834 "EDI 834 Benefit Enrollment and Maintenance"]

[[Category:Data management]]</text>
      <sha1>nxkmw9as3ynru7oy5nsxlcab39yp909</sha1>
    </revision>
  </page>
  <page>
    <title>Personal, Inc.</title>
    <ns>0</ns>
    <id>43509883</id>
    <revision>
      <id>742529608</id>
      <parentid>742419464</parentid>
      <timestamp>2016-10-04T07:16:07Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor />
      <comment>[[WP:CHECKWIKI]] error fixes using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="23354" xml:space="preserve">{{Infobox company |
| name=Personal, Inc.
| logo=[[File:Personal, Inc. logo.png]]
| type=[[Private company|Private]]
| foundation=2009
| location=[[Washington, D.C.]], US
| industry=[[Internet]]
| homepage={{URL|www.personal.com}}
}}

'''Personal''' (also referred to as Personal.com or Personal, Inc.) was a consumer [[Personal Data Service]] and [[identity management system]] for individuals to aggregate, manage and reuse their own data. It was re-launched in May 2016 as a collaborative data management and security solution for the workplace called TeamData.&lt;ref&gt;{{Cite web|url=http://www.prnewswire.com/news-releases/personalcom-becomes-teamdata-300275063.html|title=Personal.com Becomes TeamData|last=TeamData|website=www.prnewswire.com|access-date=2016-10-03}}&lt;/ref&gt;

Personal's consumer products included: the Data Vault with Cloud Sync for secure management and sharing of data and documents between an individual and other individuals, companies, sites, apps and devices; and Data Imports to import information from third parties, including [[Social networking services|social media services]], companies and the [[United States Department of Education|U.S. Department of Education]], and  the Fill It App for automated completion of web and mobile forms, logins and checkouts.

The Personal platform supported user-centric [[DataPortability|data management and portability]] for over 1,200 different types (or fields) of structured, machine-readable, human-readable data. The platform also provided tools and APIs for developers and companies to integrate Fill It and the Data Vault into their websites and applications, primarily to give data back to their customers so they can autofill web and mobile forms.

==History==
Personal was founded in 2009 in [[Washington, DC]] by the management team that built The Map Network, which was acquired by [[Nokia]]/[[Navteq|NAVTEQ]] in 2006.&lt;ref name=acquisition&gt;{{cite web |url=http://www.directionsmag.com/pressreleases/navteq-announces-agreement-to-acquire-the-map-network/110396 |title=NAVTEQ Announces Agreement to Acquire The Map Network |date=6 December 2006 |website=Directions Magazine |accessdate=21 August 2014}}&lt;/ref&gt; Founded in 1999, The Map Network (previously called URHere.com) built the first platform for places and events to create and distribute digital online and mobile maps, location data and content. The Map Network served as the official mapping solution for over 100 cities and thousands of events and venues, from the NFL Super Bowl to the Democratic and Republican National Conventions to the Smithsonian Institution. It also produced the most-used interactive map of 9/11 relief and rescue efforts.&lt;ref name=acquisition /&gt;&lt;ref&gt;{{cite web |url=http://www.prnewswire.com/news-releases/interactive-relief-and-rescue-map-aids-in-nyc-response-72052587.html |title=Interactive Relief and Rescue Map Aids in NYC Response |date=17 September 2001 |website=PR Newswire |accessdate=25 August 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://spatialnews.geocomm.com/dailynews/2001/sep/11/ |title=The Geospatial Industry's Response to Terrorism |date=11 September 2001 |website=GeoCommunity |accessdate=26 August 2014}}&lt;/ref&gt;

Called a “life management platform” by [[The Economist]]&lt;ref&gt;{{cite web |url=http://www.economist.com/blogs/babbage/2011/11/personal-data |title=A life-management platform? |last=L. |first=G. |date=17 November 2011 |website=The Economist |accessdate=8 August 2014}}&lt;/ref&gt; and a “personal encrypted cloud service” by TIME for its user-centric approach to data,&lt;ref name="time.com"&gt;{{cite web|url=http://time.com/3069834/how-to-take-control-of-your-personal-data/|title=How to Take Control of Your Personal Data|last=Stokes|first=Natasha|date=1 August 2014|website=Time Inc.|accessdate=8 August 2014}}&lt;/ref&gt; the company has been associated with both the [[Infomediary]] model originated in 1999 by [[John Hagel III]] and Mark Singer, as well as the [[Vendor relationship management|vendor relationship management (VRM)]] model developed by Doc Searls. Personal closed $7.6m in funding in December 2010, including [[Steve Case]]’s Revolution Ventures, Grotech Ventures, [[Allen &amp; Company]], [[Ted Leonsis]], Neil Ashe and [[Jonathan Miller (businessman)|Jonathan Miller]].&lt;ref&gt;{{cite web |url=http://techcrunch.com/2011/01/06/personal-raises-7m-from-steve-case-and-others-to-help-consumers-protect-their-digital-data/ |title=Personal Raises $7M From Steve Case And Others To Help Consumers Protect Their Digital Data |last=Rao |first=Leena |date=6 January 2011 |website=TechCrunch |accessdate=8 August 2014}}&lt;/ref&gt;

Personal was early to embrace “small data,” which it defines as “big data for the benefit of individuals.”&lt;ref&gt;{{cite web |url=http://blog.personal.com/2012/03/the-era-of-small-data-begins/ |title=The Era of Small Data Begins |last=Green |first=Shane |date=6 March 2012 |website=Personal |accessdate=20 August 2014 }}&lt;/ref&gt; The term “small data” may have been originally coined by [[Jeremie Miller]] of Sing.ly, who mentioned it in a talk at the Web 2.0 Summit in November 2011 and is cited in ''The Intention Economy''.&lt;ref&gt;{{cite web |url=http://siliconprairienews.com/2011/11/watch-jeremie-miller-present-singly-at-the-web-2-0-summit/ |title=Watch Jeremie Miller present Singly at the Web 2.0 Summitt |first=Michael |date=9 November 2011 |website=Silicon Prairie News |accessdate=8 August 2014}}&lt;/ref&gt; In 2011, Personal was a part of the first group of companies to join the [[Personal Data Ecosystem Consortium]]'s ''Startup Circle.''&lt;ref&gt;{{cite web |url=http://pde.cc/startup-circle/#2011 |title=Members of the PDEC Startup Circle |website=Personal Data Ecosystem Consortium |accessdate=20 August 2014}}&lt;/ref&gt; A Small Data [[Meetup (website)|Meetup]] group has also formed in New York City, bringing together technology, legal and business experts to exchange ideas about user-centric and user-driven models for internet products and services.&lt;ref&gt;http://www.meetup.com/smalldata/&lt;/ref&gt; Personal ultimately raised $24 million, including $4.5m from Bill Miller of [[Legg Mason]] and [[Esther Dyson]] of EDventures in October 2013.&lt;ref&gt;{{cite web |url=http://www.reuters.com/article/2013/10/15/idUS412005883920131015 |title=Personal raises $4.5 million to be the personal data vault we so desperately need |date=15 October 2013 |website=Reuters |accessdate=8 August 2014}}&lt;/ref&gt;

==Products and services ==

===Overview===
The Personal Platform was a privacy- and security-by-design platform for individuals to manage and reuse their own data and information. The Fill It app was a 1-click form-filling solution for web and mobile logins, checkouts and forms, and the Data Vault app served as the main cloud-based repository for a user's data. Personal helped individuals take control and benefit from their information while knowing that the information in their Data Vault remained legally theirs and could not be used without their permission.&lt;ref&gt;{{Cite web|url=http://www.zdnet.com/article/intel-execs-on-big-data-and-privacy-its-a-balancing-act/|title=Intel execs on big data and privacy: It's a balancing act {{!}} ZDNet|last=King|first=Rachel|website=ZDNet|access-date=2016-10-03}}&lt;/ref&gt;

===Data Vault with Cloud Sync===
Personal spent two years building the Personal Platform before launching its Data Vault product in beta in November 2011. Following [[Privacy by Design]] principles, Personal only enabled users to see or share the sensitive data and all the files they stored in their Data Vault. Such information was encrypted, and could only be decrypted with a user’s password. Only users could choose and know their passwords to their vault because Personal did not store user passwords – and therefore could not reset them without deleting a user’s sensitive data and all files stored in their vault.&lt;ref&gt;{{cite web |url=http://www.ipc.on.ca/images/Resources/pbd-pde.pdf |title=Privacy by Design and the Emerging Personal Data Ecosystem |last=Cavoukian |first=Ann |last2=Green |first2=Shane |date=October 2012 |website=Office of the Information and Privacy Commissioner |accessdate=8 August 2014}}&lt;/ref&gt; All Personal apps and services were linked to a user’s private Data Vault.

The Data Vault featured automatic synchronization of data and files added on any device logged into Personal. It also featured a “Secure Share” function that created a live, private network, allowing registered users to share access to data and files through an exchange of encrypted keys without the risk of transmitting the data or files through non-secure, direct means. It also allowed users to immediately update data across their own network and revoke access to it when they choose.

Personal launched its [[Android (operating system)|Android]] app on November 30, 2011.&lt;ref name=mashable&gt;{{cite web |url=http://mashable.com/2011/11/17/personal/ |title=Never Fill Out a Form Again? Personal Seeks to Be the Data Vault for Your Private Information |last=Parr |first=Ben |date=17 November 2011 |website=Mashable |accessdate=8 August 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=https://www.personal.com/s/pages/news/personal-android-release/ |title=Personal Releases Android App for Its Private, Personal Network and Data Vault Service |date=30 November 2011 |website=Personal, Inc. |accessdate=8 August 2014}}&lt;/ref&gt; The [[iOS]] Data Vault app was released on May 7, 2012.&lt;ref&gt;{{cite web |url=http://techcrunch.com/2012/05/07/personal-takes-its-secure-vault-for-all-of-your-private-digital-data-mobile-with-ios-app/ |title=Personal Takes Its Secure Vault For All Of Your Private, Digital Data Mobile WIth iOS App |last=Rao |first=Leena |date=7 May 2012 |website=TechCrunch |accessdate=8 August 2014}}&lt;/ref&gt; Personal officially launched its [[Application programming interface|application programming interface (APIs)]] on October 2, 2012 at the Mashery Business of APIs Conference.&lt;ref&gt;{{cite web |url=https://www.personal.com/s/pages/news/personal-launches-personal-platform/ |title=Personal Launches 'Personal Platform at Business of APIs Conference, Opening APIs for Developers |date=2 October 2012 |website=Personal, Inc. |accessdate=8 August 2014}}&lt;/ref&gt; A review by [[CNET]] highlighted the challenges of getting people to trust such a new service with their sensitive data and spending the time required entering enough data to make it useful.&lt;ref&gt;{{cite web |url=http://www.cnet.com/news/what-hump-personals-private-database-faces-challenges/ |title=What hump? Personal's private database faces challenges |last=Needleman |first=Rafe |date=30 November 2011 |website=CNET |accessdate=8 August 2014}}&lt;/ref&gt;

===Fill It App and Form Index===
When the Data Vault was launched in November 2011, ''[[Mashable]]'' posed the question: “Never Fill Out a Form Again?”&lt;ref name=mashable /&gt; The [[World Economic Forum]] in its February 2013 report highlighted the possibility of saving 10 billion hours globally “and improv[ing] the delivery of public and private sector services” through automated form-filling tools, specifically citing Personal’s Fill It app.&lt;ref&gt;{{cite web |url=http://www3.weforum.org/docs/WEF_IT_UnlockingValuePersonalData_CollectionUsage_Report_2013.pdf |title=Unlocking the Value of Personal Data: From Collection to Usage |date=February 2013 |website=World Economic Forum |accessdate=8 August 2014}}&lt;/ref&gt; In January 2013, Personal launched Fill It in beta as a web bookmarklet for automatic form-filling.&lt;ref&gt;{{cite web |url=http://www.digitaltrends.com/web/personal-coms-new-fill-it-feature-makes-quick-work-of-long-web-forms/#!bBp6iJ |title=PERSONAL.COM'S NEW FILL IT APP MAKES QUICK WORK OF LONG ONLINE FORMS |last=Couts |first=Andrew |date=16 January 2013 |website=Digital Trends |accessdate=8 August 2014}}&lt;/ref&gt;

On June 11, 2014, Personal released Fill It as a web extension and announced that it was publishing an index of over 140,000 1-click online forms at www.fillit.com.&lt;ref name=extensionlaunch&gt;{{cite web |url=http://tech.co/dc-based-startup-personal-launches-fill-it-for-quick-and-safe-auto-filling-on-online-forms-2014-06 |title=DC-Based Personal Launches Fill It for Quick and Safe Auto-Filling on Online Forms |last=Barba |first=Ronald |date=16 June 2014 |website=Tech Cocktail |accessdate=8 August 2014}}&lt;/ref&gt; The company also announced that a mobile version of the product will launch later in the year. According to a story in ''[[Tech Cocktail]]'' about the launch, Personal’s “web extension and mobile app are able to support over 1,200 different types of reusable data, even enabling them to unlock more confidential information so they can complete longer forms, including patient registrations, job applications, event registrations, school admissions, insurance and bank applications, and government forms.”&lt;ref name=extensionlaunch /&gt; In November 2014, a mobile version of Fill It was launched that could autofill mobile forms using APIs.&lt;ref&gt;{{Cite web|url=http://tech.co/personal-launches-fill-it-mobile-2014-11|title=Personal Launches Fill It Mobile at #pii2014|date=2014-11-13|language=en-US|access-date=2016-10-03}}&lt;/ref&gt;

Personal’s form portal ultimately indexed more than 500,000 forms with three components, which, together, allowed data to be captured and reused across any of the forms: (1) a form graph, which mapped individual form fields to the Personal ontology; (2) a semantic layer, which determined how data was required on a form (e.g. one field vs. three fields for a U.S. telephone number); and (3) a correlations graph, which helped individuals match their specific data to a form without looking at the data value (e.g. knowing which phone number is a mobile phone number, which address is a billing address, or that a person uses their middle name as a first name on most forms).&lt;ref&gt;{{cite web |url=http://tech.co/dc-startup-personal-university-data-privacy-security-2014-08 |title=Personal Launches "Personal University," a Video Series on Data Privacy and Security |last=Barba |first=Ronald |date=8 August 2014 |website=Tech Cocktail |accessdate=8 August 2014}}&lt;/ref&gt;

===Monetizing personal data===
With the [[initial public offering]] of [[Facebook]] in May 2012, there was media interest in the question of the monetary value of personal data and whether tools and services might emerge to help consumers monetize their own data. Personal was frequently cited as a company that could potentially offer such a service. Articles and pieces focusing on this subject have appeared in ''[[The New York Times]]'', ''[[AdWeek]]'', the ''[[MIT Technology Review]]'', and on ''[[CNN]]'' and ''[[National Public Radio]]''.&lt;ref&gt;{{cite web |url=http://www.nytimes.com/2012/02/13/technology/start-ups-aim-to-help-users-put-a-price-on-their-personal-data.html?_r=0 |title=Start-Ups Seek to Help Users Put a Price on Their Personal Data |last=Brustein |first=Joshua |date=12 February 2012 |website=The New York Times |accessdate=8 August 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://www.technologyreview.com/view/426235/is-personal-data-the-new-currency/ |title=Is Personal Data the New Currency? |last=Zax |first=David |date=30 November 2011 |website=MIT Technology Review |accessdate=8 August 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://edition.cnn.com/2012/02/24/tech/web/owning-your-data-online |title=Manage (and make cash with?) your data online |last=Gross |first=Doug |date=27 February 2012 |website=CNN |accessdate=8 August 2014}}&lt;/ref&gt; Company Co-founder and CEO Shane Green was quoted as saying that “the average American consumer would soon be able to realize over $1,000 per year” by granting limited, anonymous access to their data to marketers, but that figure was never supported by Green or the company.&lt;ref&gt;{{cite web |url=http://www.ft.com/cms/s/2/61c4c378-60bd-11e2-a31a-00144feab49a.html#axzz39ptB1It4 |title=Data mining offers rich seam |last=Palmer |first=Maija |date=18 February 2013 |website=Financial Times |accessdate=8 August 2014}}&lt;/ref&gt;

==Reception and Re-launch as TeamData==
Personal was the first online consumer-facing company to be named an Ambassador for [[Privacy by Design]] for its technical, business and legal commitments to providing users with control over the data they store in Personal’s service.&lt;ref&gt;{{cite web |url=http://www.privacybydesign.ca/index.php/ambassador/personal-com/ |title=Personal.com |website=Privacy by Design |accessdate=15 August 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://www.privacybydesign.ca/content/uploads/2010/03/2011-10-24-Personal.com_.pdf |title=Personal and Privacy by Design |website=Privacy by Design |accessdate=20 August 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://www.privacybydesign.ca/index.php/ambassador/joshua-p-galper/ |title=Joshua P. Galper |website=Privacy by Design |accessdate=20 August 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://www.privacybydesign.ca/index.php/ambassador/shane-green/ |title=Shane Green |website=Privacy by Design |accessdate=20 August 2014}}&lt;/ref&gt; The company received recognition for its user agreement, called the Owner Data Agreement,&lt;ref&gt;{{cite web |url=https://www.personal.com/owner-data-agreement/ |title=Owner Data Agreement |date=7 February 2014 |website=Personal, Inc. |accessdate=8 August 2014}}&lt;/ref&gt; which acted like a reverse license agreement when data was shared between registered parties and emphasized that data ownership resides with the user. [[Doc Searls]] wrote in ''[[The Intention Economy: When Customers Take Charge]]'' that the Owner Data Agreement “had no precedent and modeled a new legal position, both for vendors and for intermediaries.”&lt;ref&gt;{{cite book |last=Searls |first=Doc |date=May 1, 2012 |title=The Intention Economy: When Customers Take Charge |publisher=Harvard Business Review Press |page=186 |isbn=978-1422158524 }}&lt;/ref&gt; 
[[Fast Company (magazine)|Fast Company]] called the Data Vault “a tool that will simplify our lives.”&lt;ref&gt;{{cite web |url=http://www.fastcompany.com/1836521/personalcom-creates-online-vault-manage-all-your-data |title=PERSONAL.COM CREATES AN ONLINE VAULT TO MANAGE ALL YOUR DATA |last=Boyd |first=E.B. |date=7 May 2012 |website=Fast Company |accessdate=8 August 2014}}&lt;/ref&gt; Personal has been included in case studies by Ctrl-Shift and Forrester regarding Personal Data Stores and Personal Identity Management.&lt;ref&gt;{{cite web |url=https://www.ctrl-shift.co.uk/index.php/research/product/64 |title=Personal Data Stores |website=Ctrl-Shift |accessdate=20 August 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://blog.personal.com/uploads/2011/10/Forrester-Research-personal_identity_management.pdf |last=Khatibloo |first=Fatemeh |last2=Frankland |first2=Dave |last3=Maler |first3=Eve |last4=Smith |first4=Allison |date=30 September 2011 |title=Personal Identity Management |website=Forrester |accessdate=20 August 2014}}&lt;/ref&gt;

In 2011, Personal received the Innovator Spotlight Award at Privacy Identity Innovation Conference (pii2011) and participated in the Technology Showcase at pii2012.&lt;ref&gt;{{cite web |url=http://www.prweb.com/releases/2011/5/prweb8484188.htm |last=Fonseca |first=Natalie |title=Personal and Passtouch Receive Innovator Spotlight Award at Privacy Identity Innovation Conference (pii2011) |website=PRWeb |accessdate=20 August 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=https://www.privacyidentityinnovation.com/pii2012-seattle/pii2012-technology-showcase |title=pii2012 Technology Showcase |website=Privacy Identity Innovation |accessdate=20 August 2014}}&lt;/ref&gt; In 2012, TechHive named Personal as one of the top five apps or web services of [[SXSW]].&lt;ref&gt;{{cite web |url=http://www.techhive.com/article/251744/hot_apps_and_web_services_of_sxsw.html |last=Sullivan |first=Mark |date=13 March 2012 |title=Hot Apps and Web Services of SXSW |accessdate=20 August 2014}}&lt;/ref&gt; Personal won the 2013 Campus Technology Innovators Award with Lone Star College in July 2013.&lt;ref&gt;{{cite web |url=http://campustechnology.com/articles/2013/07/23/2013-innovators-awards.aspx |last=Raths |first=David |last2=Namahoe |first2=Kanoe |last3=Lloyd |first3=Meg |date=23 July 2013 |title=2013 Innovators Awards |website=Campus Technology |accessdate=20 August 2014}}&lt;/ref&gt; Personal was included in a list of Executive Travel Magazine's favorite travel apps for 2013 in its May/June issue.&lt;ref&gt;{{citation |url=http://www.executivetravelmagazine.com/articles/ets-favorite-travel-apps-for-2013 |last=Null |first=Christopher |title=ET's Favorite Travel Apps of 2013 |archiveurl=https://web.archive.org/web/20131023184535/http://www.executivetravelmagazine.com/articles/ets-favorite-travel-apps-for-2013 |archivedate=2013-10-23 |accessdate=20 August 2014}}&lt;/ref&gt;
In 2013, Personal was also included as part of NYU GovLab's Open Data 500 and was named by J. Walter Thompson as one of 100 things to watch for in 2014.&lt;ref&gt;{{cite web |url=http://www.opendata500.com/us/Personal-Inc/ |title=Personal, Inc. |website=Open Data 500 |accessdate=20 August 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://www.jwtintelligence.com/2013/12/100-watch-2014/#axzz2qyBVCrMs |last=Mack |first=Ann |date=26 December 2013 |title=100 Things to Watch in 2014 |website=JWT Intelligence |accessdate=20 August 2014}}&lt;/ref&gt; In 2015, the National Law Journal named Company Chief Policy Officer and General Counsel, Joshua P. Galper, as one of their 50 "Cybersecurity &amp; Privacy Trailblazers."&lt;ref&gt;{{Cite web|url=http://pdfserver.amlaw.com/nlj/flipbook/Cybersecurity_Trailblazers_2015/Cyber_Security_Trailblazers_2015_Web.html#p%253D14%2523p=8|title=Cybersecurity_Trailblazers|website=pdfserver.amlaw.com|access-date=2016-10-03}}&lt;/ref&gt;

In May 2016, Personal Co-Founder and CEO Shane Green announced the launch of TeamData with one of the other co-founders, Tarik Kurspahic, and new board chair [[Eric C. Anderson]]. TeamData focuses on the problem of securing and collaboratively managing data in the workplace, and is based on the technology and platform of Personal.&lt;ref&gt;{{Cite web|url=https://medium.com/@shanegreen/why-personal-com-graduated-to-teamdata-today-f75e0d539ba1#.yrkhukyec|title=Why Personal.com "graduated" to TeamData today|last=Green|first=Shane|date=2016-05-20|access-date=2016-10-03}}&lt;/ref&gt; Onboardly included the new collaborative TeamData solution in its list of "Top 10 apps to keep your team on track" and as part of its Top 50 list of "all time best content marketing tools."&lt;ref&gt;{{Cite web|url=http://onboardly.com/content-marketing/all-time-best-tools-for-content-marketing-teams/|title=All Time Best Tools for Content Marketing Teams via @Onboardly|date=2016-04-07|language=en-US|access-date=2016-10-03}}&lt;/ref&gt;

==References==
{{reflist|2}}

==External links==
*{{Official website}}
**{{URL|www.fillit.com|Fill It homepage}}
*{{ITunes Preview App|493536192|Personal}}
*{{ITunes Preview App|910517122|Fill It}}
*{{Google Play|com.personal.android|Personal}}
*{{Google Play|com.personal.fillit|Fill It}}
*[http://tech.co/tag/personal Personal] collected news and commentary at ''[[Tech Cocktail]]''
*{{Crunchbase|Personal|Personal}}

[[Category:American websites]]
[[Category:Android (operating system) software]]
[[Category:Companies based in Washington, D.C.]]
[[Category:Companies established in 2009]]
[[Category:Data management]]
[[Category:Data security]]
[[Category:Internet companies of the United States]]
[[Category:Internet properties established in 2009]]
[[Category:IOS software]]</text>
      <sha1>adwdknhj13sx1i4rkk0huqyf6zvnxhh</sha1>
    </revision>
  </page>
  <page>
    <title>Government Performance Management</title>
    <ns>0</ns>
    <id>26105075</id>
    <revision>
      <id>749479844</id>
      <parentid>745922747</parentid>
      <timestamp>2016-11-14T15:38:26Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor />
      <comment>5 archive templates merged to {{[[template:webarchive|webarchive]]}} ([[User:Green_Cardamom/Webarchive_template_merge|WAM]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8624" xml:space="preserve">{{Refimprove|date=October 2014}}
'''Government Performance Management''' (GPM) consists of a set of processes that help government organizations optimize their business performance. It provides a framework for organizing, automating, and analyzing business methodologies, metrics, processes and systems that drive business performance.&lt;ref&gt;{{cite web|url=http://www.information-management.com/bissues/20070301/2600312-1.html|title=Performance Management for Government|author=Michael Owellen|date=28 February 2007|work=BI Review Magazine|accessdate=22 October 2014}}&lt;/ref&gt; Some commentators{{who|date=October 2014}} see GPM as the next generation of [[business intelligence]] (BI) for governments. GPM helps governments to make use of their financial, human, material, and other resources.  In the past, owners have sought to drive strategy down and across their organizations; they have struggled to transform strategies into actionable metrics and they have grappled with meaningful analysis to expose the cause-and-effect relationships that, if understood, could give profitable insight to their operational decision-makers.  GPM software and methods allow a systematic, integrated approach that links government strategy to core processes and activities. "Running by the numbers" now means something: planning, budgeting, analysis, and reporting can give the measurements that empower management decisions.&lt;ref&gt;{{cite web|url=http://www.encyclopedia.com/doc/1O12-performancemanagement.html |accessdate=February 7, 2010 |deadurl=yes |archiveurl=https://web.archive.org/web/20090112223214/http://www.encyclopedia.com/doc/1O12-performancemanagement.html |archivedate=January 12, 2009 }}&lt;/ref&gt;

== Performance Management (PM) Market ==
According to [[Gartner]]{{citation needed|date=October 2014}}, the Enterprise Performance Management (EPM) suite market continues to experience strong momentum, growing 19% during 2007. This is slightly in advance of their earlier market sizing and forecast analysis, which anticipated 2007 revenue to be $1.836 million, representing an 18% year-over-year growth. In the latest forecast, Gartner believe that the market for EPM will be more than $3 billion by 2011, representing a 14.4% compound annual growth rate. Several factors contributed to the continued significant growth in EPM revenue during 2007:
* Many organizations replaced difficult-to-maintain, inflexible, or outmoded spreadsheets and homegrown financial applications.
* Continued growth in large enterprises was fueled by desires to achieve greater transparency and adherence to governance and compliance legislation.
* Increased demand for applications that support strategic plans and operational activities drove new momentum in the deployment of scorecards.
* There was increased demand from mid-size enterprises, representing one of the largest untapped and dynamic areas of the business application software sector.
* Advertising and PR from increasingly large vendors and system integrators are raising the EPM profile and generating greater demand.

Gartner also expects the Business Intelligence software market to reach $3 billion in 2009. "Companies around the world have purchased more than US $40 billion worth of enterprise applications, including ERP, CRM and HR, during the past few years," said Colleen Graham, principal research analyst at Gartner. "This has generated significant volumes of data in support of the operational processes they automate. By investing in BI, companies can further leverage their enterprise application investments and turn the torrent of data into meaningful insight to better measure performance, respond more quickly to market changes and opportunities and comply with an increasingly complex regulatory environment."&lt;ref&gt;{{cite web|url=http://www.gartner.com/press_releases/asset_144782_11.html|title=Gartner News Room|publisher=Gartner.com|accessdate=22 October 2014}}&lt;/ref&gt;

== ITWorx Government Performance Management (GPM) ==

[[ITWorx]] GPM is a bilingual, [[Microsoft]]-based framework that gives governments the capability to cascade, share, track, and update strategies and plans organization-wide. It creates detailed views of multi-source [[Key Performance Indicator]]s (KPIs) using customized [[balance scorecard]]s, dashboards, strategy maps, statistical charts, and reports, as well as provides ad hoc analytical and reporting tools.

== ITWorx GPM Features ==
{{Merge to|section=yes|ITWorx|date=October 2014}}
{{advert|date=October 2014}}
ITWorx GPM,&lt;ref&gt;[http://www.itworx.com/Solutions/MicrosoftGovernmentPerformance/Pages/default.aspx]  {{webarchive |url=https://web.archive.org/web/20110311185821/http://www.itworx.com/Solutions/MicrosoftGovernmentPerformance/Pages/default.aspx |date=March 11, 2011 }}&lt;/ref&gt; provides a top-down approach in recording government strategy. The strategy is cascaded and shared across government bodies to define objectives and balancing targets.

It also links strategy to execution. Operational plans are recorded, linked to strategies, assigned a time-range for implementation, broken down to initiatives and business activities, approved, and then propagated to all levels.

ITWorx GPM,&lt;ref&gt;[http://www.itworx.com/Solutions/MicrosoftGovernmentPerformance/Pages/default.aspx]  {{webarchive |url=https://web.archive.org/web/20110311185821/http://www.itworx.com/Solutions/MicrosoftGovernmentPerformance/Pages/default.aspx |date=March 11, 2011 }}&lt;/ref&gt; defines a time-range for implementing an initiative, its owners, cost drivers, budgets, KPIs, and targets; and links initiatives to strategic objectives. Business activities, contributing in strategy execution, are defined including their KPIs and targets. KPIs can be entered and configured manually, calculated using other KPIs, or extracted from external data sources.

ITWorx GPM,&lt;ref&gt;[http://www.itworx.com/Solutions/MicrosoftGovernmentPerformance/Pages/default.aspx]  {{webarchive |url=https://web.archive.org/web/20110311185821/http://www.itworx.com/Solutions/MicrosoftGovernmentPerformance/Pages/default.aspx |date=March 11, 2011 }}&lt;/ref&gt; enables the definition of government-specific business rules such as KPI calculation formulas; it also enables administration of system settings such as the configuration of the organization structure and definition of approval [[workflows]] for each organization unit. Furthermore, administrators can manage user roles and groups as well as archive plans and approvals.

ITWorx GPM,&lt;ref&gt;[http://www.itworx.com/Solutions/MicrosoftGovernmentPerformance/Pages/default.aspx]  {{webarchive |url=https://web.archive.org/web/20110311185821/http://www.itworx.com/Solutions/MicrosoftGovernmentPerformance/Pages/default.aspx |date=March 11, 2011 }}&lt;/ref&gt; calculates measurement formulas, compares actual values against targets, and performs analysis. Color-coded KPIs are represented through strategic and customized scorecards, dashboards, strategy maps, and statistical charts and graphs, in addition to ad hoc analytical and reporting tools.

The solution enables communication throughout the decision-making process by allowing users to post comments and discuss topics regarding a strategy, KPI, or report. Keeping a documented record of why and when decisions are made, ITWorx GPM retains the history of contributions.

ITWorx GPM,&lt;ref&gt;[http://www.itworx.com/Solutions/MicrosoftGovernmentPerformance/Pages/default.aspx]  {{webarchive |url=https://web.archive.org/web/20110311185821/http://www.itworx.com/Solutions/MicrosoftGovernmentPerformance/Pages/default.aspx |date=March 11, 2011 }}&lt;/ref&gt; provides a mechanism for policy-makers and strategy implementers to facilitate the strategic management process without compromising data. Government frontline officials are provided with a feedback channel to submit change requests and propositions to approved strategic plans, targets, and actual data while securing the validity and consistency of data.

Frontline officials can monitor performance though consolidated views while detailed views are provided for department and executive levels. Based on privileges, users can view rolled-up KPIs and drill-down for [[root cause analysis]] or corrective actions.

==References==
{{Reflist}}

==External links==
*[http://gpm.itworx.com Gpm.itworx.com]
*[http://www.microsoft.com/downloads/details.aspx?displaylang=en&amp;FamilyID=efdc60d3-2622-44f5-aa5d-2b79d10c93ab  Microsoft.com]
*[http://www.pcmag-mideast.com/gitex/tag/itworx/ Pcmag-mideast.com]
*[http://www.itp.net/578068-itworx-releases-new-gpm-suite Itp.net]

[[Category:Business intelligence]]
[[Category:Data management]]</text>
      <sha1>tw0pkk9vos6zhzj1pqjjqlj1rwf04eh</sha1>
    </revision>
  </page>
  <page>
    <title>Semantic heterogeneity</title>
    <ns>0</ns>
    <id>43972057</id>
    <revision>
      <id>757144058</id>
      <parentid>734610485</parentid>
      <timestamp>2016-12-29T01:38:16Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <comment>/* Classification of semantic heterogeneities */ simplify heading, tweak italics</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="13832" xml:space="preserve">{{About|semantic differences in data|other uses|Heterogeneity (disambiguation)}}

'''Semantic heterogeneity''' is when [[database schema]] or [[Data set|datasets]] for the same domain are developed by independent parties, resulting in differences in meaning and interpretation of data values.&lt;ref&gt;{{cite journal |title=Why your data won't mix |author=Alon Halevy|journal=Queue |volume=3 |issue=8 |year=2005 |url=http://queue.acm.org/detail.cfm?id=1103836}}&lt;/ref&gt; Beyond [[Data structure|structured data]], the problem of semantic heterogeneity is compounded due to the flexibility of [[semi-structured data]] and various [[Tag (metadata)|tagging]] methods applied to documents or [[unstructured data]]. Semantic heterogeneity is one of the more important sources of differences in [[Heterogeneous database system|heterogeneous datasets]].

Yet, for multiple data sources to [[Interoperability|interoperate]] with one another, it is essential to reconcile these [[Semantics|semantic]] differences. Decomposing the various sources of semantic heterogeneities provides a basis for understanding how to map and transform data to overcome these differences.

== Classification ==

One of the first known classification schemes applied to [[Semantic data model|data semantics]] is from William Kent more than two decades ago.&lt;ref&gt;{{cite conference |title=The many forms of a single fact |author=William Kent |conference=Proceedings of the IEEE COMPCON |date=February 27 – March 3, 1989 |location=San Francisco |number=HPL-SAL-88-8, Hewlett-Packard Laboratories, Oct. 21, 1988 | at=13 pp. |url=http://www.bkent.net/Doc/manyform.htm}}&lt;/ref&gt; Kent's approach dealt more with structural [[Data mapping|mapping]] issues than differences in meaning, which he pointed to [[Data dictionary|data dictionaries]] as potentially solving.

One of the most comprehensive classifications is from Pluempitiwiriyawej and Hammer, "Classification Scheme for Semantic and Schematic Heterogeneities in XML Data Sources".&lt;ref&gt;{{cite news |title=A classification scheme for semantic and schematic heterogeneities in XML data sources |author=Charnyote Pluempitiwiriyawej and Joachim Hammer |publisher=University of Florida |at=Technical Report TR00-004 |location=Gainesville, Florida |date=September 2000 |url=https://cise.ufl.edu/tr/DOC/REP-2000-396.pdf}}&lt;/ref&gt; They classify heterogeneities into three broad classes:

* ''[[Data structure|Structural]]'' conflicts arise when the schema of the sources representing related or overlapping data exhibit discrepancies. Structural conflicts can be detected when comparing the underlying schema. The class of structural conflicts includes generalization conflicts, aggregation conflicts, internal path discrepancy, missing items, element ordering, constraint and type mismatch, and naming conflicts between the element types and attribute names.
* ''[[Data domain|Domain]]'' conflicts arise when the semantics of the data sources that will be integrated exhibit discrepancies. Domain conflicts can be detected by looking at the information contained in the schema and using knowledge about the underlying data domains. The class of domain conflicts includes schematic discrepancy, scale or unit, precision, and data representation conflicts.
* ''[[Data]]'' conflicts refer to discrepancies among similar or related data values across multiple sources. Data conflicts can only be detected by comparing the underlying sources. The class of data conflicts includes ID-value, missing data, incorrect spelling, and naming conflicts between the element contents and the attribute values.

Moreover, mismatches or conflicts can occur between set elements (a "population" mismatch) or attributes (a "description" mismatch).

Michael Bergman expanded upon this schema by adding a fourth major explicit category of language, and also added some examples of each kind of semantic heterogeneity, resulting in about 40 distinct potential categories &lt;ref&gt;{{cite web |title=Sources and classification of semantic heterogeneities |author=M.K. Bergman |website=AI3:::Adaptive Information |date=6 June 2006 |accessdate=28 September 2014 |url=http://www.mkbergman.com/232/sources-and-classification-of-semantic-heterogeneities/}}&lt;/ref&gt;
.&lt;ref&gt;{{cite web |title=Big structure and data interoperability |author=M.K. Bergman |website=AI3:::Adaptive Information |date=12 August 2014 |accessdate=28 September 2014 |url=http://www.mkbergman.com/1782/big-structure-and-data-interoperability/}}&lt;/ref&gt; This table shows the combined 40 possible sources of semantic heterogeneities across sources:

{|  style="text-align: left; width: 100%;" border="1" cellpadding="3" cellspacing="0"
|  style="vertical-align: middle; text-align: center; font-weight: bold; background: #EFEFEF" | Class
|  style="vertical-align: middle; text-align: center; font-weight: bold; background: #EFEFEF" | Category
|  style="vertical-align: middle; text-align: center; font-weight: bold; background: #EFEFEF" | Subcategory
|  style="vertical-align: middle; text-align: center; font-weight: bold; background: #EFEFEF" | Examples
|-
| rowspan="8" colspan="1" |
'''[[Language]]'''
| rowspan="4" colspan="1" |
[[Character encoding|Encoding]]
| Ingest Encoding Mismatch
|
For example, [[US-ASCII|ASCII]] ''v'' [[UTF-8]]
|-
| Ingest Encoding Lacking
| Mis-recognition of tokens because not being parsed with the proper encoding
|-
| Query Encoding Mismatch
| For example, ASCII ''v'' UTF-8 in search
|-
| Query Encoding Lacking
| Mis-recognition of search tokens because not being parsed with the proper encoding
|-
| rowspan="4" colspan="1" | Languages
| Script Mismatch
| Variations in how parsers handle, say, stemming, white spaces or hyphens
|-
| Parsing / Morphological Analysis Errors (many)
| Arabic languages (right-to-left) ''v'' Romance languages (left-to-right)
|-
| Syntactical Errors (many)
|
Ambiguous sentence references, such as ''I'm glad I'm a man, and so is Lola'' ([[Lola (song)|Lola]] by [[Ray Davies]] and the [[Kinks]])
|-
| Semantics Errors (many)
| River ''bank'' ''v'' money ''bank'' ''v'' billiards ''bank'' shot
|-
| rowspan="17" colspan="1" | '''Conceptual'''
| rowspan="5" colspan="1" | Naming
| Case Sensitivity
| Uppercase ''v'' lower case ''v'' Camel case
|-
|
[[Synonym]]s
| United States ''v'' USA ''v'' America ''v'' Uncle Sam ''v'' Great Satan
|-
|
[[Acronym]]s
| United States ''v'' USA ''v'' US
|-
|
[[Homonym]]s
| Such as when the same name refers to more than one concept, such as Name referring to a person ''v'' Name referring to a book
|-
| Misspellings
| As stated
|-
| rowspan="1" colspan="2" | Generalization / Specialization
| When single items in one schema are related to multiple items in another schema, or vice versa. For example, one schema may refer to "phone" but the other schema has multiple elements such as "home phone", "work phone" and "cell phone"
|-
| rowspan="2" colspan="1" | Aggregation
| Intra-aggregation
| When the same population is divided differently (such as, Census ''v'' Federal regions for states, England ''v'' Great Britain ''v'' United Kingdom, or full person names ''v'' first-middle-last)
|-
| Inter-aggregation
| May occur when sums or counts are included as set members
|-
| rowspan="1" colspan="2" | Internal Path Discrepancy
| Can arise from different source-target retrieval paths in two different schemas (for example, hierarchical structures where the elements are different levels of remove)
|-
| rowspan="4" colspan="1" | Missing Item
| Content Discrepancy
| Differences in set enumerations or including items or not (say, US territories) in a listing of US states
|-
| Missing Content
| Differences in scope coverage between two or more datasets for the same concept
|-
| Attribute List Discrepancy
| Differences in attribute completeness between two or more datasets
|-
| Missing Attribute
| Differences in scope coverage between two or more datasets for the same attribute
|-
| rowspan="2" colspan="2" | Item Equivalence
|
When two types (classes or sets) are asserted as being the same when the scope and reference are not (for example, [[Berlin]] the city ''v''  [[States of Germany#Subdivisions|Berlin]] the official city-state)
|-
|
When two individuals are asserted as being the same when they are actually distinct (for example, [[John F. Kennedy]] the president ''v''  [[USS John F. Kennedy (CV-67)|''John F. Kennedy'']] the aircraft carrier)
|-
| rowspan="1" colspan="2" | Type Mismatch
| When the same item is characterized by different types, such as a person being typed as an animal ''v'' human being ''v'' person
|-
| rowspan="1" colspan="2" | Constraint Mismatch
| When attributes referring to the same thing have different cardinalities or disjointedness assertions
|-
| rowspan="9" colspan="1" |
'''[[Domain of discourse|Domain]]'''
| rowspan="4" colspan="1" | Schematic Discrepancy
| Element-value to Element-label Mapping
| rowspan="4" colspan="1" | One of four errors that may occur when attribute names (say, Hair ''v'' Fur) may refer to the same attribute, or when same attribute names (say, Hair ''v'' Hair) may refer to different attribute scopes (say, Hair ''v'' Fur) or where values for these attributes may be the same but refer to different actual attributes or where values may differ but be for the same attribute and putative value. &lt;br /&gt;&lt;br /&gt; Many of the other semantic heterogeneities herein also contribute to schema discrepancies
|-
| Attribute-value to Element-label Mapping
|-
| Element-value to Attribute-label Mapping
|-
| Attribute-value to Attribute-label Mapping
|-
| rowspan="2" colspan="1" | Scale or Units
| Measurement Type
| Differences, say, in the metric ''v'' English measurement systems, or currencies
|-
| Units
| Differences, say, in meters ''v'' centimeters ''v'' millimeters
|-
| rowspan="1" colspan="2" | Precision
| For example, a value of 4.1 inches in one dataset ''v'' 4.106 in another dataset
|-
| rowspan="2" colspan="1" |
[[Data representation]]
| Primitive Data Type
|
Confusion often arises in the use of literals ''v'' [[Uniform resource identifier|URIs]] ''v'' object types
|-
| Data Format
| Delimiting decimals by period ''v'' commas; various date formats; using exponents or aggregate units (such as thousands or millions)
|-
| rowspan="8" colspan="1" |
'''[[Data]]'''
| rowspan="5" colspan="1" | Naming
| Case Sensitivity
| Uppercase ''v'' lower case ''v'' Camel case
|-
| Synonyms
| For example, centimeters ''v'' cm
|-
| Acronyms
| For example, currency symbols ''v'' currency names
|-
| Homonyms
| Such as when the same name refers to more than one attribute, such as Name referring to a person ''v'' Name referring to a book
|-
| Misspellings
| As stated
|-
| rowspan="1" colspan="2" | ID Mismatch or Missing ID
| URIs can be a particular problem here, due to actual mismatches but also use of name spaces or not and truncated URIs
|-
| rowspan="1" colspan="2" | Missing Data
|
A common problem, more acute with closed world approaches than with [[Open world assumption|open world ones]]
|-
| rowspan="1" colspan="2" | Element Ordering
| Set members can be ordered or unordered, and if ordered, the sequences of individual members or values can differ
|}

A different approach toward classifying semantics and integration approaches is taken by [[Amit Sheth|Sheth]] et al.&lt;ref&gt;{{cite journal |title=Semantics for the semantic Web: the implicit, the formal and the powerful | author1=Amit P. Sheth|author2=Cartic Ramakrishnan|author3=Christopher Thomas|journal=Int’l Journal on Semantic Web &amp; Information Systems |volume=1 |issue=1 |pages=1–18 |date=2005 |url=http://www.informatik.uni-trier.de/~ley/db/journals/ijswis/ijswis1.html}}&lt;/ref&gt; Under their concept, they split semantics into three forms: implicit, formal and powerful. Implicit semantics are what is either largely present or can easily be extracted; formal languages, though relatively scarce, occur in the form of [[Ontology (information science)|ontologies]] or other [[description logic]]s; and powerful (soft) semantics are fuzzy and not limited to rigid set-based assignments. Sheth et al.'s main point is that [[first-order logic]] (FOL) or description logic is inadequate alone to properly capture the needed semantics.

== Relevant applications ==

Besides data interoperabiity, relevant areas in [[information technology]] that depend on reconciling semantic heterogeneities include [[data mapping]], [[semantic integration]], and [[enterprise information integration]], among many others. From the conceptual to actual data, there are differences in perspective, vocabularies, measures and conventions once any two data sources are brought together. Explicit attention to these semantic heterogeneities is one means to get the information to integrate or interoperate.

A mere twenty years ago, information technology systems expressed and stored data in a multitude of formats and systems. The Internet and Web protocols have done much to overcome these sources of differences. While there is a large number of categories of semantic heterogeneity, these categories are also patterned and can be anticipated and corrected. These patterned sources inform what kind of work must be done to overcome semantic differences where they still reside.

==See also==
* [[Data integration]]
* [[Data mapping]]
* [[Enterprise information integration]]
* [[Heterogeneous database system]]
* [[Interoperability]]
* [[Ontology-based data integration]]
* [[Schema matching]]
* [[Semantic integration]]
* [[Semantic matching]]
* [[Semantics]]

==References==
&lt;references/&gt;

==Further reading==
* [http://wiki.opensemanticframework.org/index.php/Classification_of_Semantic_Heterogeneity Classification of semantic heterogeneity]

[[Category:Data management]]
[[Category:Interoperability]]
[[Category:Knowledge management]]
[[Category:Semantics]]</text>
      <sha1>g5zzdt1wsg86kg07m7degw1yq19ymqq</sha1>
    </revision>
  </page>
  <page>
    <title>BBC Genome Project</title>
    <ns>0</ns>
    <id>44172733</id>
    <revision>
      <id>762519208</id>
      <parentid>762519193</parentid>
      <timestamp>2017-01-29T09:13:56Z</timestamp>
      <contributor>
        <username>Tim!</username>
        <id>203786</id>
      </contributor>
      <comment>added [[Category:Databases in the United Kingdom]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5276" xml:space="preserve">[[File:BBC Genome Logo.png|thumb|BBC Genome Logo]]

The '''BBC Genome Project''' is a digitised searchable database of programme listings from the [[Radio Times]] from the first issue in 1923, to 2009.&lt;ref name=About&gt;{{cite web|title=About this project|url=http://genome.ch.bbc.co.uk/about|publisher=[[BBC]]|accessdate=21 October 2014}}&lt;/ref&gt; 

==History==
===Prior===
BBC Genome is not the BBC's first online searchable database; in April 2006 the BBC gave the public access to Infax, the BBC's programme database. Infax contained around 900,000 entries, but not every programme ever broadcast, and it ceased operation in December 2007.&lt;ref name="About Infax"&gt;{{cite web|title=About This Prototype|url=http://open.bbc.co.uk/cataloguemeta/2005/11/about_this_prototype.html|publisher=[[BBC]]|accessdate=2 February 2016|archiveurl=https://web.archive.org/web/20060613100552/http://open.bbc.co.uk/cataloguemeta/2005/11/about_this_prototype.html|archivedate=13 June 2006}}&lt;/ref&gt; The front page of the website is still available to see via the [[Internet Archive]] [https://web.archive.org/web/20060512054648/http://open.bbc.co.uk/catalogue/infax here.] After Infax ceased, a message on the website said that it would be incorporating in the information into individual programme pages.&lt;ref name="Prototype End"&gt;{{cite web|title=This experimental prototype trial has now concluded|url=http://www.bbc.co.uk/archive/catalogue_offline.shtml|publisher=[[BBC]]|accessdate=2 February 2016}}&lt;/ref&gt; In 2012, it was replaced by the database Fabric but this is only for internal use within the BBC.

==''Radio Times''== 
[[File:BBC Genome OCR error.jpg|thumb|Screenshot of an OCR error (since corrected) in Genome. The text, "Uza TarbuclC's Christmas", should read "[[Liza Tarbuck]]'s Christmas".]]
In December 2012, the [[BBC]] completed a digitisation exercise, scanning the listings from [[Radio Times]] of all BBC programmes 1923-2009 from an entire run of about 4,500 copies of the magazine.&lt;ref name="Kelion"&gt;{{cite web|url=http://www.bbc.co.uk/news/technology-20625884|title=BBC finishes Radio Times archive digitisation effort|last=Kelion|first=Leo|work=[[BBC Online]]|accessdate=20 January 2013}}&lt;/ref&gt; They identified around five million programmes, involving 8.5 million actors, presenters, writers and technical staff.&lt;ref name="Kelion" /&gt; The listings are as published, in advance, and so do not include late changes or cancellations.

The issues were scanned at high resolution, producing [[TIF]] images and [[Optical Character Recognition]] (OCR) was then used to turn the text from the page into searchable text on the Genome database.&lt;ref name=About/&gt;

BBC Genome was released for public use on 15 October 2014.&lt;ref name="Hilbish"&gt;{{cite web|url=http://www.bbc.co.uk/blogs/aboutthebbc/posts/Genome-The-Radio-Times-Archive-is-now-live|title=Genome – Radio Times archive now live|last=Bishop|first=Hilary|work=[[BBC Online]]|accessdate=15 October 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite news|last=Sweney|first=Mark|title=BBC digitises Radio Times back issues|url=https://www.theguardian.com/media/2014/oct/16/bbc-digitises-radio-times-back-issues-genome-project|newspaper=Guardian|date=16 October 2014}} &lt;/ref&gt;

The aim of this project is to allow researchers to be able to find out information easier and to help [[BBC Archives]] to build up a picture of what exists and what is currently missing from the archive.&lt;ref&gt;{{cite web|title=BBC's Genome Project offers radio and TV archive listings|url=http://www.bbc.co.uk/news/technology-29643662|publisher=[[BBC]]|accessdate=21 October 2014|date=16 October 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last1=Sweney|first1=Mark|title=BBC digitises Radio Times back issues|url=https://www.theguardian.com/media/2014/oct/16/bbc-digitises-radio-times-back-issues-genome-project|publisher=[[The Guardian]]|accessdate=21 October 2014|date=16 October 2014}}&lt;/ref&gt; Corrections to OCR errors and changes to advertised schedules are being [[Crowdsourcing|crowdsourced]],&lt;ref name="Hilbish" /&gt; with over 180,000 user generated edits accepted as of January 2017. &lt;ref&gt;{{Cite web|url=http://genome.ch.bbc.co.uk/schedules/bbcone/london/1964-04-20|title=BBC One London - 20 April 1964 - BBC Genome|website=genome.ch.bbc.co.uk|access-date=2017-01-09}}&lt;/ref&gt; 

Each listing entry has a unique identifier which may be expressed as a URL. For example, the very first screening of ''[[Doctor Who]]'' is http://genome.ch.bbc.co.uk/8f81c193ba224e84981f353cae480d49 A broadcast programme may have more than one such identifier, if it was screened (and thus listed) on repeat occasions.

==See also==
*[[BBC Archives]]
* [[Timeline of the BBC]]

==References==
{{Reflist|2}}

==External links==

{{Wikidata property|P1573|BBC Genome identifiers}}

*[http://genome.ch.bbc.co.uk/ BBC Genome website]
*[http://www.bbc.co.uk/blogs/genome BBC Genome blog] 
*[http://twitter.com/bbcgenome/ BBC Genome on Twitter] 
*[http://www.facebook.com/bbcgenome/ BBC Genome on Facebook]

{{BBC}}

[[Category:BBC]]
[[Category:BBC New Media|Archives]]
[[Category:Data management]]
[[Category:Broadcasting websites]]
[[Category:British websites]]
[[Category:History of television in the United Kingdom]]
[[Category:History of radio]]
[[Category:BBC history]]
[[Category:Databases in the United Kingdom]]</text>
      <sha1>jaawtz7kv2x0n2jc0bmq8br0l5dy9qp</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Information privacy</title>
    <ns>14</ns>
    <id>44363114</id>
    <revision>
      <id>633318258</id>
      <timestamp>2014-11-11T01:41:38Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <comment>Robot: Moved from Category:Data privacy. Authors: Alex299006, DHN-bot, 213.223.20.191, 62.147.39.198, EmausBot, Addbot, Elmondo21st, Escarbot, TCY, C.Löser, SporkBot, Ugur Basak Bot, Eliyak, Pnm, Qu3a, Maurreen, Good Olfactory, Hooperbloob, ArnoldRein...</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="177" xml:space="preserve">{{Cat main}}

[[Category:Data management|Privacy]]
[[Category:Computer law|Privacy]]
[[Category:Privacy]]
[[Category:Computer security|Privacy]]
[[Category:Information|Privacy]]</text>
      <sha1>q4g66ivyer72zhhl9eaphjhi5pe856c</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Extract, transform, load tools</title>
    <ns>14</ns>
    <id>44363181</id>
    <revision>
      <id>633318793</id>
      <timestamp>2014-11-11T01:45:57Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <comment>Robot: Moved from Category:ETL tools. Authors: DePiep, TXiKiBoT, Addbot, EmausBot, DSisyphBot, JonHarder, 203.91.193.5, Luckas-bot, CG janitor, 203.116.208.9, Good Olfactory, Stevage, 88.217.121.60</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="201" xml:space="preserve">'''Extract, transform, load tools''' are software packages that facilitate the performing of [[Extract, transform, load|ETL]] tasks.

[[Category:Data management]]
[[Category:Data warehousing products]]</text>
      <sha1>tsdhmcux8y8jmna0iwhvk5ye6zpz8ql</sha1>
    </revision>
  </page>
  <page>
    <title>Data management</title>
    <ns>0</ns>
    <id>759312</id>
    <revision>
      <id>759832794</id>
      <parentid>759831987</parentid>
      <timestamp>2017-01-13T11:20:35Z</timestamp>
      <contributor>
        <username>Mean as custard</username>
        <id>10962546</id>
      </contributor>
      <minor />
      <comment>Reverted edits by [[Special:Contribs/Suraj jain2|Suraj jain2]] ([[User talk:Suraj jain2|talk]]) to last version by AnomieBOT</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11095" xml:space="preserve">{{lead too short|date=October 2014}}
'''Data management''' comprises all the [[List of academic disciplines|disciplines]] related to managing [[data]] as a valuable resource.

== Overview ==
The official definition provided by [[DAMA]] International, the professional organization for those in the data management profession, is: "Data Resource Management is the development and execution of architectures, policies, practices and procedures that properly manage the full data lifecycle needs of an enterprise." This definition is fairly broad and encompasses a number of professions which may not have direct technical contact with lower-level aspects of data management, such as [[relational database]] management.

[[File:The Data Lifecycle.jpg|thumb|The data lifecycle]]

Alternatively, the definition provided in the DAMA Data Management Body of Knowledge (&lt;ref&gt;[https://technicspub.com/dmbok/ DAMA-DMBOK]&lt;/ref&gt;) is:
"Data management is the development, execution and supervision of plans, policies, programs and practices that control, protect, deliver and enhance the value of data and information assets."&lt;ref&gt;"DAMA-DMBOK Guide (Data Management Body of Knowledge) Introduction &amp; Project Status" (Note: PDF no longer available online at https://www.dama.org, current version available for purchase)&lt;/ref&gt;

The concept of "Data Management" arose in the 1980s as technology moved from sequential processing (first cards, then tape) to [[random access]] processing.  Since it was now technically possible to store a single fact in a single place and access that using random access disk, those suggesting that "Data Management" was more important than "[[Process Management]]" used arguments such as "a customer's home address is stored in 75 (or some other large number) places in our computer systems."  During this period, random access processing was not competitively fast, so those suggesting "Process Management" was more important than "Data Management" used batch processing time as their primary argument.  As applications moved into real-time, [[interactive]] applications, it became obvious to most practitioners that both management processes were important.  If the data was not well defined, the data would be mis-used in applications.  If the process wasn't well defined, it was impossible to meet user needs.

==Corporate Data Quality Management==
[[Corporate Data Quality Management]] (CDQM) is, according to the [[EFQM|European Foundation for Quality Management]] and the Competence Center Corporate Data Quality (CC CDQ, University of St. Gallen), the whole set of activities intended to improve corporate data quality (both reactive and preventive). Main premise of CDQM is the business relevance of high-quality corporate data. CDQM comprises with following activity areas:.&lt;ref&gt;[https://benchmarking.iwi.unisg.ch/Framework_for_CDQM.pdf EFQM ; IWI-HSG: EFQM Framework for Corporate Data Quality Management. Brussels : EFQM Press, 2011]&lt;/ref&gt;
* '''Strategy for Corporate Data Quality''': As CDQM is affected by various business drivers and requires involvement of multiple divisions in an  organization; it must be considered a company-wide endeavor.
* '''Corporate Data Quality Controlling''': Effective CDQM requires compliance with standards, policies, and procedures. Compliance is monitored according to previously defined metrics and performance indicators and reported to stakeholders.
* '''Corporate Data Quality Organization''': CDQM requires clear roles and responsibilities for the use of corporate data. The CDQM organization defines tasks and privileges for decision making for CDQM.
* '''Corporate Data Quality Processes and Methods''': In order to handle corporate data properly and in a standardized way across the entire organization and to ensure corporate data quality, standard procedures and guidelines must be embedded in company’s daily processes.
* '''Data Architecture for Corporate Data Quality''': The data architecture consists of the data object model - which comprises the unambiguous definition and the conceptual model of corporate data - and the data storage and distribution architecture.
* '''Applications for Corporate Data Quality''': Software applications support the activities of Corporate Data Quality Management. Their use must be planned, monitored, managed and continuously improved.

== Topics in Data Management ==
Topics in Data Management, grouped by the DAMA DMBOK Framework,&lt;ref&gt;[http://dama-dach.org/dama-dmbok-functional-framework DAMA-DMBOK Functional Framework v3]&lt;/ref&gt; include:
{{colbegin|2}}
# [[Data governance]]
#* [[Data asset]]
#* [[Data governance]]
#* [[Data steward]]
# Data Architecture, Analysis and Design
#* [[Data analysis]]
#* [[Data architecture]]
#* [[Data modeling]]
# Database Management
#* [[Data maintenance]]
#* [[Database administration]]
#* [[Database management system]]
# Data Security Management
#* [[Data access]]
#* [[Data erasure]]
#* [[Data privacy]]
#* [[Data security]]
# Data Quality Management
#* [[Data cleansing]]
#* [[Data integrity]]
#* [[Data enrichment]]
#* [[Data quality]]
#* [[Data quality assurance]]
# Reference and Master Data Management
#* [[Data integration]]
#* [[Master data management]]
#* [[Reference data]]
# Data Warehousing and Business Intelligence Management
#* [[Business intelligence]]
#* [[Data mart]]
#* [[Data mining]]
#* Data movement ([[Extract, transform, load ]])
#* [[Data warehouse]]
# Document, Record and Content Management
#* [[Document management system]]
#* [[Records management]]
# Meta Data Management
#* [[Meta-data management]]
#* [[Metadata]]
#* [[Metadata discovery]]
#* [[Metadata publishing]]
#* [[Metadata registry]]
# Contact Data Management
#* [[Business continuity planning]]
#* [[Marketing operations]]
#* [[Customer data integration]]
#* [[Identity management]]
#* [[Identity theft]]
#* [[Data theft]]
#* [[ERP software]]
#* [[CRM software]]
#* [[Address (geography)]]
#* [[Postal code]]
#* [[Email address]]
#* [[Telephone number]]
{{colend}}

==Body of Knowledge==
The DAMA Guide to the Data Management Body of Knowledge" (DAMA-DMBOK Guide), under the guidance of a new DAMA-DMBOK Editorial Board. This publication is available from April 5, 2009.

==Usage==

In modern [[management fad|management usage]], one can easily discern a trend away from the term "data" in composite expressions to the term "[[information]]" or even "[[knowledge]]" when talking in a non-technical context. Thus there exists not only data management, but also [[information management]] and [[knowledge management]]. This is a misleading trend as it obscures that traditional data are managed or somehow [[data processing|processed]] on second looks.{{cn|date=June 2016}} The distinction between data and derived values can be seen in the [[information ladder]].{{cn|date=June 2016}} While data can exist as such, "information" and "knowledge" are always in the "eye" (or rather the brain) of the beholder and can only be measured in relative units.

Several organisations have established a data management centre (DMC)&lt;ref&gt;
For example: {{cite book
| last1                 = Kumar
| first1                = Sangeeth
| last2                 = Ramesh
| first2                = Maneesha Vinodini
| chapter               = Lightweight Management framework (LMF) for a Heterogeneous Wireless Network for Landslide Detection
| editor1-last          = Meghanathan
| editor1-first         = Natarajan
| editor2-last          = Boumerdassi
| editor2-first         = Selma
| editor3-last          = Chaki
| editor3-first         = Nabendu
| editor4-last          = Nagamalai
| editor4-first         = Dhinaharan
| title                 = Recent Trends in Networks and Communications: International Conferences, NeCoM 2010, WiMoN 2010, WeST 2010,Chennai, India, July 23-25, 2010. Proceedings
| url                   = https://books.google.com/books?id=8i5qCQAAQBAJ
| series                = Communications in Computer and Information Science
| volume                = 90
| publisher             = Springer
| publication-date      = 2010
| page                  = 466
| isbn                  = 9783642144936
| accessdate            = 2016-06-16
| quote                 = 4.4 Data Management Center (DMC)[:] The Data Management Center is the data center for all of the deployed cluster networks. Through the DMC, the LMF allows the user to list the services in any cluster member belonging to any cluster [...].
}}
&lt;/ref&gt;
for their operations.

==Integrated data management==

'''Integrated data management''' (IDM) is a tools approach to facilitate data management and improve performance. IDM consists of an integrated, modular environment to manage enterprise application data, and optimize data-driven applications over its [[Information Lifecycle Management|lifetime]].&lt;ref&gt;[http://www.ibm.com/developerworks/data/library/techarticle/dm-0807hayes/?S_TACT=105AGX11&amp;S_CMP=FP#ibm-content  Integrated Data Management: Managing data across its lifecycle] by Holly Hayes&lt;/ref&gt;&lt;ref&gt;[http://www.ibmsystemsmagmainframedigital.com/nxtbooks/ibmsystemsmag/mainframe_20090708/index.php#/34 Organizations thrive on Data] by Eric Naiburg&lt;/ref&gt;&lt;ref&gt;[http://download.boulder.ibm.com/ibmdl/pub/software/data/sw-library/data-management/optim/reports/fragmented.pdf Fragmented Management Across The Data Life Cycle Increases Cost And Risk] - A commissioned study conducted by Forrester Consulting on behalf of IBM October 2008&lt;/ref&gt;&lt;ref&gt;[http://publib.boulder.ibm.com/infocenter/idm/v2r1/index.jsp integrated IBM Data Management information center]&lt;/ref&gt; IDM's purpose is to:
*Produce enterprise-ready applications faster
*Improve data access, speed iterative testing
*Empower collaboration between architects, developers and DBAs
*Consistently achieve service level targets
*Automate and simplify operations
*Provide contextual intelligence across the [[solution stack]]
*Support business growth
*Accommodate new initiatives without expanding infrastructure
*Simplify application upgrades, consolidation and retirement
*Facilitate alignment, consistency and governance
*Define business policies and standards up front;  share, extend, and apply throughout the lifecycle

==See also==
{{colbegin|2}}
* [[Open data]]
* [[Information architecture]]
* [[Information management]]
* [[Enterprise architecture]]
* [[Information design]]
* [[Information system]]
* [[Controlled vocabulary]]
* [[Data curation]]
* [[Data retention]]
* [[Data governance]]
* [[Data quality]]
* [[Data modeling]]
* [[Data management plan]]
* [[Information lifecycle management]]
* [[Computer data storage]]
* [[Data proliferation]]
* [[Digital preservation]]
* [[Digital perpetuation]]
* [[Document management]]
* [[Enterprise content management]]
* [[Hierarchical storage management]]
* [[Information repository]]
* [[Records management]]
* [[System integration]]
{{colend}}

== References ==
{{Reflist}}

==External links==
* {{dmoz|Computers/Software/Master_Data_Management/Articles/}}

{{DEFAULTSORT:Data Management}}
[[Category:Data management| ]]
[[Category:Information technology management]]</text>
      <sha1>brrnhrscuhleamr2lud3zz3hh9w4siu</sha1>
    </revision>
  </page>
  <page>
    <title>Open Compute Project</title>
    <ns>0</ns>
    <id>31547791</id>
    <revision>
      <id>747591190</id>
      <parentid>747414885</parentid>
      <timestamp>2016-11-03T06:33:49Z</timestamp>
      <contributor>
        <username>Wikideas1</username>
        <id>20468248</id>
      </contributor>
      <minor />
      <comment>/* See also */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="16176" xml:space="preserve">{{Infobox Organization
|name           = Open Compute Project
|image          = OpenCompute logo.jpg
|mcaption       = 
|formation      = 2011
|type           = Industry trade group
|purpose        = Sharing designs of [[data center]] products
|headquarters   = 
|membership     = 
|website        = {{URL|opencompute.org}}
|remarks        =
}}
[[File:Open Compute Server Front.jpg|thumb|Open Compute V2 Server]]
[[File:Open Compute 1U Drive Tray Bent.jpg|thumb|Open Compute V2 Drive Tray,&lt;br /&gt;2nd lower tray extended]]
The '''Open Compute Project''' ('''OCP''') is an organization that shares designs of [[data center]] products among companies, including [[Facebook]], [[Intel]], [[Nokia]], [[Google]], [[Apple Inc.|Apple]], [[Microsoft]], [[Seagate Technology]], [[Dell]], [[Rackspace]], [[Ericsson]],  [[Cisco]], [[Juniper Networks]], [[Goldman Sachs]], [[Fidelity Investments|Fidelity]], [[Lenovo]] and [[Bank of America]].&lt;ref&gt;{{cite web|url=http://www.wired.com/2015/03/facebook-got-even-apple-back-open-source-hardware/|title=How Facebook Changed the Basic Tech That Runs the Internet|date=11 Apr 2015}}&lt;/ref&gt;&lt;ref&gt;{{Cite web|url=http://www.opencompute.org/about/ocp-incubation-committee/|title=Incubation Committee|website=Open Compute|access-date=2016-08-19}}&lt;/ref&gt;

The Open Compute Project's mission is to design and enable the delivery of the most efficient server, storage and data center hardware designs for scalable computing. "We believe that openly sharing ideas, specifications and other intellectual property is the key to maximizing innovation and reducing operational complexity in the scalable computing space."&lt;ref&gt;{{cite web|url=http://www.opencompute.org/about/mission-and-principles/|title=Mission and Principles|website = Open Compute|accessdate = 2016-05-13}}&lt;/ref&gt;&lt;br /&gt;
All Facebook Data Centers are 100% OCP: Prineville Data Center, Forest City Data Center, Altoona Data Center, Luleå Data Center (Sweden).
Facebook Data Centers under construction: Fort Worth Data Center, Clonee Data Center (Ireland).&lt;ref&gt;{{cite web|url=http://uk.businessinsider.com/facebook-eu-data-center-open-compute-project-2016-1|first=Matt|last=Weinberger|title=Facebook's newest data center is going to make some big tech companies very nervous|website=Open Compute|date=January 25, 2016|accessdate = 2016-05-16}}&lt;/ref&gt;

==Details==
The initiative was announced in April 2011 by Jonathan Heiliger&lt;ref&gt;{{cite news|last1 = Heiliger|first1 = Jonathan|title = Why I Started the Open Compute Project|url = http://www.vertexventures.com/2015/06/why-i-started-the-open-compute-project/|accessdate = 18 June 2015|date = 2015-06-15}}&lt;/ref&gt; at [[Facebook]] to openly share designs of [[data center]] products.&lt;ref&gt;{{cite web |url= http://www.datacenterknowledge.com/archives/2011/04/14/will-open-compute-alter-the-data-center-market/ |title=Will Open Compute Alter the Data Center Market? |date=April 14, 2011 |first= Rich|last= Miller |work= Data Center Knowledge |accessdate= July 9, 2013 }}&lt;/ref&gt;
The effort came out of a redesign of [[Facebook]]'s data center in [[Prineville, Oregon]].&lt;ref&gt;{{Cite web |url= http://www.facebook.com/notes/facebook-engineering/building-efficient-data-centers-with-the-open-compute-project/10150144039563920 |title= Building Efficient Data Centers with the Open Compute Project |first= Jonathan|last= Heiliger |date= April 7, 2011 |work= Facebook Engineering's notes |accessdate= July 9, 2013 }}&lt;/ref&gt;
After two years, with regards to a more module server design, it was admitted that "the new design is still a long way from live data centers".&lt;ref&gt;{{Cite news |title= Facebook Shatters the Computer Server Into Tiny Pieces |date= January 16, 2013 |first= Cade|last= Metz |work= Wired |url= http://www.wired.com/wiredenterprise/2013/01/facebook-server-pieces/ |accessdate= July 9, 2013 }}&lt;/ref&gt;
However, some aspects published were used in the Prineville center to improve the energy efficiency, as measured by the [[power usage effectiveness]] index defined by [[The Green Grid]].&lt;ref name="Stanford"&gt;{{Cite web |title= Facebook's Open Compute Project |work= Stanford EE Computer Systems Colloquium |date= February 15, 2012  |url= http://www.stanford.edu/class/ee380/Abstracts/120215.html |first= Amir|last= Michael |publisher= [[Stanford University]]}}  ([http://ee380.stanford.edu/cgi-bin/videologger.php?target=120215-ee380-300.asx video archive])&lt;/ref&gt;

The Open Compute Project Foundation is a 501(c)(6) non-profit incorporated in the state of Delaware. Corey Bell serves as the Foundation's CEO. Currently there are 7 members who serve on board of directors which is made up of two individual members and five organizational members.  Jason Taylor ([[Facebook]]) is the Foundation's president and chairman. Frank Frankovsky (formerly of Facebook and past president and chairman) and  [[Andy Bechtolsheim]] are the two individual members.  In addition to Jason Taylor who represents [[Facebook]], other organizations on the Open Compute board of directors include [[Intel]] (Jason Waxman), [[Goldman Sachs]] (Don Duet), [[Rackspace]] (Mark Roenick), and [[Microsoft]] (Bill Laing).&lt;ref&gt;{{Cite web|title = Organization and Board|url = http://www.opencompute.org/about/organization-and-board/|website = Open Compute|accessdate = 2015-09-12}}&lt;/ref&gt;

On March 11, 2015 [[Apple Inc.|Apple]], [[Cisco]] and [[Juniper Networks]] joined the project.&lt;ref&gt;{{Cite web |title= Open Compute: Apple, Cisco Join While HP Expands |first= Charles|last= Babcock |date= March 11, 2015 |url=http://www.informationweek.com/cloud/infrastructure-as-a-service/open-compute-apple-cisco-join-while-hp-expands/d/d-id/1319421  |accessdate= March 11, 2015 }}&lt;/ref&gt;

On November 16, 2015 [[Nokia]] joined the project.&lt;ref&gt;{{Cite web |title= Nokia Networks joins Open Compute Project to advance its AirFrame Data Center Solution|date= November 16, 2015 |url=http://company.nokia.com/en/news/press-releases/2015/11/16/nokia-networks-joins-open-compute-project-to-advance-its-airframe-data-center-solution}}&lt;/ref&gt;

On February 23, 2016 [[Lenovo]] joined the project.&lt;ref&gt;{{Cite web |title= Lenovo joins Open Compute Project |date= February 23, 2016 |url=http://news.lenovo.com/blog/lenovo-joins-open-compute-projects.htm }}&lt;/ref&gt;

On March 9, 2016 [[Google]] joined the project.&lt;ref&gt;{{Cite web |title= Google joins the Open Compute Project |date= March 9, 2016 |url=http://techcrunch.com/2016/03/09/google-joins-the-open-compute-project/ }}&lt;/ref&gt;

Components of the Open Compute Project include:

* Server compute nodes included one for [[Intel]] processors and one for [[Advanced Micro Devices|AMD]] processors. In 2013, [[Calxeda]] contributed a design with [[ARM architecture]] processors.&lt;ref&gt;{{Cite web |title= ARM Server Motherboard Design for Open Vault Chassis Hardware v0.3 MB-draco-hesperides-0.3 |first= Tom|last= Schnell |date= January 16, 2013 |url=http://www.opencompute.org/wp/wp-content/uploads/2013/01/Open_Compute_Project_ARM_Server_Specification_v0.3.pdf  |accessdate= July 9, 2013 }}&lt;/ref&gt;&lt;br /&gt;Several generations of server designs have been deployed. So far being: Freedom (Intel), Spitfire (AMD), Windmill (Intel E5-2600), Watermark (AMD), Winterfell (Intel E5-2600 v2) and Leopard (Intel E5-2600 v3)&lt;ref&gt;{{Cite web |title=Guide to Facebook’s Open Source Data Center Hardware
|author=Data Center Knowledge|date=April 28, 2016|url=http://www.datacenterknowledge.com/archives/2016/04/28/guide-to-facebooks-open-source-data-center-hardware/|accessdate=May 13, 2016}}&lt;/ref&gt;&lt;ref&gt;{{Cite web |title=Facebook rolls out new web and database server designs|first=The|last=Register|date=January 17, 2013|url=http://www.theregister.co.uk/2013/01/17/open_compute_facebook_servers/|accessdate=May 13, 2016}}&lt;/ref&gt;

* Open Vault storage building blocks offer high disk densities, with 30 drives in a 2U Open Rack chassis designed for easy [[disk drive]] replacement. The 3.5 inch disks are stored in two drawers, five across and three deep in each drawer, with connections via [[serial attached SCSI]].&lt;ref&gt;{{Cite web |title= Open Vault Storage Hardware V0.7 OR-draco-bueana-0.7 |author= Mike Yan and Jon Ehlen |date= January 16, 2013 |url= http://www.opencompute.org/wp/wp-content/uploads/2013/01/Open_Compute_Project_Open_Vault_Storage_Specification_v0.7.pdf |accessdate= July 9, 2013 }}&lt;/ref&gt; This storage is also called Knox, there is also a cold storage variant where the disks power down if not used to save energy consumption.&lt;ref&gt;{{Cite web |title=Under the hood: Facebook’s cold storage system|date=May 4, 2015|url=https://code.facebook.com/posts/1433093613662262/-under-the-hood-facebook-s-cold-storage-system-/|accessdate=May 13, 2016}}&lt;/ref&gt; Another design concept was contributed by Hyve Solutions, a division of [[Synnex]] in 2012.&lt;ref&gt;{{Cite web |title= Hyve Solutions Contributes Storage Design Concept to OCP Community |work= News release |date= January 17, 2013 |url= http://ir.synnex.com/releasedetail.cfm?ReleaseID=733922 |accessdate= July 9, 2013 }}&lt;/ref&gt;&lt;ref&gt;{{Cite web |title= Torpedo Design Concept Storage Server for Open Rack Hardware v0.3 ST-draco-chimera-0.3 |first= Conor|last= Malone |date= January 15, 2012 |url= http://www.opencompute.org/wp/wp-content/uploads/2013/01/Open_Compute_Project_Storage_Server_for_Open_Rack_Specification_v0.3.pdf  |accessdate= July 9, 2013 }}&lt;/ref&gt;&lt;br /&gt;At the OCP Summit 2016 Facebook together with Taiwanese ODM Wistron's spin-off Wiwynn introduced Lightning, a flexible NVMe JBOF (just a bunch of flash), based on the existing Open Vault (Knox) design.&lt;ref&gt;{{Cite web |title=Introducing Lightning: A flexible NVMe JBOF|first=Chris|last=Petersen|date=March 9, 2016|url=https://code.facebook.com/posts/989638804458007/introducing-lightning-a-flexible-nvme-jbof/|accessdate= May 13, 2016}}&lt;/ref&gt;&lt;ref&gt;{{Cite web |title=Wiwynn Showcases All-Flash Storage Product with Leading-edge NVMe Technology|date=March 9, 2016|url=http://www.wiwynn.com/english/company/newsinfo/23|accessdate= May 13, 2016}}&lt;/ref&gt;
* Mechanical mounting system: Open racks have the same outside width (600&amp;nbsp;mm) and depth as standard [[19-inch rack]]s, but are designed to mount wider chassis with a 537&amp;nbsp;mm width (about 21 inches). This allows more equipment to fit in the same volume and improves air flow. Compute chassis sizes are defined in multiples of an OpenU, which is 48&amp;nbsp;mm, slightly larger than the typical [[rack unit]].
* Data center designs for energy efficiency, include 277 VAC power distribution that eliminates one transformer stage in typical data centers. A single voltage (12.5 VDC) power supply designed to work with 277 VAC input and 48 VDC battery backup.&lt;ref name="Stanford" /&gt;
* On May 8, 2013, an effort to define an open [[network switch]] was announced.&lt;ref&gt;{{Cite web |title= Up next for the Open Compute Project: The Network |date= May 8, 2013 |author= Jay Hauser for Frank Frankovsky |work= Open Compute blog |url= http://www.opencompute.org/blog/up-next-for-the-open-compute-project-the-network/ |accessdate= June 20, 2014 }}&lt;/ref&gt; The plan was to allow Facebook to load its own [[operating system]] software onto the switch. Press reports predicted that more expensive and higher-performance switches would continue to be popular, while less expensive products treated more like a [[commodity]] (using the [[buzzword]] "top-of-rack") might adopt the proposal.&lt;ref&gt;{{Cite news |title= Can Open Compute change network switching? |first= David|last= Chernicoff |work= ZDNet |date= May 9, 2013 |url= http://www.zdnet.com/can-open-compute-change-network-switching-7000015141/ |accessdate= July 9, 2013 }}&lt;/ref&gt;&lt;br /&gt;A similar project for a custom switch for the [[Google platform]] had been rumored, and evolved to use the [[OpenFlow]] protocol.&lt;ref&gt;{{Cite news |title= Facebook Rattles Networking World With ‘Open Source’ Gear |date= May 8, 2013 |first= Cade|last= Metz |work= Wired |url= http://www.wired.com/wiredenterprise/2013/05/facebook_networking/ |accessdate= July 9, 2013 }}&lt;/ref&gt;&lt;ref&gt;{{Cite news |title= Going With the Flow: Google’s Secret Switch to the Next Wave of Networking |date= April 17, 2012 |first= Steven|last= Levy |work= Wired |url= http://www.wired.com/wiredenterprise/2012/04/going-with-the-flow-google/ |accessdate= July 9, 2013 }}&lt;/ref&gt;&lt;br /&gt;The first switch Open Sourced by Facebook was designed together with Taiwanese ODM Accton using Broadcom Trident II chip and is called Wedge, the Linux OS that it runs is called FBOSS.&lt;ref&gt;{{cite web|url=https://code.facebook.com/posts/681382905244727/introducing-wedge-and-fboss-the-next-steps-toward-a-disaggregated-network/|title=Introducing "Wedge" and "FBOSS," the next steps toward a disaggregated network|website =Meet the engineers who code Facebook|date=June 18, 2014|accessdate = 2016-05-13}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://code.facebook.com/posts/843620439027582/facebook-open-switching-system-fboss-and-wedge-in-the-open/|title=Facebook Open Switching System ("FBOSS") and Wedge in the open|website=Meet the engineers who code Facebook|date=March 10, 2015|accessdate = 2016-05-13}}&lt;/ref&gt; Later switch contributions include "6-pack" and Wedge-100, based on Broadcom Tomahawk chips.&lt;ref&gt;{{cite web|url=https://code.facebook.com/posts/203733993317833/opening-designs-for-6-pack-and-wedge-100/|title=Opening designs for 6-pack and Wedge 100|website=Meet the engineers who code Facebook|date=March 9, 2016|accessdate = 2016-05-13}}&lt;/ref&gt; Similar switch hardware designs have been contributed by: Edge-Core Networks Corporation (Accton spin-off), Mellanox Technologies, Interface Masters Technologies, Agema Systems.&lt;ref&gt;{{cite web|url=http://www.opencompute.org/wiki/Networking/SpecsAndDesigns|title=Accepted or shared hardware specifications|website=Open Compute|accessdate = 2016-05-13}}&lt;/ref&gt; Capable of running ONIE compatible Operating Systems such as Cumulus Linux, Big Switch or Pica8.&lt;ref&gt;{{cite web|url=http://www.opencompute.org/wiki/Networking/ONIE/NOS_Status|title=Current Network Operating System (NOS) List|website=Open Compute|accessdate = 2016-05-13}}&lt;/ref&gt;

== Providers ==
The promoted vendors include:&lt;ref&gt;[http://www.opencompute.org/about/open-compute-project-solution-providers/ open compute project solution providers]&lt;/ref&gt;
* [[AMAX Information Technologies]]
* Circle B
* [[Itochu Techno-Solutions]] (CTC)
* [[Hewlett Packard Enterprise]]
* Hyperscale IT
* [[Synnex|Hyve Solutions]]
* [[Penguin Computing]]
* [[Nokia]]
* [[Quanta Computer]]
* Racklive
* Stack Velocity
* Wiwynn

== See also ==
* [[Novena (computing platform)]]
* [[Open-source computing hardware]]
* [[OpenPOWER Foundation]]
* [[Telecom Infra Project]]  - [[Facebook]] sister project focusing on [[Optical networking|Optical]] [[broadband networks]] and open [[cellular network|cellular networks]]

== References ==
{{reflist|33em}}

== External links ==
{{Commons category|Data Centers}}
* {{Official website|http://opencompute.org/}}
* [https://www.facebook.com/PrinevilleDataCenter/ Prineville Data Center]
* [https://www.facebook.com/ForestCityDataCenter/ Forest City Data Center]
* [https://www.facebook.com/AltoonaDataCenter/ Altoona Data Center]
* [https://www.facebook.com/LuleaDataCenter/ Luleå Data Center (Sweden)]
* [https://www.facebook.com/FortWorthDataCenter/ Fort Worth Data Center]
* [https://www.facebook.com/CloneeDataCenter/ Clonee Data Center (Ireland)]
* Videos
** {{youtube|2hTfzUmdAOw|HC23-T2: The Open Compute Project}}, Hot Chips 23, 2011 2.5 Hour Tutorial
** {{youtube|QtTF9pDQxPc|Facebook Open Compute Server}}, Facebook V1 Open Compute Server
** {{youtube|ckNzwqhDS60|Facebook V2 Windmill Server}}
** {{youtube|GbzQe3jO4hc|Hyve: Adapting Facebook's Servers for Your Data Center}}, Open Compute starts at 5:40

{{Facebook navbox|state=collapsed}}

[[Category:Open-source hardware]]
[[Category:Facebook]]
[[Category:2011 software]]
[[Category:Data centers]]
[[Category:Data management]]
[[Category:Servers (computing)]]
[[Category:Distributed data storage]]
[[Category:Distributed data storage systems]]
[[Category:Applications of distributed computing]]
[[Category:Cloud storage]]
[[Category:Computer networking]]
[[Category:Science and technology in the San Francisco Bay Area]]</text>
      <sha1>etkvayg2tdstx0ujgt2sau4ujoi08gk</sha1>
    </revision>
  </page>
  <page>
    <title>MaPS S.A.</title>
    <ns>0</ns>
    <id>46489271</id>
    <revision>
      <id>743007109</id>
      <parentid>719527623</parentid>
      <timestamp>2016-10-07T05:45:42Z</timestamp>
      <contributor>
        <username>Cnwilliams</username>
        <id>10190671</id>
      </contributor>
      <minor />
      <comment>Disambiguated: [[PLC]] → [[Public limited company]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2927" xml:space="preserve">{{Orphan|date=July 2015}}

{{Infobox company
| name             = MaPS System 
| image            = [[File:MaPS-System Logo.png|250px]]
| industry         = [[Public limited company|PLC]]
| founder          = Thierry Muller
| headquarters     = [[Luxembourg]]
| area_served      = [[France]], [[Luxembourg]], [[Germany]], [[Switzerland]], [[Belgium]]
| products         = [[Product Information Management]], [[Digital Asset Management]], [[Master Data Management]] and [[Business Process Management]]
| homepage         = http://www.maps-system.com/
}}

'''MaPS S.A.''' is a software editor founded in 2011 by Thierry Muller which is headquartered in [[Luxembourg]]. Its platform, called MaPS System, provides [[Data management|Data Management]] solutions for [[Multichannel Marketing]].

==History and Funding==

In 1999, the founder, had realized that certain challenges arose with several tools for [[Customer relationship management|Customer Relationship Management]], [[Public relations|Public Relations]] and in particular [[Marketing]] tools set in place. Complex data structures developed difficulties for organizations who lost focus of their dispersed data when wanting to operate and sell in an international and [[Multichannel Marketing|Multichannel]] environment.

The founder drafted his initial ideas on the topic of [[Multichannel Marketing]] and developed his first version of MaPS System under the agency Prem1um S.A. in 2005, which in combination with the [[Data Management]] solution also provided various Multimedia &amp; Marketing activities.

In 2011, after being successful, Prem1um S.A. decided to enable the software MaPS System to operate independently under MaPS S.A., as a separate company and editor of the software. The first financial supports were provided by Malta ICI, a Venture Capital firm, and the local partner Chameleon Invest, a seed-capital fund led by Business Angels, who invested €900.000. In a second investment round in 2014 led by Newion Investments, a Venture Capital firm, €1.4 Million were raised, thus amounting to total assets of €2.2 Million.

==Products==
The services included in MaPS System range from the data centralization, [[Data Governance]] to an optimized [[Multichannel Marketing]]. The software today features more than 35 modules for [[Master Data Management]], [[Product Information Management]], [[Digital Asset Management]], [[Business Process Management]] including catalogue [[Publishing]] features.

==References==
* {{Official website | http://www.maps-system.com/ MaPS S.A.}}
* Newion invests in MaPS System | http://www.newion-investments.com/news/newion-invests-in-maps-system/1]
* Introducing MaPS System | http://www.siliconluxembourg.lu/introducing-maps-system-a-centralized-information-management-solution/]

[[Category:Data management software]]
[[Category:Data management]]
[[Category:Software companies]]
[[Category:Companies of Luxembourg]]</text>
      <sha1>ixv68bp67cxl0siqe0ja3evuxkxtw1b</sha1>
    </revision>
  </page>
  <page>
    <title>Comparison of CDMI server implementations</title>
    <ns>0</ns>
    <id>40002906</id>
    <revision>
      <id>669096545</id>
      <parentid>667318977</parentid>
      <timestamp>2015-06-28T21:16:34Z</timestamp>
      <contributor>
        <username>Communal t</username>
        <id>25091450</id>
      </contributor>
      <minor />
      <comment>Added [[:Template:DEFAULTSORT]] with sortkey ''CDMI server implementation comparison''.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="25815" xml:space="preserve">
{| class="wikitable"
|-
! Implementation !! SNIA Reference Implementation !! CDMI-Serve !! CDMI-Proxy !! CDMI for OpenStack's Swift !! CDMI-z !! onedata
|-
| Version || [http://www.snia.org/forums/csi/programs/CDMIportal 1.0e] || [https://github.com/koenbollen/cdmi-serve 238c28fc7c] || [https://github.com/livenson/vcdm 0.1] || [https://github.com/osaddon/cdmi f0e3ad9bac] || 1 || [http://packages.onedata.org/oneprovider-Linux.rpm 2.0]
|-
| [[CDMI]] Version || 1.0.2 || ? || 1.0.1 || ? || 1.0.2 || 1.0.2
|-
| colspan="7" align="center" | '''HTTP features'''
|-
| [[HTTPS]] || ? || ? || {{Yes}} || ? || ? || {{Yes}}
|-
| [[Basic authentication]] || ? || ? || {{Yes}} || ? || ? || ?
|-
| [[Digest authentication]] || ? || ? || {{Yes}} || ? || ? || ?
|-
| [[X.509|X.509 authentication]] || ? || ? || ? || ? || ? || {{Yes}}
|-
| X.509-VOMS authentication || ? || ? || ? || ? || ? || {{Yes}}
|-
| Token based authentication || ? || ? || ? || ? || ? || {{Yes}}
|-
| colspan="7" align="center" | '''Data access methods'''
|-
| [[Filesystem in Userspace|FUSE]] || ? || ? || ? || ? || ? || {{Yes}}
|-
| [[GridFTP]] || ? || ? || ? || ? || ? || {{No}}
|-
| [[iSCSI]] || {{Yes}} || ? || ? || ? || ? || {{No}}
|-
| [[WebDAV]] || ? || ? || ? || ? || ? || {{No}}
|-
| [[Network File System|NFS]] || ? || ? || ? || ? || ? || {{No}}
|-
| [[Browser user interface|BUI]] || ? || ? || ? || ? || ? || {{Yes}}
|-
| colspan="7" align="center" | '''System-Wide CDMI Capabilities'''
|-
| cdmi_domains || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false}}
|-
| cdmi_export_cifs || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_dataobjects || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}}
|-
| cdmi_export_iscsi || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_export_nfs || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_export_occi_iscsi || {{Yes|"true"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_export_webdav || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_metadata_maxitems || 1024 || ? || ? || ? || 4096 || 1024
|-
| cdmi_metadata_maxsize || 4096 || ? || ? || ? || 4096 || 4096
|-
| cdmi_metadata_maxtotalsize || ∞ || ? || ? || ? || 1048576 || 1048576
|-
| cdmi_notification || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_logging || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_query || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_query_regex || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_query_contains || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_query_tags || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_query_value || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_queues || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}}
|-
| cdmi_security_access_control || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_security_audit || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_security_data_integrity || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_security_encryption || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_security_immutability || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_security_sanitization || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_serialization_json || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_snapshots || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_references || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_object_move_from_local || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_object_move_from_remote || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_object_move_from_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_object_move_to_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_object_copy_from_local || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_object_copy_from_remote || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_object_access_by_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_post_dataobject_by_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}}
|-
| cdmi_post_queue_by_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}}
|-
| cdmi_deserialize_dataobject_by_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_deserialize_queue_by_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_serialize_dataobject_to_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_serialize_domain_to_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_serialize_container_to_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_serialize_queue_to_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_copy_dataobject_by_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_copy_queue_by_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_create_reference_by_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}}
|-
| colspan="7" align="center" | '''Data Object Capabilities'''
|-
| cdmi_read_value || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_read_value_range || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_read_metadata || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_modify_value || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}|| {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_modify_value_range || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_modify_metadata || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_modify_deserialize_dataobject || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_delete_dataobject || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}}
|-
| cdmi_acl || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_size || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_ctime || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_atime || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_mtime || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_acount || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_mcount || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_assignedsize || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_redundancy || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|"false"}}
|-
| cdmi_data_dispersion || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_retention || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_autodelete || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_holds || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_encryption || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|"false"}}
|-
| cdmi_geographic_placement || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_immediate_redundancy || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|"false"}}
|-
| cdmi_infrastructure_redundancy || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|"false"}}
|-
| cdmi_latency || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_RPO || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_RTO || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_sanitization_method || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|"false"}}
|-
| cdmi_throughput || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_value_hash || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|"false"}}
|-
| colspan="7" align="center" | '''Container Capabilities'''
|-
| cdmi_list_children || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}}
|-
| cdmi_list_children_range || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_read_metadata || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_modify_metadata || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_modify_deserialize_container || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_snapshot || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_serialize_dataobject || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_serialize_container || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_serialize_queue || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_serialize_domain || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_deserialize_container || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_deserialize_queue || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_deserialize_dataobject || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_create_dataobject || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}}
|-
| cdmi_post_dataobject || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}}
|-
| cdmi_post_queue || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}}
|-
| cdmi_create_container || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}}
|-
| cdmi_create_queue || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}}
|-
| cdmi_create_reference || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_export_container_cifs || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_export_container_nfs || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_export_container_iscsi || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_export_container_occi || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_export_container_webdav || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_delete_container || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}}
|-
| cdmi_move_container || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_copy_container || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_move_dataobject || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_copy_dataobject" || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_acl || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_size || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_ctime || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_atime || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_mtime || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_acount || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_mcount || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_assignedsize || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_redundancy || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|"false"}}
|-
| cdmi_data_dispersion || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_retention || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_autodelete || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_holds || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_encryption || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|"false"}}
|-
| cdmi_geographic_placement || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_immediate_redundancy || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|"false"}}
|-
| cdmi_infrastructure_redundancy || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|"false"}}
|-
| cdmi_latency || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_RPO || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_RTO || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_sanitization_method || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|"false"}}
|-
| cdmi_throughput || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_value_hash || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|"false"}}
|-
| colspan="7" align="center" | '''Domain Object Capabilities'''
|-
| cdmi_create_domain || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{Yes|"true"}} || {{No|"false"}}
|-
| cdmi_delete_domain || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{Yes|"true"}} || {{No|"false"}}
|-
| cdmi_domain_summary || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_domain_members || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_list_children || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{Yes|"true"}} || {{No|"false"}}
|-
| cdmi_read_metadata || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_modify_metadata || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_modify_deserialize_domain || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_copy_domain || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_deserialize_domain || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_acl || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_size || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_ctime || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_atime || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_mtime || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_acount || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_mcount || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_assignedsize || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_redundancy || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|""}} || {{No|"false"}}
|-
| cdmi_data_dispersion || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_retention || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_autodelete || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_holds || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_encryption || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|[]}} || {{No|"false"}}
|-
| cdmi_geographic_placement || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_immediate_redundancy || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|""}} || {{No|"false"}}
|-
| cdmi_infrastructure_redundancy || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|""}} || {{No|"false"}}
|-
| cdmi_latency || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_RPO || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_RTO || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_sanitization_method || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|[]}} || {{No|"false"}}
|-
| cdmi_throughput || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_value_hash || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|[]}} || {{No|"false"}}
|-
| colspan="7" align="center" | '''Queue Object Capabilities'''
|-
| cdmi_read_value || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_read_metadata || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_modify_value || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_modify_metadata || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_modify_deserialize_queue || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_delete_queue || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{Yes|"true"}} || {{No|"false"}}
|-
| cdmi_move_queue || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_copy_queue || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_reference_queue || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_acl || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_size || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_ctime || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_atime || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_mtime || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_acount || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_mcount || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_assignedsize || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_redundancy || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|""}} || {{No|"false"}}
|-
| cdmi_data_dispersion || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_retention || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_autodelete || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_holds || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_encryption || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|[]}} || {{No|"false"}}
|-
| cdmi_geographic_placement || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_immediate_redundancy || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|""}} || {{No|"false"}}
|-
| cdmi_infrastructure_redundancy || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|""}} || {{No|"false"}}
|-
| cdmi_latency || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_RPO || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_RTO || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_sanitization_method || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|[]}} || {{No|"false"}}
|-
| cdmi_throughput || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_value_hash || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|[]}} || {{No|"false"}}
|}

{{DEFAULTSORT:CDMI server implementation comparison}}
[[Category:Cloud storage]]
[[Category:Data management]]</text>
      <sha1>1rafup6bbybjnu6wnp9dxf08a046o7a</sha1>
    </revision>
  </page>
  <page>
    <title>Database administrator</title>
    <ns>0</ns>
    <id>254789</id>
    <revision>
      <id>746401775</id>
      <parentid>742615669</parentid>
      <timestamp>2016-10-27T05:23:33Z</timestamp>
      <contributor>
        <username>Bumm13</username>
        <id>63286</id>
      </contributor>
      <minor />
      <comment>changed "System Monitoring" (monitoring) wikilink to "System monitoring"</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6920" xml:space="preserve">{{Infobox Occupation
|caption= Database Administrator
|official_names= Database administrator, database analyst
|activity_sector=[[Information technology]], [[information system]]s
|competencies= [[Database design|Databases design and implementation]], [[Computer programming|programming]] skills, [[database theory]], [[Computer network|networking]] basics, [[analytical skill]]s, [[critical thinking]]
|formation=At least a [[Academic certificate|certificate]] with experience.
}}

'''Database administrators''' ('''DBAs''') use specialized software to store and organize data.&lt;ref name="BLS-DBA"&gt;{{cite web | url=http://www.bls.gov/ooh/computer-and-information-technology/database-administrators.htm | title=Database Administrators | publisher=Bureau Of Labor Statistics | work=11/04/2015 | accessdate=4 November 2015}}&lt;/ref&gt;

The role may include [[capacity planning]], [[Installation (computer programs)|installation]], [[Computer configuration|configuration]], [[database design]], [[Data migration|migration]], performance monitoring, [[Computer security|security]], [[troubleshooting]], as well as [[backup]] and [[data recovery]].&lt;ref name="techrepublic"&gt;{{cite web | url=http://www.techrepublic.com/blog/the-enterprise-cloud/what-does-a-dba-do-all-day/ | title=What does a DBA do all day? | publisher=techrepublic.com | work=11/04/2015 | accessdate=4 November 2015}}&lt;/ref&gt;

==Skills==
List of skills required to become database administrators are:&lt;ref&gt;{{cite web|last1=Spenik|first1=Mark|last2=Sledge|first2=Orryn|date=2001-03-20|url=http://www.developer.com/db/article.php/718491/What-Is-a-Database-Administrator.htm|title=What is a Database Administrator? (DBA)|publisher=Developer.com|accessdate=2012-02-06|archiveurl=https://web.archive.org/web/20110613101702/http://www.developer.com/db/article.php/718491/What-Is-a-Database-Administrator.htm|archivedate=2011-06-13}}&lt;/ref&gt;&lt;ref&gt;http://www.dba-oracle.com/oracle_tips_dba_job_skills.htm&lt;/ref&gt;&lt;ref&gt;http://www.orafaq.com/wiki/Roles_and_Responsibilities&lt;/ref&gt;
* [[Communication]] skills
* Knowledge of [[database theory]]
* Knowledge of [[database design]]
* Knowledge about the [[Relational database management system|RDBMS]] itself, e.g. [[Microsoft SQL Server]] or [[MySQL]]
* Knowledge of [[SQL|structured query language]] (SQL), e.g. [[SQL/PSM]] or [[Transact-SQL]]
* General [[understanding]] of [[Distributed computing|distributed computing architectures]], e.g. [[Client–server model]]
* General understanding of [[operating system]], e.g. [[Microsoft Windows|Windows]] or [[Linux]]
* General understanding of [[Computer data storage|storage technologies]] and [[Computer network|networking]]
* General understanding of routine maintenance, recovery, and handling failover of a database

Database administrators benefit from a [[bachelor's degree]] or [[master's degree]] in [[computer science]]. An [[associate degree]] or a [[Academic certificate|certificate]] may be sufficient with work experience.&lt;ref name="study.com"&gt;{{cite web | url=http://study.com/articles/Database_Administrator_Job_Description_and_Requirements.html | title=Database Administrator: Job Description and Requirements | publisher=study.com | work=11/4/2015 | accessdate=4 November 2015}}&lt;/ref&gt;

===Certification===
There are many certifications available for becoming a certified database administrator. Many of these certifications are offered by database vendors themselves. By passing a series of tests and sometimes other requirements, you can earn a database administrator certification. Schools offering Database Administration degrees can also be found.&lt;ref name="learn.org"&gt;{{cite web | url=http://learn.org/articles/How_Do_I_Become_a_Certified_Database_Administrator.html | title=How Do I Become a Certified Database Administrator? | publisher=learn.org | work=learn.org | accessdate=4 November 2015}}&lt;/ref&gt;

For example:
* IBM Certified Advanced Database Administrator - DB2 10.1 for Linux, Unix and Windows&lt;ref name="ibm.com"&gt;{{cite web |url=http://www-03.ibm.com/certify/index.shtml |title=IBM Professional Certification Program |work=ibm.com |publisher=[[IBM]] |accessdate=2014-08-10}}&lt;/ref&gt;
* IBM Certified Database Administrator - DB2 10.1 for Linux, Unix, and Windows&lt;ref name="ibm.com"/&gt;
* Oracle Database 11g Administrator Certified Professional&lt;ref&gt;{{cite web |url=http://education.oracle.com/pls/web_prod-plq-dad/db_pages.getpage?page_id=143&amp;p_org_id=1001&amp;lang=US |title=Oracle Certification Program |work=oracle.com |publisher=[[Oracle Corporation]] |accessdate=2011-06-18}}&lt;/ref&gt;
* Oracle MySQL 5.6 Database Administrator Certified Professional&lt;ref&gt;{{cite web |url=https://education.oracle.com/pls/web_prod-plq-dad/ou_product_category.getPage?p_cat_id=159&amp;p_org_id=15941&amp;lang=US#tabs-3 |title=Oracle Certified Professional, MySQL 5.6 Database Administrator |work=oracle.com |publisher=[[Oracle Corporation]] |accessdate=2016-09-18}}&lt;/ref&gt;
* MCSA SQL Server 2012&lt;ref name=MCSASQL&gt;{{cite web |url=https://www.microsoft.com/en-us/learning/mcsa-sql-certification.aspx |title=MCSA: SQL Server |work=microsoft.com |publisher=[[Microsoft]] |accessdate=2015-11-03}}&lt;/ref&gt;
* MCSE Data Platform Solutions Expert &lt;ref name="microsoftsolutionsexpert"&gt;{{cite web | url=https://www.microsoft.com/en-us/learning/mcse-sql-data-platform.aspx | title=MCSE: Data Platform | publisher=microsoft.com | work=11/4/2015 | accessdate=4 November 2015}}&lt;/ref&gt;

==Duties==
A database administrator's responsibilities can include the following tasks:&lt;ref&gt;{{cite web |url=http://docs.oracle.com/cd/B10501_01/server.920/a96521/dba.htm#852 |title=Oracle DBA Responsibilities |work=[[Oracle Corporation]] |accessdate=2012-02-06}}&lt;/ref&gt;
* [[Installation (computer programs)|Installing]] and [[upgrade|upgrading]] the database server and application tools
* Allocating system storage and [[planning]] future storage requirements for the database system
* Modifying the database structure, as necessary, from information given by application developers
* Enrolling users and maintaining system [[Computer security|security]]
* Ensuring compliance with database vendor [[license|license agreement]]
* Controlling and [[System Monitoring|monitoring]] [[user (computing)|user]] access to the database
* Monitoring and [[Program optimization|optimizing]] the performance of the database
* Planning for [[backup]] and recovery of database information
* Maintaining [[archive]]d data
* Backing up and restoring databases
* Contacting database [[vendor]] for [[technical support]]
* Generating various reports by querying from database as per need

==See also==
* [[Comparison of database tools]]

==References==
{{Reflist}}

==External links==
"Database Administrators"

{{Database}}

{{Use British English|date=June 2012}}
{{Use dmy dates|date=June 2012}}

{{DEFAULTSORT:Database Administrator}}
[[Category:Computer occupations]]
[[Category:Data management]]
[[Category:Database specialists| ]]</text>
      <sha1>l7crakjrfyajfda7e3b2jt2yij4y5ay</sha1>
    </revision>
  </page>
  <page>
    <title>Embedded analytics</title>
    <ns>0</ns>
    <id>47485347</id>
    <revision>
      <id>758183392</id>
      <parentid>752487254</parentid>
      <timestamp>2017-01-03T23:27:26Z</timestamp>
      <contributor>
        <ip>66.201.57.34</ip>
      </contributor>
      <comment>/* Tools */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3545" xml:space="preserve">'''Embedded analytics''' is the technology designed to make [[data analysis]] and [[business intelligence]] more accessible by all kinds of application or user.

==Definition==

According to Gartner analysts Kurt Schlegel, traditional [[business intelligence]] were suffering in 2008 a lack of integration between the data and the business users.&lt;ref&gt;{{cite web
| last = Kelly
| first = Jeff
| title = Gartner Business Intelligence Summit: Embed BI within business processes
| publisher = TechTarget
| accessdate = August 2015
| url = http://searchbusinessanalytics.techtarget.com/news/1507180/Gartner-Business-Intelligence-Summit-Embed-BI-within-business-processes
}}&lt;/ref&gt; This technology intention is to be more pervasive by real-time autonomy and self-service of data visualization or customization, meanwhile decision makers, business users or even customers are doing their own daily workflow and tasks.

==History==

First mentions of the concept were made by Howard Dresner, consultant, author, former Gartner analyst and inventor of the term "business intelligence".&lt;ref&gt;{{cite web
| last = Kelly
| first = Jeff
| title = Gartner Business Intelligence Summit: Embed BI within business processes
| publisher = TechTarget
| accessdate= August 2015
| url = http://searchbusinessanalytics.techtarget.com/news/1507180/Gartner-Business-Intelligence-Summit-Embed-BI-within-business-processes
}}&lt;/ref&gt; Consolidation of [[business intelligence]] "doesn't mean the BI market has reached maturity" &lt;ref&gt;{{cite web
| last = Dresner
| first = Howard 
| title = Howard Dresner predicts the future of business intelligence
| publisher = TechTarget
| accessdate= August 2015
| url = http://searchbusinessanalytics.techtarget.com/podcast/Howard-Dresner-predicts-the-future-of-business-intelligence
}}&lt;/ref&gt; said Howard Dresner while he was working for Hyperion Solutions, a company that Oracle bought in 2007. Oracle started then to use the term "embedded analytics" at their press release for Oracle® Rapid Planning on 2009.&lt;ref&gt;{{cite web
| title = Oracle Announces Oracle® Rapid Planning
| publisher = Oracle
| accessdate= August 2015
| url = http://www.oracle.com/us/corporate/press/040402
}}&lt;/ref&gt; Gartner Group, a company for which Howard Dresner has been working,  finally added the term to their IT Glossary on November 5, 2012. 
&lt;ref&gt;{{cite web
| title = Gartner IT Glossary: Embedded Analytics 
| publisher = Gartner
| accessdate= August 2015
| url = http://www.gartner.com/it-glossary/embedded-analytics 
}}&lt;/ref&gt;
. It was clear this was a mainstream technology when Dresner Advisory Services published the 2014 Embedded Business Intelligence Market Study as part of the Wisdom of Crowds® Series of Research, including 24 vendors.&lt;ref&gt;{{cite web
| title = 2014 Embedded Business Intelligence Market Study Now Available From Dresner Advisory Services 
| publisher = Market Wired
| accessdate= August 2015
| url = http://www.marketwired.com/press-release/2014-embedded-business-intelligence-market-study-now-available-from-dresner-advisory-1962227.htm
}}&lt;/ref&gt;

==Tools==

{{colbegin|2}}

* [[Actuate Corporation|Actuate]]
* [[Dundas Data Visualization]]
* [[GoodData]]
* [[IBM]]
* [[icCube]]
* [[Logi Analytics]]
* [[Pentaho]]
* [[Qlik]]
* [[SAP_SE|SAP]]
* [[SAS_(software)|SAS]]
* [[ServiceNow]]
* [[Tableau Software|Tableau]]
* [[ThoughtSpot]]
* [[TIBCO]]
* [[Sisense]]

{{colend}}

==References==
{{Reflist}}

[[Category:Types of analytics]]
[[Category:Big data|analytics]]
[[Category:Business intelligence]]
[[Category:Data management]]</text>
      <sha1>ihkm9tmn6te8ocqkg0a72kg2pvwz1v1</sha1>
    </revision>
  </page>
  <page>
    <title>Big data</title>
    <ns>0</ns>
    <id>27051151</id>
    <revision>
      <id>762178998</id>
      <parentid>762178871</parentid>
      <timestamp>2017-01-27T04:16:33Z</timestamp>
      <contributor>
        <username>SeeChange</username>
        <id>30098935</id>
      </contributor>
      <minor />
      <comment>Added ref</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="95164" xml:space="preserve">{{About|large collections of data|the band|Big Data (band)}}
[[File:Hilbert InfoGrowth.png|thumb|right|400px|Growth of and digitization of global information-storage capacity&lt;ref&gt;{{cite web|url= http://www.martinhilbert.net/WorldInfoCapacity.html|title= The World’s Technological Capacity to Store, Communicate, and Compute Information|work= MartinHilbert.net|accessdate= 13 April 2016}}&lt;/ref&gt;]]

'''''Big data''''' is a term for [[data set]]s that are so large or complex that traditional [[data processing]] applications are inadequate to deal with them.  Challenges include [[Data analysis|analysis]], capture, [[data curation]], search, [[Data sharing|sharing]], [[Computer data storage|storage]], [[Data transmission|transfer]], [[Data visualization|visualization]], [[Query language|querying]], updating and [[information privacy]]. The term "big data" often refers simply to the use of [[predictive analytics]], [[user behavior analytics]], or certain other advanced data analytics methods that extract value from data, and seldom to a particular size of data set.&lt;ref&gt;{{Cite book|url= http://link.springer.com/10.1007/978-3-319-21569-3 |title= New Horizons for a Data-Driven Economy – Springer|doi= 10.1007/978-3-319-21569-3}}&lt;/ref&gt; "There is little doubt that the quantities of data now available are indeed large, but that’s not the most relevant characteristic of this new data ecosystem."&lt;ref&gt;{{cite journal |last1=boyd |first1=dana |last2=Crawford |first2=Kate |title=Six Provocations for Big Data |journal=Social Science Research Network: A Decade in Internet Time: Symposium on the Dynamics of the Internet and Society |date=September 21, 2011 |doi=10.2139/ssrn.1926431}}&lt;/ref&gt;

Analysis of data sets can find new correlations to "spot business trends, prevent diseases, combat crime and so on".{{r|Economist}} Scientists, business executives, practitioners of medicine, advertising and [[Government database|governments]] alike regularly meet difficulties with large data-sets in areas including [[Web search engine|Internet search]], finance, [[urban informatics]], and [[business informatics]].  Scientists encounter limitations in [[e-Science]] work, including [[meteorology]], [[genomics]],&lt;ref&gt;{{cite journal |title= Community cleverness required |journal= Nature |volume= 455 |issue= 7209 |page= 1 |date= 4 September 2008 |doi= 10.1038/455001a |url= http://www.nature.com/nature/journal/v455/n7209/full/455001a.html}}&lt;/ref&gt; [[connectomics]], complex physics simulations, biology and environmental research.&lt;ref&gt;{{cite journal |last1= Reichman |first1= O.J. |last2= Jones |first2= M.B. |last3= Schildhauer |first3= M.P. |title= Challenges and Opportunities of Open Data in Ecology |journal= Science |volume= 331 |issue= 6018 |pages= 703–5 |year= 2011 |doi= 10.1126/science.1197962 |pmid= 21311007 }}&lt;/ref&gt;

Data sets grow rapidly - in part because they are increasingly gathered by cheap and numerous information-sensing [[mobile device]]s, aerial ([[remote sensing]]), software logs, [[Digital camera|cameras]], microphones, [[radio-frequency identification]] (RFID) readers and [[wireless sensor networks]].&lt;ref&gt;{{cite web |author= Hellerstein, Joe |title= Parallel Programming in the Age of Big Data |date= 9 November 2008 |work= Gigaom Blog |url= http://gigaom.com/2008/11/09/mapreduce-leads-the-way-for-parallel-programming/}}&lt;/ref&gt;&lt;ref&gt;{{cite book |first1= Toby |last1= Segaran |first2= Jeff |last2= Hammerbacher |title= Beautiful Data: The Stories Behind Elegant Data Solutions |url= https://books.google.com/books?id=zxNglqU1FKgC |year= 2009 |publisher= O'Reilly Media |isbn= 978-0-596-15711-1 |page= 257}}&lt;/ref&gt; The world's technological per-capita capacity to store information has roughly doubled every 40 months since the 1980s;&lt;ref name="martinhilbert.net"&gt;{{cite journal | last1 = Hilbert | first1 = Martin | first2 = Priscila |last2=López | title = The World's Technological Capacity to Store, Communicate, and Compute Information | journal = Science | volume = 332 | issue = 6025 | pages = 60–65 | year = 2011 | doi = 10.1126/science.1200970 | pmid = 21310967 | url= http://martinhilbert.net/WorldInfoCapacity.html | ref= harv}}&lt;/ref&gt; {{As of|2012|lc=on}}, every day 2.5 [[exabyte]]s (2.5×10&lt;sup&gt;18&lt;/sup&gt;) of data are generated.&lt;ref&gt;{{cite web|url= http://www.ibm.com/big-data/us/en/ |title= IBM What is big data? – Bringing big data to the enterprise |publisher= www.ibm.com |accessdate= 2013-08-26}}&lt;/ref&gt; One question for large enterprises is determining who should own big-data initiatives that affect the entire organization.&lt;ref&gt;Oracle and FSN, [http://www.fsn.co.uk/channel_bi_bpm_cpm/mastering_big_data_cfo_strategies_to_transform_insight_into_opportunity#.UO2Ac-TTuys "Mastering Big Data: CFO Strategies to Transform Insight into Opportunity"], December 2012&lt;/ref&gt;

[[Relational database management system]]s and desktop statistics- and visualization-packages often have difficulty handling big data. The work may require "massively parallel software running on tens, hundreds, or even thousands of servers".&lt;ref&gt;{{cite web |author= Jacobs, A. |title= The Pathologies of Big Data |date= 6 July 2009 |work= ACMQueue |url= http://queue.acm.org/detail.cfm?id=1563874}}&lt;/ref&gt; What counts as "big data" varies depending on the capabilities of the users and their tools, and expanding capabilities make big data a moving target. "For some organizations, facing hundreds of gigabytes of data for the first time may trigger a need to reconsider data management options. For others, it may take tens or hundreds of terabytes before data size becomes a significant consideration."&lt;ref&gt;
{{cite journal 
|last1= Magoulas |first1= Roger 
|last2= Lorica |first2= Ben 
|title= Introduction to Big Data 
|journal= Release 2.0 
|issue= 11 |date= February 2009 
|url= http://radar.oreilly.com/r2/release2-0-11.html 
|publisher= O'Reilly Media 
|location= Sebastopol CA
}}
&lt;/ref&gt;

== Definition ==
[[File:Viegas-UserActivityonWikipedia.gif|thumb|Visualization of daily Wikipedia edits created by IBM. At multiple [[terabyte]]s in size, the text and images of Wikipedia are an example of big data.]]
The term has been in use since the 1990s, with some giving credit to [[John Mashey]] for coining or at least making it popular.&lt;ref&gt;{{Cite web |title=  Big Data ... and the Next Wave of InfraStress |author= John R. Mashey |date= 25 April 1998 |publisher= Usenix |work= Slides from invited talk |url= http://static.usenix.org/event/usenix99/invited_talks/mashey.pdf |accessdate= 28 September 2016 }}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=The Origins of ‘Big Data’: An Etymological Detective Story |author=Steve Lohr |date= 1 February 2013 |url=http://bits.blogs.nytimes.com/2013/02/01/the-origins-of-big-data-an-etymological-detective-story/ |publisher= [[New York Times]] |accessdate= 28 September 2016 }}&lt;/ref&gt;
Big data usually includes data sets with sizes beyond the ability of commonly used software tools to [[data acquisition|capture]], [[data curation|curate]], manage, and process data within a tolerable elapsed time.&lt;ref name="Editorial"&gt;{{cite journal | last1 = Snijders | first1 = C. | last2 = Matzat | first2 = U. | last3 = Reips | first3 = U.-D. | year = 2012 | title = 'Big Data': Big gaps of knowledge in the field of Internet | url = http://www.ijis.net/ijis7_1/ijis7_1_editorial.html | journal = International Journal of Internet Science | volume = 7 | issue = | pages = 1–5 }}&lt;/ref&gt; Big data "size" is a constantly moving target, {{As of|2012|lc=on}} ranging from a few dozen terabytes to many [[petabyte]]s of data.
Big data requires a set of techniques and technologies with new forms of integration to reveal insights from datasets that are diverse, complex, and of a massive scale.&lt;ref&gt;{{cite journal | last1 = Ibrahim | first1 =  | last2 = Targio Hashem | first2 = Abaker | last3 = Yaqoob | first3 = Ibrar | last4 = Badrul Anuar | first4 = Nor | last5 = Mokhtar | first5 = Salimah | last6 = Gani | first6 = Abdullah | last7 = Ullah Khan | first7 = Samee | year = 2015 | title = big data" on cloud computing: Review and open research issues | url = | journal = Information Systems | volume = 47 | issue = | pages = 98–115 | doi = 10.1016/j.is.2014.07.006 }}&lt;/ref&gt;

In a 2001 research report&lt;ref&gt;{{cite web |first=Douglas |last=Laney |title=3D Data Management: Controlling Data Volume, Velocity and Variety |url=http://blogs.gartner.com/doug-laney/files/2012/01/ad949-3D-Data-Management-Controlling-Data-Volume-Velocity-and-Variety.pdf |publisher=Gartner |accessdate = 6 February 2001}}&lt;/ref&gt; and related lectures, [[META Group]] (now [[Gartner]]) analyst [[Doug Laney]] defined data growth challenges and opportunities as being three-dimensional, i.e. increasing [[volume]] (amount of data), [[velocity]] (speed of data in and out), and {{linktext|variety}} (range of data types and sources). Gartner, and now much of the industry, continue to use this "3Vs" model for describing big data.&lt;ref&gt;{{cite web |last=Beyer |first=Mark |title=Gartner Says Solving 'Big Data' Challenge Involves More Than Just Managing Volumes of Data |url=http://www.gartner.com/it/page.jsp?id=1731916 |publisher=Gartner |accessdate = 13 July 2011| archiveurl= https://web.archive.org/web/20110710043533/http://www.gartner.com/it/page.jsp?id=1731916| archivedate= 10 July 2011 | deadurl= no}}&lt;/ref&gt; In 2012, [[Gartner]] updated its definition as follows: "Big data is high volume, high velocity, and/or high variety information assets that require new forms of processing to enable enhanced decision making, insight discovery and process optimization." Gartner's definition of the 3Vs is still widely used, and in agreement with a consensual definition that states that "Big Data represents the Information assets characterized by such a High Volume, Velocity and Variety to require specific Technology and Analytical Methods for its transformation into Value".&lt;ref name="Big Data Definition"&gt;{{cite journal | last1 = De Mauro | first1 = Andrea | last2 = Greco | first2 = Marco | last3 = Grimaldi | first3 = Michele | year = 2016 | title = A Formal definition of Big Data based on its essential Features | url = http://www.emeraldinsight.com/doi/abs/10.1108/LR-06-2015-0061 | journal = Library Review | volume = 65| issue = | pages = 122–135 | doi=10.1108/LR-06-2015-0061}}&lt;/ref&gt; Additionally, a new V "Veracity" is added by some organizations to describe it,&lt;ref&gt;{{cite web|title=What is Big Data?|url=http://www.villanovau.com/university-online-programs/what-is-big-data/|publisher=[[Villanova University]]}}&lt;/ref&gt; revisionism challenged by some industry authorities.&lt;ref&gt;{{cite web|last=Grimes|first=Seth|title=Big Data: Avoid 'Wanna V' Confusion|url=http://www.informationweek.com/big-data/big-data-analytics/big-data-avoid-wanna-v-confusion/d/d-id/1111077?|publisher=[[InformationWeek]]|accessdate = 5 January 2016}}&lt;/ref&gt; The 3Vs have been expanded to other complementary characteristics of big data:&lt;ref name="BD4D"&gt;{{cite web |last=Hilbert |first=Martin |title=Big Data for Development: A Review of Promises and Challenges. Development Policy Review. |url=http://www.martinhilbert.net/big-data-for-development |work=martinhilbert.net |accessdate=2015-10-07}}&lt;/ref&gt;&lt;ref name="WhatIsBigData" /&gt;
* Volume: big data doesn't sample; it just observes and tracks what happens
* Velocity: big data is often available in real-time
* Variety: big data draws from text, images, audio, video; plus it completes missing pieces through [[data fusion]]
* [[Machine Learning]]: big data often doesn't ask why and simply detects patterns&lt;ref&gt;Mayer-Schönberger, V., &amp; Cukier, K. (2013). Big data: a revolution that will transform how we live, work and think. London: John Murray.&lt;/ref&gt;
* [[Digital footprint]]: big data is often a cost-free byproduct of digital interaction&lt;ref name="WhatIsBigData"&gt;{{cite av media|url=https://www.youtube.com/watch?v=XRVIh1h47sA&amp;index=51&amp;list=PLtjBSCvWCU3rNm46D3R85efM0hrzjuAIg|title=DT&amp;SC 7-3: What is Big Data?|date=12 August 2015|publisher=|via=YouTube}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://canvas.instructure.com/courses/949415|title=Digital Technology &amp; Social Change|publisher=}}&lt;/ref&gt;

The growing maturity of the concept more starkly delineates the difference between big data and [[Business Intelligence]]:&lt;ref&gt;http://www.bigdataparis.com/presentation/mercredi/PDelort.pdf?PHPSESSID=tv7k70pcr3egpi2r6fi3qbjtj6#page=4&lt;/ref&gt;
* Business Intelligence uses [[descriptive statistics]] with data with high information density to measure things, detect trends, etc..
* Big data uses [[inductive statistics]] and concepts from [[nonlinear system identification]]&lt;ref name="SAB1"&gt;Billings S.A. "Nonlinear System Identification: NARMAX Methods in the Time, Frequency, and Spatio-Temporal Domains". Wiley, 2013&lt;/ref&gt;  to infer laws (regressions, nonlinear relationships, and causal effects) from large sets of data with low information density&lt;ref&gt;{{cite web|url=http://www.andsi.fr/tag/dsi-big-data/|title=le Blog ANDSI   » DSI Big Data|publisher=}}&lt;/ref&gt; to reveal relationships and dependencies, or to perform predictions of outcomes and behaviors.&lt;ref name="SAB1" /&gt;&lt;ref&gt;{{cite web|url=http://lecercle.lesechos.fr/entrepreneur/tendances-innovation/221169222/big-data-low-density-data-faible-densite-information-com|title=Les Echos – Big Data car Low-Density Data ? La faible densité en information comme facteur discriminant – Archives|author=Les Echos|date=3 April 2013|work=lesechos.fr}}&lt;/ref&gt;

== Characteristics ==
Big data can be described by the following characteristics:&lt;ref name="BD4D" /&gt;&lt;ref name="WhatIsBigData" /&gt;

;Volume: The quantity of generated and stored data. The size of the data determines the value and potential insight- and whether it can actually be considered big data or not.

;Variety: The type and nature of the data. This helps people who analyze it to effectively use the resulting insight.

;Velocity: In this context, the speed at which the data is generated and processed to meet the demands and challenges that lie in the path of growth and development.

;Variability: Inconsistency of the data set can hamper processes to handle and manage it.

;Veracity: The quality of captured data can vary greatly, affecting accurate analysis.

Factory work and [[Cyber-physical system]]s may have a 6C system:
* Connection (sensor and networks)
* Cloud (computing and data on demand)&lt;ref&gt;Wu, D., Liu. X., Hebert, S., Gentzsch, W., Terpenny, J. (2015). Performance Evaluation of Cloud-Based High Performance Computing for Finite Element Analysis. Proceedings of the ASME 2015 International Design Engineering Technical Conference &amp; Computers and Information in Engineering Conference (IDETC/CIE2015), Boston, Massachusetts, U.S.&lt;/ref&gt;&lt;ref&gt;{{cite journal | last1 = Wu | first1 = D. | last2 = Rosen | first2 = D.W. | last3 = Wang | first3 = L. | last4 = Schaefer | first4 = D. | year = 2015 | title = Cloud-Based Design and Manufacturing: A New Paradigm in Digital Manufacturing and Design Innovation | url = | journal = Computer-Aided Design | volume = 59 | issue = 1| pages = 1–14 | doi = 10.1016/j.cad.2014.07.006 }}&lt;/ref&gt;
* Cyber (model and memory)
* Content/context (meaning and correlation)
* Community (sharing and collaboration)
* Customization (personalization and value)

Data must be processed with advanced tools (analytics and algorithms) to reveal meaningful information. For example, to manage a factory one must consider both visible and invisible issues with various components. Information generation algorithms must detect and address invisible issues such as machine degradation, component wear, etc. on the factory floor.&lt;ref name=INDIN2014&gt;{{cite journal|last1=Lee|first1=Jay|last2=Bagheri|first2=Behrad|last3=Kao|first3=Hung-An|title=Recent Advances and Trends of Cyber-Physical Systems and Big Data Analytics in Industrial Informatics|journal=IEEE Int. Conference on Industrial Informatics (INDIN) 2014|date=2014|url=https://www.researchgate.net/profile/Behrad_Bagheri/publication/266375284_Recent_Advances_and_Trends_of_Cyber-Physical_Systems_and_Big_Data_Analytics_in_Industrial_Informatics/links/542dc0100cf27e39fa948a7d?origin=publication_detail}}&lt;/ref&gt;&lt;ref name=MfgLetters&gt;{{cite journal|last1=Lee|first1=Jay|last2=Lapira|first2=Edzel|last3=Bagheri|first3=Behrad|last4=Kao|first4=Hung-an|title=Recent advances and trends in predictive manufacturing systems in big data environment|journal=Manufacturing Letters|volume=1|issue=1|pages=38–41|doi=10.1016/j.mfglet.2013.09.005|url=http://www.sciencedirect.com/science/article/pii/S2213846313000114}}&lt;/ref&gt;

== Architecture ==

In 2000, Seisint Inc. (now [[LexisNexis|LexisNexis Group]]) developed a C++-based distributed file-sharing framework for data storage and query. The system stores and distributes structured, semi-structured, and [[unstructured data]] across multiple servers. Users can build queries in a C++ [[Dialect (computing)|dialect]] called [[ECL programming language|ECL]]. ECL uses an "apply schema on read" method to infer the structure of stored data when it is queried, instead of when it is stored. In 2004, LexisNexis acquired Seisint Inc.&lt;ref&gt;{{cite web|url=http://www.washingtonpost.com/wp-dyn/articles/A50577-2004Jul14.html|title=LexisNexis To Buy Seisint For $775 Million|publisher=Washington Post|accessdate=15 July 2004}}&lt;/ref&gt; and in 2008 acquired [[ChoicePoint|ChoicePoint, Inc.]]&lt;ref&gt;{{cite web|url=http://www.washingtonpost.com/wp-dyn/content/article/2008/02/21/AR2008022100809.html|title=LexisNexis Parent Set to Buy ChoicePoint|publisher=Washington Post|accessdate=22 February 2008}}&lt;/ref&gt; and their high-speed parallel processing platform. The two platforms were merged into [[HPCC]] (or High-Performance Computing Cluster) Systems and in 2011, HPCC was open-sourced under the Apache v2.0 License. [[Quantcast File System]] was available about the same time.&lt;ref&gt;{{cite web|url=http://www.datanami.com/2012/10/01/quantcast_opens_exabyte_ready_file_system/|title=Quantcast Opens Exabyte-Ready File System|publisher=www.datanami.com|accessdate=1 October 2012}}&lt;/ref&gt;

In 2004, [[Google]] published a paper on a process called [[MapReduce]] that uses a similar architecture. The MapReduce concept provides a parallel processing model, and an associated implementation was released to process huge amounts of data.  With MapReduce, queries are split and distributed across parallel nodes and processed in parallel (the Map step). The results are then gathered and delivered (the Reduce step). The framework was very successful,&lt;ref&gt;Bertolucci, Jeff [http://www.informationweek.com/big-data/news/software-platforms/hadoop-from-experiment-to-leading-big-d/240157176 "Hadoop: From Experiment To Leading Big Data Platform"], "Information Week", 2013. Retrieved on 14 November 2013.&lt;/ref&gt; so others wanted to replicate the algorithm. Therefore, an implementation of the MapReduce framework was adopted by an Apache open-source project named [[Apache Hadoop|Hadoop]].&lt;ref&gt;Webster, John. [http://research.google.com/archive/mapreduce-osdi04.pdf "MapReduce: Simplified Data Processing on Large Clusters"], "Search Storage", 2004. Retrieved on 25 March 2013.&lt;/ref&gt;

[[MIKE2.0 Methodology|MIKE2.0]] is an open approach to information management that acknowledges the need for revisions due to big data implications identified in an article titled "Big Data Solution Offering".&lt;ref&gt;{{cite web|url=http://mike2.openmethodology.org/wiki/Big_Data_Solution_Offering|title=Big Data Solution Offering|publisher=MIKE2.0|accessdate=8 December 2013}}&lt;/ref&gt; The methodology addresses handling big data in terms of useful [[permutation]]s of data sources, [[complexity]] in interrelationships, and difficulty in deleting (or modifying) individual records.&lt;ref&gt;{{cite web|url=http://mike2.openmethodology.org/wiki/Big_Data_Definition|title=Big Data Definition|publisher=MIKE2.0|accessdate=9 March 2013}}&lt;/ref&gt;

2012 studies showed that a multiple-layer architecture is one option to address the issues that big data presents. A [[List of file systems#Distributed parallel file systems|distributed parallel]] architecture distributes data across multiple servers; these parallel execution environments can dramatically improve data processing speeds. This type of architecture inserts data into a parallel DBMS, which implements the use of MapReduce and Hadoop frameworks. This type of framework looks to make the processing power transparent to the end user by using a front-end application server.&lt;ref&gt;{{cite journal|last=Boja|first=C|author2=Pocovnicu, A |author3=Bătăgan, L. |title=Distributed Parallel Architecture for Big Data|journal=Informatica Economica|year=2012|volume=16|issue=2|pages=116–127}}&lt;/ref&gt;

Big data analytics for manufacturing applications is marketed as a 5C architecture (connection, conversion, cyber, cognition, and configuration).&lt;ref&gt;{{cite web|url=http://www.imscenter.net/cyber-physical-platform|title=IMS_CPS — IMS Center|publisher=|accessdate=16 June 2016}}&lt;/ref&gt;

The [[data lake]] allows an organization to shift its focus from centralized control to a shared model to respond to the changing dynamics of information management. This enables quick segregation of data into the data lake, thereby reducing the overhead time.&lt;ref&gt;http://www.hcltech.com/sites/default/files/solving_key_businesschallenges_with_big_data_lake_0.pdf&lt;/ref&gt;&lt;ref&gt;{{ cite web| url=https://secplab.ppgia.pucpr.br/files/papers/2015-0.pdf | title= Method for testing the fault tolerance of MapReduce frameworks | publisher=Computer Networks | year=2015}}&lt;/ref&gt;

== Technologies ==
{{see|Enablers of big data}}
A 2011 [[McKinsey &amp; Company|McKinsey Global Institute]] report characterizes the main components and ecosystem of big data as follows:&lt;ref name="McKinsey"&gt;{{cite journal
 | last1 = Manyika
 | first1 = James
 | first2=Michael |last2=Chui |first3=Jaques |last3=Bughin |first4=Brad |last4=Brown |first5=Richard |last5=Dobbs |first6=Charles |last6=Roxburgh |first7=Angela Hung |last7=Byers
 | title = Big Data: The next frontier for innovation, competition, and productivity
 | publisher = McKinsey Global Institute
 | date = May 2011
 | url =  http://www.mckinsey.com/Insights/MGI/Research/Technology_and_Innovation/Big_data_The_next_frontier_for_innovation
|accessdate=January 16, 2016
}}&lt;/ref&gt;
* Techniques for analyzing data, such as [[A/B testing]], [[machine learning]] and [[natural language processing]]
* Big data technologies, like [[business intelligence]], [[cloud computing]] and databases
* Visualization, such as charts, graphs and other displays of the data

Multidimensional big data can also be represented as [[tensor]]s, which can be more efficiently handled by tensor-based computation,&lt;ref&gt;{{cite web |title=Future Directions in Tensor-Based Computation and Modeling |date=May 2009|url=http://www.cs.cornell.edu/cv/tenwork/finalreport.pdf}}&lt;/ref&gt; such as [[multilinear subspace learning]].&lt;ref name="MSLsurvey"&gt;{{cite journal
 |first=Haiping |last=Lu
 |first2=K.N. |last2=Plataniotis
 |first3=A.N. |last3=Venetsanopoulos
 |url=http://www.dsp.utoronto.ca/~haiping/Publication/SurveyMSL_PR2011.pdf
 |title=A Survey of Multilinear Subspace Learning for Tensor Data
 |journal=Pattern Recognition
 |volume=44 |number=7 |pages=1540–1551 |year=2011
 |doi=10.1016/j.patcog.2011.01.004
}}&lt;/ref&gt; Additional technologies being applied to big data include massively parallel-processing ([[Massive parallel processing|MPP]]) databases, [[search-based application]]s, [[data mining]],&lt;ref&gt;{{cite web|last1=Pllana|first1=Sabri|last2=Janciak|first2=Ivan|last3=Brezany|first3=Peter|last4=Wöhrer|first4=Alexander|title=A Survey of the State of the Art in Data Mining and Integration Query Languages|url=http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6041580|website=2011 International Conference on Network-Based Information Systems (NBIS 2011)|publisher=IEEE Computer Society|accessdate=2 April 2016}}&lt;/ref&gt; [[distributed file system]]s, [[distributed database]]s, [[cloud computing|cloud-based]] infrastructure (applications, storage and computing resources) and the Internet.{{Citation needed|date=September 2011}}

Some but not all [[Massive parallel processing|MPP]] relational databases have the ability to store and manage petabytes of data. Implicit is the ability to load, monitor, back up, and optimize the use of the large data tables in the [[RDBMS]].&lt;ref&gt;{{cite web |author=Monash, Curt |title=eBay's two enormous data warehouses |date=30 April 2009 |url=http://www.dbms2.com/2009/04/30/ebays-two-enormous-data-warehouses/}}&lt;br /&gt;{{cite web |author=Monash, Curt |title=eBay followup&amp;nbsp;– Greenplum out, Teradata &gt; 10 petabytes, Hadoop has some value, and more |date=6 October 2010 |url=http://www.dbms2.com/2010/10/06/ebay-followup-greenplum-out-teradata-10-petabytes-hadoop-has-some-value-and-more/}}&lt;/ref&gt;

[[DARPA]]'s [[Topological Data Analysis]] program seeks the fundamental structure of massive data sets and in 2008 the technology went public with the launch of a company called [[Ayasdi]].&lt;ref&gt;{{cite web|url=http://www.ayasdi.com/resources/|title=Resources on how Topological Data Analysis is used to analyze big data|publisher=Ayasdi}}&lt;/ref&gt;

The practitioners of big data analytics processes are generally hostile to slower shared storage,&lt;ref&gt;{{cite web |title=Storage area networks need not apply |author=CNET News |date=1 April 2011 |url=http://news.cnet.com/8301-21546_3-20049693-10253464.html}}&lt;/ref&gt; preferring direct-attached storage ([[Direct-attached storage|DAS]]) in its various forms from solid state drive ([[Ssd]]) to high capacity [[Serial ATA|SATA]] disk buried inside parallel processing nodes. The perception of shared storage architectures—[[Storage area network]] (SAN) and [[Network-attached storage]] (NAS) —is that they are relatively slow, complex, and expensive. These qualities are not consistent with big data analytics systems that thrive on system performance, commodity infrastructure, and low cost.

Real or near-real time information delivery is one of the defining characteristics of big data analytics. Latency is therefore avoided whenever and wherever possible. Data in memory is good—data on spinning disk at the other end of a [[Fiber connector|FC]] [[Storage area network|SAN]] connection is not. The cost of a [[Storage area network|SAN]] at the scale needed for analytics applications is very much higher than other storage techniques.

There are advantages as well as disadvantages to shared storage in big data analytics, but big data analytics practitioners {{As of|2011|lc=on}} did not favour it.&lt;ref&gt;{{cite web |title=How New Analytic Systems will Impact Storage |date=September 2011 |url=http://www.evaluatorgroup.com/document/big-data-how-new-analytic-systems-will-impact-storage-2/}}&lt;/ref&gt;

== Applications ==
[[File:2013-09-11 Bus wrapped with SAP Big Data parked outside IDF13 (9730051783).jpg|thumb|Bus wrapped with [[SAP AG|SAP]] Big data parked outside [[Intel Developer Forum|IDF13]].]]
Big data has increased the demand of information management specialists so much so that [[Software AG]], [[Oracle Corporation]], [[IBM]], [[Microsoft]], [[SAP AG|SAP]], [[EMC Corporation|EMC]], [[Hewlett-Packard|HP]] and [[Dell]] have spent more than $15&amp;nbsp;billion on software firms specializing in data management and analytics. In 2010, this industry was worth more than $100&amp;nbsp;billion and was growing at almost 10&amp;nbsp;percent a year: about twice as fast as the software business as a whole.{{r|Economist}}

Developed economies increasingly use data-intensive technologies. There are 4.6&amp;nbsp;billion mobile-phone subscriptions worldwide, and between 1&amp;nbsp;billion and 2&amp;nbsp;billion people accessing the internet.{{r|Economist}} Between 1990 and 2005, more than 1&amp;nbsp;billion people worldwide entered the middle class, which means more people became more literate, which in turn lead to information growth. The world's effective capacity to exchange information through telecommunication networks was 281 [[petabytes]] in 1986, 471 [[petabytes]] in 1993, 2.2 exabytes in 2000, 65 [[exabytes]] in 2007&lt;ref name="martinhilbert.net"/&gt; and predictions put the amount of internet traffic at 667 exabytes annually by 2014.{{r|Economist}} According to one estimate, one third of the globally stored information is in the form of alphanumeric text and still image data,&lt;ref name="HilbertContent"&gt;{{cite web|url=http://www.tandfonline.com/doi/abs/10.1080/01972243.2013.873748|title=An Error Occurred Setting Your User Cookie|publisher=}}&lt;/ref&gt; which is the format most useful for most big data applications. This also shows the potential of yet unused data (i.e. in the form of video and audio content).

While many vendors offer off-the-shelf solutions for big data, experts recommend the development of in-house solutions custom-tailored to solve the company's problem at hand if the company has sufficient technical capabilities.&lt;ref&gt;{{cite web |url=http://www.kdnuggets.com/2014/07/interview-amy-gershkoff-ebay-in-house-BI-tools.html |title=Interview: Amy Gershkoff, Director of Customer Analytics &amp; Insights, eBay on How to Design Custom In-House BI Tools |last1=Rajpurohit |first1=Anmol |date=11 July 2014 |website= KDnuggets|accessdate=2014-07-14|quote=Dr. Amy Gershkoff: "Generally, I find that off-the-shelf business intelligence tools do not meet the needs of clients who want to derive custom insights from their data. Therefore, for medium-to-large organizations with access to strong technical talent, I usually recommend building custom, in-house solutions."}}&lt;/ref&gt;

=== Government ===
The use and adoption of big data within governmental processes is beneficial and allows efficiencies in terms of cost, productivity, and innovation,&lt;ref&gt;{{cite web|url=http://www.computerworld.com/article/2472667/government-it/the-government-and-big-data--use--problems-and-potential.html |title=The Government and big data: Use, problems and potential |date=21 March 2012 |publisher=Computerworld |access-date=12 September 2016}}&lt;/ref&gt; but does not come without its flaws. Data analysis often requires multiple parts of government (central and local) to work in collaboration and create new and innovative processes to deliver the desired outcome. Below are some examples of initiatives the governmental big data space.

==== United States of America ====
* In 2012, the [[Presidency of Barack Obama|Obama administration]] announced the Big Data Research and Development Initiative, to explore how big data could be used to address important problems faced by the government.&lt;ref name=WH_Big_Data&gt;{{cite web|last=Kalil|first=Tom|title=Big Data is a Big Deal|url=http://www.whitehouse.gov/blog/2012/03/29/big-data-big-deal|publisher=White House|accessdate=26 September 2012}}&lt;/ref&gt; The initiative is composed of 84 different big data programs spread across six departments.&lt;ref&gt;{{cite web|last=Executive Office of the President|title=Big Data Across the Federal Government|url=http://www.whitehouse.gov/sites/default/files/microsites/ostp/big_data_fact_sheet_final_1.pdf|publisher=White House|accessdate=26 September 2012 |date=March 2012}}&lt;/ref&gt;
* Big data analysis played a large role in [[Barack Obama]]'s successful [[Barack Obama presidential campaign, 2012|2012 re-election campaign]].&lt;ref name=infoworld_bigdata&gt;{{cite web|last=Lampitt|first=Andrew|title=The real story of how big data analytics helped Obama win|url=http://www.infoworld.com/d/big-data/the-real-story-of-how-big-data-analytics-helped-obama-win-212862|work=[[Infoworld]]|accessdate=31 May 2014}}&lt;/ref&gt;
* The [[United States Federal Government]] owns six of the ten most powerful [[supercomputer]]s in the world.&lt;ref&gt;{{cite web |last=Hoover |first=J. Nicholas |title=Government's 10 Most Powerful Supercomputers |url=http://www.informationweek.com/government/enterprise-applications/image-gallery-governments-10-most-powerf/224700271 |work=Information Week |publisher=UBM |accessdate=26 September 2012}}&lt;/ref&gt;
* The [[Utah Data Center]] has been constructed by the United States [[National Security Agency]]. When finished, the facility will be able to handle a large amount of information collected by the NSA over the Internet. The exact amount of storage space is unknown, but more recent sources claim it will be on the order of a few [[exabyte]]s.&lt;ref&gt;{{cite news | last=Bamford|first=James|title=The NSA Is Building the Country's Biggest Spy Center (Watch What You Say)|url=http://www.wired.com/threatlevel/2012/03/ff_nsadatacenter/all/1|work=Wired Magazine|accessdate=2013-03-18|date=15 March 2012}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.nsa.gov/public_info/press_room/2011/utah_groundbreaking_ceremony.shtml|title=Groundbreaking Ceremony Held for $1.2 Billion Utah Data Center|publisher=National Security Agency Central Security Service|accessdate=2013-03-18}}&lt;/ref&gt;&lt;ref&gt;{{cite news | last=Hill|first=Kashmir|title=TBlueprints of NSA's Ridiculously Expensive Data Center in Utah Suggest It Holds Less Info Than Thought|url=http://www.forbes.com/sites/kashmirhill/2013/07/24/blueprints-of-nsa-data-center-in-utah-suggest-its-storage-capacity-is-less-impressive-than-thought/|work=Forbes|accessdate=2013-10-31}}&lt;/ref&gt;

==== India ====
* Big data analysis was in part responsible for the [[Bharatiya Janata Party|BJP]] to win the [[Indian general election, 2014|Indian General Election 2014]].&lt;ref&gt;{{cite web|url = http://www.livemint.com/Industry/bUQo8xQ3gStSAy5II9lxoK/Are-Indian-companies-making-enough-sense-of-Big-Data.html|title = News: Live Mint|date = 23 June 2014|accessdate = 2014-11-22|website = Are Indian companies making enough sense of Big Data?|publisher = Live Mint}}&lt;/ref&gt;
* The [[Government of India|Indian government]] utilizes numerous techniques to ascertain how the Indian electorate is responding to government action, as well as ideas for policy augmentation.&lt;ref&gt;{{cite web|url=http://decipherias.com/currentaffairs/big-data-whats-so-big-about-it/|title=Big Data- What’s so big about it?|date=18 March 2016|publisher=Decipher IAS|access-date=12 September 2016}}&lt;/ref&gt;

==== United Kingdom ====
Examples of uses of big data in public services:
* Data on prescription drugs: by connecting origin, location and the time of each prescription, a research unit was able to exemplify the considerable delay between the release of any given drug, and a UK-wide adaptation of the [[National Institute for Health and Care Excellence]] guidelines. This suggests that new or most up-to-date drugs take some time to filter through to the general patient.&lt;ref&gt;{{cite web|url=https://www.ijedr.org/papers/IJEDR1504022.pdf|title=Survey on Big Data Using Data Mining|date=2015|publisher=International Journal of Engineering Development and Research|access-date=14 September 2016}}&lt;/ref&gt;
* Joining up data: a local authority blended data about services, such as road gritting rotas, with services for people at risk, such as 'meals on wheels'. The connection of data allowed the local authority to avoid any weather related delay.&lt;ref&gt;{{cite web|url=https://www.researchgate.net/publication/297762848_Recent_advances_delivered_by_mobile_cloud_computing_and_Internet_of_Things_for_Big_data_applications_A_Survey|title=Recent advances delivered by Mobile Cloud Computing and Internet of Things for Big Data applications: a survey|date=11 March 2016|publisher=International Journal of Network Management|access-date=14 September 2016}}&lt;/ref&gt;

=== International development ===
Research on the effective usage of [[information and communication technologies for development]] (also known as [[ICT4D]]) suggests that big data technology can make important contributions but also present unique challenges to [[International development]].&lt;ref&gt;{{cite web|url=http://www.unglobalpulse.org/projects/BigDataforDevelopment|title=White Paper: Big Data for Development: Opportunities &amp; Challenges (2012) – United Nations Global Pulse|publisher=|accessdate=13 April 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=WEF (World Economic Forum), &amp; Vital Wave Consulting. (2012). Big Data, Big Impact: New Possibilities for International Development|work= World Economic Forum|accessdate=24 August 2012|url= http://www.weforum.org/reports/big-data-big-impact-new-possibilities-international-development}}&lt;/ref&gt; Advancements in big data analysis offer cost-effective opportunities to improve decision-making in critical development areas such as health care, employment, [[economic productivity]], crime, security, and [[natural disaster]] and resource management.&lt;ref name="HilbertBigData2013" /&gt;&lt;ref&gt;{{cite web|url=http://blogs.worldbank.org/ic4d/four-ways-to-talk-about-big-data/|title=Elena Kvochko, Four Ways To talk About Big Data (Information Communication Technologies for Development Series)|publisher=worldbank.org|accessdate=2012-05-30}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Daniele Medri: Big Data &amp; Business: An on-going revolution|url=http://www.statisticsviews.com/details/feature/5393251/Big-Data--Business-An-on-going-revolution.html|publisher=Statistics Views |date=21 October 2013}}&lt;/ref&gt; Additionally, user-generated data offers new opportunities to give the unheard a voice.&lt;ref&gt;{{cite web|title=Responsible use of data|author=Tobias Knobloch and Julia Manske|work= D+C, Development and Cooperation|date=11 January 2016|url= http://www.dandc.eu/en/article/opportunities-and-risks-user-generated-and-automatically-compiled-data}}&lt;/ref&gt; However, longstanding challenges for developing regions such as inadequate technological infrastructure and economic and human resource scarcity exacerbate existing concerns with big data such as privacy, imperfect methodology, and interoperability issues.&lt;ref name="HilbertBigData2013" /&gt;

=== Manufacturing ===
Based on TCS 2013 Global Trend Study, improvements in supply planning and product quality provide the greatest benefit of big data for manufacturing.&lt;ref name="TCS Big Data Study – Manufacturing"&gt;{{cite web|url=http://sites.tcs.com/big-data-study/manufacturing-big-data-benefits-challenges/# |title=Manufacturing: Big Data Benefits and Challenges |work= TCS Big Data Study|publisher=[[Tata Consultancy Services Limited]] |location=Mumbai, India |accessdate=2014-06-03}}&lt;/ref&gt; Big data provides an infrastructure for transparency in manufacturing industry, which is the ability to unravel uncertainties such as inconsistent component performance and availability. Predictive manufacturing as an applicable approach toward near-zero downtime and transparency requires vast amount of data and advanced prediction tools for a systematic process of data into useful information.&lt;ref&gt;{{cite journal|last=Lee|first=Jay|author2=Wu, F. |author3=Zhao, W. |author4=Ghaffari, M. |author5= Liao, L |title=Prognostics and health management design for rotary machinery systems—Reviews, methodology and applications|journal=Mechanical Systems and Signal Processing|date=January 2013|volume=42|issue=1}}&lt;/ref&gt; A conceptual framework of predictive manufacturing begins with data acquisition where different type of sensory data is available to acquire such as acoustics, vibration, pressure, current, voltage and controller data. Vast amount of sensory data in addition to historical data construct the big data in manufacturing. The generated big data acts as the input into predictive tools and preventive strategies such as [[Prognostics]] and Health Management (PHM).&lt;ref&gt;{{cite web|url=https://www.phmsociety.org/events/conference/phm/europe/16/tutorials|title=Tutorials|publisher=PHM Society|accessdate=27 September 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://www.itri.org.tw/eng/Content/MSGPic01/contents.aspx?&amp;SiteID=1&amp;MmmID=620651706136357202&amp;CatID=620653256103620163&amp;MSID=654532365564567545|title=Prognostic and Health Management Technology for MOCVD Equipment|publisher=Industrial Technology Research Institute|accessdate=27 September 2016}}&lt;/ref&gt;

==== Cyber-physical models ====
Current PHM implementations mostly use data during the actual usage while analytical algorithms can perform more accurately when more information throughout the machine's lifecycle, such as system configuration, physical knowledge and working principles, are included. There is a need to systematically integrate, manage and analyze machinery or process data during different stages of machine life cycle to handle data/information more efficiently and further achieve better transparency of machine health condition for manufacturing industry.

With such motivation a cyber-physical (coupled) model scheme has been developed. The coupled model is a digital twin of the real machine that operates in the cloud platform and simulates the health condition with an integrated knowledge from both data driven analytical algorithms as well as other available physical knowledge. It can also be described as a 5S systematic approach consisting of sensing, storage, synchronization, synthesis and service. The coupled model first constructs a digital image from the early design stage. System information and physical knowledge are logged during product design, based on which a simulation model is built as a reference for future analysis. Initial parameters may be statistically generalized and they can be tuned using data from testing or the manufacturing process using parameter estimation. After that step, the simulation model can be considered a mirrored image of the real machine—able to continuously record and track machine condition during the later utilization stage. Finally, with the increased connectivity offered by cloud computing technology, the coupled model also provides better accessibility of machine condition for factory managers in cases where physical access to actual equipment or machine data is limited.&lt;ref name="MfgLetters" /&gt;

=== Healthcare ===
Big data analytics has helped healthcare improve by providing personalized medicine and prescriptive analytics, clinical risk intervention and predictive analytics, waste and care variability reduction, automated external and internal reporting of patient data, standardized medical terms and patient registries and fragmented point solutions.&lt;ref name="ref135"&gt;{{cite journal|doi=10.1016/j.ijrobp.2015.10.060|title=Impending Challenges for the Use of Big Data }}&lt;/ref&gt; Some areas of improvement are more aspirational than actually implemented. The level of data generated within healthcare systems is not trivial. With the added adoption of mHealth, eHealth and wearable technologies the volume of data will continue to increase. This includes electronic health record data, imaging data, patient generated data, sensor data, and other forms of difficult to process data. There is now an even greater need for such environments to pay greater attention to data and information quality.&lt;ref&gt;{{cite journal|url=http://doi.acm.org/10.1145/2378016.2378021|title=Data Management Within mHealth Environments: Patient Sensors, Mobile Devices, and Databases|first1=John|last1=O'Donoghue|first2=John|last2=Herbert|date=1 October 2012|publisher=|volume=4|issue=1|pages=5:1–5:20|accessdate=16 June 2016|via=ACM Digital Library|doi=10.1145/2378016.2378021}}&lt;/ref&gt; "Big data very often means `dirty data' and the fraction of data inaccuracies increases with data volume growth." Human inspection at the big data scale is impossible and there is a desperate need in health service for intelligent tools for accuracy and believability control and handling of information missed.&lt;ref name="Mirkes2016"&gt;{{cite journal | last1 = Mirkes| first1 =E.M.|last2 = Coats|first2 =T.J.|last3 = Levesley|first3 =J.|last4 = Gorban|first4 = A.N.| title = Handling missing data in large healthcare dataset: A case study of unknown trauma outcomes|url =  https://www.researchgate.net/publication/300400110_Handling_missing_data_in_large_healthcare_dataset_A_case_study_of_unknown_trauma_outcomes| journal = Computers in Biology and Medicine| volume = 75| issue = | pages = 203–216| year = 2016| doi = 10.1016/j.compbiomed.2016.06.004}}&lt;/ref&gt; While extensive information in healthcare is now electronic, it fits under the big data umbrella as most is unstructured and difficult to use.&lt;ref&gt;{{Cite journal|last=Murdoch|first=Travis B.|last2=Detsky|first2=Allan S.|date=2013-04-03|title=The Inevitable Application of Big Data to Health Care|url=http://jamanetwork.com/journals/jama/article-abstract/1674245|journal=JAMA|language=en|volume=309|issue=13|doi=10.1001/jama.2013.393|issn=0098-7484}}&lt;/ref&gt;

=== Education ===
A [[McKinsey &amp; Company|McKinsey Global Institute]] study found a shortage of 1.5 million highly trained data professionals and managers&lt;ref name="McKinsey"/&gt; and a number of universities&lt;ref&gt;{{cite news
| url=http://www.forbes.com/sites/jmaureenhenderson/2013/07/30/degrees-in-big-data-fad-or-fast-track-to-career-success/?
|access-date=2016-02-21
|newspaper=Forbes
|title=Degrees in Big Data: Fad or Fast Track to Career Success}}&lt;/ref&gt; including [[University of Tennessee]] and [[UC Berkeley]], have created masters programs to meet this demand.  Private bootcamps have also developed programs to meet that demand, including free programs like [[The Data Incubator]] or paid programs like [[General Assembly]].&lt;ref&gt;{{cite news
|title=NY gets new bootcamp for data scientists: It’s free, but harder to get into than Harvard
|newspaper=Venture Beat
|access-date=2016-02-21
|url=http://venturebeat.com/2014/04/15/ny-gets-new-bootcamp-for-data-scientists-its-free-but-harder-to-get-into-than-harvard/
}}&lt;/ref&gt;

=== Media ===
To understand how the media utilises big data, it is first necessary to provide some context into the mechanism used for media process. It has been suggested by Nick Couldry and Joseph Turow that [[wikt:practitioner|practitioners]] in Media and Advertising approach big data as many actionable points of information about millions of individuals. The industry appears to be moving away from the traditional approach of using specific media environments such as newspapers, magazines, or television shows and instead taps into consumers with technologies that reach targeted people at optimal times in optimal locations.  The ultimate aim is to serve, or convey, a message or content that is (statistically speaking) in line with the consumer's mindset. For example, publishing environments are increasingly tailoring messages (advertisements) and content (articles) to appeal to consumers that have been exclusively gleaned through various [[data-mining]] activities.&lt;ref&gt;{{cite journal|last1=Couldry|first1=Nick|last2=Turow|first2=Joseph|title=Advertising, Big Data, and the Clearance of the Public Realm: Marketers’ New Approaches to the Content Subsidy|journal=International Journal of Communication|date=2014|volume=8|pages=1710–1726}}&lt;/ref&gt;
* Targeting of consumers (for advertising by marketers)
* Data-capture
* [[Data journalism]]: publishers and journalists use big data tools to provide unique and innovative insights and infographics.

==== Internet of Things (IoT) ====
{{tone|section|date=September 2016}}

{{Main article|Internet of Things}}
Big data and the IoT work in conjunction.  From a media perspective, data is the key derivative of device inter-connectivity and allows accurate targeting.  The [[Internet of Things]], with the help of big data, therefore transforms the media industry, companies and even governments, opening up a new era of economic growth and competitiveness. The intersection of people, data and intelligent algorithms have far-reaching impacts on media efficiency. The wealth of data generated allows an elaborate layer on the present targeting mechanisms of the industry.

==== Technology ====
* [[eBay.com]] uses two data warehouses at 7.5 [[petabytes]] and 40PB as well as a 40PB [[Hadoop]] cluster for search, consumer recommendations, and merchandising.&lt;ref&gt;{{cite web | last=Tay | first=Liz |url=http://www.itnews.com.au/news/inside-ebay8217s-90pb-data-warehouse-342615 | title=Inside eBay’s 90PB data warehouse | publisher=ITNews | accessdate=2016-02-12}}&lt;/ref&gt;
* [[Amazon.com]] handles millions of back-end operations every day, as well as queries from more than half a million third-party sellers. The core technology that keeps Amazon running is Linux-based and as of 2005 they had the world's three largest Linux databases, with capacities of 7.8 TB, 18.5 TB, and 24.7 TB.&lt;ref&gt;{{cite web|last=Layton |first=Julia |url=http://money.howstuffworks.com/amazon1.htm |title=Amazon Technology |publisher=Money.howstuffworks.com |accessdate=2013-03-05}}&lt;/ref&gt;
* [[Facebook]] handles 50&amp;nbsp;billion photos from its user base.&lt;ref&gt;{{cite web|url=https://www.facebook.com/notes/facebook-engineering/scaling-facebook-to-500-million-users-and-beyond/409881258919 |title=Scaling Facebook to 500 Million Users and Beyond |publisher=Facebook.com |accessdate=2013-07-21}}&lt;/ref&gt;
* As of August 2012, [[Google]] was handling roughly 100&amp;nbsp;billion searches per month.&lt;ref&gt;{{cite web|url=http://searchengineland.com/google-1-trillion-searches-per-year-212940|title=Google Still Doing at Least 1 Trillion Searches Per Year|date=16 January 2015|work=Search Engine Land|accessdate=15 April 2015}}&lt;/ref&gt;
* [[Oracle NoSQL Database]] has been tested to past the 1M ops/sec mark with 8 shards and proceeded to hit 1.2M ops/sec with 10 shards.&lt;ref&gt;{{cite web |last=Lamb |first=Charles |url=https://blogs.oracle.com/charlesLamb/entry/oracle_nosql_database_exceeds_1 |title=Oracle NoSQL Database Exceeds 1 Million Mixed YCSB Ops/Sec}}&lt;/ref&gt;

=== Private sector ===

=== Information Technology ===
Especially since 2015, big data has come to prominence within [[Business Operations]] as a tool to help employees work more efficiently and streamline the collection and distribution of [[Information Technology]] (IT). The use of big data to attack IT and data collection issues within an enterprise is called [[IT Operations Analytics]] (ITOA).&lt;ref name="ITOA1"&gt;{{cite web|last1=Solnik|first1=Ray|title=The Time Has Come: Analytics Delivers for IT Operations|url=http://www.datacenterjournal.com/time-analytics-delivers-operations/|website=Data Center Journal|accessdate=June 21, 2016}}&lt;/ref&gt; By applying big data principles into the concepts of [[machine intelligence]] and [[deep computing]], IT departments can predict potential issues and move to provide solutions before the problems even happen.&lt;ref name="ITOA1" /&gt; In this time, ITOA businesses were also beginning to play a major role in [[systems management]] by offering platforms that brought individual [[data silos]] together and generated insights from the whole of the system rather than from isolated pockets of data.

==== Retail ====
* [[Walmart]] handles more than 1 million customer transactions every hour, which are imported into databases estimated to contain more than 2.5 petabytes (2560 terabytes) of data—the equivalent of 167 times the information contained in all the books in the US [[Library of Congress]].{{r|Economist}}

==== Retail banking ====
* FICO Card Detection System protects accounts worldwide.&lt;ref name="fico.com"&gt;{{cite web|url=http://www.fico.com/en/Products/DMApps/Pages/FICO-Falcon-Fraud-Manager.aspx |title=FICO® Falcon® Fraud Manager |publisher=Fico.com |accessdate=2013-07-21}}&lt;/ref&gt;
* The volume of business data worldwide, across all companies, doubles every 1.2 years, according to estimates.&lt;ref name="KnowWPCarey.com"&gt;{{cite web|url=http://research.wpcarey.asu.edu/managing-it/ebay-study-how-to-build-trust-and-improve-the-shopping-experience |title=eBay Study: How to Build Trust and Improve the Shopping Experience |publisher=Knowwpcarey.com |date=8 May 2012 |accessdate=2015-12-20}}&lt;/ref&gt;&lt;ref&gt;[http://www.statista.com/statistics/280444/global-leading-priorities-for-big-data-according-to-business-and-it-executives/ Leading Priorities for Big Data for Business and IT]. eMarketer. October 2013. Retrieved January 2014.&lt;/ref&gt;

==== Real estate ====
* [[Windermere Real Estate]] uses anonymous GPS signals from nearly 100 million drivers to help new home buyers determine their typical drive times to and from work throughout various times of the day.&lt;ref&gt;{{cite news|last=Wingfield |first=Nick |url=http://bits.blogs.nytimes.com/2013/03/12/predicting-commutes-more-accurately-for-would-be-home-buyers/ |title=Predicting Commutes More Accurately for Would-Be Home Buyers – NYTimes.com |publisher=Bits.blogs.nytimes.com |date=12 March 2013 |accessdate=2013-07-21}}&lt;/ref&gt;

=== Science ===
The [[Large Hadron Collider]] experiments represent about 150 million sensors delivering data 40&amp;nbsp;million times per second. There are nearly 600&amp;nbsp;million collisions per second. After filtering and refraining from recording more than 99.99995%&lt;ref&gt;{{cite web|last1=Alexandru|first1=Dan|title=Prof|url=https://cds.cern.ch/record/1504817/files/CERN-THESIS-2013-004.pdf|website=cds.cern.ch|publisher=CERN|accessdate=24 March 2015}}&lt;/ref&gt; of these streams, there are 100 collisions of interest per second.&lt;ref&gt;{{cite web |title=LHC Brochure, English version. A presentation of the largest and the most powerful particle accelerator in the world, the Large Hadron Collider (LHC), which started up in 2008. Its role, characteristics, technologies, etc. are explained for the general public. |url=http://cds.cern.ch/record/1278169?ln=en |work=CERN-Brochure-2010-006-Eng. LHC Brochure, English version. |publisher=CERN |accessdate=20 January 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web |title=LHC Guide, English version. A collection of facts and figures about the Large Hadron Collider (LHC) in the form of questions and answers. |url=http://cds.cern.ch/record/1092437?ln=en |work=CERN-Brochure-2008-001-Eng. LHC Guide, English version. |publisher=CERN |accessdate=20 January 2013}}&lt;/ref&gt;&lt;ref name="nature"&gt;{{cite news |title=High-energy physics: Down the petabyte highway |work= Nature |date= 19 January 2011 |first=Geoff |last=Brumfiel |doi= 10.1038/469282a |volume= 469 |pages= 282–83 |url= http://www.nature.com/news/2011/110119/full/469282a.html }}&lt;/ref&gt;
* As a result, only working with less than 0.001% of the sensor stream data, the data flow from all four LHC experiments represents 25 petabytes annual rate before replication (as of 2012). This becomes nearly 200 petabytes after replication.
* If all sensor data were recorded in LHC, the data flow would be extremely hard to work with. The data flow would exceed 150 million petabytes annual rate, or nearly 500 [[exabyte]]s per day, before replication. To put the number in perspective, this is equivalent to 500 [[quintillion]] (5×10&lt;sup&gt;20&lt;/sup&gt;) bytes per day, almost 200 times more than all the other sources combined in the world.

The [[Square Kilometre Array]] is a radio telescope built of thousands of antennas. It is expected to be operational by 2024. Collectively, these antennas are expected to gather 14 exabytes and store one petabyte per day.&lt;ref&gt;http://www.zurich.ibm.com/pdf/astron/CeBIT%202013%20Background%20DOME.pdf&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://arstechnica.com/science/2012/04/future-telescope-array-drives-development-of-exabyte-processing/|title=Future telescope array drives development of exabyte processing|work=Ars Technica|accessdate=15 April 2015}}&lt;/ref&gt; It is considered one of the most ambitious scientific projects ever undertaken.&lt;ref&gt;{{cite web|url=http://theconversation.com/australias-bid-for-the-square-kilometre-array-an-insiders-perspective-4891|title=Australia’s bid for the Square Kilometre Array – an insider’s perspective|date=1 February 2012|publisher=[[The Conversation (website)|The Conversation]]|accessdate=27 September 2016}}&lt;/ref&gt;

==== Science and research ====
{{Expand section|date=December 2016}}
* When the [[Sloan Digital Sky Survey]] (SDSS) began to collect astronomical data in 2000, it amassed more in its first few weeks than all data collected in the history of astronomy previously. Continuing at a rate of about 200&amp;nbsp;GB per night, SDSS has amassed more than 140 terabytes of information.&lt;ref name="Economist"&gt;{{cite news |title=Data, data everywhere |url=http://www.economist.com/node/15557443 |newspaper=The Economist |date=25 February 2010 |accessdate=9 December 2012}}&lt;/ref&gt; When the [[Large Synoptic Survey Telescope]], successor to SDSS, comes online in 2020, its designers expect it to acquire that amount of data every five days.{{r|Economist}}
* Decoding the [[Human Genome Project|human genome]] originally took 10 years to process, now it can be achieved in less than a day. The DNA sequencers have divided the sequencing cost by 10,000 in the last ten years, which is 100 times cheaper than the reduction in cost predicted by [[Moore's Law]].&lt;ref&gt;[http://www.oecd.org/sti/ieconomy/Session_3_Delort.pdf#page=6 Delort P., OECD ICCP Technology Foresight Forum, 2012.]&lt;/ref&gt;
* The [[NASA]] Center for Climate Simulation (NCCS) stores 32 petabytes of climate observations and simulations on the Discover supercomputing cluster.&lt;ref&gt;{{cite web|url=http://www.nasa.gov/centers/goddard/news/releases/2010/10-051.html|title=NASA – NASA Goddard Introduces the NASA Center for Climate Simulation|publisher=|accessdate=13 April 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Webster|first=Phil|title=Supercomputing the Climate: NASA's Big Data Mission|url=http://www.csc.com/cscworld/publications/81769/81773-supercomputing_the_climate_nasa_s_big_data_mission|work=CSC World|publisher=Computer Sciences Corporation|accessdate=2013-01-18}}&lt;/ref&gt;
* Google's DNAStack compiles and organizes DNA samples of genetic data from around the world to identify diseases and other medical defects. These fast and exact calculations eliminate any 'friction points,' or human errors that could be made by one of the numerous science and biology experts working with the DNA. DNAStack, a part of Google Genomics, allows scientists to use the vast sample of resources from Google's search server to scale social experiments that would usually take years, instantly.&lt;ref&gt;{{cite web|url=http://www.theglobeandmail.com/life/health-and-fitness/health/these-six-great-neuroscience-ideas-could-make-the-leap-from-lab-to-market/article21681731/|title=These six great neuroscience ideas could make the leap from lab to market|date=20 November 2014|publisher=[[The Globe and Mail]]|accessdate=1 October 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://cloud.google.com/customers/dnastack/|title=DNAstack tackles massive, complex DNA datasets with Google Genomics|publisher=Google Cloud Platform |accessdate=1 October 2016}}&lt;/ref&gt;
* [[23andme]]'s [[DNA database]] contains genetic information of over 1,000,000 people worldwide.&lt;ref&gt;{{cite web|title=23andMe - Ancestry|url=https://www.23andme.com/en-int/ancestry/|website=23andme.com|accessdate=29 December 2016}}&lt;/ref&gt; The company explores selling the "anonymous aggregated genetic data" to other researchers and pharmaceutical companies for research purposes if patients give their consent.&lt;ref name=verge1&gt;{{cite web|last1=Potenza|first1=Alessandra|title=23andMe wants researchers to use its kits, in a bid to expand its collection of genetic data|url=http://www.theverge.com/2016/7/13/12166960/23andme-genetic-testing-database-genotyping-research|publisher=The Verge|accessdate=29 December 2016|date=13 July 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=This Startup Will Sequence Your DNA, So You Can Contribute To Medical Research|url=https://www.fastcompany.com/3066775/innovation-agents/this-startup-will-sequence-your-dna-so-you-can-contribute-to-medical-resea|publisher=Fast Company|accessdate=29 December 2016|date=23 December 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last1=Seife|first1=Charles|title=23andMe Is Terrifying, but Not for the Reasons the FDA Thinks|url=https://www.scientificamerican.com/article/23andme-is-terrifying-but-not-for-the-reasons-the-fda-thinks/|publisher=Scientific American|accessdate=29 December 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last1=Zaleski|first1=Andrew|title=This biotech start-up is betting your genes will yield the next wonder drug|url=http://www.cnbc.com/2016/06/22/23andme-thinks-your-genes-are-the-key-to-blockbuster-drugs.html|publisher=CNBC|accessdate=29 December 2016|date=22 June 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last1=Regalado|first1=Antonio|title=How 23andMe turned your DNA into a $1 billion drug discovery machine|url=https://www.technologyreview.com/s/601506/23andme-sells-data-for-drug-search/|publisher=MIT Technology Review|accessdate=29 December 2016}}&lt;/ref&gt; Ahmad Hariri, professor of psychology and neuroscience at [[Duke University]] who has been using 23andMe in his research since 2009 states that the most important aspect of the company's new service is that it makes genetic research accessible and relatively cheap for scientists.&lt;ref name=verge1/&gt; A study that identified 15 genome sites linked to depression in 23andMe's database lead to a surge in demands to access the repository with 23andMe fielding nearly 20 requests to access the depression data in the two weeks after publication of the paper.&lt;ref&gt;{{cite web|title=23andMe reports jump in requests for data in wake of Pfizer depression study {{!}} FierceBiotech|url=http://www.fiercebiotech.com/it/23andme-reports-jump-requests-for-data-wake-pfizer-depression-study|website=fiercebiotech.com|accessdate=29 December 2016}}&lt;/ref&gt;

=== Sports ===
Big data can be used to improve training and understanding competitors, using sport sensors. It is also possible to predict winners in a match using big data analytics.&lt;ref&gt;{{cite web|url=http://www.itweb.co.za/index.php?option=com_content&amp;view=article&amp;id=147241|title=Data scientists predict Springbok defeat
|author=Admire Moyo|work=www.itweb.co.za|accessdate=12 December 2015}}&lt;/ref&gt;
Future performance of players could be predicted as well. Thus, players' value and salary is determined by data collected throughout the season.&lt;ref&gt;{{cite web|url=http://www.itweb.co.za/index.php?option=com_content&amp;view=article&amp;id=147852|title= Predictive analytics, big data transform sports
 |author=Regina Pazvakavambwa|work=www.itweb.co.za|accessdate=12 December 2015}}&lt;/ref&gt;

The movie [[Moneyball (film)|''MoneyBall'']] demonstrates how big data could be used to scout players and also identify undervalued players.&lt;ref&gt;{{cite web|url=http://www.datacenterknowledge.com/archives/2011/09/23/the-lessons-of-moneyball-for-big-data-analysis/|title= The Lessons of Moneyball for Big Data Analysis|author=Rich Miller|work=www.datecenterknowledge.com|accessdate=12 December 2015}}&lt;/ref&gt;

In Formula One races, race cars with hundreds of sensors generate terabytes of data. These sensors collect data points from tire pressure to fuel burn efficiency. Then, this data is transferred to team headquarters in United Kingdom through fiber optic cables that could carry data at the speed of light.&lt;ref&gt;{{cite web|url=http://www.huffingtonpost.com/dave-ryan/sports-where-big-data-fin_b_8553884.html|title= Sports: Where Big Data Finally Makes Sense |author=Dave Ryan|work=www.huffingtonpost.com|accessdate=12 December 2015}}&lt;/ref&gt;
Based on the data, engineers and data analysts decide whether adjustments should be made in order to win a race. Besides, using big data, race teams try to predict the time they will finish the race beforehand, based on simulations using data collected over the season.&lt;ref&gt;{{cite web|url=http://www.forbes.com/sites/frankbi/2014/11/13/how-formula-one-teams-are-using-big-data-to-get-the-inside-edge//|title= How Formula One Teams Are Using Big Data To Get The Inside Edge|author=Frank Bi|work=www.forbes.com|accessdate=12 December 2015}}&lt;/ref&gt;

== Research activities ==
Encrypted search and cluster formation in big data was demonstrated in March 2014 at the American Society of Engineering Education. Gautam Siwach engaged at ''Tackling the challenges of Big Data'' by [[MIT Computer Science and Artificial Intelligence Laboratory]] and Dr. Amir Esmailpour at UNH Research Group investigated the key features of big data as formation of clusters and their interconnections. They focused on the security of big data and the actual orientation of the term towards the presence of different type of data in an encrypted form at cloud interface by providing the raw definitions and real time examples within the technology. Moreover, they proposed an approach for identifying the encoding technique to advance towards an expedited search over encrypted text leading to the security enhancements in big data.&lt;ref&gt;{{cite conference |url=http://asee-ne.org/proceedings/2014/Student%20Papers/210.pdf |title=Encrypted Search &amp; Cluster Formation in Big Data |last1=Siwach |first1=Gautam |last2=Esmailpour |first2=Amir |date=March 2014 |year= |conference=ASEE 2014 Zone I Conference |conference-url=http://ubconferences.org/ |location=[[University of Bridgeport]], [[Bridgeport, Connecticut]], US }}&lt;/ref&gt;

In March 2012, The White House announced a national "Big Data Initiative" that consisted of six Federal departments and agencies committing more than $200&amp;nbsp;million to big data research projects.&lt;ref&gt;{{cite web |title=Obama Administration Unveils "Big Data" Initiative:Announces $200 Million In New R&amp;D Investments|publisher=The White House |url=http://www.whitehouse.gov/sites/default/files/microsites/ostp/big_data_press_release_final_2.pdf}}&lt;/ref&gt;

The initiative included a National Science Foundation "Expeditions in Computing" grant of $10 million over 5 years to the AMPLab&lt;ref&gt;{{cite web|url=http://amplab.cs.berkeley.edu |title=AMPLab at the University of California, Berkeley |publisher=Amplab.cs.berkeley.edu |accessdate=2013-03-05}}&lt;/ref&gt; at the University of California, Berkeley.&lt;ref&gt;{{cite web |title=NSF Leads Federal Efforts in Big Data|date=29 March 2012|publisher=National Science Foundation (NSF) |url=http://www.nsf.gov/news/news_summ.jsp?cntn_id=123607&amp;org=NSF&amp;from=news}}&lt;/ref&gt; The AMPLab also received funds from [[DARPA]], and over a dozen industrial sponsors and uses big data to attack a wide range of problems from predicting traffic congestion&lt;ref&gt;{{cite conference|url=https://amplab.cs.berkeley.edu/publication/scaling-the-mobile-millennium-system-in-the-cloud-2/|author1=Timothy Hunter|date=October 2011|author2=Teodor Moldovan|author3=Matei Zaharia|author4=Justin Ma|author5=Michael Franklin|author6=Pieter Abbeel|author7=Alexandre Bayen|title=Scaling the Mobile Millennium System in the Cloud}}&lt;/ref&gt; to fighting cancer.&lt;ref&gt;{{cite news|title=Computer Scientists May Have What It Takes to Help Cure Cancer|author=David Patterson|publisher=The New York Times|date=5 December 2011|url=http://www.nytimes.com/2011/12/06/science/david-patterson-enlist-computer-scientists-in-cancer-fight.html?_r=0}}&lt;/ref&gt;

The White House Big Data Initiative also included a commitment by the  Department of Energy to provide $25 million in funding over 5 years to establish the Scalable Data Management, Analysis and Visualization (SDAV) Institute,&lt;ref&gt;{{cite web|title=Secretary Chu Announces New Institute to Help Scientists Improve Massive Data Set Research on DOE Supercomputers |publisher="energy.gov" |url=http://energy.gov/articles/secretary-chu-announces-new-institute-help-scientists-improve-massive-data-set-research-doe}}&lt;/ref&gt; led by the Energy Department’s [[Lawrence Berkeley National Laboratory]]. The SDAV Institute aims to bring together the expertise of six national laboratories and seven universities to develop new tools to help scientists manage and visualize data on the Department's supercomputers.

The U.S. state of [[Massachusetts]] announced the Massachusetts Big Data Initiative in May 2012, which provides funding from the state government and private companies to a variety of research institutions.&lt;ref&gt;{{cite web |title=Governor Patrick announces new initiative to strengthen Massachusetts' position as a World leader in Big Data |publisher=Commonwealth of Massachusetts |url=http://www.mass.gov/governor/pressoffice/pressreleases/2012/2012530-governor-announces-big-data-initiative.html}}&lt;/ref&gt;  The [[Massachusetts Institute of Technology]] hosts the Intel Science and Technology Center for Big Data in the [[MIT Computer Science and Artificial Intelligence Laboratory]], combining government, corporate, and institutional funding and research efforts.&lt;ref&gt;{{cite web|url=http://bigdata.csail.mit.edu/ |title=Big Data @ CSAIL |publisher=Bigdata.csail.mit.edu |date=22 February 2013 |accessdate=2013-03-05}}&lt;/ref&gt;

The European Commission is funding the 2-year-long Big Data Public Private Forum through their [[Seventh Framework Program]] to engage companies, academics and other stakeholders in discussing big data issues. The project aims to define a strategy in terms of research and innovation to guide supporting actions from the European Commission in the successful implementation of the big data economy. Outcomes of this project will be used as input for [[Horizon 2020]], their next [[Framework Programmes for Research and Technological Development|framework program]].&lt;ref&gt;{{cite web|url=http://cordis.europa.eu/search/index.cfm?fuseaction=proj.document&amp;PJ_RCN=13267529 |title=Big Data Public Private Forum |publisher=Cordis.europa.eu |date=1 September 2012 |accessdate=2013-03-05}}&lt;/ref&gt;

The British government announced in March 2014 the founding of the [[Alan Turing Institute]], named after the computer pioneer and code-breaker, which will focus on new ways to collect and analyse large data sets.&lt;ref&gt;{{cite news|url=http://www.bbc.co.uk/news/technology-26651179|title=Alan Turing Institute to be set up to research big data|publisher=[[BBC News]]|accessdate=2014-03-19|date=19 March 2014}}&lt;/ref&gt;

At the [[University of Waterloo Stratford Campus]] Canadian Open Data Experience (CODE) Inspiration Day, participants demonstrated how using data visualization can increase the understanding and appeal of big data sets and communicate their story to the world.&lt;ref&gt;{{cite web|url=http://www.betakit.com/event/inspiration-day-at-university-of-waterloo-stratford-campus/|title=Inspiration day at University of Waterloo, Stratford Campus |publisher=betakit.com/|accessdate=2014-02-28}}&lt;/ref&gt;

To make manufacturing more competitive in the United States (and globe), there is a need to integrate more American ingenuity and innovation into manufacturing ; Therefore, National Science Foundation has granted the Industry University cooperative research center for Intelligent Maintenance Systems (IMS) at [[university of Cincinnati]] to focus on developing advanced predictive tools and techniques to be applicable in a big data environment.&lt;ref&gt;{{cite journal|last=Lee|first=Jay|author2=Lapira, Edzel |author3=Bagheri, Behrad |author4= Kao, Hung-An |title=Recent Advances and Trends in Predictive Manufacturing Systems in Big Data Environment|journal=Manufacturing Letters|year=2013|volume=1|issue=1|url=http://www.sciencedirect.com/science/article/pii/S2213846313000114|DOI=10.1016/j.mfglet.2013.09.005 |pages=38–41}}&lt;/ref&gt; In May 2013, IMS Center held an industry advisory board meeting focusing on big data where presenters from various industrial companies discussed their concerns, issues and future goals in big data environment.

Computational social sciences&amp;nbsp;– Anyone can use Application Programming Interfaces (APIs) provided by big data holders, such as Google and Twitter, to do research in the social and behavioral sciences.&lt;ref name=pigdata&gt;{{cite journal|last=Reips|first=Ulf-Dietrich|author2=Matzat, Uwe |title=Mining "Big Data" using Big Data Services |journal=International Journal of Internet Science|year=2014|volume=1|issue=1|pages=1–8 | url=http://www.ijis.net/ijis9_1/ijis9_1_editorial_pre.html}}&lt;/ref&gt; Often these APIs are provided for free.&lt;ref name="pigdata" /&gt; [[Tobias Preis]] ''et al.'' used [[Google Trends]] data to demonstrate that Internet users from countries with a higher per capita gross domestic product (GDP) are more likely to search for information about the future than information about the past. The findings suggest there may be a link between online behaviour and real-world economic indicators.&lt;ref&gt;{{cite journal |first1=Tobias |last1=Preis |first2=Helen Susannah |last2=Moat, |first3=H. Eugene |last3=Stanley |first4=Steven R. |last4=Bishop |title=Quantifying the Advantage of Looking Forward |journal=Scientific Reports |volume= 2 |page=350 |year=2012 |doi=10.1038/srep00350 |pmid=22482034 |pmc=3320057}}&lt;/ref&gt;&lt;ref&gt;{{cite web | url=http://www.newscientist.com/article/dn21678-online-searches-for-future-linked-to-economic-success.html | title=Online searches for future linked to economic success |first=Paul |last=Marks |work=New Scientist | date=5 April 2012 | accessdate=9 April 2012}}&lt;/ref&gt;&lt;ref&gt;{{cite web | url=http://arstechnica.com/gadgets/news/2012/04/google-trends-reveals-clues-about-the-mentality-of-richer-nations.ars | title=Google Trends reveals clues about the mentality of richer nations |first=Casey |last=Johnston |work=Ars Technica | date=6 April 2012 | accessdate=9 April 2012}}&lt;/ref&gt; The authors of the study examined Google queries logs made by ratio of the volume of searches for the coming year ('2011') to the volume of searches for the previous year ('2009'), which they call the '[[future orientation index]]'.&lt;ref&gt;{{cite web | url = http://www.tobiaspreis.de/bigdata/future_orientation_index.pdf | title = Supplementary Information: The Future Orientation Index is available for download | author = Tobias Preis | date = 24 May 2012 | accessdate = 2012-05-24}}&lt;/ref&gt; They compared the future orientation index to the per capita GDP of each country, and found a strong tendency for countries where Google users inquire more about the future to have a higher GDP. The results hint that there may potentially be a relationship between the economic success of a country and the information-seeking behavior of its citizens captured in big data.

[[Tobias Preis]] and his colleagues [[Helen Susannah Moat]] and [[H. Eugene Stanley]] introduced a method to identify online precursors for stock market moves, using trading strategies based on search volume data provided by Google Trends.&lt;ref&gt;{{cite web | url=http://www.nature.com/news/counting-google-searches-predicts-market-movements-1.12879 | title=Counting Google searches predicts market movements | author=[[Philip Ball]] | work=Nature | date=26 April 2013 | accessdate=9 August 2013}}&lt;/ref&gt; Their analysis of [[Google]] search volume for 98 terms of varying financial relevance, published in ''[[Scientific Reports]]'',&lt;ref&gt;{{cite journal | author=Tobias Preis, Helen Susannah Moat and H. Eugene Stanley | title=Quantifying Trading Behavior in Financial Markets Using Google Trends | journal=[[Scientific Reports]] | volume= 3 | pages=1684 | year=2013 | doi=10.1038/srep01684 | pmid=23619126 | pmc=3635219}}&lt;/ref&gt; suggests that increases in search volume for financially relevant search terms tend to precede large losses in financial markets.&lt;ref&gt;{{cite news | url=http://bits.blogs.nytimes.com/2013/04/26/google-search-terms-can-predict-stock-market-study-finds/ | title= Google Search Terms Can Predict Stock Market, Study Finds | author=Nick Bilton | work=[[New York Times]] | date=26 April 2013 | accessdate=9 August 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite news | url=http://business.time.com/2013/04/26/trouble-with-your-investment-portfolio-google-it/ | title=Trouble With Your Investment Portfolio? Google It! | author=Christopher Matthews | work=[[TIME Magazine]] | date=26 April 2013 | accessdate=9 August 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web | url= http://www.nature.com/news/counting-google-searches-predicts-market-movements-1.12879 | title=Counting Google searches predicts market movements | author=Philip Ball |work=[[Nature (journal)|Nature]] | date=26 April 2013 | accessdate=9 August 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web | url=http://www.businessweek.com/articles/2013-04-25/big-data-researchers-turn-to-google-to-beat-the-markets | title='Big Data' Researchers Turn to Google to Beat the Markets | author=Bernhard Warner | work=[[Bloomberg Businessweek]] | date=25 April 2013 | accessdate=9 August 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite news | url=http://www.independent.co.uk/news/business/comment/hamish-mcrae/hamish-mcrae-need-a-valuable-handle-on-investor-sentiment-google-it-8590991.html | title=Hamish McRae: Need a valuable handle on investor sentiment? Google it | author=Hamish McRae | work=[[The Independent]] | date=28 April 2013 | accessdate=9 August 2013 | location=London}}&lt;/ref&gt;&lt;ref&gt;{{cite web | url=http://www.ft.com/intl/cms/s/0/e5d959b8-acf2-11e2-b27f-00144feabdc0.html | title= Google search proves to be new word in stock market prediction | author=Richard Waters | work=[[Financial Times]] | date=25 April 2013 | accessdate=9 August 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite news | url=http://www.forbes.com/sites/davidleinweber/2013/04/26/big-data-gets-bigger-now-google-trends-can-predict-the-market/ | title=Big Data Gets Bigger: Now Google Trends Can Predict The Market | author=David Leinweber | work=[[Forbes]] | date=26 April 2013 | accessdate=9 August 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite news | url=http://www.bbc.co.uk/news/science-environment-22293693 | title=Google searches predict market moves | author=Jason Palmer | work=[[BBC]] | date=25 April 2013 | accessdate=9 August 2013}}&lt;/ref&gt;

Big data sets come with algorithmic challenges that previously did not exist. Hence, there is a need to fundamentally change the processing ways.&lt;ref&gt;E. Sejdić, "Adapt current tools for use with big data," ''Nature,'' vol. vol. 507, no. 7492, pp. 306, Mar. 2014.&lt;/ref&gt;

The Workshops on Algorithms for Modern Massive Data Sets (MMDS) bring together computer scientists, statisticians, mathematicians, and data analysis practitioners to discuss algorithmic challenges of big data.&lt;ref&gt;
Stanford.
[http://web.stanford.edu/group/mmds/ "MMDS. Workshop on Algorithms for Modern Massive Data Sets"].
&lt;/ref&gt;

=== Sampling big data ===
An important research question that can be asked about big data sets is whether you need to look at the full data to draw certain conclusions about the properties of the data or is a sample good enough. The name big data itself contains a term related to size and this is an important characteristic of big data. But [[Sampling (statistics)]] enables the selection of right data points from within the larger data set to estimate the characteristics of the whole population. For example, there are about 600 million tweets produced every day. Is it necessary to look at all of them to determine the topics that are discussed during the day? Is it necessary to look at all the tweets to determine the sentiment on each of the topics? In manufacturing different types of sensory data such as acoustics, vibration, pressure, current, voltage and controller data are available at short time intervals. To predict down-time it may not be necessary to look at all the data but a sample may be sufficient.  Big Data can be broken down by various data point categories such as demographic, psychographic, behavioral, and transactional data.  With large sets of data points, marketers are able to create and utilize more customized segments of consumers for more strategic targeting.

There has been some work done in Sampling algorithms for big data. A theoretical formulation for sampling Twitter data has been developed.&lt;ref&gt;{{cite conference |author1=Deepan Palguna |author2=Vikas Joshi |author3=Venkatesan Chakaravarthy |author4=Ravi Kothari |author5=L. V. Subramaniam |last-author-amp=yes | title=Analysis of Sampling Algorithms for Twitter | journal=[[International Joint Conference on Artificial Intelligence]] | year=2015 }}&lt;/ref&gt;

== Critique ==
Critiques of the big data paradigm come in two flavors, those that question the implications of the approach itself, and those that question the way it is currently done.&lt;ref&gt;{{cite journal | doi = 10.1002/joe.21642 | title = Big Data and Business Intelligence: Debunking the Myths | journal = Global Business and Organizational Excellence| volume = 35 | issue = 1 | pages = 23–34 | year = 2015 | last1 = Kimble | first1 = C. | last2 = Milolidakis | first2 = G. }}&lt;/ref&gt; One approach to this criticism is the field of [[Critical data studies]].

=== Critiques of the big data paradigm ===
"A crucial problem is that we do not know much about the underlying empirical micro-processes that lead to the emergence of the[se] typical network characteristics of Big Data".&lt;ref name="Editorial" /&gt; In their critique, Snijders, Matzat, and [[Ulf-Dietrich Reips|Reips]] point out that often very strong assumptions are made about mathematical properties that may not at all reflect what is really going on at the level of micro-processes. Mark Graham has leveled broad critiques at [[Chris Anderson (writer)|Chris Anderson]]'s assertion that big data will spell the end of theory:&lt;ref&gt;{{cite web|url=http://www.wired.com/science/discoveries/magazine/16-07/pb_theory|title=The End of Theory: The Data Deluge Makes the Scientific Method Obsolete|author=Chris Anderson|date=23 June 2008|work=WIRED}}&lt;/ref&gt; focusing in particular on the notion that big data must always be contextualized in their social, economic, and political contexts.&lt;ref&gt;{{cite news |author=Graham M. |title=Big data and the end of theory? |newspaper=The Guardian |url=https://www.theguardian.com/news/datablog/2012/mar/09/big-data-theory |location=London |date=9 March 2012}}&lt;/ref&gt; Even as companies invest eight- and nine-figure sums to derive insight from information streaming in from suppliers and customers, less than 40% of employees have sufficiently mature processes and skills to do so. To overcome this insight deficit, big data, no matter how comprehensive or well analysed, must be complemented by "big judgment," according to an article in the Harvard Business Review.&lt;ref&gt;{{cite web|title=Good Data Won't Guarantee Good Decisions. Harvard Business Review|url=http://hbr.org/2012/04/good-data-wont-guarantee-good-decisions/ar/1|work=Shah, Shvetank; Horne, Andrew; Capellá, Jaime;|publisher=HBR.org|accessdate=8 September 2012}}&lt;/ref&gt;

Much in the same line, it has been pointed out that the decisions based on the analysis of big data are inevitably "informed by the world as it was in the past, or, at best, as it currently is".&lt;ref name="HilbertBigData2013"&gt;{{cite web|url=http://papers.ssrn.com/abstract=2205145|title=Big Data for Development: From Information- to Knowledge Societies|publisher=}}&lt;/ref&gt;  Fed by a large number of data on past experiences, algorithms can predict future development if the future is similar to the past.&lt;ref name="HilbertTEDx"&gt;[https://www.youtube.com/watch?v=UXef6yfJZAI Big Data requires Big Visions for Big Change.], Hilbert, M. (2014). London: TEDxUCL, x=independently organized TED talks&lt;/ref&gt; If the systems dynamics of the future change (if it is not a [[stationary process]]), the past can say little about the future. In order to make predictions in changing environments, it would be necessary to have a thorough understanding of the systems dynamic, which requires theory.&lt;ref name="HilbertTEDx"/&gt;  As a response to this critique it has been suggested to combine big data approaches with computer simulations, such as [[agent-based model]]s&lt;ref name="HilbertBigData2013" /&gt; and [[Complex Systems]]. Agent-based models are increasingly getting better in predicting the outcome of social complexities of even unknown future scenarios through computer simulations that are based on a collection of mutually interdependent algorithms.&lt;ref&gt;{{cite web|url=http://www.theatlantic.com/magazine/archive/2002/04/seeing-around-corners/302471/|title=Seeing Around Corners|author=Jonathan Rauch|date=1 April 2002|work=The Atlantic}}&lt;/ref&gt;&lt;ref&gt;Epstein, J. M., &amp; Axtell, R. L. (1996). Growing Artificial Societies: Social Science from the Bottom Up. A Bradford Book.&lt;/ref&gt; In addition, use of multivariate methods that probe for the latent structure of the data, such as [[factor analysis]] and [[cluster analysis]], have proven useful as analytic approaches that go well beyond the bi-variate approaches (cross-tabs) typically employed with smaller data sets.

In health and biology, conventional scientific approaches are based on experimentation. For these approaches, the limiting factor is the relevant data that can confirm or refute the initial hypothesis.&lt;ref&gt;[http://www.bigdataparis.com/documents/Pierre-Delort-INSERM.pdf#page=5 Delort P., Big data in Biosciences, Big Data Paris, 2012]&lt;/ref&gt;
A new postulate is accepted now in biosciences: the information provided by the data in huge volumes ([[omics]]) without prior hypothesis is complementary and sometimes necessary to conventional approaches based on experimentation.&lt;ref&gt;{{cite web|url=http://www.cs.cmu.edu/~durand/03-711/2011/Literature/Next-Gen-Genomics-NRG-2010.pdf|title=Next-generation genomics: an integrative approach|date=July 2010|publisher=nature|accessdate=18 October 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://www.researchgate.net/publication/283298499_BIG_DATA_IN_BIOSCIENCES|title=BIG DATA IN BIOSCIENCES|date=October 2015|publisher=ResearchGate|accessdate=18 October 2016}}&lt;/ref&gt; In the massive approaches it is the formulation of a relevant hypothesis to explain the data that is the limiting factor.&lt;ref&gt;{{cite web|url=https://next.ft.com/content/21a6e7d8-b479-11e3-a09a-00144feabdc0|title=Big data: are we making a big mistake?|date=28 March 2014|publisher=Financial Times|accessdate=20 October 2016}}&lt;/ref&gt; The search logic is reversed and the limits of induction ("Glory of Science and Philosophy scandal", [[C. D. Broad]], 1926) are to be considered.{{Citation needed|date=April 2015}}

[[Consumer privacy|Privacy]] advocates are concerned about the threat to privacy represented by increasing storage and integration of [[personally identifiable information]]; expert panels have released various policy recommendations to conform practice to expectations of privacy.&lt;ref&gt;{{cite web |first=Paul |last=Ohm |title=Don't Build a Database of Ruin |publisher=Harvard Business Review |url=http://blogs.hbr.org/cs/2012/08/dont_build_a_database_of_ruin.html}}&lt;/ref&gt;&lt;ref&gt;Darwin Bond-Graham, ''[http://www.counterpunch.org/2013/12/03/iron-cagebook/ Iron Cagebook – The Logical End of Facebook's Patents],'' [[Counterpunch.org]], 2013.12.03&lt;/ref&gt;&lt;ref&gt;Darwin Bond-Graham, ''[http://www.counterpunch.org/2013/09/11/inside-the-tech-industrys-startup-conference/  Inside the Tech industry’s Startup Conference],'' [[Counterpunch.org]], 2013.09.11&lt;/ref&gt;

=== Critiques of big data execution ===
Big data has been called a "fad" in scientific research and its use was even made fun of as an absurd practice in a satirical example on "pig data".&lt;ref name="pigdata" /&gt; Researcher [[Danah Boyd]] has raised concerns about the use of big data in science neglecting principles such as choosing a [[Sampling (statistics)|representative sample]] by being too concerned about actually handling the huge amounts of data.&lt;ref name="danah"&gt;{{cite web | url=http://www.danah.org/papers/talks/2010/WWW2010.html | title=Privacy and Publicity in the Context of Big Data | author=[[danah boyd]] | work=[[World Wide Web Conference|WWW 2010 conference]] | date=29 April 2010 | accessdate = 2011-04-18}}&lt;/ref&gt; This approach may lead to results [[Bias (statistics)|bias]] in one way or another. Integration across heterogeneous data resources—some that might be considered big data and others not—presents formidable logistical as well as analytical challenges, but many researchers argue that such integrations are likely to represent the most promising new frontiers in science.&lt;ref&gt;{{cite journal |last1=Jones |first1=MB |last2=Schildhauer |first2=MP |last3=Reichman |first3=OJ |last4=Bowers |first4=S |title=The New Bioinformatics: Integrating Ecological Data from the Gene to the Biosphere |journal=Annual Review of Ecology, Evolution, and Systematics |volume=37 |issue=1 |pages=519–544 |year=2006 |doi=10.1146/annurev.ecolsys.37.091305.110031 |url=http://www.pnamp.org/sites/default/files/Jones2006_AREES.pdf |format=PDF}}&lt;/ref&gt;
In the provocative article "Critical Questions for Big Data",&lt;ref name="danah2"&gt;{{cite journal | doi = 10.1080/1369118X.2012.678878| title = Critical Questions for Big Data| journal = Information, Communication &amp; Society| volume = 15| issue = 5| pages = 662–679| year = 2012| last1 = Boyd | first1 = D. | last2 = Crawford | first2 = K. }}&lt;/ref&gt; the authors title big data a part of [[mythology]]: "large data sets offer a higher form of intelligence and knowledge [...], with the aura of truth, objectivity, and accuracy". Users of big data are often "lost in the sheer volume of numbers", and "working with Big Data is still subjective, and what it quantifies does not necessarily have a closer claim on objective truth".&lt;ref name="danah2" /&gt; Recent developments in BI domain, such as pro-active reporting especially target improvements in usability of big data, through automated [[Filter (software)|filtering]] of non-useful data and correlations.&lt;ref name="Big Decisions White Paper"&gt;[http://www.fortewares.com/Administrator/userfiles/Banner/forte-wares--pro-active-reporting_EN.pdf Failure to Launch: From Big Data to Big Decisions], Forte Wares.&lt;/ref&gt;

Big data analysis is often shallow compared to analysis of smaller data sets.&lt;ref name="kdnuggets-berchthold"&gt;{{cite web|url=http://www.kdnuggets.com/2014/08/interview-michael-berthold-knime-research-big-data-privacy-part2.html|title=Interview: Michael Berthold, KNIME Founder, on Research, Creativity, Big Data, and Privacy, Part 2|date=12 August 2014|author=Gregory Piatetsky|authorlink=Gregory I. Piatetsky-Shapiro|publisher=KDnuggets|accessdate=2014-08-13}}&lt;/ref&gt; In many big data projects, there is no large data analysis happening, but the challenge is the [[extract, transform, load]] part of data preprocessing.&lt;ref name="kdnuggets-berchthold" /&gt;

Big data is a [[buzzword]] and a "vague term",&lt;ref&gt;{{cite web|last1=Pelt|first1=Mason|title="Big Data" is an over used buzzword and this Twitter bot proves it|url=http://siliconangle.com/blog/2015/10/26/big-data-is-an-over-used-buzzword-and-this-twitter-bot-proves-it/|website=siliconangle.com|publisher=SiliconANGLE|accessdate=4 November 2015}}&lt;/ref&gt;&lt;ref name="ft-harford"&gt;{{cite web |url=http://www.ft.com/cms/s/2/21a6e7d8-b479-11e3-a09a-00144feabdc0.html |title=Big data: are we making a big mistake? |last1=Harford |first1=Tim |date=28 March 2014 |website=[[Financial Times]] |publisher=[[Financial Times]] |accessdate=2014-04-07}}&lt;/ref&gt; but at the same time an "obsession"&lt;ref name="ft-harford" /&gt; with entrepreneurs, consultants, scientists and the media. Big data showcases such as [[Google Flu Trends]] failed to deliver good predictions in recent years, overstating the flu outbreaks by a factor of two. Similarly, [[Academy awards]] and election predictions solely based on Twitter were more often off than on target.
Big data often poses the same challenges as small data; and adding more data does not solve problems of bias, but may emphasize other problems. In particular data sources such as Twitter are not representative of the overall population, and results drawn from such sources may then lead to wrong conclusions. [[Google Translate]]—which is based on big data statistical analysis of text—does a good job at translating web pages. However, results from specialized domains may be dramatically skewed.
On the other hand, big data may also introduce new problems, such as the [[multiple comparisons problem]]: simultaneously testing a large set of hypotheses is likely to produce many false results that mistakenly appear significant.
Ioannidis argued that "most published research findings are false"&lt;ref name="Ioannidis"&gt;{{cite journal | last1 = Ioannidis | first1 = J. P. A. | authorlink1 = John P. A. Ioannidis| title = Why Most Published Research Findings Are False | journal = PLoS Medicine | volume = 2 | issue = 8 | pages = e124 | year = 2005 | pmid = 16060722 | pmc = 1182327 | doi = 10.1371/journal.pmed.0020124}}&lt;/ref&gt; due to essentially the same effect: when many scientific teams and researchers each perform many experiments (i.e. process a big amount of scientific data; although not with big data technology), the likelihood of a "significant" result being actually false grows fast – even more so, when only positive results are published.
&lt;!-- sorry, this started overlapping with above section more and more... merging is welcome; I already dropped the intended subheadline "Hype cycle and inflated expectations". --&gt;
Furthermore, big data analytics results are only as good as the model on which they are predicated.  In an example, big data took part in attempting to predict the results of the 2016 U.S. Presidential Election&lt;ref&gt;{{Cite news|url=http://www.nytimes.com/2016/11/10/technology/the-data-said-clinton-would-win-why-you-shouldnt-have-believed-it.html|title=How Data Failed Us in Calling an Election|last=Lohr|first=Steve|date=2016-11-10|last2=Singer|first2=Natasha|newspaper=The New York Times|issn=0362-4331|access-date=2016-11-27}}&lt;/ref&gt; with varying degrees of success.  Forbes predicted "If you believe in ''Big Data'' analytics, it’s time to begin planning for a Hillary Clinton presidency and all that entails.".&lt;ref&gt;{{Cite news|url=http://www.forbes.com/sites/jonmarkman/2016/08/08/big-data-and-the-2016-election/#4802f20846d7|title=Big Data And The 2016 Election|last=Markman|first=Jon|newspaper=Forbes|access-date=2016-11-27}}&lt;/ref&gt;

== See also ==
{{portal|Information technology}}
{{Category see also|LABEL=For a list of companies, and tools, see also|Big data}}
&lt;!-- NO COMPANIES OR TOOL SPAM HERE. That would be an endless list! "See also" concepts, not linked above. --&gt;
* [[Big memory]]
* [[Datafication]]
* [[Data defined storage]]
* [[Data journalism]]
* [[Data lineage]]
* [[Data philanthropy]]
* [[Data science]]
* [[Machine learning]]
* [[Statistics]]
* [[Small data]]
* [[Urban informatics]]
* [[List of buzzwords]]

== References ==
{{Reflist|30em}}

==Further reading==
*{{cite magazine|editors=Peter Kinnaird, Inbal Talgam-Cohen|series=[[XRDS (magazine)|XRDS: Crossroads, The ACM Magazine for Students]]|title=Big Data|issue=19 (1)|date=2012|publisher=[[Association for Computing Machinery]]|issn=1528-4980 |oclc=779657714 |url=http://dl.acm.org/citation.cfm?id=2331042}}
*{{cite book|title=Mining of massive datasets|author1=[[Jure Leskovec]]|author2=[[Anand Rajaraman]]|author3=[[Jeffrey D. Ullman]]|year=2014|publisher=Cambridge University Press|url=http://mmds.org/|isbn=9781107077232 |oclc=888463433}}
*{{cite book|author1=[[Viktor Mayer-Schönberger]]|author2=[[Kenneth Cukier]]|title=Big Data: A Revolution that Will Transform how We Live, Work, and Think|date=2013|publisher=Houghton Mifflin Harcourt|isbn=9781299903029 |oclc=828620988}}
*{{cite web |url=http://www.forbes.com/sites/gilpress/2013/05/09/a-very-short-history-of-big-data |title=A Very Short History Of Big Data |first=Gil |last=Press |work=forbes.com |date=2013-05-09 |accessdate=2016-09-17 |publisher=[[Forbes Magazine]] |location=Jersey City, NJ}}

== External links ==
*{{Commonsinline}}
* {{Wiktionary-inline|big data}}

{{Use dmy dates|date=December 2015}}
{{Authority control}}

[[Category:Big data| ]]
[[Category:Data management]]
[[Category:Distributed computing problems]]
[[Category:Technology forecasting]]
[[Category:Transaction processing]]</text>
      <sha1>6tg9nbf2y1isei0drc0xvul3qkvuzko</sha1>
    </revision>
  </page>
  <page>
    <title>Copyright</title>
    <ns>0</ns>
    <id>5278</id>
    <revision>
      <id>762384948</id>
      <parentid>762384891</parentid>
      <timestamp>2017-01-28T13:39:30Z</timestamp>
      <contributor>
        <username>ClueBot NG</username>
        <id>13286072</id>
      </contributor>
      <minor />
      <comment>Reverting possible vandalism by [[Special:Contribs/Souda007|Souda007]] to version by Zzuuzz. [[WP:CBFP|Report False Positive?]] Thanks, [[WP:CBNG|ClueBot NG]]. (2913989) (Bot)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="62606" xml:space="preserve">{{Redirect2|Copyrighting|Copyrights|the use of words to promote or advertise|Copywriting|the Wikipedia policy about copyright issues|Wikipedia:Copyrights}}
{{pp-move-indef|small=yes}}
{{Intellectual property}}
{{Capitalism|Concepts}}
{{Use dmy dates|date=January 2011}}
{{Use American English|date=January 2014}}

'''Copyright''' is a [[Natural and legal rights|legal right]] created by the law of a country that grants the creator of an original work [[exclusive right]]s for its use and distribution. This is usually only for a limited time. The exclusive rights are not absolute but limited by [[limitations and exceptions to copyright]] law, including fair use. A major limitation on copyright is that copyright protects only the original expression of ideas, and not the underlying ideas themselves.&lt;ref&gt;{{cite web|url=http://www.bitlaw.com/copyright/unprotected.html#ideas|title=Works Unprotected by Copyright Law|publisher=Bitlaw|author=Daniel A. Tysver}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://digital-law-online.info/lpdi1.0/treatise9.html |title=Legal Protection of Digital Information |page=''Chapter 1: An Overview of Copyright'', Section II.E. Ideas Versus Expression.|author=Lee A. Hollaar}}&lt;/ref&gt;

Copyright is a form of [[intellectual property]], applicable to certain forms of creative work. Some, but not all jurisdictions require "fixing" copyrighted works in a tangible form. It is often shared among multiple authors, each of whom holds a set of rights to use or license the work, and who are commonly referred to as rights holders.&lt;ref&gt;{{Citation|title=Copyright|publisher=University of California|year=2014|url=http://copyright.universityofcalifornia.edu/ownership/joint-works.html|accessdate=2014-12-15}}&lt;/ref&gt;&lt;ref&gt;http://www.jetlaw.org/publish/journal-conventions/&lt;/ref&gt;&lt;ref&gt;https://books.google.de/books?id=kz1F6uAHtaEC&amp;pg=PA81&amp;dq=%22rights+holder%22&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiG4OnUo87RAhXqBcAKHQgZAD8Q6AEIHDAA#v=onepage&amp;q=%22rights%20holder%22&amp;f=false&lt;/ref&gt;&lt;ref&gt;https://books.google.de/books?id=xD_iBwAAQBAJ&amp;pg=PT465&amp;dq=%22rights+holder%22&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiG4OnUo87RAhXqBcAKHQgZAD8Q6AEIKDAC#v=onepage&amp;q=%22rights%20holder%22&amp;f=false&lt;/ref&gt; These rights frequently include reproduction, control over [[derivative work]]s, distribution, [[Performing rights|public performance]], and "[[moral rights]]" such as attribution.&lt;ref&gt;{{Citation|title=17 U.S.C. § 106|publisher=United States of America|year=2011|url=http://www.copyright.gov/title17/92chap1.html#106|accessdate=2014-12-15}}&lt;/ref&gt;

Copyrights are considered ''territorial rights'', which means that they do not extend beyond the territory of a specific jurisdiction.  While many aspects of national copyright laws have been standardized through [[international copyright agreements]], copyright laws vary by country.&lt;ref name="International Copyright Law Survey"&gt;{{cite web|url=http://worldcopyrightlaw.com/copyrightsurvey|title=International Copyright Law Survey|publisher=Mincov Law Corporation}}&lt;/ref&gt;

Typically, the [[Copyright term|duration of a copyright]] spans the author's life plus 50 to 100 years (that is, copyright typically expires 50 to 100 years after the author dies, depending on the jurisdiction).  Some countries require certain copyright [[copyright formalities|formalities]] to establishing copyright, but most recognize copyright in any completed work, without formal registration. Generally, copyright is enforced as a [[civil law (common law)|civil]] matter, though some jurisdictions do apply [[criminal law|criminal]] sanctions.

Most jurisdictions recognize copyright limitations, allowing "fair" exceptions to the creator's exclusivity of copyright and giving users certain rights. The development of digital media and computer network technologies have prompted reinterpretation of these exceptions, introduced new difficulties in enforcing copyright, and inspired additional challenges to copyright law's philosophic basis. Simultaneously, businesses with great economic dependence upon copyright, such as those in the music business, have advocated the extension and expansion of copyright and sought additional legal and technological enforcement.

==History==
{{main|History of copyright law}}
===Background===
Copyright came about with the invention of [[Printing press|the printing press]] and with wider literacy. As a legal concept, its origins in Britain were from a reaction to printers' monopolies at the beginning of the 18th&amp;nbsp;century. [[Charles II of England]] was concerned by the unregulated [[copying]] of books and passed the [[Licensing of the Press Act 1662]] by Act of Parliament,&lt;ref&gt;''Copyright in Historical Perspective'', p. 136-137, Patterson, 1968, Vanderbilt Univ. Press&lt;/ref&gt; which established a register of licensed books and required a copy to be deposited with the [[Worshipful Company of Stationers and Newspaper Makers|Stationers' Company]], essentially continuing the licensing of material that had long been in effect.

Copyright laws allow products of creative human activities, such as literary and artistic production, to be preferentially exploited and thus incentivized. Different cultural attitudes, social organizations, economic models and legal frameworks are seen to account for why copyright emerged in [[Europe]] and not, for example, in Asia. In the [[Middle Ages]] in Europe, there was generally a lack of any concept of literary property due to the general relations of production, the specific organization of literary production and the role of culture in society. The latter refers to the tendency of oral societies, such as that of Europe in the medieval period, to view knowledge as the product and expression of the collective, rather than to see it as individual property. However, with copyright laws, intellectual production comes to be seen as a product of an individual, with attendant rights. The most significant point is that patent and copyright laws support the expansion of the range of creative human activities that can be commodified. This parallels the ways in which [[capitalism]] led to the [[commodification]] of many aspects of social life that earlier had no monetary or economic value per&amp;nbsp;se.&lt;ref&gt;Bettig, Ronald V. (1996). ''Copyrighting Culture: The Political Economy of Intellectual Property. Westview Press''. p. 9–17. ISBN 0-8133-1385-6.&lt;/ref&gt;

Copyright has grown from a legal concept regulating copying rights in the publishing of books and maps to one with a significant effect on nearly every modern industry, covering such items as [[Sound recording and reproduction|sound recordings]], films, photographs, software, and architectural works.

===National copyrights===
{{See also|Statute of Anne|History of US Copyright Law}}
[[File:Statute of anne.jpg|thumb|left|The [[Statute of Anne]] (the Copyright Act 1709) came into force in 1710.]]
Often seen as the first real copyright law, the 1709 British [[Statute of Anne]] gave the publishers rights for a fixed period, after which the copyright expired.&lt;ref name="Rethinking copyright: history, theory, language"&gt;{{cite book|url=https://www.google.com/books?id=dMYXq9V1JBQC&amp;dq=statute+of+anne+copyright&amp;lr=&amp;as_brr=3&amp;source=gbs_navlinks_s |title=Rethinking copyright: history, theory, language |page=13 |author=Ronan, Deazley |isbn=978-1-84542-282-0 |year=2006 |publisher=Edward Elgar Publishing. |deadurl=yes |archiveurl=https://web.archive.org/web/20111119042246/https://www.google.com/books?id=dMYXq9V1JBQC&amp;dq=statute+of+anne+copyright&amp;lr=&amp;as_brr=3&amp;source=gbs_navlinks_s |archivedate=19 November 2011 }}&lt;/ref&gt;
The act also alluded to individual rights of the artist. It began, "Whereas Printers, Booksellers, and other Persons, have of late frequently taken the Liberty of Printing... Books, and other Writings, without the Consent of the Authors... to their very great Detriment, and too often to the Ruin of them and their Families:".&lt;ref&gt;{{cite web|url=http://www.copyrighthistory.com/anne.html |title=Statute of Anne |publisher=Copyrighthistory.com |accessdate=2012-06-08}}&lt;/ref&gt; A right to benefit financially from the work is articulated, and court rulings and legislation have recognized a right to control the work, such as ensuring that the integrity of it is preserved. An irrevocable right to be recognized as the work's creator appears in some countries' copyright laws.

The [[Copyright Clause]] of the United States Constitution (1787) authorized copyright legislation: "To promote the Progress of Science and useful Arts, by securing for limited Times to Authors and Inventors the exclusive Right to their respective Writings and Discoveries." That is, by guaranteeing them a period of time in which they alone could profit from their works, they would be enabled and encouraged to invest the time required to create them, and this would be good for society as a whole. A right to profit from the work has been the philosophical underpinning for much legislation extending the duration of copyright, to the life of the creator and beyond, to their heirs.

The original length of copyright in the United States was 14&amp;nbsp;years, and it had to be explicitly applied for. If the author wished, they could apply for a second 14‑year monopoly grant, but after that the work entered the [[public domain]], so it could be used and built upon by others.

Copyright law was enacted rather [[Copyright in Germany|late in German states]], and the historian Eckhard Höffner argues that the absence of copyright laws in the early 19th century encouraged publishing, was profitable for authors, led to a proliferation of books, enhanced knowledge, and was ultimately an important factor in the ascendency of Germany as a power during that century.&lt;ref name="thad"&gt;{{cite web |url=http://www.spiegel.de/international/zeitgeist/0,1518,710976,00.html | author=Frank Thadeusz |title=No Copyright Law: The Real Reason for Germany's Industrial Expansion? |publisher=[[Der Spiegel]] |date= 18 August 2010 |accessdate= 11 April 2015}}&lt;/ref&gt;

===International copyright treaties===
{{See also|International copyright agreements|List of parties to international copyright agreements}}
[[File:Joseph Ferdinand Keppler - The Pirate Publisher - Puck Magazine - Restoration by Adam Cuerden.jpg|'' The Pirate Publisher—An International Burlesque that has the Longest Run on Record'', from ''[[Puck (magazine)|Puck]]'', 1886, satirizes the then-existing situation where a publisher could profit by simply stealing newly published works from one country, and publishing them in another, and vice versa.|thumb|300px]]
The 1886 [[Berne Convention for the Protection of Literary and Artistic Works|Berne Convention]] first established recognition of copyrights among [[Sovereignty|sovereign nations]], rather than merely bilaterally. Under the Berne Convention, copyrights for [[creative works]] do not have to be asserted or declared, as they are automatically in force at creation: an author need not "register" or "apply for" a copyright in countries adhering to the Berne Convention.&lt;ref name="Berne Convention for the Protection of Literary and Artistic Works Article 5"&gt;{{cite web |url=http://www.wipo.int/treaties/en/ip/berne/trtdocs_wo001.html#P109_16834 |title=Berne Convention for the Protection of Literary and Artistic Works Article 5 |accessdate=2011-11-18 |publisher=World Intellectual Property Organization}}&lt;/ref&gt; As soon as a work is "fixed", that is, written or recorded on some physical medium, its author is automatically entitled to all copyrights in the work, and to any derivative works unless and until the author explicitly disclaims them, or until the copyright expires. The Berne Convention also resulted in foreign authors being treated equivalently to domestic authors, in any country signed onto the Convention. The UK signed the Berne Convention in 1887 but did not implement large parts of it until 100&amp;nbsp;years later with the passage of the ''Copyright, Designs and Patents Act of 1988''. The United States did not sign the Berne Convention until 1989.&lt;ref&gt;Garfinkle, Ann M; Fries, Janet; Lopez, Daniel; Possessky, Laura (1997). "Art conservation and the legal obligation to preserve artistic intent". [[JAIC]]   36 (2): 165–179.&lt;/ref&gt;

The United States and most [[Latin America]]n countries instead entered into the [[Buenos Aires Convention]] in 1910, which required a copyright notice on the work (such as ''[[all rights reserved]]''), and permitted signatory nations to limit the duration of copyrights to shorter and renewable terms.&lt;ref&gt;[http://www.copyright.gov/circs/circ38a.pdf "International Copyright Relations of the United States"], U.S.&amp;nbsp;Copyright Office Circular No.&amp;nbsp;38a, August&amp;nbsp;2003.&lt;/ref&gt;&lt;ref&gt;[http://www.unesco.org/culture/copyright/html_eng/ucc52ms.pdf Parties to the Geneva Act of the Universal Copyright Convention] as of 2000-01-01: the dates given in the document are dates of ratification, not dates of coming into force. The Geneva Act came into force on 16 September 1955, for the first twelve to have ratified (which included four non-members of the Berne Union as required by Art.&amp;nbsp;9.1), or three months after ratification for other countries. {{webarchive |url=https://web.archive.org/web/20080625003242/http://www.unesco.org/culture/copyright/html_eng/ucc52ms.pdf |date=25 June 2008 }}&lt;/ref&gt;&lt;ref&gt;[http://www.copyright.ht/en 165&amp;nbsp;Parties to the Berne Convention for the Protection of Literary and Artistic Works] as of May 2012.&lt;/ref&gt; The [[Universal Copyright Convention]] was drafted in 1952 as another less demanding alternative to the Berne Convention, and ratified by nations such as the [[Soviet Union]] and developing nations.

The regulations of the [[Berne Convention for the Protection of Literary and Artistic Works|Berne Convention]] are incorporated into the [[World Trade Organization]]'s [[Agreement on Trade-Related Aspects of Intellectual Property Rights|TRIPS]] agreement (1995), thus giving the Berne Convention effectively near-global application.&lt;ref name="Contemporary Intellectual Property: Law and Policy"&gt;{{cite book |title=Contemporary Intellectual Property: Law and Policy|url= https://www.google.com/books?id=_Iwcn4pT0OoC&amp;dq=contemporary+intellectual+property&amp;source=gbs_navlinks_s |page=39 |author1=MacQueen, Hector L |author2=Charlotte Waelde |author3=Graeme T Laurie |isbn=978-0-19-926339-4 |year=2007 |publisher=Oxford University Press}}&lt;/ref&gt; 

In 1961, the [[United International Bureaux for the Protection of Intellectual Property]] signed the [[Rome Convention for the Protection of Performers, Producers of Phonograms and Broadcasting Organizations]]. In 1996, this organization was succeeded by the founding of the [[World Intellectual Property Organization]], which launched the 1996 [[WIPO Performances and Phonograms Treaty]] and the 2002 [[World Intellectual Property Organization Copyright Treaty|WIPO Copyright Treaty]], which enacted greater restrictions on the use of technology to copy works in the nations that ratified it. The [[Trans-Pacific Partnership]] includes [[Trans-Pacific Partnership Intellectual Property Provisions|intellectual Property Provisions]] relating to copyright.

Copyright laws are standardized somewhat through these international conventions such as the [[Berne Convention for the Protection of Literary and Artistic Works|Berne Convention]] and [[Universal Copyright Convention]]. These multilateral treaties have been ratified by nearly all countries, and [[international organizations]] such as the [[European Union]] or [[World Trade Organization]] require their member states to comply with them.

==Obtaining protection==
===Ownership===
The original holder of the copyright may be the employer of the author rather than the author himself, if the work is a "[[work for hire]]".&lt;ref&gt;17 U.S.C. § 201(b); Cmty. for Creative Non-Violence v. Reid, 490 U.S. 730 (1989)&lt;/ref&gt; For example, in [[English law]] the ''Copyright, Designs and Patents Act'' 1988 provides that if a copyrighted work is made by an employee in the course of that employment, the copyright is automatically owned by the employer which would be a "Work for Hire."

===Eligible works===
Copyright may apply to a wide range of creative, intellectual, or artistic forms, or "works". Specifics vary by [[jurisdiction]], but these can include [[poem]]s, [[theses]], [[drama|plays]] and other [[book|literary works]], [[film|motion pictures]], [[choreography]], [[music|musical compositions]], [[sound recording]]s, [[painting]]s, [[drawing]]s, [[sculpture]]s, [[photography|photographs]], [[computer software]], [[radio]] and [[television]] [[Broadcasting|broadcasts]], and [[industrial design]]s. Graphic [[designs]] and industrial designs may have separate or overlapping laws applied to them in some jurisdictions.&lt;ref name="Intellectual Property and Information Wealth: Copyright and related rights"&gt;{{cite book |title=Intellectual Property and Information Wealth: Copyright and related rights|url=https://www.google.com/books?id=tgK9BzcF5WgC&amp;dq=statute+of+anne+copyright&amp;lr=&amp;as_brr=3&amp;source=gbs_navlinks_s|page=346 |author=Peter K, Yu |isbn=978-0-275-98883-8 |year=2007 |publisher=Greenwood Publishing Group}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url= http://www.wipo.int/freepublications/en/intproperty/909/wipo_pub_909.pdf |format=PDF| last = World Intellectual Property Organization | title= Understanding Copyright and Related Rights|publisher=WIPO|accessdate=11 August 2016|page=8}}&lt;/ref&gt;

Copyright does not cover ideas and information themselves, only the form or manner in which they are expressed.&lt;ref name="Art and copyright"&gt;{{cite book |title=Art and copyright|url=https://www.google.com/books?id=h-XBqKIryaQC&amp;dq=idea-expression+dichotomy&amp;lr=&amp;as_brr=3&amp;source=gbs_navlinks_s|pages=48–49 |author=Simon, Stokes |isbn=978-1-84113-225-9 |year=2001 |publisher=Hart Publishing }}&lt;/ref&gt; For example, the copyright to a [[Mickey Mouse]] cartoon restricts others from making copies of the cartoon or creating [[derivative work]]s based on [[The Walt Disney Company|Disney's]] particular [[anthropomorphic]] mouse, but does not prohibit the creation of other works about anthropomorphic mice in general, so long as they are different enough to not be judged copies of Disney's.&lt;ref name="Art and copyright"/&gt; Note additionally that Mickey Mouse is not copyrighted because characters cannot be copyrighted; rather, [[Steamboat Willie]] is copyrighted and Mickey Mouse, as a character in that copyrighted work, is afforded protection.

===Originality===
{{main|Threshold of originality}}
Typically, a work must meet [[Threshold of originality|minimal standards of originality]] in order to qualify for copyright, and the copyright expires after a set period of time (some jurisdictions may allow this to be extended). Different countries impose different tests, although generally the requirements are low; in the [[United Kingdom]] there has to be some "skill, labour, and judgment" that has gone into it.&lt;ref&gt;''Express Newspaper Plc v News (UK) Plc'', F.S.R. 36 (1991)&lt;/ref&gt; In [[Australia]] and the United Kingdom it has been held that a single word is insufficient to comprise a copyright work. However, single words or a short string of words can sometimes be registered as a [[trademark]] instead.

Copyright law recognizes the right of an author based on whether the work actually is an original creation, rather than based on whether it is unique; two authors may own copyright on two substantially identical works, if it is determined that the duplication was coincidental, and neither was copied from the other.

===Registration===
{{main|Copyright registration}}
[[File:Fermat Last Theorem "proof" registered by Ukraine officials.jpg|thumb|right|A copyright certificate for proof of the Fermat theorem, issued by the State Department of Intellectual Property of Ukraine.]]
In all countries where the [[Berne Convention for the Protection of Literary and Artistic Works|Berne Convention]] standards apply, copyright is automatic, and need not be obtained through official registration with any government office. Once an idea has been reduced to tangible form, for example by securing it in a fixed medium (such as a drawing, sheet music, photograph, a videotape, or a computer file), the copyright holder is entitled to enforce his or her exclusive rights.&lt;ref name="Berne Convention for the Protection of Literary and Artistic Works Article 5"/&gt; However, while registration isn't needed to exercise copyright, in jurisdictions where the laws provide for registration, it serves as ''[[prima facie]]'' evidence of a valid copyright and enables the copyright holder to seek [[statutory damages for copyright infringement|statutory damages]] and attorney's fees.&lt;ref&gt;{{cite web|title=Subject Matter and Scope of Copyright|url=http://copyright.gov/title17/92chap1.pdf|website=copyright.gov|accessdate=4 June 2015}}&lt;/ref&gt; (In the USA, registering after an infringement only enables one to receive actual damages and lost profits.)

A widely circulated strategy to avoid the cost of copyright registration is referred to as the [[poor man's copyright]]. It proposes that the creator send the work to himself in a sealed envelope by registered mail, using the [[postmark]] to establish the date. This technique has not been recognized in any published opinions of the United States courts. &lt;!-- Note to editors: The previously-worded statement, "This technique has not been recognized by any United States court" is overbroad because not all such cases are reported, and it is impossible to know whether this is correct.--&gt; The United States Copyright Office says the technique is not a substitute for actual registration.&lt;ref&gt;{{cite web|title=Copyright in General (FAQ)|url=http://www.copyright.gov/help/faq/faq-general.html#poorman|publisher=U.S Copyright Office|accessdate=11 Aug 2016}}&lt;/ref&gt; The United Kingdom Intellectual Property Office discusses the technique and notes that the technique (as well as commercial registries) does not constitute dispositive proof that the work is original nor who the creator of the work is.&lt;ref&gt;[http://www.ipo.gov.uk/copy/c-claim/c-register.htm "Copyright Registers"], United Kingdom Intellectual Property Office&lt;/ref&gt;&lt;ref&gt;[http://www.ipo.gov.uk/types/copy/c-about/c-auto.htm "Automatic right"], United Kingdom Intellectual Property Office&lt;/ref&gt;  &lt;!-- Note to editors: The previously-worded statement, "The United Kingdom Intellectual Property Office discusses the technique but does not recommend its use." overstates the UK IPO position; the IPO does NOT recommend against the PMC approach.--&gt;

===Fixing===
The [[Berne Convention]] allows member countries to decide whether creative works must be "fixed" to enjoy copyright. Article 2, Section 2 of the Berne Convention states: "It shall be a matter for legislation in the countries of the Union to prescribe that works in general or any specified categories of works shall not be protected unless they have been fixed in some material form." Some countries do not require that a work be produced in a particular form to obtain copyright protection. For instance, Spain, France, and Australia do not require fixation for copyright protection. The United States and Canada, on the other hand, require that most works must be "fixed in a tangible medium of expression" to obtain copyright protection.&lt;ref name="cyber.law.harvard.edu"&gt;See Harvard Law School, [http://cyber.law.harvard.edu/copyrightforlibrarians/Module_3:_The_Scope_of_Copyright_Law#Fixation ''Module 3: The Scope of Copyright Law'']. See also Tyler T. Ochoa, [http://digitalcommons.law.scu.edu/chtlj/vol20/iss4/5 ''Copyright, Derivative Works and Fixation: Is Galoob a Mirage, or Does the Form(GEN) of the Alleged Derivative Work Matter?''], 20 {{smallcaps|Santa Clara High Tech.}} L.J. 991, 999–1002 (2003) ("Thus, both the text of the Act and its legislative history demonstrate that Congress intended that a derivative work does not need to be fixed in order to infringe."). The legislative history of the 1976 Copyright Act says this difference was intended to address transitory works such as ballets, pantomimes, improvised performances, dumb shows, mime performances, and dancing.&lt;/ref&gt; U.S. law requires that the fixation be stable and permanent enough to be "perceived, reproduced or communicated for a period of more than transitory duration." Similarly, Canadian courts consider fixation to require that the work be "expressed to some extent at least in some material form, capable of identification and having a more or less permanent endurance."&lt;ref name="cyber.law.harvard.edu"/&gt;

===Copyright notice===
{{main|Copyright notice}}
[[File:Copyright.svg|thumb|upright|A copyright symbol used in copyright notice.]]
Before 1989, United States law required the use of a copyright notice, consisting of the [[copyright symbol]] (©, the letter '''C''' inside a circle), the abbreviation "Copr.", or the word "Copyright", followed by the year of the first publication of the work and the name of the copyright holder.&lt;ref&gt;Copyright Act of 1976, {{USPL|94|553}}, 90 Stat. 2541, § 401(a) (19 October 1976)&lt;/ref&gt;&lt;ref&gt;The Berne Convention Implementation Act of 1988 (BCIA), {{USPL|100|568}}, 102 Stat. 2853, 2857. One of the changes introduced by the BCIA was to section&amp;nbsp;401, which governs copyright notices on published copies, specifying that notices "may be placed on" such copies; prior to the BCIA, the statute read that notices "shall be placed on all" such copies. An analogous change was made in section&amp;nbsp;402, dealing with copyright notices on phonorecords.&lt;/ref&gt; Several years may be noted if the work has gone through substantial revisions. The proper copyright notice for sound recordings of musical or other audio works is a [[sound recording copyright symbol]] (℗, the letter&amp;nbsp;'''P''' inside a circle), which indicates a sound recording copyright, with the letter&amp;nbsp;'''P''' indicating a "phonorecord". In addition, the phrase ''[[All rights reserved]]'' was once required to assert copyright, but that phrase is now legally obsolete.

In 1989 the United States enacted the [[Berne Convention for the Protection of Literary and Artistic Works|Berne Convention]] Implementation Act, amending the 1976&amp;nbsp;Copyright Act to conform to most of the provisions of the Berne Convention. As a result, the use of copyright notices has become optional to claim copyright, because the Berne Convention makes copyright automatic.&lt;ref&gt;{{cite web|url=http://www.copyright.gov/circs/circ03.pdf |title=U.S. Copyright Office – Information Circular |format=PDF |accessdate=2012-07-07}}&lt;/ref&gt; However, the lack of notice of copyright using these marks may have consequences in terms of reduced damages in an infringement lawsuit&amp;nbsp;– using notices of this form may reduce the likelihood of a defense of "innocent infringement" being successful.&lt;ref&gt;[[17 U.S.C.]]{{UnitedStatesCodeSec|17|401(d)}}&lt;/ref&gt;

==Enforcement==
Copyrights are generally enforced by the holder in a [[Civil law (private law)|civil law]] court, but there are also criminal infringement statutes in some jurisdictions. While [[copyright registry|central registries]] are kept in some countries which aid in proving claims of ownership, registering does not necessarily prove ownership, nor does the fact of copying (even without permission) necessarily [[legal proof|prove]] that copyright was infringed. Criminal sanctions are generally aimed at serious counterfeiting activity, but are now becoming more commonplace as copyright collectives such as the [[RIAA]] are increasingly targeting the [[file sharing]] home Internet user. Thus far, however, most such cases against file sharers have been settled out of court. (See: [[Legal aspects of file sharing]])

In most jurisdictions the copyright holder must bear the cost of enforcing copyright. This will usually involve engaging legal representation, administrative and or court costs. In light of this, many copyright disputes are settled by a direct approach to the infringing party in order to settle the dispute out of court.

===Copyright infringement===
{{main|Copyright infringement}}
For a work to be considered to infringe upon copyright, its use must have occurred in a nation that has domestic copyright laws and/or adheres to a bilateral treaty or established international convention such as the [[Berne Convention for the Protection of Literary and Artistic Works|Berne Convention]] or [[World Intellectual Property Organization Copyright Treaty|WIPO Copyright Treaty]]. Improper use of materials outside of legislation is deemed "unauthorized edition", not copyright infringement.&lt;ref&gt;{{Cite journal | last1 = Owen | first1 = L. | doi = 10.1087/09531510125100313 | title = Piracy | journal = Learned Publishing | volume = 14 | pages = 67–70 | year = 2001 | pmid =  | pmc = }}&lt;/ref&gt;

Copyright infringement most often occurs to software, film and music. However, infringement upon books and other text works remains common, especially for educational reasons. Statistics regarding the effects of copyright infringement are difficult to determine. Studies have attempted to determine whether there is a monetary loss for industries affected by copyright infringement by predicting what portion of pirated works would have been formally purchased if they had not been freely available.&lt;ref&gt;Butler, S. Piracy Losses "Billboard" 199(36)&lt;/ref&gt; Other reports indicate that copyright infringement does not have an adverse effect on the entertainment industry, and can have a positive effect.&lt;ref&gt;{{cite web|url=http://www.ejpd.admin.ch/content/ejpd/de/home/dokumentation/mi/2011/2011-11-30.html |title=Urheberrechtsverletzungen im Internet: Der bestehende rechtliche Rahmen genügt |publisher=Ejpd.admin.ch}}&lt;/ref&gt; In particular, a 2014 university study concluded that free music content, accessed on YouTube, does not necessarily hurt sales, instead has the potential to increase sales.&lt;ref&gt;{{cite journal|publisher=Social Science Electronic Publishing|url=http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2425386|title=Video Killed the Radio Star? Online Music Videos and Digital Music Sales|ISSN=2042-2695|year=2014|authors=Tobias Kretschmer &amp; Christian Peukert}}&lt;/ref&gt;

==Rights granted==
===Exclusive rights===
Several exclusive rights typically attach to the holder of a copyright:
* to produce copies or reproductions of the work and to sell those copies (including, typically, electronic copies)
* to import or export the work
* to create [[derivative work]]s (works that adapt the original work)
* to perform or display the work publicly
* to sell or cede these rights to others
* to transmit or display by radio or video.&lt;ref name=autogenerated1&gt;{{cite book |title=Intellectual Property and Information Wealth: Copyright and related rights |page=346 |author=Peter K, Yu |isbn=978-0-275-98883-8 |year=2007 |publisher=Greenwood Publishing Group}}&lt;/ref&gt;

The phrase "exclusive right" means that only the copyright holder is free to exercise those rights, and others are prohibited from using the work without the holder's permission. Copyright is sometimes called a "negative right", as it serves to prohibit certain people (e.g., readers, viewers, or listeners, and primarily publishers and would be publishers) from doing something they would otherwise be able to do, rather than permitting people (e.g., authors) to do something they would otherwise be unable to do. In this way it is similar to the [[unregistered design right]] in [[English law]] and [[European law]]. The rights of the copyright holder also permit him/her to not use or exploit their copyright, for some or all of the term. There is, however, a critique which rejects this assertion as being based on a [[Philosophy of copyright|philosophical interpretation of copyright law]] that is not universally shared. There is also debate on whether copyright should be considered a [[property right]] or a [[Moral rights (copyright law)|moral right]].&lt;ref&gt;Tom G. Palmer, [http://www.tomgpalmer.com/wp-content/uploads/papers/morallyjustified.pdf "Are Patents and Copyrights Morally Justified?"] Accessed 5 February 2013.&lt;/ref&gt;

If a pictorial, graphic or sculptural work is a useful article, it is copyrighted only if its aesthetic features are separable from its utilitarian features. A useful article is an article having an intrinsic utilitarian function that is not merely to portray the appearance of the article or to convey information. They must be separable from the functional aspect to be copyrighted.&lt;ref name="U.S Copyright Office - Copyright Law: Chapter 1"&gt;{{cite web |url=http://www.copyright.gov/title17/92chap1.pdf |title=U.S Copyright Office – Copyright Law: Chapter 1 |accessdate=2012-06-27}}&lt;/ref&gt;

===Duration===&lt;!-- This section is linked from [[Little Nemo]] --&gt;
{{main|Copyright term|List of countries' copyright length}}
[[File:Tom Bell's graph showing extension of U.S. copyright term over time.svg|thumb|300px|Expansion of U.S. copyright law (currently based on the date of creation or publication).]]
Copyright subsists for a variety of lengths in different jurisdictions. The length of the term can depend on several factors, including the type of work (e.g. musical composition, novel), whether the work has been [[Publication|published]], and whether the work was created by an individual or a corporation. In most of the world, the default length of copyright is the life of the author plus either 50 or 70 years. In the United States, the term for most existing works is a fixed number of years after the date of creation or publication. Under most countries' laws (for example, the United States&lt;ref&gt;{{usc|17|305}}&lt;/ref&gt; and the United Kingdom&lt;ref&gt;The Duration of Copyright and Rights in Performances Regulations 1995, [http://www.opsi.gov.uk/si/si1995/Uksi_19953297_en_3.htm part II], Amendments of the UK Copyright, Designs and Patents Act 1988&lt;/ref&gt;), copyrights expire at the end of the calendar year in question.

The length and requirements for copyright duration are subject to change by legislation, and since the early 20th century there have been a number of adjustments made in various countries, which can make determining the duration of a given copyright somewhat difficult. For example, the United States used to require copyrights to be renewed after 28 years to stay in force, and formerly required a copyright notice upon first publication to gain coverage. In Italy and France, there were post-wartime extensions that could increase the term by approximately 6 years in Italy and up to about 14 in France. Many countries have extended the length of their copyright terms (sometimes retroactively). International treaties establish minimum terms for copyrights, but individual countries may enforce longer terms than those.&lt;ref&gt;{{cite book
|title=Copyright: Sacred Text, Technology, and the DMCA|last=Nimmer|first=David |publisher=Kluwer Law International|year=2003|isbn=  978-90-411-8876-2|oclc=50606064|page=63|url= https://books.google.com/books?id=RYfRCNxgPO4C}}&lt;/ref&gt;

In the United States, all books and other works published before 1923 have expired copyrights and are in the public domain.&lt;ref&gt;"[http://copyright.cornell.edu/resources/publicdomain.cfm Copyright Term and the Public Domain in the United States].", ''[[Cornell University]]''.&lt;/ref&gt; In addition, works published before 1964 that did not have their copyrights renewed 28 years after first publication year also are in the public domain. Hirtle points out that the great majority of these works (including 93% of the books) were not renewed after 28 years and are in the public domain.&lt;ref&gt;See Peter B. Hirtle, "Copyright Term and the Public Domain in the United States 1 January 2015" [https://copyright.cornell.edu/resources/publicdomain.cfm online at footnote 8]&lt;/ref&gt;  Books originally published outside the US by non-Americans are exempt from this renewal requirement, if they are still under copyright in their home country.

But if the intended exploitation of the work includes publication (or distribution of derivative work, such as a film based on a book protected by copyright) outside the U.S., the terms of copyright around the world must be considered. If the author has been dead more than 70 years, the work is in the public domain in most, but not all, countries.

In 1998, the length of a copyright in the United States was increased by 20 years under the [[Copyright Term Extension Act]]. This legislation was strongly promoted by corporations which had valuable copyrights which otherwise would have expired, and has been the subject of substantial criticism on this point.&lt;ref&gt;Lawrence Lessig, ''Copyright's First Amendment'', 48 UCLA L. Rev. 1057, 1065 (2001)&lt;/ref&gt;

{{globalize/US|date=September 2016}}

==Limitations and exceptions==
{{main|Limitations and exceptions to copyright|Traditional safety valves}}

In many jurisdictions, copyright law makes exceptions to these restrictions when the work is copied for the purpose of commentary or other related uses. It should be noted that US copyright does NOT cover names, title, short phrases or Listings (such as ingredients, recipes, labels, or formulas).&lt;ref&gt;[http://copyright.gov/circs/circ34.pdf (2012) ''Copyright Protection Not Available for Names, Titles, or Short Phrases'' U.S. Copyright Office]&lt;/ref&gt; However, there are protections available for those areas copyright does not cover – such as [[trademark]]s and [[patent]]s.

There are some exceptions to what copyright will protect. Copyright will not protect:
* Names of products
* Names of businesses, organizations, or groups
* Pseudonyms of individuals
* Titles of works
* Catchwords, catchphrases, mottoes, slogans, or short advertising expressions
* Listings of ingredients in recipes, labels, and formulas, though the directions can be copyrighted

===Idea–expression dichotomy and the merger doctrine===
{{main|Idea–expression divide}}

The idea–expression divide differentiates between ideas and expression, and states that copyright protects only the original expression of ideas, and not the ideas themselves. This principle, first clarified in the 1879 case of [[Baker v. Selden]], has since been codified by the [[Copyright Act of 1976]] at 17 U.S.C. § 102(b).

===The first-sale doctrine and exhaustion of rights===

{{main|First-sale doctrine|Exhaustion of rights}}
Copyright law does not restrict the owner of a copy from reselling legitimately obtained copies of copyrighted works, provided that those copies were originally produced by or with the permission of the copyright holder. It is therefore legal, for example, to resell a copyrighted book or [[compact disc|CD]]. In the United States this is known as the [[first-sale doctrine]], and was established by the [[court]]s to clarify the legality of reselling books in second-hand [[bookstore]]s.

Some countries may have [[parallel importation]] restrictions that allow the copyright holder to control the [[aftermarket (merchandise)|aftermarket]]. This may mean for example that a copy of a book that does not infringe copyright in the country where it was printed does infringe copyright in a country into which it is imported for retailing. The first-sale doctrine is known as [[exhaustion of rights]] in other countries and is a principle which also applies, though somewhat differently, to [[patent]] and [[trademark]] rights. It is important to note that the first-sale doctrine permits the transfer of the particular legitimate copy involved. It does not permit making or distributing additional copies.

In ''[[Kirtsaeng v. John Wiley &amp; Sons, Inc.]]'',&lt;ref&gt;{{cite web|title=John Wiley &amp; Sons Inc. v. Kirtsaeng |url= http://www.supremecourt.gov/opinions/12pdf/11-697_d1o2.pdf}}&lt;/ref&gt; in 2013, the [[United States Supreme Court]] held in a 6-3 decision that the first-sale doctrine applies to goods manufactured abroad with the copyright owner's permission and then imported into the US without such permission.  The case involved a plaintiff who imported Asian editions of textbooks that had been manufactured abroad with the publisher-plaintiff's permission. The defendant, without permission from the publisher, imported the textbooks and resold on eBay. The Supreme Court's holding severely limits the ability of copyright holders to prevent such importation.

In addition, copyright, in most cases, does not prohibit one from acts such as modifying, defacing, or destroying his or her own legitimately obtained copy of a copyrighted work, so long as duplication is not involved. However, in countries that implement [[Moral rights (copyright law)|moral rights]], a copyright holder can in some cases successfully prevent the mutilation or destruction of a work that is publicly visible.

===Fair use and fair dealing===
{{main|Fair use|Fair dealing}}
Copyright does not prohibit all copying or replication. In the United States, the [[Fair use|fair use doctrine]], codified by the [[United States Copyright Act of 1976|Copyright Act of 1976]] as 17 U.S.C. Section 107, permits some copying and distribution without permission of the copyright holder or payment to same. The statute does not clearly define fair use, but instead gives four non-exclusive factors to consider in a fair use analysis. Those factors are:
# the purpose and character of one's use
# the nature of the copyrighted work
# what amount and proportion of the whole work was taken, and
# the effect of the use upon the potential market for or value of the copyrighted work.&lt;ref&gt;{{cite web|url=http://www4.law.cornell.edu/uscode/17/107.html |title=US CODE: Title 17,107. Limitations on exclusive rights: Fair use |publisher=.law.cornell.edu |date=2009-05-20 |accessdate=2009-06-16}}&lt;/ref&gt;

In the [[United Kingdom]] and many other [[Commonwealth of Nations|Commonwealth]] countries, a similar notion of fair dealing was established by the [[court]]s or through [[legislation]]. The concept is sometimes not well defined; however in [[Canada]], private copying for personal use has been expressly permitted by statute since 1999. In ''[[Alberta (Education) v. Canadian Copyright Licensing Agency (Access Copyright)]]'', 2012 SCC 37, the [[Supreme Court of Canada]] concluded that limited copying for educational purposes could also be justified under the fair dealing exemption. In Australia, the fair dealing exceptions under the ''Copyright Act 1968'' (Cth) are a limited set of circumstances under which copyrighted material can be legally copied or adapted without the copyright holder's consent. Fair dealing uses are research and study; review and critique; news reportage and the giving of professional advice (i.e. [[legal advice]]). Under current [[Law of Australia|Australian law]], although it is still a breach of copyright to copy, reproduce or adapt copyright material for personal or private use without permission from the copyright owner, owners of a legitimate copy are permitted to “format shift” that work from one medium to another for personal, private use, or to “time shift” a broadcast work for later, once and only once, viewing or listening. Other technical exemptions from infringement may also apply, such as the temporary reproduction of a work in machine readable form for a computer.

In the United States the AHRA ([[Audio Home Recording Act]] Codified in Section 10, 1992) prohibits action against consumers making noncommercial recordings of music, in return for royalties on both media and devices plus mandatory copy-control mechanisms on recorders.

:''Section 1008. Prohibition on certain infringement actions''

:''No action may be brought under this title alleging infringement of copyright based on the manufacture, importation, or distribution of a digital audio recording device, a digital audio recording medium, an analog recording device, or an analog recording medium, or based on the noncommercial use by a consumer of such a device or medium for making digital musical recordings or analog musical recordings.''

Later acts amended US Copyright law so that for certain purposes making 10 copies or more is construed to be commercial, but there is no general rule permitting such copying. Indeed, making one complete copy of a work, or in many cases using a portion of it, for commercial purposes will not be considered fair use. The [[Digital Millennium Copyright Act]] prohibits the manufacture, importation, or distribution of devices whose intended use, or only significant commercial use, is to bypass an access or copy control put in place by a copyright owner.&lt;ref name="Intellectual Property and Information Wealth: Copyright and related rights"/&gt; An appellate court has held that fair use is not a defense to engaging in such distribution.

The [[copyright directive]] allows EU member states to implement a set of exceptions to copyright. Examples of those exceptions are:
*photographic reproductions on paper or any similar medium of works (excluding sheet music) provided that the rightholders receives fair compensation,
*reproduction made by libraries, educational establishments, museums or archives, which are non-commercial
*archival reproductions of broadcasts,
*uses for the benefit of people with a disability,
*for demonstration or repair of equipment,
*for non-commercial research or private study
*when used in [[parody]]

===Accessible copies===
It is legal in several countries including the United Kingdom and the United States to produce alternative versions (for example, in large print or braille) of a copyrighted work to provide improved access to a work for blind and visually impaired persons without permission from the copyright holder.&lt;ref&gt;[http://www.copyright.gov/title17/92chap1.html#121 Copyright Law of the USA, Chapter 1 Section 121]&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.rnib.org.uk/xpedio/groups/public/documents/publicwebsite/public_cvipsact2002.hcsp|title=Copyright (Visually Impaired Persons) Act 2002 comes into force|publisher=Royal National Institute of Blind People|date=1 January 2011|accessdate=11 Aug 2016}}&lt;/ref&gt;

=={{anchor|Transfer and licensing, and assignment}} Transfer, assignment and licensing==
{{see also|Collective rights management|extended collective licensing|Compulsory license|Copyright transfer agreement}}
[[File:All rights reserved.jpg|thumb|right|300px|DVD: [[All Rights Reserved]].]]
A copyright, or aspects of it (e.g. reproduction alone, all but moral rights), may be assigned or transferred from one party to another.&lt;ref name="WIPO Guide on the Licensing of Copyright and Related Rights"&gt;{{cite book |title=WIPO Guide on the Licensing of Copyright and Related Rights|url=https://www.google.com/books?id=LvRRvXBIi8MC&amp;dq=copyright+transfer+and+licensing&amp;as_brr=3&amp;source=gbs_navlinks_s|isbn=978-92-805-1271-7 |year=2004 |publisher=World Intellectual Property Organization|page=15}}&lt;/ref&gt; For example, a musician who records an album will often sign an agreement with a record company in which the musician agrees to transfer all copyright in the recordings in exchange for royalties and other considerations. The creator (and original copyright holder) benefits, or expects to, from production and marketing capabilities far beyond those of the author. In the digital age of music, music may be copied and distributed at minimal cost through the [[Internet]]; however, the [[record industry]] attempts to provide promotion and marketing for the artist and his or her work so it can reach a much larger audience. A copyright holder need not transfer all rights completely, though many publishers will insist. Some of the rights may be transferred, or else the copyright holder may grant another party a non-exclusive license to copy and/or distribute the work in a particular region or for a specified period of time.

A transfer or licence may have to meet particular formal requirements in order to be effective,&lt;ref name="WIPO Guide on the Licensing of Copyright and Related Rights(2)"&gt;{{cite book |title=WIPO Guide on the Licensing of Copyright and Related Rights|url=https://www.google.com/books?id=LvRRvXBIi8MC&amp;dq=copyright+transfer+and+licensing&amp;as_brr=3&amp;source=gbs_navlinks_s|page=8 |isbn=978-92-805-1271-7 |year=2004 |publisher=World Intellectual Property Organization}}&lt;/ref&gt; for example under the Australian [[Copyright law of Australia#Copyright Act 1968|Copyright Act 1968]] the copyright itself must be expressly transferred in writing. Under the U.S. Copyright Act, a transfer of ownership in copyright must be memorialized in a writing signed by the transferor. For that purpose, ownership in copyright includes exclusive licenses of rights. Thus exclusive licenses, to be effective, must be granted in a written instrument signed by the grantor. No special form of transfer or grant is required. A simple document that identifies the work involved and the rights being granted is sufficient. Non-exclusive grants (often called non-exclusive licenses) need not be in writing under [[Law of the United States|U.S. law]]. They can be oral or even implied by the behavior of the parties. Transfers of copyright ownership, including exclusive licenses, may and should be recorded in the U.S. Copyright Office. (Information on recording transfers is available on the Office's web site.) While recording is not required to make the grant effective, it offers important benefits, much like those obtained by recording a deed in a [[real estate]] transaction.

Copyright may also be [[license]]d.&lt;ref name="WIPO Guide on the Licensing of Copyright and Related Rights"/&gt; Some jurisdictions may provide that certain classes of copyrighted works be made available under a prescribed [[statutory license]] (e.g. musical works in the United States used for radio broadcast or performance). This is also called a [[compulsory license]], because under this scheme, anyone who wishes to copy a covered work does not need the permission of the copyright holder, but instead merely files the proper notice and pays a set fee established by statute (or by an agency decision under statutory guidance) for every copy made.&lt;ref name="WIPO Guide on the Licensing of Copyright and Related Rights(3)"&gt;{{cite book |title=WIPO Guide on the Licensing of Copyright and Related Rights|url=https://www.google.com/books?id=LvRRvXBIi8MC&amp;dq=copyright+transfer+and+licensing&amp;as_brr=3&amp;source=gbs_navlinks_s|page=16 |isbn=978-92-805-1271-7 |year=2004 |publisher=World Intellectual Property Organization}}&lt;/ref&gt; Failure to follow the proper procedures would place the copier at risk of an infringement suit. Because of the difficulty of following every individual work, [[copyright collective]]s or [[collecting societies]] and [[performance rights organisation|performing rights organizations]] (such as [[ASCAP]], [[Broadcast Music Incorporated|BMI]], and [[SESAC]]) have been formed to collect royalties for hundreds (thousands and more) works at once. Though this market solution bypasses the statutory license, the availability of the statutory fee still helps dictate the price per work collective rights organizations charge, driving it down to what avoidance of procedural hassle would justify.

===Free licences===
Copyright licenses known as ''open'' or [[free license]]s seek to grant several rights to licensees, either for a fee or not, to an effect inspired by the [[public domain]]. ''Free'' in this context isn't much of a reference to price as it is to freedom. What constitutes free licensing has been characterised in a number of similar definitions, including by order of longevity the [[Free Software Definition]], the [[Debian Free Software Guidelines]], the [[Open Source Definition]] and the [[Definition of Free Cultural Works]]. Further refinements to these licenses have resulted in categories such as [[copyleft]] and [[permissive license|permissive]]. Common examples of free licences are the [[GNU General Public License]], [[BSD license]]s and some [[Creative Commons licenses]].

Founded in 2001 by [[James Boyle (academic)|James Boyle]], [[Lawrence Lessig]], and [[Hal Abelson]], the [[Creative Commons]] (CC) is a non-profit organization&lt;ref name="CC"&gt;{{cite web|url=http://creativecommons.org/ |title=Creative Commons Website|website=creativecommons.org|accessdate=24 October 2011}}&lt;/ref&gt; which aims to facilitate the legal sharing of creative works. To this end, the organization provides a number of generic copyright license options to the public, [[gratis versus libre|gratis]]. These licenses allow copyright holders to define conditions under which others may use a work and to specify what types of use are acceptable.&lt;ref name="CC" /&gt;

Terms of use have traditionally been negotiated on an individual basis between copyright holder and potential licensee. Therefore, a general CC license outlining which rights the copyright holder is willing to waive enables the general public to use such works more freely. Six general types of CC licenses are available (although some of them aren't properly free per the above definitions and per Creative Commons' own advice). These are based upon copyright holder stipulations such as whether he or she is willing to allow modifications to the work, whether he or she permits the creation of derivative works and whether he or she is willing to permit commercial use of the work.&lt;ref name="Rubin"&gt;Rubin, R. E. (2010) 'Foundations of Library and Information Science: Third Edition', Neal-Schuman Publishers, Inc., New York, p. 341&lt;/ref&gt; {{As of|2009}} approximately 130 million individuals had received such licenses.&lt;ref name="Rubin" /&gt;

==Criticism==
Some sources are critical of particular aspects of the copyright system. This is known as a debate over [[copynorms]]. Particularly on the internet, there is discussion about the [[copyright aspects of downloading and streaming]], the [[copyright aspects of hyperlinking and framing]]. Such concerns are often couched in the language of [[digital rights]] and [[database right]]s. Discussions include ''[[Free Culture (book)|Free Culture]]'', a 2004 book by [[Lawrence Lessig]]. Lessig coined the term [[permission culture]] to describe a worst-case system. ''[[Good Copy Bad Copy]]'' (documentary) and [[RiP!: A Remix Manifesto]], discuss copyright. Some suggest an [[alternative compensation system]]. 

Some groups reject copyright altogether, taking an [[anti-copyright]] stance. The perceived inability to enforce copyright online leads some to advocate [[Crypto-anarchism|ignoring legal statutes when on the web]].

==Public domain==
{{main|Public domain}}
Copyright, like other [[intellectual property rights]], is subject to a statutorily determined term. Once the term of a copyright has expired, the formerly copyrighted work enters the public domain and may be freely used or exploited by anyone. Courts in common law countries, such as the United States and the United Kingdom, have rejected the doctrine of a [[common law copyright]]. Public domain works should not be confused with works that are publicly available. Works posted in the [[internet]], for example, are publicly available, but are not generally in the public domain. Copying such works may therefore violate the author's copyright.

==See also==
{{Portal|Social and political philosophy|Law}}
{{colbegin|colwidth=15em}}
* [[Adelphi Charter]]
* [[Artificial scarcity]]
* [[Conflict of laws]]
* [[Copyright Alliance]]
* [[Copyright in architecture in the United States]]
* [[Copyright on the content of patents and in the context of patent prosecution]]
* [[Copyright for Creativity]]
* [[Copyright infringement of software]]
* [[Copyright on religious works]]
* [[Creative Barcode]]
* [[Digital rights management]]
* [[Digital watermarking]]
* [[Entertainment law]]
* [[Freedom of panorama]]
* [[Intellectual property education]]
* [[Intellectual property protection of typefaces]]
* [[List of Copyright Acts]]
* [[List of copyright case law]]
* [[Model release]]
* [[Paracopyright]]
* [[Photography and the law]]
* [[Pirate Party]]
* [[Private copying levy]]
* [[Production music]]
* [[Rent-seeking]]
* [[Reproduction fees]]
* [[Samizdat]]
* [[Software copyright]]
* [[Threshold pledge system]]
{{colend}}

==References==
{{Reflist|30em}}

==Further reading==
{{refbegin|30em}}
* {{Cite book
  |author=Dowd, Raymond J.
  |title=Copyright Litigation Handbook
  |publisher=Thomson West |edition=1st |year=2006
  |isbn=0-314-96279-4
  |ref=Dowd, Litigation handbook
}}
* Ellis, Sara R. ''Copyrighting Couture: An Examination of Fashion Design Protection and Why the DPPA and IDPPPA are a Step Towards the Solution to Counterfeit Chic'', 78 Tenn. L. Rev. 163 (2010), ''available at'' http://ssrn.com/abstract=1735745.
* {{Cite book
  |author1=Gantz, John  |author2=Rochester, Jack B.
  |title=Pirates of the Digital Millennium
  |publisher=Financial Times Prentice Hall
  |year=2005
  |isbn=0-13-146315-2
  |ref=Gantz, Pirates
}}
* [[Shuman Ghosemajumder|Ghosemajumder, Shuman]]. ''[http://dspace.mit.edu/handle/1721.1/8438 Advanced Peer-Based Technology Business Models]''. [[MIT Sloan School of Management]], 2002.
* [[Bruce Lehman|Lehman, Bruce]]: ''Intellectual Property and the National Information Infrastructure'' (Report of the Working Group on Intellectual Property Rights, 1995)
* Lindsey, Marc: ''Copyright Law on Campus.'' [[Washington State University]] Press, 2003. ISBN 978-0-87422-264-7.
* Mazzone, Jason. ''[[Copyfraud]]''. [http://ssrn.com/abstract=787244 SSRN]
* McDonagh, Luke. ''Is Creative use of Musical Works without a licence acceptable under Copyright?'' International Review of Intellectual Property and Competition Law (IIC) 4 (2012) 401-426, available at [http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2521081 SSRN]
* {{Cite book
  | last = Nimmer | first = Melville |authorlink=Melville Nimmer |author2=David Nimmer | title = [[Nimmer on Copyright]] | publisher = Matthew Bender | year=1997| isbn = 0-8205-1465-9
}}
* {{Cite book
  |title=Copyright in Historical Perspective
  |author=Patterson, Lyman Ray
  |year=1968|publisher=Vanderbilt University Press
  |isbn=0-8265-1373-5
  |version=Online Version
}}
* Rife, by Martine Courant. ''Convention, Copyright, and Digital Writing''  (Southern Illinois University Press; 2013) 222 pages; Examines legal, pedagogical, and other aspects of online authorship.
* {{cite book | last = Rosen | first = Ronald | title = Music and Copyright | publisher = Oxford University Press | location = Oxford Oxfordshire | year = 2008 | isbn = 0-19-533836-7 }}
* Shipley, David E. [http://ssrn.com/abstract=1076789 Thin But Not Anorexic: Copyright Protection for Compilations and Other Fact Works] UGA Legal Studies Research Paper No. 08-001; Journal of Intellectual Property Law, Vol. 15, No. 1, 2007.
* Silverthorne, Sean. ''[http://hbswk.hbs.edu/item.jhtml?id=4206&amp;t=innovation Music Downloads: Pirates- or Customers?]''. [[Harvard Business School|Harvard Business School Working Knowledge]], 2004.
* Sorce Keller, Marcello. "Originality, Authenticity and Copyright", ''Sonus'', VII(2007), no. 2, pp.&amp;nbsp;77–85.
* {{Cite book
  |author1=Steinberg, S.H.  |author2=Trevitt, John
  |title=Five Hundred Years of Printing
  |location=London and New Castle |publisher=The British Library and Oak Knoll Press
  |edition=4th |year=1996
  |isbn=1-884718-19-1
  |ref=Steinberg, Five hundred years
}}
* {{Cite book
  |title=The Copy/South Dossier: Issues in the Economics, Politics and Ideology of Copyright in the Global South 
  |url=http://copysouth.org/en/documents/csdossier.pdf
  |editor1=Story, Alan |editor2=Darch, Colin |editor3=Halbert, Deborah |year=2006|publisher=Copy/South Research Group
  |isbn=978-0-9553140-1-8
}}
* [http://whynotaskme.org/ WhyNotAskMe.org]: ''Organization demanding democratic participation in copyright legislation and a moratorium on secret and fast-tracked copyright negotiations''
{{refend}}

==External links==
{{Wikisource1911Enc|Copyright}}
{{Wikisource|Wikisource:Copyright law|Copyright law}}
{{Spoken Wikipedia|En-Copyright.ogg|2008-12-30}}
{{Library resources box}}
* {{Wikiquote-inline}}
* {{Commons-inline|Copyright}}
* {{dmoz|Society/Law/Legal_Information/Intellectual_Property/Copyrights}}
* [http://www.wipo.int/clea/en/ Collection of laws for electronic access] from [[WIPO]] – intellectual property laws of many countries
* [http://purl.fdlp.gov/GPO/gpo55676 Compendium of Copyright Practices] (3rd ed.) [[United States Copyright Office]]
* [http://ucblibraries.colorado.edu/govpubs/us/copyrite.htm Copyright] from ''UCB Libraries GovPubs''
* [http://www.ipo.gov.uk/types/copy.htm About Copyright] at the UK Intellectual Property Office
* [http://www.lawtech.jus.unitn.it/index.php/copyright-history/bibliography A Bibliography on the Origins of Copyright and Droit d'Auteur]
* [http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-912-introduction-to-copyright-law-january-iap-2006/ 6.912 Introduction to Copyright Law] taught by Keith Winstein, MIT OpenCourseWare January IAP 2006
* [http://www.wipo.int/treaties/en/ShowResults.jsp?country_id=ALL&amp;start_year=ANY&amp;end_year=ANY&amp;search_what=C&amp;treaty_id=15 Copyright Berne Convention: Country List]  List of the 164 members of the Berne Convention for the protection of literary and artistic works
* [http://www.copyrightservice.co.uk/copyright/p01_uk_copyright_law UK Copyright Law fact sheet] (April 2000) a concise introduction to UK Copyright legislation
* [http://www.jisc.ac.uk/whatwedo/themes/content/contentalliance/reports/ipr.aspx IPR Toolkit – An Overview, Key Issues and Toolkit Elements] (September 2009) by Professor Charles Oppenheim and Naomi Korn at the [http://www.jisc.ac.uk/whatwedo/themes/content/contentalliance.aspx Strategic Content Alliance]
* [http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-912-introduction-to-copyright-law-january-iap-2006/ MIT OpenCourseWare 6.912 Introduction to Copyright Law] Free self-study course with video lectures as offered during the January 2006, Independent Activities Period (IAP)
* [http://www.loc.gov/rr/rarebook/coll/067.html Early Copyright Records] From the [http://www.loc.gov/rr/rarebook/ Rare Book and Special Collections Division at the Library of Congress]
* [http://copyright.gov/title17/ Copyright Law of the United States Documents], US Government
{{Copyright law by country}}
{{Intellectual property activism}}

{{Authority control}}

[[Category:Copyright law| ]]
[[Category:Data management]]
[[Category:Intellectual property law]]
[[Category:Metadata]]
[[Category:Monopoly (economics)]]
[[Category:Public records]]</text>
      <sha1>p642694jelqd413wy5o0hm74fb8r50c</sha1>
    </revision>
  </page>
  <page>
    <title>Data warehouse automation</title>
    <ns>0</ns>
    <id>48752218</id>
    <revision>
      <id>751539838</id>
      <parentid>751372607</parentid>
      <timestamp>2016-11-26T09:59:09Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor />
      <comment>/* General */[[WP:CHECKWIKI]] error fixes using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3873" xml:space="preserve">'''Data warehouse automation''' or DWA refers to the process of accelerating and automating the [[data warehouse]] development cycles, while assuring quality and consistency. DWA is believed to provide automation of the entire lifecycle of a data warehouse, from source [[Systems analysis|system analysis]] to [[Software testing|testing]] to  [[documentation]]. It helps improve productivity, reduce cost, and improve overall quality.&lt;ref&gt;{{cite web|title=Automate and accelerate your data transformations|url=http://www.attunity.com/products/prepare-data-compose|website=www.attunity.com|publisher=Attunity|accessdate=7 December 2015}}&lt;/ref&gt;

==General==
Data warehouse automation primarily focuses on automation of each and every step involved in the lifecycle of a data warehouse, thus reducing the efforts required in managing it.&lt;ref&gt;{{cite web|title=New Buzzword! Data Warehouse Automation|url=http://blogs.jetreports.com/2015/03/05/new-buzzword-data-warehouse-automation/|website=blogs.jetreports.com|publisher=jetreports|accessdate=7 December 2015}}&lt;/ref&gt;
Data warehouse automation works on the principles of design patterns. It comprises a central repository of design patterns, which encapsulate architectural standards as well as best practices for data design, data management, data integration, and data usage.&lt;ref&gt;{{cite web|url=https://www.wherescape.com/media/1988/data-warehouse-automation-decision-guide.pdf|title=Data Warehouse Automation - A Decision Guide|website=www.wherescape.com|publisher=David L. Wells, Infocentric LLC|accessdate=7 December 2015}}&lt;/ref&gt;
In November 2015, an analyst firm has published a guide ''Which Data Warehouse Automation Tool is Right for You?'' covering four of the leading products in the DWA space.&lt;ref&gt;{{cite web|title=Which Data Warehouse Automation Tool is Right for You?|url=http://eckerson.com/register?content=which-data-warehouse-automation-tool-is-right-for-you|website=eckerson.com|publisher= Wayne Eckerson|accessdate=9 December 2015}}&lt;/ref&gt; In November 2015, an international software and technology services company engaged in developing ‘agile tools’ for the data integration industry, was named by CIO Review as one of the 20 most promising productivity tools solution providers 2015 &lt;ref&gt;{{cite web|title=CIO Magazine Award - 20 Most promising productivity tools|url=http://analytixds.com/latest_news/analytix-data-services-wins-cio-reviews-2015/|website=www.analtyixds.com|publisher=AnalytiX DS|accessdate=25 November 2016}}&lt;/ref&gt;

==Benefits==
Data warehouse automation can provide advantages like source data exploration, warehouse data models, ETL generation, test automation, metadata management, managed deployment, scheduling, change impact analysis and easier maintenance and modification of the data warehouse.&lt;ref&gt;{{cite web|title=Data Warehouse Automation (DWA)?|url=http://www.timextender.com/software/data-warehouse-automation/business-value/|website=timextender.com|publisher=TimeXtender Software 2015|accessdate=7 December 2015}}&lt;/ref&gt;
More important than the technical features of DWA tools, however, is the ability to deliver projects faster and with less resources.&lt;ref&gt;{{cite web|title=Deliver Faster|url=http://kalido.com/products/kalido-information-engine/deliver-faster/|website=kalido.com|publisher=Magnitude Software|accessdate=9 December 2015}}&lt;/ref&gt;

==References==
{{reflist}}

==External links==
* {{Dmoz|Computers/Software/Databases/Data_Warehousing/Data_Warehouse_Automation|Data Warehouse Automation}}
* [http://analytixds.com/wp-content/uploads/2016/07/analytix-data-services-top-20-cio-review.pdf CIO Magazine Award - 20 Most promising productivity tools 'CIO Review', November 10, 2015]

==See also==
*[[Data warehouse]]
*[[Data mart]]
*[[Data warehouse appliance]]
*[[Data integration]]

{{Data_warehouse}}

[[Category:Data management]]
[[Category:Data warehousing]]</text>
      <sha1>m69cq0ecgpq63st6auycrmikgfzorz4</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Data management software</title>
    <ns>14</ns>
    <id>32164666</id>
    <revision>
      <id>701110853</id>
      <parentid>546175104</parentid>
      <timestamp>2016-01-22T15:32:53Z</timestamp>
      <contributor>
        <username>Horcrux92</username>
        <id>10845682</id>
      </contributor>
      <comment>new key for [[Category:Data management]]: "Software" using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="458" xml:space="preserve">[[Software]], typically proprietary products or open-source projects, with a primary purpose of [[data management]].  [[:Category:Database management systems by license|Database management system software]] could be considered a related category, though those will typically exist for the purpose of managing a [[database]] in a particular structure (i.e. relational, object-oriented). 
[[Category:Data management|Software]]
[[Category:Application software]]</text>
      <sha1>948qwe30ruw7cmbyqb8sxgu80cydeca</sha1>
    </revision>
  </page>
  <page>
    <title>Altitude3.Net</title>
    <ns>0</ns>
    <id>47288071</id>
    <revision>
      <id>717345805</id>
      <parentid>717345798</parentid>
      <timestamp>2016-04-27T04:43:45Z</timestamp>
      <contributor>
        <username>DaltonCastle</username>
        <id>16866178</id>
      </contributor>
      <comment>added [[Category:Technical communication]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5346" xml:space="preserve">{{Infobox software
| name                   = Altitude3.Net
| logo                   = Altitude-en.png
| screenshot             = 
| caption                = Altitude3.Net Dashboard
| developer              = Nmédia Solutions
| status                 = Active
| released               = {{start date and age|2000|06|01}}
| operating system       = 
| platform               = [[.NET Framework|.Net]]
| genre                  = [[Content Management System]], [[Content Management Framework]]
| alexa                  =
| website                = {{URL|http://altitude3.net}}
}}

'''Altitude3.Net''' is an electronic business development platform that allows to create web and mobile solutions along with interactive communication strategies. The platform has the same functionalities &lt;ref&gt;{{cite web|language=fr|title = Pinpoint|url = https://pinpoint.microsoft.com/fr-CA/Companies/4296539037/Services|publisher = pinpoint.microsoft.com|accessdate = 2015-06-22}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Altitude3.Net|url=http://www.cmsmatrix.org/matrix/cms-matrix/altitude-3.net|website=CMS Matrix|accessdate=27 July 2015}}&lt;/ref&gt; than a content management system (CMS) and communicates with other systems (accounting systems, manufacturing management software (MRP), business management software (enterprise resource planning (ERP)), database, Excel files, XML, CSV or all other kinds of structural data).

Nmédia solutions developed Altitude3.Net in 2001 using Microsoft's .NET Framework technology. The platform is currently using the 4.5 version of Microsoft’s Framework.

== History ==
In 2001, Nmédia solutions created the content management system Altitude&lt;sup&gt;mc&lt;/sup&gt;.&lt;ref&gt;{{cite web|title=About|url=http://altitude3.net/home|website=Altitude3.Net|accessdate=27 July 2015}}&lt;/ref&gt; As it went on, many versions were developed:
* Altitude Moto and Altitude Auto (2001 to 2006);
* Altitude 2 (2006);
* Altitude3.Net (2010).

== List of main functionalities ==
The Altitude3.Net platform is structured in many modules:&lt;ref&gt;{{cite web|title=Altitude advantages|url=http://altitude3.net/home|website=Altitude3.Net|accessdate=27 July 2015}}&lt;/ref&gt;
* Content management
* contact management and mass-emailing
* Control of advanced SEO parameters
* Microsoft flexibility &amp; computability
* Security &amp; access management
* Security &amp; permission management
* E-commerce solutions: Centralized Product Management (CPM) services. This module includes several functionalities: interface for mass product modification, centralized coupon management, custom management by product group, inventory by store location, shopping cart, price &amp; currency management, catalog management, centralized database, supplier management, product by media, product comparison tool (based on common characteristics), syncing accounting software inventory with Altitude3.Net
* A [[Microsoft Azure]] solution (cloud computing)
* Omnichannel marketing
* Other functionalities : On-site search engines for meta data and documents (text, Word, Excel and PDF); HTML5 video player with descending compatibility; Integrated functions enabling an entire site to be generated in Hypertext Markup Language (HTML) or enabling to export all its data (DATA) and import it in any other CMS

== Awards and recognitions ==
* In 2010: Altitude3.Net is finalist in the IT category (software application) of the Mérites du français during the Francofête.
* In 2011: Nmédia solutions wins the title of Web Development Partner of the Year awarded by the Microsoft Partner Network.&lt;ref&gt;{{cite news|title=Microsoft honore deux entreprises de la région|url=http://www.lapresse.ca/la-tribune/economie-et-innovation/201112/19/01-4479237-microsoft-honore-deux-entreprises-de-la-region.php|accessdate=27 July 2015|publisher=[[La Presse (Canadian newspaper)]]|date=19 December 2011|language=fr}}&lt;/ref&gt;&lt;ref&gt;{{cite news|title=Drummondville triomphe à Toronto|url=http://www.journalexpress.ca/Actualites/Economie/2011-12-05/article-2825943/Drummondville-triomphe-a-Toronto/1|accessdate=27 July 2015|publisher=Journal l'Express|date=5 December 2011|language=fr}}&lt;/ref&gt;
* In 2012: Altitude3.Net wins the Prix Franco awarded by the Drummondville Young Chamber of Commerce during its annual gala.&lt;ref&gt;{{cite web|title=Prix Franco 2012 Nmédia Solutions Inc.|url=http://www.jccd.ca/Concours-Prix-Franco/Prix-Franco-2012/NMedia-Solutions-Inc.aspx|website=Jeune Chambre de Commerce de Drummondville|accessdate=27 July 2015|language=fr}}&lt;/ref&gt;
* In 2015: The CPM module of Altitude3 is finalist in the Web Solutions category at the Octas.&lt;ref&gt;{{cite web|title=Les lauréats du concours des Octas 2015|url=http://www.actionti.com/microsites/octas/gagnants/nos-gagnants|website=Réseau Action TI|accessdate=27 July 2015|language=fr}}&lt;/ref&gt;&lt;ref&gt;{{cite news|title=Nmédia en lice aux Octas|url=http://www.journalexpress.ca/Actualites/2015-05-12/article-4144312/Nmedias-en-lice-aux-Octas/1|accessdate=27 July 2015|publisher=Journal l'Express|date=12 May 2015|language=fr}}&lt;/ref&gt;

== See also ==
*[[List of content management systems]]

== References ==
{{Reflist|30em}}

== External links ==
* [http://altitude3.net Altitude3.Net's website]

[[:Category:Content management systems]]
[[:Category:Website management]]



[[Category:Content management systems]]
[[Category:Data management]]
[[Category:Technical communication]]</text>
      <sha1>eivll36nj7w5zfkcw4718as2gbujesz</sha1>
    </revision>
  </page>
  <page>
    <title>WCF Data Services</title>
    <ns>0</ns>
    <id>10988544</id>
    <revision>
      <id>723199784</id>
      <parentid>635858836</parentid>
      <timestamp>2016-06-01T16:41:08Z</timestamp>
      <contributor>
        <username>GrahamHardy</username>
        <id>2956291</id>
      </contributor>
      <minor />
      <comment>- defsort</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7369" xml:space="preserve">{{Primary sources|date=November 2010}}
'''[[Windows Communication Foundation|WCF]] Data Services''' (formerly '''ADO.NET Data Services''',&lt;ref&gt;{{cite web|url=http://blogs.msdn.com/astoriateam/archive/2009/11/17/simplifying-our-n-tier-development-platform-making-3-things-1-thing.aspx|title=Simplifying our n-tier development platform: making 3 things 1 thing|date=2009-11-17|accessdate=2009-12-17|publisher=ADO.NET Data Services Team Blog}}&lt;/ref&gt; codename '''"Astoria"''')&lt;ref&gt;{{cite web | url = http://blogs.msdn.com/data/archive/2007/12/10/ado-net-data-services-ctp-released.aspx | title = ADO.NET Data Services CTP Released! | accessdate = 2007-11-12}}&lt;/ref&gt; is a platform for what [[Microsoft]] calls ''Data Services''. It is actually a combination of the runtime and a [[web service]] through which the services are exposed. In addition, it also includes the '''Data Services Toolkit''' which lets Astoria Data Services be created from within [[ASP.NET]] itself. The Astoria project was announced at [[MIX (Microsoft)|MIX]] 2007, and the first developer preview was made available on April 30, 2007. The first [[Software release life cycle#Beta|CTP]] was made available as a part of the [[ASP.NET]] 3.5 Extensions Preview. The final version was released as part of [[Service Pack]] 1 of the [[.NET Framework 3.5]] on August 11, 2008.  The name change from ADO.NET Data Services to WCF data Services was announced at the 2009 [[Professional Developers Conference|PDC]].

==Overview==
WCF Data Services exposes data, represented as [[ADO.NET#Entity Framework|Entity Data Model]] (EDM) objects, via web services accessed over [[HTTP]]. The data can be addressed using a [[Representational State Transfer|REST]]-like [[URI]]. The data service, when accessed via the HTTP GET method with such a URI, will return the data. The web service can be configured to return the data in either plain [[XML]], [[JSON]] or [[Resource Description Framework|RDF+XML]]. In the initial release, formats like [[RSS]] and [[ATOM]] are not supported, though they may be in the future. In addition, using other HTTP methods like PUT, POST or DELETE, the data can be updated as well. POST can be used to create new entities, PUT for updating an entity, and DELETE for deleting an entity.

==Description==

Windows Communication Foundation (WCF) comes to the rescue when we find ourselves not able to achieve what we want to achieve using web services, i.e., other protocols support and even duplex communication. With WCF, we can define our service once and then configure it in such a way that it can be used via HTTP, TCP, IPC, and even Message Queues. We can consume Web Services using server side scripts (ASP.NET), JavaScript Object Notations (JSON), and even REST (Representational State Transfer).

'''Understanding the basics'''

When we say that a WCF service can be used to communicate using different protocols and from different kinds of applications, we will need to understand how we can achieve this. If we want to use a WCF service from an application, then we have three major questions:

'''1.'''Where is the WCF service located from a client's perspective?
'''2.'''How can a client access the service, i.e., protocols and message formats?
'''3.'''What is the functionality that a service is providing to the clients?

Once we have the answer to these three questions, then creating and consuming the WCF service will be a lot easier for us. The WCF service has the concept of endpoints. A WCF service provides endpoints which client applications can use to communicate with the WCF service. The answer to these above questions is what is known as the ABC of WCF services and in fact are the main components of a WCF service. So let's tackle each question one by one.

'''Address:''' Like a webservice, a WCF service also provides a URI which can be used by clients to get to the WCF service. This URI is called as the Address of the WCF service. This will solve the first problem of "where to locate the WCF service?" for us.

'''Binding:''' Once we are able to locate the WCF service, we should think about how to communicate with the service (protocol wise). The binding is what defines how the WCF service handles the communication. It could also define other communication parameters like message encoding, etc. This will solve the second problem of "how to communicate with the WCF service?" for us.

'''Contract:''' Now the only question we are left up with is about the functionalities that a WCF service provides. Contract is what defines the public data and interfaces that WCF service provides to the clients.


The URIs representing the data will contain the physical location of the service, as well as the service name. In addition, it will also need to specify an EDM Entity-Set or a specific entity instance, as in respectively
 &lt;nowiki&gt;http://dataserver/service.svc/MusicCollection&lt;/nowiki&gt;
or
 &lt;nowiki&gt;http://dataserver/service.svc/MusicCollection[SomeArtist]&lt;/nowiki&gt;
The former will list all entities in the ''Collection'' set whereas the latter will list only for the entity which is indexed by ''SomeArtist''.

In addition, the URIs can also specify a traversal of a relationship in the Entity Data Model. For example,
 &lt;nowiki&gt;http://dataserver/service.svc/MusicCollection[SomeSong]/Genre&lt;/nowiki&gt;
traverses the relationship ''Genre'' (in SQL parlance, joins with the ''Genre'' table) and retrieves all instances of ''Genre'' that are associated with the entity ''SomeSong''. Simple predicates can also be specified in the URI, like
 &lt;nowiki&gt;http://dataserver/service.svc/MusicCollection[SomeArtist]/ReleaseDate[Year eq 2006]&lt;/nowiki&gt;
will fetch the items that are indexed by ''SomeArtist'' and had their ''release'' in ''2006''. Filtering and partition information can also be encoded in the URL as
 &lt;nowiki&gt;http://dataserver/service.svc/MusicCollection?$orderby=ReleaseDate&amp;$skip=100&amp;$top=50&lt;/nowiki&gt;
It is important to note that although the presence of skip and top keywords indicate paging support, in Data Services version 1 there is no method of determining the number of records available and thus impossible to determine how many pages there may be.  The [[Open Data Protocol|OData]] 2.0 spec adds support for the '''$count''' path segment (to return just a count of entities) and '''$inlineCount''' (to retrieve a page worth of entities and a total count without a separate round-trip....).&lt;ref&gt;http://msdn.microsoft.com/en-us/library/ee373845.aspx&lt;/ref&gt;

==References==
{{Reflist}}
{{Refbegin}}
* {{cite web | url = http://blogs.msdn.com/pablo/archive/2007/04/30/codename-astoria-data-services-for-the-web.aspx | title = Codename "Astoria": Data Services for the Web | accessdate = 2007-04-30}}
* [http://astoria.mslivelabs.com/ ADO.NET Data Services Framework (formerly "Project Astoria")]
{{Refend}}

==External links==
*[http://msdn.microsoft.com/en-us/library/cc907912.aspx Using Microsoft ADO.NET Data Services]
*[http://www.asp.net/downloads/3.5-extensions/ ASP.NET 3.5 Extensions Preview]
*[http://blogs.msdn.com/astoriateam/ ADO.NET Data Services (Project Astoria) Team Blog]
*[http://entmag.com/news/article.asp?EditorialsID=9105 Access Cloud Data with Astoria: ENT News Online]

{{.NET Framework}}

[[Category:Data management]]
[[Category:Web services]]
[[Category:ADO.NET Data Access technologies]]
[[Category:.NET Framework]]</text>
      <sha1>nsmfky1f7eu997ud8fz9vty1knfd3xx</sha1>
    </revision>
  </page>
  <page>
    <title>Data discovery</title>
    <ns>0</ns>
    <id>40008710</id>
    <revision>
      <id>754355898</id>
      <parentid>754355780</parentid>
      <timestamp>2016-12-12T05:53:17Z</timestamp>
      <contributor>
        <ip>220.225.34.211</ip>
      </contributor>
      <comment>/* Players */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2172" xml:space="preserve">'''Data discovery''' is a [[business intelligence]] architecture aimed at creating and using interactive reports and explorable [[data]] from multiple sources. According to the [[United States]] information technology research and advisory firm [[Gartner]] "Data discovery has become a mainstream architecture in 2012".&lt;ref&gt;Kern, J., (2013), [http://www.information-management.com/news/data-discovery-saas-lead-bi-market-review-10024484-1.html Data Discovery, SaaS Lead BI Market Review], ''Information Management''/&lt;/ref&gt;

==Definition==
Data discovery is a user-driven process of searching for patterns or specific items in a data set.  Data discovery applications use visual tools such as geographical maps, pivot-tables, and heat-maps to make the process of finding patterns or specific items rapid and intuitive.  Data discovery may leverage statistical and [[data mining]] techniques to accomplish these goals.

==Data discovery and business intelligence (BI)==
Data discovery is a type of [[business intelligence]] in that they both provide the end-user with an application that visualizes [[data]].  Traditional BI covered dashboards, static and parameterized reports, and pivot tables.  Visualization of data in traditional BI incorporated standard charting, KPIs, and limited graphical representation and interactivity. BI is undergoing transformation in capabilities it offers, with a focus on end-user data analysis and discovery, access to larger volumes of data and an ability to create high fidelity presentations of information. 

==Players==
Data Discovery vendors include: [[Tableau_Software|Tableau]], [[Qlik]],  [[TIBCO_Software|TIBCO Spotfire]], Microsoft Power BI, [[MicroStrategy]], SAP (Lumira), Platfora, Datameer, ClearStory Data, [[AnswerRocket]], and Datawatch.&lt;ref&gt;The Rise of Data Discovery Has Set the Stage for a Major Strategic Shift in the BI and Analytics Platform Market 
15 June 2015 G00277789 
Analyst(s): Josh Parenteau | Rita L. Sallam | Cindi Howson 
&lt;/ref&gt;

==See also==
* [[Business intelligence]]
* [[Business intelligence tools]]

==References==
{{Reflist|30em}}

[[Category:Business intelligence]]
[[Category:Data management]]</text>
      <sha1>h5y914vy4swbi06z5d2jng2t77qal1m</sha1>
    </revision>
  </page>
  <page>
    <title>Small Data: The Tiny Clues that Uncover Huge Trends</title>
    <ns>0</ns>
    <id>49959657</id>
    <revision>
      <id>747019496</id>
      <parentid>728578353</parentid>
      <timestamp>2016-10-31T00:00:32Z</timestamp>
      <contributor>
        <username>Filedelinkerbot</username>
        <id>20611691</id>
      </contributor>
      <comment>Bot: Removing [[Commons:File:Small Data Cover.JPG]] ([[:File:Small Data Cover.JPG|en]]). It was deleted on Commons by [[:Commons:User:INeverCry|INeverCry]] (per [[Commons:Commons:Deletion requests/Files of User:Pookiegalore]]).</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2129" xml:space="preserve">{{npov|date=May 2016}}

{{Infobox book
| name = Small Data: The Tiny Clues That Uncover Huge Trends
| author = Martin Lindstrom
| language = English
| country = United States
| publisher = St. Martins
| isbn = 978-1250080684
}}
Small Data: the Tiny Clues that Uncover Huge Trends is [[Martin Lindstrom]]'s seventh book. It chronicles his work as a branding expert, working with consumers across the world to better understand their behavior. The theory behind the book is that businesses can better create products and services based on observing consumer behavior in their homes, as opposed to relying solely on [[big data]].

== Content ==
The book is based on a several year period of consumer studies for major corporations across the globe.&lt;ref&gt;{{Cite web|url=https://www.martinlindstrom.com/small-data/|title=Small Data - Martin Lindstrom - Bestselling Author|website=MartinLindstrom.com - Martin Lindstrom - Branding Expert &amp; Consultant|language=en-US|access-date=2016-03-27}}&lt;/ref&gt; It features case studies of the author's work interviewing consumers in their homes and using his observations to create hypotheses as to why they use products the way that they do.

== Public Reception ==

The book was a New York Times Bestseller&lt;ref&gt;{{Cite web|url=http://www.nytimes.com/best-sellers-books/2016-03-13/advice-how-to-and-miscellaneous/list.html|title=Best Sellers - The New York Times|website=www.nytimes.com|access-date=2016-03-27}}&lt;/ref&gt; upon release and was positively reviewed on several websites, Including [[Entrepreneur (magazine)|Entrepreneur]]&lt;ref&gt;{{Cite web|url=http://www.entrepreneur.com/article/271992|title=From an Elon Musk Bio to Malcolm Gladwell's 'Blink', These 9 Books Are Must-Reads|last=Agius|first=Aaron|website=Entrepreneur|access-date=2016-03-27}}&lt;/ref&gt; and [[Forbes]]&lt;ref&gt;{{Cite web|url=http://www.forbes.com/sites/davidburkus/2016/01/10/16-must-read-business-books-for-2016/#4ce073648bae|title=16 Must-Read Business Books For 2016|website=Forbes|access-date=2016-03-27}}&lt;/ref&gt;

==References==
{{Reflist}}



[[Category:2016 books]]
[[Category:2016 non-fiction books]]
[[Category:Data management]]</text>
      <sha1>ofsjz227aeku4sa3cwtz1up5fflm3zz</sha1>
    </revision>
  </page>
  <page>
    <title>Cambridge Semantics</title>
    <ns>0</ns>
    <id>50373790</id>
    <revision>
      <id>761892340</id>
      <parentid>749270743</parentid>
      <timestamp>2017-01-25T12:13:27Z</timestamp>
      <contributor>
        <username>Mitch Ames</username>
        <id>6326132</id>
      </contributor>
      <comment>Remove supercategory of existing category per [[WP:SUBCAT]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10385" xml:space="preserve">{{Infobox company
|name             = Cambridge Semantics
|logo             =
|logo_size        =200
|type             = [[Private company|Private]]
|genre            =
|fate             =
|predecessor      =
|successor        =
|foundation       = 2007
|founder          = Sean Martin&lt;br&gt;Lee Feigenbaum&lt;br&gt;Simon Martin&lt;br&gt;Emmett Eldred
|defunct          =
|location_city    = [[Boston, MA]]
|location_country = [[United States]]
|locations        = (2) [[Boston, MA]] &amp; [[San Diego, CA]]
|area_served      =
|key_people       =  Chuck Pieper (CEO)&lt;br&gt;Alok Prasad (President)
|industry         =  [[Computer Software]]
|products         =
|production       =
|services         =
|revenue          = 
|operating_income =
|net_income       =
|aum              =
|assets           =
|equity           =
|slogan           = The Smart Data Company
|owner            =
|num_employees    =
|parent           =
|divisions        =
|subsid           =
|homepage         =  {{URL|CambridgeSemantics.com}} 
|footnotes        =
|intl             =
}}

'''Cambridge Semantics''' is a privately held company headquartered in [[Boston, Massachusetts]] with a West Coast office in [[San Diego, California]]. The company develops and sells a suite of smart data products for Data Management, Data Discovery and Enterprise Analytics.

==History==

Cambridge Semantics was founded in 2007 by Sean Martin, Lee Feigenbaum, Simon Martin, Rouben Meschian and Emmett Eldred who all previously worked at [[IBM]]'s Advanced Technology Internet Group.&lt;ref&gt;{{cite web|last1=Lynch|first1=Brendan|website=[[Boston Business Journal]]|title=Ex-IBMers aim at better search tech|url=http://www.bizjournals.com/boston/blog/mass-high-tech/2008/03/ex-ibmers-aim-at-better-search-tech.html|accessdate=27 April 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last1=Resende|first1=Patricia|title=With explosion of big data comes big growth for Cambridge Semantics|url=http://www.bizjournals.com/boston/blog/techflash/2015/02/with-explosion-of-big-data-comes-big-growth-for.html|website=[[Boston Business Journal]]|accessdate=27 April 2016}}&lt;/ref&gt;

In 2012, Cambridge Semantics appointed Chuck Pieper as Chief Executive Officer. Prior to joining Cambridge Semantics, Pieper was Vice Chairman of Alternative Investments and Managing Director of [[Credit Suisse]] within the Asset Management Division.&lt;ref&gt;{{cite web|last1=Seiffert|first1=Don|title=Chuck Pieper named CEO at Cambridge Semantics|url=http://www.bizjournals.com/boston/blog/techflash/2012/12/chuck-pieper-named-ceo-at-cambridge.html|website=[[Boston Business Journal]]|accessdate=27 April 2016}}&lt;/ref&gt;

In 2015, Cambridge Semantics formed an alliance with [[MarkLogic]].&lt;ref&gt;{{cite web|title=Cambridge Semantics and MarkLogic Partner to Advance Semantic-Driven Data Management|url=http://www.dbta.com/Editorial/News-Flashes/Cambridge-Semantics-and-MarkLogic-Partner-to-Advance-Semantic-Driven-Data-Management-106569.aspx|website=Dbta.com|accessdate=27 April 2016|language=en-US|date=24 September 2015}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=MarkLogic, Cambridge Semantics partner for NoSQL|url=http://www.kmworld.com/Articles/News/News/MarkLogic-Cambridge-Semantics-partner-for-NoSQL-106568.aspx|website=[[KMWorld|KMWorld Magazine]]|accessdate=27 April 2016|language=en-US|date=24 September 2015}}&lt;/ref&gt;

In January 2016, Cambridge Semantics acquired SPARQL City and its [[graph database]] [[intellectual property]].&lt;ref&gt;{{cite web|last1=Leopold|first1=George|title=Cambridge Semantics Buys Graph Database Specialist|url=http://www.datanami.com/2016/01/14/cambridge-semantics-buys-graph-database-specialist/|website=Datanami|accessdate=27 April 2016|date=14 January 2016}}&lt;/ref&gt;

==Products==
* Anzo Smart Data Platform is a platform for building unified information solutions based on a set of open data standards implemented using [[Semantic Web |Semantic Web Technologies]].&lt;ref&gt;{{cite web|last1=Bertolucci|first1=Jeff|title=Big Data + Semantic Web: Love At First Terabyte? - InformationWeek|url=http://www.informationweek.com/big-data/big-data-analytics/big-data-+-semantic-web-love-at-first-terabyte/d/d-id/1107520?|website=[[InformationWeek]]|accessdate=28 April 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last1=Shacklett|first1=Mary|title=A start to solving the enterprise data usage problem - TechRepublic|url=http://www.techrepublic.com/article/a-start-to-solving-the-enterprise-data-usage-problem/|website=[[TechRepublic]]|accessdate=28 April 2016}}&lt;/ref&gt; It allows IT departments and their business users to quickly and flexibly access all of their diverse data for breakthrough insights.&lt;ref&gt;{{cite web|last1=Lawson|first1=Loraine|title=Cambridge Semantics Offers New Integration Tool|url=http://www.itbusinessedge.com/blogs/integration/cambridge-semantics-offers-new-integration-tool.html|website=IT Business Edge|accessdate=27 April 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Cambridge Semantics Launches Anzo Smart Data Integration|url=http://www.econtentmag.com/Articles/News/News-Item/Cambridge-Semantics-Launches-Anzo-Smart-Data-Integration-98007.htm|website=EContent Magazine|accessdate=27 April 2016|language=en-US|date=3 July 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=The time for Smart Data has finally arrived: Cambridge Semantics Inc.|url=http://thesiliconreview.com/magazines/the-time-for-smart-data-has-finally-arrived-cambridge-semantics-inc/|website=The Silicon Review|accessdate=27 April 2016|language=en-US}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last1=Kutz|first1=Erin|title=Cambridge Semantics, Looking to Put Microsoft Excel "On Steroids," Brings Intelligent Data Sorting to Non-Techies|url=http://www.xconomy.com/boston/2010/07/08/cambridge-semantics-looking-to-put-microsoft-excel-on-steroids-brings-intelligent-data-sorting-to-non-techies/|website=[[Xconomy]]|accessdate=27 April 2016|language=en-US|date=8 July 2010}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last1=McNamara|first1=Paul|title=Book of Odds opening eyes to new probabilities|url=http://www.networkworld.com/article/2231870/data-center/book-of-odds-opening-eyes-to-new-probabilities.html|website=[[Network World]]|accessdate=28 April 2016}}&lt;/ref&gt;
* Anzo Smart Data Manager
* Anzo Graph Query Engine
* Anzo Smart Data Lake

==Awards and recognition==

* Cambridge Semantics named [[Software and Information Industry Association|SIIA]] [[CODiE Award]] 2016 finalist.&lt;ref&gt;{{cite web|title=2016 Finalists|url=https://www.siia.net/codie/2016-Finalists|website=Siia.net|accessdate=27 April 2016}}&lt;/ref&gt;
* Cambridge Semantics named [[KMWorld]]’s 2016 ‘100 Companies That Matter in Knowledge Management’&lt;ref&gt;{{cite web|title=KMWorld 100 COMPANIES That Matter in Knowledge Management|url=http://www.kmworld.com/Articles/Editorial/Features/KMWorld-100-COMPANIES-That-Matter-in-Knowledge-Management-109344.aspx|website=[[KMWorld|KMWorld Magazine]]|accessdate=27 April 2016|language=en-US|date=1 March 2016}}&lt;/ref&gt; and [[KMWorld]] Trend-Setting Products of 2015.&lt;ref&gt;{{cite web|last1=McKellar|first1=Hugh|title=KMWorld Trend-Setting Products of 2015|url=http://www.kmworld.com/Articles/Editorial/Features/KMWorld-Trend-Setting-Products-of-2015-105783.aspx|website=[[KMWorld|KMWorld Magazine]]|accessdate=27 April 2016|language=en-US|date=1 September 2015}}&lt;/ref&gt;
* Cambridge Semantics named  2016 Bio-IT World Best of Show People's Choice Award Contenders&lt;ref&gt;{{cite web|title=2016 Bio-IT World Best of Show People's Choice Award Contenders|url=http://www.bio-itworld.com/2016/3/29/2016-best-of-show-peoples-choice-award-contenders.asp|website=Bio-IT World|accessdate=27 April 2016}}&lt;/ref&gt; and 2015 Bio-IT best of show finalist.&lt;ref&gt;{{cite web|title=Bio-IT World Recognizes 2015 Best of Show Winners|url=http://www.bio-itworld.com/2015/4/27/bio-it-world-recognizes-2015-best-of-show-winners.html|website=Bio-IT World|accessdate=27 April 2016}}&lt;/ref&gt;
* CIO Review Recognizes Cambridge Semantics as 2015 Top 20 Tech Solution Provider for [[Pharmaceutical industry|Pharma and Life Sciences Industry]].&lt;ref&gt;{{cite web|title=20  Most Promising Pharma and Life Sciences Tech Solution Providers  20 15|url=http://pharma-life-sciences.cioreview.com/vendors/2015/20special1|website=CIOReview|accessdate=27 April 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Cambridge Semantics:Smart Data Management and Advanced Analytics for Pharma and Life Sciences|url=http://pharma-life-sciences.cioreview.com/vendor/2015/cambridge_semantics|website=CIOReview|accessdate=27 April 2016}}&lt;/ref&gt;
* Anzo Insider Trading Investigation and Surveillance named 2015 [[CODiE Award]] finalist.&lt;ref&gt;{{cite web|title=Finalists - 2015 SIIA CODiE Awards|url=https://www.siia.net/archive/codies/2015/finalists.asp|website=Siia.net|accessdate=27 April 2016}}&lt;/ref&gt;
* Cambridge Semantics Selected as Finalist for 2014 [[MIT Sloan]] CIO Symposium's Innovation Showcase.&lt;ref&gt;{{cite web|title=Lead Your Digital Enterprise Mit Sloan Cio|url=http://www.mitcio.com/wp-content/uploads/2015/12/mitcio_2014.pdf|accessdate=27 April 2016}}&lt;/ref&gt;
* Cambridge Semantics named [[Software and Information Industry Association|SIIA]] [[CODiE Award]] 2014 finalist.&lt;ref&gt;{{cite web|title=Finalists - 2014 SIIA CODiE Awards |url=http://archive.siia.net/codies/2014/finalist_detail.asp?ID=3 |website=Siia.net |accessdate=27 April 2016 }}{{dead link|date=November 2016 |bot=InternetArchiveBot |fix-attempted=yes }}&lt;/ref&gt;
* Cambridge Semantics Win 2013 [[Software and Information Industry Association|SIIA]] [[CODiE Award]] for best business intelligence and analytics solution.&lt;ref&gt;{{cite web|title=2013 CODiE Award Winners|url=http://www.siia.net/archive/codies/2015/pw_2013.asp|website=Siia.net|accessdate=27 April 2016}}&lt;/ref&gt;
* Cambridge Semantics wins [[KMWorld]] 2012 Promise Award.&lt;ref&gt;{{cite web|title=KMWorld Promise Award Winner|url=http://www.kmworld.com/Articles/Editorial/Features/KMWorld-2012-Promise-and-Reality-award-winners-and-finalists-85829.aspx|website=KMWorld Magazine|accessdate=27 April 2016|language=en-US|date=30 October 2012}}&lt;/ref&gt;
* Cambridge Semantics wins Best of Show at 2012 Bio-IT World Conference.&lt;ref&gt;{{cite web|title=2012 Best of Show Winners|url=http://www.bio-itworld.com/2012/04/26/2012-best-of-show-winners.html|website=Bio-IT World|accessdate=27 April 2016}}&lt;/ref&gt;

==References==
{{reflist|2}}

==External links==
* [https://www.cambridgesemantics.com/ Official website]

[[Category:Software companies based in Massachusetts]]
[[Category:Companies established in 2007]]
[[Category:Data management]]</text>
      <sha1>7xdigwvbfim1lue2b35sanurscm8ql9</sha1>
    </revision>
  </page>
  <page>
    <title>Database normalization</title>
    <ns>0</ns>
    <id>8640</id>
    <revision>
      <id>762261030</id>
      <parentid>759999494</parentid>
      <timestamp>2017-01-27T18:03:37Z</timestamp>
      <contributor>
        <ip>91.146.162.142</ip>
      </contributor>
      <comment>A list is better than prose.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="13286" xml:space="preserve">'''Database normalization''', or simply '''normalization''', is the process of organizing the [[column (database)|columns]] (attributes) and [[table (database)|tables]] (relations) of a [[relational database]] to reduce [[data redundancy]] and improve data integrity.

Normalization involves arranging attributes in tables based on [[Dependency theory (database theory)|dependencies]] between attributes, ensuring that the dependencies are properly enforced by database integrity constraints. Normalization is accomplished through applying some formal rules either by a process of synthesis or decomposition. Synthesis creates a normalized database design based on a known set of dependencies. Decomposition takes an existing (insufficiently normalized) database design and improves it based on the known set of dependencies.

[[Edgar F. Codd]], the inventor of the [[relational model]] (RM), introduced the concept of normalization and what we now know as the [[First normal form]] (1NF) in 1970.&lt;ref name="Codd1970"&gt;{{cite journal|first=E. F.|last=Codd|authorlink=E.F. Codd|title=A Relational Model of Data for Large Shared Data Banks|journal=[[Communications of the ACM]]|volume=13|issue=6|date=June 1970|pages=377–387|url=http://www.acm.org/classics/nov95/toc.html | doi = 10.1145/362384.362685}}&lt;/ref&gt; Codd went on to define the [[Second normal form]] (2NF) and [[Third normal form]] (3NF) in 1971,&lt;ref name="Codd, E.F 1971"&gt;Codd, E.F. "Further Normalization of the Data Base Relational Model". (Presented at Courant Computer Science Symposia Series 6, "Data Base Systems", New York City, May 24–25, 1971.) IBM Research Report RJ909 (August 31, 1971). Republished in Randall J. Rustin (ed.), ''Data Base Systems: Courant Computer Science Symposia Series 6''. Prentice-Hall, 1972.&lt;/ref&gt; and Codd and [[Raymond F. Boyce]] defined the Boyce-Codd Normal Form ([[Boyce–Codd normal form|BCNF]]) in 1974.&lt;ref name="CoddBCNF"&gt;Codd, E. F. "Recent Investigations into Relational Data Base Systems". IBM Research Report RJ1385 (April 23, 1974). Republished in ''Proc. 1974 Congress'' (Stockholm, Sweden, 1974). , N.Y.: North-Holland (1974).&lt;/ref&gt; Informally, a relational database table is often described as "normalized" if it meets Third Normal Form.&lt;ref name="DateIntroDBSys"&gt;C.J. Date.  ''An Introduction to Database Systems''. Addison-Wesley (1999), p. 290&lt;/ref&gt;  Most 3NF tables are free of insertion, update, and deletion anomalies.


==Objectives==
A basic objective of the [[first normal form]] defined by Codd in 1970 was to permit data to be queried and manipulated using a "universal data sub-language" grounded in [[first-order logic]].&lt;ref&gt;"The adoption of a relational model of data ... permits the development of a universal data sub-language based on an applied predicate calculus. A first-order predicate calculus suffices if the collection of relations is in first normal form. Such a language would provide a yardstick of linguistic power for all other proposed data languages, and would itself be a strong candidate for embedding (with appropriate syntactic modification) in a variety of host Ianguages (programming, command- or problem-oriented)."  Codd, [http://www.acm.org/classics/nov95/toc.html "A Relational Model of Data for Large Shared Data Banks"], p. 381&lt;/ref&gt; ([[SQL]] is an example of such a data sub-language, albeit one that Codd regarded as seriously flawed.)&lt;ref&gt;Codd, E.F.  Chapter 23, "Serious Flaws in SQL", in ''The Relational Model for Database Management: Version 2''. Addison-Wesley (1990), pp. 371–389&lt;/ref&gt;

The objectives of normalization beyond 1NF (First Normal Form) were stated as follows by Codd:

{{Quotation|
# To free the collection of relations from undesirable insertion, update and deletion dependencies;
# To reduce the need for restructuring the collection of relations, as new types of data are introduced, and thus increase the life span of application programs;
# To make the relational model more informative to users;
# To make the collection of relations neutral to the query statistics, where these statistics are liable to change as time goes by.
|E.F. Codd|"Further Normalization of the Data Base Relational Model"&lt;ref&gt;Codd, E.F. "Further Normalization of the Data Base Relational Model", p. 34&lt;/ref&gt;}}

The sections below give details of each of these objectives.

===Free the database of modification anomalies===
[[File:Update anomaly.svg|280px|thumb|right|An '''update anomaly'''. Employee 519 is shown as having different addresses on different records.]]
[[File:Insertion anomaly.svg|280px|thumb|right|An '''insertion anomaly'''. Until the new faculty member, Dr. Newsome, is assigned to teach at least one course, his details cannot be recorded.]]
[[File:Deletion anomaly.svg|280px|thumb|right|A '''deletion anomaly'''. All information about Dr. Giddens is lost if he temporarily ceases to be assigned to any courses.]]
When an attempt is made to modify (update, insert into, or delete from) a table, undesired side-effects may arise in tables that have not been sufficiently normalized. An insufficiently normalized table might have one or more of the following characteristics:

* The same information can be expressed  on multiple rows; therefore updates to the table may result in logical inconsistencies. For example, each record in an "Employees' Skills" table might contain an Employee ID, Employee Address, and Skill; thus a change of address for a particular employee will potentially need to be applied to multiple records (one for each skill). If the update is not carried through successfully—if, that is, the employee's address is updated on some records but not others—then the table is left in an inconsistent state. Specifically, the table provides conflicting answers to the question of what this particular employee's address is. This phenomenon is known as an '''update anomaly'''.
* There are circumstances in which certain facts cannot be recorded at all. For example, each record in a "Faculty and Their Courses" table might contain a Faculty ID, Faculty Name, Faculty Hire Date, and Course Code—thus we can record the details of any faculty member who teaches at least one course, but we cannot record the details of a newly hired faculty member who has not yet been assigned to teach any courses except by setting the Course Code to null. This phenomenon is known as an '''insertion anomaly'''.
* Under certain circumstances, deletion of data representing certain facts necessitates deletion of data representing completely different facts. The "Faculty and Their Courses" table described in the previous example suffers from this type of anomaly, for if a faculty member temporarily ceases to be assigned to any courses, we must delete the last of the records on which that faculty member appears, effectively also deleting the faculty member, unless we set the Course Code to null in the record itself.  This phenomenon is known as a '''deletion anomaly'''.

===Minimize redesign when extending the database structure===
When a fully normalized database structure is extended to allow it to accommodate new types of data, the pre-existing aspects of the database structure can remain largely or entirely unchanged. As a result, applications interacting with the database are minimally affected.

Normalized tables, and the relationship between one normalized table and another, mirror real-world concepts and their interrelationships.

===Example===
Querying and manipulating the data within a data structure that is not normalized, such as the following non-1NF representation of customers, credit card transactions, involves more complexity than is really necessary:

{| class="wikitable"
! Customer !! Cust. ID !! Transactions
|-
| Jones || 1
|| 
{| class="wikitable"
! Tr. ID !! Date !! Amount
|-
| 12890
| 14-Oct-2003
| &amp;minus;87
|-
| 12904
| 15-Oct-2003
| &amp;minus;50
|}
|-
| Wilkins || 2
|| 
{| class="wikitable"
! Tr. ID !! Date !! Amount
|-
| 12898
| 14-Oct-2003
| &amp;minus;21
|}
|-
| Stevens || 3
|| 
{| class="wikitable"
! Tr. ID !! Date !! Amount
|-
| 12907
| 15-Oct-2003
| &amp;minus;18
|-
| 14920
| 20-Nov-2003
| &amp;minus;70
|-
| 15003
| 27-Nov-2003
| &amp;minus;60
|}
|}
&lt;br&gt;
To each customer corresponds a ''repeating group'' of transactions.  The automated evaluation of any query relating to customers' transactions therefore would broadly involve two stages:
# Unpacking one or more customers' groups of transactions allowing the individual transactions in a group to be examined, and
# Deriving a query result based on the results of the first stage

For example, in order to find out the monetary sum of all transactions that occurred in October 2003 for all customers, the system would have to know that it must first unpack the ''Transactions'' group of each customer, then sum the ''Amounts'' of all transactions thus obtained where the ''Date'' of the transaction falls in October 2003.

One of Codd's important insights was that this structural complexity could always be removed completely, leading to much greater power and flexibility in the way queries could be formulated (by [[user (computing)|users]] and [[application software|applications]]) and evaluated (by the [[database management system|DBMS]]).  The normalized equivalent of the structure above would look like this:

{| class="wikitable"
|-
! Customer !! Cust. ID
|-
| Jones || 1
|-
| Wilkins || 2
|-
| Stevens || 3
|}

{| class="wikitable"
|-
! Cust. ID !! Tr. ID !! Date !! Amount
|-
| 1 || 12890 || 14-Oct-2003 || &amp;minus;87
|-
| 1 || 12904 || 15-Oct-2003 || &amp;minus;50
|-
| 2 || 12898 || 14-Oct-2003 || &amp;minus;21
|-
| 3 || 12907 || 15-Oct-2003 || &amp;minus;18
|-
| 3 || 14920 || 20-Nov-2003 || &amp;minus;70
|-
| 3 || 15003 || 27-Nov-2003 || &amp;minus;60
|}

In the modified structure, the keys are {Customer} and {Cust. ID} in the first table, {Cust. ID, Tr ID} in the second table.

Now each row represents an individual credit card transaction, and the DBMS can obtain the answer of interest, simply by finding all rows with a Date falling in October, and summing their Amounts.  The data structure places all of the values on an equal footing, exposing each to the DBMS directly, so each can potentially participate directly in queries; whereas in the previous situation some values were embedded in lower-level structures that had to be handled specially.  Accordingly, the normalized design lends itself to general-purpose query processing, whereas the unnormalized design does not. The normalized version also allows the user to change the customer name in one place and guards against errors that arise if the customer name is misspelled on some records.

==List of Normal Forms==
* UNF - "[[Denormalization|Unnormalized]] Form"
* [[First normal form|1NF - First Normal Form]]
* [[Second normal form|2NF - Second Normal Form]]
* [[Third normal form|3NF - Third Normal Form]]
* [[Elementary Key Normal Form|EKNF - Elementary Key Normal Form]]
* [[Boyce–Codd normal form|BCNF - Boyce–Codd Normal Form]]
* [[Fourth normal form|4NF - Fourth Normal Form]]
* [http://researcher.watson.ibm.com/researcher/files/us-fagin/icdt12.pdf ETNF - Essential Tuple Normal Form]
* [[Fifth normal form|5NF - Fifth Normal Form]]
* [[Sixth normal form|6NF - Sixth Normal Form]]
* [[Domain/key normal form|DKNF - Domain/Key Normal Form]]

==See also==
*[[Refactoring]]

==Notes and references==
{{reflist|2}}
{{refbegin}}
{{refend}}

==Further reading==
* Date, C. J. (1999), ''[http://www.aw-bc.com/catalog/academic/product/0,1144,0321197844,00.html  An Introduction to Database Systems]'' (8th ed.). Addison-Wesley Longman. ISBN 0-321-19784-4.
* Kent, W. (1983) ''[http://www.bkent.net/Doc/simple5.htm A Simple Guide to Five Normal Forms in Relational Database Theory]'', Communications of the ACM, vol. 26, pp.&amp;nbsp;120–125
* H.-J. Schek, P. Pistor Data Structures for an Integrated Data Base Management and Information Retrieval System

==External links==
* [http://databases.about.com/od/specificproducts/a/normalization.htm Database Normalization Basics] by Mike Chapple (About.com)
* [http://www.databasejournal.com/sqletc/article.php/1428511 Database Normalization Intro], [http://www.databasejournal.com/sqletc/article.php/26861_1474411_1 Part 2]
* [http://mikehillyer.com/articles/an-introduction-to-database-normalization/ An Introduction to Database Normalization] by Mike Hillyer.
* [http://phlonx.com/resources/nf3/ A tutorial on the first 3 normal forms] by Fred Coulson
* [http://www.dbnormalization.com/ DB Normalization Examples]
* [http://support.microsoft.com/kb/283878 Description of the database normalization basics] by Microsoft
* [http://www.barrywise.com/2008/01/database-normalization-and-design-techniques/ Database Normalization and Design Techniques] by Barry Wise, recommended reading for the Harvard MIS.
* [http://www.bkent.net/Doc/simple5.htm A Simple Guide to Five Normal Forms in Relational Database Theory]
* [http://beginnersbook.com/2015/05/normalization-in-dbms/ Normalization in DBMS by Chaitanya (beginnersbook.com)]

{{Database normalization}}
{{Database}}
{{Databases}}

{{DEFAULTSORT:Database Normalization}}
[[Category:Database normalization| ]]
[[Category:Database constraints]]
[[Category:Data management]]
[[Category:Data modeling]]
[[Category:Relational algebra]]</text>
      <sha1>7rmmnbjsmcu6l0xscyvpuotw16rnwa3</sha1>
    </revision>
  </page>
  <page>
    <title>Physical data model</title>
    <ns>0</ns>
    <id>1030540</id>
    <revision>
      <id>749352529</id>
      <parentid>738527426</parentid>
      <timestamp>2016-11-13T22:29:58Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor />
      <comment>1 archive template merged to {{[[template:webarchive|webarchive]]}} ([[User:Green_Cardamom/Webarchive_template_merge|WAM]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4043" xml:space="preserve">{{Refimprove|date=April 2008}}
[[File:Physical Data Model Options.jpg|thumb|320px|Physical Data Model Options.&lt;ref name="WH05"&gt;[http://georgewbush-whitehouse.archives.gov/omb/egov/documents/CRM.PDF FEA Consolidated Reference Model Document]. whitehouse.gov May 2005. p.91.  {{webarchive |url=https://web.archive.org/web/20100705040628/http://georgewbush-whitehouse.archives.gov/omb/egov/documents/CRM.PDF |date=July 5, 2010 }}&lt;/ref&gt;]]

A '''physical data model''' (or '''[[database design]]''') is a representation of a data design as implemented, or intended to be implemented, in a [[database management system]].  In the [[Project lifecycle | lifecycle of a project]] it typically derives from a [[logical data model]], though it may be [[reverse-engineer]]ed from a given [[database]] implementation.  A complete physical data model will include all the [[database artifact]]s required to create [[relationships between table]]s or to achieve performance goals, such as [[index (database)|index]]es, constraint definitions, linking tables, [[partitioned table]]s or [[cluster (computing)|cluster]]s.  Analysts can usually use a  physical data model to calculate storage estimates; it may include specific storage allocation details for a given database system.

{{As of | 2012}} seven main databases dominate the commercial marketplace: [[Informix Dynamic Server|Informix]], [[Oracle Database|Oracle]], [[PostgreSQL|Postgres]], [[Microsoft SQL Server|SQL Server]], [[Sybase]], [[IBM DB2|DB2]] and [[MySQL]]. Other RDBMS systems tend either to be legacy databases or used within academia such as universities or further education colleges. Physical data models for each implementation would differ significantly, not least due to underlying [[operating system | operating-system]] requirements that may sit underneath them.  For example: SQL Server runs only on [[Microsoft Windows]] operating-systems, while Oracle and MySQL can run on Solaris, Linux and other UNIX-based operating-systems as well as on Windows. This means that the disk requirements, security requirements and many other aspects of a physical data model will be influenced by the RDBMS that a [[database administrator]] (or an organization) chooses to use.

==Physical schema==
''Physical schema'' is a term used in [[data management]] to describe how [[data]] is to be represented and stored (files, indices, ''et al.'') in [[secondary storage]] using a particular [[database management system]] (DBMS) (e.g., Oracle RDBMS, Sybase SQL Server, etc.).

In the [[ANSI-SPARC Architecture|ANSI/SPARC Architecture]] [[three schema approach]], the ''internal schema'' is the view of data that involved data management technology.  This is as opposed to an ''external schema'' that reflects an individual's view of the data, or the ''[[conceptual schema]]'' that is the integration of a set of external schemas.

Subsequently{{Citation needed|date=June 2012}} the internal schema was recognized to have two parts:

The [[logical schema]] was the way data were represented to conform to the constraints of a particular approach to database management.  At that time the choices were hierarchical and network.  Describing the logical schema, however, still did not describe how physically data would be stored on disk drives.  That is the domain of the ''physical schema''.  Now logical schemas describe data in terms of relational ''tables and columns'', object-oriented ''classes'', and [[XML]] ''tags''.

A single set of tables, for example, can be implemented in numerous ways, up to and including an architecture where table rows are maintained on computers in different countries.

==See also==
*[[Database schema]]
*[[Logical schema]]

==References==
{{Reflist}}

{{DEFAULTSORT:Physical Data Model}}
[[Category:Data modeling]]
[[Category:Data management]]
[http://www.whitehouse.gov/sites/default/files/omb/assets/fea_docs/FEA_CRM_v23_Final_Oct_2007_Revised.pdf FEA Consolidated Reference Model Document] (whitehouse.gov) Oct 2007.

[[ja:スキーマ (データベース)]]</text>
      <sha1>g8bfysxvwtxvpkswduzugck0czz8erz</sha1>
    </revision>
  </page>
  <page>
    <title>StoredIQ</title>
    <ns>0</ns>
    <id>51073850</id>
    <revision>
      <id>741004297</id>
      <parentid>741004060</parentid>
      <timestamp>2016-09-24T19:16:07Z</timestamp>
      <contributor>
        <username>Eustachiusz</username>
        <id>20008731</id>
      </contributor>
      <comment>+[[Category:Data management]]; +[[Category:Data warehousing]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4398" xml:space="preserve">{{Underlinked|date=July 2016}}

&lt;!-- Don't mess with this line! --&gt;&lt;!-- Write your article below this line --&gt;
'''StoredIQ''' was a company founded for [[information lifecycle management]] (ILM) of unstructured data. Founded in 2001 as Deepfile&lt;ref&gt;{{cite news|title=Deepfile Comes to the Surface|url=http://www.networkcomputing.com/storage/deepfile-comes-surface/865316998|publisher=Network Computing}}&lt;/ref&gt; in [[Austin, Texas]] by Jeff Erramouspe, Jeff Bone, Russell Turpin, Rudy Rouhana, Laura Arbilla and Brett Funderburg.&lt;ref&gt;{{cite news|title=Enterprise file management made easy|url=http://www.networkworld.com/article/2332452/wireless/deepfile.html|publisher=Network World}}&lt;/ref&gt; The company changed its name in 2005 to StoredIQ&lt;ref&gt;{{cite news|title=Deep file Becomes StoredIQ|url=http://www.networkcomputing.com/storage/deepfile-becomes-storediq/1788209585|publisher=Network Computing}}&lt;/ref&gt; and continued to operate successfully for over a decade until it was acquired in 2012 by IBM.&lt;ref&gt;{{cite web|title=IBM Extends ILG Suite and Big Data Governance with StoredIQ Acquisition|url=http://public.dhe.ibm.com/software/data/sw-library/ecm-programs/Parity_Research_StoredIQ_Whitepaper.pdf|website=IBM}}&lt;/ref&gt; It now serves as a platform for IBM's information life cycle governance, [[big data]] governance and [[enterprise content management]] technologies.&lt;ref&gt;{{cite web|title=StoredIQ is now an IBM Company|url=https://www-01.ibm.com/software/info/storediq/|website=IBM}}&lt;/ref&gt;

StoredIQ was awarded five patents by the USPTO. The first, originally filed in 2003, enabled unstructured data in file systems to be manipulated in a similar way to information stored in databases.&lt;ref&gt;{{cite web|title=Method and apparatus for managing file systems and file-based data storage|url=http://patents.justia.com/assignee/storediq-inc|website=JUSTIA Patents}}&lt;/ref&gt; Subsequent patents only added to StoredIQ's market dominance by building upon the patented actionable file system with further enhancements specific to Enterprise Policy Management  and expanding the reach of StoredIQ's management capability all the way to individual desktops.&lt;ref&gt;{{cite web|title=Patents by Assignee Storediq, Inc.|url=http://patents.justia.com/assignee/storediq-inc|website=JUSTIA Patents}}&lt;/ref&gt;

In 2008 StoredIQ was recognized as "Best in Compliance" by Network Products Guide.&lt;ref&gt;{{cite web|title=StoredIQ Wins Network Products Guide Award For Best In Compliance|url=http://www.datastorageconnection.com/doc/storediq-network-products-best-in-compliance-0001|publisher=Data Storage Connection}}&lt;/ref&gt; At the same time, StoredIQ was being recognized as a "Top 5 Provider" by the prestigious Socha-Gelbmann eDiscovery survey.&lt;ref&gt;{{cite web|title=StoredIQ Recognized With "Top 5 Provider" Rating In Socha-Gelbmann eDiscovery Survey|url=http://www.datastorageconnection.com/doc/torediq-ediscovery-survey-storage-0001|publisher=Data Storage Connection}}&lt;/ref&gt; This incredible breath of information governance capability is what originally drew the attention of [[EMC Corporation]], StoredIQ's first potential acquirer. Initially a strategic investor in StoredIQ, many experts{{Who|date=August 2016}} predicted an inevitable acquisition. However, the company shunned their first suitor; leaving EMC to acquire a competitor.&lt;ref&gt;{{cite web|title=EMC Acquires Kazeon, Stiffs StoredIQ|url=http://www.informationweek.com/software/information-management/emc-acquires-kazeon-stiffs-storediq/d/d-id/1082836?|publisher=Information Week}}&lt;/ref&gt;

The company published a whitepaper titled ''The Truth About Big Data''. This promotion combined with StoredIQ's patented, technology led to [[IBM]] selecting StoredIQ as the basis for some products.&lt;ref&gt;{{cite news|last1=Butta|first1=Tom|title=The Truth Behind IBM’s Plans to Acquire Big Data Company, StoredIQ|url=http://www.huffingtonpost.com/entry/ibm-storediq_b_2377339|publisher=Huffington Post|date=2012-12-31}}&lt;/ref&gt;

==References==
{{reflist}}
&lt;!-- After listing your sources please cite them using inline citations and place them after the information they cite. Please see http://en.wikipedia.org/wiki/Wikipedia:REFB for instructions on how to add citations. --&gt;



[[Category:Companies established in 2001]]
[[Category:Companies based in Austin, Texas]]
[[Category:Information technology management]]
[[Category:Data management]]
[[Category:Data warehousing]]</text>
      <sha1>gkflrxv80n0q265ztsryte67dw3x0w9</sha1>
    </revision>
  </page>
  <page>
    <title>Data exhaust</title>
    <ns>0</ns>
    <id>51905821</id>
    <revision>
      <id>746686060</id>
      <parentid>744239518</parentid>
      <timestamp>2016-10-28T22:30:58Z</timestamp>
      <contributor>
        <username>Pegship</username>
        <id>355698</id>
      </contributor>
      <minor />
      <comment>stub sort</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1470" xml:space="preserve">{{Multiple issues|
{{Orphan|date=October 2016}}
{{refimprove|date=October 2016}}
}}

'''Data exhaust''' refers to the trail of [[data]] left by the activities of an [[Internet]] user during his/her online activity. An enormous amount of often raw data are created. These data (which can take the form of [[Cookie (computing)|cookies]], temporary files, [[log file]]s etc.) can help to improve the online experience (for example through customized content). But they can also compromise privacy, as they offer a valuable insight into the user’s habits. It can be used to improve tracking trends and studying data exhaust also improves the user interface and the layout design. &lt;ref name=techtarget&gt;{{cite web|url=http://whatis.techtarget.com/definition/data-exhaust|title=What is data exhaust? - Definition from WhatIs.com|publisher=}}&lt;/ref&gt;

Unlike primary content, these data are not purposefully created by the user, who is often unaware of their very existence. A bank for example would consider as [[primary data]] information concerning the sums and parties of a transaction, whilst [[secondary data]] might include the percentage of transactions carried out at a [[cash machine]] instead of a real bank.&lt;ref&gt;{{cite web|url=http://www.pcworld.com/article/3069507/5-things-you-need-to-know-about-data-exhaust.html|title=5 things you need to know about data exhaust|publisher=}}&lt;/ref&gt;

==References==
&lt;references /&gt;


[[Category:Data management]]

{{internet-stub}}</text>
      <sha1>j2kbfib0qmgr5gos9m9cplevmpe7bfg</sha1>
    </revision>
  </page>
  <page>
    <title>Object storage</title>
    <ns>0</ns>
    <id>40572678</id>
    <revision>
      <id>762727634</id>
      <parentid>760637203</parentid>
      <timestamp>2017-01-30T11:43:33Z</timestamp>
      <contributor>
        <ip>89.145.186.214</ip>
      </contributor>
      <comment>/* External links */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="33593" xml:space="preserve">'''Object storage''' (also known as '''object-based storage'''&lt;ref&gt;{{cite journal|last=Mesnier|first=Mike|author2=Gregory R. Ganger |author3=Erik Riedel |title=Object-Based Storage|journal=IEEE Communications Magazine|date=August 2003|pages=84–90|url=http://www.storagevisions.com/White%20Papers/MesnierIEEE03.pdf|accessdate=27 October 2013|doi=10.1109/mcom.2003.1222722 }}&lt;/ref&gt;) is a storage architecture that manages data as objects, as opposed to other storage architectures like [[file systems]] which manage data as a file hierarchy and [[block storage]] which manages data as blocks within sectors and tracks.&lt;ref&gt;{{cite web|last=Porter De Leon|first=Yadin|author2=Tony Piscopo|title=Object Storage versus Block Storage: Understanding the Technology Differences|url=http://www.druva.com/blog/object-storage-versus-block-storage-understanding-technology-differences/|publisher=Druva.com|accessdate=19 January 2015}}&lt;/ref&gt; Each object typically includes the data itself, a variable amount of [[metadata]], and a globally unique identifier. Object storage can be implemented at multiple levels, including the device level (object storage device), the system level, and the interface level. In each case, object storage seeks to enable capabilities not addressed by other storage architectures, like interfaces that can be directly programmable by the application, a namespace that can span multiple instances of physical hardware, and data management functions like data replication and data distribution at object-level granularity.

Object storage systems allow relatively inexpensive, scalable and self-healing retention of massive amounts of [[unstructured data]]. Object storage is used for diverse purposes such as storing photos on [[Facebook]], songs on [[Spotify]], or files in online collaboration services, such as [[Dropbox (service)|Dropbox]].&lt;ref&gt;{{cite web|authors=Chandrasekaran, Arun, Dayley, Alan|title=Critical Capabilities for Object Storage|publisher=Gartner Research|date=11 February 2014|url=http://www.gartner.com/technology/reprints.do?id=1-1R78PJ9&amp;ct=140226&amp;st=sb}}&lt;/ref&gt;

==History==

===Origins===
In 1995, new research by Garth Gibson, ''et al.'' on [[Network Attached Secure Disks]] first promoted the concept of splitting less common operations, like namespace manipulations, from common operations, like reads and writes, to optimize the performance and scale of both.&lt;ref name="NASD"&gt;{{cite web|title=File Server Scaling with Network-Attached Secure Disks|url=http://www.pdl.cmu.edu/ftp/NASD/Sigmetrics97.pdf|publisher=Proceedings of the ACM International Conference on Measurement and Modeling of Computer Systems (Sigmetrics ‘97)|accessdate=27 October 2013|author=Garth A. Gibson |author2=Nagle D. |author3=Amiri K. |author4=Chan F. |author5=Feinberg E. |author6=Gobioff H. |author7=Lee C. |author8=Ozceri B. |author9=Riedel E. |author10=Rochberg D. |author11=Zelenka J.}}&lt;/ref&gt;  In the same year, 1995, a Belgium company - FilePool - was established to build the basis for archiving functions by using those and own concepts. Object storage was proposed  at [[Carnegie Mellon University|Carnegie Mellon University's]] Parallel Data Lab as a research project in 1996 .&lt;ref&gt;{{cite web|last1=Factor|first1=Michael|last2=Meth|first2=K.|last3=Naor|first3=D.|last4=Rodeh|first4=O.|last5=Satran |first5=J.|title=Object Storage: The Future Building Block for Storage Systems|url=http://webhdd.ru/library/files/PositionOSD.pdf|publisher=IBM Haifa Research Labs|accessdate=26 September 2013}}&lt;/ref&gt;   Another key concept was abstracting the writes and reads of data to more flexible data containers (objects). Fine grained access control through object storage architecture&lt;ref&gt;{{cite web|title=Security for Network Attached Storage Devices (CMU-CS-97-185)|url=http://repository.cmu.edu/cgi/viewcontent.cgi?article=1147&amp;context=pdl|publisher=Parallel Data Laboratory|accessdate=7 November 2013|author=Gobioff, Howard|author2=Gibson, Garth A. |author3= Tygar, Doug |date=1 October 1997}}&lt;/ref&gt;  was further described by one of the NASD team, Howard Gobioff, who later was one of the inventors of the [[Google File System]].&lt;ref&gt;{{cite web|title=The Google File System|url=http://research.google.com/archive/gfs-sosp2003.pdf|publisher=Google|accessdate=7 November 2013|author=Sanjay Ghemawat |author2=Howard Gobioff |author3=Shun-Tak Leung|date=October 2003}}&lt;/ref&gt;  Other related work includes the [[Coda (file system)|Coda]] filesystem project at [[Carnegie Mellon]], which started in 1987, and spawned the [[Lustre (file system)|Lustre file system]].&lt;ref name="Lustre"&gt;{{cite web|last=Braam|first=Peter|title=Lustre: The intergalactic ﬁle system|url=http://ols.fedoraproject.org/OLS/Reprints-2002/braam-reprint.pdf|accessdate=17 September 2013}}&lt;/ref&gt; There is also the OceanStore project at UC Berkeley,&lt;ref&gt;{{cite web|title=OceanStore|url=http://oceanstore.cs.berkeley.edu/|accessdate=18 September 2013}}&lt;/ref&gt; which started in 1999.&lt;ref&gt;{{cite journal|last1=Kubiatowicz|first1=John|last2=Bindel|first2=D.|last3=Chen|first3=Y.|last4=Czerwinski|first4=S.|last5=Eaton|first5=P.|last6=Geels|first6=D.|last7=Gummadi|first7=R.|last8=Rhea|first8=S.|last9=Weatherspoon|first9=H.|last10=Weimer |first10=W.|last11=Wells|first11=C.|last12=Zhao|first12=B.|title=OceanStore: An Architecture for Global-Scale Persistent Storage|journal=Proceedings of the Ninth international Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS 2000)|date=November 2000|url=http://oceanstore.cs.berkeley.edu/publications/papers/pdf/asplos00.pdf|accessdate=18 September 2013}}&lt;/ref&gt;

One of the earliest and best-known object storage products, EMC's Centera, debuted in 2002.&lt;ref&gt;{{cite news|title=EMC Unveils Low-Cost Data-Storage Product|url=http://articles.latimes.com/2002/apr/30/business/fi-techbriefs30.3|accessdate=17 September 2013|newspaper=LA Times|date=April 30, 2002}}&lt;/ref&gt; [[Content-addressable storage|Centera's technology]]  has been developed at Filepool and the company had been acquired  by EMC² in 2002.

===Development===
Overall industry investment in object storage technology has been sustained for over a decade. From 1999 to 2013, there has been at least $300 million of venture financing related to object storage, including vendors like Amplidata, Bycast, Cleversafe, Cloudian, Nirvanix, and Scality.&lt;ref&gt;{{cite web|last=Leung|first=Leo|title=After 10 years, object storage investment continues and begins to bear significant fruit|url=http://blog.oxygencloud.com/2013/09/16/after-10-years-object-storage-investment-continues-and-begins-to-bear-significant-fruit/|accessdate=17 September 2013|date=16 September 2013}}&lt;/ref&gt; This doesn't include millions of dollars of private engineering from systems vendors like DataDirect Networks (WOS), [http://www.emc.com/en-us/storage/ecs/index.htm#collapse=&amp;tab14=0 Dell EMC Elastic Cloud Storage], Centera, [[Atmos]], HDS (HCP), HP ([[HP OpenStack]]), IBM, NetApp (StorageGRID), Redhat GlusterFS and [http://www.keepertech.com Keeper Technology] ([http://www.keepertech.com/products/keepersafe/ keeperSAFE]), cloud services vendors like Amazon ([[AWS S3]]), Microsoft ([[Microsoft Azure]]) and Google ([[Google Cloud Storage]]), or the many man years of open source development at [[Lustre (file system)|Lustre]], OpenStack ([[OpenStack#Object Storage .28Swift.29|Swift]]), MogileFS, [[Ceph (file system)|Ceph]], [[Skylable SX (object storage)|Skylable SX]] and OpenIO.&lt;ref name="Mellor"&gt;{{cite web|last=Mellor|first=Chris (Dec. 2, 2015)|title=Openio's objective is opening up object storage space|url=http://www.theregister.co.uk/2015/12/02/openio_object_storage_upstart/}}&lt;/ref&gt;&lt;ref name="Nicolas"&gt;{{cite web|last=Nicolas|first=Philippe (Oct. 2, 2015)|title=OpenIO, ready to take off|url=http://filestorage.blogspot.fr/2015/10/openio-ready-to-take-off.html/}}&lt;/ref&gt;&lt;ref name="Raffo"&gt;{{cite web|last=Raffo|first=Dave (May 20, 2016)|title=OpenIO joins object storage cloud scrum|url=http://searchcloudstorage.techtarget.com/news/450296765/OpenIO-joins-object-storage-cloud-scrum/}}&lt;/ref&gt;&lt;ref name="Maleval"&gt;{{cite web|last=Maleval|first=Jean-Jacques (Apr. 25, 2016)|title=Start-Up Profile: OpenIO|url=http://www.storagenewsletter.com/rubriques/start-ups/start-up-profile-openio/}}&lt;/ref&gt;

A great article written by Philippe Nicolas illustrating products' timeline was published in July 2016 on The Register with all players, pioneers, mergers and acquisitions and of course genesis with CAS included.&lt;ref&gt;{{cite web|last=Nicolas|first=Philippe (July 15, 2016)|title=The History Boys: Object storage ... from the beginning|url=http://www.theregister.co.uk/2016/07/15/the_history_boys_cas_and_object_storage_map/}}&lt;/ref&gt;

==Architecture==
[[File:High level object storage architecture.png|thumb]]

===Abstraction of storage===
One of the design principles of object storage is to abstract some of the lower layers of storage away from the administrators and applications. Thus, data is exposed and managed as objects instead of files or blocks. Objects contain additional descriptive properties which can be used for better indexing or management. Administrators do not have to perform lower level storage functions like constructing and managing [[Logical unit number|logical volumes]] to utilize disk capacity or setting [[RAID]] levels to deal with disk failure.

Object storage also allows the addressing and identification of individual objects by more than just file name and file path. Object storage adds a unique identifier within a bucket, or across the entire system, to support much larger namespaces and eliminate name collisions.

=== Inclusion of rich custom metadata within the object ===
Object storage explicitly separates file metadata from data to support additional capabilities:
As opposed to fixed metadata in file systems (filename, creation date, type, etc.), object storage provides for full function, custom, object-level metadata in order to:
* Capture application-specific or user-specific information for better indexing purposes
* Support data management policies (e.g. a policy to drive object movement from one storage tier to another)
* Centralize management of storage across many individual nodes and clusters
* Optimize metadata storage (e.g. encapsulated, database or key value storage) and caching/indexing (when authoritative metadata is encapsulated with the metadata inside the object) independently from the data storage (e.g. unstructured binary storage)

Additionally, in some object-based file system implementations:
* The file system clients only contact metadata servers once when the file is opened and then get content directly via object storage servers (vs. block-based file systems which would require constant metadata access)
* Data objects can be configured on a per-file basis to allow adaptive stripe width, even across multiple object storage servers, supporting optimizations in bandwidth and I/O

'''Object-based storage devices''' ('''OSD''') as well as some software implementations (e.g., Caringo Swarm) manage metadata and data at the storage device level:
* Instead of providing a block-oriented interface that reads and writes fixed sized blocks of data, data is organized into flexible-sized data containers, called objects
* Each object has both data (an uninterpreted sequence of bytes) and metadata (an extensible set of attributes describing the object); physically encapsulating both together benefits recoverability.
* The command interface includes commands to create and delete objects, write bytes and read bytes to and from individual objects, and to set and get attributes on objects
* Security mechanisms provide per-object and per-command access control

===Programmatic data management===
Object storage provides programmatic interfaces to allow applications to manipulate data. At the base level, this includes [[CRUD]] functions for basic read, write and delete operations. Some object storage implementations go further, supporting additional functionality like object versioning, object replication, and movement of objects between different tiers and types of storage. Most API implementations are [[Representational state transfer|ReST]]-based, allowing the use of many standard [[HTTP]] calls.

==Implementation==

===Object-based storage devices===
Object storage at the protocol and device layer was proposed 20 years ago and approved for the [[SCSI]] command set nearly 10 years ago as "Object-based Storage Device Commands" (OSD),&lt;ref&gt;{{cite web|last=Riedel|first=Erik|title=Object Storage and Applications|url=https://www.usenix.org/legacy/event/lsf07/tech/riedel.pdf|accessdate=3 November 2013|author2=Sami Iren |date=February 2007}}&lt;/ref&gt; but has not been productized until the development of the Seagate Kinetic Open Storage platform.&lt;ref&gt;{{cite web|title=The Seagate Kinetic Open Storage Vision|url=http://www.seagate.com/tech-insights/kinetic-vision-how-seagate-new-developer-tools-meets-the-needs-of-cloud-storage-platforms-master-ti/|publisher=Seagate|accessdate=3 November 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite news|last=Gallagher|first=Sean|title=Seagate introduces a new drive interface: Ethernet|url=http://arstechnica.com/information-technology/2013/10/seagate-introduces-a-new-drive-interface-ethernet/|accessdate=3 November 2013|newspaper=Arstechnica.com|date=27 October 2013}}&lt;/ref&gt;  The [[SCSI]] command set for Object Storage Devices was developed by a working group of the [[Storage Networking Industry Association]] (SNIA) for the T10 committee of the [[International Committee for Information Technology Standards]] (INCITS).&lt;ref&gt;{{cite web|last=Corbet|first=Jonathan|title=Linux and object storage devices|url=https://lwn.net/Articles/305740/|accessdate=8 November 2013|newspaper=LWN.net|date=4 November 2008}}&lt;/ref&gt;  T10 is responsible for all SCSI standards.

===Object-based file systems===
Some distributed file systems use an object-based architecture, where file metadata is stored in metadata servers and file data is stored in object storage servers. File system client software interacts with the distinct servers, and abstracts them to present a full file system to users and applications. [[IBM General Parallel File System|IBM Spectrum Scale (also known as GPFS)]], [http://www.emc.com/en-us/storage/ecs/index.htm#collapse=&amp;tab14=0 Dell EMC Elastic Cloud Storage], [[Ceph (software)|Ceph]], [[XtreemFS]], and [[Lustre (file system)|Lustre]] are examples of this type of object storage.

===Archive storage===
Some early incarnations of object storage were used for archiving, as implementations were optimized for data services like immutability, not performance. [[Content-addressable storage|EMC Centera]] and Hitachi HCP (formerly known as HCAP) are two commonly cited object storage products for archiving. Another example is Quantum Lattus Object Storage Platform.

===Cloud storage===
The vast majority of cloud storage available in the market leverages an object storage architecture. Two notable examples are [[AWS S3|Amazon Web Services S3]], which debuted in 2005, and [[Rackspace]] Files (whose code was released as [[OpenStack#Swift|OpenStack Swift]]). Other major cloud storage services include Microsoft Azure, Google Cloud Storage, Alibaba Cloud OSS, Oracle Elastic Storage Service and DreamHost based on Ceph.

==="Captive" object storage===
Some large internet companies developed their own software when object storage products were not commercially available or use cases were very specific. Facebook famously invented their own object storage software, code-named Haystack, to address their particular massive scale photo management needs efficiently.&lt;ref name="haystack"&gt;{{cite web|last=Vajgel|first=Peter|title=Needle in a haystack: efficient storage of billions of photos|url=https://www.facebook.com/note.php?note_id=76191543919|accessdate=17 September 2013}}&lt;/ref&gt;

===Hybrid storage===
A few object storage systems, such as [[Ceph (software)|Ceph]], [[GlusterFS]], [[Cloudian]],&lt;ref name="Primesberger"&gt;{{cite web|last=Primesberger|first=Chris (27 October 2016)|title=Cloudian Raises $41 Million VC for Hybrid Cloud Object Storage|url=http://www.eweek.com/storage/cloudian-raises-41-million-vc-for-hybrid-cloud-object-storage.html}}&lt;/ref&gt; and [[Scality]] support Unified File and Object (UFO) storage, allowing some clients to store objects on a storage system while simultaneously other clients store files on the same storage system. While "hybrid storage" is not a widely accepted term for this concept, interoperable interfaces to the same set of data is becoming available in some object storage products.

===Virtual object storage===
In addition to object storage systems that own the managed files, some systems provide an object abstraction on top of one or more traditional filesystem based solutions. These solutions do not own the underlaying raw storage, but instead actively mirror the filesystem changes and replicate them in their own object catalog, alongside any metadata that can be automatically extracted from the files. Users can then contribute additional metadata through the virtual object storage APIs. A global namespace and replication capabilities both inside and across filesystems are typically supported.

Notable examples in this category are [[Nirvana (software)|Nirvana]], and its open-source cousin iRODS.

Most products in this category have recently extended their capabilities to support other Object Store solutions as well.

===Object storage systems===
More general purpose object storage systems came to market around 2008. Lured by the incredible growth of "captive" storage systems within web applications like Yahoo Mail and the early success of cloud storage, object storage systems promised the scale and capabilities of cloud storage, with the ability to deploy the system within an enterprise, or at an aspiring cloud storage service provider. Notable examples of object storage systems include [[EMC Atmos]], [[OpenStack#Object Storage (Swift)|OpenStack Swift]], [[Scality|Scality RING]], Caringo Swarm&lt;ref&gt;{{cite web|last=Nicolas|first=Philippe (Sept. 21, 2009)|title=Caringo FileFly, back to the future|url=http://continuousdataprotection.blogspot.fr/2015/09/caringo-filefly-back-to-future.html}}&lt;/ref&gt; (formerly CAStor), [[Cloudian]],&lt;ref name="Primesberger"/&gt; and OpenIO.&lt;ref name="Mellor"/&gt;

==Market adoption==
[[File:Titan supercomputer at the Oak Ridge National Laboratory.jpg|thumb|The Titan supercomputer at Oak Ridge National Laboratory]]
One of the first object storage products, Lustre, is used in 70% of the Top 100 supercomputers and ~50% of the [[Top 500]].&lt;ref&gt;{{cite web|last=Dilger|first=Andreas|title=Lustre Future Development|url=http://storageconference.org/2012/Presentations/M04.Dilger.pdf|publisher=IEEE MSST|accessdate=27 October 2013}}&lt;/ref&gt; As of June 16, 2013, this includes 7 of the top 10, including the current fastest system on the list - China's Tianhe-2 and the second fastest, the [[Titan (supercomputer)|Titan supercomputer]] at [[Oak Ridge National Laboratory]] (pictured on the right).&lt;ref&gt;{{cite web|title=Datadirect Networks to build world's fastest storage system for Titan, the world's most powerful supercomputer|url=http://www.multivu.com/mnr/60497-datadirect-networks-titan-supercomputer-storage-system-ornl|accessdate=27 October 2013}}&lt;/ref&gt;

Object storage systems had good adoption in the early 2000s as an archive platform, particularly in the wake of compliance laws like [[Sarbanes-Oxley]]. After five years in the market, EMC's Centera product claimed over 3,500 customers and 150 [[petabytes]] shipped by 2007.&lt;ref&gt;{{cite web|title=EMC Marks Five Years of EMC Centera Innovation and Market Leadership|url=http://www.emc.com/about/news/press/us/2007/04182007-5028.htm|publisher=EMC|accessdate=3 November 2013|date=18 April 2007}}&lt;/ref&gt; Hitachi's HCP product also claims many [[petabyte]]-scale customers.&lt;ref&gt;{{cite web|title=Hitachi Content Platform Supports Multiple Petabytes, Billions of Objects|url=http://www.techvalidate.com/portals/hitachi-content-platform-customers-with-more-than-1pb-of-data-stored|publisher=Techvalidate.com|accessdate=19 September 2013}}&lt;/ref&gt; Newer object storage systems have also gotten some traction, particularly around very large custom applications like eBay's auction site, where EMC Atmos is used to manage over 500 million objects a day.&lt;ref&gt;{{cite news|last=Robb|first=Drew|title=EMC World Continues Focus on Big Data, Cloud and Flash|url=http://www.infostor.com/backup-and_recovery/cloud-storage/emc-world-continues-focus-on-big-data-cloud-and-flash-.html|accessdate=19 September 2013|newspaper=Infostor|date=11 May 2011}}&lt;/ref&gt; As of March 3, 2014, EMC claims to have sold over 1.5 exabytes of Atmos storage.&lt;ref&gt;{{cite web|last=Hamilton|first=George|title=In it for the Long Run: EMC's Object Storage Leadership|url=http://www.rethinkstorage.com/in-it-for-the-long-run-emcs-object-storage-leadership#.UyEzj9yllFI|accessdate=15 March 2014}}&lt;/ref&gt; On July 1, 2014, [[Los Alamos National Lab]] chose the [[Scality|Scality RING]] as the basis for a 500 petabyte storage environment, which would be among the largest ever.&lt;ref&gt;{{cite news|last1=Mellor|first1=Chris|title=Los Alamos National Laboratory likes it, puts Scality's RING on it|url=http://www.theregister.co.uk/2014/07/01/scalitys_ring_goes_faster/|accessdate=26 January 2015|publisher=The Register|date=1 July 2014}}&lt;/ref&gt;

"Captive" object storage systems like Facebook's Haystack have scaled impressively. In April 2009, Haystack was managing 60 billion photos and 1.5 petabytes of storage, adding 220 million photos and 25 terabytes a week.&lt;ref name="haystack" /&gt;&lt;ref&gt;{{cite web|last=Nicolas|first=Philippe (Sept. 13, 2009)|title=Haystack chez Facebook|url=http://filestorage.blogspot.com/2009/09/haystack-chez-facebook.html}}&lt;/ref&gt; Facebook more recently stated that they were adding 350 million photos a day and were storing 240 billion photos.&lt;ref&gt;{{cite news|last=Miller|first=Rich|title=Facebook Builds Exabyte Data Centers for Cold Storage|url=http://www.datacenterknowledge.com/archives/2013/01/18/facebook-builds-new-data-centers-for-cold-storage/|accessdate=6 November 2013|newspaper=Datacenterknowledge.com|date=13 January 2013}}&lt;/ref&gt; This could equal as much as 357 petabytes.&lt;ref&gt;{{cite web|last=Leung|first=Leo|title=How much data does x store?|url=http://techexpectations.org/2014/05/17/how-much-data-does-x-store/|publisher=Techexpectations.org|accessdate=23 May 2014|date=17 May 2014}}&lt;/ref&gt;

Cloud storage has become pervasive as many new web and mobile applications choose it as a common way to store [[binary data]].&lt;ref&gt;{{cite web|last=Leung|first=Leo|title=Object storage already dominates our days (we just didn’t notice)|url=http://blog.oxygencloud.com/2012/01/11/object-storage-already-dominates/|accessdate=27 October 2013|date=January 11, 2012}}&lt;/ref&gt;  As the storage backend to many popular applications like [[Smugmug]] and [[Dropbox (service)|Dropbox]], AWS S3 has grown to massive scale, citing over 2 trillion objects stored in April 2013.&lt;ref&gt;{{cite news|last=Harris|first=Derrick|title=Amazon S3 goes exponential, now stores 2 trillion objects|url=http://gigaom.com/2013/04/18/amazon-s3-goes-exponential-now-stores-2-trillion-objects/|accessdate=17 September 2013|newspaper=Gigaom|date=18 April 2013}}&lt;/ref&gt; Two months later, Microsoft claimed that they stored even more objects in Azure at 8.5 trillion.&lt;ref&gt;{{cite news|last=Wilhelm|first=Alex|title=Microsoft: Azure powers 299M Skype users, 50M Office Web Apps users, stores 8.5T objects|url=http://thenextweb.com/microsoft/2013/06/27/microsoft-our-cloud-powers-hundreds-of-millions/|accessdate=18 September 2013|newspaper=thenextweb.com|date=27 June 2013}}&lt;/ref&gt; By April 2014, Azure claimed over 20 trillion objects stored.&lt;ref&gt;{{cite news|last1=Nelson|first1=Fritz|title=Microsoft Azure's 44 New Enhancements, 20 Trillion Objects|url=http://www.tomsitpro.com/articles/microsoft-azure-paas-iaas-cloud-computing,1-1841.html|accessdate=3 September 2014|publisher=Tom's IT Pro|date=4 April 2014}}&lt;/ref&gt; Windows Azure Storage manages Blobs (user files), Tables (structured storage), and Queues (message delivery) and counts them all as objects.&lt;ref&gt;{{cite web|last=Calder|first=Brad|title=Windows Azure Storage: A Highly Available Cloud Storage Service with Strong Consistency|url=http://sigops.org/sosp/sosp11/current/2011-Cascais/printable/11-calder.pdf|publisher=Microsoft|accessdate=6 November 2013|location=23rd ACM Symposium on Operating Systems Principles (SOSP)}}&lt;/ref&gt;

==Market analysis==
[[International Data Corporation|IDC]] has begun to assess the object-based storage market annually using its MarketScape methodology. IDC describes the MarketScape as: "...a quantitative and qualitative assessment of the characteristics that assess a vendor's current and future success in the said market or market segment and provide a measure of their ascendancy to become a Leader or maintain a leadership. IDC MarketScape assessments are particularly helpful in emerging markets that are often fragmented, have several players, and lack clear leaders."&lt;ref&gt;{{cite web|last1=Nadkarni|first1=Ashish|title=IDC MarketScape: Worldwide Object-Based Storage 2013 Vendor Assessment|url=http://www.idc.com/getdoc.jsp?containerId=244081|website=http://www.idc.com|publisher=IDC|accessdate=26 January 2015}}&lt;/ref&gt;

In 2013, IDC rated [[Cleversafe]], [[Scality]], [[DataDirect Networks]], [[Amplidata]], and [[EMC Corporation|EMC]] as leaders.&lt;ref&gt;{{cite news|last1=Mellor|first1=Chris|title=IDC's explicit snapshot: Everyone who's anyone in object storage: In 3D|url=http://www.theregister.co.uk/2013/11/27/idcs_objectscape_pretty_as_a_picture/|accessdate=26 January 2015|publisher=The Register|date=27 November 2013}}&lt;/ref&gt; In 2014, it rated [[Scality]], [[Cleversafe]], [[DataDirect Networks]], [[Hitachi Data Systems]], [[Amplidata]], [[EMC Corporation|EMC]], and [[Cloudian]]&lt;ref&gt;{{cite web|last=Nicolas|first=Philippe (Sept. 14, 2015)|title=Cloudian shakes the object storage market|url=http://filestorage.blogspot.fr/2015/09/cloudian-shakes-object-storage-market.html}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Mellor|first=Chris (June. 21, 2016)|title=Cloudian clobbers car drivers with targeted ads|url=http://www.theregister.co.uk/2016/06/21/cloudian_could_clobber_car_drives_with_targeted_ads/}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Nicolas|first=Philippe (June. 22, 2016)|title=Cloudian is the real S3 leader|url=http://filestorage.blogspot.fr/2016/06/cloudian-is-real-s3-leader.html}}&lt;/ref&gt; as leaders.&lt;ref&gt;{{cite news|last1=Mellor|first1=Chris|title=IDC: Who's HOT and who's NOT (in object storage) in 2014|url=http://www.theregister.co.uk/2015/01/06/idc_shows_emcs_object_presence_shrinking/|accessdate=26 January 2015|publisher=The Register|date=6 January 2015}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Mellor|first=Chris (Nov. 24, 2015)|title=We pick storage brains: Has object storage endgame started?|url=http://www.channelregister.co.uk/2015/11/24/object_storage_endgame/}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Nicolas|first=Philippe (Oct. 19, 2015)|title=Red alert for Object Storage vendors|url=http://filestorage.blogspot.com/2015/10/red-alert-for-object-storage-vendors.html}}&lt;/ref&gt;

==Standards==

===Object-based storage device standards===

====OSD version 1====
In the first version of the OSD standard,&lt;ref&gt;{{cite web|title=INCITS 400-2004|url=http://www.techstreet.com/cgi-bin/detail?product_id=1204555|publisher=InterNational Committee for Information Technology Standards|accessdate=8 November 2013}}&lt;/ref&gt; objects are specified with a 64-bit partition ID and a 64-bit object ID. Partitions are created and deleted within an OSD, and objects are created and deleted within partitions. There are no fixed sizes associated with partitions or objects; they are allowed to grow subject to physical size limitations of the device or logical quota constraints on a partition.

An extensible set of attributes describe objects. Some attributes are implemented directly by the OSD, such as the number of bytes in an object and the modify time of an object. There is a special policy tag attribute that is part of the security mechanism. Other attributes are uninterpreted by the OSD. These are set on objects by the higher-level storage systems that use the OSD for persistent storage. For example, attributes might be used to classify objects, or to capture relationships among different objects stored on different OSDs.

A list command returns a list of identifiers for objects within a partition, optionally filtered by matches against their attribute values. A list command can also return selected attributes of the listed objects.

Read and write commands can be combined, or piggy-backed, with commands to get and set attributes. This ability reduces the number of times a high-level storage system has to cross the interface to the OSD, which can improve overall efficiency.

====OSD version 2====
A second generation of the SCSI command set, "Object-Based Storage Devices - 2" (OSD-2) added support for snapshots, collections of objects, and improved error handling.&lt;ref&gt;{{cite web|title=INCITS 458-2011|url=http://www.techstreet.com/products/1801667|publisher=InterNational Committee for Information Technology Standards|accessdate=8 November 2013|date=15 March 2011}}&lt;/ref&gt;

A [[snapshot (computer storage)|snapshot]] is a point in time copy of all the objects in a partition into a new partition. The OSD can implement a space-efficient copy using [[copy-on-write]] techniques so that the two partitions share objects that are unchanged between the snapshots, or the OSD might physically copy the data to the new partition. The standard defines clones, which are writeable, and snapshots, which are read-only.

A collection is a special kind of object that contains the identifiers of other objects. There are operations to add and delete from collections, and there are operations to get or set attributes for all the objects in a collection. Collections are also used for error reporting.  If an object becomes damaged by the occurrence of a media defect (i.e., a bad spot on the disk) or by a software error within the OSD implementation, its identifier is put into a special error collection. The higher-level storage system that uses the OSD can query this collection and take corrective action as necessary.

==Differences between Key-Value and Object Stores==
{{Disputed|date=December 2015}}
Let’s first clarify what a key/value store and an object store are. Using the traditional block storage interface, one has a series of fixed size blocks which are numbered starting at 0. Data must be that exact fixed size and can be stored in a particular block which is identified by its logical block number (LBN). Later, one can retrieve that block of data by specifying its unique LBN.

With a key/value store, data is identified by a key rather than a LBN. A key might be "cat" or "olive" or "42". It can be an arbitrary sequence of bytes of arbitrary length. Data (called a value in this parlance) does not need to be a fixed size and also can be an arbitrary sequence of bytes of arbitrary length. One stores data by presenting the key and data (value) to the data store and can later retrieve the data by presenting the key. You’ve seen this concept before in programming languages. Python calls them dictionaries, Perl calls them hashes, Java and C++ call them maps, etc. Several data stores also implement key/value stores such as Memcached, Redis and CouchDB.

Object stores are similar to key/value stores except that the key must be a positive integer like a LBN. However, unlike a LBN, the key can be any positive integer; it does not have to map to an existing logical block number. In practice, it is usually limited to 64 bits. More like a key/value store than the traditional block storage interface, data is not limited to a fixed size block but may be an arbitrary size. Object stores also allow one to associate a limited set of attributes with each piece of data. The key, value and set of attributes is referred to as an object. To add more confusion, sometimes key/value stores are loosely referred to as object stores but technically there is a difference.&lt;ref&gt;http://blog.gigaspaces.com/were-flash-keyvalue-and-object-stores-made-for-each-other-guest-post-by-johann-george-sandisk/&lt;/ref&gt;

==See also==
*[[Cloud storage]]
*[[Clustered file system]]
*[[Object access method]]

==References==
{{Reflist|2}}
&lt;!--- After listing your sources please cite them using inline citations and place them after the information they cite. Please see http://en.wikipedia.org/wiki/Wikipedia:REFB for instructions on how to add citations. ---&gt;
*
*
*
*

==External links==
*[http://docs.aws.amazon.com/AmazonS3/latest/API/Welcome.html AWS S3 API Documentation]
*[https://developers.google.com/storage/ Google Cloud Storage API Documentation]
*[http://docs.openstack.org/developer/swift/ Openstack Swift API Documentation]
*[https://developers.seagate.com/display/KV/Kinetic+Open+Storage+Documentation+Wiki Seagate Kinetic Open Storage Documentation]
*[http://msdn.microsoft.com/en-us/library/windowsazure/dd179355.aspx Windows Azure Storage API Documentation]
*[https://nkolayofis.com a Saas solution in Turkey]
*[https://quictransfer.com a Cloud storage]


[[Category:Data management]]
[[Category:Data management software]]
[[Category:Computer file systems]]
[[Category:Computer data storage]]
[[Category:Network file systems]]
[[Category:Cloud storage]]</text>
      <sha1>rxwbacfda537a0wadiv4b51b09a5tyq</sha1>
    </revision>
  </page>
  <page>
    <title>Wiping</title>
    <ns>0</ns>
    <id>89314</id>
    <revision>
      <id>763082313</id>
      <parentid>759995595</parentid>
      <timestamp>2017-02-01T06:12:15Z</timestamp>
      <contributor>
        <ip>180.191.150.1</ip>
      </contributor>
      <comment>/* Philippines */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="60260" xml:space="preserve">{{About|the broadcasting practice||Wipe (disambiguation)}}
{{Multiple issues|
{{refimprove|date=February 2007}}
{{original research|date=September 2009}}
{{globalize|date=September 2009}}
}}

'''Wiping''', also known as '''junking''', is a colloquial term for action taken by [[radio]] and [[television]] production and broadcasting companies, in which old [[audiotape]]s, [[videotape]]s, and [[telerecording]]s ([[kinescope]]s), are [[List of lost television broadcasts|erased, reused, or destroyed]]. Although the practice was once very common, especially in the 1960s and 1970s, wiping is now practiced much less frequently. Older video and audio formats took up much more storage space than modern digital video or audio files, making their retention more costly, thus increasing the incentive of discarding existing broadcast material to recover storage space for newer programmes.

The advent of domestic audiovisual playback technology (e.g., videocassette and [[DVD]]) has made wiping less beneficial, with broadcasters and production houses realizing both the economic and cultural value of keeping archived material for both rebroadcast and potential profits through release on [[home video]].

==Australia==
Australian broadcasters did not gain access to videotape-recording technology until the early 1960s, and as a result nearly all programmes prior to that were broadcast live-to-air. Very little programming survives from the earliest years of Australian TV (1956–1960), as [[kinescope]] recording to film was expensive and most of what was recorded in this way has since been lost or destroyed. Some early programmes have survived, however; for example, ATN-7, a Sydney station, prerecorded (via kinescopes) some of their 1950s output such as ''[[Autumn Affair]]'' (1958–1959), ''[[The Pressure Pak Show]]'' (1957–1958) and ''[[Leave It to the Girls (Australian TV series)|Leave it to the Girls]]'' (1957–1958); some of these kinescopes have survived and are now held by the [[National Film and Sound Archive]],&lt;ref&gt;{{cite web| url = http://colsearch.nfsa.gov.au/nfsa/search/summary/summary.w3p;adv=;group=;groupequals=;page=0;parentid=;query=autumn%20affair%20Media%3A%22TELEVISION%22;querytype=;resCount=200|title = NSFA, Autumn Affair}}&lt;/ref&gt;&lt;ref&gt;{{cite web| url = http://colsearch.nfsa.gov.au/nfsa/search/summary/summary.w3p;adv=;group=;groupequals=;page=0;parentid=;query=leave%20it%20to%20the%20girls%20Media%3A%22TELEVISION%22;querytype=;resCount=10|title = NSFA, Leave it to the Girls}}&lt;/ref&gt;&lt;ref&gt;{{cite web| url = http://colsearch.nfsa.gov.au/nfsa/search/summary/summary.w3p;adv=yes;group=;groupequals=;page=0;parentid=;query=pressure%20pak%20show%20Years%3A%3E%3D1957%20Years%3A%3C%3D1958%20Media%3A%22TELEVISION%22;querytype=;resCount=20|title = NSFA, Pressure Pak Show°}}&lt;/ref&gt; with soap opera ''Autumn Affair'' surviving near-intact, likely one of the earliest Australian series for which this is the case.

===ABC===

The [[Australian Broadcasting Corporation]] (ABC) erased much of its early output. Much of the videotaped ABC programme material from the 1960s and early 1970s was erased as part of an economy policy instituted in the late 1970s in which old programme tapes were surrendered for bulk erasure and reuse. This policy particularly targeted older programmes recorded in black-and-white, leading to the loss of many recordings made before 1975, when Australian television converted to colour. The ABC continued erasing older television output into the early 1980s.

Programmes known to have been lost include most studio segments from the 1960s current affairs shows ''[[This Day Tonight]]'' and ''Monday Conference'', hundreds of episodes of the long-running rural serial ''[[Bellbird (TV series)|Bellbird]]'', all but a handful of episodes of the early-1970s drama series ''[[Certain Women (television series)|Certain Women]]'', an early-1970s miniseries of dramatizations based on [[Norman Lindsay]]'s novels, and nearly all of the first 18 months of the weekly pop-music show ''[[Countdown (Australian TV series)|Countdown]]''.

===Network Ten===

Many episodes of popular Australian commercial TV series are also lost. In the 1970s, [[Network Ten]] had an official policy to reuse tapes; hence, many tapes of ''[[Young Talent Time]]'' and ''[[Number 96 (TV series)|Number 96]]'' were wiped. To this day, Network Ten still only keeps some of its programming.{{Citation needed|date=November 2008}} Other notable losses from the Ten archive include hundreds of episodes of the Melbourne-based pop music shows commissioned and broadcast by ATV-0 Melbourne in the 1960s and early 1970s—''[[The Go!! Show]]'' (1964–1967), ''Kommotion'' (1964–1967), ''Uptight'' (1968–70), and the ''Happening 70s'' series (1970–1972).

===Nine Network===

The [[Nine Network]] discarded copies of some of their programs, including the popular [[GTV-9]] series ''[[In Melbourne Tonight]]'' hosted by [[Graham Kennedy]]. Though it ran five nights a week from 1957 to 1970, fewer than 100 episodes are known to survive, and many of the surviving episodes are edited prints made for rebroadcast across Australia. Early episodes of ''[[Hey Hey It's Saturday]]'' do not exist because the programme was broadcast live and did not begin videotape recordings until a number of years later.

==Brazil==
From 1968–1969, [[Rede Tupi|TV Tupi]] produced new episodes of the soap opera ''[[Beto Rockfeller]]'' by recording over previous episodes; as a result, few episodes survive. After the closure of TV Tupi in 1980 the 536 tapes at its São Paulo studios were simply left to deteriorate until they were recovered in 1985 and subsequently restored by [[TV Cultura]] in 1989. Only two TV Tupi O&amp;Os are known to have any preserved videotapes; TV Itacolomi's archives are now owned by the unrelated [[TV Alterosa]], affiliated with [[Sistema Brasileiro de Televisão|SBT]], whereas the few remaining tapes belonging to TV Piratini are stored privately in a museum in Porto Alegre, albeit in a deteriorated state.

[[Rede Record]] also lost much footage from the 1960s due to wiping, fires, and deterioration; most of the [[Música popular brasileira|MPB]] music festivals no longer exist, and the sitcom ''[[:pt:Família Trapo|Família Trapo]]'' has only one surviving episode, featuring [[Pelé]]. Until 1997 Rede Record had no policy on archiving videotapes, since then at least 600 videotapes that were previously believed to be lost have been recovered.

[[Rede Globo]] lost the first 35 broadcasts of both ''[[Fantástico]]'' and ''[[Jornal Nacional]]'', in addition to many segments of their other soap operas, as a result of wiping, and also due to three fires that occurred in 1969, 1971 and 1976, where an estimated 920 to 1500 tapes were destroyed.

Most of [[Rede Excelsior]]'s output was damaged in a fire in 1969; however, in the late 1990s about 100 tapes of Rede Excelsior programming were discovered and these tapes were subsequently donated to the [[Cinemateca Brasileira]] in 2001.

==Canada==
The [[Canadian Broadcasting Corporation]] never practiced wiping, and maintains a complete archive of all programming that was recorded.&lt;ref&gt;{{cite web|url=http://archives.cbc.ca/info/archives/archives_en_04.asp?IDLan=1|title=CBC Archives|date=10 April 2013|publisher=}}&lt;/ref&gt;

The [[CTV Television Network]] has admitted to wiping many programmes during the 1970s. Because of [[Canadian content]] requirements, the need for Canadian-produced programming led to more preservation of the shows they produced, and even very poorly received programmes (such as the infamous ''[[The Trouble with Tracy]]'') were saved and rerun for several years after their cancellation. Furthermore, Canadian rebroadcasts have been a source of some broadcasts that are otherwise lost in the United States and the United Kingdom.

==Japan==
Some TV stations in Japan practiced wiping, this example included the [[Doraemon (1973 anime)|first anime]] adaption of ''[[Doraemon]]''.

==Philippines==
Episodes from 1979 to 1982 of the longest running noontime show, ''[[Eat Bulaga!]]'', have been lost.

Another example of the wiping of TV archives in the Philippines was when martial law was declared, soldiers raided the [[ABS-CBN|ABS-CBN Broadcast Center]] and placed it under military control. As a result, ABS-CBN's pre-martial law archives, dating from 1953 to 1972, were lost.

==Mexico==
Due to its multiple studio facilities, namely its Chapultepec and [[Televisa San Angel|San Angel studios]], [[Televisa]] preserved most of its scripted series for broadcast years after the preserved programs had ended their original runs.  Some Televisa programs, however, were lost not due to wiping, but due to the [[1985 Mexico City earthquake]] that destroyed part of the network's archive. However, smaller channels, such as [[XEIPN-TV]] and [[XHDF-TV]], did not began to preserve their recorded broadcasts until the early 1980s. [[Monterrey]]'s [[Multimedios Televisión]] keeps most of its programming, though some special historical programming dealing with [[XHAW-TDT|its flagship station]]'s history clearly shows that some footage has been either donated by viewers recorded from its original broadcast, or uses footage of its programming recorded by fans and uploaded to [[YouTube]].

==United Kingdom==

===BBC===
The [[BBC]], the United Kingdom's first [[public service broadcaster]], had no policy on archiving until 1978.&lt;ref&gt;{{cite web|url=http://cuttingsarchive.org.uk/missing/mis_overv.htm |title=Cuttings Archive: The Missing Episodes - Overview |publisher=Cuttings Archive |accessdate=2008-11-23 |deadurl=yes |archiveurl=https://web.archive.org/web/20080725012437/http://www.cuttingsarchive.org.uk/missing/mis_overv.htm |archivedate=July 25, 2008 }}&lt;/ref&gt; Much of the corporation's output between the 1930s and 1980s has been lost. Rationales behind this policy include:

====Technological====
The BBC's [[television]] service dates back to 1936 and was originally a nearly live-only medium. The hours of transmission were very limited and the bulk of the programming was transmitted either live from the studio, or from [[Outside broadcasting|outside broadcast (OB)]] units; film was a minor contributor to the output. When the first television broadcasts were made, there were two competing systems in use. The EMI electronic system (using [[405 lines]]) competed with the Baird 240-line [[mechanical television]] system. Baird adopted an intermediate film technique where the live material was filmed using a standard film camera mounted on a large cabinet which contained a rapid processing unit and an early [[flying spot scanner]] to produce the video output for transmission. The pioneer broadcasts were not, however, preserved on this intermediate film as the nitrate (celluloid) stock was scanned while still wet from the fixer bath and never washed to remove the fixer chemicals. Consequently, the film decomposed very soon after transmission; nothing is known to have survived.
No studio or OB programmes from 1936 to 1939 or 1946 to 1947 have survived because there was no means of preserving them. Historical 'firsts' from this era; the world's earliest television crime drama ''[[Telecrime]]'' (1938–39 and 1946) or ''[[Pinwright's Progress]]'' (1946–47, the world's first regular [[situation comedy]]), only remain visually as a handful of still photographs.

The earliest recording method for television was [[Kinescope|telerecording]], which involved recording the image from a special television monitor onto film with a modified film camera. Early examples made by this method include the first two episodes of ''[[The Quatermass Experiment]]'' (1953), transmitted live while simultaneously telerecorded. The visual quality of the second episode's recording was considered so poor—a fly entered the gap between the camera and monitor at one point—that the remainder of the series was not recorded.

Although [[Quadruplex videotape]] recording technology was utilised in the UK from 1958, this system was expensive and complex; recorded programmes were often erased after broadcast. The vast majority of live programmes were never recorded at all. Videotape was not initially thought to be a permanent archivable medium – its high cost and the potential reuse of the tapes led to the transfer of programme material to film via [[Kinescope|telerecording]] whenever sales of overseas screening rights were possible or preservation deemed worthwhile. The recycling of videotapes, coupled with savings made on the storage of the bulky 2" tapes,&lt;ref&gt;By 1973, about 20,000 hours of recorded material was stored on videotape at the BBC weighing about 400,000 lbs in total. See ''BBC Engineering'', No.95, September 1973, London: BBC Publications, p.3&lt;/ref&gt; enabled the BBC to keep costs down.

====Cultural====
Drama and entertainment output was studio-based and followed the tradition of live [[theatre]]. Conventional filmmaking was only gradually introduced from the 1960s. ''The Sunday Night Play'' (a major event in the 1950s) was performed live in the studio. On Thursday, because telerecording was of insufficient broadcast quality, another live performance followed, the artists returning to perform the play again.

Today, most programmes are pre-recorded and it is relatively inexpensive to preserve programming for posterity; even so, the [[BBC Charter]] makes no mention of any obligation to retain all of them.

====Rights====
All television programmes have copyright and other rights issues associated with them. For some genres of programmes—such as drama and entertainment—the actors, writers, and musicians involved in a production all have underlying rights. In the past, these rights were defended rigorously—permission could even be denied by a contributor for the repeat or re-use of a programme. Talent [[Trade union|unions]] were highly suspicious of the threat to new work if programmes were repeated; indeed, before 1955 [[Equity (trade union)|Equity]] insisted that any telerecording made (of a repeat performance) could only "be viewed privately" on BBC premises and not transmitted.

====Colour television====
The introduction of colour television in the United Kingdom from 1967 meant that broadcasters felt there was even less value in retaining monochrome recordings. Such tapes could not be re-used for colour production, so they were disposed of to create space for the new colour tapes in the archives, which were quickly filling up. The increased cost of colour [[2 inch Quadruplex videotape]]—approximately £1000 per tape at today's prices—meant that companies still often re-used the tapes for efficiency. Negative attitudes to a programme's value also persisted. For these reasons, many programmes survive only as monochrome film recordings, if at all.

Some colour productions were telerecorded onto monochrome film for export to countries which did not yet have colour television. In some cases, early colour programmes only survive in this form.

====Significant wiped programmes====
High-profile examples of programme losses include many early episodes of [[Doctor Who]] (97), ''[[The Wednesday Play]]'', most of the seminal comedy series ''[[Not Only But Also]]'', all of the 1950s televised [[Francis Durbridge]] serials (further, the first two serials were never recorded), the vast majority of the BBC's [[Apollo 11]] [[British television Apollo 11 coverage|Moon landing studio coverage]], all but one of the 39 episodes of ''[[The First Lady (TV series)|The First Lady]]'',&lt;ref&gt;{{cite web|url=http://www.lostshows.com/default.aspx?programme=ec5863f7-6843-4552-acda-07e19396fdae|title=Lost UK TV Shows Search Engine|author=Simon Coward, Invisible Technology Ltd|publisher=}}&lt;/ref&gt; and all 147 episodes of the [[soap opera]] ''[[United!]]''. There are many gaps in many long-running BBC series (''[[Dixon of Dock Green]]'', ''[[Hancock's Half Hour]]'', ''[[Sykes]]'', ''[[Out of the Unknown]]'', and ''[[Z-Cars]]''). [[The Beatles]]' only live appearance on ''[[Top Of The Pops]]'' in 1966, performing the single "[[Paperback Writer]]" is believed to have been wiped clean in a clear-out in the 1970s.

The first acting appearance of musician [[Bob Dylan]], in a 1963 play entitled ''[[The Madhouse on Castle Street]]'', was erased in 1968.&lt;ref&gt;{{cite news|url = http://www.offthetelly.co.uk/reviews/2005/arenadylan.htm|title = Arena: Dylan in the Madhouse|date = 2005-09-28|last = Worthington|first = TJ|work = OFF THE TELLY}}&lt;/ref&gt;

There is lost material in all genres &amp;mdash; as late as 1993, a large number of videotaped children's programmes from the 1970s and 1980s were irretrievably wiped by Adam Lee of the [[BBC]] [[archives]] on the assumption that they were of "no use", without consulting the BBC children's department itself.&lt;ref&gt;{{cite news |url=http://www.offthetelly.co.uk/oldott/www.offthetelly.co.uk/index8e01.html?page_id=781 |title=Of Finger Mice and Mr. Men - The Story of Watch with Mother Part Eleven: Andy is Waving Goodbye |last=Worthington |first=TJ |date = November 2006|work=Off the Telly}}&lt;/ref&gt;

====Other lost material====
Virtually the entire runs of the corporation's pre-1970s soap operas have been lost. In the 1950s and 1960s, the [[BBC]] soap operas ''[[The Appleyards]]'', ''[[The Grove Family]]'', ''[[Compact (soap opera)|Compact]]'', ''[[The Newcomers (TV series)|The Newcomers]]'', ''[[199 Park Lane]]'', and ''[[United!]]'' produced approximately 1200 episodes altogether.

There are no episodes of either  ''United!'' or ''199 Park Lane'' in the archives, while only one episode of ''The Appleyards'', three episodes of ''The Grove Family'', and four episodes each of ''Compact'' and ''The Newcomers'' are known to exist.

Also vulnerable to the corporation's wiping policy were programmes that only lasted for one season. ''[[Abigail and Roger]]'', ''[[The Airbase]]'', ''[[As Good Cooks Go]]'', the 1960 adaptation of ''[[The Citadel (novel)|The Citadel]]'', the 1956 adaptation of ''[[David Copperfield (novel)|David Copperfield]]'', ''[[The Dark Island]]'', ''[[The Gnomes of Dulwich]]'', ''[[Hurricane]]'', ''[[For Richer...For Poorer]]'', ''[[Hereward the Wake]]'', ''The Naked Lady'', ''Night Train To Surbiton'', ''Outbreak of Murder'', ''Where do I Sit?'', and ''Witch Hunt'' have all been wiped with no footage surviving while four out of seven episodes of the paranormal anthology series ''[[Dead of Night (TV series)|Dead of Night]]'' were wiped.

An edition of ''[[Hugh and I]]'' ("Chinese Crackers"), starring [[Hugh Lloyd]], [[Terry Scott]], [[John Le Mesurier]] and [[David Jason]] was located by [[Kaleidoscope Publishing]] in 2010 in the archives of [[UCLA]], and brought to general public attention in February 2011.

Early episodes of the pop music-chart show ''[[Top of the Pops]]'' were wiped or never recorded while they were being transmitted live, including the only in-studio appearance by [[The Beatles]]. Clips of [[the Beatles]] miming "[[Can't Buy Me Love]]" and "[[You Can't Do That]]" on an episode from 25 March 1964 were found online by missing episode hunter Ray Langstone in 2015. The last lost edition dates from 8 September 1977. There are only four complete ''TOTP'' episodes surviving from the 1960s, while many otherwise-missing episodes survive only as fragments. Only two episodes still exist of ''[[The Sandie Shaw Supplement]]'' (a music-variety show hosted by the singer), recorded in 1967.

====Finding missing BBC programmes====
Since the establishment of an archival policy for television in 1978, BBC archivists and others over the years have used various contacts in the UK and abroad to try to track down missing programmes. For example, all [[BBC Worldwide]] customers—broadcasters around the world—who had bought programmes from the corporation were contacted to see if they still had copies which could be returned; ''Doctor Who'' is a prime example of how this method recovered episodes that the corporation did not hold itself. At the turn of the 21st century, the BBC established its [[BBC Archives#Archive Treasure Hunt|Archive Treasure Hunt]], a public appeal to recover lost productions, which has had some successes.&lt;ref name="BBCTH"&gt;{{cite web|url=http://www.bbc.co.uk/cult/treasurehunt/about/listoffinds.shtml |title=BBC Online - Cult - Treasure Hunt - List of Finds |publisher=Bbc.co.uk |date= |accessdate=30 July 2010}}&lt;/ref&gt;

The BBC also has close contacts with the [[National Film and Television Archive]], which is part of the [[British Film Institute]] and its "Missing Believed Wiped" event which was first held in 1993 and is part of a campaign to locate lost items from British television's past. There is also a network of collectors who, if they find any programmes missing from the BBC archives, will contact the corporation with information—or sometimes even the actual footage. Some examples of programmes recovered for the archives are ''[[Doctor Who]]'', ''[[Steptoe and Son]]'', ''[[Dad's Army]]'', ''[[Letter from America]]'',&lt;ref&gt;[http://www.bbc.co.uk/informationandarchives/archivenews/2014/letters_from_america_rediscovered.html Letter from America rediscovered], bbc.co.uk, 28 March 2014&lt;/ref&gt; ''[[The Likely Lads]]'', and ''[[Play for Today]]''.

For many years the [[television pilot|pilot]] episode of ''[[Are You Being Served?]]'' survived only in black and white, appearing in this form on the 2003 DVD release of the show. In 2009, a colour version was [[colour recovery|reconstructed]] when it was realised that the black and white film reel had actually recorded sufficient colour information as a [[dot crawl]] pattern to allow [[colour recovery]].

===ITV===
The BBC was not alone in this practice – the commercial companies that formed its main rival [[ITV (TV network)|ITV]] also wiped videotapes and destroyed [[telerecording]]s, leaving gaps in their archive holdings. The state of the archives varies greatly between the different companies; [[Granada Television]] holds a large number of its older black-and-white programmes, the company having an unofficial policy of retaining as much of its broadcast material (albeit by telerecording) as possible despite financial hardship in its early years. This includes the entirety of the soap opera ''[[Coronation Street]]'' which is now held at the [[Yorkshire Television]] archive, which itself possesses largely intact archives, although some early colour shows from the late 1960s and the early 1970s such as the entire output of the drama ''Castle Haven'', the first two series of ''[[Sez Les]]'' and the children's variety show ''[[Junior Showtime]]'' are missing and believed wiped. The former ITV company [[Thames Television]] also has a significant library.

These cases tend to be the exception, however; the former nature of the ITV network, in which private independent companies were awarded licences to serve geographical areas for a set period of time, meant that when companies lost their licences their archives were often sold to third parties and became fragmented—and/or risked being destroyed, as ownership and [[copyright]] remained with the production companies rather than with the network. The archive of networked programmes made by [[Southern Television]], for example, is now owned by the otherwise-unconnected Australian media company [[Southern Star Group]] but Southern's regional output is in the hands of [[ITV plc]]. The few surviving tapes of [[Associated-Rediffusion]] belong to many different organisations as the majority of Associated-Rediffusion's tapes were recorded in [[monochrome]] and therefore deemed of no use upon the arrival of colour broadcasting; as such they were disposed of by London successor [[Thames Television]]), although in recent years there have been occasional discoveries such as a 1959 episode of ''[[Double Your Money]]'' and the remaining missing episode of ''[[Around the World with Orson Welles]]'', found by Ray Langstone in 2011. Many master tapes belonging to [[Associated Television|ATV]] have since deteriorated due to bad storage and are unsuitable for broadcasting. In particular, the ATV version of the popular soap ''[[Crossroads (soap opera)|Crossroads]]'' is missing 2,850 episodes of its original 3,555. Also often largely lost are quiz shows; few editions exist of the 1970s version of ''Celebrity Squares'' with [[Bob Monkhouse]], or Southern's children's quiz ''[[Runaround (game show)|Runaround]]''.{{cn|date=December 2016}}

Further, responsibility for archive preservation was left to individual companies. For example, ITV has no record of its live coverage of [[Apollo 11|the 1969 Moon landings]] after the station responsible for providing the coverage, [[London Weekend Television]], wiped the tapes. Of the 96 [[United Kingdom|British]] inserts to the 1980s franchised [[United Kingdom|Anglo]]-[[United States|American]]-[[Canada|Canadian]] children's show ''[[Fraggle Rock]]'', only 12 are known to exist as the library of the British producer ([[Television South|TVS]]) has been sold and subsequently split up.

In recent years, the trend of preserving material has started to change. The archives of [[Westward Television]] and [[Television South West]] are now held in trust for the public as the South West Film and Television Archive, whilst changes in legislation mean that ITV companies which lose their franchises must donate archives to the [[British Film Institute]]. However, the change of ITV from a federal structure to one centralised company means that changes of regional companies in the future seems highly unlikely.

Most material from the 1960s also only survive as telerecordings. Some early episodes are also believed to be damaged or in poor quality, whereas much of the output of other broadcasters – such as many early episodes of ''[[The Avengers (TV series)|The Avengers]]'' which were shot in the electronic studio rather than on film, produced by [[Associated British Corporation]] – have been destroyed.

No copies of ''The Adventures of [[Francie and Josie|Francie &amp; Josie]]'' exist, as most of [[Scottish Television]]'s early shows were destroyed in a fire in late 1969 (although some sources state 1973). ''The Adventures Of Francie &amp; Josie'' was made from 1961 to 1965 by STV.

===Recovery of missing programmes===
Since the BBC library was first audited in 1978, missing programmes or extracts on either film or tape are often found in unexpected places. An appeal to broadcasters in other countries who had shown missing programmes (notably [[Australia]], [[New Zealand]], [[Canada]], and [[Africa]]n nations such as [[Nigeria]]) produced "missing" episodes from the archives of those television companies. Episodes have also been returned to broadcasters by private film collectors who had acquired 16mm film copies from various sources.{{cn|date=December 2016}}
* Two Series 1 episodes of ''[[The Avengers (TV series)|The Avengers]]'' (an [[Associated British Corporation]] production) which were thought to be missing were recovered from the [[UCLA]] Film &amp; Television Archive in the United States.
* It emerged in September 2010 that more than 60 recordings of BBC and ITV drama productions originally sent for broadcast in the United States by the [[Public Broadcasting Service|PBS]] station [[WNET]] (which serves [[New York City]] and [[New Jersey]]) had been found at the [[Library of Congress]].&lt;ref&gt;Vanessa Thorpe [https://www.theguardian.com/tv-and-radio/2010/sep/12/lost-tapes-classic-british-television "Lost tapes of classic British television found in the US"], ''The Observer'', 15 September 2010&lt;/ref&gt;
* The BBC [[sitcom]] ''[[Steptoe and Son]]'' is completely intact, although approximately half of the colour episodes only exist in monochrome; this was after copies of episodes thought to be lost were recovered in the late 1990s from early non-broadcast standard video recordings made for writers [[Ray Galton]] and [[Alan Simpson (scriptwriter)|Alan Simpson]] by BBC technicians.
* A few audio recordings of ''[[Til Death Us Do Part]]'' have been recovered, as well as an extract of the pilot and two episodes from series three.

Copies of several compilations from the British 1960s comedy ''[[At Last The 1948 Show]]'', held by many to be a forerunner of ''[[Monty Python's Flying Circus]]'', were discovered in the archives of the Swedish broadcaster [[Sveriges Television|SVT]], to whom the producers [[Associated Rediffusion|Rediffusion London]] had sold them upon the companies' loss of its broadcasting licence. The master tapes, along with much of Rediffusion's programming, were wiped or disposed of by London successor Thames Television. Their recovery enabled the reconstruction of otherwise missing original editions of the programme, meaning most of the series exists in visual form.

Off-air home audio recordings of various television programmes have also been recovered, at least preserving the soundtracks to otherwise missing shows, and some of these (particularly from ''[[Doctor Who]]'') have been released on [[CD]] by the BBC following restoration and the addition of narration to describe purely visual elements. [[Tele-snaps]], a commercial service of off-screen shots of programmes often purchased by [[actor]]s and [[television director]]s to keep a record of their work in the days before [[videocassette recorder]]s, have also been recovered for many lost programmes.

===Preservation of the current archive===
Advances in technology have resulted in old programmes being transferred to new digital media, where they can be restored or (if they are damaged or otherwise cannot be restored) kept from decaying further. In the United Kingdom, the archives of both the BBC and those available of ITV, along with other channels, are being switched from cumbersome [[Quadruplex videotape|2-inch quadruplex videotape]] to digital format. This is an extensive and expensive process and one that will take many years to complete.

Live broadcasts in Britain are still not necessarily kept, and wiping of material has not ceased. According to writer and broadcaster [[Matthew Sweet (writer)|Matthew Sweet]], there are "big gaps in the record of children's television of the Nineties."&lt;ref&gt;Matthew Sweet [http://www.telegraph.co.uk/culture/tvandradio/10492487/Searching-for-televisions-missing-gems-Doctor-Who-Woody-Allen-Ridley-Scott-and-Dennis-Potter.html "Searching for television's missing gems: Doctor Who, Woody Allen, Ridley Scott and Dennis Potter"], telegraph.co.uk, 4 December 2013&lt;/ref&gt;

==United States==
In the [[United States]], the major broadcast networks also engaged in the practice of wiping recordings until the late 1970s. Many episodes were erased, especially daytime and late-night programming such as daytime [[soap opera]]s and [[game show]]s. The daytime shows, almost all of them having been taped, were erased because it was believed at the time that nobody wanted to see them after their first broadcast. The success of [[cable television]] networks devoted to reruns of these genres proved that this was not the case, as the large number of episodes that were required for a daily program made even a short-run game show an ideal candidate for [[broadcast syndication|syndication]]. By this time, however, the damage had already been done.

===Preservation by institutions such as museums===

Some museums and other [[cultural institution]]s such as the Paley Center for Media have taken steps to discover and preserve (see, e.g., "[[Paley Center for Media#Archives]]") old recordings previously thought to have been wiped or discarded, lost, or misfiled.

===Hosting sequences===
Hosting sequences on videotape, nearly always featuring celebrities, were sometimes made for telecasts of family films, notably for the first nine telecasts of MGM's ''[[The Wizard of Oz (1939 film)|The Wizard of Oz]]''. It is not known if those made for ''Oz'' survived since they have not been seen since 1967. One hosting sequence from that era that does survive is the one [[Eddie Albert]]  made for the 1965 CBS telecast of ''[[The Nutcracker]]'', starring [[Edward Villella]], [[Patricia McBride]], and [[Melissa Hayden (dancer)|Melissa Hayden]]. It has even been included on the DVD release of the program.&lt;ref&gt;{{cite web|url=http://www.wbshop.com/product/nutcracker+the+1965+tv+sp+1000179869.do?from=Search|title=Nutcracker, The (1965 TV SP) (MOD)|work=www.WBShop.com}}&lt;/ref&gt;

===Ernie Kovacs===
Many of [[Ernie Kovacs]]'s videotaped network programs were also wiped. During different times as comedian, writer, and performer Kovacs had programs on all four major television networks ([[American Broadcasting Company|ABC]], [[CBS]], [[DuMont Television Network|DuMont]], and [[NBC]]). After Kovacs's death, the networks wiped many programs. Kovacs's widow [[Edie Adams]] obtained as many programs and episodes as she could find, donating them to [[UCLA]]'s [[Special Collections]].

===Soap operas===
Though most soap operas transitioned from live broadcast to videotaping their shows during the 1960s, it was still common practice to wipe and reuse the tapes. This practice was due to the high cost of videotape at the time. While soap operas began routinely saving their episodes between 1976 and 1979, several soaps have saved recordings of most or all their episodes. ''[[Days of Our Lives]]''  has recordings of all its episodes; its first two episodes exist on their original master tapes, and were aired by [[SOAPnet]] in 2005. ''[[The Young and the Restless]]'',  ''[[Dark Shadows]]''  and ''[[Ryan's Hope]]'' saved most of their episodes, despite the fact that they debuted during the 1960s and 1970s, before retaining tapes became common practice. Episodes of ''[[The Doctors (soap opera)|The Doctors]]'' began to be saved no later than December 4, 1967; this is where reruns of the series began when picked up by [[Retro Television Network]] in September 2014. Episodes of other soaps broadcast during the 1950s to 1970s do exist in different forms and have been showcased in various places online.

[[Procter and Gamble]] started saving their shows around 1979. Very few pre-1979 color episodes of the Procter and Gamble-sponsored shows survive, with most extant episodes preserved as monochrome kinescopes. Exceptions include two episodes of ''[[The Guiding Light]]'' (1973 and 1977), which have been released on DVD. ''[[As the World Turns]]'' and ''[[The Edge of Night]]'' aired live until 1975, the year ''The Edge of Night'' moved to [[American Broadcasting Company|ABC]] and ''As the World Turns'' expanded from a 30-minute broadcast to one hour. Both shows began taping episodes in preparation for the move of ''The Edge of Night'' to ABC. ''The Edge of Night''&lt;nowiki&gt;'s&lt;/nowiki&gt; ABC debut is believed to have survived. Overall, the number of surviving monochrome episodes recorded on kinescope outnumber color episodes for these programs.

[[Agnes Nixon]] initially produced her series ''[[One Life to Live]]'' and ''[[All My Children]]'' through her own production company, Creative Horizons, Inc., and kept a complete archive of monochrome kinescopes until ABC bought the shows from her in 1975. When the network wanted to expand ''All My Children'' from 30 minutes to a full hour in the late 1970s, Nixon agreed on the condition that the network would begin saving the episodes. ABC complied, and full hour broadcasts began on April 25, 1977. However, a fire destroyed the vast majority of the early-1970s kinescopes, leaving only a few random episodes from each season.

Virtually all episodes of ''[[General Hospital]]'', from its premiere in April 1963 through August 1970, are archived at [[UCLA]]. The [[UCLA Film &amp; Television Archive]] holds a large number of daytime television airings that were spared from the wiping practice.  Also archived there are handfuls of episodes of each soap opera that was on the air from 1971 and 1973, including ''[[A World Apart (TV series)|A World Apart]]'', ''[[Where the Heart Is (1969 TV series)|Where the Heart Is]]'', and ''[[Return to Peyton Place (TV series)|Return to Peyton Place]]''.

===DuMont programs===
It is believed that virtually the entire archive of the [[DuMont Television Network]], covering its whole history from 1946 to 1956, was disposed of during the 1970s by a "successor" broadcaster (presumably [[Metromedia]], the holder of DuMont's assets), who dumped all of the kinescopes/videotapes into the [[East River]] to make room for other tapes (believed to be ABC's) at a New York City warehouse.&lt;ref name="LoC"&gt;{{cite web|last = Adams|first = Edie|authorlink = Edie Adams|title = Television/Video Preservation Study: Los Angeles Public Hearing|work = National Film Preservation Board| publisher = Library of Congress|date = March 1996|url = http://www.loc.gov/film/hrng96la.html|accessdate = 2008-05-09}}&lt;/ref&gt; Further, a large number of DuMont's kinescopes were destroyed in about 1958 for their silver content.

Of the over 20,000 shows carried by DuMont in its ten-year existence, [[List of surviving DuMont Television Network broadcasts|approximately 350 or so episodes of DuMont programming are known to exist today]], less than two percent of its total output. The remainder were either never recorded (e.g., ''[[NFL on DuMont]]'') or were dumped in the earlier purges.

===''The Tonight Show''===
{{See also|The Tonight Show Starring Johnny Carson}}
Almost all of ''[[The Tonight Show]]'' with [[Jack Paar]] and the first ten years hosted by his successor [[Johnny Carson]] were taped over by the network, with Carson's blessing, under the assumption that the broadcasts were of no real value.&lt;ref&gt;[http://www.washingtonpost.com/entertainment/tv/carson-on-tcm-shows-why-johnny-was-the-king/2013/07/03/3c25d1ce-e264-11e2-aef3-339619eab080_story.html Carson on TCM shows why Johnny was the king]. ''The Washington Post''. Retrieved July 9, 2013.&lt;/ref&gt; This is part of the reason why Carson's late 1960s shows had poorer picture quality{{Citation needed|date=November 2010}} compared to his competitor [[Dick Cavett]] on [[American Broadcasting Company|ABC]]; [[NBC]] was using the ''Tonight Show'' tapes repeatedly. Another reason for their poorer quality is that many of the 1960s ''Tonight Show'' episodes only survived in the kinescope format. (Cavett's ABC shows were also taped over by his network in favor of other shows produced at ABC's studios in New York.)

===Early sporting events===
{{See also|List of World Series broadcasters|List of Super Bowl broadcasters|NFL on CBS|NFL on NBC}}

Many early sporting events, such as the [[World Series]] and the first two [[Super Bowl]]s, were also lost, though a nearly intact recording of the first Super Bowl was found in 2005.

====[[National Football League]]====
''[[Super Bowl I]]'' was aired by both [[CBS]] and [[NBC]] (the only Super Bowl to be aired by two networks), but neither network felt the need to preserve the game long-term; CBS saved the telecast for a few months and reran it as filler programming at least once before wiping it. A color videotape containing the first, second and fourth quarters of the telecast from [[WYOU]] (the CBS affiliate for [[Scranton, Pennsylvania]]) was found in 2005 and is in the process of being restored.&lt;ref&gt;Fybush, Scott (2011-02-07). [http://www.fybush.com/NERW/2011/110207/nerw.html Will New York Outlaw Pirate Radio?]. ''NorthEast Radio Watch''. Retrieved 2011-02-07.&lt;/ref&gt; On January 15, 2016, the [[NFL Network]] reaired the first Super Bowl, featuring audio from [[NBC Radio]] and most of the TV network broadcast and newly discovered [[NFL Films]] footage of the game. ''[[Super Bowl II]]'' was aired exclusively by CBS and was believed to have been erased, but it was later found that the entire telecast fully exists and rests in the vaults of [[NFL Films]].&lt;ref name="foo"&gt;{{cite web
 | url        = http://www.marketwatch.com/story/the-hunt-for-tvs-lost-baseball-treasures-2010-10-27?pagenumber=2
 | title      = The hunt for TV’s lost baseball treasures
 | author     = David B. Wilkerson
 | date       = October 27, 2010
 | work       =
 | publisher  = [[Wall Street Journal]] Marketwatch
 | accessdate = November 26, 2012
}}&lt;/ref&gt;  Though the telecast of ''[[Super Bowl III]]'' exists in full color, only half of the ''[[Super Bowl IV]]'' broadcast does (the rest was preserved by Canadian television in black-and-white). The first three quarters of ''[[Super Bowl V]]'' broadcast by NBC Los Angeles affiliate [[KNBC]] exist, but the fourth quarter is missing, though the [[Mike Curtis (American football)|Mike Curtis]] interception and [[Jim O'Brien (American football)|Jim O'Brien]] game-winning field goal were recovered via news highlights from [[CBC Television|CBC]]. ''[[Super Bowl VI]]'' also exists in its entirety. It was not until ''[[Super Bowl VII]]'' that a continuous archive was established.&lt;ref name="foo" /&gt;

Similarly, all of the telecasts of the [[NFL Championship Game]]s prior to the Super Bowl are believed to have been lost, with all surviving footage of those games coming from separately produced film. The status of most regular season and playoff games from the early years of television up to the immediate years following the 1970 [[AFL–NFL merger]] are also unknown. Among the footage that has survived include at least some of NBC's coverage from the 1972 AFC Divisional Playoff game between the [[1972 Pittsburgh Steelers season|Pittsburgh Steelers]] and [[1972 Oakland Raiders season|Oakland Raiders]] that featured the [[Immaculate Reception]], as well as the inaugural telecast of ''[[Monday Night Football]]'' between the [[1970 Cleveland Browns season|Cleveland Browns]] and the [[1970 New York Jets season|New York Jets]], though several ''Monday Night Football'' games in the ensuing seasons were lost. A [[1974 NFL season|1974 game]] that featured [[John Lennon]] being interviewed by [[Howard Cosell]] in the booth only survived due to a [[home video]] recording of the game; the game itself was wiped by ABC. CBS kept coverage of a 1978 [[Eagles–Giants rivalry|matchup]] between the [[1978 New York Giants season|New York Giants]] and [[1978 Philadelphia Eagles season|Philadelphia Eagles]] that would feature the now-infamous [[Miracle at the Meadowlands]], although the existence of many 1978 games on CBS by private collectors shows that the networks by that point started keeping recordings of regular season games. There are rare exceptions of CBS games from [[1977 NFL season|1977]] back, but by [[1978 NFL season|1978]] the library of most teams is almost fully complete. NBC is another story.

The NFL had its own filmmakers, [[NFL Films]], filming the game with its own equipment. Thus, preserving the telecasts on tape was not seen as a priority by the networks when another source was available – though the sportscasters' play-by-play comments, as a result, were lost.

====World Series telecasts====
All telecasts of World Series games starting in [[1975 World Series|1975]] ([[1975 Cincinnati Reds season|Reds]]–[[1975 Boston Red Sox season|Red Sox]]) are known to exist in full.&lt;ref name="Surviving World Series Telecasts"&gt;{{cite web|url=http://www.dbsforums.com/vbulletin/showthread.php?t=78232|title=www.dbsforums.com|publisher=}}&lt;/ref&gt; Follows is the known footage of World Series telecasts prior to 1975:
* [[1952 World Series|1952]] ([[1952 New York Yankees season|Yankees]]–[[1952 Brooklyn Dodgers season|Dodgers]]) – Games 6–7 are intact.
* [[1955 World Series|1955]] ([[1955 New York Yankees season|Yankees]]–[[1955 Brooklyn Dodgers season|Dodgers]]) – Only the first half of Game 5 is known to exist.
* [[1956 World Series|1956]] ([[1956 New York Yankees season|Yankees]]–[[1956 Brooklyn Dodgers season|Dodgers]]) – Only the last three innings of Game 2 are known to exist. Game 3 is intact minus the second and third inning. Game 5 ([[Don Larsen]]'s [[perfect game]]) is intact minus the first inning, and was aired on January 1, 2009 during the [[MLB Network]]'s first broadcast day.
* [[1957 World Series|1957]] ([[1957 New York Yankees season|Yankees]]–[[1957 Milwaukee Braves season|Braves]]) – Game 1 is intact by way of a print from the United States [[American Forces Network|Armed Forces Radio and Television Service]].&lt;ref&gt;https://www.youtube.com/watch?v=72Eo0pIka4o&lt;/ref&gt; Game 3 is intact, minus a snip of [[Tony Kubek]]'s second home run in the top 7th inning. Games 6 (most of the first six innings) and 7 reportedly exist as well.
* [[1960 World Series|1960]] ([[1960 New York Yankees season|Yankees]]–[[1960 Pittsburgh Pirates season|Pirates]]) – Game 7 (with [[Bill Mazeroski]]'s series-clinching walk-off home run) was found intact on [[kinescope]] in December 2009 in the wine cellar of Pirates' part-owner [[Bing Crosby]], who had the game recorded at his own expense. MLB Network aired it in December 2010.&lt;ref&gt;{{cite news|url = http://www.nytimes.com/2010/09/24/sports/baseball/24crosby.html?_r=1&amp;src=mv|title = In Bing Crosby's Wine Cellar, Vintage Baseball|first = Richard|last = Sandomir|authorlink = Richard Sandomir|publisher = ''[[The New York Times]]''|date = 2010-09-23|accessdate = 2010-09-25}}&lt;/ref&gt;
* [[1961 World Series|1961]] ([[1961 New York Yankees season|Yankees]]–[[1961 Cincinnati Reds season|Reds]]) – Half-hour segments of Games 3 (the first two innings), 4 (the 4th and 5th innings), and 5 (open and top of the 1st inning) are known to exist.
* [[1963 World Series|1963]] ([[1963 New York Yankees season|Yankees]]–[[1963 Los Angeles Dodgers season|Dodgers]]) – Game 3 is intact.
* [[1965 World Series|1965]] ([[1965 Minnesota Twins season|Twins]]–[[1965 Los Angeles Dodgers season|Dodgers]]) – All seven games were preserved by the [[CBC Television|CBC]] on [[kinescope]].
* [[1968 World Series|1968]] ([[1968 Detroit Tigers season|Tigers]]–[[1968 St. Louis Cardinals season|Cardinals]]) – All seven games were preserved by the [[CBC Television|CBC]] on [[kinescope]].
** It is likely the 1965 and 1968 Series were preserved by the CBC due to the Twins' and Tigers' proximity to Canada; the country would not get its own MLB team until the [[Montreal Expos]] began play in 1969.
* [[1969 World Series|1969]] ([[1969 Baltimore Orioles season|Orioles]]–[[1969 New York Mets season|Mets]]) – Games 1–2 were preserved by the [[CBC Television|CBC]] on [[kinescope]], while Games 3–5 exist on their original color videotape from "truck feeds".
* [[1970 World Series|1970]] ([[1970 Baltimore Orioles season|Orioles]]–[[1970 Cincinnati Reds season|Reds]]) – Games 1–4 were preserved by the [[CBC Television|CBC]] on [[kinescope]], while Game 5 exists on its original color videotape from the "truck feed".
* [[1971 World Series|1971]] ([[1971 Baltimore Orioles season|Orioles]]–[[1971 Pittsburgh Pirates season|Pirates]]) – Games 1–2 and 6–7 are intact, while Games 3–5 only partially exist and Game 4 (the first World Series night game) is near-complete.
* [[1972 World Series|1972]] ([[1972 Oakland Athletics season|A's]]–[[1972 Cincinnati Reds season|Reds]]) – Game 4 is intact, along with nearly all of Game 5 and a fair chunk of Game 2. Fragments exist for Games 1, 3, and 6, while Game 7 is missing.
* [[1973 World Series|1973]] ([[1972 Oakland Athletics season|A's]]–[[1972 New York Mets season|Mets]]) – Game 1 is intact, Game 2 is missing the last inning and a half (including both [[Mike Andrews]] plays), Game 3 is complete minus the last inning, Game 4 is intact from the pregame show to the top of the 4th inning, and Game 5 only has the last two innings. About 30 minutes of excerpts from Game 6 survive, while Game 7 cuts off with one out at the top of the 9th inning.
** While the last inning and a half of Game 2 is missing from the Major League Baseball/[[Major League Baseball on NBC|NBC]] copy, the Andrews plays (totaling about 60 seconds of coverage) survived because after the World Series, NBC put together a 20-minute presentation tape narrated by [[Curt Gowdy]] to submit to the [[Peabody Awards]] in order to get consideration for an award for their coverage by the committee; the tape includes the two Andrews plays with Gowdy and [[Tony Kubek]]'s calls and analysis of them. The presentation tape is held by the Peabody vault, creating a case where "reconstructing" a game in an incomplete format would require going to two different outlets.
* [[1974 World Series|1974]] ([[1974 Oakland Athletics season|A's]]–[[1974 Los Angeles Dodgers season|Dodgers]]) – Games 1–4 are complete. Game 5 is near intact, but the bottom of the 9th inning is missing and only exists on the original radio broadcast.

====League Championship Series telecasts====
For the League Championship Series telecasts spanning from 1969 to 1975, only Game 2 of the [[1972 American League Championship Series]] ([[1972 Oakland Athletics season|Oakland]]–[[1972 Detroit Tigers season|Detroit]]) is known to exist;&lt;ref name="Surviving World Series Telecasts"/&gt; however, the copy on the trade circuit is missing the [[Bert Campaneris]]–[[Lerrin LaGrow]] brawl.

There are some instances where the only brief glimpse of telecast footage of an early LCS game can be seen in a surviving newscast from that night.
* Clips of Game 5 of the [[1972 National League Championship Series]] featuring the then-[[Cincinnati Reds]] announcer [[Al Michaels]] calling the two crucial plays of the game, the game-tying home run by [[Johnny Bench]] and wild pitch bringing home [[George Foster (baseball)|George Foster]] with the series-clinching run, are available.
* The last out of the [[1973 National League Championship Series]] as described by [[Jim Simpson (sportscaster)|Jim Simpson]] was played on that night's ''[[NBC Nightly News]]'', but other than that the entire game is gone.
* On the day the [[1969 New York Mets season|New York Mets]] and [[1969 Baltimore Orioles season|Baltimore Orioles]] wrapped up their respective League Championship Series in 1969, a feature story on the ''[[CBS Evening News]]'' showed telecast clips of the [[1969 American League Championship Series|ALCS]] game (albeit with no original sound). This is all that likely remains of anything from that third game of the [[1969 Baltimore Orioles season|Orioles]]–[[1969 Minnesota Twins season|Twins]] series.

While all telecasts of World Series games starting with [[1972 World Series|1975]] are accounted for and exist, the LCS is still a spotty situation through the late 1970s:
* [[1976 American League Championship Series|1976 ALCS]] – Game 5 is intact, from the [[American Broadcasting Company|ABC]] vault.
* [[1976 National League Championship Series|1976 NLCS]] – Game 3 is intact, albeit an off-air recording taped in the [[KATU|Portland market]]. Apparently, this copy is the only extant version because the ABC vault copy has no sound.
* [[1977 National League Championship Series|1977 NLCS]] – Game 3 is intact, from the [[1977 Philadelphia Phillies season|Philadelphia Phillies]]' local [[KYW-TV|NBC affiliate]]. A copy is held by Major League Baseball, who also appears to have Game 4 as well.
* [[1977 American League Championship Series|1977 ALCS]] – Game 5 is intact, with both the [[WPIX-TV|WPIX]] and [[Major League Baseball on NBC|NBC]] versions existing through off-air recordings.
** Clips of these games may be seen in highlight shows such as ''[[Yankeeography]]''. It is believed that incomplete tapes of the ALCS exist. It is possible these games are not shown in part because the audio quality is poor. A common method of getting around such deficiencies would be to overlay a radio telecast or narration by a player or commentator where gaps exist.
* [[1978 American League Championship Series|1978 ALCS]] – All four games ([[Major League Baseball on ABC|ABC]] version) are intact via off-air recordings.
* [[1978 National League Championship Series|1978 NLCS]] – Game 4 is intact, again from off-air recordings.

====NBA Finals====
{{see also|List of NBA Finals broadcasters}}
* [[1963 NBA Finals|1963]]: [[Boston Celtics|Celtics]]–[[Los Angeles Lakers|Lakers]] – Game 6 is intact.
* [[1969 NBA Finals|1969]]: Celtics–Lakers – only the entire 4th quarter of Game 7 exists.
* [[1970 NBA Finals|1970]]: Lakers–[[New York Knicks|Knicks]] – Game 7 is intact.
* [[1971 NBA Finals|1971]]: [[Milwaukee Bucks|Bucks]]–[[Washington Bullets|Bullets]] – only nearly all of the second half of game 4 exists.
* [[1972 NBA Finals|1972]]: Knicks–Lakers – Game 5 is intact with the exception of the last 3–4 minutes of the game
* [[1973 NBA Finals|1973]]: Knicks–Lakers – Games 1–4 are missing, while the entire Game 5 wasn't found until 2013 and some of which was shown in the ''[[30 for 30]]'' documentary ''When The Garden Was Eden''.
* [[1974 NBA Finals|1974]]: Bucks–Celtics – only the 4th quarter and 2 overtime of Game 6 and the 4th quarter of Game 7 exist.*[1975 NBA Finals -.Bullets-Warriors game ..1,2,&amp;3 intact
* [[1976]]: Suns-Celtics - Games 5 &amp; 6 are intact.

===Wiped programs===

====Early live shows====
Many programs in the early days of television were live broadcasts that are lost because they were not recorded. Most prime-time programs that were preserved used the [[kinescope|kinescope recording]] process, which involved filming the live broadcast from a television screen using a motion-picture camera (videotape, for recording programs, was not perfected until the late 1950s and was not widely used until the late 1960s). This was also a common practice for broadcasting live TV shows to the [[West Coast of the United States|west coast]], as performers often performed a show back-to-back, but never back-to-back-to-back.

Daytime programs, however, were generally not kinescoped for preservation (although many were temporarily kinescoped for later broadcast, episodes recorded in this way were often junked). Many local station and network newscasts were prone to wiping.

====News====
Some early news programs, such as ''[[Camel News Caravan]]'', are largely lost. Moving images of [[Walter Cronkite]] reading the news in his studio every night for six years are gone with the exception of his coverage of the [[Cuban Missile Crisis]] in 1962 and the [[JFK assassination]] in 1963. Studio shots of [[Peter Jennings]] inside his [[American Broadcasting Company|ABC]] studio during his first year there (1965) are also gone.

[[Vanderbilt University]] has kept all evening national news telecasts since Monday, August 5, 1968.

As of 1997, CBS had saved 1,000,000 videotapes of news reports, broadcasts, stock footage, and outtakes according to a report that year from the [[National Film Preservation Board]]. The same report added, "Television stations still erase and recycle their video cassettes", referring to local news programs.&lt;ref&gt;{{cite web
 | url        = http://www.loc.gov/film/tvstudy.html
 | title      = Television/Video Preservation Study: Volume 1: Report
 | author     = [[Librarian of Congress]]
 | date       = October 1997
 | work       =
 | publisher  = [[Library of Congress]]
 | accessdate = 26 November 2012
}}&lt;/ref&gt; Many local stations contract with outside companies for archiving news coverage.

====Situation comedy====
Little of the first [[sitcom]], ''[[Mary Kay and Johnny|The Mary Kay and Johnny Show]]'', remains today. It was initially live and not recorded, but later in its run kinescopes were made for rebroadcasting. Fragments of episodes and one complete installment are known to exist.

====Game shows====
[[Game show]]s, more than any other genre, were prone to wiping. Many games between 1941 and 1980 had insignificantly-short runs (some measured in a span of weeks or even days) that the networks felt it unnecessary to keep them for posterity, whereas recycling the tapes would be more profitable and less of an effort than attempting to sell the series in reruns, in an era before [[cable television]].

[[Mark Goodson]]–[[Bill Todman]] Productions (and to a lesser extent, [[Barry &amp; Enright Productions|Barry-Enright Productions]] and [[Chuck Barris Productions]]) and to an even lesser extent [[Heatter-Quigley Productions]] had the foresight to preserve many of their games for later reruns; for years, these shows dominated the [[Game Show Network]] (GSN) line-up and now make up a major portion of [[Buzzr TV]]'s lineup.

Most other game shows from that era were not so fortunate.  All of the [[Bob Stewart (television)|Bob Stewart]] (except ''[[Pyramid (game show)|Pyramid]]''), [[Heatter-Quigley Productions|Heatter–Quigley]] except for ''[[PDQ (game show)|PDQ]]'' which aired in syndication as well as many episodes of ''[[Hollywood Squares]]'', [[Stefan Hatos-Monty Hall Productions|Hatos–Hall]] (except for a large portion of ''[[Let's Make a Deal]]''), and pre-1980 [[Merv Griffin]] productions have been destroyed, with the exception of a few rare pilots and "cast aside" episodes. The few remaining episodes have therefore become collectors' items, and an active trading circuit exists among collectors.

NBC and ABC continued the wiping process well into the 1970s; while ABC ceased in early 1978, NBC continued to wipe some shows into 1980, leaving much of their daytime game show content lost forever. CBS abandoned the wiping process by September 1972, largely as a result of their collaboration with Goodson-Todman; as a result, even the network's shorter-lived games (such as ''[[Spin-Off (game show)|Spin-Off]]'') still exist in their entirety. Incidentally, all three networks ended their wiping practices during the time [[Fred Silverman]] led their respective networks.

While it remained in business, DuMont wished to keep its programs as intact as possible. However, the network ceased to exist in 1956 and its archive was destroyed in the 1970s. The corporate successor to DuMont, [[Fox Broadcasting Company|Fox]], not only has never aired any daytime programming (other than its [[Fox Kids]] block from 1990 to 2001) but debuted in 1986, well beyond the wiping era.

====Award shows====
Several award shows from the 1950s and 1960s, such as the [[Academy Awards]] and the [[Emmy Awards]], only survive in kinescope format. From [[29th Academy Awards|1957]] to [[37th Academy Awards|1965]], the Academy Awards were taped in black and white, but only survive in kinescope format for overseas distribution, especially for the European TV audiences, which used another system ([[576i|625 lines]] as opposed to [[480i|525 lines]]), as the tapes used for late broadcasting were reused. All of the taped broadcasts of the Oscars from [[38th Academy Awards|1966]] (the first to be broadcast in color) remain intact.

==See also==
{{portal|Television}}
* [[Doctor Who missing episodes|''Doctor Who'' missing episodes]]
* [[British television Apollo 11 coverage]]
* [[Missing Believed Wiped]]
* [[Kinescope]]
* [[Lost film]]
* [[List of lost television broadcasts]]
* [[Film preservation]]
* [[List of surviving DuMont Television Network broadcasts]]

==Footnotes==
{{Reflist|2}}

==References==
*{{cite book |last= Fiddy |first= Dick |authorlink= |coauthors= |title= Missing, Believed Wiped: Searching for the Lost Treasures of British Television |year= 2002 |publisher= [[British Film Institute]] |location= London |isbn= 978-0-85170-866-9 }}

==External links==
*[http://www.bbc.co.uk/cult/treasurehunt/ Full Details of the BBC's treasure Hunt]
*[http://www.lostshows.com/default.aspx? Lost Shows (UK) search engine], Kaleidoscope website
*[http://www.missing-episodes.com/ British TV Missing Episodes Index]
*[http://www.wipednews.com/ Wiped News.Com - A news and features website devoted to missing TV, Film &amp; Radio]
*[http://www.marketwatch.com/story/story/print?guid=E880D4C8-E078-11DF-B7D4-002128049AD6 The hunt for TV’s lost baseball treasures]
*[http://www.tvobscurities.com/lost/lostormissing/ Television Obscurities &gt;&gt; Television — Lost or Missing]

{{Major League Baseball on national television}}
{{National Basketball Association on television}}
{{National Football League on television and radio}}

[[Category:Television terminology]]
[[Category:Data management]]
[[Category:Television preservation]]</text>
      <sha1>rm817yipa9nz5pzyy9z4q3nny87dhvk</sha1>
    </revision>
  </page>
  <page>
    <title>Lambda architecture</title>
    <ns>0</ns>
    <id>43539426</id>
    <revision>
      <id>751624124</id>
      <parentid>749778644</parentid>
      <timestamp>2016-11-26T21:52:40Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <comment>removed [[Category:Data analysis]]; added [[Category:Data management]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9474" xml:space="preserve">[[File:Diagram of Lambda Architecture (generic).png|thumb|Flow of data through the processing and serving layers of a generic lambda architecture]]
'''Lambda architecture''' is a [[data processing|data-processing]] architecture designed to handle massive quantities of data by taking advantage of both [[batch processing|batch]]- and [[stream processing|stream-processing]] methods. This approach to architecture attempts to balance [[latency (engineering)|latency]], [[throughput]], and [[fault-tolerance]] by using batch processing to provide comprehensive and accurate views of batch data, while simultaneously using real-time stream processing to provide views of online data. The two view outputs may be joined before presentation. The rise of lambda architecture is correlated with the growth of [[big data]], real-time analytics, and the drive to mitigate the latencies of [[map-reduce]].&lt;ref&gt;{{cite web|last1=Schuster|first1=Werner|title=Nathan Marz on Storm, Immutability in the Lambda Architecture, Clojure|url=http://www.infoq.com/interviews/marz-lambda-architecture|website=www.infoq.com}} Interview with Nathan Marz, 6 April 2014&lt;/ref&gt;

Lambda architecture depends on a data model with an append-only, immutable data source that serves as a system of record.&lt;ref name=bijnens-slide&gt;Bijnens, Nathan. [http://lambda-architecture.net/architecture/2013-12-11-a-real-time-architecture-using-hadoop-and-storm-devoxx/ "A real-time architecture using Hadoop and Storm"]. 11 December 2013.&lt;/ref&gt;{{rp|32}} It is intended for ingesting and processing timestamped events that are appended to existing events rather than overwriting them. State is determined from the natural time-based ordering of the data.

==Overview==
Lambda architecture describes a system consisting of three layers: batch processing, speed (or real-time) processing, and a serving layer for responding to queries.&lt;ref name=big-data&gt;Marz, Nathan; Warren, James. ''Big Data: Principles and best practices of scalable realtime data systems''. Manning Publications, 2013.&lt;/ref&gt;{{rp|13}} The processing layers ingest from an immutable master copy of the entire data set.

===Batch layer===
The batch layer precomputes results using a distributed processing system that can handle very large quantities of data. The batch layer aims at perfect accuracy by being able to process ''all'' available data when generating views. This means it can fix any errors by recomputing based on the complete data set, then updating existing views. Output is typically stored in a read-only database, with updates completely replacing existing precomputed views.&lt;ref name=big-data /&gt;{{rp|18}}

[[Hadoop|Apache Hadoop]] is the de facto standard batch-processing system used in most high-throughput architectures.&lt;ref&gt;Kar, Saroj. [http://cloudtimes.org/2014/05/28/hadoop-sector-will-have-annual-growth-of-58-for-2013-2020/ "Hadoop Sector will Have Annual Growth of 58% for 2013-2020"], 28 May 2014. ''Cloud Times''.&lt;/ref&gt;

===Speed layer===
[[File:Diagram of Lambda Architecture (named components).png|thumb|Diagram showing the flow of data through the processing and serving layers of lambda architecture. Example named components are shown.]]
The speed layer processes data streams in real time and without the requirements of fix-ups or completeness. This layer sacrifices throughput as it aims to minimize latency by providing real-time views into the most recent data. Essentially, the speed layer is responsible for filling the "gap" caused by the batch layer's lag in providing views based on the most recent data. This layer's views may not be as accurate or complete as the ones eventually produced by the batch layer, but they are available almost immediately after data is received, and can be replaced when the batch layer's views for the same data become available.&lt;ref name=big-data /&gt;{{rp|203}}

Stream-processing technologies typically used in this layer include [[Storm (event processor)|Apache Storm]], [[Sqlstream|SQLstream]] and [[Apache Spark]]. Output is typically stored on fast NoSQL databases.&lt;ref name=kinley&gt;Kinley, James. [http://jameskinley.tumblr.com/post/37398560534/the-lambda-architecture-principles-for-architecting "The Lambda architecture: principles for architecting realtime Big Data systems"], retrieved 26 August 2014.&lt;/ref&gt;&lt;ref&gt;Ferrera Bertran, Pere. [http://www.datasalt.com/2014/01/lambda-architecture-a-state-of-the-art/ "Lambda Architecture: A state-of-the-art"]. 17 January 2014, Datasalt.&lt;/ref&gt;

===Serving layer===
[[File:Diagram of Lambda Architecture (Druid data store).png|thumb|Diagram showing a lambda architecture with a Druid data store.]]
Output from the batch and speed layers are stored in the serving layer, which responds to ad-hoc queries by returning precomputed views or building views from the processed data.

Examples of technologies used in the serving layer include [[Druid (open-source data store)|Druid]], which provides a single cluster to handle output from both layers.&lt;ref name=metamarkets-lambda&gt;Yang, Fangjin, and Merlino, Gian. [https://speakerdeck.com/druidio/real-time-analytics-with-open-source-technologies-1 "Real-time Analytics with Open Source Technologies"]. 30 July 2014.&lt;/ref&gt; Dedicated stores used in the serving layer include [[Apache Cassandra]] or [[Apache HBase]] for speed-layer output, and [https://github.com/nathanmarz/elephantdb Elephant DB] or [[Cloudera Impala]] for batch-layer output.&lt;ref name=bijnens-slide /&gt;{{rp|45}}&lt;ref name=kinley /&gt;

==Optimizations==
To optimize the data set and improve query efficiency, various rollup and aggregation techniques are executed on raw data,&lt;ref name=metamarkets-lambda /&gt;{{rp|23}} while estimation techniques are employed to further reduce computation costs.&lt;ref&gt;Ray, Nelson. [https://metamarkets.com/2013/histograms/ "The Art of Approximating Distributions: Histograms and Quantiles at Scale"]. 12 September 2013. Metamarkets.&lt;/ref&gt; And while expensive full recomputation is required for fault tolerance, incremental computation algorithms may be selectively added to increase efficiency, and techniques such as ''partial computation'' and resource-usage optimizations can effectively help lower latency.&lt;ref name=big-data /&gt;{{rp|93,287,293}}

==Lambda architecture in use==
Metamarkets, which provides analytics for companies in the programmatic advertising space, employs a version of the lambda architecture that uses [[Druid (open-source data store)|Druid]] for storing and serving both the streamed and batch-processed data.&lt;ref name=metamarkets-lambda /&gt;{{rp|42}}

For running analytics on its advertising data warehouse, [[Yahoo]] has taken a similar approach, also using [[Storm (event processor)|Apache Storm]], [[Hadoop|Apache Hadoop]], and [[Druid (open-source data store)|Druid]].&lt;ref name=yahoo-lambda&gt;Rao, Supreeth; Gupta, Sunil. [http://www.slideshare.net/Hadoop_Summit/interactive-analytics-in-human-time?next_slideshow=1 "Interactive Analytics in Human Time"]. 17 June 2014&lt;/ref&gt;{{rp|9,16}}

The [[Netflix]] Suro project has separate processing paths for data, but does not strictly follow lambda architecture since the paths may be intended to serve different purposes and not necessarily to provide the same type of views.&lt;ref name=netflix&gt;Bae, Jae Hyeon; Yuan, Danny; Tonse, Sudhir. [http://techblog.netflix.com/2013/12/announcing-suro-backbone-of-netflixs.html "Announcing Suro: Backbone of Netflix's Data Pipeline"], ''[[Netflix]]'', 9 December 2013&lt;/ref&gt; Nevertheless, the overall idea is to make selected real-time event data available to queries with very low latency, while the entire data set is also processed via a batch pipeline. The latter is intended for applications that are less sensitive to latency and require a map-reduce type of processing.

==Criticism==
Criticism of lambda architecture has focused on its inherent complexity and its limiting influence. The batch and streaming sides each require a different code base that must be maintained and kept in sync so that processed data produces the same result from both paths. Yet attempting to abstract the code bases into a single framework puts many of the specialized tools in the batch and real-time ecosystems out of reach.&lt;ref&gt;{{cite web|last1=Kreps|first1=Jay|title=Questioning the Lambda Architecture|url=http://radar.oreilly.com/2014/07/questioning-the-lambda-architecture.html|website=radar.oreilly.com|publisher=Oreilly|accessdate=15 August 2014|ref=kreps}}&lt;/ref&gt;

In a technical discussion over the merits of employing a pure streaming approach, it was noted that using a flexible streaming framework such as [[Apache Samza]] could provide some of the same benefits as batch processing without the latency.&lt;ref&gt;[https://news.ycombinator.com/item?id=7976785 Hacker News] retrieved 20 August 2014&lt;/ref&gt; Such a streaming framework could allow for collecting and processing arbitrarily large windows of data, accommodate blocking, and handle state.

== References ==
&lt;!--- See http://en.wikipedia.org/wiki/Wikipedia:Footnotes on how to create references using &lt;ref&gt;&lt;/ref&gt; tags, these references will then appear here automatically --&gt;
{{Reflist}}

== External links ==
* [http://lambda-architecture.net/ Repository of Information on Lambda of Architecture]

&lt;!--- Categories ---&gt;
[[Category:Articles created via the Article Wizard]]
[[Category:Data processing]]
[[Category:Big data]]
[[Category:Data management]]
[[Category:Free software projects]]
[[Category:Software architecture]]</text>
      <sha1>t781nt95sbnlymxlf5mxb49ypph7n72</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Computer logging</title>
    <ns>14</ns>
    <id>52734920</id>
    <revision>
      <id>757516453</id>
      <parentid>757515801</parentid>
      <timestamp>2016-12-31T03:16:34Z</timestamp>
      <contributor>
        <username>Fixuture</username>
        <id>19796795</id>
      </contributor>
      <comment>+[[Category:Computing]]; +[[Category:Data management]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="51" xml:space="preserve">[[Category:Computing]]
[[Category:Data management]]</text>
      <sha1>bib0r2low7hskfj7ca4yi0kb8irmsur</sha1>
    </revision>
  </page>
  <page>
    <title>Cut, copy, and paste</title>
    <ns>0</ns>
    <id>157115</id>
    <revision>
      <id>762387291</id>
      <parentid>762386663</parentid>
      <timestamp>2017-01-28T14:03:33Z</timestamp>
      <contributor>
        <username>David.moreno72</username>
        <id>16075528</id>
      </contributor>
      <minor />
      <comment>Reverted edits by [[Special:Contributions/78.148.132.216|78.148.132.216]] ([[User talk:78.148.132.216|talk]]): Violation of [[WP:ELNO|external links]] policy ([[WP:HG|HG]]) (3.1.20)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="18889" xml:space="preserve">{{Redirect|Copy &amp; Paste|the album|Hurricane Venus}}
{{other uses|Cut and paste (disambiguation)}}
{{Refimprove|date=August 2008}}
In [[human–computer interaction]], '''cut''', '''copy''' and '''paste''' are related [[Command (computing)|commands]] that offer a [[user interface|user-interface]] [[interprocess communication]] technique for transferring [[data (computing)|data]]. The '''cut''' command removes the [[Selection (user interface)|selected data]] from its original position, while the '''copy''' command creates a duplicate; in both cases the selected data is kept in a temporary storage tool called the [[Clipboard (software)|clipboard]]. The data in the clipboard is later inserted in the position where the '''paste''' command is issued.

The command names are an [[interface metaphor]] based on the physical procedure used in [[manuscript]] editing to create a [[page layout]].

This [[interaction technique]] has close associations with related techniques in [[graphical user interface]]s that use [[pointing device]]s such as a [[computer mouse]] (by [[drag and drop]], for example).

The capability to replicate information with ease, changing it between contexts and applications, involves [[privacy]] concerns because of the risks of disclosure when handling [[Information sensitivity|sensitive information]]. Terms like ''cloning'', ''copy forward'', ''carry forward'', or ''re-use'' refer to the dissemination of such information through documents, and may be subject to regulation by [[administrative body|administrative bodies]].&lt;ref name="Laubach"&gt;{{cite web|url=http://hcca-info.org/portals/0/pdfs/resources/conference_handouts/regional_conference/2012/seattle/laubachwakefieldprint2.pdf|title=Cloning and Other Compliance Risks in Electronic Medical Records|last1=Laubach|first1=Lori|last2=Wakefield|first2=Catherine|date=June 8, 2012|publisher=[[Moss Adams LLP]], [[MultiCare]]|accessdate=April 23, 2014}}&lt;/ref&gt;

==History==

===Origins===
The term "''cut and paste''" comes from the traditional practice in manuscript-editings whereby people would cut paragraphs from a page with [[scissors]] and [[Adhesive|paste]] them onto another page. This practice remained standard into the 1980s. Stationery stores formerly sold "editing scissors" with blades long enough to cut an 8½"-wide page. The advent of [[photocopier]]s made the practice easier and more flexible.

The act of copying/transferring text from one part of a computer-based document ("[[Data buffer|buffer]]") to a different location within the same or different computer-based document was a part of the earliest on-line computer editors. As soon as computer data entry moved from punch-cards to online files (in the mid/late 1960s) there were "commands" for accomplishing this operation. This mechanism was often used to transfer frequently-used commands or text snippets from additional buffers into the document, as was the case with the [[QED (text editor)|QED]] editor.&lt;ref name="communications1967"&gt;{{citation|doi=10.1145/363848.363863|last1=Deutsch|first1=L. Peter|authorlink1=L. Peter Deutsch|last2=Lampson|first2=Butler W.|authorlink2=Butler Lampson|title=An online editor|journal=Communications of the ACM |volume=10|issue=12|year=1967|pages=793–799, 803|url=http://research.microsoft.com/en-us/um/people/blampson/04-OnlineEditor/04-OnlineEditor.htm&lt;!-- http://portal.acm.org/citation.cfm?id=363848.363863&amp;coll=ACM&amp;dl=ACM&amp;CFID=15669714&amp;CFTOKEN=68334085 --&gt;}}, p. 793.&lt;/ref&gt;

===Early methods===
The earliest editors, since they were designed for [[teleprinter]] terminals, provided [[computer keyboard|keyboard]] commands to delineate contiguous regions of text, remove such regions, or move them to some other location in the file.  Since moving a region of text required first removing it from its initial location and then inserting it into its new location various schemes had to be invented to allow for this multi-step process to be specified by the user.

Often this was done by the provision of a 'move' command, but some text editors required that the text be first put into some temporary location for later retrieval/placement. In 1983, the [[Apple Lisa]] became the first text editing system to call that temporary location "the clipboard".

Earlier control schemes such as [[NLS (computer system)|NLS]] used a [[Linguistic typology#Subject.E2.80.93verb.E2.80.93object positioning|verb-object command structure]], where the command name was provided first and the object to be copied or moved was second. The inversion from [[Subject–verb–object|verb-object]] to [[Subject–object–verb|object-verb]] on which copy and paste are based, where the user selects the object to be operated before initiating the operation, was an innovation crucial for the success of the desktop metaphor as it allowed copy and move operations based on [[direct manipulation]].&lt;ref&gt;{{cite paper|title=Metaphors create theories for users|author=Kuhn, Werner|journal=Spatial Information Theory A Theoretical Basis for GIS|pages=366–376|year=1993|publisher=Springer}}&lt;/ref&gt;

===Popularization===
Inspired by early line and character editors that broke a move or copy operation into two steps—between which the user could invoke a preparatory action such as navigation—[[Lawrence G. Tesler]] (Larry Tesler) proposed the names "cut" and "copy" for the first step and "paste" for the second step. Beginning in 1974, he and colleagues at [[Xerox PARC|Xerox Corporation Palo Alto Research Center (PARC)]] implemented several text editors that used cut/copy-and-paste commands to move/copy text.&lt;ref&gt;{{cite web|url=http://www.designinginteractions.com/ |title=Bill Moggridge, Designing Interactions, MIT Press 2007, pp. 63–68 |publisher=Designinginteractions.com |date= |accessdate=2011-11-25}}&lt;/ref&gt;

[[Apple Computer]] widely popularized the computer-based cut/copy-and-paste paradigm through the [[Apple Lisa|Lisa]] (1983) and [[Apple Macintosh|Macintosh]] (1984) operating systems and applications. Apple mapped the functionalities to key combinations consisting of the [[Command key]] (a special [[modifier key]]) held down while typing the letters X (for cut), C (for copy), and V (for paste), choosing a handful of [[keyboard shortcuts]] to control basic editing operations. The keys involved all cluster together at the left end of the bottom row of the standard [[QWERTY]] keyboard, and each key is combined with a special [[modifier key]] to perform the desired operation:
* [[control-Z|Z]] to [[undo]]
* [[control-X|X]] to cut
* [[control-C|C]] to copy
* [[control-V|V]] to paste
The [[IBM Common User Access]] (CUA) standard also uses combinations of the [[Insert key|Insert]], [[Del key|Del]], [[Shift key|Shift]] and [[Control key]]s.  Early versions of [[Microsoft Windows|Windows]]{{Dubious|date=March 2014}} used the IBM standard. [[Microsoft]] later also adopted the Apple key combinations with the introduction of [[Microsoft Windows|Windows]]{{Dubious|date=January 2016}}, using the [[control key]] as [[modifier key]]. For users migrating to Windows from [[MS-DOS]] this was a big change as MS-DOS users used the "copy" and "move" commands.

Similar patterns of key combinations, later borrowed by others, remain widely available {{As of|2007|alt= today}} in most GUI text editors, word processors, and file system browsers.

== Cut and paste ==
Computer-based editing can involve very frequent use of cut-and-paste operations. Most software-suppliers provide several methods for performing such tasks, and this can involve (for example)  key combinations, pulldown menus, pop-up menus, or [[toolbar]] buttons.
# The user selects or "highlights" the text or file for moving by some method, typically by [[dragging]] over the text or file name with the pointing-device or holding down the [[Shift key]] while using the [[arrow keys]] to move the [[Cursor (computers)|text cursor]].
# The user performs a "cut" operation via key combination [[Control key|Ctrl]]+x ([[Command key|⌘]]+x for [[Macintosh]] users), menu, or other means.
# Visibly, "cut" text immediately disappears from its location.  "Cut" files typically change color to indicate that they will be moved.
# Conceptually, the text has now moved to a location often called the [[Clipboard (software)|clipboard]]. The clipboard typically remains invisible. On most systems only one clipboard location exists, hence another cut or copy operation overwrites the previously stored information. Many [[Unix|UNIX]] text-editors provide multiple clipboard entries, as do some Macintosh programs such as Clipboard Master,&lt;ref&gt;{{cite web |title=Clipboard Master |work=Clipboard Master 2.0 by In Phase Consulting, July 1994|url=http://forums.info-mac.org/viewtopic.php?f=243&amp;t=14244&amp;sid=739ce1119f88340c52dc2aed3c788fff |accessdate=14 September 2009}}&lt;/ref&gt; and Windows [[clipboard manager|clipboard-manager]] programs such as the one in [[Microsoft Office]].
# The user selects a location for insertion by some method, typically by clicking at the desired insertion point.
# A ''paste'' operation takes place which visibly inserts the clipboard text at the insertion point. (The paste operation does not typically destroy the clipboard text: it remains available in the clipboard and the user can insert additional copies at other points).
Whereas cut-and-paste often takes place with a mouse-equivalent in Windows-like GUI environments, it may also occur entirely from the keyboard, especially in [[Unix|UNIX]] [[text editor]]s, such as [[Pico (text editor)|Pico]] or [[vi]]. Cutting and pasting without a mouse can involve a selection (for which Ctrl+x is pressed in most graphical systems) or the entire current line, but it may also involve text after the [[cursor (computers)|cursor]] until the end of the line and other more sophisticated operations.

When a software environment provides ''cut'' and ''paste'' functionality, a nondestructive operation called ''copy''  usually accompanies them; ''copy'' places a copy of the selected text in the clipboard without removing it from its original location.

The clipboard usually stays invisible, because the operations of cutting and pasting, while actually independent, usually take place in quick succession, and the user (usually) needs no assistance in understanding the operation or maintaining mental context. Some application programs provide a means of viewing, or sometimes even editing, the data on the clipboard.

== Copy and paste ==
The term "copy-and-paste" refers to the popular, simple method of reproducing [[Character (computing)|text]] or other [[data]] from a source to a destination. It differs from '''cut and paste''' in that the original source text or data does not get deleted or removed. The popularity of this method stems from its simplicity and the ease with which users can move data between various applications visually – without resorting to [[Disk storage|permanent storage]].

Once one has copied data into the [[clipboard]], one may '''paste''' the contents of the clipboard into a destination document.

The [[X Window System]] maintains an additional clipboard containing the most recently selected text; middle-clicking pastes the content of this "selection" clipboard into whatever the [[pointer (computing WIMP)|pointer]] is on at that time.

Most [[terminal emulator]]s and some other applications support the key combinations Ctrl-Insert to copy and Shift-Insert to paste. This is in accordance with the [[IBM Common User Access]] (CUA) standard.

== Find and go ==
The [[NeXTStep]] operating system extended the concept of having a single copy buffer by adding a second system-wide ''' Find buffer''' used for searching. The Find buffer is also available in [[OSX|Mac OS X]].

Text can be placed in the Find buffer by either using the Find panel or by selecting text and hitting {{key press|⌘E}}.

The text can then be searched with '''Find Next''' {{key press|⌘G}} and '''Find Previous''' {{key press|⌘D}}.

The functionality comes in handy when for example editing [[source code]]. To find the occurrence of a variable or function name elsewhere in the file, simply select the name by double clicking, hit {{key press|⌘E}} and then jump to the next or previous occurrence with {{key press|⌘G}} / {{key press|⌘D}}.

Note that this does ''not'' destroy your copy buffer as with other [[User interface|UIs]] like [[Windows]] or the [[X Window System]].

Together with copy and paste this can be used for quick and easy replacement of repeated text:
* select the text that you want to replace (i.e. by double clicking)
* put the text in the Find buffer with {{key press|⌘E}}
* overwrite the selected text with your replacement text
* select the replacement text (try {{key press| ⎇⇧←}} to avoid lifting your hands from the keyboard)
* copy the replacement text {{key press|⌘C}}
* find the next or previous occurrence {{key press|⌘G}} / {{key press|⌘D}}
* paste the replacement text {{key press|⌘V}}
* repeat the last two steps as often as needed
or in short:
* select {{key press|⌘ E}}  {{key press|replstr}}  {{key press| ⎇⇧←}}  {{key press|⌘C}}  {{key press|⌘G}}{{key press|⌘V}} {{key press|⌘G}}{{key press|⌘V}} ...
While this might sound a bit complicated at first, it is often ''much'' faster than using the find panel, especial when only a few occurrences shall be replaced or when only some of the occurrences shall be replaced. When a text shall not be replaced, simply hit {{key press|⌘G}} again to skip to the next occurrence.

The find buffer is system wide. That is, if you enter a text in the find panel (or with {{key press|⌘E}}) in one application and then switch to another application you can immediately start searching without having to enter the search text again.

== Common keyboard shortcuts ==
{| class="wikitable"
|-
! &amp;nbsp;
! Cut
! Copy
! Paste
|-
! Apple
| Command+X
| Command-C
| Command-V
|-
! Windows/GNOME/KDE
| Control-X / Shift-Delete
| Control-C / Control-Insert
| Control-V / Shift-Insert
|-
! GNOME/KDE terminal emulators
| &lt;!-- cut --&gt;
| Shift-Control-C / Control-Insert
| Shift-Control-V / Shift-Control-Insert (Shift-Insert for pasting selected text)
|-
! BeOS
| Alt-X
| Alt-C
| Alt-V
|-
! Common User Access
| Shift+Delete
| Control+Insert
| Shift+Insert
|-
! Emacs
| Control-W (to mark)&lt;br /&gt;Control-K (to end of line)
| [[Meta key|meta]]-W (to mark)
| Control-Y
|-
! vi
| d (delete)
| y (yank)
| p (put)
|-
! X Window System
| &lt;!-- cut --&gt;
| click-and-drag to highlight
| middle mouse button
|}

== Copy and paste automation ==
Copying data one by one from one application to another, such as from [[Microsoft Excel|Excel]] to a [[Form (HTML)|web form]], might involve a lot of manual work. Copy and paste can be automated with the help of a [[Computer program|program]] that would iterate through the values list and paste them to the active [[Window (computing)|application window]]. Such programs might come in the form of [[Macro (computer science)|macros]] or dedicated programs which involve more or less scripting. Alternatively, applications supporting [[simultaneous editing]] may be used to copy or move collections of items.

== Additional differences between moving and copying ==&lt;!-- This section is linked from [[Spreadsheet]] --&gt;
In a spreadsheet, moving (cut and paste) need not equate to copying (copy and paste) and then deleting the original: when moving, references to the moved cells may move accordingly.

[[Windows Explorer]] also differentiates moving from merely copy-and-delete: a "cut" file will not actually disappear until pasted elsewhere and cannot be pasted more than once. The icon fades to show the transient "cut" state until it is pasted somewhere. Cutting a second file while the first one is cut will release the first from the "cut" state and leave it unchanged. Shift+Delete cannot be used to cut files; instead it deletes them without using the Recycle bin.

== Multiple clipboards ==
Several editors allow copying text into or pasting text from specific clipboards, typically using a special keystroke-sequence to specify a particular clipboard-number.

[[Clipboard manager]]s can be very convenient productivity-enhancers by providing many more features than system-native clipboards. Thousands of clips from the clip history are available for future pasting, and can be searched, edited, or deleted. Favorite clips that a user frequently pastes (for example, the current date, or the various fields of a user's contact info) can be kept standing ready to be pasted with a few clicks or keystrokes.

Similarly, a '''kill ring''' provides a [[LIFO (computing)|LIFO]] [[stack (data structure)|stack]] used for cut-and-paste operations as a type of clipboard capable of storing multiple pieces of data.&lt;ref&gt;{{cite web|url=http://www.ai.sri.com/~gkb/general.html#kill-ring |title=GKB (Generic Knowledge Base) Editor user's manual |work=[[Artificial Intelligence Center]] |publisher=[[SRI International]] |accessdate=2011-11-25}}&lt;/ref&gt;
For example, the [[GNU Emacs]] text editor provides a kill ring.&lt;ref&gt;{{cite web|url=https://www.gnu.org/software/emacs/manual/html_mono/emacs.html#Kill-Ring |title=GNU Emacs manual |publisher=Gnu.org |date= |accessdate=2011-11-25}}&lt;/ref&gt;
Each time a user performs a cut or copy operation, the system adds the affected text to the ring. The user can then access the contents of a specific (relatively numbered) buffer in the ring when performing a subsequent paste-operation. One can also give kill-buffers individual names, thus providing another form of multiple-clipboard functionality.

==Use in healthcare==
Concerns have been raised over the use of copy and paste functions in healthcare documentation and [[electronic health records]]. There is potential for the introduction of [[medical error|errors]], [[information overload]], and [[fraud]].&lt;ref name="Laubach" /&gt;&lt;ref&gt;{{cite web|url=http://library.ahima.org/xpedio/groups/public/documents/ahima/bok1_050621.pdf|title=Appropriate Use of the Copy and Paste Functionality in Electronic Health Records|date=March 17, 2014|publisher=[[American Health Information Management Association]]|accessdate=April 23, 2014}}&lt;/ref&gt;

==Use in software development==
[[Copy and paste programming]] is an [[antipattern]] arising from the blind pasting of pre-existing code into another [[source code]] file.

== See also ==
* [[Clipboard (software)|Clipboard]]
* [[Control key]]
* [[Cut and paste job]]
* [[Drag and drop]]
* [[Photomontage]]
* [[Publishing Interchange Language]]
* [[Simultaneous editing]]
* [[X Window selection]]

== References ==
{{Reflist}}

== External links ==
* [http://tronche.com/gui/x/icccm/sec-2.html 2. Peer-to-Peer Communication by Means of Selections] in the [[ICCCM]]

[[Category:User interface techniques]]
[[Category:Data management]]
[[Category:Clipboard (computing)]]</text>
      <sha1>s1e4b6bqomqly4u9q3m6u6qieewxtqe</sha1>
    </revision>
  </page>
  <page>
    <title>Rubrik</title>
    <ns>0</ns>
    <id>52955620</id>
    <revision>
      <id>762349594</id>
      <parentid>761580008</parentid>
      <timestamp>2017-01-28T06:35:01Z</timestamp>
      <contributor>
        <username>Majora</username>
        <id>25977978</id>
      </contributor>
      <comment>+logo</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5235" xml:space="preserve">'''Rubrik''' is a privately held cloud data management company headquartered in [[Palo Alto, California|Palo Alto, CA]]. Rubrik offers a data management platform for enterprises in private, public, and hybrid [[Cloud computing|cloud environments]].&lt;ref&gt;{{Cite web|url=http://social.techcrunch.com/2016/08/16/rubrik-snares-61-million-series-c-led-by-khosla-ventures/|title=Rubrik snares $61 million Series C led by Khosla Ventures|last=Miller|first=Ron|website=TechCrunch|access-date=2017-01-23}}&lt;/ref&gt;

{{Infobox company
| name = Rubrik
| type = Private
| logo = Rubrik logo.png
| industry = Cloud Data Management
| founder =  Bipul Sinha, Arvind Jain, Soham Mazumdar, and Arvind Nithrakashyap
| hq_location_city = Palo Alto, CA
| hq_location_country = United States
| products = Rubrik Cloud Data Management platform
| website = {{URL|www.rubrik.com/}}
}}

== History ==
Rubrik was founded in 2014 by Bipul Sinha, Arvind Jain, Soham Mazumdar, and Arvind Nithrakashyap.&lt;ref&gt;{{Cite news|url=http://www.forbes.com/sites/benkepes/2015/03/24/with-an-a-grade-founding-team-and-a-grade-investors-rubrik-launches/#219bf1ea2b6e|title=With An A-Grade Founding Team And A-Grade Investors, Rubrik Launches|last=Kepes|first=Ben|newspaper=Forbes|access-date=2017-01-23}}&lt;/ref&gt; It raised $10 million in March 2015,&lt;ref&gt;{{Cite news|url=http://blogs.wsj.com/venturecapital/2015/03/24/rubrik-emerges-with-10-million-for-software-defined-backup/|title=Rubrik Emerges With $10 Million for Software-Defined Backup|last=Gage|first=Deborah|newspaper=WSJ|language=en-US|access-date=2017-01-23}}&lt;/ref&gt; followed by a $41 million round that same May. In August 2016, Rubrik raised an additional $61 million in funding led by [[Khosla Ventures]].&lt;ref&gt;{{Cite news|url=http://www.businessinsider.com/rubrik-raises-61-million-khosla-ventures-2016-8|title=This investor turned founder scoffs at funding slowdown: 'real businesses' can still get money|newspaper=Business Insider|language=en|access-date=2017-01-23}}&lt;/ref&gt; Sinha, who is Founding Investor at [[Nutanix]] and a partner at [[Lightspeed Venture Partners]], currently serves as CEO.

In 2015, Rubrik launched with its r300 series appliances, called "Briks", in two configurations for VMware environments. The appliance includes a console from which users can manage and monitor their data.&lt;ref&gt;{{Cite web|url=http://cormachogan.com/2015/05/26/a-closer-look-at-rubrik/|title=A closer look at Rubrik|date=2015-05-26|website=CormacHogan.com|access-date=2017-01-23}}&lt;/ref&gt; In April 2016, it launched its r528 Brik, which is [[FIPS 140-2]] Level 2 certified.&lt;ref&gt;{{Cite web|url=http://www.thepaypers.com/default/rubrik-r528-provides-storage-encryption-and-security/764067-0|title=Rubrik r528 provides storage encryption and security|website=www.thepaypers.com|access-date=2017-01-23}}&lt;/ref&gt;

In April 2016, the company released their Cloud Data Management platform with Rubrik Firefly, which extended capabilities to physical [[SQL]] and [[Linux]] environments, and with Rubrik Edge, a software appliance that extended protection to remote and branch offices. The release also included a software upgrade to utilize [[Erasure code|erasure coding]].&lt;ref&gt;{{Cite web|url=http://www.theregister.co.uk/2016/08/16/rubriks_extra_funding_as_firefly_data_management_flies_out_of_the_coop/|title=Rubrik's extra funding as Firefly extended data management flies out of the coop|last=18:13|first=16 Aug 2016 at|last2=tweet_btn()|first2=Chris Mellor|access-date=2017-01-23}}&lt;/ref&gt;&lt;ref&gt;{{Cite web|url=http://blog.mwpreston.net/2016/08/16/rubrik-firefly-released/|title=Rubrik Firefly - Now with physical, edge, and moar cloud!|date=2016-08-16|website=mwpreston.net|access-date=2017-01-23}}&lt;/ref&gt;

== Awards ==
Rubrik was a winner of the 2015 Virtualization Review Editor's Choice Awards.&lt;ref&gt;{{Cite web|url=https://virtualizationreview.com/articles/2015/12/01/editors-choice-awards.aspx|title=The 2015 Virtualization Review Editor's Choice Awards -|last=Ward|first=By Keith|last2=01/04/2016|website=Virtualization Review|access-date=2017-01-23}}&lt;/ref&gt; In 2016, Rubrik was named a [[Gartner]] Cool Vendor in Storage Technologies.&lt;ref&gt;{{Cite web|url=https://www.gartner.com/doc/3290518/cool-vendors-storage-technologies-|title=Cool Vendors in Storage Technologies, 2016|website=www.gartner.com|access-date=2017-01-23}}&lt;/ref&gt; Rubrik won “Best security or data protection project” for Best of VMworld Europe User Awards 2016.&lt;ref&gt;{{Cite news|url=http://www.techtarget.com/press-release/techtargets-searchservervirtualization-com-announces-best-vmworld-2016-award-winners/|title=TechTarget’s SearchServerVirtualization.com Announces “Best of VMworld” 2016 Award Winners - TechTarget|newspaper=TechTarget|language=en-US|access-date=2017-01-23}}&lt;/ref&gt; In October 2016, Rubrik was selected as one of 25 “Next Billion-Dollar Startups” by [[Forbes]].&lt;ref&gt;{{Cite news|url=http://www.forbes.com/sites/amyfeldman/2016/10/19/next-billion-dollar-startups-2016/#1221eca0554e|title=Next Billion-Dollar Startups 2016|last=Feldman|first=Amy|newspaper=Forbes|access-date=2017-01-23}}&lt;/ref&gt;

== References ==
{{Reflist}}

[[Category:Data management]]
[[Category:Cloud computing providers]]
[[Category:Companies based in California]]</text>
      <sha1>9ufu6w39csishpj99q1y1c7w1fl1o6f</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Video storage</title>
    <ns>14</ns>
    <id>764184</id>
    <revision>
      <id>620148985</id>
      <parentid>543806562</parentid>
      <timestamp>2014-08-06T21:48:02Z</timestamp>
      <contributor>
        <username>Jdaloner</username>
        <id>4460044</id>
      </contributor>
      <minor />
      <comment>Changed how this is sorted in "Video" category.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="120" xml:space="preserve">[[Category:Video|Storage]]
[[Category:Electronic documents]]
[[Category:Information storage]]
[[Category:Storage media]]</text>
      <sha1>6q44jesv56i5ac1rlsw4iuebb18knx1</sha1>
    </revision>
  </page>
  <page>
    <title>Electronic article</title>
    <ns>0</ns>
    <id>1005736</id>
    <revision>
      <id>737642855</id>
      <parentid>643129478</parentid>
      <timestamp>2016-09-04T04:45:41Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor />
      <comment>[[User:Green Cardamom/WaybackMedic 2|WaybackMedic 2]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3215" xml:space="preserve">'''Electronic articles''' are [[Article (publishing)|article]]s in [[academic journal|scholarly journal]]s or [[magazine]]s  that can be accessed via electronic transmission. They are a specialized form of [[electronic document]], with a specialized content, purpose, format, [[metadata]], and availability&amp;ndash;they consist of individual articles from scholarly journals or  magazines (and now sometimes popular magazines), they have the purpose of providing material for academic [[research]] and study, they are formatted approximately like printed journal articles, the metadata is entered into specialized databases, such as the [[Directory of Open Access Journals]] as well as the databases for the discipline, and they are predominantly available through [[academic library|academic libraries]] and special [[library|libraries]], generally at a fixed charge. 

Electronic articles can be found in [[online and offline|online]]-only journals (par excellence), but in the 21st century they have also become common as online versions of articles that also appear in printed journals. The practice of [[Electronic publishing|publishing of an electronic version]] of an article before it later appears in print is sometimes called '''epub ahead of print''', particularly in [[PubMed]].&lt;ref&gt;{{cite web |url=http://www.nlm.nih.gov/services/ldepubahead.html |title=FAQ: Loansome Doc Article Ordering Service - Epub Ahead of Print |work= |accessdate=2010-10-23}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://www.gwumc.edu/library/blog/client/index.cfm/2007/11/26/Epub-ahead-of-print-What-does-this-mean |title=Himmelfarb Library Blog: Epub ahead of print… What does this mean?? |format= |work= |archiveurl=https://web.archive.org/web/20100119081653/http://www.gwumc.edu/library/blog/client/index.cfm/2007/11/26/Epub-ahead-of-print-What-does-this-mean |archivedate=2010-01-19 |deadurl=yes }}&lt;/ref&gt;

The term can also be used for the electronic versions of less formal publications, such as online archives, working paper archives from universities, government agencies, private and public think tanks and institutes and private websites. In many academic areas, specialized [[bibliographic database]]s are available to find their online content.

Most commercial sites are [[subscription business model|subscription]]-based, or allow pay-per-view access. Many universities subscribe to electronic journals to provide access to their students and faculty, and it is generally also possible for individuals to subscribe. An increasing number of journals are now available with open access, requiring no subscription. Most working paper archives and articles on personal homepages are free, as are collections in [[institutional repository|institutional repositories]] and [[disciplinary repository|subject repositories]].

The most common formats of transmission are [[HTML]], [[Portable Document Format|PDF]] and, in specialized fields like mathematics and physics, [[TeX]] and [[PostScript]].

==See also==
* [[Academic publishing]]
* [[Eprint]]
* [[Electronic journal]]
* [[Scholarly article]]

== References ==
{{reflist}}

[[Category:Academic publishing]]
[[Category:Electronic publishing]]
[[Category:Electronic documents]]</text>
      <sha1>94d8dp10ur695gsnfblszfl3g7zmhn1</sha1>
    </revision>
  </page>
  <page>
    <title>Xplor International</title>
    <ns>0</ns>
    <id>17762769</id>
    <revision>
      <id>761461298</id>
      <parentid>736922667</parentid>
      <timestamp>2017-01-23T03:10:04Z</timestamp>
      <contributor>
        <username>Ira Leviton</username>
        <id>25046916</id>
      </contributor>
      <minor />
      <comment>Fixed a spelling error.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4028" xml:space="preserve">{{about|the trade association|the software package|X-PLOR}}
{{Refimprove|date=June 2008}}
'''Xplor International'''&lt;ref&gt;'''Technology Trends: Xplor Directions Survey''',
Dr. Keith Davidson PhD, edp, ''Enterprise Journal, 1998''. Example of Xplor industry research on [[transaction documents]] and [[laser printing]] trends http://esj.com/article.aspx?ID=10229843143PM&lt;/ref&gt; also known as '''The Electronic Document Systems Association''' is an international [[trade association]] specifically focused on the issues of [[transaction document]]s. Transaction documents are legally relevant documents that are either printed and mailed or are electronically delivered e.g. [[Bill (payment)|bills]], [[bank statements]], [[insurance policies]] etc.

The acronym XPLOR was derived from ''Xerox Printer Liaison ORganization'', the original association name. Xplor expanded its mission in 1983 to include other vendors' technology and adopted the acronym as the organization's name.

== History ==
Xplor International was founded in 1980 as a trade association specifically for transaction document applications, due to the difference in emphasis on variable data. Originally a user group for the [[Xerox 9700]] laser printer, they reshaped its mission in the early 1980s to address the entire transaction document industry. Hardware companies like [[IBM]], [[Siemens]] (later [[Océ]]), [[Hewlett Packard]], [[Pitney Bowes]], [[Bell &amp; Howell]], and [[Xerox]] have been actively involved as have software companies like Image Sciences (later Docucorp International), Document Sciences, [[Cincom Systems]], [[GMC Software AG|GMC Software Technology]], Xenos, Crawford Technologies, supported Xplor in order to promote a venue for the issues that are unique to the creation of transaction documents.

In the 1990s, Xplor began to shift from solely document “printing” to document “printing and presentation”, as transaction documents came to be presented on the Web.

==Membership==
Xplor’s membership of users and vendors is worldwide, with approximately 45% of the membership in the early 2000s being outside the US.&lt;ref&gt;William J. 'Bill' McCalpin edp, former General Manager of Xplor International, 2008&lt;/ref&gt;

== Xplor honours and awards ==
Xplor awards various technology providers with awards each year, including:

*The Technology Application Award is presented to an individual, a company, or an organization to recognize outstanding achievement in the imaginative application of current technology and/or unique implementation of existing [[electronic document]] systems.
*The Innovator of the Year Award honors an individual, company, or organization that has conceived and developed an original concept leading to a significant advancement in the industry. The "Innovator" has advanced a new program product or technology that notably enhances the capabilities of [[electronic document]] systems.
*The Xplorer of the Year is Xplor International's most prestigious award; it honors significant service to the Association, dedication to the Xplor mission, and notable achievement promoting the interest of the [[electronic document]] systems industry.
*The Brian Platte Lifetime Achievement Award, established in 2007, is given to an individual whose efforts and contributions have significantly changed the course and development of the digital document industry.

=== Electronic Document Professional ===

Xplor manages the [[Electronic Document Professional]] (EDP) certification program for people experienced in [[electronic document]] systems and/or application development.

==Associations in related fields==
* [[Association for Information and Image Management]], the association for electronic content management
* [[Association of Records Managers and Administrators]], the association for records management professionals

==External links==
* [http://www.xplor.org Xplor International] website

== References ==

{{Reflist}}

[[Category:Electronic documents]]
[[Category:International trade associations]]</text>
      <sha1>fujpocswp0fu3ymcg8uvh7h69mrwpbt</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Content management systems</title>
    <ns>14</ns>
    <id>691651</id>
    <revision>
      <id>701125595</id>
      <parentid>694311607</parentid>
      <timestamp>2016-01-22T17:11:25Z</timestamp>
      <contributor>
        <username>Horcrux92</username>
        <id>10845682</id>
      </contributor>
      <comment>removed [[Category:Data management]]; added [[Category:Data management software]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="575" xml:space="preserve">{{Category diffuse}}
{{Cat main|Content management system}}
{{Commons category}}

A '''[[content management system]]''' ('''CMS''') is a system used to organize and facilitate collaborative content creation. Recently, the term has been used specifically to refer to programs on [[WWW]] [[Web server|servers]], but it can also refer to hardware devices that manage documents on a large network.

[[Category:Web software]]
[[Category:Internet Protocol based network software]]
[[Category:Data management software]]
[[Category:Office software]]
[[Category:Electronic documents]]</text>
      <sha1>1c2iimulxcdelecy24wfon2tnlnuk01</sha1>
    </revision>
  </page>
  <page>
    <title>E-bible</title>
    <ns>0</ns>
    <id>25199152</id>
    <revision>
      <id>654173743</id>
      <parentid>654173240</parentid>
      <timestamp>2015-03-30T12:44:02Z</timestamp>
      <contributor>
        <ip>46.231.189.26</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1069" xml:space="preserve">{{Orphan|date=December 2009}}
{{Refimprove|date=December 2009}}

Sometimes known as document bibles or transaction deal bibles, '''e-bibles''' are a means of storing, indexing and comprehensively searching large volumes of [[document]]s related to any corporate transaction. 

They are commonly used by [[Legal firm]]s to collate documents from a certain case in order to store or give to a client at the end of a project. e-bibles are a means of storing complex legal folders which were usually kept in hard copy.

In 2009, Proposals&lt;ref&gt;http://www.litig.org/index.php?option=com_content&amp;task=category&amp;sectionid=2&amp;id=20&amp;Itemid=33 &lt;/ref&gt; were put in place in order to standardise the creation of e-bibles throughout the [[legal industry]].

There are few suppliers of COTS solutions, however Diskbuilder&lt;ref&gt;http://www.diskbuilder.co.uk&lt;/ref&gt;  and Ideagen&lt;ref&gt;https://www.ideagenplc.com/&lt;/ref&gt; (formerly Capgen) are notable exceptions.
== References ==
{{Reflist}}

[[Category:Legal documents]]
[[Category:Electronic documents]]
[[Category:Document management systems]]</text>
      <sha1>3h70nx0rnaezrbdh1zj26bpdacm3r59</sha1>
    </revision>
  </page>
  <page>
    <title>Aperture card</title>
    <ns>0</ns>
    <id>8403499</id>
    <revision>
      <id>749306895</id>
      <parentid>744606694</parentid>
      <timestamp>2016-11-13T17:40:05Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor />
      <comment>1 archive template merged to {{[[template:webarchive|webarchive]]}} ([[User:Green_Cardamom/Webarchive_template_merge|WAM]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6058" xml:space="preserve">[[Image:Aperture card.JPG|400px|right]]
An '''aperture card''' is a type of '''[[punched card]]''' with a cut-out window into which a chip of '''[[microform|microfilm]]''' is mounted.  Such a card is used for [[archive|archiving]] or for making multiple inexpensive copies of a document for ease of distribution.  The card is typically punched with machine-readable [[metadata]] associated with the microfilm image, and printed across the top of the card for visual identification.  The microfilm chip is most commonly 35mm in height, and contains an [[optics|optically reduced]] image, usually of some type of reference document, such as an [[engineering drawing]], that is the focus of the archiving process.  Aperture cards have several advantages and disadvantages when compared to digital systems.  Machinery exists to automatically store, retrieve, sort, duplicate, create, and digitize cards with a high level of automation.  While many aperture cards still play an important role in archiving, their role is gradually being replaced by digital systems.

== Usage ==
Aperture cards are used for engineering drawings from all engineering disciplines.  The [[U.S. Department of Defense]] once made extensive use of aperture cards, and some are still in use, but most data is now digital.&lt;ref&gt;[https://web.archive.org/web/20060530111716id_/http://federalvoice.dscc.dla.mil/federalvoice/030924/tech.html Federal use of aperture cards (Archived Copy)]&lt;/ref&gt;

Information about the drawing, for example the drawing number, could be both punched and printed on the remainder of the card.  With the proper machinery, this allows for automated handling.  In the absence of such machinery, the cards can still be read by a human with a lens and a light source.

=== Advantages ===
Aperture cards have, for archival purposes, some advantages over digital systems.  They have a 500-year lifetime, they are human readable, and there is no expense or risk in converting from one digital format to the next when computer systems become obsolete.&lt;ref&gt;{{cite journal|first=Ed |last=LoTurco |title=The Engineering Aperture Card: Still Active, Still Vital |publisher=EDM Consultants |date=January 2004 |url=http://www.aiimne.org/library/LoTurcoWhitePaper1.pdf |accessdate=October 10, 2007 |archiveurl=https://web.archive.org/web/20071128162738/http://www.aiimne.org/library/LoTurcoWhitePaper1.pdf |archivedate=November 28, 2007 |deadurl=no |df= }}&lt;/ref&gt;

=== Disadvantages ===
{{unreferenced section|date=February 2015}}
Most of the disadvantages are related to the well established differences in analog and digital technology. In particular, searching for given strings within content is considerably slower.  Handling physical cards requires proprietary machinery and processing optical film takes significant time.

The very nature of microfilm cameras and the high contrast properties of microfilm stock itself also impose limits on the amount of detail that can be resolved particularly at the higher reduction ratios (36x or greater) needed to film larger drawings. Faded drawings or those of low or uneven contrast do not reproduce well and significant detail or annotations may be lost.

In common with other forms of microfilm mis-filing cards after use, particularly in large archives, results in the card being for all intents and purposes lost forever unless it's later found by accident.

Aperture cards created from 35mm roll film mounted on to blank cards have to be treated with great care. Bending the card can cause the film to detach and excessive pressure to a stack of cards can cause the mounting glue to ooze creating clumps of cards which will feed through duplicators and other machinery either poorly or not at all. Feeding a de-laminated card through machinery not only risks destroying the image it also risks jamming or damaging the machinery.

== Machinery ==
A set of cards could be rapidly sorted by drawing number or other punched data using a [[IBM 80 series Card Sorters|card sorter]].  Machines are now available that [[Image scanner|scan]] aperture cards and produce a digital version.&lt;ref&gt;For example, this aperture card scanner from  [http://www.oceusa.com/main/product_detail.jsp?FOLDER%3C%3Efolder_id=1408474395186237&amp;PRODUCT%3C%3Eprd_id=845524441761057 Oce']&lt;/ref&gt;  Aperture card plotters are machines that use a laser to create the image on the film.&lt;ref&gt;For example, this aperture card plotter from [http://www.wwl.co.uk/apertureplotters.htm Wicks &amp; Wilson] {{webarchive |url=https://web.archive.org/web/20060627060408/http://www.wwl.co.uk/apertureplotters.htm |date=June 27, 2006 }}&lt;/ref&gt;

== Conversion ==
Aperture cards can be converted to digital documents using scanning equipment and software. The scan software we use allows for significant image cleanup and enhancement. Often, the digital image produced is better than the visual quality available prescan. A variety of output image types can be generated, most notably, Group 4 TIFF and PDF.&lt;ref&gt;{{cite web|last1=Bryant|first1=Joe|title=Aperture Card Scanning|url=http://www.microcomseattle.com/solutions/document-scanning/aperture-card/|website=Micro Com Seattle|accessdate=17 March 2015}}&lt;/ref&gt;

== References ==
{{Reflist}}

== External links ==
* [http://stinet.dtic.mil/oai/oai?verb=getRecord&amp;metadataPrefix=html&amp;identifier=AD0232960 1959 Defense Technical Information Center report] on the technology and its use for submitting engineering plans to the military.
* [http://www.wipo.int/export/sites/www/scit/en/standards/pdf/03-07-a.pdf Detailed description of a particular format]{{dead link|date=October 2016 |bot=InternetArchiveBot |fix-attempted=yes }} of Aperture cards from [[WIPO]].
* [https://web.archive.org/web/20090327011959/http://www.green-sheet.net/tutorial2.6.htm Detailed information regarding duplicating microforms and aperture cards] (select and highlight to read black on black text)

[[Category:Archival science]]
[[Category:History of computing]]
[[Category:Infographics]]
[[Category:Technical drawing]]
[[Category:Electronic documents]]</text>
      <sha1>a92bnuc4unheomcofdyhq6hibigpfvo</sha1>
    </revision>
  </page>
  <page>
    <title>Quickstart guide</title>
    <ns>0</ns>
    <id>12918613</id>
    <revision>
      <id>407353675</id>
      <parentid>383762117</parentid>
      <timestamp>2011-01-11T22:08:34Z</timestamp>
      <contributor>
        <username>Frap</username>
        <id>612852</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="538" xml:space="preserve">A '''quickstart guide''' is a short, simple introductory guide to a piece of equipment for many consumer electronics products (e.g. [[television]]) or recently, [[automobile]]s, [[mobile phone]]s, computers connection. 

With the increase in complexity and functions with electronics products quickstart guides are created to get users quickly accustomed to the basic operations of the product. Complex or detailed operations are usually left in the full-length [[owner's manual]].

{{Electronics-stub}}

[[Category:Electronic documents]]</text>
      <sha1>949y2weqpt0efoxg7dui0iznnzk10z6</sha1>
    </revision>
  </page>
  <page>
    <title>Structured document</title>
    <ns>0</ns>
    <id>23524003</id>
    <revision>
      <id>738692612</id>
      <parentid>602852155</parentid>
      <timestamp>2016-09-10T14:13:56Z</timestamp>
      <contributor>
        <username>Owen Ambur</username>
        <id>3035628</id>
      </contributor>
      <comment>/* See also */ Added link to Machine-Readable Documents</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5049" xml:space="preserve">{{Refimprove|date=April 2014}}
A '''structured document''' is an [[electronic document]] where some method of [[embedded coding]], such as [[markup language|mark-up]], is used to give the whole, and parts, of the document various structural meanings according to a [[Database schema|schema]]. A structured document whose mark-up doesn't break the schema and is designed to conform to and which obeys the [[syntax]] rules of its [[markup language|mark-up language]] is "well-formed".

{{Quote|The [[Standard Generalized Markup Language]] (SGML) has pioneered the concept of structured documents|[http://www.w3.org/People/Janne/porject/paper.html Multi-purpose publishing using HTML, XML, and CSS], [http://www.w3.org/People/howcome/ Håkon Wium Lie] &amp; [http://www.w3.org/People/Janne/ Janne Saarela]}}

As of 2009 the most widely used [[markup language]], in all its evolving forms, is [[HTML]], which is used to structure documents according to various [[Document Type Definition|Document Type Definition (DTD)]] schema defined and described by the [[W3C]], which continually reviews, refines and evolves the specifications.

{{Quote|[[XML]] is the universal format for structured documents and data on the Web|[http://www.w3.org/MarkUp/#related XHTML2 Working Group], [[W3C]]}}

==Structural semantics==
In writing structured documents the focus is on encoding the logical structure of a document, with no explicit concern in the structural markup for its presentation to humans by printed pages, screens or other means. Structured documents, especially well formed ones, can easily be processed by computer systems to extract and present [[metadata]] about the document. In most Wikipedia articles for example, a table of contents is automatically generated from the different heading tags in the body of the document. Popular [[word processor]]s can have such a function available.

In [[HTML]] a part of the logical structure of a document may be the document body; &lt;code&gt;&lt;nowiki&gt;&lt;body&gt;&lt;/nowiki&gt;&lt;/code&gt;, containing a first level heading; &lt;code&gt;&lt;nowiki&gt;&lt;h1&gt;&lt;/nowiki&gt;&lt;/code&gt;, and a paragraph; &lt;code&gt;&lt;nowiki&gt;&lt;p&gt;&lt;/nowiki&gt;&lt;/code&gt;.

&lt;code&gt;&lt;nowiki&gt;&lt;body&gt;&lt;/nowiki&gt;&lt;/code&gt;
:&lt;code&gt;&lt;nowiki&gt;&lt;h1&gt;Structured document&lt;/h1&gt;&lt;/nowiki&gt;&lt;/code&gt;
:&lt;code&gt;&lt;nowiki&gt;&lt;p&gt;A &lt;strong class="selflink"&gt;structured document&lt;/strong&gt; is an &lt;a href="/wiki/Electronic_document" title="Electronic document"&gt;electronic document&lt;/a&gt; where some method of &lt;a href="/w/index.php?title=Embedded_coding&amp;amp;action=edit&amp;amp;redlink=1" class="new" title="Embedded coding (page does not exist)"&gt;embedded coding&lt;/a&gt;, such as &lt;a href="/wiki/Markup_language" title="Markup language"&gt;markup&lt;/a&gt;, is used to give the whole, and parts, of the document various structural meanings according to a &lt;a href="/wiki/Schema" title="Schema"&gt;schema&lt;/a&gt;.&lt;/nowiki&gt;&lt;/code&gt;&lt;code&gt;&lt;nowiki&gt;&lt;/p&gt;&lt;/nowiki&gt;&lt;/code&gt;
&lt;code&gt;&lt;nowiki&gt;&lt;/body&gt;&lt;/nowiki&gt;&lt;/code&gt;

One of the most attractive features of structured documents is that they can be reused in many contexts and presented in various ways on mobile phones, TV screens, speech synthesisers, and any other device which can be programmed to process them.

=== Other semantics ===
Other meaning can be ascribed to text which isn't structural. In the [[HTML]] fragment above, there is semantic markup which has nothing to do with structure; the first of these, the &lt;code&gt;&lt;nowiki&gt;&lt;strong&gt;&lt;/nowiki&gt;&lt;/code&gt; tag, means that the enclosed text should be given a strong emphasis. In visual terms this is equivalent to the bold, &lt;code&gt;&lt;nowiki&gt;&lt;b&gt;&lt;/nowiki&gt;&lt;/code&gt; tag, but in speech synthesisers this means a voice inflection giving strong emphasis is used. The term [[semantic markup]] excludes markup like the bold tag which has no meaning other than an instruction to a visual display. The strong tag means that the presentation of the enclosed text should have a strong emphasis in all presentation forms, not just visual.&lt;br /&gt;
The anchor &lt;code&gt;&lt;nowiki&gt;&lt;a&gt;&lt;/nowiki&gt;&lt;/code&gt; tag is a more obvious example of semantic markup unconcerned with structure, with its href attribute set it means that the text it surrounds is a [[hyperlink]].

[[HTML]] from early on has also had tags which gave presentational semantics, i.e. there were tags to give '''bold''' (&lt;code&gt;&lt;nowiki&gt;&lt;b&gt;&lt;/nowiki&gt;&lt;/code&gt;)or ''italic'' (&lt;code&gt;&lt;nowiki&gt;&lt;i&gt;&lt;/nowiki&gt;&lt;/code&gt;) text, or to alter &lt;small&gt;font sizes&lt;/small&gt; or which had other effects on the presentation.&lt;ref&gt;{{cite web|url=http://www.w3.org/MarkUp/draft-ietf-iiir-html-01.txt|accessdate=5 March 2014}}&lt;/ref&gt; Modern versions of markup languages discourage such markup in favour of [[Style sheet language|style sheets]]. Different style sheets can be attached to any markup, semantic or presentational, to produce different presentations. In [[HTML]], tags such as; &lt;code&gt;&lt;nowiki&gt;&lt;a&gt;, &lt;blockquote&gt;, &lt;em&gt;, &lt;strong&gt;&lt;/nowiki&gt;&lt;/code&gt; and others do not have a structural meaning, but do have a meaning.

== See also ==
* [[Document processor]]
* [[Machine-Readable Documents]]

 {{reflist}}

{{DEFAULTSORT:Structured Document}}
[[Category:Electronic documents]]</text>
      <sha1>a8axve109ysslvg3a75t4601b11kd4y</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Data security</title>
    <ns>14</ns>
    <id>4842680</id>
    <revision>
      <id>544355757</id>
      <parentid>542975087</parentid>
      <timestamp>2013-03-15T12:17:03Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor />
      <comment>[[User:Addbot|Bot:]] Migrating 5 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q6160030]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="300" xml:space="preserve">{{Commons category|Computer security}}
{{Portal|Computer security}}
{{Cat main|Data security}}

[[Category:Computer security]]
[[Category:Computer data|Security]]
[[Category:Electronic documents]]
[[Category:National security]]
[[Category:Information technology management]]

[[fi:Luokka:Tietoturva]]</text>
      <sha1>dzq3p3ptwuo4vm8m26wt0618xxnnrp3</sha1>
    </revision>
  </page>
  <page>
    <title>Bibcode</title>
    <ns>0</ns>
    <id>14092434</id>
    <revision>
      <id>751428378</id>
      <parentid>751427529</parentid>
      <timestamp>2016-11-25T16:39:07Z</timestamp>
      <contributor>
        <username>Modest Genius</username>
        <id>593712</id>
      </contributor>
      <comment>/* top */ +infobox (not much in it yet)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5685" xml:space="preserve">{{selfref|For the Wikipedia template to link to bibcoded articles, see [[Template:bibcode]]}}
{{Infobox identifier
| name          = Bibcode
| image         = 
| image_size    = 
| image_caption = 
| image_alt     = 
| image_border  = 
| full_name     = Bibliographic code
| acronym       = 
| number        = 
| start_date    = 1990s&lt;!-- {{Start date|YYYY|MM|DD|df=y}} --&gt;
| organisation  = 
| digits        = 19
| check_digit   = none
| example       = 1924MNRAS..84..308E
| website       = &lt;!-- {{URL|example.org}} --&gt;
}}
The '''bibcode''' (also known as the "refcode") is a compact identifier used by several [[astronomy|astronomical]] data systems to uniquely specify literature references.

== Adoption ==
The Bibliographic Reference Code (REFCODE) was originally developed to be used in [[SIMBAD]] and the [[NASA/IPAC Extragalactic Database]] (NED), but it became a de facto standard and is now used more widely, for example, by the NASA [[Astrophysics Data System]] who coined and prefer the term "bibcode".&lt;ref name=a&gt;{{ cite book| url=http://cdsweb.u-strasbg.fr/simbad/refcode/refcode-paper.html| chapter= NED and SIMBAD Conventions for Bibliographic Reference Coding| title=Information &amp; On-Line Data in Astronomy|editor= Daniel Egret|editor2= Miguel A. Albrecht|publisher= Kluwer Academic Publishers|date=1995|isbn =0-7923-3659-3|author= M. Schmitz|author2= G. Helou|author3= P. Dubois|author4= C. LaGue|author5= B.F. Madore|author6= H. G. Corwin Jr.|author7= S. Lesteven|last-author-amp= yes|accessdate= 2011-06-22| archiveurl= https://web.archive.org/web/20110607153038/http://cdsweb.u-strasbg.fr/simbad/refcode/refcode-paper.html| archivedate= 7 June 2011 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;&lt;ref name=b&gt;{{cite web|url=http://adsabs.harvard.edu/abs_doc/help_pages/data.html|title= The ADS Data, help page|publisher= NASA ADS |accessdate=November 5, 2007| archiveurl= https://web.archive.org/web/20071014195855/http://adsabs.harvard.edu/abs_doc/help_pages/data.html| archivedate= 14 October 2007 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;

== Format ==
The code has a fixed length of 19 characters and has the form
&lt;center&gt;&lt;tt&gt;YYYYJJJJJVVVVMPPPPA&lt;/tt&gt;&lt;/center&gt;
where &lt;tt&gt;YYYY&lt;/tt&gt; is the four-digit year of the reference and &lt;tt&gt;JJJJJ&lt;/tt&gt; is a code indicating where the reference was published. In the case of a journal reference, &lt;tt&gt;VVVV&lt;/tt&gt; is the volume number, &lt;tt&gt;M&lt;/tt&gt; indicates the section of the journal where the reference was published (e.g., &lt;tt&gt;L&lt;/tt&gt; for a letters section), &lt;tt&gt;PPPP&lt;/tt&gt; gives the starting page number, and &lt;tt&gt;A&lt;/tt&gt; is the first letter of the last name of the first author. Periods (&lt;tt&gt;.&lt;/tt&gt;) are used to fill unused fields and to pad fields out to their fixed length if too short; padding is done on the right for the publication code and on the left for the volume number and page number.&lt;ref name=a /&gt;&lt;ref name=b /&gt; Page numbers greater than 9999 are continued in the &lt;tt&gt;M&lt;/tt&gt; column. The 6-digit article ID numbers (in lieu of page numbers) used by the Physical Review publications since the late 1990s are treated as follows: The first two digits of the article ID, corresponding to the issue number, are converted to a lower-case letter (01 = a etc.) and inserted into column &lt;tt&gt;M&lt;/tt&gt;. The remaining four digits are used in the page field.&lt;ref name=b /&gt;

== Examples ==
Some examples of the code are:
{| class="wikitable"
|-
! Bibcode
! Reference
|-
| &lt;tt&gt;[http://adsabs.harvard.edu/abs/1974AJ.....79..819H 1974AJ.....79..819H]&lt;/tt&gt;
| {{cite journal
   |last=Heintz |first=W. D.
   |date=1974
   |title=Astrometric study of four visual binaries
   |journal=[[The Astronomical Journal]]
   |volume=79 |pages=819–825
   |doi=10.1086/111614
   |bibcode = 1974AJ.....79..819H }}
|-
| &lt;tt&gt;[http://adsabs.harvard.edu/abs/1924MNRAS..84..308E 1924MNRAS..84..308E]&lt;/tt&gt;
| {{cite journal
   |last=Eddington |first=A. S.
   |date=1924
   |title=On the relation between the masses and luminosities of the stars
   |journal=[[Monthly Notices of the Royal Astronomical Society]]
   |volume=84 |issue=5
   |pages=308–332
   |bibcode = 1924MNRAS..84..308E 
   | doi = 10.1093/mnras/84.5.308 }}
|-
| &lt;tt&gt;[http://adsabs.harvard.edu/abs/1970ApJ...161L..77K 1970ApJ...161L..77K]&lt;/tt&gt;
| {{cite journal
   |last1=Kemp |first1=J. C.
   |last2=Swedlund |first2=J. B.
   |last3=Landstreet |first3=J. D.
   |last4=Angel |first4=J. R. P.
   |date=1970
   |title=Discovery of circularly polarized light from a white dwarf
   |journal=[[The Astrophysical Journal Letters]]
   |volume=161 |pages=L77–L79
   |doi=10.1086/180574
 |bibcode = 1970ApJ...161L..77K }}
|-
| &lt;tt&gt;[http://adsabs.harvard.edu/abs/2004PhRvL..93o0801M 2004PhRvL..93o0801M]&lt;/tt&gt;
| {{cite journal
   |last1=Mukherjee |first1=M.
   |last2=Kellerbauer |first2=A.
   |last3=Beck |first3=D.
   |last4=Blaum |first4=K.
   |last5=Bollen |first5=G.
   |last6=Carrel |first6=F.
   |last7=Delahaye |first7=P.
   |last8=Dilling |first8=J.
   |last9=George |first9=S.
   |last10=Guénaut |first10=C.
   |last11=Herfurth |first11=F.
   |last12=Herlert |first12=A.
   |last13=Kluge |first13=H.-J.
   |last14=Köster |first14=U.
   |last15=Lunney |first15=D.
   |last16=Schwarz |first16=S.
   |last17=Schweikhard |first17=L.
   |last18=Yazidjian |first18=C.
   |display-authors=3
   |date=2004
   |title=The Mass of &lt;sup&gt;22&lt;/sup&gt;Mg
   |journal=[[Physical Review Letters]]
   |volume=93 |issue=15
   |pages=150801
   |doi=10.1103/PhysRevLett.93.150801
 |bibcode = 2004PhRvL..93o0801M }}
|}

== See also ==
* [[Digital object identifier]]

== References ==
{{Reflist|30em}}

[[Category:Index (publishing)]]
[[Category:Identifiers]]
[[Category:Electronic documents]]
[[Category:Computational astronomy]]</text>
      <sha1>jctf6r37v2iuskuw25yuofb8nwe35lk</sha1>
    </revision>
  </page>
  <page>
    <title>Registry of Research Data Repositories</title>
    <ns>0</ns>
    <id>41872647</id>
    <revision>
      <id>754916806</id>
      <parentid>739907976</parentid>
      <timestamp>2016-12-15T05:56:30Z</timestamp>
      <contributor>
        <username>JFG</username>
        <id>168812</id>
      </contributor>
      <minor />
      <comment>Target page renamed (via JWB)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7616" xml:space="preserve">{{Infobox website
| name = re3data.org
| logo = Re3data Logo RGB 72dpi.png
| logocaption = The logo of re3data.org, the online Registry of Research Data Repositories
| registration = none
| language = English
| type = Online registry
| owner = [[Karlsruhe Institute of Technology]], [[GFZ German Research Centre for Geosciences]], [[Berlin School of Library and Information Science]]
| commercial = no
| launch date = {{Start date and years ago|mf=yes|2013|05|28}}
| current status  = Online
| content license = Website: [[Creative Commons licenses|CC-BY]], Database: [[Creative Commons licenses|CC0]] 
| url = {{URL|http://www.re3data.org/}}
}}

The '''Registry of Research Data Repositories''' ('''re3data.org''') is an [[Open Science]] tool that offers researchers, funding organizations, libraries and publishers an overview of existing international [[disciplinary repository|repositories]] for [[research data]].

== Background ==

re3data.org is a global registry of research data repositories from all academic disciplines. It provides an overview of existing research data repositories in order to help researchers to identify a suitable repository for their data and thus comply with requirements set out in data policies.&lt;ref name=Pampel2013&gt;{{cite journal|last=Pampel|first=Heinz|author2=Vierkant, Paul |author3=Scholze, Frank |author4=Bertelmann, Roland |author5=Kindling, Maxi |title=Making Research Data Repositories Visible: The re3data.org Registry|journal=PLoS ONE|date=4 November 2013|volume=8|issue=11|pages=e78080|doi=10.1371/journal.pone.0078080|pmid=24223762|url=http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0078080|accessdate=7 February 2014|bibcode=2013PLoSO...878080P|pmc=3817176|display-authors=etal}}&lt;/ref&gt;
The registry was officially launched in May 2013.&lt;ref name=Wellander2013&gt;{{cite web | url=http://sparceurope.org/registry-of-research-data-repositories-launched-re3data-org/ | title=Registry of Research Data Repositories launched – re3data.org | author=Wellander, Janna | date= 4 June 2013 | work=SPARC Europe | accessdate= 5 February 2014}}&lt;/ref&gt;

== Content ==

In March 2014 the registry lists 634 research data repositories from around the world covering all academic disciplines. 586 of these are described in detail using the re3data.org schema.&lt;ref&gt;{{cite web|title=re3data.org – from Funding to Growing|url=http://www.re3data.org/2014/03/re3data-org-from-funding-to-growing/|work=re3data.org|accessdate=21 March 2014|date=19 March 2014}}&lt;/ref&gt;
The project makes all metadata in the registry available for open use under the Creative Commons deed CC0.&lt;ref&gt;{{cite web|title=DataCite, re3data.org, and Databib Announce Collaboration|url=http://www.re3data.org/2014/03/datacite-re3data-org-databib-collaboration/|work=re3data|accessdate=25 March 2014}}&lt;/ref&gt;
[[File:DRYAD entry in re3data.org 2014-03-21.png|thumb|A screenshot of the DataDryad entry in re3data.org.]]

== Features ==

The majority of the listed research data repositories are described in detail by a comprehensive schema, namely the re3data.org Schema for the Description of Research Data Repositories.&lt;ref name=Vierkant&gt;{{cite web|last=Vierkant|first=Paul|title=Schema for the description of research data repositories|work=GFZ Helmholtz-Zentrum Potsdam|accessdate=7 February 2014|author2=Spier, Shaked |author3=Rücknagel, Jessika |author4= et. al. |url=http://gfzpublic.gfz-potsdam.de/pubman/faces/viewItemFullPage.jsp?itemId=escidoc:301641|doi=10.2312/re3.004}}&lt;/ref&gt;
Information icons support researchers to identify an adequate repository for the storage and reuse of their data.&lt;ref name=Wellander2013 /&gt;&lt;ref&gt;{{cite web|last=Fenner|first=Martin|title=registry of research data repositories launched|url=http://blogs.plos.org/mfenner/2013/06/01/re3data-org-registry-of-research-data-repositories-launched/|work=PLOS Blog|publisher=Gobbledygook|accessdate=5 February 2014}}&lt;/ref&gt;
[[File:Journal.pone.0078080.g001.png|thumb|Aspects of a Research Data Repository with the corresponding icons used in re3data.org.]]

== Inclusion criteria ==

A repository is indexed when the minimum requirements for inclusion in re3data.org are met: the repository has to be run by a legal entity, such as a sustainable institution (e.g. library, university) and clearly state access conditions to the data and repository as well as the terms of use. Additionally, an English [[graphical user interface]] (GUI) plus a focus on research [[data]] is needed.&lt;ref name=Vierkant /&gt;

== Partners and Cooperation ==

re3data.org is a joint project of the [[Berlin School of Library and Information Science]], the [[GFZ German Research Centre for Geosciences]] and the Library of the [[Karlsruhe Institute of Technology]] (KIT). The project is funded by the [[Deutsche Forschungsgemeinschaft|German Research Foundation]] (DFG).&lt;ref name=Pampel2013 /&gt;
The project cooperates with other Open Science initiatives like Databib,&lt;ref&gt;{{cite web|last=Kratz|first=John|title=Finding Disciplinary Data Repositories with DataBib and re3data|url=http://datapub.cdlib.org/2014/03/03/finding-disciplinary-data-repositories-with-databib-and-re3data/|work=Data Pub|accessdate=21 March 2014|author2=Nicholls, Natsuko |date=3 March 2014}}&lt;/ref&gt; BioSharing,&lt;ref&gt;{{cite web|title=Databases|url=http://www.biosharing.org/biodbcore|work=biosharing|accessdate=5 February 2014}}&lt;/ref&gt; [[DataCite]]&lt;ref&gt;{{cite web|title=Resources|url=http://www.datacite.org/resources|work=DataCite|accessdate=5 February 2014}}&lt;/ref&gt; and OpenAIRE.&lt;ref&gt;{{cite web|title=re3data.org and OpenAIRE sign Memorandum of Understanding|url=https://www.openaire.eu/it/component/content/article/481-re3data-and-openaire-sign-memorandum-of-understanding|work=OpenAIRE|accessdate=5 February 2014|date=21 October 2013}}&lt;/ref&gt; Several publishers, research institutions and funders refer to re3data.org in their Editorial Policies and guidelines as a tool for the identification of suitable data repositories, e.g. [[Nature (journal)|Nature]],&lt;ref&gt;{{cite web|title=The paper trail|url=http://www.nature.com/news/the-paper-trail-1.13123|work=Nature|accessdate=5 February 2014|date=4 June 2013}}&lt;/ref&gt; [[Springer Science+Business Media|Springer]]&lt;ref&gt;{{cite web|title=About SpringerPlus - Editorial policies|url=http://www.springerplus.com/about#editorialpolicies|work=SpringerPlus|accessdate=5 February 2014}}&lt;/ref&gt; and the [[European Commission]].&lt;ref&gt;{{cite web|title=Guidelines on Open Access to Scientific Publications and Research Data in Horizon 2020|url=http://ec.europa.eu/research/participants/data/ref/h2020/grants_manual/hi/oa_pilot/h2020-hi-oa-pilot-guide_en.pdf|publisher=European Commission|accessdate=20 March 2014}}&lt;/ref&gt;

== See also ==

*[[Scientific data archiving]]
*[[Data sharing]]
*[[Data archive]]
*[[Data library]]
*[[Data curation]]

== External links ==
* [http://www.re3data.org/ Official website]

== References ==

{{reflist}}

&lt;!-- Just press the "Save page" button below without changing anything! Doing so will submit your article submission for review. Once you have saved this page you will find a new yellow 'Review waiting' box at the bottom of your submission page. If you have submitted your page previously, either the old pink 'Submission declined' template or the old grey 'Draft' template will still appear at the top of your submission page, but you should ignore it. Again, please don't change anything in this text box. Just press the "Save page" button below. --&gt;


[[Category:Academic publishing]]
[[Category:Electronic documents]]
[[Category:Identifiers]]
[[Category:Index (publishing)]]
[[Category:Open science]]</text>
      <sha1>jxt4uc6vn4wotduwsa11hnjhmlm3mii</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Eprint archives</title>
    <ns>14</ns>
    <id>43511230</id>
    <revision>
      <id>738754911</id>
      <parentid>679712778</parentid>
      <timestamp>2016-09-10T21:42:45Z</timestamp>
      <contributor>
        <ip>98.230.192.179</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="578" xml:space="preserve">An [[eprint]] archive contains electronic copies of [[Academic journal|research paper]]s (which can be closed or open).

{{cat see also|Open-access archives|Bibliographic databases and indexes}}

== External links ==
[http://roar.eprints.org/ Registry of Open Access Repositories] (ROAR)

{{DEFAULTSORT:Eprint archives}}
[[Category:Academic publishing]]
[[Category:Electronic documents]]
[[Category:Electronic publishing]]
[[Category:Lists of publications in science]]
[[Category:Online archives]]
[[Category:Digital libraries]]
[[Category:Full text scholarly online databases]]</text>
      <sha1>8l24eyzmoorrzoowbebj7a86t57g1l7</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Open-access archives</title>
    <ns>14</ns>
    <id>43515750</id>
    <revision>
      <id>668257984</id>
      <parentid>643620719</parentid>
      <timestamp>2015-06-23T08:09:47Z</timestamp>
      <contributor>
        <username>Look2See1</username>
        <id>11406674</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="284" xml:space="preserve">{{cat main|Open archive}}
{{cat see also|Eprint archives}}
 
[[Category:Open access (publishing)|Archives]]
[[Category:Online archives]]
[[Category:Academic publishing]]
[[Category:Digital libraries]]
[[Category:Electronic documents]]
[[Category:Full text scholarly online databases]]</text>
      <sha1>cln4ycknrm2ld5dnxbjpfnax8hp1gib</sha1>
    </revision>
  </page>
  <page>
    <title>DjVu</title>
    <ns>0</ns>
    <id>610868</id>
    <revision>
      <id>762498297</id>
      <parentid>751569210</parentid>
      <timestamp>2017-01-29T05:33:03Z</timestamp>
      <contributor>
        <ip>70.184.214.35</ip>
      </contributor>
      <comment>alphabetized the categories</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11205" xml:space="preserve">{{About|a computer file format|a computer-assisted translation software tool|Déjà Vu (software)}}

{{ infobox file format
| icon = Djvu icon.svg
| logo = [[File:DjVu-logo.svg|frameless]]
| screenshot =
| caption =
| extension = .djvu, .djv
| mime = image/vnd.djvu, image/x-djvu
| type code = DJVU
| uniform type =
| magic =
| owner = [[AT&amp;T Labs|AT&amp;T Labs – Research]]
| released = {{start date and age|1998}}
| latest release version = Version 26&lt;ref name=djvuvers&gt;[http://www.djvu.org/forum/phpbb/viewtopic.php?p=952&amp;sid=33819a3f9de6fe1db7870159f47c4dd5 DjVu File Format Version], By Jim Rile, Posted: Fri Feb 23, 2007 1:08 am, PlanetDjVu&lt;/ref&gt;
| latest release date = {{start date and age|2006|06}}
| genre = [[Image file formats]]
| container for =
| contained by =
| extended from =
| extended to =
| standard =
| free = GNU GPLv2 for DjVu Reference Library and DjVuLibre-3.5;&lt;br&gt;License grants under the GNU GPL for several patents that cover aspects of the library&lt;ref&gt;{{cite web |title=DjVu Licensing |work=DjVu Sourceforge page |publisher=Sourceforge.net |date=2011-08-17 |url=http://djvu.sourceforge.net/licensing.html |accessdate=2011-09-21}}&lt;/ref&gt;
| url = {{url|http://www.djvu.org/}}
}}
'''DjVu''' ({{IPAc-en|ˌ|d|eɪ|ʒ|ɑː|ˈ|v|uː}} {{respell|DAY|zhah|VOO|'}},&lt;ref name="DjVuOverview"&gt;{{Cite web|url=http://www.cuminas.jp/en/technology/?src=technology_djvu.aspx|title=DjVu Technology|publisher=Cuminas|accessdate=2014-02-12}}&lt;/ref&gt; like {{lang-fr|[[déjà vu]]}} {{IPA-fr|deʒavy|}}) is a [[computer]] [[file format]] designed primarily to store [[image scanner|scanned documents]], especially those containing a combination of text, line drawings, indexed color images, and photographs. It uses technologies such as image layer separation of text and background/images, [[Interlacing (bitmaps)|progressive loading]], [[arithmetic coding]], and [[lossy compression]] for bitonal ([[monochrome]]) images. This allows high-quality, readable images to be stored in a minimum of space, so that they can be made available on the [[World Wide Web|web]].

DjVu has been promoted as an alternative to [[Portable Document Format|PDF]], promising smaller files than PDF for most scanned documents.&lt;ref name="DjVu"&gt;{{Cite web|url=http://djvu.org/resources/whatisdjvu.php|title=What is DjVu – DjVu.org|publisher=DjVu.org|accessdate=2009-03-05}}&lt;/ref&gt; The DjVu developers report that color magazine pages compress to 40–70&amp;nbsp;kB, black-and-white technical papers compress to 15–40&amp;nbsp;kB, and ancient manuscripts compress to around 100&amp;nbsp;kB; a satisfactory [[JPEG]] image typically requires 500&amp;nbsp;kB.&lt;ref name=djvupaper&gt;{{cite journal |author1=Léon Bottou |author2=Patrick Haffner |author3=Paul G. Howard |author4=Patrice Simard |author5=Yoshua Bengio |author6=Yann Le Cun |title=High Quality Document Image Compression with DjVu, 7(3):410–425|publisher=Journal of Electronic Imaging |year=1998 |url=http://leon.bottou.org/publications/pdf/jei-1998.pdf}}&lt;/ref&gt; Like PDF, DjVu can contain an [[Optical character recognition|OCR]] text layer, making it easy to perform [[copy and paste]] and text search operations.

Free browser plug-ins and desktop viewers from different developers are available from the djvu.org website. DjVu is supported by a number of multi-format document viewers and e-book reader software on Linux ([[Okular]], [[Evince]]) and Windows ([[SumatraPDF]]).

== History ==
The DjVu technology was originally developed&lt;ref name=djvupaper/&gt; by [[Yann LeCun]], [[Léon Bottou]], Patrick Haffner, and Paul G. Howard at [[AT&amp;T Labs]] from 1996 to 2001.

Due to its declared higher compression ratio (and thus smaller file size) and the ease of converting large volumes of text into DjVu format, and because it is an [[open file format]], some independent technologists (such as [[Brewster Kahle]]&lt;ref name="Kahle2005"&gt;{{Cite web|url=http://itc.conversationsnetwork.org/shows/detail400.html|author=Brewster Kahle|title=Universal Access to All Knowledge|format=Audio; Speech at 1h:31m:20s|publisher=Conversations Network|date=December 16, 2004}}&lt;/ref&gt;) have historically considered it superior to [[PDF]].

The DjVu library distributed as part of the open-source package ''DjVuLibre'' has become the reference implementation for the DjVu format. DjVuLibre has been maintained and updated by the original developers of DjVu since 2002.&lt;ref&gt;http://djvu.sourceforge.net/&lt;/ref&gt;

The DjVu file format specification has gone through a number of revisions:

{| class="wikitable sortable"
|+ Revision history
! Support status
! Version
! Release date
! Notes
|-
| Unsupported
| 1–19&lt;ref name=djvuvers /&gt;
| 1996–1999
| Developmental versions by AT&amp;T labs preceding the sale of the format to [[LizardTech]].
|-
| Unsupported
| Version 20&lt;ref name=djvuvers /&gt;&lt;!--keep the word ''version'' as it is part of the name--&gt;
| April 1999
| DjVu version 3. DjVu changed from a single-page format to a multipage format.
|-
| Older, still supported
| Version 21&lt;ref name=djvuvers /&gt;
| September 1999
| Indirect storage format replaced. The searchable text layer was added.
|-
| Older, still supported
| Version 22&lt;ref name=djvuvers /&gt;
| April 2001
| Page orientation, color JB2
|-
| Unsupported
| Version 23&lt;ref name=djvuvers /&gt;
| July 2002
| CID chunk
|-
| Unsupported
| Version 24&lt;ref name=djvuvers /&gt;
| February 2003
| LTAnno chunk
|-
| Older, still supported
| Version 25&lt;ref name=djvuvers /&gt;
| May 2003
| NAVM chunk. Support for DjVu bookmarks (outlines) was added. Changes made by Versions 23 and 24 were made obsolete.
|-
| Current
| Version 26&lt;ref name=djvuvers /&gt;
| April 2005
| Text/line annotations
|-
|}

== Technical overview ==

=== File structure ===
The DjVu file format is based on the [[Interchange File Format]] and is composed of hierarchically organized chunks. The IFF structure is preceded by a 4-byte &lt;code&gt;AT&amp;T&lt;/code&gt; [[magic number (programming)|magic number]]. Following is a single &lt;code&gt;FORM&lt;/code&gt; chunk with a secondary identifier of either &lt;code&gt;DJVU&lt;/code&gt; or &lt;code&gt;DJVM&lt;/code&gt; for a single-page or a multi-page document, respectively.

==== Chunk types ====
{| class="wikitable"
|-
! Chunk identifier !! Contained by !! Description
|-
| FORM:DJVU || FORM:DJVM || Describes a single page. Can either be at the root of a document and be a single-page document or referred to from a &lt;code&gt;DIRM&lt;/code&gt; chunk. 
|-
| FORM:DJVM || {{n/a}} || Describes a multi-page document. Is the document's root chunk.
|-
| FORM:DJVI || FORM:DJVM || Contains data shared by multiple pages.
|-
| FORM:THUM || FORM:DJVM || Contains thumbnails.
|-
| INFO || FORM:DJVU || Must be the first chunk. Describes the page width, height, format version, DPI, gamma, and rotation.
|-
| DIRM || FORM:DJVM || Must be the first chunk. References other &lt;code&gt;FORM&lt;/code&gt; chunks. These chunks can either follow this chunk inside the &lt;code&gt;FORM:DJVM&lt;/code&gt; chunk or be contained in external files. These types of documents are referred to as ''bundled'' or ''indirect'', respectively.
|-
| NAVM || FORM:DJVM || If present, must immediately follow the &lt;code&gt;DIRM&lt;/code&gt; chunk. Contains a BZZ-compressed outline of the document.
|}

=== Compression ===
DjVu divides a single image into many different images, then compresses them separately. To create a DjVu file, the initial image is first separated into three images: a background image, a foreground image, and a mask image. The background and foreground images are typically lower-resolution color images (e.g., 100 dpi); the mask image is a high-resolution bilevel image (e.g., 300 dpi) and is typically where the text is stored. The background and foreground images are then compressed using a [[Wavelet compression#Wavelet compression|wavelet-based compression]] algorithm named IW44.&lt;ref name=djvupaper/&gt; The mask image is compressed using a method called JB2 (similar to [[JBIG2]]). The JB2 encoding method identifies nearly identical shapes on the page, such as multiple occurrences of a particular character in a given font, style, and size. It compresses the bitmap of each unique shape separately, and then encodes the locations where each shape appears on the page. Thus, instead of compressing a letter "e" in a given font multiple times, it compresses the letter "e" once (as a compressed bit image) and then records every place on the page it occurs.

Optionally, these shapes may be mapped to [[UTF-8]] codes (either by hand or potentially by a [[Text recognition|text recognition system]]), and stored in the DjVu file. If this mapping exists, it is possible to select and copy text.

Since JBIG2 was based on JB2, both compression methods have the same problems when performing lossy compression.  Numbers may be substituted with similar looking numbers (such as replacing 6 with 8) if the text was scanned at a low DPI prior to lossy compression.

== Format licensing ==
DjVu is an [[open file format]] with patents.&lt;ref name="DjVu"/&gt; The file format specification is published, as well as source code for the reference library.&lt;ref name="DjVu"/&gt; The original authors distribute an [[Open-source software|open-source]] implementation named "''DjVuLibre''" under the [[GNU General Public License]]. The rights to the commercial development of the encoding software have been transferred to different companies over the years, including [[AT&amp;T Corporation]], [[LizardTech]], ''Celartem'' and ''Cuminas''.

==Support==
[[SumatraPDF]] (Windows) among others can manipulate DjVu files.

In 2002, the DjVu file format was chosen by the [[Internet Archive]] as a format in which its ''[[Million Book Project]]'' provides scanned [[public domain]] books online (along with [[TIFF]] and PDF).&lt;ref&gt;{{Cite web|url=http://wiki.laptop.org/go/DJVU |title=Image file formats – OLPC |publisher=Wiki.laptop.org |date= |accessdate=2008-09-09}}&lt;/ref&gt;

[[Wikimedia Commons]], a media repository used by [[Wikipedia]] among others, conditionally permits PDF and DjVu media files.&lt;ref&gt;[[commons:Commons:Scope#PDF and DjVu formats|PDF and DjVu]]&lt;/ref&gt;

== See also ==
* [[JBIG2]]
* [[Comparison of e-book formats]]

== References ==
{{reflist}}

== External links ==
{{Commons category|DjVu}}
* [http://djvu.org/ "The premier menu for DjVu resources"] (status of the site, which is maintained by an anonymous webmaster, is unclear)
* [http://djvu.sourceforge.net/ DjVuLibre site]
* [http://jwilk.net/software/ Jakub Wilk's pdf2djvu and other DjVu tools]
* [https://bitbucket.org/jsbien/ndt/wiki/wyniki Poliqarp for DjVu search engine and other DjVu tools]
* [http://djvu.org/forum/phpbb/viewtopic.php?t=146 Why won't Google index DjVu files after all this time?] – topic on  PlanetDjVu
* [http://any2djvu.djvuzone.org Any2Djvu Server - online document converter]
* [https://www.cuminas.jp/en/downloads Cuminas Software Downloads]
* [http://www.djvu-soft.narod.ru/soft/ Table of Djvu Programmes (Russian)]

{{Office document file formats}}
{{Graphics file formats}}

[[Category:1998 introductions]]
[[Category:Computer file formats]]
[[Category:Electronic documents]]
[[Category:Electronic publishing]]
[[Category:Filename extensions]]
[[Category:Graphics file formats]]
[[Category:Office document file formats]]
[[Category:Open formats]]</text>
      <sha1>54o36uudjldyv8hd3h8ev1xdou6v5vg</sha1>
    </revision>
  </page>
  <page>
    <title>Zathura (document viewer)</title>
    <ns>0</ns>
    <id>46505081</id>
    <revision>
      <id>741958483</id>
      <parentid>741955753</parentid>
      <timestamp>2016-09-30T19:19:03Z</timestamp>
      <contributor>
        <username>B-21</username>
        <id>5486213</id>
      </contributor>
      <comment>added Source Mage package historical entry</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5948" xml:space="preserve">{{notability|Products|date=April 2015}}
{{Infobox software
| name                   = Zathura
| screenshot             = Zathura Screenshot.png
| caption = Screenshot of zathura viewing a PDF file in Arch Linux.
| author                 = Moritz Lipp, Sebastian Ramacher
| developer              = pwmt&lt;ref&gt;{{cite web|url=https://pwmt.org |title=Programs With Movie Titles}}&lt;/ref&gt;
| released               = {{Start date|2009|09|18}}
| latest release version = 0.3.6
| latest release date    = {{Release date|2016|04|18}}&lt;ref&gt;{{cite web |url=https://pwmt.org/news/zathura-0-3-6/ |title=ZATHURA 0.3.6 |website=pwmt.org |date=2016-04-18 |accessdate=2016-08-28}}&lt;/ref&gt;
| programming language   = [[C (programming language)|C]]
| operating system       = [[Unix-like]]
| status                 = Active
| genre                  = Document viewer
| license                = [[Free software license|Free software]]
| website                = {{URL|pwmt.org/projects/zathura}}
}}

'''Zathura''' is a [[Free software|free]], [[Plug-in (computing)|plugin-based]] [[document viewer]]. Plugins are available for [[Portable Document Format|PDF]] (via [[Poppler_(software)|poppler]] or [[MuPDF]]), [[PostScript]], [[DjVu]], and [[EPUB]]. It was written to be lightweight and controlled with [[Vim (text editor)|vim]]-like keybindings. Zathura's customizability makes it well-liked by many Linux users.&lt;ref&gt;{{cite web|url=http://www.maketecheasier.com/8-alternative-pdf-readers-for-your-consideration/|title=MakeTechEasier list of alternative PDF viewers|access-date=24 April 2015}}&lt;/ref&gt;

Zathura has a mature, well-established codebase and a large development team.&lt;ref&gt;{{cite web|url=https://www.openhub.net/p/zathura-pdf-viewer|title=OpenHUB analysis of Zathura PDF Viewer|access-date=24 April 2015}}&lt;/ref&gt; It has official packages available in [[Arch linux]],&lt;ref&gt;{{cite web|url=https://www.archlinux.org/packages/community/x86_64/zathura/|title=Arch Linux zathura package}}&lt;/ref&gt;
[[Debian]],&lt;ref&gt;{{cite web|url=https://packages.debian.org/en/sid/zathura|title=Debian zathura package}}&lt;/ref&gt;
[[Fedora (operating system)|Fedora]],&lt;ref&gt;{{cite web|url=http://pkgs.org/altlinux-sisyphus/classic-x86_64/zathura-devel-0.3.3-alt1.x86_64.rpm.html|title=Fedora zathura package}}&lt;/ref&gt;
[[Gentoo linux|Gentoo]],&lt;ref&gt;{{cite web|url=https://packages.gentoo.org/package/app-text/zathura|title=Gentoo zathura package}}&lt;/ref&gt;
[[Ubuntu (operating system)|Ubuntu]],&lt;ref&gt;{{cite web|url=http://packages.ubuntu.com/precise/zathura|title=Ubuntu zathura package}}&lt;/ref&gt;
[[Source Mage GNU/Linux]],&lt;ref&gt;{{cite web|url=http://download.sourcemage.org/codex/test/doc/zathura/|title=Source Mage zathura package}}&lt;/ref&gt;
[[OpenBSD]],&lt;ref&gt;{{cite web|url=http://openports.se/textproc/zathura|title=OpenBSD zathura package}}&lt;/ref&gt;
and [[Mac OS X]].&lt;ref&gt;{{cite web|url=https://www.macports.org/ports.php?by=name&amp;substr=zathura|title=MacPorts zathura package}}&lt;/ref&gt;

Zathura was named after the [[Zathura (film)|film]] of the same name.&lt;ref&gt;https://git.pwmt.org/groups/pwmt&lt;/ref&gt;

== History ==

Development on Zathura began on 12 August 2009.&lt;ref&gt;{{cite web|url=https://github.com/pwmt/zathura/commit/0eeb457bea2f93983e556d07028c2cfdb49b898c|title=Zathura initial commit}}&lt;/ref&gt; On 18 September 2009, version 0.0.1 was announced to the Arch Linux community.&lt;ref&gt;{{cite web|url=https://bbs.archlinux.org/viewtopic.php?id=80458|title=zathura - a document viewer}}&lt;/ref&gt;

Zathura has been an official Arch Linux package since April 2010.&lt;ref&gt;{{cite web|url=https://projects.archlinux.org/svntogit/community.git/log/trunk?h=packages/zathura|title=Arch Linux package history for Zathura}}&lt;/ref&gt; Same year, by the end of July it was imported into Source Mage test grimoire.&lt;ref&gt;{{cite web|url=http://scmweb.sourcemage.org/?p=smgl/grimoire.git;a=commit;h=c0963f5c0a65a0536d21a03a528ffaff4245cce7|title=zathura package in Source Mage}}&lt;/ref&gt; It has been an official Debian package since at least 2011, as part of Debian Squeeze.&lt;ref&gt;{{cite web|url=https://packages.debian.org/squeeze/zathura|title=Debian Squeeze package for Zathura}}&lt;/ref&gt;

== Features ==

Zathura automatically reloads documents. When working in compiled documents such as those written in [[LaTeX]], Zathura will refresh the output whenever compilation takes place. Zathura has the option of enabling [[inverse search]] (using "synctex").&lt;ref&gt;{{cite web|url=https://wiki.math.cmu.edu/iki/wiki/tips/20140310-zathura-fsearch.html|title=LaTeX forward/inverse searches with Zathura}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://gist.github.com/vext01/16df5bd48019d451e078|title=Vim+Zathura+Synctex}}&lt;/ref&gt;

Zathura can adjust the document to best-fit or to fit width, and it can rotate pages. It can view pages side-by-side and has a fullscreen mode. Pages can also be recolored to have a black background and white foreground.

Zathura can search for text and copy text to the [[X Window selection|primary X selection]]. It supports bookmarks and can open encrypted files.

The behavior and appearance of Zathura can be customised using a [[configuration file]]. Zathura has the ability to execute external [[List of command-line interpreters#Unix-like systems|shell]] commands. It can be opened in tabs using {{URL|http://tools.suckless.org/tabbed/|tabbed}}.&lt;ref&gt;{{cite web|url=http://taitran.ca/vim/latex/markdown/2015/03/11/vim-latex-and-markdown-preview-scripts.html|title=Vim, Latex and Markdown preview scripts}}&lt;/ref&gt;

== See also ==
{{Portal|Free software}}

* [[List of PDF software]]
* [[Zathura (film)]]

==References==
{{Reflist|30em}}

==External links==
* {{official website|http://pwmt.org/projects/zathura/}}
* {{URL|https://wiki.archlinux.org/index.php/List_of_applications/Documents#Graphical_2|Arch Linux list of document viewers}}

{{PDF readers}}

[[Category:Free PDF readers]]
[[Category:PostScript]]
[[Category:Free software programmed in C]]
[[Category:Office software that uses GTK+]]
[[Category:Electronic documents]]</text>
      <sha1>tq080uk0yruhsuag6o1mkcie88dj81j</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Electronic lab notebook</title>
    <ns>14</ns>
    <id>49921430</id>
    <revision>
      <id>714733841</id>
      <parentid>711596622</parentid>
      <timestamp>2016-04-11T14:44:57Z</timestamp>
      <contributor>
        <username>Le Deluge</username>
        <id>8853961</id>
      </contributor>
      <comment>cat</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="148" xml:space="preserve">{{catmain}}

[[Category:Electronic documents]]
[[Category:Data management software]]
[[Category:Science software]]
[[Category:Scientific documents]]</text>
      <sha1>bj7uf1uzmpcxlcrw74wenxw129p5oz8</sha1>
    </revision>
  </page>
  <page>
    <title>E-receipt</title>
    <ns>0</ns>
    <id>40545818</id>
    <revision>
      <id>754262390</id>
      <parentid>753724960</parentid>
      <timestamp>2016-12-11T18:50:04Z</timestamp>
      <contributor>
        <username>Whoisjohngalt</username>
        <id>272397</id>
      </contributor>
      <comment>Added a reference, added categories, and cleaned up using [[WP:AutoEd|AutoEd]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="664" xml:space="preserve">An '''E-receipt''' is an electronic [[receipt]] of any goods/services that have been purchased, opposed to a paper receipt. They are usually sent via [[email]] to avoid wasting [[paper]] and for marketing purposes.&lt;ref&gt;{{cite news |last=Perring |first=Rebecca |url=http://www.express.co.uk/news/uk/639219/Ereceipts-British-shops-shopping-electronic-Internet |title=Retailers are now monitoring YOUR shopping habits and transactions with the eReceipt... |work=[[Daily Express#Sunday Express]] |date=2016-01-29 |accessdate=2016-12-11 }}&lt;/ref&gt;

==References==
{{reflist}}

[[Category:Electronic documents]]
[[Category:Accounting source documents]]

{{retailing-stub}}</text>
      <sha1>0vwyh8m2fitwdymde0y3a2qwlpovff0</sha1>
    </revision>
  </page>
  <page>
    <title>Portable Document Format</title>
    <ns>0</ns>
    <id>24077</id>
    <revision>
      <id>762882722</id>
      <parentid>762882676</parentid>
      <timestamp>2017-01-31T06:17:21Z</timestamp>
      <contributor>
        <username>Whitegum</username>
        <id>15445920</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="70129" xml:space="preserve">{{Redirect|PDF|3=}}
{{Infobox file format
| name                   = Portable Document Format
| icon                   = [[File:Adobe PDF.svg|frameless|SVG logo|150px]]
| iconcaption            = Adobe PDF icon
| extension              = .pdf
| mime                   = {{plainlist|
* &lt;code&gt;application/pdf&lt;/code&gt;,&lt;ref name="rfc3778"&gt;{{citation |url=http://tools.ietf.org/html/rfc3778 |title=The application/pdf Media Type, RFC 3778, Category: Informational |year=2004}}&lt;/ref&gt;
* &lt;code&gt;application/x-pdf&lt;/code&gt;
* &lt;code&gt;application/x-bzpdf&lt;/code&gt;
* &lt;code&gt;application/x-gzpdf&lt;/code&gt;
 }}
| _nomimecode            = true
| magic                  = &lt;code&gt;%PDF&lt;/code&gt;
| released               = {{Start date and age|1993|6|15}}
| standard               = ISO 32000-1
| free                   = Yes
| url                    = {{URL|https://www.adobe.com/devnet/pdf/pdf_reference_archive.html}}
| image                  = 
| typecode               = 'PDF '&lt;ref name="rfc3778" /&gt; (including a single space)
| uniform type           = com.adobe.pdf
| owner                  = [[Adobe Systems]]
| latest release version = 1.7
| latest release date    = &lt;!-- {{Start date and age|YYYY|mm|dd|df=yes}} --&gt;
| genre                  =
| container for          =
| contained by           =
| extended from          =
| extended to            = [[PDF/A]], [[PDF/E]], [[PDF/UA]], [[PDF/VT]], [[PDF/X]]
}}
The '''Portable Document Format''' ('''PDF''') is a [[file format]] used to present [[document]]s in a manner independent of [[application software]], [[Computer hardware|hardware]], and [[operating system]]s.&lt;ref name="pdf-ref-1.7"&gt;Adobe Systems Incorporated, [https://www.adobe.com/devnet/acrobat/pdfs/pdf_reference_1-7.pdf PDF Reference, Sixth edition, version 1.23 (30 MB)], Nov 2006, p. 33.&lt;/ref&gt; Each PDF file encapsulates a complete description of a fixed-layout flat document, including the text, [[font]]s, graphics, and other information needed to display it. &lt;!-- Today, three dimensional objects can be embedded in PDF documents with Acrobat 3D using [[U3D]] or [[PRC (file format)|PRC]] and various other data formats.&lt;ref name="3d#1" /&gt;&lt;ref name="3d#2" /&gt; --&gt;

&lt;blockquote&gt;A PDF file captures document text, fonts, images, and even formatting of documents from a variety of applications. You can e-mail a PDF document to your friends and it will look the same on their screens as it looks on yours, even if they have Apple computers and you have a PC.&lt;ref&gt;[http://techterms.com/definition/pdf TechTerms.com]&lt;/ref&gt;
&lt;/blockquote&gt;

== History and standardization ==

{{main article|History and standardization of Portable Document Format}}

PDF was developed in the early 1990s&lt;ref&gt;{{cite web|url=http://www.planetpdf.com/enterprise/article.asp?ContentID=6650|title=Adobe's Bob Wulff knows Acrobat and PDF -- inside and out}}&lt;/ref&gt; as a way to share computer documents, including text formatting and inline images.&lt;ref&gt;{{cite web|url=http://www.planetpdf.com/planetpdf/pdfs/warnock_camelot.pdf|title=The Camelot Project}}&lt;/ref&gt; It was among a number of competing formats such as [[DjVu]], [[Envoy (WordPerfect)|Envoy]], Common Ground Digital Paper, Farallon Replica and even [[Adobe Systems|Adobe]]'s own [[PostScript]] format. In those early years before the rise of the [[World Wide Web]] and [[HTML]] documents, PDF was popular mainly in [[desktop publishing]] [[workflow]]s.
Adobe Systems made the PDF specification available free of charge in 1993. PDF was a [[proprietary format]] controlled by Adobe, until it was officially released as an [[open standard]] on July 1, 2008, and published by the [[International Organization for Standardization]] as ISO 32000-1:2008,&lt;ref name="iso-standard"&gt;{{cite web|url=http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=51502 |title=ISO 32000-1:2008 - Document management – Portable document format – Part 1: PDF 1.7 |publisher=Iso.org |date=2008-07-01 |accessdate=2010-02-21}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Orion |first=Egan |title=PDF 1.7 is approved as ISO 32000 |work=[[The Inquirer]] |publisher=[[The Inquirer]] |date=2007-12-05 |url=http://www.theinquirer.net/gb/inquirer/news/2007/12/05/pdf-approved-iso-32000 |accessdate=2007-12-05 |deadurl=yes |archiveurl=https://web.archive.org/web/20071213004627/http://www.theinquirer.net/gb/inquirer/news/2007/12/05/pdf-approved-iso-32000 |archivedate=December 13, 2007 }}&lt;/ref&gt; at which time control of the specification passed to an ISO Committee of volunteer industry experts. In 2008, Adobe published a Public Patent License to ISO 32000-1 granting [[royalty-free]] rights for all patents owned by Adobe that are necessary to make, use, sell, and distribute PDF compliant implementations.&lt;ref&gt;{{citation |url=https://www.adobe.com/pdf/pdfs/ISO32000-1PublicPatentLicense.pdf |title=Public Patent License, ISO 32000-1: 2008 – PDF 1.7 |author=Adobe Systems Incorporated |year=2008 |accessdate=2011-07-06}}&lt;/ref&gt;

However, there are still some proprietary technologies defined only by Adobe, such as [[XFA|Adobe XML Forms Architecture]] (XFA) and [[JavaScript]] extension for Acrobat, which are referenced by ISO 32000-1 as [[normative]] and indispensable for the application of the ISO 32000-1 specification. These proprietary technologies are not standardized and their specification is published only on Adobe’s website.&lt;ref&gt;{{cite web |url=http://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=SWD:2013:0224:FIN:EN:PDF |title=Guide for the procurement of standards-based ICT - Elements of Good Practice, Against lock-in: building open ICT systems by making better use of standards in public procurement |quote=Example: ISO/IEC 29500, ISO/IEC 26300 and ISO 32000 for document formats reference information that is not accessible by all parties (references to proprietary technology and brand names, incomplete scope or dead web links). |publisher=European Commission |date=2013-06-25 |accessdate=2013-10-20}}&lt;/ref&gt;&lt;ref name="iso-meeting-n603"&gt;{{citation |url=http://pdf.editme.com/files/pdfREF-meetings/ISO-TC171-SC2-WG8_N0603_SC2WG8_MtgRept_SLC.pdf |title=ISO/TC 171/SC 2/WG 8 N 603 - Meeting Report |quote=XFA is not to be ISO standard just yet. ... The Committee urges Adobe Systems to submit the XFA Specification, XML Forms Architecture (XFA), to ISO for standardization ... The Committee is concerned about the stability of the XFA specification ... Part 2 will reference XFA 3.1 |date=2011-06-27}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://www.plosone.org/article/fetchSingleRepresentation.action?uri=info:doi/10.1371/journal.pone.0069446.s001 |title=Embedding and publishing interactive, 3-dimensional, scientificfigures in Portable Document Format (PDF) files |quote=... the implementation of the U3D standard was not complete and proprietary extensions were used. |accessdate=2013-10-20}}&lt;/ref&gt;&lt;ref name="rosenthol-adobe-2012"&gt;{{cite web |url=http://cdn.parleys.com/p/5148922a0364bc17fc56c6e5/iSUM2012_00_LRO_presentation.pdf |title=PDF and Standards |author=Leonard Rosenthol, Adobe Systems |year=2012 |accessdate=2013-10-20}}&lt;/ref&gt;&lt;ref&gt;{{citation |url=http://www.planetpdf.com/enterprise/article.asp?ContentID=Is_PDF_an_open_standard&amp;page=1 |title=Is PDF an open standard? - Adobe Reader is the de facto Standard, not PDF |author=Duff Johnson |date=2010-06-10 |accessdate=2014-01-19}}&lt;/ref&gt; Many of them are also not supported by popular third-party implementations of PDF.  So when organizations publish PDFs which use these proprietary technologies, they present accessibility issues for some users.

In 2014, ISO TC 171 voted to deprecate XFA for ISO 32000-2 ("Next-generation PDF").&lt;ref name="DRAFT INTERNATIONAL"&gt;{{Cite web|url=http://www.iso.org/iso/catalogue_detail.htm?csnumber=63534|title=DRAFT INTERNATIONAL STANDARD ISO/DIS 32000-2|last=|first=|date=|website=|publisher=ISO|access-date=2016-08-04|quote=Editor’s note: XFA forms have been deprecated from ISO 32000-2 in accordance with the outcome of the letter ballot following the Pretoria meetings.}}&lt;/ref&gt;

== Technical foundations ==
The PDF combines three technologies:
* A subset of the [[PostScript]] page description programming language, for generating the layout and graphics.
* A [[font embedding|font-embedding]]/replacement system to allow fonts to travel with the documents.
* A structured storage system to bundle these elements and any associated content into a single file, with [[data compression]] where appropriate.

=== PostScript ===
[[PostScript]] is a [[page description language]] run in an [[interpreter (computing)|interpreter]] to generate an image, a process requiring many resources. It can handle graphics and standard features of [[programming language]]s such as &lt;code&gt;if&lt;/code&gt; and &lt;code&gt;loop&lt;/code&gt; commands. PDF is largely based on PostScript but simplified to remove flow control features like these, while graphics commands such as &lt;code&gt;lineto&lt;/code&gt; remain.

Often, the PostScript-like PDF code is generated from a source PostScript file. The graphics commands that are output by the PostScript code are collected and [[Lexical analysis|tokenized]]. Any files, graphics, or fonts to which the document refers also are collected. Then, everything is compressed to a single file. Therefore, the entire PostScript world (fonts, layout, measurements) remains intact.

As a document format, PDF has several advantages over PostScript:
* PDF contains tokenized and interpreted results of the PostScript source code, for direct correspondence between changes to items in the PDF page description and changes to the resulting page appearance.
* PDF (from version 1.4) supports [[transparency (graphic)|graphic transparency]]; PostScript does not.
* PostScript is an [[interpreted programming language]] with an implicit global state, so instructions accompanying the description of one page can affect the appearance of any following page. Therefore, all preceding pages in a PostScript document must be processed to determine the correct appearance of a given page, whereas each page in a PDF document is unaffected by the others. As a result, PDF viewers allow the user to quickly jump to the final pages of a long document, whereas a PostScript viewer needs to process all pages sequentially before being able to display the destination page (unless the optional PostScript [[Document Structuring Conventions]] have been carefully complied with).

== Technical overview ==

=== File structure ===

A PDF file is a 7-bit ASCII file, except for certain elements that may have binary content.
A PDF file starts with a header containing the [[magic number (programming)|magic number]] and the version of the format such as &lt;code&gt;%PDF-1.7&lt;/code&gt;. The format is a subset of a COS ("Carousel" Object Structure) format.&lt;ref&gt;{{cite web|url=http://jimpravetz.com/blog/2012/12/in-defense-of-cos/|title=In Defense of COS, or Why I Love JSON and Hate XML|author=Jim Pravetz|work=jimpravetz.com}}&lt;/ref&gt; A COS tree file consists primarily of ''objects'', of which there are eight types:&lt;ref&gt;Adobe Systems, PDF Reference, p. 51.&lt;/ref&gt;
* [[Boolean data type|Boolean]] values, representing ''true'' or ''false''
* Numbers
* [[String (computer science)|Strings]], enclosed within parentheses (&lt;code&gt;(...)&lt;/code&gt;), may contain 8-bit characters.
* Names, starting with a forward slash (&lt;code&gt;/&lt;/code&gt;)
* [[Array data type|Array]]s, ordered collections of objects enclosed within square brackets (&lt;code&gt;[...]&lt;/code&gt;)
* [[Dictionary (data structure)|Dictionaries]], collections of objects indexed by Names enclosed within double pointy brackets (&lt;code&gt;&amp;lt;&amp;lt;...&amp;gt;&amp;gt;&lt;/code&gt;)
* [[Stream (computing)|Streams]], usually containing large amounts of data, which can be compressed and binary
* The [[Pointer (computer programming)|null]] object
Furthermore, there may be comments, introduced with the percent sign (&lt;code&gt;%&lt;/code&gt;). Comments may contain 8-bit characters.

Objects may be either ''direct'' (embedded in another object) or ''indirect''. Indirect objects are numbered with an ''object number'' and a ''generation number'' and defined between the &lt;code&gt;obj&lt;/code&gt; and &lt;code&gt;endobj&lt;/code&gt; keywords. An index table, also called the cross-reference table and marked with the &lt;code&gt;xref&lt;/code&gt; keyword, follows the main body and gives the byte offset of each indirect object from the start of the file.&lt;ref&gt;Adobe Systems, PDF Reference, pp. 39–40.&lt;/ref&gt; This design allows for efficient [[random access]] to the objects in the file, and also allows for small changes to be made without rewriting the entire file (''incremental update''). Beginning with PDF version 1.5, indirect objects may also be located in special streams known as ''object streams''. This technique reduces the size of files that have large numbers of small indirect objects and is especially useful for ''Tagged PDF''.

At the end of a PDF file is a trailer introduced with the &lt;code&gt;trailer&lt;/code&gt; keyword. It contains

* a dictionary
* an offset to the start of the cross-reference table (the table starting with the &lt;code&gt;xref&lt;/code&gt; keyword)
* and the &lt;code&gt;%%EOF&lt;/code&gt; [[end-of-file]] marker.

The dictionary contains

* a reference to the root object of the tree structure, also known as the ''catalog''
* the count of indirect objects in the cross-reference table
* and other optional information.

There are two layouts to the PDF files: non-linear (not "optimized") and linear ("optimized"). Non-linear PDF files consume less disk space than their linear counterparts, though they are slower to access because portions of the data required to assemble pages of the document are scattered throughout the PDF file. Linear PDF files (also called "optimized" or "web optimized" PDF files) are constructed in a manner that enables them to be read in a Web browser plugin without waiting for the entire file to download, since they are written to disk in a linear (as in page order) fashion.&lt;ref name="pdf-ref"&gt;{{cite web |url=https://www.adobe.com/devnet/pdf/pdf_reference.html |title=Adobe Developer Connection: PDF Reference and Adobe Extensions to the PDF Specification |publisher=Adobe Systems |accessdate=2010-12-13}}&lt;/ref&gt; PDF files may be optimized using [[Adobe Acrobat]] software or [[QPDF]].

=== Imaging model ===
The basic design of how [[graphics]] are represented in PDF is very similar to that of PostScript, except for the use of [[transparency (graphic)|transparency]], which was added in PDF 1.4.

PDF graphics use a [[device independence|device-independent]] [[Cartesian coordinate system]] to describe the surface of a page. A PDF page description can use a [[matrix (mathematics)|matrix]] to [[scale (ratio)|scale]], [[rotate]], or [[Shear mapping|skew]] graphical elements. A key concept in PDF is that of the ''graphics state'', which is a collection of graphical parameters that may be changed, saved, and restored by a ''page description''. PDF has (as of version 1.6) 24 graphics state properties, of which some of the most important are:
* The ''current transformation matrix'' (CTM), which determines the coordinate system
* The ''[[clipping path]]''
* The ''[[color space]]''
* The ''[[alpha compositing|alpha constant]]'', which is a key component of transparency

==== Vector graphics ====
As in PostScript, [[vector graphics]] in PDF are constructed with ''paths''. Paths are usually composed of lines and cubic [[Bézier curve]]s, but can also be constructed from the outlines of text. Unlike PostScript, PDF does not allow a single path to mix text outlines with lines and curves. Paths can be stroked, filled, or used for [[clipping path|clipping]]. Strokes and fills can use any color set in the graphics state, including ''patterns''.

PDF supports several types of patterns. The simplest is the ''tiling pattern'' in which a piece of artwork is specified to be drawn repeatedly. This may be a ''colored tiling pattern'', with the colors specified in the pattern object, or an ''uncolored tiling pattern'', which defers color specification to the time the pattern is drawn. Beginning with PDF 1.3 there is also a ''shading pattern'', which draws continuously varying colors. There are seven types of shading pattern of which the simplest are the ''axial shade'' (Type 2) and ''radial shade'' (Type 3). &lt;!-- Pictures desperately needed here! --&gt;

==== Raster images ====
[[Raster graphics|Raster images]] in PDF (called ''Image XObjects'') are represented by dictionaries with an associated stream. The dictionary describes properties of the image, and the stream contains the image data. (Less commonly, a raster image may be embedded directly in a page description as an ''inline image''.) Images are typically ''filtered'' for compression purposes. Image filters supported in PDF include the general purpose filters
* '''ASCII85Decode''' a filter used to put the stream into 7-bit [[ASCII]]
* '''ASCIIHexDecode''' similar to ASCII85Decode but less compact
* '''FlateDecode''' a commonly used filter based on the [[deflate]] algorithm defined in RFC 1951 (deflate is also used in the [[gzip]], [[Portable Network Graphics|PNG]], and [[ZIP (file format)|zip]] file formats among others); introduced in PDF 1.2; it can use one of two groups of predictor functions for more compact zlib/deflate compression: ''Predictor 2'' from the [[TIFF]] 6.0 specification and predictors (filters) from the [[Portable Network Graphics|PNG]] specification (RFC 2083)
* '''LZWDecode''' a filter based on [[LZW]] Compression; it can use one of two groups of predictor functions for more compact LZW compression: ''Predictor 2'' from the TIFF 6.0 specification and predictors (filters) from the PNG specification
* '''RunLengthDecode''' a simple compression method for streams with repetitive data using the [[run-length encoding]] algorithm and the image-specific filters
* '''DCTDecode''' a [[lossy]] filter based on the [[JPEG]] standard
* '''CCITTFaxDecode''' a [[lossless]] [[bi-level image|bi-level]] (black/white) filter based on the Group 3 or [[Group 4 compression|Group 4]] [[CCITT]] (ITU-T) [[fax]] compression standard defined in ITU-T [[T.4]] and T.6
* '''JBIG2Decode''' a lossy or lossless bi-level (black/white) filter based on the [[JBIG2]] standard, introduced in PDF 1.4
* '''JPXDecode''' a lossy or lossless filter based on the [[JPEG 2000]] standard, introduced in PDF 1.5

Normally all image content in a PDF is embedded in the file. But PDF allows image data to be stored in external files by the use of ''external streams'' or ''Alternate Images''. Standardized subsets of PDF, including [[PDF/A]] and [[PDF/X]], prohibit these features.

==== Text ====
Text in PDF is represented by ''text elements'' in page content streams. A text element specifies that ''characters'' should be drawn at certain positions. The characters are specified using the ''encoding'' of a selected ''font resource''.

===== Fonts =====
A font object in PDF is a description of a digital [[typeface]]. It may either describe the characteristics of a typeface, or it may include an embedded ''font file''. The latter case is called an ''embedded font'' while the former is called an ''unembedded font''. The font files that may be embedded are based on widely used standard digital font formats: '''[[PostScript fonts|Type 1]]''' (and its compressed variant '''CFF'''), '''[[TrueType]]''', and (beginning with PDF 1.6) '''[[OpenType]]'''. Additionally PDF supports the '''Type 3''' variant in which the components of the font are described by PDF graphic operators. &lt;!--- Type 3 bit is awkward and should be cleaned up ---&gt;

===== Standard Type 1 Fonts (Standard 14 Fonts) =====
Fourteen typefaces, known as the ''standard 14 fonts'', have a special significance in PDF documents:
* [[Times Roman|Times]] (v3) (in regular, italic, bold, and bold italic)
* [[Courier (typeface)|Courier]] (in regular, oblique, bold and bold oblique)
* [[Helvetica]] (v3) (in regular, oblique, bold and bold oblique)
* [[Symbol (typeface)|Symbol]]
* [[Zapf Dingbats]]
These fonts are sometimes called the ''base fourteen fonts''.&lt;ref&gt;{{cite web|url=http://desktoppub.about.com/od/glossary/g/base14fonts.htm|title=Desktop Publishing: Base 14 Fonts - Definition|work=About.com Tech}}&lt;/ref&gt; These fonts, or suitable substitute fonts with the same metrics, should be available in most PDF readers. However, since Adobe Acrobat version 6, most of these fonts are not ''guaranteed'' to be available in the reader, and may only display correctly if the system has them installed.&lt;ref name="aquarium"&gt;[http://www.planetpdf.com/planetpdf/pdfs/pdf2k/03e/merz_fontaquarium.pdf The PDF Font Aquarium]&lt;/ref&gt; Fonts may be substituted if they are not embedded in a PDF.

===== Encodings =====
Within text strings, characters are shown using ''character codes'' (integers) that map to glyphs in the current font using an ''encoding''. There are a number of predefined encodings, including ''WinAnsi'', ''MacRoman'', and a large number of encodings for East Asian languages, and a font can have its own built-in encoding. (Although the WinAnsi and MacRoman encodings are derived from the historical properties of the [[Microsoft Windows|Windows]] and [[Macintosh]] operating systems, fonts using these encodings work equally well on any platform.) PDF can specify a predefined encoding to use, the font's built-in encoding or provide a lookup table of differences to a predefined or built-in encoding (not recommended with TrueType fonts).&lt;ref&gt;{{cite web|url=https://www.adobe.com/devnet/acrobat/pdfs/pdf_reference_1-7.pdf |title=PDF Reference Sixth Edition, version 1.7, table 5.11}}&lt;/ref&gt; The encoding mechanisms in PDF were designed for Type 1 fonts, and the rules for applying them to TrueType fonts are complex.

For large fonts or fonts with non-standard glyphs, the special encodings ''Identity-H'' (for horizontal writing) and ''Identity-V'' (for vertical) are used. With such fonts it is necessary to provide a ''ToUnicode'' table if semantic information about the characters is to be preserved.

==== Transparency ====
The original imaging model of PDF was, like PostScript's, ''opaque'': each object drawn on the page completely replaced anything previously marked in the same location. In PDF 1.4 the imaging model was extended to allow transparency. When transparency is used, new objects interact with previously marked objects to produce blending effects. The addition of transparency to PDF was done by means of new extensions that were designed to be ignored in products written to the PDF 1.3 and earlier specifications. As a result, files that use a small amount of transparency might view acceptably in older viewers, but files making extensive use of transparency could be viewed incorrectly in an older viewer without warning.

The transparency extensions are based on the key concepts of ''transparency groups'', ''blending modes'', ''shape'', and ''alpha''. The model is closely aligned with the features of [[Adobe Illustrator]] version 9. The blend modes were based on those used by [[Adobe Photoshop]] at the time. When the PDF 1.4 specification was published, the formulas for calculating blend modes were kept secret by Adobe. They have since been published.&lt;ref&gt;[https://www.adobe.com/content/dam/Adobe/en/devnet/pdf/pdfs/pdf_reference_archives/blend_modes.pdf PDF Blend Modes Addendum]&lt;/ref&gt;

The concept of a transparency group in PDF specification is independent of existing notions of "group" or "layer" in applications such as Adobe Illustrator. Those groupings reflect logical relationships among objects that are meaningful when editing those objects,
but they are not part of the imaging model.

=== Interactive elements ===

PDF files may contain interactive elements such as annotations, form fields, video and Flash animation.

'''Rich Media PDF''' is a term that is used to describe interactive content that can be embedded or linked to inside of a PDF. This content must be produced using the Flash file format. When Adobe bought Macromedia, the jewel of the company was Flash, and the Flash player was embedded inside Adobe Acrobat and Adobe Reader, removing the need for third-party plug-ins such as Flash, QuickTime, or Windows Media. Unfortunately, this caused a rift with Apple as QuickTime video was prohibited from PDF.  [[Rich Media]] expert [[Bob Connolly (Canadian film director)#Books, eBooks and Magazine Articles|Robert Connolly]] believes this event triggered the war between Apple and Adobe over the Flash iPhone/iPad dispute. Rich Media PDF will not operate in Apple's iOS devices such as the iPad, and interactivity is limited.

'''Interactive Forms''' is a mechanism to add forms to the PDF file format.

PDF currently supports two different methods for integrating data and PDF forms. Both formats today coexist in PDF specification:&lt;ref name="iso32000"&gt;{{citation |url=https://www.adobe.com/devnet/acrobat/pdfs/PDF32000_2008.pdf |title=Document Management – Portable Document Format – Part 1: PDF 1.7, First Edition |author=Adobe Systems Incorporated |date=2008-07-01 |accessdate=2010-02-19}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://gnupdf.org/Forms_Data_Format |title=Gnu PDF - PDF Knowledge - Forms Data Format |archiveurl=https://web.archive.org/web/20130101054615/http://www.gnupdf.org/Forms_Data_Format |archivedate=2013-01-01 |accessdate=2010-02-19}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://livedocs.adobe.com/coldfusion/8/htmldocs/help.html?content=formsPDF_02.html |title=About PDF forms |accessdate=2010-02-19}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://forums.adobe.com/thread/301733 |title=Convert XFA Form to AcroForm? |year=2008 |accessdate=2010-02-19}}&lt;/ref&gt;
* '''AcroForms''' (also known as '''Acrobat forms'''), introduced in the PDF 1.2 format specification and included in all later PDF specifications.
* '''[[XML Forms Architecture|Adobe XML Forms Architecture]] (XFA)''' forms, introduced in the PDF 1.5 format specification. The XFA specification is not included in the PDF specification, it is only referenced as an optional feature. Adobe XFA Forms are not compatible with AcroForms.&lt;ref&gt;{{cite web |url=http://partners.adobe.com/public/developer/tips/topic_tip2.html |title=Migrating from Adobe Acrobat forms to XML forms |accessdate=2010-02-22}}&lt;/ref&gt;

==== AcroForms ====
AcroForms were introduced in the PDF 1.2 format. AcroForms permit using objects (''e.g.'' [[text box]]es, [[Radio button]]s, ''etc.'') and some code (''e.g.'' [[JavaScript]]).

Alongside the standard PDF action types, interactive forms (AcroForms) support submitting, resetting, and importing data. The "submit" action transmits the names and values of selected interactive form fields to a specified uniform resource locator (URL). Interactive form field names and values may be submitted in any of the following formats, (depending on the settings of the action’s ExportFormat, SubmitPDF, and XFDF flags):&lt;ref name="iso32000" /&gt;
* HTML Form format (HTML 4.01 Specification since PDF 1.5; HTML 2.0 since 1.2)
* Forms Data Format (FDF)
* XML Forms Data Format (XFDF) (external XML Forms Data Format Specification, Version 2.0; supported since PDF 1.5; it replaced the "XML" form submission format defined in PDF 1.4)
* PDF (the entire document can be submitted rather than individual fields and values). (defined in PDF 1.4)

AcroForms can keep form field values in external stand-alone files containing key:value pairs. The external files may use Forms Data Format (FDF) and XML Forms Data Format (XFDF) files.&lt;ref&gt;{{cite web |url=http://kb2.adobe.com/cps/325/325874.html |title=Using Acrobat forms and form data on the web |author=Adobe Systems Incorporated |date=2007-10-15 |accessdate=2010-02-19}}&lt;/ref&gt;&lt;ref name="xfdf"&gt;{{citation |url=http://partners.adobe.com/public/developer/en/xml/xfdf_2.0.pdf |format=PDF |title=XML Forms Data Format Specification, version 2 |date=September 2007 |accessdate=2010-02-19}}&lt;/ref&gt;&lt;ref name="fdf-exchange"&gt;{{citation |url=https://www.adobe.com/devnet/acrobat/pdfs/fdf_data_exchange.pdf |format=PDF |title=FDF Data Exchange Specification |date=2007-02-08 |accessdate=2010-02-19}}&lt;/ref&gt; The usage rights (UR) signatures define rights for import form data files in FDF, XFDF and text ([[comma-separated values|CSV]]/[[delimiter-separated values|TSV]]) formats, and export form data files in FDF and XFDF formats.&lt;ref name="iso32000" /&gt;

===== Forms Data Format (FDF) =====
{{Infobox file format
| name                   = Forms Data Format (FDF)
| icon                   =
| logo                   =
| screenshot             =
| caption                =
| extension              = .fdf
| mime                   = application/vnd.fdf&lt;ref&gt;{{citation |url=http://www.iana.org/assignments/media-types/application/ |title=IANA Application Media Types - vnd.fdf |accessdate=2010-02-22}}&lt;/ref&gt;
| type code              = 'FDF'
| uniform type           =
| magic                  =
| owner                  = [[Adobe Systems]]
| released               = {{Start date|1996}}&lt;!-- {{Start date|YYYY|mm|dd|df=yes}} --&gt; (PDF 1.2)
| latest release version =
| latest release date    = &lt;!-- {{Start date and age|YYYY|mm|dd|df=yes}} --&gt;
| genre                  =
| container for          =
| contained by           =
| extended from          = PDF
| extended to            = XFDF
| standard               = ISO 32000-1:2008
| free                   = Yes
| url                    =
}}

The Forms Data Format (FDF) is based on PDF, it uses the same syntax and has essentially the same file structure, but is much simpler than PDF, since the body of an FDF document consists of only one required object. Forms Data Format is defined in the PDF specification (since PDF 1.2). The Forms Data Format can be used when submitting form data to a server, receiving the response, and incorporating into the interactive form. It can also be used to export form data to stand-alone files that can be imported back into the corresponding PDF interactive form. Beginning in PDF 1.3, FDF can be used to define a container for annotations that are separate from the PDF document they apply to. FDF typically encapsulates information such as [[X.509|X.509 certificates]], requests for certificates, directory settings, timestamp server settings, and embedded PDF files for network transmission.&lt;ref name="fdf-exchange" /&gt; The FDF uses the MIME content type application/vnd.fdf, filename extension .fdf and on Mac OS it uses file type 'FDF'.&lt;ref name="iso32000" /&gt; Support for importing and exporting FDF stand-alone files is not widely implemented in free or freeware PDF software. For example, there is no import/export support in Evince, Okular, Poppler, KPDF or Sumatra PDF, however, Evince, Okular and Poppler support filling in of PDF Acroforms and saving filled data inside the PDF file. Import support for stand-alone FDF files is implemented in Adobe Reader; export and import support (including saving of FDF data in PDF) is for example implemented in Foxit Reader and PDF-XChange Viewer Free; saving of FDF data in a PDF file is also supported in pdftk.

===== XML Forms Data Format (XFDF) =====
{{Infobox file format
| name                   = XML Forms Data Format (XFDF)
| icon                   =
| logo                   =
| screenshot             =
| caption                =
| extension              = .xfdf
| mime                   = application/vnd.adobe.xfdf&lt;ref&gt;{{citation |url=http://www.iana.org/assignments/media-types/application/vnd.adobe.xfdf |title=IANA Application Media Types - Vendor Tree - vnd.adobe.xfdf |accessdate=2010-02-22}}&lt;/ref&gt;
| type code              = 'XFDF'
| uniform type           =
| magic                  =
| owner                  = [[Adobe Systems]]
| released               = {{Start date|2003|07|df=yes}} (referenced in PDF 1.5)
| latest release version = 3.0
| latest release date    = {{Start date and age|2009|08|df=yes}}
| genre                  =
| container for          =
| contained by           =
| extended from          = PDF, FDF, [[XML]]
| extended to            =
| standard               = No (under standardization as ISO/CD 19444-1&lt;ref name="iso-xfdf"&gt;{{citation |url=http://www.iso.org/iso/home/store/catalogue_ics/catalogue_detail_ics.htm?ics1=35&amp;ics2=240&amp;ics3=30&amp;csnumber=64911 |title=ISO/CD 19444-1 - Document management - XML forms data format - Part 1: XFDF 3.0 |accessdate=2014-11-26}}&lt;/ref&gt;)
| free                   =
| url                    = [https://partners.adobe.com/public/developer/en/xml/XFDF_Spec_3.0.pdf XFDF 3.0 specification]
}}

XML Forms Data Format (XFDF) is the XML version of Forms Data Format, but the XFDF implements only a subset of FDF containing forms and annotations. There are not XFDF equivalents for some entries in the FDF dictionary - such as the Status, Encoding, JavaScript, Pages keys, EmbeddedFDFs, Differences and Target. In addition, XFDF does not allow the spawning, or addition, of new pages based on the given data; as can be done when using an FDF file. The XFDF specification is referenced (but not included) in PDF 1.5 specification (and in later versions). It is described separately in ''XML Forms Data Format Specification''.&lt;ref name="xfdf" /&gt; The PDF 1.4 specification allowed form submissions in XML format, but this was replaced by submissions in XFDF format in the PDF 1.5 specification. XFDF conforms to the XML standard. As of November 2014, XFDF 3.0 is in the ISO/IEC standardization process under the formal name ''ISO/CD 19444-1 - Document management - XML forms data format - Part 1: XFDF 3.0''.&lt;ref name="iso-xfdf"/&gt;

XFDF can be used the same way as FDF; e.g., form data is submitted to a server, modifications are made, then sent back and the new form data is imported in an interactive form. It can also be used to export form data to stand-alone files that can be imported back into the corresponding PDF interactive form. A support for importing and exporting XFDF stand-alone files is not widely implemented in free or freeware PDF software. Import of XFDF is implemented in Adobe Reader 5 and later versions; import and export is implemented in PDF-XChange Viewer Free; embedding of XFDF data in PDF form is implemented in pdftk (pdf toolkit).

==== Adobe XML Forms Architecture (XFA) ====
{{Main article|XFA|l1=XML Forms Architecture}}
In the PDF 1.5 format, [[Adobe Systems]] introduced a new, proprietary format for forms, namely Adobe XML Forms Architecture (XFA) forms. The XFA 2.02 is referenced in the PDF 1.5 specification (and also in later versions) but is described separately in ''Adobe XML Forms Architecture (XFA) Specification'', which has several versions.&lt;ref name="xfa-adobe"&gt;{{cite web |url=http://partners.adobe.com/public/developer/xml/index_arch.html |title=Adobe XML Forms Architecture (XFA) |author=Adobe Systems Incorporated |accessdate=2010-02-19}}&lt;/ref&gt; XFA specification is not included in ISO 32000-1 PDF 1.7 and is only referenced as an external proprietary specification created by Adobe. XFA was not standardized as an ISO standard. In 2011 the ISO Committee (TC 171/SC 2/WG 8) urged Adobe Systems to submit the XFA Specification for standardization.&lt;ref name="iso-meeting-n603" /&gt;

Adobe XFA Forms are not compatible with AcroForms. Adobe Reader contains "disabled features" for use of XFA Forms, that activate only when opening a PDF document that was created using enabling technology available only from Adobe.&lt;ref&gt;{{citation |url=https://www.adobe.com/products/eulas/pdfs/Reader_Player_AIR_WWEULA-Combined-20080204_1313.pdf |format=PDF |title=Adobe Reader - Software license agreement |accessdate=2010-02-19}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://www.adobe.com/go/readerextensions |title=LiveCycle Reader Extensions ES features and benefits |accessdate=2010-02-19 |deadurl=yes |archiveurl=https://web.archive.org/web/20091219163323/http://www.adobe.com/go/readerextensions |archivedate=December 19, 2009 }}&lt;/ref&gt; The XFA Forms are not compatible with Adobe Reader prior to version 6.

XFA forms can be created and used as PDF files or as XDP ([[XML Data Package]]) files. The format of an XFA resource in PDF is described by the XML Data Package Specification.&lt;ref name="iso32000" /&gt; The XDP may be a standalone document or it may in turn be carried inside a PDF document. XDP provides a mechanism for packaging form components within a surrounding XML container. An XDP can also package a PDF file, along with XML form and template data.&lt;ref name="xfa-adobe" /&gt; PDF may contain XFA (in XDP format), but also XFA may contain PDF.&lt;ref name="xfa-adobe" /&gt; When the XFA (XML Forms Architecture) grammars used for an XFA form are moved from one application to another, they must be packaged as an XML Data Package.&lt;ref name="xfa25"&gt;{{citation |url=http://partners.adobe.com/public/developer/en/xml/xfa_spec_2_5.pdf |format=PDF |title=XML Forms Architecture (XFA) Specification Version 2.5 |date=2007-06-08 |accessdate=2010-02-19}}&lt;/ref&gt;

When the PDF and XFA are combined, the result is a form in which each page of the XFA form overlays a PDF background. This architecture is
sometimes referred to as XFAF (XFA Foreground). The alternative is to express all of the form, including boilerplate, directly in XFA (without using PDF, or only using "Shell PDF" which is a container for XFA with minimal skeleton of PDF markup, or using a pre-rendered depiction of a static XFA form as PDF pages). It is sometimes called ''full'' XFA.&lt;ref name="xfa25" /&gt;

Starting with PDF 1.5, the text contents of variable text form fields, as well as markup annotations may include formatting information (style information). These rich text strings are XML documents that conform to the rich text conventions specified for the XML Forms Architecture specification 2.02, which is itself a subset of the XHTML 1.0 specification, augmented with a restricted set of CSS2 style attributes.&lt;ref name="iso32000" /&gt;
In PDF 1.6, PDF supports the rich text elements and attributes specified in the XML Forms Architecture (XFA) Specification, 2.2.
In PDF 1.7, PDF supports the rich text elements and attributes specified in the XML Forms Architecture (XFA) Specification, 2.4.&lt;ref name="iso32000" /&gt;

Most PDF processors do not handle XFA content. When generating a shell PDF it is recommended to include in the PDF markup a simple one-page PDF image displaying a warning message (e.g. "To view the full contents of this document, you need a later version of the PDF viewer.", etc.). PDF processors that can render XFA content should either not display the supplied warning page image or replace it quickly with the dynamic form content.&lt;ref name="xfa33"&gt;{{citation |url=http://partners.adobe.com/public/developer/en/xml/xfa_spec_3_3.pdf |title=XML Forms Architecture (XFA) Specification Version 3.3 |date=2012-01-09 |accessdate=2014-04-09}}&lt;/ref&gt; Examples of PDF software with some support of XFA rendering include Adobe Reader for Windows, Linux, macOS (but not Adobe Reader Mobile for Android or iOS) or Nuance PDF Reader.

In 2014, ISO TC 171 voted to deprecate XFA for ISO 32000-2 ("Next-generation PDF").&lt;ref name="DRAFT INTERNATIONAL" /&gt;

=== Logical structure and accessibility ===

A "tagged" PDF (ISO 32000-1:2008 14.8) includes document structure and semantics information to enable reliable text extraction and accessibility. Technically speaking, tagged PDF is a stylized use of the format that builds on the logical structure framework introduced in PDF 1.3. Tagged PDF defines a set of standard structure types and attributes that allow page content (text, graphics, and images) to be extracted and reused for other purposes.&lt;ref&gt;[http://www.planetpdf.com/enterprise/article.asp?ContentID=6067 What is Tagged PDF?]&lt;/ref&gt;

Tagged PDF is not required in situations where a PDF file is intended only for print. Since the feature is optional, and since the rules for Tagged PDF as specified in ISO 32000-1 are relatively vague, support for tagged PDF amongst consuming devices, including assistive technology (AT), is uneven.&lt;ref&gt;{{cite web|url=http://www.washington.edu/doit/Stem/articles?1002|title=Is PDF accessible?|work=washington.edu}}&lt;/ref&gt;

An [[AIIM]] project to develop an ISO-standardized subset of PDF specifically targeted at accessibility began in 2004, eventually becoming [[PDF/UA]].

=== Security and signatures ===

A PDF file may be encrypted for security, or digitally signed for authentication.

The standard security provided by Acrobat PDF consists of two different methods and two different passwords, ''user password'', which encrypts the file and prevents opening, and ''owner password'', which specifies operations that should be restricted even when the document is decrypted, which can include: printing, copying text and graphics out of the document, modifying the document, or adding or modifying text notes and [[Acroforms|AcroForm]] fields. The user password (controls opening) encrypts the file and requires [[password cracking]] to defeat, with difficulty depending on password strength and encryption method – it is potentially very secure (assuming good password and encryption method without known attacks). The owner password (controls operations) does not encrypt the file, and instead relies on client software to respect these restrictions, and is not secure. An "owner password" can be removed by many commonly available "PDF cracking" software, including some free online services.&lt;ref&gt;{{cite web|url=http://freemypdf.com/|title=FreeMyPDF.com - Removes passwords from viewable PDFs|work=freemypdf.com}}&lt;/ref&gt; Thus, the use restrictions that a document author places on a PDF document are not secure, and cannot be assured once the file is distributed; this warning is displayed when applying such restrictions using Adobe Acrobat software to create or edit PDF files.

Even without removing the password, most freeware or open source PDF readers ignore the permission "protections" and allow the user to print or make copy of excerpts of the text as if the document were not limited by password protection.&lt;ref&gt;{{cite web |url= http://www.macworld.com/article/1137343/pdf.html |title=Adobe admits new PDF password protection is weaker |author= Jeremy Kirk}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url= http://www.cs.cmu.edu/~dst/Adobe/Gallery/PDFsecurity.pdf  |title= How secure is PDF |author=Bryan Guignard}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url= http://www.planetpdf.com/planetpdf/pdfs/pdf2k/01W/merz_securitykeynote.pdf |title= PDF Security Overview: Strengths and Weaknesses }}&lt;/ref&gt;

There are a number of commercial solutions including [[Adobe LiveCycle]] [[Adobe LiveCycle#LiveCycle Rights Management ES4|Rights Management]] and Locklizard PDF DRM&lt;ref&gt;{{cite web |url=http://www.infosecurity-magazine.com/news/locklizard-develops-zero-footprint-solution-for/ |title=LockLizard Develops Zero Footprint Solution for PDF Security }}&lt;/ref&gt; that are more robust means of [[information rights management]]. Not only can they restrict document access but they also reliably enforce [[File system permissions|permissions]] in ways that the standard security handler does not.&lt;ref&gt;{{cite web |url=http://www.locklizard.com/pdf_security_drm/ |title=PDF DRM Security Software for Adobe Document Protection}}&lt;/ref&gt;

==== Usage rights ====
Beginning with PDF 1.5, Usage rights (UR) signatures are used to enable additional interactive features that are not available by default in a particular PDF viewer application. The signature is used to validate that the permissions have been granted by a bona fide granting authority. For example, it can be used to allow a user:&lt;ref name="iso32000" /&gt;
* to save the PDF document along with modified form and/or annotation data
* import form data files in FDF, XFDF and text (CSV/TSV) formats
* export form data files in FDF and XFDF formats
* submit form data
* instantiate new pages from named page templates
* apply a [[Digital data|digital]] [[signature]] to existing [[digital signature]] form field
* create, delete, modify, copy, import, export annotations

For example, Adobe Systems grants permissions to enable additional features in Adobe Reader, using public-key [[cryptography]]. Adobe Reader verifies that the signature uses a [[Public key certificate|certificate]] from an Adobe-[[authorize]]d certificate authority. The PDF 1.5 specification declares that other PDF viewer applications are free to use this same mechanism for their own purposes.&lt;ref name="iso32000" /&gt;

=== File attachments ===

PDF files can have document-level and page-level file attachments, which the reader can access and open or save to their local filesystem. PDF attachments can be added to existing PDF files for example using [[pdftk]]. Adobe Reader provides support for attachments, and [[poppler (software)|poppler]]-based readers like [[Evince]] or [[Okular]] also have some support for document-level attachments.

=== Metadata ===
PDF files can contain two types of metadata.&lt;ref&gt;[https://www.adobe.com/devnet/acrobat/pdfs/pdf_reference_1-7.pdf Adobe PDF reference version 1.7], section 10.2&lt;/ref&gt; The first is the Document Information Dictionary, a set of key/value fields such as author, title, subject, creation and update dates. This is stored in the optional Info trailer of the file. A small set of fields is defined, and can be extended with additional text values if required.

In PDF 1.4, support was added for Metadata Streams, using the [[Extensible Metadata Platform]] (XMP) to add XML standards-based extensible metadata as used in other file formats. This allows metadata to be attached to any stream in the document, such as information about embedded illustrations, as well as the whole document (attaching to the document catalog), using an extensible schema.

== Intellectual property ==

Anyone may create applications that can read and write PDF files without having to pay royalties to [[Adobe Systems]]; Adobe holds patents to PDF, but licenses them for [[royalty-free]] use in developing software complying with its PDF specification.&lt;ref&gt;{{cite web|url=http://partners.adobe.com/public/developer/support/topic_legal_notices.html|title=Developer Resources|work=adobe.com}}&lt;/ref&gt;

== Technical issues ==

=== Accessibility ===
PDF files can be created specifically to be accessible for disabled people.&lt;ref&gt;{{cite web |url=http://www.webaim.org/techniques/acrobat/ |title=PDF Accessibility |publisher=WebAIM |accessdate=2010-04-24}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://www.alistapart.com/articles/pdf_accessibility |title=Facts and Opinions About PDF Accessibility |author=Joe Clark |date=2005-08-22 |accessdate=2010-04-24}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://wac.osu.edu/pdf/ |title=Accessibility and PDF documents |publisher=Web Accessibility Center |accessdate=2010-04-24}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://www.bbc.co.uk/guidelines/futuremedia/accessibility/accessible_pdf.shtml |title=PDF Accessibility Standards v1.2 |accessdate=2010-04-24}}&lt;/ref&gt;&lt;ref&gt;{{citation |url=http://www.csus.edu/training/handouts/workshops/creating_accessible_pdfs.pdf |format=PDF |title=PDF Accessibility |publisher=California State University |accessdate=2010-04-24}}&lt;/ref&gt; PDF file formats in use {{As of|2014|lc=on}} can include tags ([[XML]]), text equivalents, captions, audio descriptions, etc. Tagged PDF is required in the [[PDF/A]]-1a specification.&lt;ref&gt;{{citation |url=http://www.aiim.org/documents/standards/PDF-A/19005-1_FAQ.pdf |title=Frequently Asked Questions (FAQs) – ISO 19005-1:2005 – PDF/A-1, Date: July 10, 2006 |format=PDF |date=2006-07-10 |accessdate=2011-07-06}}&lt;/ref&gt;&lt;ref name="pdfa1-tech"&gt;{{cite web |url=http://www.pdfa.org/doku.php?id=artikel:en:pdfa_a_look_at_the_technical-side |title=PDF/A – A Look at the Technical Side |accessdate=2011-07-06}}&lt;/ref&gt; Some software can automatically produce tagged PDFs, but this feature is not always enabled by default.&lt;ref&gt;{{citation |url=http://help.libreoffice.org/Common/Export_as_PDF#PDF.2FA-1a |title=LibreOffice Help - Export as PDF |accessdate=2012-09-22}}&lt;/ref&gt;&lt;ref&gt;{{citation |url=http://www.oooninja.com/2008/01/generating-pdfa-for-long-term-archiving.html |title=Exporting PDF/A for long-term archiving |date=2008-01-11}}&lt;/ref&gt; Leading [[screen reader]]s, including [[JAWS (screen reader)|JAWS]], [[Window-Eyes]], Hal, and [[Kurzweil Educational Systems|Kurzweil 1000 and 3000]] can read tagged PDFs aloud, as can later versions of the Acrobat and Acrobat Reader programs.&lt;ref&gt;{{cite web |url=http://help.adobe.com/en_US/Reader/8.0/help.html?content=WS58a04a822e3e50102bd615109794195ff-7d15.html |title=Adobe Reader 8 - Read a PDF with Read Out Loud |accessdate=2010-04-24}}&lt;/ref&gt;&lt;ref&gt;{{cite news |url=http://gadgetwise.blogs.nytimes.com/2009/04/10/tip-of-the-week-adobe-readers-read-aloud-feature/ |title=Tip of the Week: Adobe Reader’s ‘Read Aloud’ Feature |accessdate=2010-04-24 | work=The New York Times | date=2009-04-10 |first=J.D. |last=Biersdorfer}}&lt;/ref&gt;&lt;ref&gt;{{citation |url=https://www.adobe.com/accessibility/pdfs/accessing-pdf-sr.pdf |format=PDF |title=Accessing PDF documents with assistive technology: A screen reader user's guide |publisher=Adobe |accessdate=2010-04-24}}&lt;/ref&gt; Moreover, tagged PDFs can be re-flowed and magnified for readers with visual impairments. Problems remain with adding tags to older PDFs and those that are generated from scanned documents. In these cases, accessibility tags and re-flowing are unavailable, and must be created either manually or with OCR techniques. These processes are inaccessible to some disabled people.

One of the significant challenges with PDF accessibility is that PDF documents have three distinct views, which, depending on the document's creation, can be inconsistent with each other. The three views are (i) the physical view, (ii) the tags view, and (iii) the content view. The physical view is displayed and printed (what most people consider a PDF document). The tags view is what screen readers and other assistive technologies use to deliver a high-quality navigation and reading experience to users with disabilities. The content view is based on the physical order of objects within the PDF's content stream and may be displayed by software that does not fully support the tags view, such as the Reflow feature in Adobe's Reader.

[[PDF/UA]], the International Standard for accessible PDF based on ISO 32000-1 was published as ISO 14289-1 in 2012, and establishes normative language for accessible PDF technology.

=== Viruses and exploits ===
{{see also|Adobe Acrobat#Security}}
PDF attachments carrying viruses were first discovered in 2001. The virus, named ''OUTLOOK.PDFWorm'' or ''Peachy'', uses [[Microsoft Outlook]] to send itself as an attachment to an Adobe PDF file. It was activated with Adobe Acrobat, but not with Acrobat Reader.&lt;ref&gt;Adobe Forums, [https://forums.adobe.com/thread/302989 Announcement: PDF Attachment Virus "Peachy"], 15 August 2001.&lt;/ref&gt;

From time to time, new vulnerabilities are discovered in various versions of Adobe Reader,&lt;ref&gt;{{cite web|url=https://www.adobe.com/support/security/#readerwin |title=Security bulletins and advisories |publisher=Adobe |date= |accessdate=2010-02-21}}&lt;/ref&gt; prompting the company to issue security fixes. Other PDF readers are also susceptible. One aggravating factor is that a PDF reader can be configured to start automatically if a web page has an embedded PDF file, providing a vector for attack. If a malicious web page contains an infected PDF file that takes advantage of a vulnerability in the PDF reader, the system may be compromised even if the browser is secure. Some of these vulnerabilities are a result of the PDF standard allowing PDF documents to be scripted with JavaScript. Disabling JavaScript execution in the PDF reader can help mitigate such future exploits, although it does not protect against exploits in other parts of the PDF viewing software. Security experts say that JavaScript is not essential for a PDF reader, and that the security benefit that comes from disabling JavaScript outweighs any compatibility issues caused.&lt;ref&gt;[http://www.grc.com/sn/sn-187.txt Steve Gibson - SecurityNow Podcast]&lt;/ref&gt; One way of avoiding PDF file exploits is to have a local or web service convert files to another format before viewing.

On March 30, 2010 security researcher Didier Stevens reported an Adobe Reader and Foxit Reader exploit that runs a malicious executable if the user allows it to launch when asked.&lt;ref&gt;{{cite web|url=http://blogs.pcmag.com/securitywatch/2010/03/malicious_pdfs_execute_code_wi.php|title=Malicious PDFs Execute Code Without a Vulnerability|work=PCMAG}}&lt;/ref&gt;

=== Usage restrictions and monitoring ===

PDFs may be [[encrypted]] so that a password is needed to view or edit the contents. The PDF Reference defines both 40-bit and 128-bit encryption, both making use of a complex system of [[RC4]] and [[MD5]]. The PDF Reference also defines ways that third parties can define their own encryption systems for PDF.

PDF files may also contain embedded [[digital rights management|DRM]] restrictions that provide further controls that limit copying, editing or printing. The restrictions on copying, editing, or printing depend on the reader software to obey them, so the security they provide is limited.

The PDF Reference has technical details for an end-user overview.&lt;ref&gt;{{cite web|url=http://createpdf.adobe.com/cgi-feeder.pl/help_security?BP=&amp;LOC=en_US |title=Create Adobe PDF Online - Security Settings Help |publisher=Createpdf.adobe.com |date= |accessdate=2010-02-21}}&lt;/ref&gt;  Like HTML files, PDF files may submit information to a web server. This could be used to track the [[IP address]] of the client PC, a process known as [[phoning home]]. After update 7.0.5 to Acrobat Reader, the user is notified "...&amp;nbsp;via a dialogue box that the author of the file is auditing usage of the file, and be offered the option of continuing."&lt;ref&gt;[https://www.adobe.com/support/techdocs/332208.html New features and issues addressed in the Acrobat 7.0.5 Update (Acrobat and Adobe Reader for Windows and Mac OS)]&lt;/ref&gt;

Through its [[Adobe LiveCycle|LiveCycle Policy Server]] product, Adobe provides a method to set security policies on specific documents. This can include requiring a user to authenticate and limiting the period during which a document can be accessed or amount of time a document can be opened while offline. Once a PDF document is tied to a policy server and a specific policy, that policy can be changed or revoked by the owner. This controls documents that are otherwise "in the wild." Each document open and close event can also be tracked by the policy server. Policy servers can be set up privately or Adobe offers a public service through Adobe Online Services. As with other forms of DRM, adherence to these policies and restrictions may or may not be enforced by the reader software being used.

=== Default display settings ===
PDF documents can contain display settings, including the page display layout and zoom level. Adobe Reader uses these settings to override the user's default settings when opening the document.&lt;ref&gt;{{cite web | title=Getting Familiar with Adobe Reader &amp;gt; Understanding Preferences | url=http://www.adobepress.com/articles/article.asp?p=412914 | accessdate=2009-04-22}}&lt;/ref&gt; The free Adobe Reader cannot remove these settings.

== Content ==
A PDF file is often a combination of [[vector graphics]], text, and [[bitmap graphics]]. The basic types of content in a PDF are:
* Text stored as content streams (i.e., not text)
* Vector graphics for illustrations and designs that consist of shapes and lines
* Raster graphics for photographs and other types of image
* Multimedia objects in the document

In later PDF revisions, a PDF document can also support links (inside document or web page), forms, JavaScript (initially available as plugin for Acrobat 3.0), or any other types of embedded contents that can be handled using plug-ins.

PDF 1.6 supports interactive 3D documents embedded in the PDF - 3D drawings can be embedded using [[U3D]] or [[PRC (file format)|PRC]] and various other data formats.&lt;ref name="3d#1"&gt;{{cite web|url=https://www.adobe.com/manufacturing/resources/3dformats/ |title=3D supported formats |publisher=Adobe |date=2009-07-14 |accessdate=2010-02-21}}&lt;/ref&gt;&lt;ref name="3d#2"&gt;{{cite web|url=https://www.adobe.com/devnet/acrobat3d/ |title=Acrobat 3D Developer Center |publisher=Adobe |date= |accessdate=2010-02-21}}&lt;/ref&gt;

Two PDF files that look similar on a computer screen may be of very different sizes. For example, a high resolution raster image takes more space than a low resolution one. Typically higher resolution is needed for printing documents than for displaying them on screen. Other things that may increase the size of a file is embedding full fonts, especially for Asiatic scripts, and storing text as graphics.

== Software ==
{{Details|List of PDF software}}
PDF viewers are generally provided free of charge, and many versions are available from a variety of sources.

There are many software options for creating PDFs, including the PDF printing capabilities built into [[macOS]] and most [[Linux]] distributions, [[LibreOffice]], [[Microsoft Office 2007]] (if updated to [[Office 2007#Service Pack 2|SP2]]) and later,&lt;ref&gt;{{cite web |url=http://support.microsoft.com/kb/953195|title=Description of 2007 Microsoft Office Suite Service Pack 2 (SP2) |publisher=[[Microsoft]] |accessdate=2009-05-09}}&lt;/ref&gt; [[WordPerfect]] 9, [[Scribus]], numerous PDF print drivers for [[Microsoft Windows]], the [[pdfTeX]] typesetting system, the [[DocBook]] PDF tools, applications developed around [[Ghostscript]] and [[Adobe Acrobat]] itself as well as [[Adobe InDesign]], [[Adobe FrameMaker]], [[Adobe Illustrator]], [[Adobe Photoshop]]. [[Google]]'s online office suite [[Google Docs]] also allows for uploading and saving to PDF.

[[Raster image processor]]s (RIPs) are used to convert PDF files into a [[raster graphics|raster format]] suitable for imaging onto paper and other media in printers, digital production presses and [[prepress]] in a process known as [[rasterisation]]. RIPs capable of processing PDF directly include the Adobe PDF Print Engine&lt;ref&gt;{{cite web|url=https://www.adobe.com/products/pdfprintengine/overview.html|title=Adobe PDF Print Engine|work=adobe.com}}&lt;/ref&gt; from [[Adobe Systems]] and Jaws&lt;ref&gt;{{cite web|url=http://www.globalgraphics.com/products/jaws_rip/|title=Jaws® 3.0 PDF and PostScript RIP SDK|work=globalgraphics.com}}&lt;/ref&gt; and the [[Harlequin RIP]] from [[Global Graphics]].

=== Editing ===
{{Expand section|date=July 2010|reason=[[hybrid PDF]], a variant of [[LibreOffice]] isn't mentioned}}
There is specialized software for editing PDF files, though the choices are much more limited and often more expensive than creating and editing standard editable document formats. Version 0.46 and later of [[Inkscape]] allows PDF editing through an intermediate translation step involving [[Poppler (software)|Poppler]].

[[Serif PagePlus]] can open, edit and save existing PDF documents, as well as publishing of documents created in the package.

[[Enfocus]] PitStop Pro, a plugin for Acrobat, allows manual and automatic editing of PDF files,&lt;ref&gt;{{cite web|url=http://www.enfocus.com/product.php?id=855|title=Preflight and edit PDF files in Acrobat|work=enfocus.com}}&lt;/ref&gt; while the free Enfocus Browser makes it possible to edit the low-level structure of a PDF.&lt;ref&gt;{{cite web|url=http://www.enfocus.com/product.php?id=4530|title=Enfocus product overview - online store|work=enfocus.com}}&lt;/ref&gt;

[[Dochub]], is a free online PDF editing tool that can be used without purchasing anything.&lt;ref&gt;{{Cite web|title = DocHub|url = http://www.dochub.com|website = DocHub|accessdate = 2015-12-12}}&lt;/ref&gt;

=== Annotation ===
{{See also|Comparison of notetaking software}}
[[Adobe Acrobat]] is one example of proprietary software that allows the user to annotate, highlight, and add notes to already created PDF files. One UNIX application available as [[free software]] (under the [[GNU General Public License]]) is [[PDFedit]]. Another GPL-licensed application native to the unix environment is Xournal. Xournal allows for annotating in different fonts and colours, as well as a rule for quickly underlining and highlighting lines of text or paragraphs. Xournal also has a shape recognition tool for squares, rectangles and circles. In Xournal annotations may be moved, copied and pasted. The [[freeware]] [[Foxit Reader]], available for [[Microsoft Windows]], [[macOS]] and [[Linux]], allows annotating documents. Tracker Software's [[PDF-XChange Viewer]] allows annotations and markups without restrictions in its freeware alternative. [[Apple Inc.|Apple]]'s [[macOS]]'s integrated PDF viewer, Preview, does also enable annotations as does the freeware [[Skim (software)|Skim]], with the latter supporting interaction with [[LaTeX]], SyncTeX, and PDFSync and integration with [[BibDesk]] reference management software. Freeware [[Qiqqa]] can create an annotation report that summarizes all the annotations and notes one has made across their library of PDFs.

For mobile annotation, [[iAnnotate PDF]] (from Branchfire) and [[GoodReader]] (from Aji) allow annotation of PDFs as well as exporting summaries of the annotations.

There are also [[web annotation]] systems that support annotation in pdf and other documents formats, e.g., [[A.nnotate]], [[crocodoc]], WebNotes.

In cases where PDFs are expected to have all of the functionality of paper documents, ink annotation is required. Some programs that accept ink input from the mouse may not be responsive enough for handwriting input on a tablet. Existing solutions on the PC include [[PDF Annotator]] and [[Qiqqa]].

=== Other ===
Examples of PDF software as online services including [[Scribd]] for viewing and storing, [[Pdfvue]] for online editing, and [[Zamzar]] for conversion.

In 1993 the Jaws [[raster image processor]] from [[Global Graphics]] became the first shipping prepress RIP that interpreted PDF natively without conversion to another format. The company released an upgrade to their Harlequin RIP with the same capability in 1997.&lt;ref&gt;{{cite web |url= http://www.globalgraphics.com/products/harlequin-multi-rip |title=Harlequin MultiRIP|accessdate=2014-03-02}}&lt;/ref&gt;

[[Agfa-Gevaert]] introduced and shipped Apogee, the first prepress workflow system based on PDF, in 1997.

Many commercial offset printers have accepted the submission of press-ready PDF files as a print source, specifically the PDF/X-1a subset and variations of the same.&lt;ref&gt;[http://www.prepressx.com/ Press-Ready PDF Files] "For anyone interested in having their graphic project commercially printed directly from digital files or PDFs." (last checked on 2009-02-10).&lt;/ref&gt; The submission of press-ready PDF files are a replacement for the problematic need for receiving collected native working files.

PDF was selected as the "native" [[metafile]] format for [[macOS|Mac OS X]], replacing the [[PICT]] format of the earlier [[classic Mac OS]]. The imaging model of the [[Quartz (graphics layer)|Quartz]] graphics layer is based on the model common to [[Display PostScript]] and PDF, leading to the nickname ''Display PDF''. The Preview application can display PDF files, as can version 2.0 and later of the [[Safari (web browser)|Safari]] web browser. System-level support for PDF allows Mac OS X applications to create PDF documents automatically, provided they support the OS-standard printing architecture. The files are then exported in PDF 1.3 format according to the file header. When taking a screenshot under Mac OS X versions 10.0 through 10.3, the image was also captured as a PDF; later versions save screen captures as a [[Portable Network Graphics|PNG]] file, though this behaviour can be set back to PDF if desired.

In 2006 PDF was widely accepted as the standard print job format at the [[Open Source Development Labs]] Printing Summit. It is supported as a print job format by the [[CUPS|Common Unix Printing System]] and desktop application projects such as [[GNOME]], [[KDE]], [[Firefox]], [[Mozilla Thunderbird|Thunderbird]], [[LibreOffice]] and [[OpenOffice.org|OpenOffice]] have switched to emit print jobs in PDF.&lt;ref&gt;{{cite web|title=PDF as Standard Print Job Format|url=http://www.linuxfoundation.org/collaborate/workgroups/openprinting/pdf_as_standard_print_job_format|website=The Linux Foundation|publisher=[[Linux Foundation]]|accessdate=21 June 2016}}&lt;/ref&gt;

Some desktop printers also support direct PDF printing, which can interpret PDF data without external help. Currently, all PDF capable printers also support PostScript, but most PostScript printers do not support direct PDF printing.

The [[Free Software Foundation]] once considered one of their [[High priority free software projects|high priority projects]] to be "developing a free, high-quality and fully functional set of libraries and programs that implement the PDF file format and associated technologies to the ISO 32000 standard."&lt;ref&gt;On 2014-04-02, a note dated 2009-02-10 referred to [http://www.fsf.org/campaigns/priority.html Current FSF High Priority Free Software Projects] as a source. Content of the latter page, however, changes over time.&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://gnupdf.org/Goals_and_Motivations |title=Goals and Motivations |authors=GNUpdf contributors| publisher=''GNUpdf'' |date=2007-11-28 |website=gnupdf.org |accessdate=2014-04-02 }}&lt;/ref&gt; In 2011, however, the [[GNU PDF]] project was removed from the list of "high priority projects" due to the maturation of the [[Poppler (software)|Poppler library]],&lt;ref&gt;{{cite web|title=GNU PDF project leaves FSF High Priority Projects list; mission complete! |url=http://www.fsf.org/blogs/community/gnu-pdf-project-leaves-high-priority-projects-list-mission-complete|date=2011-10-06|first=Matt|last=Lee|publisher=Free Software Foundation|website=fsf.org|accessdate=2014-04-02}}&lt;/ref&gt; which has enjoyed wider use in applications such as [[Evince]] with the [[GNOME]] desktop environment. Poppler is based on [[Xpdf]]&lt;ref&gt;[http://poppler.freedesktop.org/ Poppler homepage] "Poppler is a PDF rendering library based on the xpdf-3.0 code base." (last checked on 2009-02-10)&lt;/ref&gt;&lt;ref&gt;[http://cgit.freedesktop.org/poppler/poppler/tree/README-XPDF Xpdf license] "Xpdf is licensed under the GNU General Public License (GPL), version 2 or 3." (last checked on 2012-09-23).&lt;/ref&gt; code base. There are also commercial development libraries available as listed in [[List of PDF software]].

The [[Apache PDFBox]] project of the [[Apache Software Foundation]] is an open source Java library for working with PDF documents. PDFBox is licensed under the [[Apache License]].&lt;ref&gt;[http://pdfbox.apache.org/ The Apache PDFBox project] . Retrieved 2009-09-19.&lt;/ref&gt;

== See also ==
{{Portal|Software}}{{columns-list|2|
* [[Open XML Paper Specification]]
* [[Comparison of OpenXPS and PDF]]
* [[DjVu]]
* [[PAdES]], &lt;small&gt;PDF Advanced Electronic Signature&lt;/small&gt;
* [[Web document]]
* [[XSL Formatting Objects]]
}}

== References ==
{{Reflist|30em}}

== Further reading ==
* {{Cite book | last1 = Hardy | first1 = M. R. B. | last2 = Brailsford | first2 = D. F. | chapter = Mapping and displaying structural transformations between XML and PDF | title = Proceedings of the 2002 ACM symposium on Document engineering  - DocEng '02 | pages = 95–102| year = 2002 | url = http://www.cs.nott.ac.uk/~dfb/Publications/Download/2002/Hardy02.pdf| doi = 10.1145/585058.585077| publisher = Proceedings of the 2002 ACM symposium on Document engineering| isbn = 1-58113-594-7}}
*Standards
** PDF 1.7 [http://www.adobe.com/content/dam/Adobe/en/devnet/acrobat/pdfs/pdf_reference_1-7.pdf]
** PDF 1.6 (ISBN 0-321-30474-8)
** PDF 1.4 (ISBN 0-201-75839-3)
** PDF 1.3 (ISBN 0-201-61588-6)

== External links ==
{{Commons category|PDF}}
* [http://www.quora.com/PDF-file-format/How-was-the-PDF-format-created How was the PDF format created? Quora]
* [http://www.pdfa.org/ PDF Association] - The PDF Association is the industry association for software developers producing or processing PDF files.
* [http://partners.adobe.com/public/developer/tips/topic_tip31.html Adobe PDF 101: Summary of PDF]
* [https://www.adobe.com/print/features/psvspdf/ Adobe: PostScript vs. PDF] – Official introductory comparison of PS, EPS vs. PDF.
* {{webarchive |url=https://web.archive.org/web/20110424013530/http://www.aiim.org/Resources/Archive/Magazine/2007-Jul-Aug/33448 |date=April 24, 2011 |title=''PDF Standards....transitioning the PDF specification from a de facto standard to a de jure standard'' }} – Information about PDF/E and PDF/UA specification for accessible documents file format (archived by [[Wayback Machine|The Wayback Machine]])
* [http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=38920 ISO 19005-1:2005] the PDF/A-1 ISO standard published by the [[International Organization for Standardization]] (chargeable)
* [https://www.adobe.com/devnet/pdf/pdf_reference.html PDF Reference and Adobe Extensions to the PDF Specification]
* [http://www.mactech.com/articles/mactech/Vol.15/15.09/PDFIntro/ Portable Document Format: An Introduction for Programmers] – Introduction to PDF vs. PostScript and PDF internals (up to v1.3)
* [http://www.planetpdf.com/enterprise/article.asp?ContentID=6519 The Camelot Paper] – the paper in which John Warnock outlined the project that created PDF
* [http://river-valley.zeeba.tv/everything-you-wanted-to-know-about-pdf-but-were-afraid-to-ask/ Everything you wanted to know about PDF but were afraid to ask] - recording of talk by Leonard Rosenthol (Adobe Systems) at TUG 2007
* [http://www.data2type.de/en/xml-xslt-xslfo/xsl-fo/ How to produce PDF with XSL-FO]
* [http://pdfextractoronline.com/ PDF To Excel Converter]
{{Graphics file formats}}
{{Office document file formats}}
{{ISO standards}}
{{Ebooks}} &lt;!--navbox--&gt;

[[Category:1993 introductions]]
[[Category:Adobe Systems]]
[[Category:Digital press]]
[[Category:Electronic documents]]
[[Category:Graphics file formats]]
[[Category:ISO standards]]
[[Category:Office document file formats]]
[[Category:Open formats]]
[[Category:Page description languages]]
[[Category:Vector graphics]]</text>
      <sha1>s7di8iua6533bmdpwlkzpqps301i0rk</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Legal citators</title>
    <ns>14</ns>
    <id>24447352</id>
    <revision>
      <id>389225528</id>
      <parentid>315734059</parentid>
      <timestamp>2010-10-07T02:14:01Z</timestamp>
      <contributor>
        <username>SporkBot</username>
        <id>12406635</id>
      </contributor>
      <minor />
      <comment>Merging catmore1/catmore2 per [[Wikipedia:Templates for discussion/Log/2010 September 10|TFD]], and renaming catmore/catmore2 per [[Template talk:cat main|discussion]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="107" xml:space="preserve">{{Cat main|Citator}}

[[Category:Citation indices]]
[[Category:Legal research]]
[[Category:Legal citation]]</text>
      <sha1>rtwjcp6ezchgu1mcxoud4orozv4mdjv</sha1>
    </revision>
  </page>
  <page>
    <title>Science Citation Index</title>
    <ns>0</ns>
    <id>6852678</id>
    <revision>
      <id>760552398</id>
      <parentid>760304433</parentid>
      <timestamp>2017-01-17T18:26:14Z</timestamp>
      <contributor>
        <username>Schlind</username>
        <id>25113919</id>
      </contributor>
      <comment>updated</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10341" xml:space="preserve">{{incomplete|date=January 2014}}
{{ infobox bibliographic database
| title = Science Citation Index
| image = 
| caption = 
| producer = [[Thomson Reuters]]
| country = United States
| history = 2000-present
| languages = 
| providers = 
| cost = 
| disciplines = Science, medicine, and technology
| depth = 
| formats = 
| temporal = 
| geospatial = 
| number = 

| updates = 
| p_title = 
| p_dates = 
| ISSN = 0036-827X
| web = http://thomsonreuters.com/science-citation-index-expanded/
| titles = 
}}
The '''Science Citation Index''' ('''SCI''') is a [[citation index]] originally produced by the [[Institute for Scientific Information]] (ISI) and created by [[Eugene Garfield]]. It was officially launched in 1964. It is now owned by [[Clarivate Analytics]] (previously the Intellectual Property and Science business of [[Thomson Reuters]]).&lt;ref name=dimension&gt;
{{cite journal
|doi=10.1126/science.122.3159.108
|title=Citation Indexes for Science: A New Dimension in Documentation through Association of Ideas
|url=http://ije.oxfordjournals.org/content/35/5/1123.full
|format=Free web article download
|year=1955
|last1=Garfield
|first1=E.
|journal=Science
|volume=122
|issue=3159
|pages=108–11
|pmid=14385826|bibcode=1955Sci...122..108G
}}&lt;/ref&gt;&lt;ref name=evolve&gt;
{{cite journal
 |last = Garfield 
 |first = Eugene
 |doi=10.2436/20.1501.01.10
 |url=http://garfield.library.upenn.edu/papers/barcelona2007a.pdf
 |format=Free PDF download
 |title=The evolution of the Science Citation Index|doi-broken-date = 2017-01-16
 }} International microbiology '''10.'''1 (2010): 65-69.&lt;/ref&gt;&lt;ref name=gOverview&gt;
{{cite web
 | last = Garfield 
 | first = Eugene
 | authorlink =
 | coauthors =
 | title = Science Citation Index
 | work = Science Citation Index 1961
 | publisher = Garfield Library - UPenn
 | date = 1963
 | url = http://garfield.library.upenn.edu/papers/80.pdf
 | format = Free PDF download
 | doi =
 | accessdate = 2013-05-27}} 
* Originally published by the Institute of Scientific Information in 1964
* Other titles in this document are: What is a Citation Index? , How is the Citation Index Prepared? , How is the Citation Index Used? , Applications of the Science Citation Index , Source Coverage and Statistics , and a Glossary.&lt;/ref&gt;&lt;ref name=history-cite-indexing&gt;
{{cite web
 | title =History of Citation Indexing 
 | work =Needs of researchers create demand for citation indexing 
 | publisher =Thomson Ruters 
 | date =November 2010 
 | url =http://thomsonreuters.com/products_services/science/free/essays/history_of_citation_indexing/ 
 | format =Free HTML download 
 | accessdate =2010-11-04}}&lt;/ref&gt; The larger version ('''Science Citation Index Expanded''') covers more than 8,500 notable and significant [[Scientific journal|journals]], across 150 disciplines, from 1900 to the present. These are alternatively described as the world's leading journals of [[science]] and [[technology]], because of a rigorous selection process.&lt;ref name=Expanded&gt;
{{cite web 
|url=https://www.thomsonreuters.com/en/products-services/scholarly-scientific-research/scholarly-search-and-discovery/science-citation-index-expanded.html 
|title=Science Citation Index Expanded 
|work= |accessdate=2017-01-17}}&lt;/ref&gt;&lt;ref name=wetland&gt;{{cite journal| doi= 10.1007/s12665-012-2193-y|title= The Top-cited Wetland Articles in Science Citation Index Expanded: characteristics and hotspots|url=http://dns2.asia.edu.tw/~ysho/YSHO-English/Publications/PDF/Env%20Ear%20Sci-Ma.pdf|date= December 2012| last1= Ma| first1= Jiupeng| last2= Fu| first2= Hui-Zhen| last3= Ho| first3= Yuh-Shan| journal= Environmental Earth Sciences|volume= 70|issue= 3|pages= 1039}} (Springer-Verlag)&lt;/ref&gt;&lt;ref name=shan&gt;
{{cite journal 
| doi= 10.1007/s11192-012-0837-z 
|title= The top-cited research works in the Science Citation Index Expanded 
|url= http://trend.asia.edu.tw/Publications/PDF/Scientometrics94,%201297.pdf 
| year= 2012 
| last1= Ho 
| first1= Yuh-Shan 
| journal= Scientometrics 
| volume= 94 
| issue= 3 
| page= 1297}}&lt;/ref&gt;

The index is made available online through different platforms, such as the [[Web of Science]]&lt;ref name=AtoZ&gt;{{cite web |last=ISI Web of Knowledge platform |title =Available databases A to Z |publisher=Thomson Reuters |year=2010 |url=http://wokinfo.com/products_tools/products/ |format=Choose databases on method of discovery and analysis |accessdate=2010-06-24}}&lt;/ref&gt;&lt;ref&gt;[http://wokinfo.com/media/pdf/SSR1103443WoK5-2_web3.pdf Thomson Reuters Web of Knowledge. Thomson Reuters, 2013.]&lt;/ref&gt; and SciSearch.&lt;ref&gt;{{cite web |url=http://library.dialog.com/bluesheets/html/bl0034.html |title=SCISEARCH - A CITED REFERENCE SCIENCE DATABASE |publisher=Library.dialog.com |date= |accessdate=2014-04-17}}&lt;/ref&gt; (There are also CD and printed editions, covering a smaller number of journals). This database allows a researcher to identify which later articles have cited any particular earlier article, or have cited the articles of any particular author, or have been cited most frequently. Thomson Reuters also markets several subsets of this database, termed "Specialty Citation Indexes",&lt;ref name=SpCI&gt;
{{cite web 
|url=http://thomsonreuters.com/products_services/science/science_products/a-z/specialty_citation_indexes/ 
|title=Specialty Citation Indexes 
|work= |accessdate=2009-08-30}}&lt;/ref&gt; 
such as the '''Neuroscience Citation Index'''&lt;ref name=NCI&gt;
{{cite web 
|url=http://science.thomsonreuters.com/cgi-bin/jrnlst/jlresults.cgi?PC=MD 
|title=Journal Search - Science - |work= |accessdate=2009-08-30}}&lt;/ref&gt; and the '''Chemistry Citation Index'''.&lt;ref&gt;{{cite web |url=http://science.thomsonreuters.com/cgi-bin/jrnlst/jloptions.cgi?PC=CD 
|title=Journal Search - Science - Thomson Reuters |accessdate=14 January 2011}}&lt;/ref&gt;

==Chemistry Citation Index==
The Chemistry Citation Index was first introduced by Eugene Garfield, a chemist by training. His original "search examples were based on [his] experience as a chemist".&lt;ref name=Garcci/&gt; In 1992 an electronic and print form of the index was derived from a core of 330 chemistry journals, within which all areas were covered. Additional information was provided from articles selected from 4,000 other journals. All chemistry subdisciplines were covered: organic, inorganic, analytical, physical chemistry, polymer, computational, organometallic, materials chemistry, and electrochemistry.&lt;ref name=Garcci&gt;Garfield, Eugene. "[http://garfield.library.upenn.edu/essays/v15p007y1992-93.pdf New Chemistry Citation Index On CD-ROM Comes With Abstracts, Related Records, and Key-Words-Plus]." Current Contents 3 (1992): 5-9.&lt;/ref&gt;

By 2002 the core journal coverage increased to 500 and related article coverage increased to 8,000 other journals.&lt;ref&gt;
[http://www.chinweb.com/cgi-bin/chemport/getfiler.cgi?ID=k4l7vyYF5FimYvScsOm3pxWVmEhBoH0ZuYgxjLdKBfqdmURDHLrjuVv78i16JLPX&amp;VER=E Chemistry Citation Index]. Institute of Process Engineering of the Chinese Academy of Sciences. 2003.&lt;/ref&gt; 

One 1980 study reported the overall citation indexing benefits for chemistry, examining the use of citations as a tool for the study of the sociology of chemistry and illustrating the use of citation data to "observe" chemistry subfields over time.&lt;ref&gt;
{{cite journal
|doi=10.1007/BF02016348
|title=Science citation index and chemistry
|year=1980
|last1=Dewitt
|first1=T. W.
|last2=Nicholson
|first2=R. S.
|last3=Wilson
|first3=M. K.
|journal=Scientometrics
|volume=2
|issue=4
|page=265}}&lt;/ref&gt;

==See also==
* [[Arts and Humanities Citation Index]], which covers 1130 journals, beginning with 1975.
* [[Impact factor]]
* [[List of academic databases and search engines]]
* [[Social Sciences Citation Index]], which covers 1700 journals, beginning with 1956.

==References==
{{Reflist|30em}}

==Further reading==
*{{cite journal
|doi= 10.1002/aris.1440360102
|url= http://polaris.gseis.ucla.edu/cborgman/pubs/borgmanfurnerarist2002.pdf
|title=Scholarly Communication and Bibliometrics
|year= 2005
|last1= Borgman
|first1= Christine L.
|last2= Furner
|first2= Jonathan
|journal= Annual Review of Information Science and Technology
|volume= 36
|issue= 1 
|pages=3–72}}

*{{cite journal
|doi= 10.1002/asi.20677
|url= http://staff.aub.edu.lb/~lmeho/meho-yang-impact-of-data-sources.pdf
|format= Free PDF download
|title= Impact of data sources on citation counts and rankings of LIS faculty: Web of science versus scopus and google scholar
|year= 2007
|last1= Meho
|first1= Lokman I.
|last2= Yang
|first2= Kiduk
|journal= Journal of the American Society for Information Science and Technology
|volume= 58
|issue= 13
|page= 2105}}

*{{cite journal
|doi= 10.1002/asi.5090140304
|url= http://www.garfield.library.upenn.edu/essays/v6p492y1983.pdf
|format= Free PDF download
|title= New factors in the evaluation of scientific literature through citation indexing
|year= 1963
|last1= Garfield
|first1= E.
|last2= Sher
|first2= I. H.
|journal= American Documentation
|volume= 14
|issue= 3
|page= 195}}

*{{cite journal
|doi= 10.1038/227669a0
|url= http://www.garfield.library.upenn.edu/essays/V1p133y1962-73.pdf
|format= Free PDF download
|title= Citation Indexing for Studying Science
|year= 1970
|last1= Garfield
|first1= E.
|journal= Nature
|volume= 227
|issue= 5259
|pages= 669–71
|pmid= 4914589|bibcode= 1970Natur.227..669G
}}

*{{cite book
 | last =Garfield
 | first =Eugene 
 | authorlink =
 | title =Citation Indexing: Its Theory and Application in Science, Technology, and Humanities
 | publisher =Wiley-Interscience
 | series = Information Sciences Series
 | edition = 1st
 | origyear = 1979| year = 1983
 | location = New York
 | isbn =9780894950247}}

==External links==
* [http://scientific.thomson.com/products/wos/ Introduction to SCI]
* [http://science.thomsonreuters.com/mjl/ Master journal list]
* [https://en.wikibooks.org/wiki/Chemical_Information_Sources/Author_and_Citation_Searches Chemical Information Sources/ Author and Citation Searches]. on WikiBooks. 
* [http://scientific.thomson.com/tutorials/citedreference/crs1.htm Cited Reference Searching: An Introduction]. Thomson Reuters. 
* [http://www.chinweb.com/cgi-bin/chemport/getfiler.cgi?ID=k4l7vyYF5FimYvScsOm3pxWVmEhBoH0ZuYgxjLdKBfqdmURDHLrjuVv78i16JLPX&amp;VER=E Chemistry Citation Index]. Chinweb.

{{Thomson Reuters}}

[[Category:Citation indices]]
[[Category:Online databases]]
[[Category:Thomson Reuters]]</text>
      <sha1>2sfjgeetu0x6qz4eab70liaq02ylaa2</sha1>
    </revision>
  </page>
  <page>
    <title>SPIN bibliographic database</title>
    <ns>0</ns>
    <id>28010203</id>
    <revision>
      <id>678206401</id>
      <parentid>678205296</parentid>
      <timestamp>2015-08-28T00:15:29Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor />
      <comment>Robot - Moving category Bibliographic databases to [[:Category:Bibliographic databases and indexes]] per [[WP:CFD|CFD]] at [[Wikipedia:Categories for discussion/Log/2015 July 4]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5091" xml:space="preserve">{{Infobox Bibliographic Database
|title =SPIN  (Searchable Physics Information Notices)  
|image = 
|caption = 
|producer =[[American Institute of Physics]] (AIP) 
|country =USA, Russia, Ukraine
|history = 
|languages =English, [[Russian language|Russian]], [[Ukrainian language|Ukrainian]] 
|providers =[[Dialog (online database)|Dialog]], [[American Institute of Physics|AIP website]], [[SPIE|SPIE Digital Library]] 
|cost = 
|disciplines =Physics, Astronomy, Mathematics, Geophysics, Geosciences, Nuclear Science, Science &amp; Technology 
|depth =Word, Phrase, Abstract, Author and Author affiliations, Descriptor, Errata (coden, or date, or volume) Identifier, Title, Astronomical objects, CODEN, Conference (location, or title, or year), Journal name, and more...   
|formats =Journal Articles, Book Reviews, Conferences, Meetings, Patents, Symposia
|temporal =1975 to the present  
|geospatial =International 
|number =over 1.5 million 
|updates =Weekly 
|p_title =No print counterparts 
|p_dates = 
|ISSN =
|web =https://scitation.aip.org/jhtml/scitation/coverage.jsp 
|titles =  
}}

'''SPIN''' (Searchable Physics Information Notices) '''bibliographic database''' is an indexing and abstracting service produced by the [[American Institute of Physics]] (AIP). The content focus of SPIN is described as the most significant areas of [[physics]] [[research]]. This type of [[scientific literature|literature coverage]] spans the major [[scientific journal|physical science journals]] and magazines. Major [[conference proceedings]] that are reported by the American Institute of Physics, member societies, as well as affiliated organizations are also included as part of this database. References, or citations, provide access to more than 1.5 million articles as of 2010. ''SPIN''  has no print counterpart.&lt;ref name=DialogSpin/&gt;&lt;ref name=AIP-SPIN/&gt;

==Journals==
Delivery of timely indexing and abstracting is for, what are deemed to be, the significant or important [[physics]] and [[astronomy]] journals from the [[United States]], [[Russia]], and the [[Ukraine]]. Citations for journal articles are derived from original publications of the ''AIP'', which includes published translated works. At the same time, citations are included from member societies, and selectively chosen American journals. Citations become typically available online on the same date as the corresponding journal article.&lt;ref name=DialogSpin/&gt;&lt;ref name=AIP-SPIN&gt; {{Cite web
  | title =What is the SPIN database? 
  | work =Information about SPIN 
  | publisher =[[American Institute of Physics]] 
  | date =July 2010 
  | url =http://scitation.aip.org/servlet/HelpSystem?KEY=SCI&amp;TYPE=HELP/FAQ#ques3 
  | format = 
  | accessdate =2010-07-12}}&lt;/ref&gt;

==Sources==
Overall, the source citations are derived from material published by the AIP and member societies,  which are English-speaking, Russian, and Ukrainian journals and conference proceedings. Certain American physics-related articles are also sources of citations. About 60 journals have cover to cover indexing, and about 100 journals, overall, are indexed.&lt;ref name=DialogSpin/&gt;&lt;ref name=pub-coverage&gt;{{Cite web
  | title =SPIN Publication Coverage 
  | work =Complete list of publications covered and coverage years. 
  | publisher =American Institute of Physics 
  | date =July 2010 
  | url =http://scitation.aip.org/jhtml/scitation/spincodens.jsp 
  | format = 
  | accessdate =2010-07-12}}&lt;/ref&gt;  

==Scope==
Subject coverage encompasses the following: &lt;ref name=DialogSpin&gt;  {{Cite web
  | title =Indexes and Databases 
  | work =SPIN: Searchable Physics Information Notices
  | publisher =Raymond H. Fogler Library, The University of Maine
  | date =October 2010 
  | url =http://www.library.umaine.edu/indexesdb/dbdetails.asp?field=Name&amp;search=SPIN:+Searchable+Physics+Information+Notices 
  | format = 
  | accessdate =2010-07-12}}&lt;/ref&gt;

*[[Applied physics]], [[Electromagnetic spectrum|Electromagnetic]] technology, [[Microelectronics]] 
*[[Atomic physics]] and [[Molecular physics]] 
*[[Biological physics]] and [[Medical physics]] 
*[[Classical physics]] and [[Quantum physics]] 
*[[Condensed matter physics]] 
*[[Elementary particle physics]] 
*[[Physics|General physics]], [[Optics]], [[Acoustics]], and [[Fluid dynamics]] 
*[[Geophysics]], [[Astronomy]], [[Astrophysics]] 
*[[Materials science]] 
*[[Nuclear physics]] 
*[[Plasma physics]] 
*[[Physical chemistry]]

==See also==
*[[List of academic databases and search engines]]

==References==
{{Reflist}}

==External links==
*[http://www.aip.org/press_release/spin.html AIP'S SPIN Database Reaches One Million Records].  American Institute of Physics. March 1, 2002.
*[http://scholarlykitchen.sspnet.org/2009/06/17/physics-papers-and-the-arxiv/ Can everything published in physics can be found in the [[arXiv]]?]. The Scholarly Kitchen. [[Society for Scholarly Publishing]]. June, 2010.
*[http://www.pub4stm.org/ AIP partnerships] (society publishing). July 2010.


[[Category:Bibliographic databases and indexes]]
[[Category:Citation indices]]
[[Category:Scientific databases]]</text>
      <sha1>jyccaikszf3qpu59e9oecjsjl9nmp6d</sha1>
    </revision>
  </page>
  <page>
    <title>Book Citation Index</title>
    <ns>0</ns>
    <id>46862330</id>
    <revision>
      <id>729367224</id>
      <parentid>723222330</parentid>
      <timestamp>2016-07-11T18:41:54Z</timestamp>
      <contributor>
        <username>Headbomb</username>
        <id>1461430</id>
      </contributor>
      <minor />
      <comment>/* Further reading */clean up, use arxiv parameter, remove url redundant with arxiv using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5637" xml:space="preserve">The '''Book Citation Index''' ('''BCI''', '''BKCI''') is an online subscription-based scientific [[citation index]]ing service maintained by [[Thomson Reuters]] and is part of the [[Web of Science|Web of Science Core Collection]].&lt;ref&gt;{{cite book|last1=Campbell|first1=Robert|last2=Pentz|first2=Ed|last3=Borthwick|first3=Ian|title=Academic and Professional Publishing|date=2012|publisher=Chandos Publishing|isbn=9781780633091|pages=247–248|url=https://books.google.com/books?id=IpRlAgAAQBAJ&amp;pg=PA247&amp;dq=%22Book+Citation+Index%22&amp;hl=en&amp;sa=X&amp;ei=Ig9sVZ2QB4rLsASwyYHYBw&amp;ved=0CGoQ6AEwCg#v=onepage&amp;q=%22Book%20Citation%20Index%22&amp;f=false|accessdate=1 June 2015}}&lt;/ref&gt; It was first launched in 2011 and indexes over 60,000 editorially selected books, starting from 2005.&lt;ref name=ATL&gt;{{cite journal|title=Thomson reuters launches Book Citation Index|journal=Advanced Technology Libraries|date=11/01/2011|volume=40|issue=11|page=3|url=http://web.a.ebscohost.com.ezproxy2.library.drexel.edu/ehost/pdfviewer/pdfviewer?sid=3118f8aa-bb72-4992-b82e-be196198670d%40sessionmgr4002&amp;vid=1&amp;hid=4212|accessdate=1 June 2015}}&lt;/ref&gt; Books in the index are electronic and print scholarly texts that contain articles based on [[original research]] and/or reviews of such literature.&lt;ref name=ATL /&gt;

==Content==
The index covers series and non-series books as long as they include full footnotes and the index has two separate editions, a Science edition and a Social Sciences &amp; Humanities edition. The Science edition covers physics and chemistry, engineering, computing and technology, clinical medicine, life sciences, and agriculture and biology. Currently both series only contain books that date back to 2005.&lt;ref&gt;{{cite book|last1=Mann|first1=Thomas|title=The Oxford Guide to Library Research|date=2015|publisher=Oxford University Press|isbn=9780199394463|url=https://books.google.com/books?id=llVLBgAAQBAJ&amp;pg=PT193&amp;dq=%22Book+Citation+Index%22&amp;hl=en&amp;sa=X&amp;ei=Ig9sVZ2QB4rLsASwyYHYBw&amp;ved=0CF4Q6AEwCA#v=onepage&amp;q=%22Book%20Citation%20Index%22&amp;f=false|accessdate=1 June 2015}}&lt;/ref&gt;

==Reception==
In their 2014 book ''Beyond Bibliometrics: Harnessing Multidimensional Indicators of Scholarly Impact'', [[Blaise Cronin]] and Cassidy R. Sugimoto noted that "for impact assessment of book-based fields, bibliometricians need a database with large numbers of books" and that while the Book Citation Index did meet this need, [[Google Books]] also fulfilled this purpose and was not only free, but was (at the time) more comprehensive for bibliometric analyses.&lt;ref&gt;{{cite book|last1=Cronin|first1=Blaise|last2=Sugimoto|first2=Cassidy R.|title=Beyond Bibliometrics: Harnessing Multidimensional Indicators of Scholarly Impact|date=2014|publisher=MIT Press|isbn=9780262323291|pages=33, 289, 296|url=https://books.google.com/books?id=xxSaAwAAQBAJ&amp;pg=PA296&amp;dq=%22Book+Citation+Index%22&amp;hl=en&amp;sa=X&amp;ei=Ig9sVZ2QB4rLsASwyYHYBw&amp;ved=0CE8Q6AEwBQ#v=onepage&amp;q=%22Book%20Citation%20Index%22&amp;f=false|accessdate=1 June 2015}}&lt;/ref&gt; A 2013 article in the ''[[Journal of the American Society for Information Science and Technology]]'' remarked on the index's opportunities and limitations. It stated that the "most significant limitations to this potential application are the high share of publications without address information, the inflation of publication counts, the lack of cumulative citation counts from different hierarchical levels, and inconsistency in citation counts between the cited reference search and the book citation index."&lt;ref name=journal&gt;{{cite journal|last1=Gorraiz|first1=Juan|last2=Purnell|first2=Philip J.|last3=Glänze|first3=Wolfgang|title=Opportunities for and limitations of the Book Citation Index|journal=Journal of the American Society for Information Science and Technology|date=July 2013|volume=64|issue=7|pages=1388–1398|doi=10.1002/asi.22875|url=http://onlinelibrary.wiley.com/doi/10.1002/asi.22875/full|accessdate=1 June 2015}}&lt;/ref&gt; They also stated that the Book Citation Index was "a first step toward creating a reliable and necessary citation data source for monographs — a very challenging issue, because, unlike journals and conference proceedings, books have specific requirements, and several problems emerge not only in the context of subject classification, but also in their role as cited publications and in citing publications."&lt;ref name=journal /&gt;

==Further reading==
*{{cite journal|last1=Torres-Salinas|first1=Daniel|last2=Robinson-Garcia|first2=Nicolas|last3=Miguel Campanario|first3=Juan|last4=Emilio|first4=Delgado López-Cózar|title=Coverage, field specialisation and the impact of scientific publishers indexed in the Book Citation Index|journal=Online Information Review|date=January 2014|volume=38|issue=1|pages=24–42|doi=10.1108/OIR-10-2012-0169|url=http://www.emeraldinsight.com.ezproxy2.library.drexel.edu/doi/full/10.1108/OIR-10-2012-0169|accessdate=1 June 2015}}
*{{cite journal|last1=Torres-Salinas|first1=Daniel|last2=Rodriguez-Sánchez|first2=Rosa|last3=Robinson-Garcia|first3=Nicolas|last4=Fdez-Valdivia|first4=J|last5=García|first5=J.A.|title=Mapping Citation Patterns of Book Chapters in the Book Citation Index|journal=Journal of Informetrics|date=February 2013|volume=7|issue=2|pages=412–424|doi=10.1016/j.joi.2013.01.004|arxiv=1302.5544}}

==References==
{{reflist}}

==External links==
*{{official website|http://wokinfo.com/products_tools/multidisciplinary/bookcitationindex/}}

[[Category:Bibliographic databases and indexes]]
[[Category:Full text scholarly online databases]]
[[Category:Thomson family]]
[[Category:Thomson Reuters]]
[[Category:Citation indices]]
[[Category:Books]]</text>
      <sha1>tb27nwa6hqsbvgju8ab29s5z2ikh288</sha1>
    </revision>
  </page>
  <page>
    <title>Chinese Science Citation Database</title>
    <ns>0</ns>
    <id>47180611</id>
    <revision>
      <id>678867458</id>
      <parentid>678205635</parentid>
      <timestamp>2015-09-01T03:14:21Z</timestamp>
      <contributor>
        <username>Fgnievinski</username>
        <id>6727347</id>
      </contributor>
      <comment>CSCD Journal List</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="815" xml:space="preserve">{{Infobox bibliographic database 
 |title=Chinese Science Citation Database
}}
The '''Chinese Science Citation Database''' (CSCD) is a [[bibliographic database]] and [[citation index]] produced by the [[Chinese Academy of Sciences]].

It is hosted by [[Thomson Reuters]], and it was the first database in its [[Web of Science]] product in a language other than English.&lt;ref&gt;[http://wokinfo.com/products_tools/multidisciplinary/cscd/]&lt;/ref&gt;

==See also==
*[[Chinese Social Sciences Citation Index]]

==References==
{{reflist}}

==External links==
*[http://thomsonreuters.com/content/dam/openweb/documents/pdf/scholarly-scientific-research/methodology/cscd-journal-list.pdf CSCD Journal List]

[[Category:Bibliographic databases and indexes]]
[[Category:Citation indices]]
[[Category:Science and technology in China]]</text>
      <sha1>917guhtk4x47wuwp5gxhxh37kg4ac36</sha1>
    </revision>
  </page>
  <page>
    <title>ScienceOpen</title>
    <ns>0</ns>
    <id>48598824</id>
    <revision>
      <id>757592311</id>
      <parentid>733034837</parentid>
      <timestamp>2016-12-31T15:35:24Z</timestamp>
      <contributor>
        <username>BattyBot</username>
        <id>15996738</id>
      </contributor>
      <minor />
      <comment>/* top */changed {{Notability}} to {{Notability|Organizations}} &amp; [[WP:AWB/GF|general fixes]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11635" xml:space="preserve">{{Multiple issues|
{{notability|Organizations|date=February 2016}}
{{refimprove|date=February 2016}}
}}

{{Infobox organization
| name                = ScienceOpen
| native_name         = 
| native_name_lang    = 
| named_after         = 
| image               =
| image_size          = 300px
| alt                 =  ScienceOpen's logo
| caption             = 
| map                 = 
| map_size            = 
| map_alt             = 
| map_caption         = 
| map2                = 
| map2_size           = 
| map2_alt            = 
| map2_caption        = 
| abbreviation        = 
| motto               = 
| predecessor         = 
| merged              = 
| successor           = 
| formation           = &lt;!-- use {{start date|YYYY|MM|DD|df=y}} --&gt;
| founder             = 
| founding_location   = 
| extinction          = &lt;!-- use {{end date and age|YYYY|MM|DD}} --&gt;
| merger              = 
| type                = 
| tax_id              = &lt;!-- or | vat_id = (for European organizations) --&gt;
| registration_id     = &lt;!-- for non-profit org --&gt;
| status              = 
| purpose             = 
| headquarters        = [[Berlin]], [[Germany]]
| location            = 
| coords              = &lt;!-- {{coord|LAT|LON|display=inline,title}} --&gt;
| region              = 
| services            = 
| products            = 
| methods             = 
| fields              = 
| membership          = 
| membership_year     = 
| language            = 
| owner               = &lt;!-- or | owners = --&gt;
| sec_gen             = 
| leader_title        = 
| leader_name         = 
| leader_title2       = 
| leader_name2        = 
| leader_title3       = 
| leader_name3        = 
| leader_title4       = 
| leader_name4        = 
| board_of_directors  = 
| key_people          = 
| main_organ          = 
| parent_organization = 
| subsidiaries        = 
| secessions          = 
| affiliations        = 
| budget              = 
| budget_year         = 
| revenue             = 
| revenue_year        = 
| disbursements       = 
| expenses            = 
| expenses_year       = 
| endowment           = 
| staff               = 
| staff_year          = 
| volunteers          = 
| volunteers_year     = 
| slogan              = 
| mission             = 
| website             = {{URL|scienceopen.com}}
| remarks             = 
| formerly            = 
| footnotes           = 
}}
'''ScienceOpen''' is a privately owned discovery and research network with three roles:  aggregation, [[Open access|open access publishing]] and the evaluation of scholarly literature in all scholarly disciplines.

== History ==
ScienceOpen began in 2013&lt;ref&gt;{{Cite web|title = OA interviews: Alexander Grossmann, ScienceOpen - Open-access publishing - Research Information|url = http://www.researchinformation.info/features/feature.php?feature_id=473|website = www.researchinformation.info|accessdate = 2015-11-19}}&lt;/ref&gt; when Alexander Grossmann, a professor of Publishing Management at the [[Leipzig University of Applied Sciences]] and former publishing director at scientific house [http://www.degruyter.com/dg/page/15/the-publishing-house De Gruyter], and Tibor Tscheke, president and CEO of content management systems company [http://ovitas.com/ Ovitas], decided to start a platform. Their idea was to allow researchers to share scientific information, both formally by publishing articles and participating in [https://futureofscipub.wordpress.com/open-post-publication-peer-review/ post-publication peer review], and informally by reviewing their colleagues’ work, providing endorsements and comments, and by updating their own papers.

Its beta version was introduced in November 2013, and release 1.0 launched in May 2014.&lt;ref&gt;{{Cite web|title = ScienceOpen: the next wave of Open Access? - EuroScientist Webzine|url = http://www.euroscientist.com/scienceopen-next-wave-open-access|website = EuroScientist Webzine|accessdate = 2015-11-19|language = en-US}}&lt;/ref&gt; As of September 2015 the site has 10 million articles and records&lt;ref&gt;{{Cite web|title = Open and Shut?: The OA Interviews: ScienceOpen’s Alexander Grossmann|url = http://poynder.blogspot.com/2015/11/the-oa-interviews-scienceopens.html|website = Open and Shut?|date = 2015-11-16|accessdate = 2015-11-19|first = Richard|last = Poynder}}&lt;/ref&gt; from [[PubMed Central]], [[ArXiv]], [[PubMed]] and ScienceOpen, and a publicly available citation index which is free for researchers to use wherever they are and is provided at no cost to libraries, which in February 2016 was dubbed the Open Citation Index.&lt;ref&gt;http://blog.scienceopen.com/2016/02/the-open-citation-index/&lt;/ref&gt; All content on the platform is available for post-publication peer review by scientific members with five or more peer-reviewed publications on their [[ORCID]], and all articles can be publicly commented on by members with one or more items.

ScienceOpen appoints members of the research community as Collection Editors&lt;ref&gt;{{Cite web|title = ScienceOpen Collections|url = http://about.scienceopen.com/scienceopen-collections/#more-390|website = About ScienceOpen|accessdate = 2015-11-19|language = en-US}}&lt;/ref&gt; who curate articles from multiple publishers in any topic. [[Thieme Medical Publishers|Thieme]], a German medical publisher, mirrors three open access journals [https://www.scienceopen.com/collection/Thieme on the platform].

The organization is based in Berlin and has a technical office in Boston. It is a member of [[CrossRef]], [[ORCID]], the [[Open Access Scholarly Publishers Association]]&lt;ref&gt;{{Cite web|url=http://oaspa.org/member/scienceopen/|title=Member Record: ScienceOpen|last=|first=|date=|website=|publisher=|access-date=}}&lt;/ref&gt; and the [[Directory of Open Access Journals]]. The company was designated as one of  “10 to Watch” by research advisory firm Outsell in its report “[http://img.en25.com/Web/CopyrightClearanceCenterInc/%7Bfc9f07ac-b2c9-4cd7-b763-2f21e0c6e94b%7D_Outsell_2015_Open_Access_Report.pdf Open Access 2015: Market Size, Share, Forecast, and Trends].”

In 2015, Tscheke provided further clarification of ScienceOpen’s focus on aggregation and filtering content.&lt;ref&gt;{{Cite web|title = There’s more to Open Access than APCs, right? – ScienceOpen Blog|url = http://blog.scienceopen.com/2015/10/theres-more-to-open-access-than-apcs-right/|website = blog.scienceopen.com|accessdate = 2015-11-19}}&lt;/ref&gt;

== Business model ==
ScienceOpen publishes articles of almost any type and from any research field, including the social sciences and humanities. This includes primary research articles, opinion papers, posters, case studies, negative results, and data publications. To fund article publication, ScienceOpen charges a publication fee ($800 as of this time of writing, in 2015) to be paid by the author or the author’s employer, funder or library. This is for a post-publication peer review process and publication after editorial control,&lt;ref&gt;{{Cite web|url=http://blog.scienceopen.com/2016/06/review-instructions-for-scienceopen/|title=Review Instructions for ScienceOpen|last=|first=|date=|website=|publisher=|access-date=27 June 2016}}&lt;/ref&gt; and is facilitated through integration with [[ORCID]].&lt;ref&gt;{{Cite web|url=http://blog.scienceopen.com/2016/06/orcid-integration-at-scienceopen/|title=ORCID integration at ScienceOpen|last=|first=|date=|website=|publisher=|access-date=27 June 2016}}&lt;/ref&gt; Additionally, authors can opt to use the 'Peer Review by Endorsement' model,&lt;ref&gt;{{cite journal |last= Jan Velterop |date= 29 September 2015 |title= Peer review – issues, limitations, and future development |url= https://www.scienceopen.com/document?15&amp;vid=1dcfbe69-c30c-4eaa-a003-948c9700da40 |journal= ScienceOpen Research |doi= 10.14293/S2199-1006.1.SOR-EDU.AYXIPS.v1 |access-date=14 June 2016}}&lt;/ref&gt; in which peer review is conducted prior to submission, and for a fee of $400. Included in this fee are up to two article revisions within 12 months, with full version control. Revised versions have a new DOI so that it is easier to link back to cited versions. A partial or full fee waiver is available to those who demonstrate need. [[Poster session]] publishing is free. All published articles are published via a [[Creative Commons]] Attribution 4.0 International Public License.&lt;ref&gt;{{Cite web|url=http://about.scienceopen.com/open-access-explanation-of-cc-by-license/|title=Open Access License Agreement|last=|first=|date=|website=|publisher=|access-date=}}&lt;/ref&gt;

From 2016, ScienceOpen has started partnering with publishers to offer advanced indexing services. In June 2016 they partnered with [[SciELO]], the largest publisher in Latin America{{Cite web|title = ScienceOpen helps to put scientific research in a global context with more than 15 million article records – ScienceOpen Blog|url = http://blog.scienceopen.com/2016/06/scienceopen-helps-to-put-scientific-research-in-a-global-context-with-more-than-15-million-article-records/|website = blog.scienceopen.com|accessdate = 2016-06-14}}. Additional publishing partners include Higher Education Press&lt;ref&gt;{{Cite web|url=http://blog.scienceopen.com/2016/04/higher-education-press-indexing-partnership-with-scienceopen/|title=Higher Education Press indexing partnership with ScienceOpen|last=|first=|date=|website=|publisher=|access-date=27 June 2016}}&lt;/ref&gt; and The Italian Society of Victimology.&lt;ref&gt;{{Cite web|url=http://blog.scienceopen.com/2016/06/welcome-to-the-italian-society-of-victimology/|title=Welcome to the Italian Society of Victimology|last=|first=|date=|website=|publisher=|access-date=27 June 2016}}&lt;/ref&gt; In September 2015, ScienceOpen hit the 10 million article record mark,&lt;ref&gt;{{Cite web|url=http://www.prnewswire.com/news-releases/scienceopen-hits-the-10-million-article-mark-527671151.html|title=ScienceOpen Hits the 10 Million Article Mark|last=|first=|date=|website=|publisher=|access-date=27 June 2016}}&lt;/ref&gt; and as of 27 June 2016 has more than 13 million records.

Every research article on ScienceOpen has a traceable genealogy through citations, a public peer review process, and social interaction tracked by [[altmetrics]], which they call research "context".&lt;ref&gt;{{Cite web|url=http://blog.scienceopen.com/2016/05/why-context-is-important-for-research/|title=Why ‘context’ is important for research|last=|first=|date=|website=|publisher=|access-date=27 June 2016}}&lt;/ref&gt; The technology behind the ScienceOpen platform is provided by Ovitas.

ScienceOpen are findable on [https://www.youtube.com/user/ScienceOpen YouTube], [https://www.linkedin.com/company/scienceopen-inc- LinkedIn], [https://www.facebook.com/scienceopen/ Facebook], and [https://twitter.com/science_open Twitter].

== Publications ==
* [https://www.scienceopen.com/collection/scienceopen_research ScienceOpen Research], ISSN [https://www.worldcat.org/search?q=n2%3A2199-1006&amp;qt=results_page 2199-1006]
* [https://www.scienceopen.com/collection/scienceopen_posters ScienceOpen Posters], ISSN [https://www.worldcat.org/search?q=n2%3A2199-8442&amp;qt=results_page 2199-8442]

== Headquarters ==
ScienceOpen has its headquarters located at Pappelallee 78-79, 10437 Berlin and its technical hub is at 60 Mall Rd., Burlington, Mass.

== See also ==
* [[Open Access Scholarly Publishers Association]] (OASPA)
* [[Directory of Open Access Journals]] (DOAJ)
* [[Registry of Open Access Repositories Mandatory Archiving Policies]] (ROARMAP)

== References ==
{{Reflist}}

== External links ==
* {{official website|http://www.scienceopen.com}}

[[Category:Open access publishers]]
[[Category:Open access (publishing)]]
[[Category:Scholarly communication]]
[[Category:Citation indices]]</text>
      <sha1>m90rf9cano1dutysci467eyu29raiyu</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Telephone directory publishing companies</title>
    <ns>14</ns>
    <id>5154927</id>
    <revision>
      <id>653601083</id>
      <parentid>382986830</parentid>
      <timestamp>2015-03-26T12:56:53Z</timestamp>
      <contributor>
        <username>Stefanomione</username>
        <id>186638</id>
      </contributor>
      <comment>refine category</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="142" xml:space="preserve">[[Category:Publishing companies by medium]]
[[Category:Directories]]
[[Category:Directory assistance services]]
[[Category:Telephone numbers]]</text>
      <sha1>a6yk1509m9z06tqy1kqkok2zjy6m94j</sha1>
    </revision>
  </page>
  <page>
    <title>Thomas Register</title>
    <ns>0</ns>
    <id>4755498</id>
    <revision>
      <id>747139425</id>
      <parentid>746816027</parentid>
      <timestamp>2016-10-31T17:42:04Z</timestamp>
      <contributor>
        <username>Natg 19</username>
        <id>3492060</id>
      </contributor>
      <minor />
      <comment>Disambiguating links to [[CAD]] (link changed to [[Computer-aided design]]) using [[User:Qwertyytrewqqwerty/DisamAssist|DisamAssist]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4166" xml:space="preserve">[[Image:ThomasRegister.png|frame|1905 ''Thomas' Register of American Manufacturers'']]
The '''''Thomas Register of American Manufacturers''''', now '''ThomasNet''', is an online platform for supplier discovery and product sourcing in the USA and Canada. It was once known  as the "big green books" and "Thomas Registry", and was a multi-volume [[Yellow Pages|directory]] of [[Industry|industrial]] product information covering 650,000 [[distributors]], [[manufacturers]] and service companies within 67,000-plus [[industry|industrial]] categories that is now published on ThomasNet.

==History==
The books were first published in 1898 by Harvey Mark Thomas as ''Hardware and Kindred Trades. ''In their heyday, '''''Thomas Register of American Manufacturers ''''' was a 34-volume, 3 section buying guide offering sourcing information on industrial products and services, along with comprehensive specifications and detailed product information from thousands of manufacturers. The Thomas Regional Directory Company began as a division of Thomas Publishing in 1976. Thomas Regional Regional Industrial Buying Guides provided information in print and on CD-ROM,  on local OEMs, distributors, MRO services and other custom manufacturing services in 19 regional editions covering much of the United States. Thomas Register and Thomas Regional were available online from the mid 1990s. The company stopped publishing its print products in 2006.

Thomas has moved its database [[online]] as ThomasNet, published and maintained by Thomas Industrial Network, one of Thomas’ five business units. ThomasNet has expanded to provide not only product and company information, but also [[Online shopping|online catalog]]s, [[computer-aided design]] ([[Computer-aided design|CAD]]) drawings, [[news]], [[press releases]] and [[blogs]].

==Thomas Publishing Company, LLC==

Thomas Publishing Company, LLC of [[New York City]] has been [[privately held]] since its inception. It used independent representatives to sell advertising space around its listings in print products like the Thomas Register and the Thomas Industrial Regional Directories, and these representatives continue to sell Internet related products to manufacturers, distributors, and other companies.

==ThomasNet==

ThomasNet is an information and technology company based in New York City. In April 2006 the [[New York Public Library]] named ThomasNet.com as one of its [http://www.nypl.org/branch/books/index2.cfm?ListID=300 25 Best of Reference] sources for the [[reference librarian]], and is currently listed in their [http://www.nypl.org/weblinks/1382 Best of the Web] list for Industry Information.

Since November 2010, ThomasNet has been a founding partner of GlobalTrade.net, a marketplace for international trade service providers.

==ThomasNet News==
ThomasNet News is a product of Thomas Publishing Company, LLC. ThomasNet News was introduced with “the mission of delivering timely, new industrial product information covering the whole range of products …” It manually reviews press releases submitted through the website and publishes with a small description in one of 51 different categories.

In 2000, ThomasNet News released Industry Market Trends (IMT), its first Journal. In the IMT, editors published editorials, interviews, and long form journalism on issues ranging from career skills, developments in the industry, and discussions with leading experts. Soon after, IMT Green &amp; Clean was launched in response to the growing interest in green technology and its impact on the world. In 2011, the IMT Machining Journal was launched followed by the IMT Fluid &amp; Gas Flow Journal, the IMT Career Journal, and the IMT Procurement Journal.

==Research==
Starting in 2010, ThomasNet began reaching out to its database of manufacturers to get a better understanding of where the community was, where their shortcomings were, and where they saw the landscape going in the future. This yearly survey is called the Industry Market Barometer.

== External links ==
* {{Official website}}

[[Category:Promotion and marketing communications]]
[[Category:Marketing books]]
[[Category:Directories]]</text>
      <sha1>c4ijqvmsho8dbo2llhgmqf2x87dw2zk</sha1>
    </revision>
  </page>
  <page>
    <title>Vsya Moskva</title>
    <ns>0</ns>
    <id>11017700</id>
    <revision>
      <id>736928670</id>
      <parentid>710445841</parentid>
      <timestamp>2016-08-30T18:57:42Z</timestamp>
      <contributor>
        <username>Iridescent</username>
        <id>937705</id>
      </contributor>
      <minor />
      <comment>/* top */[[WP:AWB/T|Typo fixing]], [[WP:AWB/T|typo(s) fixed]]: between 500 to → between 500 and using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3782" xml:space="preserve">{{italic title}}
{{unreferenced|date=December 2008}}
'''''Vsya Moskva''''' (literally translated "''All Moscow''" or "''The Entire Moscow''") was a series of [[city directories]] of [[Moscow]], Russia, published on a yearly basis from 1872 to 1936 by [[Aleksei Sergeevich Suvorin]]. 
The directories contained detailed lists of private residents, names of streets and squares across the city with the details of their occupants and owners, government offices,  public services and medium and large businesses present in the city. Each volume was anywhere between 500 and 1500 pages long.  They are often used by [[genealogists]] for family research in pre-revolutionary Russia and the early [[Soviet Union|Soviet]] period when [[vital records]] are missing or prove difficult to find. [[Historian]]s use them to research the [[social histories]] of the city.

==List of residents==
Each directory was written exclusively in Russian Cyrillic only, and contains various sections among which was an [[alphabetical]] list of residents in the city. Those listed usually were the head of their respective household and so spouses and minors are not listed. 

The following information can be found:
*Person's surname and first name
*[[Patronymic]]
*Street address with apartment number
*[[Profession]]
*Telephone numbers (few private residents could afford a [[telephone]] before 1918)

==List of occupants of each building on every street and square==
A section immediately preceding or following that listing residents in alphabetical order was a directory of all streets, houses and flats with the names of their owners and occupants. In this way readers could determine all those people who lived on a particular street of in a certain apartment block.

==Other sections==
The following information can also be found in each directory:

*Maps of the city
*Interior theater seating plan layouts
*Lists of personnel in state, public and private institutions
*Original Advertising

== Interruption in the series ==

No volumes were published in the following years:
*1918
*1919
*1920
*1921

This was due to the events of the [[Russian revolution of 1917]] and the subsequent [[Russian civil war]].

== Termination of series ==

Publication came to a halt after the edition of 1936, coinciding with the time of [[Joseph Stalin]]'s [[great purge]]s and [[Moscow Trials]].

==Historical and genealogical value==
Because numerous residents emigrated from Moscow after the [[Russian Revolution of 1917]] and tens of thousands more were either arrested, shot, or sent to the [[gulag]] by the [[Cheka]] and the [[NKVD]] after 1918 the section detailing residents names is especially useful in determining until when a certain person was still living in the city, and under which address.

==Availability==
Many original directories in the series (or [[microfiche]] copies thereof) can be found in libraries across the [[United States]], [[Europe]] (including [[The Baltics]], [[Finland]] the [[United Kingdom]] and [[Germany]]) however most only have an incomplete collection.

==Other city directories in Russia==
Suvorin also published city directories for [[Saint Petersburg]] under the title ''[[Ves Petersburg]]'' (''All Petersburg'') for the years 1894 to 1940 and for the whole country under the titles ''[[Vsya Rossiya]]'' (''All Russia'') from 1895 to 1923 and continued under than name ''[[Ves SSSR]]'' (''All USSR'') from 1924 to 1931.

== See also ==

*''[[Ves Petersburg]]''
*''[[Vsya Rossiya]]''

== External links ==
*[http://surname.litera-ru.ru/ A Russian website offering a search engine in Cyrillic for some city directories.]

[[Category:Directories]]
[[Category:History of Moscow]]
[[Category:Russian non-fiction books]]
[[Category:Media in Moscow]]
[[Category:1872 books]]</text>
      <sha1>t82br6q6jpd9qhmgwcpyd3telkoelj0</sha1>
    </revision>
  </page>
  <page>
    <title>Gallia Christiana</title>
    <ns>0</ns>
    <id>13968535</id>
    <revision>
      <id>541072231</id>
      <parentid>512863654</parentid>
      <timestamp>2013-02-27T23:07:47Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor />
      <comment>[[User:Addbot|Bot:]] Migrating 5 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q748022]] ([[User talk:Addbot|Report Errors]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8073" xml:space="preserve">The '''''Gallia Christiana''''', a type of work of which there have been several editions, is a documentary catalogue or list, with brief historical notices, of all the Catholic dioceses and abbeys of France from the earliest times, also of their occupants.

== First efforts ==

In 1621 [[Jean Chenu]], an ''[[avocat]]'' at the [[Parlement of Paris]], published ''Archiepiscoporum et episcoporum Galliæ chronologica historia''. Nearly a third of the bishops are missing, and the episcopal succession as given by Chenu was very incomplete. In 1626, Claude Robert, a priest of [[Langres]], published with the approbation of [[Baronius]], a ''Gallia Christiana''. He entered a large number of churches outside of [[Gaul]], and gave a short history of the [[metropolitan see]]s, cathedrals, and abbeys.

== The Samarthani ==

Two brothers de Sainte-Marthe, Scévole (1571–1650) and Louis (1571–1656), appointed royal historiographers of France in 1620, had assisted Chenu and Robert. At the [[assembly of the French Clergy]] in 1626, a number of prelates commissioned these brothers to compile a more definitive work. They died before the completion of their work, and it was issued in 1656 by the sons of [[Scévole de Sainte-Marthe]], [[Pierre de Sainte-Marthe]] (1618–90), himself historiographer of France, [[Abel de Sainte-Marthe]] (1620–71), theologian, and later general of the [[Oratory (worship)|Oratory]], and [[Nicolas-Charles de Sainte-Marthe]] (1623–62), prior of [[Claunay]]. On 13 September 1656, the Sainte-Marthe brothers were presented to the assembly of the French Clergy, who accepted the dedication of the work on condition that a passage suspected of [[Jansenism]] be suppressed. The work formed four volumes [[in folio]], the first for the [[archdiocese]]s, the second and third for the dioceses, and the fourth for the abbeys, all in alphabetical order.&lt;ref&gt;The title was ''Gallia Christiana, qua series omnia archiepiscoporum, episcoporum et abbatum Franciæ vicinarumque ditionum ab origine ecclesiarum ad nostra tempora per quattor tomos deducitur, et probator ex antiquæ fidei manuscriptis Vaticani, regnum, principum tabulariis omnium Galliæ cathedralium et abbatarium''.&lt;/ref&gt; It reproduced a large number of manuscripts. Defects and omissions, however, were obvious. The Sainte-Marthe brothers themselves announced in their preface the early appearance of a second edition corrected and enlarged. 

As early as 1660 the Jesuit [[Jean Colomb]] published at Lyons the ''Noctes Blancalandanæ'', which contains certain additions to the work of the Samarthani, as the brothers and their successors are often called. "The name of Sainte-Marthe", wrote Voltaire, "is one of those of which the country has most reason to be proud." The edition promised by the Sainte-Marthe brothers did not appear.

== Revision by the Maurists ==

In 1710 the Assembly of the French Clergy offered four thousand livres to [[Denys de Sainte-Marthe]] (1650–1725), a Benedictine monk of the [[Congregation of Saint-Maur]], renowned for his polemics against the Trappist [[Abbé de Rancé]] on the subject of monastic studies, on condition that he should bring the revision of the ''Gallia Christiana'' to a successful conclusion, that the first volume should appear at the end of four years, and that his Congregation should continue the undertaking after his death. Through his efforts the first volume appeared in 1715, devoted to the ecclesiastical provinces of Albi, Aix, Arles, Avignon, and Auch. In 1720 he produced the second volume dealing with the provinces of Bourges and Bordeaux; and in 1725 the third, which treated Cambrai, Cologne, and Embrun. 

After his death the Benedictines issued the fourth volume (1728) on Lyons, and the fifth volume (1731) on Mechelen and Mainz. Between 1731 and 1740, on account of the controversies over the Bull ''[[Unigenitus]]'', Dom [[Félix Hodin]] and Dom [[Etienne Brice]], who were preparing the latter volumes of the ''Gallia Christiana'', were expelled from [[Saint-Germain-des-Prés]]. They returned to Paris in 1739 and issued the sixth volume, dealing with Narbonne, also (1744) the seventh and eighth volumes on Paris and its [[suffragan see]]s. [[Père Duplessis]] united his efforts with theirs, and the ninth and tenth volumes, both on the [[province of Reims]], appeared in 1751. The eleventh volume (1759) dealing with the [[province of Rouen]] was issued by Père [[Pierre Henri]] and Dom [[Jacques Taschereau]]. In 1770 the twelfth volume on the [[province of Sens]] and [[province of Tarentaise]] appeared, and in 1785 the thirteenth, on the provinces of Toulouse and Trier. 

At the outbreak of the revolution, four volumes were lacking: Tours, Besançon, Utrecht, and Vienne. Barthélemy Hauréau published (in 1856, 1860 and 1865), for the provinces of Tours, Besançon and Vienne, respectively, and according to the Benedictine method, the fourteenth, fifteenth and sixteenth volumes of the ''Gallia Christiana''. 

The province of Utrecht alone has no place in this great collection, but this defect has been remedied in part by the ''Bullarium Trajectense'', edited by [[Gisbert Brom]], and extending from the earliest times to 1378 (The Hague, 1891–96). 

The new ''Gallia Christiana'', of which volumes I to V and XI to XIII were reprinted by Dom [[Paul Piolin]] between 1870 and 1877, and volumes VI to IX and XII by the publisher H. Welter, places after each metropolitan see its suffragan sees, and after each see the abbeys belonging to it. The original documents, instead of encumbering the body of the articles, are inserted at the end of each diocese under in a section titled ''Instrumenta''. This colossal work does great honour to the Benedictines and to the Sainte-Marthe family. "The name of Sainte-Marthe", wrote Voltaire, "is one of those of which the country has most reason to be proud."

== Later works ==

In 1774 the Abbé [[Hugues du Temps]], vicar-general of Bordeaux, undertook in seven volumes an abridgement of the ''Gallia'' under the title "Le clergé de France" of which only four volumes appeared. About 1867 [[Honoré Fisquet]] undertook the publication of an episcopal history of France ([http://gallica.bnf.fr/Catalogue/noticesInd/FRBNF34044240.htm]''La France Pontificale''), in which, for the early period, he utilized the ''Gallia'', at the same time bringing the history of each diocese down to modern times. Twenty-two volumes appeared. 

[[Canon Albanès]] projected a complete revision of the ''Gallia Christiana'', each ecclesiastical province to form a volume. Albanès, who was one of the first scholars to search the Lateran and Vatican libraries, in his efforts to determine the initial years of some episcopal reigns, found occasionally either the acts of election or the Bulls of provision. He hoped in this way to remove certain suppositious bishops who had been introduced to fill gaps in the catalogues, but died in 1897 before the first volume appeared. Through the use of his notes and the efforts of Canon [[Ulysse Chevalier]] three addition volumes of this "Gallia Christiana (novissima)", treating Arles, Aix, and Marseilles, appeared at Montbéliard.

== See also ==
* [[Jean-Barthélemy Hauréau]]

== References ==

&lt;references/&gt;
* [[Dreux du Radier]], ''Bibliothèque historique et critique du Poitou'' (Paris, 1754)
* ''Gallia Christiana'', Vol. IV, Préface
* ''Gallia Christiana (novissima)'' (Montbéliard, 1899), Préface to the Aix volume
* [[de Longuemare]], ''Une famille d'auteurs aux seizième, dix-septième et dix-huitième siècles; les Sainte-Marthe'' (Paris, 1902)
* Victor Fouque, ''Du "Gallia christiana" et de ses auteurs: étude bibliographique'', Paris: E. Tross, 1857. Available on the Bibliothèque nationale's [http://gallica.bnf.fr/Catalogue/noticesInd/FRBNF30453708.htm ''Gallica''] site.

== External links ==
* {{CathEncy|url=http://www.newadvent.org/cathen/06350c.htm|title=Gallia Christiana}}

{{Catholic|wstitle=Gallia Christiana}}

[[Category:Directories]]
[[Category:Religious studies books]]</text>
      <sha1>tnjjbimhvrx7697xu1g90eqtmmx3r0s</sha1>
    </revision>
  </page>
  <page>
    <title>American Art Directory</title>
    <ns>0</ns>
    <id>21099152</id>
    <revision>
      <id>743781910</id>
      <parentid>740762621</parentid>
      <timestamp>2016-10-11T06:56:16Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 1 sources and tagging 0 as dead. #IABot (v1.2.4)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4042" xml:space="preserve">{{Infobox Magazine
| title           = American Art Directory
| image_file      = American Art Directory logo, 1898.svg
| image_size      = 140px
| image_caption   = ''Frontispiece from 1898 volume''
| editor          = 
| editor_title    = 
| previous_editor = 
| staff_writer    = 
| frequency       = [[Annual publication|Annual]]
| circulation     = 
| category        = 
| company         = 
| publisher       = 
| firstdate       = 1898
| country         = 
| based           = 
| language        = [[English language|English]]
| website         = http://www.americanartdir.com/
| issn            = 0065-6968
}}

The '''''American Art Directory''''' is a yearly publication covering [[art museum]]s, [[Arts centre|arts centers]], and [[Art school|art educational institutions]] as well as news, obituaries, book and magazine publications, etc. related to the artistic community in the [[United States]].  Established in 1898, it was originally entitled ''American Art Annual''.

Art consultant, advisor, author, and independent appraiser [[Alan Bamberger]] describes the Directory as "...a required reference for art museums, libraries, arts organizations, art schools, and corporations with art holdings."&lt;ref&gt;{{citation
 |first1=Alan S. 
 |last1=Bamberger 
 |title=Who's Who In American Art, Official Museum Directory, American Art Directory 
 |publisher=ArtBusiness.com 
 |accessdate=2009-01-14 
 |url=http://www.artbusiness.com/revs0608.html 
 |deadurl=bot: unknown 
 |archiveurl=http://www.webcitation.org/5dphZr5S6?url=http%3A%2F%2Fwww.artbusiness.com%2Frevs0608.html 
 |archivedate=2009-01-15 
 |df= 
}}, archived by [[WebCite]] &lt;/ref&gt;

A yearly feature is the "Review of the Year" article discussing the touring exhibitions, commissions, grants to organizations, construction starts at museums and other facilities, and various other events that occur within the art community.

Initially the directory was the work of the [[New York City|New York]] area artist [[Florence Nightingale Levy]] and published by [[Macmillan Publishers|The Macmillan Company]].&lt;ref name="NYTObit"&gt;{{Citation
  | title = NOTES AND NEWS.; Items Gathered During This Week's Tour of the Publishing Houses.
  | newspaper = [[The New York Times]]
  | date = April 1, 1899
  | url = http://query.nytimes.com/gst/abstract.html?res=9407E1DF1538E433A25752C0A9629C94689ED7CF}}&lt;/ref&gt;  The [[American Federation of Arts]], with which Mrs. Levy was associated and which she would later become the president of, was founded in 1909&lt;ref&gt;{{citation
  | title = About the AFA
  | publisher = [[American Federation of Arts]]
  | accessdate = 2009-01-14
  | url = http://www.afaweb.org/about/}}&lt;/ref&gt; and in 1913 the directory became an official publication of that organization.&lt;ref name="Torchbearers"&gt;{{citation
  | first1 = Karen J.
  | last1 = Blair
  | authorlink1 = Karen J. Blair
  | title = The Torchbearers: Women and Their Amateur Arts Associations in America, 1890-1930
  | publisher = [[Indiana University Press]]
  | location = [[Bloomington, Indiana|Bloomington]]
  | year = 1994
  | isbn = 978-0-253-31192-4
  | page = 80
  | oclc = 27677514
  | url = https://books.google.com/books?id=wP5pq2aBYBAC&amp;printsec=frontcover#PPA80,M1}}&lt;/ref&gt;  It later became the independent publication it exists as currently.

In 1952 the ''American Art Annual'' was split into two separate publications, ''[[Who's Who in American Art]]'' and the ''American Art Directory''.&lt;ref&gt;{{citation
  | title = Who’s Who in American Art
  | publisher = [[R. R. Bowker]]
  | location = [[New York City|New York]]
  | year = 1953
  | issn = 0000-0191}}&lt;/ref&gt;

==References==
{{Reflist}}
&lt;div style="text-align:center;height:3em;"&gt;&amp;#160;&lt;/div&gt;

==External links==
* [http://www.americanartdir.com/ Official website]

[[Category:Books about visual art]]
[[Category:Annual magazines]]
[[Category:Magazines established in 1898]]
[[Category:Directories]]
[[Category:Arts in the United States]]
[[Category:American arts magazines]]


{{art-mag-stub}}
{{US-arts-org-stub}}
{{art-book-stub}}</text>
      <sha1>julw8nnntiu5p0to5shwga9og65e70c</sha1>
    </revision>
  </page>
  <page>
    <title>Artists' Bluebook</title>
    <ns>0</ns>
    <id>19928728</id>
    <revision>
      <id>659247876</id>
      <parentid>578214597</parentid>
      <timestamp>2015-04-26T05:37:15Z</timestamp>
      <contributor>
        <username>GoodDay</username>
        <id>589223</id>
      </contributor>
      <comment>per [[WP:BOLDTITLE]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="706" xml:space="preserve">{{primary sources|date=October 2013}}
The '''Artists' Bluebook''' is an international [[database]] of over 270,000 visual artists developed by AskART since 1999 (http://www.askart.com/AskART/help/AskART_about_us.aspx). Revised from its original 1993 print and CD format to digital online access, the Artists' Bluebook is considered a favorite resource for research into artists' lives, artworks and values, and where to buy or sell.

==External links==
* [http://www.askart.com/AskART/index.aspx The Artists' Bluebook website]
* [http://www.ala.org/rusa/sections/mars/marspubs/marsbestfreewebsites/marsbestref2003 AskART Bluebook 2003 - review by American Library Association(ALA)]
[[Category:Directories]]</text>
      <sha1>suuag61hctndf2cdhsnpqsbobs5xu3h</sha1>
    </revision>
  </page>
  <page>
    <title>Reverse telephone directory</title>
    <ns>0</ns>
    <id>5279920</id>
    <revision>
      <id>755512943</id>
      <parentid>743175336</parentid>
      <timestamp>2016-12-18T14:16:57Z</timestamp>
      <contributor>
        <username>Rjwilmsi</username>
        <id>203434</id>
      </contributor>
      <minor />
      <comment>/* United States */Journal cites, added 1 DOI using [[Project:AWB|AWB]] (12130)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6844" xml:space="preserve">A '''reverse telephone directory''' (also known as a '''gray pages''' directory, criss-cross directory or '''reverse phone lookup''') is a collection of telephone numbers and associated customer details. However, unlike a standard [[telephone directory]], where the user uses customer's details (such as name and address) in order to retrieve the telephone number of that person or business, a reverse telephone directory allows users to search by a telephone service number in order to retrieve the customer details for that service.

Reverse telephone directories are used by law enforcement and other emergency services in order to determine the origin of any request for assistance, however these systems include both publicly accessible (listed) and private (unlisted) services. As such, these directories are restricted to internal use only. Some forms of [[city directory|city directories]] provide this form of lookup for listed services by phone number, along with address cross-referencing.

Publicly accessible reverse telephone directories may be provided as part of the standard directory services from the telecommunications carrier in some countries. In other countries these directories are often created by [[phreaking|phone phreaker]]s by collecting the information available via the publicly accessible directories and then providing a search function which allows users to search by the telephone service details.

==History==
Printed reverse phone directories have been produced by the telephone companies (in the United States) for decades, and were distributed to the phone companies, law enforcement, and [[public library|public libraries]].&lt;ref&gt;{{cite news | url=https://news.google.com/newspapers?nid=1454&amp;dat=19720102&amp;id=87osAAAAIBAJ&amp;sjid=vgkEAAAAIBAJ&amp;pg=3122,379459 | title=Clinton Directory Issued | date=Jan 2, 1972 | accessdate=9 February 2014 | location=Page 16}}&lt;/ref&gt; In the early 1990s, businesses started offering reverse telephone lookups for fees, and by the early 2000s advertising-based reverse directories were available online, prompting occasional alarms about privacy concerns.

==Australia==
In 2001, a legal case ''[[Telstra|Telstra Corporation Ltd]] v Desktop Marketing Systems Pty Ltd'' was heard in the Australian Federal Court.&lt;ref&gt;{{cite web|url=http://www.austlii.edu.au/au/cases/cth/federal_ct/2001/612.html|title=Telstra Corporation Limited v Desktop Marketing Systems Pty Ltd (2001) FCA 612 (25 May 2001)|author=[[Federal Court of Australia]]|publisher=Australasian Legal Information Institute|accessdate=2008-01-03}}&lt;/ref&gt;&lt;ref name=austliiPP&gt;{{cite web|url=http://www.austlii.edu.au/au/journals/PLPR/2001/25.html|title=Private parts - PLPR 25; (2001) 8 PLPR 24|publisher=Australasian Legal Information Institute|accessdate=2008-01-03}}&lt;/ref&gt; gave Telstra, the predominant carrier within Australia and the maintainer of the publicly accessible [[White Pages]] (residential) and [[Yellow Pages]] (commercial) directories, [[copyright]] over the content of these directories.

In February 2010 a Federal Court of Australia case ''[[Telstra|Telstra Corporation Ltd]] v Phone Directories Company Pty Ltd'' determined that Telstra does not hold copyright in the White Pages or the Yellow Pages.&lt;ref&gt;{{cite news|url=http://www.smh.com.au/business/copyright-to-enter-a-new-dimension-20101215-18y9o.html|title=Copyright to enter a new dimension|newspaper=[[The Sydney Morning Herald]]| first=Malcolm|last=Maiden|date=16 December 2010|accessdate=20 December 2012}}&lt;/ref&gt;

As it currently{{when|date=October 2014}} stands there is no legal way to ensure a particular number is not listed in the directories currently available.

==United States==

In United States, landline phone subscribers can pay a small fee to exclude their number from the directory. This service is usually called "Your Listing Not Published" and the cost ranges between $0.80 and $1.50 for residential customers.

As [[cellular phones]] become more popular, there has been debate about releasing cell phone numbers into public [[4-1-1|411]] and reverse number directories. (S. 1963, the "Wireless 411 Privacy Act" 9/2004). However, opposition led by leading consumer-protection organization [[Consumers Union]] presented several privacy concerns in their congressional [http://www.consumersunion.org/pub/wireless%20411%20senate%20testimony%20final.pdf testimony]. Right now,{{when|date=October 2014}} cell phone numbers are not available in any public 411 or reverse-number directories. However, several information companies provide reverse cell phone lookups that are obtained from utility resources, and are available online. Because there is no central database of cell phone numbers, reverse phone directories that claim to be free cannot return information on those numbers.&lt;ref&gt;{{cite journal | title=Evaluating the utility and accuracy of a reverse telephone directory to identify the location of survey respondents. | work=2005 Feb |vauthors=Schootman M, Jeffe D, Kinman E, Higgs G, Jackson-Thompson J | pmid=15652722 | doi=10.1016/j.annepidem.2004.06.005 | volume=15 | pages=160-6}}&lt;/ref&gt;

In recent years{{when|date=October 2014}} community web based services offer a reverse telephone directory of known telemarketers, debt collectors, fund raisers, and other solicitors which contact consumers by telephone.  Users of these services can perform a search of the telephone number which showed up on their caller ID and read through user comments to find the identity of the calling company or individual.

==United Kingdom==
In the United Kingdom proper, reverse directory information is not publicly available.&lt;ref&gt;{{cite web | url=http://ico.org.uk/for_organisations/privacy_and_electronic_communications/the_guide/directories_of_subscribers | title=Directories of subscribers | publisher=Information Commissioner's Office | accessdate=9 February 2014}}&lt;/ref&gt; However, in the [[Channel Islands]] it is provided in the printed telephone directories.

Although the information is, of necessity, available to emergency services, for other agencies it is treated as 'communication data' in the [[RIPA]] regime and subject to the same controls as requests for lists of and content of calls.

==References==
{{reflist}}

==External links==
&lt;!-- Do not delete these comments. --&gt;
&lt;!-- Do not put commercial links into this list. Doing so can get you blocked with no further warning. --&gt; 
*[https://web.archive.org/web/20010721175437/http://blackpages.2600.org.au/ Wayback Machine (21 July 2001) archive of http://blackpages.2600.org.au]
*[http://www.austlii.edu.au/au/cases/cth/federal_ct/2001/612.html Federal Court of Australia Case 612 (25 May 2001): Telstra Corporation Limited v Desktop Marketing Systems Pty Ltd]

[[Category:Telephone numbers]]
[[Category:Directories]]
[[Category:Information retrieval systems]]</text>
      <sha1>n7qp47ly0f5rqr5az9kaon4az2ljw38</sha1>
    </revision>
  </page>
  <page>
    <title>EADP</title>
    <ns>0</ns>
    <id>23508467</id>
    <revision>
      <id>725522998</id>
      <parentid>725522839</parentid>
      <timestamp>2016-06-16T05:36:01Z</timestamp>
      <contributor>
        <username>JoshMuirWikipedia</username>
        <id>25843868</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1256" xml:space="preserve">{{refimprove|date=August 2014}}

The '''European Association of Directory and Database Publishers''', known as '''EADP''', was founded in 1966. EADP is the key representative for the [[Europe|European]] directory and database publishing sector. As such, EADP has 180 members from 36 countries and represents the interests of some 340 [[Telephone directory|directory]] publishers. The associations members and affiliate members include [[Publishing|publishers]] and stakeholders from the industry such as suppliers and vendors.&lt;ref&gt;{{cite web|title=About EADP|url=http://www.eadp.org/index.php?q=aboutus|accessdate=25 July 2013}}&lt;/ref&gt;

EADP's activities include:

* Maintaining an up-to-date member directory
* Facilitating an annual congress and a separate annual conference
* Monitoring EU legal activities of relevance to the industry
* Compiling an annual statistical report and benchmarking studies

The [[North America|North American]] counterpart to the EADP is the [[Yellow Pages Association]] (YPA).

== References ==
&lt;references /&gt;

==External links==
*[http://www.ypassociation.org/ YPA web-site]
*[http://www.eadp.org/ EADP web-site]

{{DEFAULTSORT:Eadp}}
[[Category:Companies established in 1966]]
[[Category:Directories]]


{{telephony-stub}}</text>
      <sha1>hek52me4ie3hpprqz8b07z1rnpa09er</sha1>
    </revision>
  </page>
  <page>
    <title>Boston Directory</title>
    <ns>0</ns>
    <id>25348863</id>
    <revision>
      <id>743306723</id>
      <parentid>743305496</parentid>
      <timestamp>2016-10-09T01:46:49Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor />
      <comment>Robot - Moving category 20th century in Boston, Massachusetts to [[:Category:20th century in Boston]] per [[WP:CFD|CFD]] at [[Wikipedia:Categories for discussion/Log/2016 September 6]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="28517" xml:space="preserve">{{italic title}}
[[Image:1807 BostonDirectory title page.png|100px|thumb|1807 Boston Directory [[title page]]]]
'''''The Boston Directory''''' of  [[Boston]], [[Massachusetts]], was first published in 1789. It contained "a list of the merchants, mechanics, traders, and others, of the town of Boston; in order to enable strangers to find the residence of any person." Also included were listings for public officials, doctors, bank directors, and firemen.&lt;ref&gt;Boston Directory. 1789.&lt;/ref&gt; The directory was issued annually after 1825; previously it had appeared irregularly.

The number of listings in each directory reflected fluctuations in the population size of Boston. In 1789, the directory included some 1,474 listings; by 1875, there were 126,769.&lt;ref name="auto"&gt;Advertisement for Boston Directory. Boston Almanac, 1875.&lt;/ref&gt;

Publishers included John Norman (1789); John West (1796-1803); Edward Cotton (1805-1818); Charles Stimpson (1820-1846); George Adams (1846-1857);&lt;ref&gt;{{citation |url=https://books.google.com/books?id=Ors-AAAAYAAJ&amp;pg=PA87 |year=1866 |title=New England Historical &amp; Genealogical Register }}&lt;/ref&gt; Adams, Sampson &amp; Co. (1858-1865); Sampson, Davenport &amp; Co. (1865-1884); Sampson, Murdock &amp; Co. (1885-1903); Sampson &amp; Murdock Co. (1904-ca.1930); [[R.L. Polk &amp; Co.]] (1944-ca.1980).&lt;ref name="auto"/&gt;&lt;ref&gt;{{cite web|url=http://www.worldcat.org/oclc/228685309|title=The Boston directory ... including all localities within the city limits, as Allston, Brighton, Charlestown, Dorchester, Hyde Park, Roslindale, Roxbury, West Roxbury ...|work=worldcat.org}}&lt;/ref&gt;
{{TOC right}}

==Boston Directories==

===18th century===
{| class="wikitable" style="font-size: 95%;"
!Title||Publisher||Year||Google Books||[[HathiTrust]]||Internet Archive||Other
|-
| Boston Directory
| &lt;!-- PUBLISHER --&gt;
| 1789
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| [https://archive.org/stream/bostondirectory00sampgoog#page/n10/mode/2up via Internet Archive]
| &lt;!-- OTHER --&gt;
|-
| Boston Directory
| &lt;!-- PUBLISHER --&gt;
| 1796
| [https://books.google.com/books?id=CJFIAAAAYAAJ&amp;client=safari&amp;pg=RA1-PA215#v=onepage&amp;q=&amp;f=false reprint via Google Books, p.&amp;nbsp;215-302]
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/1 via Boston Athenæum]
|-
| Boston Directory
| &lt;!-- PUBLISHER --&gt;
| 1798
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/2 via Boston Athenæum]
|}

===19th century===

====1800-1829====
{| class="wikitable" style="font-size: 95%;"
!Title||Publisher||Year||Google Books||[[HathiTrust]]||Internet Archive||Other
|-
| Boston Directory
| &lt;!-- PUBLISHER --&gt;
| 1800
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/3 via Boston Athenæum]
|-
| Boston Directory
| John West
| 1803
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/4 via Boston Athenæum]
|-
| Boston Directory
| Edward Cotton
| 1805
| &lt;!-- GOOGLE --&gt;
| [https://catalog.hathitrust.org/Record/010363295 via HathiTrust]
| [https://archive.org/stream/bostondirectory00inbost#page/n9/mode/2up via Internet Archive]
| &lt;!-- OTHER --&gt;
|-
| Boston Directory
| Edward Cotton
| 1806
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/5 via Boston Athenæum]
|-
| Boston Directory
| Edward Cotton
| 1807
| &lt;!-- GOOGLE --&gt;
| [http://catalog.hathitrust.org/Record/010363295 via HathiTrust]
| &lt;!-- IA --&gt;
| &lt;!-- OTHER --&gt;
|-
| Boston Directory
| Edward Cotton
| 1809
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/7 via Boston Athenæum]
|-
| Boston Directory
| Edward Cotton
| 1810
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/8 via Boston Athenæum]
|-
| Boston Directory
| Edward Cotton
| 1813
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/9 via Boston Athenæum]
|-
| Boston Directory
| Edward Cotton
| 1816
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/10 via Boston Athenæum]
|-
| Boston Directory
| Edward Cotton
| 1818
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/11 via Boston Athenæum]
|-
| Boston Directory
| Frost and Stimpson
| 1820
| &lt;!-- GOOGLE --&gt;
| [http://catalog.hathitrust.org/Record/010363295 via HathiTrust]
| &lt;!-- IA --&gt;
| &lt;!-- OTHER --&gt;
|-
| Boston Directory
| Frost and Stimpson
| 1821
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/12 via Boston Athenæum]
|-
| Boston Directory
| Frost and Stimpson
| 1822
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/15 via Boston Athenæum]
|-
| Boston Directory
| Frost and Stimpson
| 1823
| [https://books.google.com/books?id=nY4vAAAAYAAJ via Google Books]
| [http://catalog.hathitrust.org/Record/010363295 via HathiTrust]
| &lt;!-- IA --&gt;
| &lt;!-- OTHER --&gt;
|-
| Boston Directory
| Frost and Stimpson
| 1825
| &lt;!-- GOOGLE --&gt;
| [https://catalog.hathitrust.org/Record/010363295 via HathiTrust]
| [https://archive.org/stream/bostondirectorys1825bost via Internet Archive]
| &lt;!-- OTHER --&gt;
|-
| Boston Directory
| Frost and Stimpson
| 1826
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/17 via Boston Athenæum]
|-
| Boston Directory
| Hunt, Stimpson, and Frost
| 1827
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/18 via Boston Athenæum]
|-
| Boston Directory
| Hunt and Stimpson
| 1828
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/19 via Boston Athenæum]
|-
| Boston Directory
| Charles Stimpson, Jr.
| 1829
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/20 via Boston Athenæum]
|-
|}

====1830-1849====
{| class="wikitable" style="font-size: 95%;"
!Title||Publisher||Year||Google Books||[[HathiTrust]]||Internet Archive||Other
|-
| Boston Directory
| &lt;!-- PUBLISHER --&gt;
| 1830
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/41 via Boston Athenæum]
|-
| Stimpson's Boston Directory
| Stimpson &amp; Clapp
| 1831
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| [https://archive.org/stream/stimpsonsbostond3132adam#page/n29/mode/2up via Internet Archive]
| &lt;!-- OTHER --&gt;
|-
| Stimpson's Boston Directory
| Stimpson &amp; Clapp
| 1832
| [https://books.google.com/books?id=raQtAAAAYAAJ via Google Books]
| [http://catalog.hathitrust.org/Record/010363296 via HathiTrust]
| &lt;!-- IA --&gt;
| &lt;!-- OTHER --&gt;
|-
| Stimpson's Boston Directory
| Stimpson &amp; Clapp
| 1833
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/27 via Boston Athenæum]
|-
| Stimpson's Boston Directory
| Stimpson &amp; Clapp
| 1834
| &lt;!-- GOOGLE --&gt;
| [https://catalog.hathitrust.org/Record/010363296 via HathiTrust]
| [https://archive.org/stream/bostondirectory01bost#page/n5/mode/2up via Internet Archive]
| &lt;!-- OTHER --&gt;
|-
| Stimpson's Boston Directory
| Stimpson &amp; Clapp
| 1835
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| [https://archive.org/stream/bostondirectory03bost#page/n5/mode/2up via Internet Archive]
| &lt;!-- OTHER --&gt;
|-
| Stimpson's Boston Directory
| Stimpson &amp; Clapp
| 1836
| &lt;!-- GOOGLE --&gt;
| [https://catalog.hathitrust.org/Record/010363296 via HathiTrust]
| [https://archive.org/stream/stimpsonsbostond1836adam#page/n21/mode/2up via Internet Archive]
| &lt;!-- OTHER --&gt;
|-
| Stimpson's Boston Directory
| Stimpson &amp; Clapp
| 1837
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/34 via Boston Athenæum]
|-
| Stimpson's Boston Directory
| Stimpson &amp; Clapp
| 1838
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/35 via Boston Athenæum]
|-
| Stimpson's Boston Directory
| Stimpson &amp; Clapp
| 1839
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/40 via Boston Athenæum]
|-
| Stimpson's Boston Directory
| Stimpson &amp; Clapp
| 1840
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/39 via Boston Athenæum]
|-
| Stimpson's Boston Directory
| Stimpson &amp; Clapp
| 1841
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/37 via Boston Athenæum]
|-
| Stimpson's Boston Directory
| Stimpson &amp; Clapp
| 1842
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/36 via Boston Athenæum]
|-
| Stimpson's Boston Directory
| Stimpson &amp; Clapp
| 1843
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/38 via Boston Athenæum]
|-
| Stimpson's Boston Directory
| Stimpson &amp; Clapp
| 1844
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/45 via Boston Athenæum]
|-
| Stimpson's Boston Directory
| Stimpson &amp; Clapp
| 1845
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/44 via Boston Athenæum]&lt;br&gt;[http://dca.lib.tufts.edu/features/bostonstreets/people/directories.html via Tufts University]
|-
| Stimpson's Boston Directory
| Stimpson &amp; Clapp
| 1846
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/43 via Boston Athenæum]
|-
| Adams's New Directory of the City of Boston
| George Adams
| 1846-47
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/25 via Boston Athenæum]
|-
| Adams's Boston Directory
| French and Stimpson
| 1847-48
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/24 via Boston Athenæum]
|-
| Boston Directory
| French and Stimpson
| 1848-49
| &lt;!-- GOOGLE --&gt;
| [https://catalog.hathitrust.org/Record/100734377 via HathiTrust]
| [https://archive.org/stream/bostondirectory4849bost#page/n7/mode/2up via Internet Archive]
| &lt;!-- OTHER --&gt;
|-
| Boston Directory
| George Adams
| 1849-50
| &lt;!-- GOOGLE --&gt;
| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]
| [https://archive.org/stream/bostondirectory00bost#page/n7/mode/2up via Internet Archive]
| &lt;!-- OTHER --&gt;
|-
|}

====1850-1869====
{| class="wikitable" style="font-size: 95%;"
!Title||Publisher||Year||Google Books||[[HathiTrust]]||Internet Archive||Other
|-
| Directory of the City of Boston
| George Adams
| 1850
| [https://books.google.com/books?id=UHDPAAAAMAAJ via Google Books]
| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]
| &lt;!-- IA --&gt;
| &lt;!-- OTHER --&gt;
|-
| Boston Directory
| George Adams
| 1851
| [https://books.google.com/books?id=C6UqAAAAYAAJ via Google Books]
| [http://catalog.hathitrust.org/Record/100321756 via HathiTrust]
| &lt;!-- IA --&gt;
| &lt;!-- OTHER --&gt;
|-
| Boston Directory
| George Adams
| 1852
| [https://books.google.com/books?id=2tsCAAAAYAAJ via Google Books]
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| &lt;!-- OTHER --&gt;
|-
| Boston Directory
| George Adams
| 1853
| &lt;!-- GOOGLE --&gt;
| [https://catalog.hathitrust.org/Record/100321756 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/48 via Boston Athenæum]
|-
| Boston Directory
| George Adams
| 1854
| &lt;!-- GOOGLE --&gt;
| [https://catalog.hathitrust.org/Record/100734377 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/81 via Boston Athenæum]
|-
| Boston Directory
| George Adams
| 1855
| &lt;!-- GOOGLE --&gt;
| [https://catalog.hathitrust.org/Record/100321756 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/82 via Boston Athenæum]&lt;br&gt;[http://dca.lib.tufts.edu/features/bostonstreets/people/directories.html via Tufts University]
|-
| Boston Directory
| George Adams
| 1856
| [https://books.google.com/books?id=zYMqAAAAYAAJ via Google Books]
| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]
| &lt;!-- IA --&gt;
| &lt;!-- OTHER --&gt;
|-
| Boston Directory
| George Adams
| 1857
| [https://books.google.com/books?id=nYIqAAAAYAAJ via Google Books]
| [https://catalog.hathitrust.org/Record/100321756 via HathiTrust]
| &lt;!-- IA --&gt;
| &lt;!-- OTHER --&gt;
|-
| Boston Directory
| Adams, Sampson, &amp; Co.
| 1858
| [https://books.google.com/books?id=En4qAAAAYAAJ via Google Books]
| [https://catalog.hathitrust.org/Record/100321756 via HathiTrust]
| &lt;!-- IA --&gt;
| &lt;!-- OTHER --&gt;
|-
| Boston Directory
| Adams, Sampson, &amp; Co.
| 1859
| &lt;!-- GOOGLE --&gt;
| [https://catalog.hathitrust.org/Record/100734377 via HathiTrust]
| &lt;!-- IA --&gt;
| &lt;!-- OTHER --&gt;
|-
| Boston Directory
| Adams, Sampson, &amp; Co.
| 1860
| &lt;!-- GOOGLE --&gt;
| [https://catalog.hathitrust.org/Record/100734377 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/87 via Boston Athenæum]
|-
| Boston Directory
| Adams, Sampson, &amp; Co.
| 1861
| [https://books.google.com/books?id=hHwqAAAAYAAJ via Google Books]
| [https://catalog.hathitrust.org/Record/100321756 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/singleitem/collection/p16057coll32/id/93/rec/57 via Boston Athenaeum]
|-
| Boston Directory
| Adams, Sampson, &amp; Co.
| 1862
| [https://books.google.com/books?id=tH4qAAAAYAAJ via Google Books]
| [https://catalog.hathitrust.org/Record/100321756 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/91 via Boston Athenæum]
|-
| Boston Directory
| Adams, Sampson, &amp; Co.
| 1863
| &lt;!-- GOOGLE --&gt;
| [https://catalog.hathitrust.org/Record/100321756 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/92 via Boston Athenæum]
|-
| Boston Directory
| Adams, Sampson, &amp; Co.
| 1864
| [https://books.google.com/books?id=8IEqAAAAYAAJ via Google Books]
| [https://catalog.hathitrust.org/Record/100321756 via HathiTrust]
| &lt;!-- IA --&gt;
| &lt;!-- OTHER --&gt;
|-
| Boston Directory
| Adams, Sampson, &amp; Co.
| 1865
| &lt;!-- GOOGLE --&gt;
| [https://catalog.hathitrust.org/Record/100734377 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/95 via Boston Athenæum]&lt;br&gt;[http://dca.lib.tufts.edu/features/bostonstreets/people/directories.html via Tufts University]
|-
| Boston Directory
| Sampson, Davenport, &amp; Co.
| 1866
| [https://books.google.com/books?id=_A5FAQAAMAAJ via Google Books]
| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/96 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Davenport, &amp; Co.
| 1867
| &lt;!-- GOOGLE --&gt;
| [https://catalog.hathitrust.org/Record/100734377 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/97 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Davenport, &amp; Co.
| 1868
| [https://books.google.com/books?id=SFwJAQAAIAAJ via Google Books]
| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]
| &lt;!-- IA --&gt;
| &lt;!-- OTHER --&gt;
|-
| Boston Directory
| Sampson, Davenport, &amp; Co.
| 1869
| &lt;!-- GOOGLE --&gt;
| [https://catalog.hathitrust.org/Record/100321756 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/69 via Boston Athenæum]
|-
|}

====1870-1889====
{| class="wikitable" style="font-size: 95%;"
!Title||Publisher||Year||Google Books||[[HathiTrust]]||Internet Archive||Other
|-
| Boston Directory
| Sampson, Davenport, &amp; Co.
| 1870
| [https://books.google.com/books?id=GytFAQAAMAAJ via Google Books]
| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/98 via Boston Athenæum]&lt;br&gt;[http://dca.lib.tufts.edu/features/bostonstreets/people/directories.html via Tufts University]
|-
| Boston Directory
| Sampson, Davenport, &amp; Co.
| 1871
| &lt;!-- GOOGLE --&gt;
| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/99 via Boston Athenæum]
|-
| Boston Commercial Directory
| Wentworth &amp; Co.
| 1871
| [https://books.google.com/books?id=xfACAAAAYAAJ via Google Books]
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| &lt;!-- OTHER --&gt;
|-
| Boston Directory
| Sampson, Davenport, &amp; Co.
| 1872
| &lt;!-- GOOGLE --&gt;
| [http://babel.hathitrust.org/cgi/pt?id=hvd.32044092998012;view=1up;seq=19 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/65 December] supplement via Boston Athenæum&lt;br&gt;[http://dca.lib.tufts.edu/features/bostonstreets/people/directories.html via Tufts University]
|-
| Boston Directory
| Sampson, Davenport, &amp; Co.
| 1873
| [https://books.google.com/books?id=NqHNAAAAMAAJ via Google Books]
| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/67 November], [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/66 December] supplements via Boston Athenæum
|-
| Boston Directory
| Sampson, Davenport, &amp; Co.
| 1874
| &lt;!-- GOOGLE --&gt;
| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/108 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Davenport, &amp; Co.
| 1875
| [https://books.google.com/books?id=EC5FAQAAMAAJ via Google Books]
| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/104 via Boston Athenæum]&lt;br&gt;[http://dca.lib.tufts.edu/features/bostonstreets/people/directories.html via Tufts University]
|-
| Boston Directory
| Sampson, Davenport, &amp; Co.
| 1876
| &lt;!-- GOOGLE --&gt;
| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/105 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Davenport, &amp; Co.
| 1877
| [https://books.google.com/books?id=RTBFAQAAMAAJ via Google Books]
| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/103 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Davenport, &amp; Co.
| 1878
| &lt;!-- GOOGLE --&gt;
| [http://hdl.handle.net/2027/uc1.c047888986 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/110 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Davenport, &amp; Co.
| 1879
| &lt;!-- GOOGLE --&gt;
| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/111 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Davenport, &amp; Co.
| 1880
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/107 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Davenport, &amp; Co.
| 1881
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/109 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Davenport, &amp; Co.
| 1882
| [https://books.google.com/books?id=NSFFAQAAMAAJ via Google Books]
| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/112 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Davenport, &amp; Co.
| 1883
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/113 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Davenport, &amp; Co.
| 1884
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/114 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Murdock, &amp; Co.
| 1885
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/116 via Boston Athenæum]&lt;br&gt;[http://dca.lib.tufts.edu/features/bostonstreets/people/directories.html via Tufts University]
|-
| Boston Directory
| Sampson, Murdock, &amp; Co.
| 1886
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/118 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Murdock, &amp; Co.
| 1887
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/117 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Murdock, &amp; Co.
| 1888
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/119 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Murdock, &amp; Co.
| 1889
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/121 via Boston Athenæum]
|-
|}

====1890-1899====
{| class="wikitable" style="font-size: 95%;"
!Title||Publisher||Year||Google Books||[[HathiTrust]]||Internet Archive||Other
|-
| Boston Directory
| Sampson, Murdock, &amp; Co.
| 1890
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/123 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Murdock, &amp; Co.
| 1891
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/124 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Murdock, &amp; Co.
| 1892
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/125 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Murdock, &amp; Co.
| 1893
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/126 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Murdock, &amp; Co.
| 1894
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/127 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Murdock, &amp; Co.
| 1895
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/128 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Murdock, &amp; Co.
| 1896
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/129 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Murdock, &amp; Co.
| 1897
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/130 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Murdock, &amp; Co.
| 1898
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/131 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Murdock, &amp; Co.
| 1899
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/132 via Boston Athenæum]
|-
|}

===20th century===

====1900-1949====
{| class="wikitable" style="font-size: 95%;"
!Title||Publisher||Year||Google Books||[[HathiTrust]]||Internet Archive||Other
|-
| Boston Directory
| Sampson, Murdock, &amp; Co.
| 1900
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/133 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Murdock, &amp; Co.
| 1905
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://dca.lib.tufts.edu/features/bostonstreets/people/directories.html via Tufts University]
|-
| Boston Directory
| Sampson &amp; Murdock Co.
| 1916
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| [https://archive.org/stream/bostonmassachuse1916112samp#page/n9/mode/2up via Internet Archive]
| &lt;!-- OTHER --&gt;
|-
| Boston Register and Business Directory
| Sampson &amp; Murdock Co.
| 1922
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| [https://archive.org/stream/bostonregisterbu1922bost#page/n13/mode/2up via Internet Archive]
| &lt;!-- OTHER --&gt;
|-
| Boston Directory
| Sampson &amp; Murdock Co.
| 1925
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| &lt;!-- IA --&gt;
| [http://dca.lib.tufts.edu/features/bostonstreets/people/directories.html via Tufts University]
|-
|}

====1950-1999====
{| class="wikitable" style="font-size: 95%;"
!Title||Publisher||Year||Google Books||[[HathiTrust]]||Internet Archive||Other
|-
| Boston City Directory
| R.L. Polk &amp; Co.
| 1955
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| [https://archive.org/stream/bostondirectoryi00bost#page/n7/mode/2up via Internet Archive]
| &lt;!-- OTHER --&gt;
|-
| Boston City Directory
| R.L. Polk &amp; Co.
| 1956
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| [https://archive.org/stream/bostondirectoryi56bost#page/n3/mode/2up via Internet Archive]
| &lt;!-- OTHER --&gt;
|-
| Boston City Directory
| R.L. Polk &amp; Co.
| 1959
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| [https://archive.org/stream/bostondirectoryi002bost via Internet Archive]
| &lt;!-- OTHER --&gt;
|-
| Boston City Directory
| R.L. Polk &amp; Co.
| 1961
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| [https://archive.org/stream/bostondirectoryi11961bost#page/n3/mode/2up v.1], [https://archive.org/details/bostondirectoryi261bost v.2] via Internet Archive
| &lt;!-- OTHER --&gt;
|-
| Boston City Directory
| R.L. Polk &amp; Co.
| 1962
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| [https://archive.org/stream/bostondirectoryi162bost#page/n3/mode/2up via Internet Archive]
| &lt;!-- OTHER --&gt;
|-
| Boston City Directory
| R.L. Polk &amp; Co.
| 1965
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| [https://archive.org/stream/bostondirectoryi11965bost#page/n3/mode/2up via Internet Archive]
| &lt;!-- OTHER --&gt;
|-
| Boston City Directory
| R.L. Polk &amp; Co.
| 1966
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| [https://archive.org/stream/bostondirectoryi1966bost#page/n3/mode/2up via Internet Archive]
| &lt;!-- OTHER --&gt;
|-
| Boston City Directory
| R.L. Polk &amp; Co.
| 1969
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| [https://archive.org/stream/bostondirectoryi169bost#page/n3/mode/2up via Internet Archive]
| &lt;!-- OTHER --&gt;
|-
| Boston City Directory
| R.L. Polk &amp; Co.
| 1970
| &lt;!-- GOOGLE --&gt;
| &lt;!-- HATHI --&gt;
| [https://archive.org/stream/bostondirectoryi170bost#page/n3/mode/2up via Internet Archive]
| &lt;!-- OTHER --&gt;
|-
|}

==See also==
* ''[[Boston Almanac|Boston Almanac and Business Directory]]''
* ''[[Boston Almanac|Boston Register and Business Directory]]''
* ''[[Massachusetts Register]]''

==References==
{{reflist}}

==Further reading==
* [https://books.google.com/books?id=ALIUAAAAYAAJ New England historical and genealogical register]. Oct. 1862; p.&amp;nbsp;387+
* [https://books.google.com/books?id=CJFIAAAAYAAJ Report of the record commissioners of the city of Boston], Volume 10. Rockwell and Churchill, 1886; p.&amp;nbsp;163+
* {{Citation |publisher = Williams Directory Co. |publication-place = Cincinnati, Ohio |author = A.V. Williams |title = The Development and Growth of City Directories |publication-date = 1913 |chapter=Boston, Massachusetts |chapterurl=http://hdl.handle.net/2027/nyp.33433082423645?urlappend=%3Bseq=61 }}

==External links==
* HathiTrust. [https://catalog.hathitrust.org/Record/010363295 1805 etc]; [https://catalog.hathitrust.org/Record/000499337 1849-1883]
* [http://www.damrellsfire.com/cgi-bin/directory_search.pl damrellsfire.com]
* [http://cdm.bostonathenaeum.org/cdm/landingpage/collection/p16057coll32 Boston Athenæum: The Boston Directory 1789-1900 (Ongoing Project), Digital Collection].

[[Category:History of Boston]]
[[Category:Directories]]
[[Category:Publications established in 1789]]
[[Category:18th century in Boston]]
[[Category:19th century in Boston]]
[[Category:20th century in Boston]]
[[Category:1789 establishments in Massachusetts]]</text>
      <sha1>0hgvn6wnx54cj2t87zsuak717u80pli</sha1>
    </revision>
  </page>
  <page>
    <title>Army List</title>
    <ns>0</ns>
    <id>31790826</id>
    <revision>
      <id>731540468</id>
      <parentid>687562000</parentid>
      <timestamp>2016-07-26T00:39:40Z</timestamp>
      <contributor>
        <username>Bender235</username>
        <id>88026</id>
      </contributor>
      <minor />
      <comment>/* External links */clean up; http-&gt;https (see [[WP:VPR/Archive 127#RfC: Should we convert existing Google and Internet Archive links to HTTPS?|this RfC]]) using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3094" xml:space="preserve">The '''''Army List''''' is a list (or more accurately seven series of lists) of serving regular, [[militia]] or territorial [[British Army]] officers, kept in one form or another, since 1702.

Manuscript lists of army officers were kept from 1702–1752, the first official list being published in 1740.

==Regular army==
* Annual Army Lists, 1754–1879 (WO 65)
* Quarterly Army Lists (First Series), 1879–1922
* Half-Yearly Army Lists, 1923 - Feb 1950 (From 1947, annual, despite the name)
* Modern Army Lists, 1951-Ongoing
** Part 1; serving officers.
** Part 2; retired officers, as of 2011 four-yearly
** Part 3; the Gradation List, a short biography of officers, a restricted publication  not generally available.

==Other lists==
* Monthly Army Lists, 1798-June 1940. Officers of colonial, militia and territorial units are included.
* Quarterly Army Lists (Second Series), July 1940-December 1950. These superseded the Monthly Army Lists, and, for the remainder of [[World War II]] were not published but  produced as confidential documents, monthly or bi-monthly until December 1943 and quarterly until April 1947, then three times a year, April, August and December.
* [[British Home Guard|Home Guard]] List, 1939–1945
* Militia Lists - various militia lists pertaining to the eighteenth and nineteenth centuries are extant.
* ''[[Hart's Army List]]'', an unofficial list, produced between 1839 and 1915, containing details of war service which the official lists started covering only in 1881.

==Further reading and bibliography==
* ''The army lists of the [[Roundheads and Cavaliers]]: containing the names of the officers in the Royal and Parliamentary armies of 1642'', [[Edward Peacock (antiquary)|Edward Peacock]] (ed) (1874)
* ''English army lists and commission registers, 1661–1714'', [[Charles Dalton]] (ed) (1892–1904)
* [[Henry George Hart]], ''Hart's army list : the new army list exhibiting the rank, standing, and various services of every officer in the Army on full pay'' (1839-)
* William Spencer, ''Army service records of the First World War'' (seventh edition, 2006)

==See also==
* [[Navy List]]
* [[Crockford's Clerical Directory]]
*

==References==
{{Reflist}}

==External links==
* [https://books.google.com/books?id=p_BfsBzDzWYC The 1740 Army List] at [[google books]]
* [http://discovery.nationalarchives.gov.uk/SearchUI/Collection/Display?iaid=C14273&amp;parentiaid=C543 War Office: Printed Annual Army Lists 1754-1879 (WO 65) - download for free]
* Digitised copies of 'Quarterly army lists' from [http://digital.nls.uk/97136046 1913 to 1919] and from [http://digital.nls.uk/97136048 1940 to 1946] at [[National Library of Scotland]]
* Digitised copies of 'Half-yearly army lists' from [http://digital.nls.uk/97136047 1938 to 1941] at National Library of Scotland
* [http://www.nationalarchives.gov.uk/records/research-guides/british-army-lists.htm British Army Lists] ([[National Archives]]' Research Guide)
* [https://archive.org/details/nlsarmylists Hart's Army List] at the [[Internet Archive]]

[[Category:Directories]]
[[Category:British Army]]</text>
      <sha1>stwgxgnsj29ohxtwqaf899lsl8i5004</sha1>
    </revision>
  </page>
  <page>
    <title>Almanach de Gotha</title>
    <ns>0</ns>
    <id>747726</id>
    <revision>
      <id>749887621</id>
      <parentid>749885832</parentid>
      <timestamp>2016-11-16T18:05:41Z</timestamp>
      <contributor>
        <username>LouisAlain</username>
        <id>14909828</id>
      </contributor>
      <comment>link to new article</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="22637" xml:space="preserve">{{Multiple issues|
{{refimprove|date=July 2014}}
{{primary sources|date=July 2014}}
}}
{{italic title}}
{{Use dmy dates|date=June 2013}}	
{{Infobox book series
| name             = Almanach de Gotha
| image            =  DeGotha1851.jpg
| image_caption    = The Almanach de Gotha  1851
| books            = 
| author           =
| editors          = 
| title_orig       = 
| translator       = 
| illustrator      = 
| cover_artist     = 
| country          =
| language         =
| genre            =
| discipline       =
| publisher        = [[Johann Christian Dieterich|J.C. Dieterich]]&lt;br&gt;C.W. Ettinger&lt;br&gt;C.G. Ettinger&lt;br&gt;[[Justus Perthes (publishing company)|Justus Perthes]]&lt;br&gt;Almanach de Gotha, Ltd.
| pub_date         = 1763-1944&lt;br&gt;1998-
| english_pub_date = 1998-
| media_type       = 
| number_of_books  = 
| list_books       = 
| preceded by      = 
| followed by      = 
}}

The '''''Almanach de Gotha''''' was a directory of Europe's [[Royal family|royalty]] and higher [[nobility]], also including the major [[government]]al, [[military]] and [[diplomatic corps|diplomatic]] [[corps]], as well as statistical data by country. First published in 1763 by C.W. Ettinger in [[Gotha (town)|Gotha]] at the [[duke|ducal]] [[court]] of [[Frederick III, Duke of Saxe-Gotha-Altenburg|Frederick III]], Duke of [[Saxe-Gotha-Altenburg]], it came to be regarded as an authority in the classification of monarchies and their courts, reigning and former dynasties, princely and ducal families, and the genealogical, biographical and titulary details of Europe's highest level of aristocracy. It was published from 1785 annually by [[Justus Perthes (publishing company)|Justus Perthes]] Publishing House in Gotha, until 1944 when the [[Soviet]]s destroyed the ''Almanach de Gotha's'' archives.

In 1998, a London-based publisher acquired the rights for use of the title of ''Almanach de Gotha'' from Justus Perthes Verlag Gotha GmbH. Perthes regard the resultant volumes as new works, and not as a continuation of the editions which Perthes had published from 1785 to 1944.&lt;ref name=Perthes&gt;{{Cite web|url=http://www.perthes.de/geschichte_justus_perthes/almanach_de_gotha/almanach_de_gotha_english.html |title=Almanach de Gotha |accessdate=9 June 2008 |publisher=Justus Perthes }}&lt;/ref&gt; Two volumes have been printed since 1998, with Volume I containing lists of the sovereign, formerly sovereign and mediatised houses of Europe, and a diplomatic and statistical directory; and Volume II containing lists of the non-sovereign princely and ducal houses of Europe.

==Gotha publication, 1763–1944==
The original ''Almanach de Gotha'' provided detailed facts and statistics on nations of the world, including their [[reign]]ing and formerly reigning houses, those of [[Europe]] being more complete than those of other continents. It also named the highest incumbent [[Great Officer of State (disambiguation)|officers of state]], members of the [[diplomatic corps]], and Europe's upper nobility with their families. Although at its most extensive the ''Almanach'' numbered more than 1200 pages, fewer than half of which were dedicated to monarchical or aristocratic data,&lt;ref name="gotha"&gt;Almanach de Gotha. [[Justus Perthes]], Gotha, 1944, pp. 7-12, 131, 169, 363-364, 558, 581-584. French.&lt;/ref&gt; it acquired a reputation for the breadth and precision of its information on royalty and nobility compared to other [[almanac]]s.&lt;ref name="diesbach"&gt;{{Cite book|title=Secrets of the Gotha|last=de Diesbach|first=Ghislain|authorlink = Ghislain de Diesbach|year=1967|publisher=Chapman &amp; Hall|location=UK|pages=21, 23–24, 28–30}}&lt;/ref&gt;
[[File:London Library book, Gothaisches Genealogisches Taschenbuch der Freiherrlichen Häuser, 1910, Justus Perthes, Gotha.jpg|thumb|[[London Library]]'s copy of ''Gothaisches Genealogisches Taschenbuch der Freiherrlichen Häuser'', 1910.]]
The ''Almanach'''s publication by [[Justus Perthes]] began at the ducal court of [[Saxe-Coburg and Gotha]] in Germany and, its reigning dynasty was listed first therein well into the 19th century, usually followed by kindred sovereigns of the [[House of Wettin]] and then, in alphabetical order, other families of princely rank, ruling and non-ruling. Although always published in French, other almanacs in French and English were more widely sold internationally. The almanac's structure changed and its scope expanded over the years. The second portion, called the ''Annuaire diplomatique et statistique'' ("Diplomatic and Statistical Yearbook"), provided [[demography|demographic]] and governmental information by nation, similar to other [[almanac]]s. Its first portion, called the ''Annuaire généalogique'' ("Genealogical Yearbook"), came to consist essentially of three sections: reigning and formerly reigning families, [[mediatization|mediatized families]] and non-sovereign families at least one of whose members bore the title of prince or duke.&lt;ref name="diesbach"/&gt;

The first section always listed Europe's [[sovereignty|sovereign]] houses, whether they ruled as emperor, king, grand duke, duke, prince (or some other title, e.g., [[prince elector]], [[margrave]], [[landgrave]], [[count palatine]] or [[pope]]). Until 1810 these sovereign houses were listed alongside such families and entities as Barbiano-Belgiojoso, Clary, Colloredo, Furstenberg, the Emperor, Genoa, Gonzaga, Hatzfeld, Jablonowski, Kinsky, Ligne, Paar, Radziwill, Starhemberg, Thurn and Taxis, Turkey, Venice and the [[Order of Malta]] and the [[Teutonic Knights]]. In 1812, these entries began to be listed in groups.&lt;ref name="diesbach"/&gt; First were German sovereigns who held the rank of grand duke or prince elector and above (the Duke of Saxe-Gotha was, however, listed here along with, but before, France—see below).

Listed next were Germany's reigning ducal and princely dynasties under the heading "College of Princes", e.g., [[Hohenzollern]], [[County of Isenburg|Isenburg]], [[Leyen]], [[Liechtenstein]] and the other [[Ernestine duchies|Saxon duchies]]. They were followed by heads of non-German monarchies, i.e. Austria, Brazil, Great Britain, etc. Fourthly were listed non-reigning dukes and princes, whether mediatized or not, including [[Arenberg]], [[House of Croÿ|Croy]], [[Fürstenberg (princely family)|Furstenberg]] alongside [[Batthyany]], [[Jablonowski]], [[Sulkowski]], Porcia and [[Prince of Benevento|Benevento]].

In 1841 a third group was added to those of the sovereign dynasties and the non-reigning princely and ducal families. It was composed exclusively of the mediatized families of comital rank recognized as belonging, since 1825, to the same historical category and sharing some of the same privileges as reigning dynasties by the various states of the [[German Confederation]]; these families were German with a few exceptions (e.g. [[Bentinck]], [[Van Rechteren|Rechteren-Limpurg]]). The 1815 treaty of the [[Congress of Vienna]] had authorized — and Article 14 of the German Confederation's ''Bundesakt'' (charter) recognized — retention from the [[Holy Roman Empire|German Imperial]] regime of [[Royal intermarriage|equality of birth]] for marital purposes of mediatized families (called ''Standesherren'') to reigning dynasties.&lt;ref name="diesbach"/&gt; The almanac added a third section consisting exclusively of mediatized families of comital rank.

In 1877, the mediatized comital families were moved from section III to section II A, where they joined the princely mediatized families. For the first time in the century of its existence, the largely non-German, un-mediatized princely and ducal families of the ''Almanach de Gotha'' were removed from the same section as other non-reigning families bearing princely titles.&lt;ref name="diesbach"/&gt; While non-mediatized German and Austrian families (e.g. [[Prince Lichnowsky|Lichnowsky]], [[Wrede]]), were likewise relocated from the almanac's second to its third section, the second section's new preponderance of German families, princely and comital, which were henceforth recognized as possessing the exclusive privilege of inter-marriage with reigning dynasties was salient:&lt;ref name="diesbach"/&gt; Excluded were members of such historically notable families as the [[House of Rohan|Rohan]]s, [[Orsini]]s, [[Duke of Ursel|Ursels]], [[Duke of Norfolk|Norfolks]], [[Czartoryski]]s, [[Galitzine]]s, [[Duc de La Rochefoucauld|La Rochefoucaulds]], [[House of Kinsky|Kinskys]], [[Radziwiłł family|Radziwills]], [[De Mérode|Merodes]], [[Dohna (Disambiguation)#People|Dohnas]] and [[Duke of Alba|Albas]].

Although theoretically mediatized families were distinguished from Europe's other nobility by the former status of their territories as ''[[Imperial State|Reichsstand]]'' and their exercise within the Holy Roman Empire of "semi-sovereignty" or [[imperial immediacy]] (''Reichsunmittelbarkeit''), many ''Standesherr'' families, especially those bearing the [[count|comital]] title, had not been fully recognized as legally possessing immediate status within the Empire prior to its collapse in 1806. No other families whose highest title was count were admitted to any section of the almanac.&lt;ref name="diesbach"/&gt;

Moreover, other [[deposition (politics)|deposed]] European dynasties (e.g. [[House of Arenberg|Arenberg]], [[Ernst-Johann Biron, Prince of Courland|Biron]], [[Dadiani]], [[Boncompagni]]-[[Ludovisi (family)|Ludovisi]], [[Giray dynasty|Giray]], [[House of Murat|Murat]]) did not benefit ''vis-a-vis'' the almanac from a similar interpretation of their historical status. Many princely or ducal families were listed only in its third, non-dynastic section or were excluded altogether, evoking criticism in the 20th century from such genealogists as [[Cyril Toumanoff]], [[Jean-Engelbert d'Arenberg|Jean-Engelbert, Duke d'Arenberg]] and [[William Addams Reitwiesner]],&lt;ref&gt;Fra Cyril Toumanoff, "Genealogical Imperialism" (1985) vol 6 (no 134) (NS) Coat of Arms pp. 145, 147.&lt;/ref&gt;&lt;ref&gt;Duke and Prince Jean Engelbert d'[[Arenberg]], "The Lesser Princes of the Holy Roman Empire in the Napoleonic Era" dissertation, Washington, DC, 1950, published as Les Princes du St-Empire à l'époque napoléonienne (Louvain, 1951) 15ff, quoted in Almanach de Gotha (Almanach de Gotha, London, 1998) pp. 275–286.&lt;/ref&gt; the latter commenting that the changes displayed "pan-German triumphalism" and even a "fairly nasty bit of Germanic chauvinism."&lt;ref&gt;{{Cite web|url= http://www.wargs.com/essays/mediatize.html |title= Mediatization |accessdate= 19 April 2011 |last= Reitwiesner |first= William Addams |date=January 1998 |authorlink= William Addams Reitwiesner}}&lt;/ref&gt;

Even in the early 19th century the almanac's retention of [[deposition (politics)|deposed]] dynasties evoked objections, although not necessarily the desired changes. The elected Emperor [[Napoleon]] protested in writing to his foreign minister, [[Jean-Baptiste Nompère de Champagny|Champagny]]: &lt;blockquote&gt;''Monsieur de Champagny, this year's "Almanach de Gotha" is badly done. First comes the Comte de Lille [title used in exile by [[Louis XVIII of France|Louis de Bourbon, Count of Provence]] -- future King Louis XVIII of France], followed by all the princes of the [[Confederation of the Rhine|Confederation]] as if no change has been made in the constitution of Germany; the family of France is named inappropriately therein. Summon the Minister of Gotha, who is to be made to understand that in the next Almanach all of this is to be changed. The House of France must be referred to as in the [French] Imperial Almanac; there must be no further mention of the Comte de Lille, nor of any German prince other than those retained by the Articles of Confederation of the Rhine. You are to insist that the article be transmitted to you prior to publication. If other almanacs are printed in my allies' realms with inappropriate references to the Bourbons and the House of France, instruct my ministers to make it known that you have taken note, and that this is to be changed by next year.''&lt;ref&gt;{{Cite book|url= https://books.google.com/books?id=ScM3AQAAMAAJ&amp;pg=PA124&amp;dq=Napol%C3%A9on+13275+Champagny&amp;hl=en&amp;sa=X&amp;ei=W7-5U8PHMpHooATfkYCoBg&amp;ved=0CCEQ6AEwAA#v=onepage&amp;q=Napol%C3%A9on%2013275%20Champagny&amp;f=false |title= Correspondance de Napoléon I|volume=XVI |publisher=Imprimerie Impériale|date=1864|location=France|accessdate=6 July 2014}}&lt;/ref&gt;&lt;/blockquote&gt;

The response of the publishers was to humour Napoleon by producing two editions: one for France, with the recently ennobled, and another which included dynasties deposed since abolition of the [[Holy Roman Empire]]. A merged version, whose first section including recently reigning dynasties but also families which lost sovereignty after the fall of Napoleon in 1815, remained in publication until 1944, and has been replicated in subsequent dynastic compilations (e.g., ''Genealogisches Handbuch des Adels, Fürstliche Häuser'', ''Le Petit Gotha'', Ruvigny's "Titled Nobility of Europe").

In 1887 the ''Almanach'' began to include non-European dynasties in its first section, with the inclusion of one of the ruling families of India.

===World War II and aftermath===
When Soviet troops entered [[Gotha (town)|Gotha]] in 1945, they systematically destroyed all archives of the ''Almanach de Gotha''.{{Citation needed|date=September 2010}}

In 1951 a different publisher, C.A. Starke, began publication of a multi-volume German-language publication entitled the ''Genealogisches Handbuch des Adels'' ([[:de:Genealogisches Handbuch des Adels|GHdA]]). The publication is divided into subsets; the ''Fürstliche Häuser'' subset is largely equivalent to the German language ''Gothaischer Hofkalender'' and its ''Fürstlichen Häuser'' volume which was also published by Perthes, or sections 1, 2 and 3 of the ''Almanach de Gotha''. However, no single volume of the ''Fürstliche Häuser'' includes all the families included in the ''Hofkalender'' or ''Almanach de Gotha''. It is necessary to use multiple volumes to trace the majority of European royal families.

==London publication, since 1998==
[[File:2014 Almanach de Gotha Covers.jpg|right|200px|thumb|''Almanach de Gotha'', 2014, Volumes I &amp; II]]

In 1989 the family of [[Justus Perthes]] re-established its right to the use of the name ''Almanach de Gotha''. The family then sold these rights in 1995 to a new company, Almanach de Gotha Limited, formed in London.&lt;ref&gt;[https://www.thegazette.co.uk/notice/L-60158-1601227 Notice of Disclaimer]&lt;/ref&gt; The new publishers launched with the 182nd edition on 16 March 1998 at [[Claridge's Hotel]].&lt;ref&gt;{{Cite web|url= http://www.almanachdegotha.com/site/modern.htm|title=The Modern Gotha |accessdate=30 May 2008 |publisher=Almanach de Gotha |archiveurl=https://web.archive.org/web/20060211154624/http://www.almanachdegotha.com/site/modern.htm |archivedate=11 February 2006}}&lt;/ref&gt;&lt;ref&gt;Jury, Louise. [http://www.independent.co.uk/news/upper-crust-toasts-aristocrat-studbook-1150076.html Upper crust toasts aristocrat studbook] The Independent (14 March 1998)&lt;/ref&gt;  It was written in English instead of French as the editor felt that English was now the language of diplomacy.&lt;ref name=Runciman&gt;{{Cite news|first= Steven |last= Runciman |authorlink=Steven Runciman |title=The first book of kings |url=http://findarticles.com/p/articles/mi_qa3724/is_199805/ai_n8792875 |publisher=[[The Spectator]] |date=2 May 1998 |accessdate=6 June 2008 }}&lt;/ref&gt; Charlotte Pike served as editor of the 1998 edition only and John Kennedy as managing director and publisher. The new publishers also revived the Committee of Patrons under the presidency of King [[Juan Carlos I of Spain]] and chairmanship of King [[Michael I of Romania]].&lt;ref&gt;{{Cite web|url= http://www.gotha1763.com/society.html|title=The Société des Amis de l'Almanach de Gotha |accessdate=1 May 2014 |publisher=Almanach de Gotha}}&lt;/ref&gt;

The London publisher produced a further four editions of volume I (1999, 2000, 2003 and 2004) based on the 1998 edition of volume I which include Europe's and South America's reigning, formerly reigning, and mediatised princely houses, and a single edition of volume II in 2001 edited by John Kennedy and Ghislain Crassard which include other non-sovereign princely and ducal houses of Europe.&lt;ref name=Hardman&gt;{{Cite news|first=Robert |last=Hardman |title=Family almanac will unmask the noble pretenders |url=http://www.telegraph.co.uk/news/worldnews/europe/germany/1317982/Family-almanac-will-unmask-the-noble-pretenders.html |publisher=[[Daily Telegraph]] |date=19 June 2001 |accessdate=6 June 2008 }}&lt;/ref&gt; A review in ''[[The Economist]]'' criticised the low editorial standards and attacked volume II for a lack of genealogical accuracy.&lt;ref name=review_economist&gt;{{Cite web| title=The Almanach de Gotha -- Gothic horror | url=http://www.economist.com/books/displayStory.cfm?Story_ID=949183 | publisher=[[The Economist]] | date = 24 January 2002 | accessdate=7 October 2007}}&lt;/ref&gt; After a gap of eight years a new edition of volume I was published in 2012 under the editorship of John James.&lt;ref&gt;{{Cite web|url=http://www.boydellandbrewer.com/store/viewitem.asp?idproduct=13798 |title=Almanach de Gotha 2012. Volume I, parts I &amp; II |accessdate=8 February 2012 |publisher=[[Boydell &amp; Brewer]] }}&lt;/ref&gt; A review in ''[[The Times Literary Supplement]]'' praised the 2012 volume I for a "punctilious itemization of titles, lineage and heraldry [aiming] for scholarship rather than sensation...Some family legends&amp;nbsp;– such as the Ottoman boast of descent from a grandson of Noah&amp;nbsp;– do not merit inclusion in a work with authoritative aspirations. Most quixotically of all, the title page displays the word 'Annual', although it has been eight years since the last edition appeared."&lt;ref&gt;{{Cite web|url=http://www.the-tls.co.uk/tls/public/article1139358.ece |title=And dark the Sun and Moon, and the Almanach de Gotha... |accessdate=2013-01-17 |publisher=The Times Literary Supplement |deadurl=bot: unknown |archiveurl=https://web.archive.org/web/20130117050945/http://www.the-tls.co.uk/tls/public/article1139358.ece |archivedate=17 January 2013 |df=dmy }}&lt;/ref&gt;

==Structure==
As it was the practice of the diplomatic corps to employ official titles, adhere to local [[Order of precedence|precedence]] and etiquette, and to tender congratulations and condolences to members of the dynasty of the nation to which they were assigned, the almanac included a ''Calendrier des Diplomates'' ("Diplomats' Calendar") section, which detailed major national holidays, anniversaries, ceremonies and royal birthdates.&lt;ref name="gotha"/&gt;

Following [[World War I]] and the fall of many [[royal house]]s, fewer regulatory authorities remained to authenticate use of titles; however the ''Almanach de Gotha'' continued the practice of strict verification of information, requesting certified copies of [[letters patent]], genealogies confirmed by competent authorities, documents, decrees and references for titles claimed.&lt;ref name="gotha"/&gt; Europe's middle and lower nobility (families whose principal title ranked below that of prince or duke&amp;nbsp;— except [[German mediatisation|mediatized]] families, listed in a section of their own) were not included in the almanac. Nor were the [[grandee]]s or [[Portuguese dukedoms|ducal families]] of Portugal and Spain (where titles, being habitually transmissible through both male and [[cognatic|female lines]], were often inherited by relatives of non-[[patrilineality|patrilineal]] lineage). Families of some Italian and East European nations (e.g., Russia, Romania), where the princely title was claimed by many, were also incomplete. Yet the reigning, formerly reigning and noble families included in the almanac numbered in the hundreds by the time it ceased publication in 1944.&lt;ref name="gotha"/&gt;

In 1890 the almanac renamed II A to section II, and II B to section III. Dynasties ruling non-European nations were located in section I B. Families which became extinct were listed for the final time in the year following death of the last member, male or female, and subsequent editions referred readers to that volume.&lt;ref name="gotha"/&gt;

Families that ceased to be included for other reasons, such as lack of proof of a family's legitimate descendants or discovery that it did not hold a valid princely or ducal title, were henceforth excluded but added, along with dates of previous insertion, to a list following the last section of each ''Annuaire Genealogique'' (Genealogical Yearbook), which page was entitled ''Liste des Maisons authrefois publiees dans la 3e partie de l'Almanach de Gotha'' ("List of Houses formerly published in the 3rd section of the ''Almanach de Gotha''.") &lt;ref name="gotha"/&gt;

From 1927, the almanac ceased to include all families in each year's edition, henceforth rotating entries every few years. Where titles and [[style (manner of address)|style]]s (such as [[Serene Highness]]) had ceased to be recognized by national governments (e.g. Germany, Austria, Czechoslovakia), the almanac provided associated dates and details, but continued to attribute such titles and styles to individuals and families, consistent with its practice since the [[French revolution]]; deposed sovereigns and dynasties continued to be accorded their former titles and rank, but dates of deposition were noted,&lt;ref name="diesbach"/&gt; and titles exclusively associated with sovereignty (e.g. emperor, queen, grand duke, crown princess) were not accorded to those who had not borne them during the monarchy. Titles of [[pretender|pretence]] below sovereign rank were accorded to members of formerly reigning dynasties as reported by heads of their houses, otherwise self-assumed titles were not used. The almanac included an explicit disclaimer announcing that known biographical details, such as birthdates and divorces, would not be suppressed.&lt;ref name="gotha"/&gt;

==See also==
*[[Burke's Peerage]]
*[[Debrett's|Debrett’s Peerage &amp; Baronetage]]

==References==
{{Reflist|2}}

==Further reading==
*[[Ghislain de Diesbach|Diesbach, Ghislain de]]. ''Secrets of the Gotha''. Meredith Press, 1964.

==External links==
*[http://gallica.bnf.fr/ Scanned versions of the old almanachs]
*[https://archive.org/search.php?query=almanach+de+gotha%20AND%20mediatype%3Atexts Almanach de Gotha at Internet Archive]
*[http://www.gotha1763.com/ Almanach de Gotha]

{{DEFAULTSORT:Almanach De Gotha}}
[[Category:Biographical dictionaries]]
[[Category:European nobility]]
[[Category:Genealogy publications]]
[[Category:Directories]]
[[Category:Publications established in 1763]]
[[Category:Almanacs]]</text>
      <sha1>bkxw2iiltuxunrru1tb3u026etq5wdn</sha1>
    </revision>
  </page>
  <page>
    <title>Navy List</title>
    <ns>0</ns>
    <id>47170</id>
    <revision>
      <id>739831965</id>
      <parentid>739658109</parentid>
      <timestamp>2016-09-17T08:43:24Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor />
      <comment>Removed invisible unicode characters + other fixes, removed: ‪ (4) using [[Project:AWB|AWB]] (12084)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2674" xml:space="preserve">{{for|a list of countries with navies|List of navies}}
{{multiple issues|
{{more footnotes|date = March 2013}}
{{Globalize|date=May 2009}}
}}

A '''Navy List''' or '''Naval Register''' is an official list of [[navy|naval]] officers, their ranks and seniority, the ships which they command or to which they are appointed, etc., that is published by the government or naval authorities of a country.

==Background==
The Navy List fulfills an important function in [[international law]] in that warships are required by article 29 of the [[United Nations Convention on the Law of the Sea]] to be commanded by a [[commissioned officer]] whose name appears in the appropriate service list.{{why|date=June 2016}}

Past copies of the Navy List are also important sources of information for historians and genealogists.

The Navy List for the Royal Navy is no longer published in hard-copy.

The [[Royal Navy]] (United Kingdom) publishes annual lists of active and reserve officers, and biennial lists of retired officers. The equivalent in the [[United States Navy]] is the Naval Register, which is updated online on a continuous basis.  When a ship is removed from the [[Naval Vessel Register]] in the United States, or from a Naval List of any other country, the ship is said to be "[[:wikt:stricken|stricken]]".&lt;ref&gt;Edwards, Paul.  ''[https://books.google.com/books?id=OydzBgAAQBAJ&amp;pg=PA37 Small United States and United Nations Warships in the Korean War]'', p. 37 (McFarland, 2008).&lt;/ref&gt;

== Resources ==
Good sources of historical data on UK's Navy Lists are
*The Naval Historical Branch, Portsmouth Naval Base.
*The Central Library Portsmouth, Guildhall Square.
*[[The National Archives (United Kingdom)|The National Archives]], Kew, that has an almost complete set including unpublished editions produced during the Second World War for internal use by the Admiralty.
*The Caird Library of the [[National Maritime Museum]] has in its collection bound monthly lists published by the Admiralty, and the concurrently published Steel's lists

The current editor of the Navy List is Cliona Willis

== Bibliography ==

* ''The 1766 Navy List'', Edited by E. C. Coleman, Published by Ancholme Publishing, ISBN 0-9541443-0-9

==See also==
* [[Army List]]
* [[Naval Vessel Register]]

==References==
&lt;references /&gt;

== External links ==
* [http://www.royalnavy.mod.uk/~/media/royal%20navy%20responsive/documents/useful%20resources/navy%20list.pdf Navy List 2013]
* [https://navalregister.bol.navy.mil US Naval Register] (US Navy)
* [http://www.NavyListResearch.co.uk Navy List Research] (Royal Navy)

[[Category:Royal Navy]]
[[Category:United States Navy]]
[[Category:Directories]]</text>
      <sha1>p3h5df2m8oda38jxifwvb2zuodvkjf6</sha1>
    </revision>
  </page>
  <page>
    <title>Clergy List</title>
    <ns>0</ns>
    <id>34259473</id>
    <revision>
      <id>748236582</id>
      <parentid>658979837</parentid>
      <timestamp>2016-11-07T04:07:33Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* Publishers and later history */clean up; http&amp;rarr;https for [[Google Books]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4023" xml:space="preserve">The '''Clergy List''' was a professional directory of the [[Church of England]] which appeared between 1841-1917.  From the start it also covered Wales, together with more limited information relating to Scotland, Ireland, and other churches within the [[Anglican Communion]].

==Background and early contents==

An opportunity to compile and issue a new directory had been created by the effective disappearance of the earlier [[Clerical Guide or Ecclesiastical Directory]], edited by '''Richard Gilbert''', and also by the introduction of the much improved system of the [[Penny Post]]. 

The basic contents of the '''Clergy List''''s earlier editions was summarised on their title pages: 
*an alphabetical list of the clergy (or at least of those who held benefices)
*an alphabetical list of the benefices,with their post towns
*lists of the cathedral establishments
*benefices arranged under their ecclesiastical divisions
*lists of ecclesiastical preferments variously under the patronage of the Crown, the bishops, and the deans &amp; chapters, etc.

The directory was always a bit less expensive than its later rival, [[Crockford's Clerical Directory]], but not surprisingly it consequently offered considerably less in the way of biographical detail.  This was especially true in the earlier editions which offered little or no information as to previous appointments, universities attended, or lists of publications by the clergy.

==Publishers and later history==

The directory was initially published by '''Charles Cox''' at the Ecclesiastical Directory Office, [[Southampton Street, London|Southampton Street]], [[Strand, London|Strand]].   Cox – who in 1839 had taken over a periodical called the '''Ecclesiastical Gazette,''' originating during the previous year – was able to produce two separate editions during the Clergy List's inaugural year of 1841.&lt;ref name="paflin"&gt;[http://www.churchtimes.co.uk/content.asp?id=48255] [[Church Times]]: two-part article ''Shop-talk and mordant wit'', by Christopher Currie &amp; Glyn Paflin, describing the background to ''Crockford's Clerical Directory'''s first hundred editions, 6–13 December 2007&lt;/ref&gt;  Thereafter it managed to maintain annual publication right up until adverse trading conditions forced its closure as a separate volume in 1917.

Cox remained as the Clergy List's publisher for many years, but by 1881 the title had been taken over by John Hall of [[Whitehall|Parliament Street]], In 1888 it was further taken over by Hamilton, Adams &amp; Company, of London's [[Paternoster Row]].  They had earlier acquired Thomas Bosworth's '''[[Clerical Guide and Ecclesiastical Directory]]''', merging the two titles in 1889.  During the following year the combined directory was still further transferred to Kelly &amp; Company, the publishers of [[Kelly's Directories]].&lt;ref name="paflin" /&gt; 

The later volumes were considerably expanded to include much greater biographical detail – broadly comparable with Crockford – but this was not sufficient to sustain the publication in the longer term.  Over the years the number of pages also increased – ranging from around 300 in 1841 to around 700 by the 1890s. 

After 1917 the Clergy List finally merged with its long-time rival, '''Crockford's Clerical Directory'''.  At least as late as 1932 the latter continued to advertise on its preliminary pages that it "incorporated the '''Clergy List, the Clerical Guide and the Ecclesiastical Directory'''''".&lt;ref name="paflin" /&gt;

In recent years certain of the earlier editions of the Clergy List (including the first edition &lt;ref&gt;The 1841 first edition of the ''Clergy List'' may be downloaded free of charge from the Google eBookstore [https://books.google.com/ebooks]&lt;/&lt;/ref&gt;) have been reissued by various publishers – either on CD-ROM or in scanned format on the World Wide Web.

==References==
{{reflist}}

[[Category:Directories]]
[[Category:Church of England]]
[[Category:Church in Wales]]
[[Category:Scottish Episcopal Church]]
[[Category:Anglicanism]]</text>
      <sha1>bck3rx49e6yg9qeuxrii56w11ohswvi</sha1>
    </revision>
  </page>
  <page>
    <title>Spotlight (Casting Services Company)</title>
    <ns>0</ns>
    <id>5813520</id>
    <revision>
      <id>680460083</id>
      <parentid>680459700</parentid>
      <timestamp>2015-09-11T00:12:24Z</timestamp>
      <contributor>
        <username>Dl2000</username>
        <id>917223</id>
      </contributor>
      <comment>rm invalid cat</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1513" xml:space="preserve">{{Use dmy dates|date=September 2015}}
{{Use British English|date=September 2015}}
{{multiple issues|
{{no footnotes|date=January 2014}}
{{Primary sources|date=April 2013}}
}}
[[File:Cinema Museum, London object 15.JPG|thumb|Spotlight volumes preserved at the Cinema Museum, London.]]
	
'''Spotlight''' was founded in 1927 and is the [[United Kingdom|UK's]] largest casting resource. Over 60,000 performers appear in Spotlight, including [[actor]]s and actresses, [[child actor|child artist]]s, [[presenter]]s, [[dancer]]s, and [[stuntman|stunt artists]]. Thousands of [[production company|production companies]], [[broadcasting|broadcasters]], [[advertising agency|ad agencies]], and independent casting directors use Spotlight as a casting resource. Their clients range from large organisations such as the [[BBC]], [[ITV (TV network)|ITV]], and [[Channel 4]], to small production companies and individual casting [[Television director|directors]].

Spotlight also publishes the [[handbook]] Contacts both in hard copy and as an [[e-book]]. It includes listings for over 5000 [[company|companies]], services and individuals across all branches of [[television]], [[theatre|stage]], [[film industry|film]], and [[radio]].

==External links==
* [http://www.spotlight.com Spotlight website]
* [http://www.contactshandbook.com Contacts website]

{{UK-stub}}
{{DEFAULTSORT:Spotlight}}
[[Category:Directories]]
[[Category:1927 establishments in the United Kingdom]]
[[Category:Casting companies]]

{{advertising-stub}}</text>
      <sha1>6kc6gksq2q8ge0fwvu6oiqngua5pf1v</sha1>
    </revision>
  </page>
  <page>
    <title>Search.ch</title>
    <ns>0</ns>
    <id>661012</id>
    <revision>
      <id>595895732</id>
      <parentid>573133340</parentid>
      <timestamp>2014-02-17T16:48:53Z</timestamp>
      <contributor>
        <username>Wizardman</username>
        <id>713860</id>
      </contributor>
      <minor />
      <comment>clean up using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2014" xml:space="preserve">'''search.ch''' is a [[search engine]] and [[web portal]] for [[Switzerland]]. It was founded in 1995 &lt;ref name="founding" /&gt; by Rudolf Räber and Bernhard Seefeld as a regional search engine. In the following years many other services were added, such as a phonebook in 1999, a free [[Short message service|SMS]] service in 2000 (now reduced to only one free SMS per week).

The search technology is home grown. The user can restrict his search to regions of Switzerland, such as a [[cantons of Switzerland|canton]] or a [[city]]. The [[web crawler]] looks only at sites in the [[.ch]] and [[.li]] [[top-level domain]]s and a number of automatically and manually updated list of Swiss websites on other domains. The index is updated weekly.

== External links ==
* [http://www.search.ch/ search.ch]
* [http://tel.search.ch/ tel.search.ch] phonebook
* [http://map.search.ch/ map.search.ch] Swiss maps
* [http://meteo.search.ch/ meteo.search.ch] Swiss weather
* [http://news.search.ch/ news.search.ch] Swiss news
* [http://timetable.search.ch/ timetable.search.ch] Swiss public transport timetable
* [http://tv.search.ch/ tv.search.ch] Swiss TV programme
* [http://kino.search.ch/ kino.search.ch] Swiss cinema programme
* [http://sms.search.ch/ sms.search.ch] sms service
* [http://immo.search.ch/ immo.search.ch] immo portal search service
* [http://www.post.ch/ Swiss Post] &lt;ref name="post" /&gt; Swiss Post acquired search.ch
* [http://www.tamedia.ch Tamedia] &lt;ref name="tamedia" /&gt; Tamedia akquired a 75% stake from Swiss Post

==References==
{{Reflist|refs=
&lt;ref name="founding"&gt;http://www.moneyhouse.ch/en/u/search_ch_ag_CH-130.0.009.911-2.htm&lt;/ref&gt;
&lt;ref name="tamedia"&gt;http://about.search.ch/archives/2004/06/04/post-kauft-search-ch/&lt;/ref&gt;
&lt;ref name="post"&gt;http://www.post.ch/post-startseite/post-konzern/post-medien/post-archive/2009/post-mm09-fruehzustellung/post-medienmitteilungen.htm&lt;/ref&gt;
}}

{{DEFAULTSORT:Search.Ch}}
[[Category:Web portals]]
[[Category:Internet search engines]]
[[Category:Directories]]</text>
      <sha1>2t0l1ccf53l5btxwjpu8l263t8f9nso</sha1>
    </revision>
  </page>
  <page>
    <title>Trow's Directory</title>
    <ns>0</ns>
    <id>41863504</id>
    <redirect title="John Fowler Trow" />
    <revision>
      <id>594235871</id>
      <timestamp>2014-02-06T17:24:53Z</timestamp>
      <contributor>
        <username>M2545</username>
        <id>9455233</id>
      </contributor>
      <comment>[[WP:AES|←]]Redirected page to [[John Fowler Trow]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="132" xml:space="preserve">#REDIRECT[[John Fowler Trow]]

[[Category:Directories]]
[[Category:Books about New York City]]
[[Category:History of New York City]]</text>
      <sha1>kcmleenha1dmgbyvzayppoqkyhydvzn</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Index (publishing)</title>
    <ns>14</ns>
    <id>2198982</id>
    <revision>
      <id>724690645</id>
      <parentid>724690638</parentid>
      <timestamp>2016-06-10T21:13:58Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor />
      <comment>Bot: Removing CFD templates for completed action</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="77" xml:space="preserve">[[Category:Library science]]
[[Category:Publishing]]
[[Category:Directories]]</text>
      <sha1>stx01bvcpgtawnckh4o3y1xc2og84rv</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Directory services</title>
    <ns>14</ns>
    <id>30937577</id>
    <revision>
      <id>604573040</id>
      <parentid>552678615</parentid>
      <timestamp>2014-04-17T09:44:26Z</timestamp>
      <contributor>
        <username>Glenn</username>
        <id>9232</id>
      </contributor>
      <comment>+[[Category:Directories]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="210" xml:space="preserve">{{cat main|Directory service}}

[[Category:Directories]]
[[Category:Computer access control protocols]]
[[Category:Access control software]]
[[Category:Network service]]
[[Category:Database management systems]]</text>
      <sha1>mesxuugdchzdz7ub39jw103mleqsvbd</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Public records</title>
    <ns>14</ns>
    <id>20252940</id>
    <revision>
      <id>746188490</id>
      <parentid>740552358</parentid>
      <timestamp>2016-10-25T20:18:39Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <comment>removed [[Category:Administration]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="133" xml:space="preserve">{{Cat main|Public records}}

[[Category:Directories]]
[[Category:Documents]]
[[Category:Government information]]
[[Category:Privacy]]</text>
      <sha1>7don096yp4sjbvgj6rplj68qdwgpviz</sha1>
    </revision>
  </page>
  <page>
    <title>World Leaders</title>
    <ns>0</ns>
    <id>233058</id>
    <revision>
      <id>694564422</id>
      <parentid>694564191</parentid>
      <timestamp>2015-12-10T01:34:54Z</timestamp>
      <contributor>
        <username>GabeIglesia</username>
        <id>17543794</id>
      </contributor>
      <minor />
      <comment>/* top */ [[MOS:BOLDSYN]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1020" xml:space="preserve">{{italic title}}
[[Image:ChiefsCover.jpg|right|200px|thumb|Cover of ''World Leaders'']]
'''''World Leaders''''', also known as '''''Chiefs of State and Cabinet Members of Foreign Governments''''', is a [[public domain]] directory published weekly by the United States [[Central Intelligence Agency]]. It lists different state officials for each country of the world: the [[head of state]] and/or [[head of government]] and other [[cabinet minister]]s, the chief of the [[central bank]], and the [[ambassador]]s to the [[United Nations]] and the United States.

==See also==
*[[World-Check]]
*[[List of current heads of state and government]]
*[[National Security Agency academic publications]]
*''[[International Who's Who]]''

==External links==
*[https://www.cia.gov/library/publications/world-leaders-1/ ''World Leaders'']

[[Category:Central Intelligence Agency publications]]
[[Category:Heads of state]]
[[Category:Heads of government]]
[[Category:Directories]]
[[Category:Public domain databases]]

{{US-gov-stub}}</text>
      <sha1>229ga9geppkddk9q9knsw6xlue6rzgm</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Metadata registry</title>
    <ns>14</ns>
    <id>3675616</id>
    <revision>
      <id>615411220</id>
      <parentid>388532863</parentid>
      <timestamp>2014-07-03T10:30:10Z</timestamp>
      <contributor>
        <username>Glenn</username>
        <id>9232</id>
      </contributor>
      <comment>+[[Category:Directories]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="196" xml:space="preserve">All Wikipedia articles in this category are either instances of a metadata registry or related to metadata registries.
{{Cat main|Metadata registry}}
[[Category:Metadata]]
[[Category:Directories]]</text>
      <sha1>ggpzcmqqvs6ixe7zrtqsifyqxa6tbqo</sha1>
    </revision>
  </page>
  <page>
    <title>Directory of Open Access Journals</title>
    <ns>0</ns>
    <id>2241822</id>
    <revision>
      <id>761373219</id>
      <parentid>761372997</parentid>
      <timestamp>2017-01-22T16:56:48Z</timestamp>
      <contributor>
        <username>Marchitelli</username>
        <id>11251957</id>
      </contributor>
      <minor />
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5368" xml:space="preserve">{{Infobox website
|name            = Directory of Open Access Journals
|logo            = DOAJ logo.jpg
|logocaption     =
|screenshot      =
|collapsible     =
|collapsetext    =
|caption         =
|url             = {{URL|https://doaj.org/}}
|slogan          =
|commercial      =No
|type            =
|registration    =
|language        =English
|content license =
|owner           =
|author          =
|launch date     = &lt;!--{{Start date and age|YYYY|MM|DD|df=yes/no}}--&gt;
|alexa           = 58,591 (as of October 2015)&lt;ref&gt;{{cite web|title=Ranking for Doaj.org|url=http://www.alexa.com/siteinfo/doaj.org|accessdate=2015-10-20|work=[[Alexa.com]]}}&lt;/ref&gt;
|revenue         =
|current status  =Online
|footnotes       =
}}

The '''Directory of Open Access Journals''' ('''DOAJ''') is a [[website]] that lists [[open access journal]]s and is maintained by Infrastructure Services for Open Access (IS4OA).&lt;ref&gt;{{cite web |url=http://www.is4oa.org/ |title=Infrastructure Services for Open Access |publisher=Infrastructure Services for Open Access C.I.C. |accessdate=2013-03-05}}&lt;/ref&gt; The project defines open access journals as [[scientific journal|scientific]] and [[academic journal|scholarly journal]]s that meet high quality standards by exercising [[peer review]] or editorial quality control and "use a funding model that does not charge readers or their institutions for access."&lt;ref name=aboutdoaj/&gt; The [[Budapest Open Access Initiative]]'s definition of [[Open access (publishing)|open access]] is used to define required rights given to users, for the journal to be included in the DOAJ, as the rights to "read, download, copy, distribute, print, search, or link to the full texts of these articles".&lt;ref name=aboutdoaj/&gt;&lt;ref&gt;The BOAI definition is at "[http://www.earlham.edu/~peters/fos/boaifaq.htm#openaccess Budapest Open Access Initiative: Frequently Asked Questions]".&lt;/ref&gt; The aim of DOAJ is to "increase the visibility and ease of use of open access scientific and scholarly journals thereby promoting their increased usage and impact."&lt;ref name=aboutdoaj&gt;{{cite web |url= http://doaj.org/about |title=About |work=Directory of Open Access Journals |accessdate=2015-04-14}}&lt;/ref&gt;

As of March 2015, the database contained records for 10,000 journals.&lt;ref&gt;{{cite web |url= http://sparc.arl.org/blog/doaj-introduces-new-standards |title=Directory of Open Access Journals introduces new standards to help community address quality concerns |first=Caralee |last=Adams |date=5 March 2015 |publisher=SPARC |accessdate=2015-04-14}}&lt;/ref&gt; An average of four journals were being added each day in 2012.&lt;ref&gt;{{cite web|title=DOAJ Statistics |url= http://www.doaj.org/doaj?func=newTitles&amp;uiLanguage=en&amp;fromDate=1970-01-01+00:00:00&amp;orderedBy=J.first_added |archiveurl= https://web.archive.org/web/20120404125652/http://www.doaj.org/doaj?func=newTitles&amp;uiLanguage=en&amp;fromDate=1970-01-01+00:00:00&amp;orderedBy=J.first_added |archivedate= 2012-04-04 |accessdate=2013-01-06 |work=Directory of Open Access Journals}}&lt;/ref&gt;

In May 2016, DOAJ announced that they had removed approximately 3,300 journals from their database to provide better reliability on the content listed on it.&lt;ref&gt;{{cite journal |last=Marchitelli|first= Andrea|last2=Galimberti |first2=Paola |last3=Bollini |first3=Andrea|last4=Mitchell |first4=Dominic|date= January 2017 |title=
Helping journals to improve their publishing standards: a data analysis of DOAJ new criteria effects |url=http://leo.cineca.it/index.php/jlis/article/view/12052|journal= JLIS.it|volume=8 |issue=1 |pages= 39-49|doi=10.4403/jlis.it-12052|access-date=2017-01-22 }}&lt;/ref&gt;

The journals that were removed can reapply as part of an ongoing procedure. &lt;ref&gt;{{Cite web|url=https://doajournals.wordpress.com/2016/05/09/doaj-to-remove-approximately-3300-journals/|title=DOAJ to remove approximately 3300 journals|last=DOAJ|date=2016-05-09|website=News Service|access-date=2016-09-24}}&lt;/ref&gt; As of September 2016, the database now contains 9,216 journals. &lt;ref&gt;{{Cite web|url=https://doaj.org/|title=Directory of Open Access Journals|last=DOAJ|access-date=2016-09-24}}&lt;/ref&gt;

==History==
The [[Open Society Institute]] funded various open access related projects after the Budapest Open Access Initiative; the Directory was one of those projects.&lt;ref&gt;{{cite book|last=Crawford|first=Walt|title=Open access : what you need to know now|publisher=American Library Association|location=Chicago|isbn=9780838911068|page=13}}&lt;/ref&gt; The idea for the DOAJ came out of discussions at the first Nordic Conference on
Scholarly Communication in 2002, [[Lund University]] became the organization to set up and maintain the DOAJ.&lt;ref&gt;{{Cite journal | last1 = Hedlund | first1 = T. | last2 = Rabow | first2 = I. | doi = 10.1087/2009303 | title = Scholarly publishing and open access in the Nordic countries | journal = Learned Publishing | volume = 22 | issue = 3 | pages = 177-186| year = 2009 | pmid =  | pmc = }}&lt;/ref&gt; It continued to do so  until January 2013, when Infrastructure Services for Open Access (IS4OA) took over.

== See also ==
* [[List of open-access journals]]
*[[Open Access Scholarly Publishers Association]]

== References ==
&lt;references/&gt;

== External links ==
* {{official website|https://doaj.org/}}
{{Open access navbox}}

[[Category:Open access (publishing)]]
[[Category:Open access journals| ]]
[[Category:Directories]]</text>
      <sha1>g8x51pc6ahboxcfctwsf3ivam12cn8g</sha1>
    </revision>
  </page>
  <page>
    <title>Western Australia Post Office Directory</title>
    <ns>0</ns>
    <id>6013482</id>
    <revision>
      <id>700698936</id>
      <parentid>650122944</parentid>
      <timestamp>2016-01-20T02:23:14Z</timestamp>
      <contributor>
        <username>Kerry Raymond</username>
        <id>300735</id>
      </contributor>
      <comment>added [[Category:Postal system of Australia]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4474" xml:space="preserve">{{Use Australian English|date=March 2015}}
{{Use dmy dates|date=March 2015}}

The '''''Western Australia Post Office Directory''''', also known as ''Wise Directories'' or ''Wise Street Directories'' were published in Perth from 1893-1949.

They were published by H. Pierssené&lt;ref&gt;{{Citation | author1=Pierssené, Herbert | title=The Western Australian directory | publication-date=1893 | publisher=H. Pierssene | url=http://trove.nla.gov.au/work/28621466 | accessdate=6 March 2015 }}&lt;/ref&gt; and later by H. Wise &amp; Co.&lt;ref&gt;{{Citation | author1=Wise &amp; Co | title=Wise's Western Australia post office directory | publication-date=1938 | publisher=H. Wise &amp; Co. Pty Ltd | url=http://trove.nla.gov.au/work/19293522 | accessdate=6 March 2015 }}&lt;/ref&gt;  They listed household, business, society, and Government contacts in [[Perth]], [[Freemantle]], [[Kalgoorlie, Western Australia|Kalgoorlie]], [[Boulder, Western Australia|Boulder]] and [[Coolgardie, Western Australia|Coolgardie]] including some rural areas of [[Western Australia]].

==Publishers==
The ''Western Australian Directory'' was published by H. Pierssene between 1893-1895. Herbert Pierssene was a merchant and importer of English Continental and Ceylonese goods. He was an agent for McCulluch Carrying Company and a bottler of West Australian wines.&lt;ref&gt;{{cite web|title=Thomas Herbert Pierssené|url=http://www.territorystories.nt.gov.au/handle/10070/244383|website=Territory Stories|publisher=Northern Territory Department of Arts and Museums|accessdate=6 March 2015}}&lt;/ref&gt;

The ''Western Australia Post Office Directory'' was published by Wise &amp; Co. between the years 1895-1949 with the exception of 1943 and 1948.

==Wise Directories== 	
The directories provide information by locality, individual surname, government service, and by trade or profession. The addresses of householders and businesses throughout Western Australia are included.&lt;ref&gt;{{cite news |url=http://nla.gov.au/nla.news-article77356821 |title=POST-OFFICE DIRECTORY. |newspaper=[[Daily_News_(Perth,_Western_Australia)|The Daily News (Perth, WA : 1882 - 1950)]] |location=Perth, WA |date=27 April 1909 |accessdate=6 March 2015 |page=2 Edition: THIRD EDITION |publisher=National Library of Australia}}&lt;/ref&gt;  Maps were sometimes published with an edition of the directory.&lt;ref&gt; {{cite news |url=http://nla.gov.au/nla.news-article77329156 |title=WESTERN AUSTRALIA DIRECTORY. |newspaper=[[Daily_News_(Perth,_Western_Australia)|The Daily News (Perth, WA : 1882 - 1950)]] |location=Perth, WA |date=5 March 1908 |accessdate=6 March 2015 |page=6 Edition: THIRD EDITION |publisher=National Library of Australia}}&lt;/ref&gt;  The towns section of the directories normally contained separate street directories of Perth and suburbs, Fremantle and Suburbs, Kalgoorlie, Boulder and Coolgardie.&lt;ref&gt;{{Citation | author1=Wise's Directories | author2=Archive CD Books Australia | title=Western Australia Post Office directory (Wise's) 1905 | publication-date=2004 | publisher=Archive CD Books Australia | isbn=978-1-920978-23-5 }}&lt;/ref&gt;

Known colloquially to users and  book collectors as 'Wise Directories' or 'Wise Street Directories' the red covered directories were published between 1893 and 1949.  Due to the annual changes, the directories are valuable historical documents for Western Australian History.  They are scarce in the Australian rare book market.  

The directories have been invaluable referent points for such projects as the [[Dictionary of Western Australians]] and others where the street lists in the directory provide details of inhabitants and houses in some streets in the more built-up residential areas.  Country towns in the directory have name lists only. 

They have been available in microfilm form in [[J S Battye Library]], and more recently have become online (see link below) in one of the J S Battye Library digitization projects.

==References==
{{reflist}}

==External links==
* http://www.slwa.wa.gov.au/find/wa_resources/post_office_directories

==See also==
* [[Australia Post]]
*[[Australian Dictionary of Biography]]
*[[Cyclopedia of Western Australia]]
*[[Dictionary of Australian Biography]]
*[[J S Battye Library]]
*[[State Records Office of Western Australia]]

[[Category:Books about Western Australia]]
[[Category:History of Western Australia|Western Australia Post Office Directory]]
[[Category:Australian directories]]
[[Category:Directories]]
[[Category:Gazetteers]]
[[Category:Postal system of Australia]]</text>
      <sha1>h0aob8t3n29rpvhgwo26zs3sc8zn40s</sha1>
    </revision>
  </page>
  <page>
    <title>Yachting Pages</title>
    <ns>0</ns>
    <id>47332933</id>
    <revision>
      <id>733274543</id>
      <parentid>733274518</parentid>
      <timestamp>2016-08-06T17:05:10Z</timestamp>
      <contributor>
        <username>Bellerophon5685</username>
        <id>1258165</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2247" xml:space="preserve">{{Use dmy dates|date=September 2013}}
{{Use British English|date= September 2013}}
{{Infobox company
| logo     = [[File:Yachting Pages Logo Black KNOCKOUT.svg]]
| foundation       = [[Antibes]], [[France]] (2003)
| location         = [[Bristol]], [[Somerset]]
[[United Kingdom]]
| key_people       = 
| num_employees    = 34 (2013)
| industry         = [[Superyacht]]
| homepage         = {{url|www.yachting-pages.com/}}
}}
'''''Yachting Pages''''' is a [[superyacht]] business with a range of products aimed at [[Captain (nautical)|captains]] and [[crew]], [[shipyards]], refit yards and all within the superyacht industry. ''Yachting Pages'' is available either in its original printed form, or online. Since the first edition of ''Yachting Pages'' was released in 2004, the book has grown rapidly into an established superyacht [[Trade directory|directory]].

The annual print directory is available in three separate editions:

*''Mediterranean, Europe, Africa &amp; Middle East''
*''USA, the Americas &amp; Caribbean''
*''Australasia, Asia Pacific &amp; Far East''

The [[Port]] Maps section at the front of every edition totals over 350 detailed maps of the world's superyacht [[marinas]]. Copies of the printed directory are hand delivered free of charge directly to superyachts by uniformed crew, and also to superyacht marinas and land-based superyacht businesses in over 92 countries.&lt;ref&gt; Yachtingpages.com &lt;/ref&gt;

'''The Company'''

''Yachting Pages'' was founded in May 2003 from current CEO Steve Crowe's spare bedroom in [[Antibes]], [[France]], with only one other member of staff. The company is now based in [[Bristol]], [[United Kingdom]] with 34 staff members, many of whom are [[multi-lingual]]. 

The first copy of ''Yachting Pages'' was launched at the Genoa Charter Show, in May 2004. 

Since then, growth of the business has created more products: ''Yachtingpages.com, Yachting Pages Refit, Yachting Pages Delivers and Superyacht Owners' Guide (SYOG).'' 

'''Awards'''

Queens award for Enterprise: International Trade 2009. 
EADP European B2B Award 2009 

==References==
{{Reflist}}
{{refimprove|date=August 2013}}

==External links==
* [http://www.yachting-pages.com/ Yachting Pages]

[[Category:Directories]]
[[Category:Yachting]]</text>
      <sha1>alt2ut4u4fjgoth4586atb3daryb4nw</sha1>
    </revision>
  </page>
  <page>
    <title>Bengali film directory</title>
    <ns>0</ns>
    <id>19601961</id>
    <revision>
      <id>741661123</id>
      <parentid>740012480</parentid>
      <timestamp>2016-09-28T22:33:25Z</timestamp>
      <contributor>
        <username>Randy Kryn</username>
        <id>4796325</id>
      </contributor>
      <comment>italicize title, first mention and infobox title, add italics</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2609" xml:space="preserve">{{italic title}}
{{Infobox book
| name = Bengali Film Directory
| image = Bengali film directory.jpg
| caption =Cover page of ''Bengali Film Directory''
| author = [[Ansu Sur]]
| country = India
| language = English
| cover_artist =
| genre = [[Trade directory|Directory]]
| publisher = [[Nandan (Kolkata)|Nandan]]&lt;br /&gt; [[West Bengal]] Film Centre([[Calcutta]])
| release_date = 1999
| media_type = Print ([[Hardback]])
| pages =319
| preceded_by =
| followed_by =
}}

'''''Bengali Film Directory''''' is an archive of [[Cinema of West Bengal|Bengali film]]s (in English).&lt;ref&gt;{{cite web |url=http://www.rosland.freeserve.co.uk/filmbooks5.htm |title=The Howard Summers Cinema Website-National Filmographies-Asian Cinema |publisher=www.rosland.freeserve.co.uk |accessdate=2008-10-23 |archiveurl = https://web.archive.org/web/20080616195358/http://www.rosland.freeserve.co.uk/filmbooks5.htm &lt;!-- Bot retrieved archive --&gt; |archivedate = 2008-06-16}}&lt;/ref&gt; Published in March 1999 by [[Nandan (Kolkata)|Nandan]], [[West Bengal]] Film Centre ([[Calcutta]]), this directory was edited by Ansu Sur and was compiled by Abhijit Goswami. It includes all [[Bengali people|Bengali]] [[feature film]]s released from 1917 to 1998, described briefly, but including detailed cast and crew, director name, release date and release theater name.&lt;ref&gt;{{cite web |url=https://openlibrary.org/b/OL171681M |title=Bengali film directory (Open Library) |publisher=openlibrary.org |accessdate=2008-10-23}}
&lt;/ref&gt;

==Contents==
* Acknowledgements iv
* A Note from the Editor v
* Reference vi
* Abbreviations vii
* Filmography
** Silent era 1
** Sound era 9
* Studios and Post-Production Centres 267
* Production Companies 269
* Distributions 271
* Show-houses 274
* Useful Addresses 277
* Film Societies 278
* Film Journals 280
* Film Books 281
* Index
** Films 285
** Directors 298
** Actors and Actresses 303
* Appendix: Films released in 1998 315&lt;ref&gt;{{cite web |url=http://www.cscsarchive.org/MediaArchive/Library.nsf/(docid)/5BD370ADB88260A6652571AF0037A748?OpenDocument&amp;StartKey=Bengali&amp;Count=100 |title=Bengali film directory |publisher=www.cscsarchive.org |accessdate=2008-10-23}} {{Dead link|date=October 2010|bot=H3llBot}}&lt;/ref&gt;

==References==
{{Reflist}}

==External links==

{{Cinema of West Bengal}}
{{Cinema of Bangladesh}}
{{Cinema of India}}
{{World cinema}}

[[Category:Cinema of Bengal]]
[[Category:Bengali language]]
[[Category:1999 books]]
[[Category:Bengali-language media]]
[[Category:English-language media]]
[[Category:Film guides]]
[[Category:Archives in India]]
[[Category:Directories]]


{{film-org-stub}}</text>
      <sha1>mhwk0re7q1uw5t1dturemceu2rwoc7o</sha1>
    </revision>
  </page>
  <page>
    <title>Web query classification</title>
    <ns>0</ns>
    <id>16350490</id>
    <revision>
      <id>720182341</id>
      <parentid>713166845</parentid>
      <timestamp>2016-05-14T06:33:32Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor />
      <comment>/* How to use the unlabeled query logs to help with query classification? */Remove blank line(s) between list items per [[WP:LISTGAP]] to fix an accessibility issue for users of [[screen reader]]s. Do [[WP:GENFIXES]] and cl using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9169" xml:space="preserve">{{Cleanup|date=March 2011}}
''' 
A Web query topic classification/categorization is a problem in [[information science]]. The task is to assign a [[Web search query]] to one or more predefined [[Categorization|categories]], based on its topics. The importance of query classification is underscored by many services provided by Web search. A direct application is to provide better search result pages for users with interests of different categories. For example, the users issuing a Web query “''apple''” might expect to see Web pages related to the fruit apple, or they may prefer to see products or news related to the computer company. Online advertisement services can rely on the query classification results to promote different products more accurately. Search result pages can be grouped according to the categories predicted by a query classification algorithm. However, the computation of query classification is non-trivial. Different from the [[document classification]] tasks, queries submitted by Web search users are usually short and ambiguous; also the meanings of the queries are evolving over time. Therefore, query topic classification is much more difficult than traditional document classification tasks.

== KDDCUP 2005 ==

KDDCUP 2005 competition&lt;ref&gt;[http://www.kdd.org/kdd-cup/view/kdd-cup-2005 KDDCUP 2005 dataset]&lt;/ref&gt; highlighted the interests in query classification. The objective of this competition is to classify 800,000 real user queries into 67 target categories. Each query can belong to more than one target category. As an example of a QC task, given the query “''apple''”, it should be classified into ranked categories: “''Computers \ Hardware''; ''Living \ Food &amp; Cooking''”.

{| class="wikitable"
|-
! Query
! Categories
|-
| apple
| Computers \ Hardware&lt;br /&gt;Living \ Food &amp; Cooking
|-
| FIFA 2006
| Sports \ Soccer&lt;br /&gt;Sports \ Schedules &amp; Tickets&lt;br /&gt;Entertainment \ Games &amp; Toys
|-
| cheesecake recipes
| Living \ Food &amp; Cooking&lt;br /&gt;Information \ Arts &amp; Humanities
|-
| friendships poem
| Information \ Arts &amp; Humanities&lt;br /&gt;Living \ Dating &amp; Relationships
|}

[[Image:Web query length.gif]]
[[Image:Web query meaning.gif]]

== Difficulties ==

Web query topic classification is to automatically assign a query to some predefined categories. Different from the traditional document classification tasks, there are several major difficulties which hinder the progress of Web query understanding:

=== How to derive an appropriate feature representation for Web queries? ===

Many queries are short and query terms are noisy. As an example, in the KDDCUP 2005 dataset, queries containing 3 words are most frequent (22%). Furthermore, 79% queries have no more than 4 words. A user query often has multiple meanings. For example, "''apple''" can mean a kind of fruit or a computer company. "''Java''" can mean a programming language or an island in Indonesia. In the KDDCUP 2005 dataset, most of the queries contain more than one meaning. Therefore, only using the keywords of the query to set up a [[vector space model]] for classification is not appropriate.

* Query-enrichment based methods&lt;ref&gt;Shen et al.  [http://www.sigkdd.org/sites/default/files/issues/7-2-2005-12/KDDCUP2005Report_Shen.pdf "Q2C@UST: Our Winning Solution to Query Classification"]. ''ACM SIGKDD Exploration, December 2005, Volume 7, Issue 2''.&lt;/ref&gt;&lt;ref&gt;Shen et al. [http://portal.acm.org/ft_gateway.cfm?id=1165776 "Query Enrichment for Web-query Classification"]. ''ACM TOIS, Vol. 24, No. 3, July 2006''.&lt;/ref&gt; start by enriching user queries to a collection of text documents through [[search engines]]. Thus, each query is represented by a pseudo-document which consists of the snippets of top ranked result pages retrieved by search engine. Subsequently, the text documents are classified into the target categories using synonym based classifier or statistical classifiers, such as [[Naive Bayes]] (NB) and [[Support Vector Machines]] (SVMs).

How about disadvantages and advantages??
give the answers:

=== How to adapt the changes of the queries and categories over time? ===

The meanings of queries may also evolve over time. Therefore, the old labeled training queries may be out-of-data and useless soon. How to make the classifier adaptive over time becomes a big issue. For example, the word "''Barcelona''" has a new meaning of the new micro-processor of AMD, while it refers to a city or football club before 2007. The distribution of the meanings of this term is therefore a function of time on the Web.

* Intermediate taxonomy based method&lt;ref&gt;Shen et al.  [http://portal.acm.org/ft_gateway.cfm?id=1148196 "Building bridges for web query classification"]. ''ACM SIGIR, 2006''.&lt;/ref&gt; first builds a bridging classifier on an intermediate taxonomy, such as [[Open Directory Project]] (ODP), in an offline mode. This classifier is then used in an online mode to map user queries to the target categories via the intermediate taxonomy. The advantage of this approach is that the bridging classifier needs to be trained only once and is adaptive for each new set of target categories and incoming queries.

=== How to use the unlabeled query logs to help with query classification? ===

Since the manually labeled training data for query classification is expensive, how to use a very large web search engine query log as a source of unlabeled data to aid in automatic query classification becomes a hot issue. These logs record the Web users' behavior when they search for information via a search engine. Over the years, query logs have become a rich resource which contains Web users' knowledge about the World Wide Web.

* Query clustering method&lt;ref&gt;Wen et al. [http://portal.acm.org/ft_gateway.cfm?id=503108 "Query Clustering Using User Logs"], ''ACM TOIS, Volume 20, Issue 1, January 2002''.&lt;/ref&gt; tries to associate related queries by clustering “session data”, which contain multiple queries and click-through information from a single user interaction. They take into account terms from result documents that a set of queries has in common. The use of query keywords together with session data is shown to be the most effective method of performing query clustering.
* Selectional preference based method&lt;ref&gt;Beitzel et al. [http://portal.acm.org/ft_gateway.cfm?id=1229183 "Automatic Classification of Web Queries Using Very Large Unlabeled Query Logs"], ''ACM TOIS, Volume 25, Issue 2, April 2007''.&lt;/ref&gt; tries to exploit some [[association rules]] between the query terms to help with the query classification. Given the training data, they exploit several classification approaches including exact-match using labeled data, N-Gram match using labeled data and classifiers based on perception. They emphasize on an approach adapted from computational linguistics named selectional preferences. If x and y form a pair (x; y) and y belongs to category c, then all other pairs (x; z) headed by x belong to c. They use unlabeled query log data to mine these rules and validate the effectiveness of their approaches on some labeled queries.

== Applications ==

* '''[[metasearch|Metasearch engines]]''' send a user's query to multiple search engines and blend the top results from each into one overall list. The search engine can organize the large number of Web pages in the search results, according to the potential categories of the issued query, for the convenience of Web users' navigation.
* '''[[Vertical search]]''', compared to general search, focuses on specific domains and addresses the particular information needs of niche audiences and professions. Once the search engine can predict the category of information a Web user is looking for, it can select a certain vertical search engine automatically, without forcing the user to access the vertical search engine explicitly.
* '''[[Online advertising]]'''&lt;ref&gt;[http://www.kdd2007.com/workshops.html#adkdd Data Mining and Audience Intelligence for Advertising (ADKDD'07)], KDD workshop 2007&lt;/ref&gt;&lt;ref&gt;[http://research.yahoo.com/workshops/troa-2008/ Targeting and Ranking for Online Advertising (TROA'08)], WWW workshop 2008&lt;/ref&gt; aims at providing interesting advertisements to Web users during their search activities. The search engine can provide relevant advertising to Web users according to their interests, so that the Web users can save time and effort in research while the advertisers can reduce their advertising costs.
All these services rely on the understanding Web users' search intents through their Web queries.

== See also ==

* [[Document classification]]
* [[Web search query]]
* [[Information retrieval]]
* [[Query expansion]]
* [[Naive Bayes classifier]]
* [[Support vector machines]]
* [[Meta search]]
* [[Vertical search]]
* [[Online advertising]]

== References ==

{{reflist}}

== Further reading ==
* Shen.  [http://lbxml.ust.hk/th/th_search.pl?smode=VIEWBYCALLNUM&amp;skeywords=CSED%202007%20Shen "Learning-based Web Query Understanding"]. ''Phd Thesis'', ''HKUST'', June 2007.
{{Internet search}}

{{DEFAULTSORT:Web Query Classification}}
[[Category:Information retrieval techniques]]
[[Category:Internet search]]</text>
      <sha1>fhmja596w3lbuch2sv0qcbvudiqxy3x</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Real-time web</title>
    <ns>14</ns>
    <id>23686083</id>
    <revision>
      <id>470667521</id>
      <parentid>389252365</parentid>
      <timestamp>2012-01-10T19:53:22Z</timestamp>
      <contributor>
        <username>Gray eyes</username>
        <id>12633548</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="93" xml:space="preserve">{{Cat main|Real-time web}}

[[Category:Internet search]]
[[Category:Real-time computing|web]]</text>
      <sha1>62goeto6e11tzkpku3mefzgzzsok7m7</sha1>
    </revision>
  </page>
  <page>
    <title>Sponsored search auction</title>
    <ns>0</ns>
    <id>23022154</id>
    <revision>
      <id>709294939</id>
      <parentid>663625159</parentid>
      <timestamp>2016-03-10T04:13:09Z</timestamp>
      <contributor>
        <ip>14.139.180.66</ip>
      </contributor>
      <comment>/* Unthruthfulness */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5060" xml:space="preserve">A '''sponsored search auction (SSA)''', also known as a '''keyword auction''', is an indispensable part of the [[business model]] of modern [[web host]]s. It refers to results from a search engine that are not output by the main search algorithm, but
rather clearly separate advertisements paid for by third parties. These advertisements
are typically related to the terms for which the user searched. They come in the form
of a link and some information, with the hope that a search user will click on the link
and buy something from the advertiser.
In sponsored search auctions, there are typically some fixed number of slots for advertisements and more advertisers that want these slots than there are slots. The advertisers
have different valuations for these slots and slots are a scarce resource, so an auction
is done to determine how the slots will be assigned.

==History==
Prior to 1998, many advertisements were charged by impression, as it was the
easiest metric to calculate. In 1998, GoTo.com, Inc debuted a pay-per-click charging
system, with pricing and slot placement determined by an auction. GoTo used a first
price auction, where bidders were placed according to their bids and charged their bids
when they won. GoTo faced bidders who were constantly changing their bid
in response to new information and changing information from other bidders.

Currently, charging per action is a common pricing scheme in affiliate networks,
such as the Amazon Associates Program.

In 2002, [[Google AdWords]] began using a second price auction to sell the single advertisement
slot. Shortly thereafter, pages had multiple advertisements slots, which were allocated
and sold via [[generalized second-price auction]] (GSP) auction, the natural generalization of a second price, single item, multi bidder
auction.&lt;ref&gt;Hal Varian, Christopher Harris. The VCG Auction in Theory and Practice, In The
American Economic Review, Volume 104, Issue 5, pages 442-452. Elsevier B.V.,
2014.&lt;/ref&gt;

==Auction Mechanisms==
===Generalized Second Price Auction===
[[Generalized second-price auction]] (GSP) is the most commonly used auction mechanism for sponsored search.

====Untruthfulness====
An issue with GSP is that it's not a truthful auction and it is not the optimal strategy. To illustrate this, consider the following example.

There are three bidders with only two possible slots. The values of
each bidders 1, 2, and 3 are $10, $5, and $3 respectively. Suppose that the first slot click
through rate (CTR) is 300 and the second slot CTR is 290. If bidder 1 is truthful, he
would have to pay &lt;math&gt;p_1 = $5(300) = $1500&lt;/math&gt; for a utility of &lt;math&gt;u_1 = $10(300)-$1500 = $1500&lt;/math&gt;.
However, if bidder 1 decides to lie and reports a value of $4 instead then his utility
would be &lt;math&gt;u_2 = $10(290) - $3(290) = $2030&lt;/math&gt;. Notice that &lt;math&gt;u_2 &gt; u_1&lt;/math&gt; which makes GSP
untruthful and bidders have an incentive to lie.

====Quality Variant====
Google uses a minor variant of GSP to auction off advertisement slots. Potential
advertisements may be of varying quality. Suppose that there are two advertisements
for eggs. One advertisement simply fills its space with the word “egg” repeated over
and over, while the other advertisement shows a picture of eggs, contains branding
information, and mentions positive qualities about their eggs, such as cage-freeness.
The second advertisement may be thought of as having higher quality than the first
advertisement, being more useful to consumers, more likely to be clicked on, and more
likely to generate revenue for both the advertiser and Google. Advertisements that
have a history of high click through rates, are geographically targeted at the user, or
have a high quality landing page may also be thought of as having higher quality.&lt;ref&gt;Google AdWords, Check and understand Quality Score. support.google.com/adwords/answer/2454010&lt;/ref&gt;

Google assigns a numeric “quality” score &lt;math&gt;\gamma_i&lt;/math&gt; to each bidder &lt;math&gt;i&lt;/math&gt;. Bidders, rather than
being ordered purely by their bid, are instead ordered by rank, which is the product
of their bid and quality score &lt;math&gt;\gamma_1 b_1 \geq \gamma_2 b_2 \geq \dots \geq \gamma_n b_n&lt;/math&gt; . Slots are still assigned in
decreasing rank order. Bidders are charged, rather than the bid of the bidder one rank
lower (&lt;math&gt;p_i(b_i, b_{-i}) = b_{i+1}&lt;/math&gt;), are charged the minimum price for which, if it was their bid,
would keep them in their current rank: &lt;math&gt;p_i(b_i, b_{-i}) = \frac{\gamma_{i+1}b_{i+1}}{\gamma_i}&lt;/math&gt;

===Vickrey–Clarke–Groves Auction===
[[Vickrey–Clarke–Groves auction]] (VCG) is a truthful auction optimizing social welfare. VCG is more complicated to explain than GSP and that might deter many websites from using a VCG auction mechanism even though it's truthful. However, some websites use VCG as their auction mechanism, most notably [[Facebook]].

==See also==
*[[Generalized second-price auction]]
*[[Vickrey–Clarke–Groves auction]]

==References==
{{reflist}}

[[Category:Internet search]]</text>
      <sha1>s7ajt0123zrqbpup87tdzr4arjmotqj</sha1>
    </revision>
  </page>
  <page>
    <title>Mystery Seeker</title>
    <ns>0</ns>
    <id>25176438</id>
    <revision>
      <id>743413388</id>
      <parentid>689145227</parentid>
      <timestamp>2016-10-09T15:03:03Z</timestamp>
      <contributor>
        <username>Prell</username>
        <id>50792</id>
      </contributor>
      <comment>Site doesn't return a usable response.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4081" xml:space="preserve">{{Infobox website
| name               = Mystery Seeker
| logo               = MYSTERYSEEKER logo.jpeg
| logo_alt           = The mysteryseeker.com logo
| logocaption       = The logo found at [http://www.mysteryseeker.com mysteryseeker.com], decorated with fog and a moon in the background.
| screenshot         = mysteryseeker.com screenshot.jpeg
| collapsible        = y
| screenshot_alt     = Screenshot of [http://www.mysteryseeker.com mysteryseeker.com]
| caption            = Screenshot of [http://www.mysteryseeker.com mysteryseeker.com].
| url                =  {{URL|www.mysteryseeker.com}}
| slogan             = “What will you search for?”
| commercial         = &lt;!-- "Yes", "No" or leave blank --&gt;
| type               = Search engine
| registration       = None
| language           = [[English language|English]]
| content_license    = &amp;copy; Mystery Seeker, 2009
| owner              = Mystery Seeker
| launch_date        = {{start date and age|2009|10|02|df=yes}}
| alexa              = {{decrease}} [http://www.alexa.com/siteinfo/mysteryseeker.com 3110015] (Global, November 2015)
| current_status     = Online but defunct 
| footnotes          = 
}}
'''Mystery Seeker''' is a website based on the [[Google]] search engine.&lt;ref name="Chivers 2009" /&gt; Until November 30, 2009, the website was known as '''Mystery Google''', but on December 1, 2009, the name changed to '''Mystery Seeker'''. It has been featured in a number of technology blogs.&lt;ref&gt;{{cite news| url=http://www.huffingtonpost.com/2009/10/12/mystery-google-surprise-y_n_318089.html | work=Huffington Post | first=Bianca | last=Bosker | title=Mystery Google: Surprise Yourself With Someone Else's Search Results | date=October 12, 2009}}&lt;/ref&gt;&lt;ref&gt;[http://mashable.com/2009/10/12/mystery-google/ Mystery Google: The “I’m Feeling Lucky” Button Re-imagined]&lt;/ref&gt;&lt;ref&gt;[http://www.geekologie.com/2009/11/i_wasnt_looking_for_that_myste.php I Wasn't Looking For That: Mystery Google Gives You Previous Person's Search Query | Geekologie]&lt;/ref&gt; Upon a search query, Mystery Seeker returns the results from the previous search, so “you get what the person before you searched for.”&lt;ref name="Chivers 2009"&gt;{{Cite news|url=http://www.telegraph.co.uk/technology/google/6316140/Mystery-google-returns-other-peoples-search-results.html|title=Mystery Google returns other people's search results|accessdate=2009-11-23|publisher=The Telegraph|date=13 Oct 2009|author=Tom Chivers|location=London}}&lt;/ref&gt;

There is a trend among the people on Mystery Seeker to add so-called "missions", where the next user is asked to do something. For example, "Your mission is to copy and paste this until you see it again. Then and only then will you be a true ninja".&lt;ref&gt;[http://www.softsailor.com/news/12457-how-to-receivegive-google-mystery-missions-and-why-they-are-fun.html Tech Source]&lt;/ref&gt; Other examples of possible missions include telling someone you love them, sending someone a get well card, mailing a banana to someone, etc. There are also references to [[MyLifeIsAverage|MLIA]]. Due to the high number of posted missions involving phone numbers, Mystery Seeker received enough complaints to remove phone numbers from the site. However, the developers are testing Mystery Missions Beta in order to allow the continuance of missions.

A number of phrases yield intentional responses ([[easter egg (media)|easter egg]]s).

In November 2009 Mystery Seeker had 440,000 unique visitors,&lt;ref&gt;[http://siteanalytics.compete.com/mysterygoogle.com/ mysterygoogle.com UVs for November 2012 | Compete]&lt;/ref&gt; making it one of the most highly trafficked social entertainment sites online.

Google has not commented on any possible connection to the site.&lt;ref name="Chivers 2009" /&gt; The [[domain name]] ''www.mysterygoogle.com'' is registered to a private registrant {{As of|2009|10|2|lc=on}}.&lt;ref&gt;http://whois.domaintools.com/mysterygoogle.com&lt;/ref&gt;

== References ==
{{Reflist|30em}}

== External links ==
* [http://www.mysteryseeker.com/ Mystery Seeker site]

[[Category:Websites]]
[[Category:Internet search]]</text>
      <sha1>os6rq49fxawe28x2dld864au0ij5t1k</sha1>
    </revision>
  </page>
  <page>
    <title>Search/Retrieve via URL</title>
    <ns>0</ns>
    <id>8465350</id>
    <revision>
      <id>681050270</id>
      <parentid>679599001</parentid>
      <timestamp>2015-09-14T21:31:03Z</timestamp>
      <contributor>
        <username>Scott</username>
        <id>9867</id>
      </contributor>
      <comment>[[Help:Cat-a-lot|Cat-a-lot]]: Moving from [[Category:Uniform resource locator]] to [[Category:Uniform Resource Locator]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4641" xml:space="preserve">{{MOS|date=September 2015|reason=OMG! =)}}
'''Search/Retrieve via URL''' ('''SRU''') is a standard search protocol for [[Internet search]] queries, utilizing [[Contextual Query Language]] (CQL), a standard query syntax for representing queries.

Samplecode of a complete answer for this SRU Query-URL:
http://philosophy-science-humanities-controversies.com/XML/sru.php?version=1.1&amp;operation=searchRetrieve&amp;query=dc.title=Darwinism 

&lt;pre&gt;
&lt;?xml version="1.0"?&gt;
&lt;sru:searchRetrieveResponse xmlns:sru="http://www.loc.gov/zing/srw/" xmlns:diag="http://www.loc.gov/zing/srw/diagnostic/" xmlns:xcql="http://www.loc.gov/zing/cql/xcql/" xmlns:dc="http://purl.org/dc/elements/1.1/"&gt;
  &lt;sru:version&gt;1.1&lt;/sru:version&gt;
  &lt;sru:numberOfRecords&gt;4&lt;/sru:numberOfRecords&gt;
  &lt;sru:records&gt;
    &lt;sru:record&gt;
      &lt;sru:recordSchema&gt;info:srw/schema/1/dc-v1.1&lt;/sru:recordSchema&gt;
      &lt;sru:recordPacking&gt;XML&lt;/sru:recordPacking&gt;
      &lt;sru:recordData&gt;
        &lt;sru:dc&gt;
          &lt;sru:title&gt;Darwinism&lt;/sru:title&gt;
          &lt;sru:creator&gt;Dennett&lt;/sru:creator&gt;
          &lt;sru:subject&gt;The rule of the Local is a basic principle of Darwinism -  it corresponds to the principle that there is no Creator, no intelligent foresight. I 262&lt;/sru:subject&gt;
        &lt;/sru:dc&gt;
      &lt;/sru:recordData&gt;
      &lt;sru:recordNumber&gt;1&lt;/sru:recordNumber&gt;
    &lt;/sru:record&gt;
    &lt;sru:record&gt;
      &lt;sru:recordSchema&gt;info:srw/schema/1/dc-v1.1&lt;/sru:recordSchema&gt;
      &lt;sru:recordPacking&gt;XML&lt;/sru:recordPacking&gt;
      &lt;sru:recordData&gt;
        &lt;sru:dc&gt;
          &lt;sru:title&gt;Darwinism&lt;/sru:title&gt;
          &lt;sru:creator&gt;McGinn&lt;/sru:creator&gt;
          &lt;sru:subject&gt;Design argument/William Paley: organisms have a brilliant design: We have not designed them, so we have to assume that a foreign intelligence did it. Let s call this intelligence "God". So God exists. II 98
DarwinVsPaley: intelligent design does not require a Creator. Selection is sufficient. II 98
Mind/consciousness/evolution/McGinn: evolution does not explain consciousness! nor sensations. II 99
 Reason: sensation and consciousness cannot be explained through the means of Darwinian principles and physics, because if selection were to explain how sensations are supposed to be created by it, it must be possible to mold the mind from matter. II 100
 (s) Consciousness or sensations would have to be visible for selection! (Similar GouldVsDawkins)&lt;/sru:subject&gt;
        &lt;/sru:dc&gt;
      &lt;/sru:recordData&gt;
      &lt;sru:recordNumber&gt;2&lt;/sru:recordNumber&gt;
    &lt;/sru:record&gt;
    &lt;sru:record&gt;
      &lt;sru:recordSchema&gt;info:srw/schema/1/dc-v1.1&lt;/sru:recordSchema&gt;
      &lt;sru:recordPacking&gt;XML&lt;/sru:recordPacking&gt;
      &lt;sru:recordData&gt;
        &lt;sru:dc&gt;
          &lt;sru:title&gt;Darwinism&lt;/sru:title&gt;
          &lt;sru:creator&gt;Putnam&lt;/sru:creator&gt;
          &lt;sru:subject&gt;Rorty: Darwinism / Putnam: he does noit like the image of man as a more complicated animal  (scientistic and reductionist physicalism).
Rorty VI 63&lt;/sru:subject&gt;
        &lt;/sru:dc&gt;
      &lt;/sru:recordData&gt;
      &lt;sru:recordNumber&gt;3&lt;/sru:recordNumber&gt;
    &lt;/sru:record&gt;
    &lt;sru:record&gt;
      &lt;sru:recordSchema&gt;info:srw/schema/1/dc-v1.1&lt;/sru:recordSchema&gt;
      &lt;sru:recordPacking&gt;XML&lt;/sru:recordPacking&gt;
      &lt;sru:recordData&gt;
        &lt;sru:dc&gt;
          &lt;sru:title&gt;Darwinism&lt;/sru:title&gt;
          &lt;sru:creator&gt;Rorty&lt;/sru:creator&gt;
          &lt;sru:subject&gt;Darwinism/Rorty provides a useful vocabulary. "Darwinism": for me is a fable about human beings as animals with special skills and organs. But these organs and skills are just as little in a representational relation to the world as the anteater s snout. VI 69 ff
Darwinism/Rorty: it demands that we consider our doing and being as part of the same continuum, which also includes the existence of amoebae, spiders and squirrels. One way to do that is to say that our experience is just more complex. VI 424&lt;/sru:subject&gt;
        &lt;/sru:dc&gt;
      &lt;/sru:recordData&gt;
      &lt;sru:recordNumber&gt;4&lt;/sru:recordNumber&gt;
    &lt;/sru:record&gt;
  &lt;/sru:records&gt;
&lt;/sru:searchRetrieveResponse&gt;
&lt;/pre&gt;
==See also==
* [[Search/Retrieve Web Service]]

==External links==
* [http://www.loc.gov/standards/sru/ Search/Retrieve via URL] at [[Library of Congress]]
* http://www.philosophy-science-humanities-controversies.com/XML/index.php Sample Page from the Lexicon of Arguments.
* http://philosophy-science-humanities-controversies.com/XML/sru.php?version=1.1&amp;operation=searchRetrieve&amp;query=dc.title=Darwinism This is a complete example with query and answer.
{{Internet search}}

{{DEFAULTSORT:Search Retrieve via URL}}
[[Category:Internet search]]
[[Category:Information retrieval techniques]]
[[Category:Uniform Resource Locator]]

{{web-stub}}</text>
      <sha1>nnfopvhbporebhgy3r2wfk24kl5ll0n</sha1>
    </revision>
  </page>
  <page>
    <title>Hybrid search engine</title>
    <ns>0</ns>
    <id>2851233</id>
    <revision>
      <id>697148404</id>
      <parentid>666712831</parentid>
      <timestamp>2015-12-28T15:53:08Z</timestamp>
      <contributor>
        <ip>170.163.6.2</ip>
      </contributor>
      <comment>eLocalFinder.com(http://elocalfinder.com/HSearch.aspx) is one of the website that uses Hybrid Search</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="737" xml:space="preserve">{{Notability|date=December 2009}}
A '''hybrid search engine''' ('''HSE''') is a type of [[computer]] [[search engine]] that uses different types of data with or without ontologies to produce the [[algorithm]]ically generated results based on [[web crawling]]. Previous types of search engines only use text to generate their results.
Hybrid search engines use a combination of both crawler-based results and directory results. More and more search engines these days are moving to a hybrid-based model.
==References==
{{No footnotes|date=April 2010}}
*http://eprints.ecs.soton.ac.uk/17457/
*http://eprints.whiterose.ac.uk/3771/
*http://www.picollator.com
*http://elocalfinder.com/HSearch.aspx

[[Category:Internet search]]


{{web-stub}}</text>
      <sha1>b0d1ut17n7tfrh1zc5hld3dcjo8p8vx</sha1>
    </revision>
  </page>
  <page>
    <title>Notey</title>
    <ns>0</ns>
    <id>49177882</id>
    <revision>
      <id>734493892</id>
      <parentid>724196904</parentid>
      <timestamp>2016-08-14T18:52:08Z</timestamp>
      <contributor>
        <username>Josve05a</username>
        <id>12023796</id>
      </contributor>
      <minor />
      <comment>cleanup, added [[CAT:O|orphan]] tag using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2187" xml:space="preserve">{{Orphan|date=August 2016}}

{{Infobox Website
| name           = Notey
| logo           = [[Image:Notey (logo).png|220px]]
| screenshot     = 
| caption        = 
| url            = [http://www.notey.com/ notey.com]
| commercial     = 
| type           = Blog discovery platform
| language       = English
| registration   = 
| owner          = 
| launch date    = February 2015
| current status = active
| revenue        = 
| slogan         = 
| alexa          =  
}}

'''Notey''' is a [[:Category:Blog search engines|blog search]] and discovery platform founded in 2015 that helps users discover non-mainstream content and blogs. The platform ranks and features both bloggers and independent publishers on various topics including [[technology]], [[weddings]], [[sneakers]] and more than 500,000 others. Users can upvote articles they like, save them in notebooks and see what the community is reading.&lt;ref&gt;{{cite web | url=http://techcrunch.com/2015/02/17/notey | title=Notey Raises $1.6 Million For Its Topic-Focused Blog Directory  | publisher=Techcrunch | date=February 17, 2015 | accessdate=2016-01-21}}&lt;/ref&gt;

In April 2015, Business Insider named Notey “one of the fastest growing startups in the world still flying under the radar”.&lt;ref&gt;{{cite web | url= http://uk.businessinsider.com/15-of-the-fastest-growing-b2b-startups-2015-4?op=1 | title=15 of the fastest growing startups in the world still flying under the radar  | publisher=Business Insider | date=April 17, 2015 | accessdate=2016-01-21}}&lt;/ref&gt;

The company is based in Hong Kong and San Francisco.&lt;ref&gt;{{cite web | url= https://www.crunchbase.com/organization/notey | title=Notey &amp;#124; CrunchBase | year=2015 | publisher=CrunchBase | accessdate=2016-01-21}}&lt;/ref&gt; Its investors include [[Hugo Barra]], [[Ryan Holmes]], Shakil Khan and [[Steve Kirsch]].

==References==
{{reflist}}

==External links==
* [http://www.notey.com/ Notey Home Page]

{{Commons category|Internet search engines}}

[[Category:Blog search engines]]
[[Category:Websites|Search engines]]
[[Category:Internet search]]
[[Category:Aggregation websites]]
[[Category:Search engine software]]


{{searchengine-website-stub}}
{{US-company-stub}}</text>
      <sha1>o6v718j4ua2hku2k7sv13k0zxrl0yq1</sha1>
    </revision>
  </page>
  <page>
    <title>Online search</title>
    <ns>0</ns>
    <id>31385661</id>
    <revision>
      <id>758281414</id>
      <parentid>749191378</parentid>
      <timestamp>2017-01-04T14:36:48Z</timestamp>
      <contributor>
        <username>Fixuture</username>
        <id>19796795</id>
      </contributor>
      <comment>new key for [[Category:Internet search]]: " " using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="973" xml:space="preserve">'''Online search''' is the process of interactively searching for and retrieving requested information via a computer from [[database]]s that are [[online]].&lt;ref name="whatis?"&gt;{{cite journal|last1=Hawkins|first1= Donald T.|last2= Brown|first2= Carolyn P.|date=Jan 1980|title=What Is an Online Search?|journal=Online|volume=4|issue=1|pages=12–18|id=Eric:EJ214713| accessdate=2011-04-04}}&lt;/ref&gt; Interactive searches became possible in the 1980s with the advent of faster databases and [[smart terminal]]s.&lt;ref name="whatis?"/&gt; In contrast, [[computerized batch searching]] was prevalent in the 1960s and 1970s.&lt;ref name="whatis?"/&gt; Today, searches through [[web search engine]]s constitute the majority of online searches.

Online searches often supplement reference transactions.{{cn|date=June 2015}}

==References==
{{reflist}}

{{Internet search}}

[[Category:Internet terminology]]
[[Category:Information retrieval genres]]
[[Category:Internet search| ]]

{{web-stub}}</text>
      <sha1>idvwujij49z03js03e19alrzazycrkc</sha1>
    </revision>
  </page>
  <page>
    <title>Figaro Systems</title>
    <ns>0</ns>
    <id>17910258</id>
    <revision>
      <id>757564542</id>
      <parentid>669709633</parentid>
      <timestamp>2016-12-31T11:24:26Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 1 sources and tagging 0 as dead. #IABot (v1.2.7.1)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10891" xml:space="preserve">{{Infobox company
|name = Figaro Systems, Inc.
|logo = [[Image:Figaro-logo.png|Figaro logo]]
|type = [[Privately held company|Private]]
|foundation = 1993
|location_city = [[Santa Fe, New Mexico|Santa Fe]], [[New Mexico]]
|location_country =[[United States]]
|key_people = Patrick Markle, [[president]] and [[CEO]], [[Geoff Webb]], [[vice president|VP]]
|homepage = [http://www.figarosystems.com figarosystems.com]
}}

'''Figaro Systems, Inc.''' is an American company that provides  seatback and [[wireless]] titling [[software]] and system installations to [[opera houses]] and other music performance venues worldwide. The company is based in [[Santa Fe, New Mexico|Santa Fe]], New Mexico. It was established in 1993 &lt;ref&gt;Andrew Webb, “Opera Subtitle Firm Eyes New Game,” ''New Mexico Business Weekly'', Nov. 21, 2003 [http://www.bizjournals.com/albuquerque/stories/2003/11/24/story2.html]&lt;/ref&gt;
by Patrick Markle, [[Geoff Webb]], and Ron Erkman  &lt;ref name="figaro-systems.com"/&gt; and was the first company to provide [[assistive technology]] that enables individualized, simultaneous, multi-lingual [[dialogue]] and [[libretto]]-reading for audiences.
&lt;ref&gt;[http://www.highbeam.com/DocPrint.aspx?DocID=1P2:115622912 David Belcher, “Nothing Lost in Translation: [[Video]] system allows patrons to read words on chair backs,”] ''Albuquerque Journal'', June 4, 2006&lt;/ref&gt;

==History==
Figaro Systems grew out of a conversation in 1992 among three opera colleagues: Patrick Markle, at that time Production Director of The [[Santa Fe Opera]], Geoffrey Webb, Design Engineer for the [[Metropolitan Opera House (Lincoln Center)|Metropolitan Opera House]] in New York, and Ronald Erkman, then a technician for the Met. At that time, opera houses had two options for the display of libretto and dialogue subtitles: projection onto a large screen above the stage or onto smaller screens throughout the theatre. Typically, the translation was in a single language.&lt;ref&gt;[http://www.bizjournals.com/albuquerque/stories/2005/04/11/story5.html?q=Figaro%20Systems Dennis Domrzalski, "Figaro: Eyes translate when ears don't get it",] ''New Mexico Business Weekly'', April 8, 2005&lt;/ref&gt;

The [[Americans with Disabilities Act of 1990]] had recently been enacted; Markle was trying to solve the problem of venues which lacked accessibility to patrons with disabilities, including the profoundly [[deaf]].  Markle, Webb, and Erkman devised the first [[prototype]] of a personal seatback titling device and [[John Crosby (conductor)|John Crosby]], then General Director of The [[Santa Fe Opera]], saw its potential for opera patrons.&lt;ref name="figaro-systems.com"&gt;[http://www.figaro-systems.com/about.php  Figaro Systems Official Website]&lt;/ref&gt; Markle, Webb, and Erkman were further reinforced by their understanding of technology’s role in remediating the physical barriers people encounter, worldwide, which frustrate or prevent their access to the visual performing arts.&lt;ref&gt;[http://figarosystems.com/linkdownloads/052007_figaro_auditoria_article.pdf “[[User-friendly]] art: In-seat text displays that subtitle and translate”, ''Auditoria'', May 2007]&lt;/ref&gt; Markle, Webb, and Erkman applied for and were granted [[patent]]s for their invention.
&lt;ref&gt;[http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=11&amp;f=G&amp;l=50&amp;co1=AND&amp;d=PTXT&amp;s1=figaro.ASNM.&amp;OS=AN/figaro&amp;RS=AN/Figaro  United States Patent 5,739,869, "Electronic libretto display apparatus and method," issued April 14, 1998. [[United States Patent and Trademark Office]] ]&lt;/ref&gt;&lt;ref&gt;[http://www.lanl.gov/orgs/pa/News/050701.html  Los Alamos Laboratory, ''Daily News Bulletin'', May 7, 2001]&lt;/ref&gt;

Philanthropist and investor [[Alberto Vilar]] counted Figaro Systems among the companies in which he was a majority shareholder.&lt;ref&gt;[http://nymag.com/nymetro/arts/music/features/5616/ [[Robert Hilferty]], "A Knight at the Opera," ''[[New York Magazine]]'', January 14, 2002]&lt;/ref&gt;&lt;ref&gt;[http://biography.jrank.org/pages/3490/Vilar-Alberto-1940-Investor-Philanthropist-Privileges-Wealth.html  "Alberto Vilar: The Privileges of Wealth," ''The Free Encyclopedia'']&lt;/ref&gt;  He donated the company's [[electronic libretto]] system to European venues including the [[Royal Opera House]] in [[London]], La Scala's [[Teatro degli Arcimboldi]] opera houses in [[Milan, Italy|Milan]], Italy, [[Gran Teatre del Liceu]] in [[Barcelona, Spain|Barcelona]], Spain, and the [[Wiener Staatsoper]] in [[Wien]], [[Austria]]. As a consequence of his failures to pay promised donations, most of these companies lost money.

In 2005 the Met charged the New Mexico company with unlawfully using its name in advertising promoting its "Simultext, system which defendant claims can display a simultaneous translation of an opera as it occurs on a stage and that defendant represented that its system is installed at the Met." &lt;ref&gt;[http://classactionlitigation.com/library/consumerlaw2006update.html#_edn173#_edn173 Timothy E. Eble, ''Class Action Litigation Information''] on classactionlitigation.com&lt;/ref&gt;

==Products and technology==
The company’s products are known variously as seat back titles, [[surtitles]],
&lt;ref&gt;[http://app1.kuhf.org/houston_public_radio-news-display.php?articles_id=20614 Eric Skelly, "Surtitles at the Opera," ''Public Radio News and Information in Houston, Texas'', KUHF 88.7 FM Houston Public Radio] on app1.kuhf.org/&lt;/ref&gt; [[electronic libretto]] systems, opera supertitles, projected titles, and libretto translations.

Opera venues have utilized the system to display librettos in [[English language|English]], [[French language|French]], [[German language|German]], [[Italian language|Italian]], [[Japanese language|Japanese]], [[Mandarin Chinese|Mandarin]], [[Russian language|Russian]], and [[Spanish language|Spanish]]
&lt;ref&gt;[http://www.sandia.gov/news-center/news-releases/2005/tech-trans/smbusiness.html "Sandia helps 278 state businesses in 2004 through New Mexico Small Business Assistance Program," Sandia National Laboratories, Sandia Corporation, March 22, 2005] on sandia.gov&lt;/ref&gt; although the software enables the reading of the libretto in any [[written language]].
&lt;ref name="entertanmentengineering.com"&gt;[http://www.entertanmentengineering.com/v4.issue04/page.06.html  “Giving the Opera a New Voice,”] ''Entertainment Engineering," Volume 4, Issue 2, p. 6&lt;/ref&gt; Translation is provided by one screen and delivery system per person.&lt;ref&gt;[http://www.figarosystems.com  Figaro Systems Official Website]&lt;/ref&gt;

Typically, but not in all cases, the system is permanently installed along the backs of rows of seats. Each screen is positioned so that the text is clearly visible to each user. The displays were initially available in [[vacuum fluorescent display]], ([[Vacuum fluorescent display|VFD]]) and, in 2000, [[liquid crystal display]], ([[LCD]]) was used. In 2004 the displays became available with [[organic light-emitting diode]], ([[OLED]]) screens.  Each type of display provides the same text information and program annotation on eight channels simultaneously, may be turned off by the user, and is user-operated with a single button. The software is capable of supporting venues’ existing systems as well as Figaro Systems' "Simultext" system. The software enables cueing of each line as it is sung, and it appears instantly on the screen.&lt;ref name="entertanmentengineering.com"/&gt;

The company builds fully [[modular]] systems including its [[wireless]] [[handheld]] screens 
&lt;ref&gt;[http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=3&amp;f=G&amp;l=50&amp;co1=AND&amp;d=PTXT&amp;s1=figaro.ASNM.&amp;OS=AN/figaro&amp;RS=AN/Figaro  United States Patent 6,760,010. "Wireless electronic libretto display apparatus and method," issued July 6, 2004:] United States Patent and Trademark Office Patent Full-Text and Image Database&lt;/ref&gt; for users who cannot use seatback systems, for example people in [[wheelchair]]s, who may be viewing the opera in areas lacking seatback viewing, or people with compromised eyesight.

==Venues==
In the US, the company’s systems are in use in the [[Ellie Caulkins Opera House]] 
&lt;ref&gt;[http://www.highbeam.com/doc/1G1-135788390.html Marc Shulgold, "Opera dialogue shows on seat in front of you,"] ''Rocky Mountain News'' (Denver, Colorado), September 3, 2005 on highbeam.com,&lt;/ref&gt; in [[Denver, Colorado|Denver]], Colorado, The Santa Fe Opera in Santa Fe,&lt;ref&gt;[https://web.archive.org/web/20080512022822/http://www.santafeopera.org/yournite/operatitles.php  Santa Fe Opera, Santa Fe, NM. Cached webpage],&lt;/ref&gt; the [[Brooklyn Academy of Music]]&lt;ref&gt;[http://www.appliancemagazine.com/editorial.php?article=1768&amp;zone=210&amp;first=1  “An Operatic Performance,” ''Appliance Magazine'', June 2007],&lt;/ref&gt; the [[Metropolitan Opera]], New York, where it is called "MetTitles"),&lt;ref&gt;[http://www.figaro-systems.com/installations.php  Figaro Systems Official Website. Installations],&lt;/ref&gt; the [[Roy E. Disney]] Theatre in [[Albuquerque]]'s [[National Hispanic Cultural Center]], [[McCaw Hall]] in [[Seattle Washington]], the [[Opera Theatre of St. Louis]] in St. Louis, Missouri, the [[Des Moines Metro Opera]] in [[Des Moines, Iowa|Des Moines]], Iowa and the Lyric Opera of Kansas City,  Missouri.&lt;ref name="figaro-systems.com"/&gt;

In the UK and Europe, the systems have been installed in venues including the [[Royal Opera House]] in London, the [[Teatro alla Scala]] and La Scala's [[Teatro degli Arcimboldi]] opera houses in [[Milan, Italy|Milan]], Italy, the [[Gran Teatre del Liceu]] in [[Barcelona, Spain|Barcelona]], Spain, and the [[Wiener Staatsoper]] in [[Wien]], [[Austria]].
&lt;ref&gt;[http://www.entertainmentengineering.com/v4.issue04/page.06.html “Giving the Opera a New Voice,” ''Entertainment Engineering.'', Volume 4, Issue 2, p. 6], on entertainmentengineering.com&lt;/ref&gt;

==Awards==
In 2001, the company won the [[Los Alamos, New Mexico|Los Alamos]] Laboratories’ Technology Commercialization Award for its Simultext system.&lt;ref&gt;[http://www.lanl.gov/news/index.php/fuseaction/home.story/story_id/1170 Todd Hanson, "Los Alamos announces technology commercialization awards," ''Los Alamos National Laboratory News''], Los Alamos National Security, LLC, US Department of Energy's NNSA, May 7, 2001 on lanl.gov/news.&lt;/ref&gt;
In 2008, the company’s software was one of four finalists for the Excellence Award for Commercial Software awarded by the New Mexico Information Technology and Software Association.

==References==
{{Reflist}}

[[Category:Assistive technology]]
[[Category:Companies based in Santa Fe, New Mexico]]
[[Category:Companies established in 1993]]
[[Category:Educational technology companies]]
[[Category:Information retrieval organizations]]
[[Category:Privately held companies based in New Mexico]]
[[Category:Software companies based in New Mexico]]</text>
      <sha1>gyaxrxh3bhd23tjb5drzwk6myfo52j8</sha1>
    </revision>
  </page>
  <page>
    <title>Text Retrieval Conference</title>
    <ns>0</ns>
    <id>1897206</id>
    <revision>
      <id>751208525</id>
      <parentid>723140423</parentid>
      <timestamp>2016-11-24T02:02:38Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <minor />
      <comment>/* top */ adjust bold</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="12826" xml:space="preserve">{{Other uses of|TREC|TREC (disambiguation)}}
The '''Text REtrieval Conference''' ('''TREC''') is an ongoing series of [[workshop]]s focusing on a list of different [[information retrieval]] (IR) research areas, or ''tracks.'' It is co-sponsored by the [[National Institute of Standards and Technology]] (NIST) and the [[Intelligence Advanced Research Projects Activity]] (part of the office of the [[Director of National Intelligence]]), and began in 1992 as part of the [[DARPA TIPSTER Program|TIPSTER Text program]]. Its purpose is to support and encourage research within the information retrieval community by providing the infrastructure necessary for large-scale ''evaluation'' of [[text retrieval]] methodologies and to increase the speed of lab-to-product [[technology transfer|transfer of technology]].

Each track has a challenge wherein NIST provides participating groups with data sets and test problems. Depending on track, test problems might be questions, topics, or target extractable [[Features (pattern recognition)|features]]. Uniform scoring is performed so the systems can be fairly evaluated. After evaluation of the results, a workshop provides a place for participants to collect together thoughts and ideas and present current and future research work.

== Tracks ==

===Current tracks===
''New tracks are added as new research needs are identified, this list is current for TREC 2016.''&lt;ref&gt;http://trec.nist.gov/pubs/call2016.html&lt;/ref&gt;
* [http://www.trec-cds.org/ Clinical Decision Support Track] - '''Goal:''' to investigate techniques for linking medical cases to information relevant for patient care
* [http://sites.google.com/site/treccontext/ Contextual Suggestion Track] - '''Goal:''' to investigate search techniques for complex information needs that are highly dependent on context and user interests.
* [http://trec-dd.org/ Dynamic Domain Track] - '''Goal:'''  to investigate  domain-specific search algorithms that adapt to the dynamic information needs of professional users as they explore in complex domains. 
* [http://trec-liveqa.org/ LiveQA Track] -  '''Goal:'''  to  generate answers to real questions originating from real users via a live question stream, in real time. 
* [http://trec-open-search.org/ OpenSearch Track] - '''Goal:''' to  explore an evaluation paradigm for IR that involves real users of operational search engines. For this first year of the track the task will be ad hoc Academic Search.
* Real-Time Summarization Track -  '''Goal:''' to explore techniques for constructing real-time update summaries from social media streams in response to users' information needs. 
* [http://www.cs.ucl.ac.uk/tasks-track-2016/ Tasks Track] - '''Goal:'''  to test whether systems can induce the possible tasks users might be trying to accomplish given a query. 
* [http://trec-total-recall.org/ Total Recall Track] -  '''Goal:''': to evaluate methods to achieve very high recall, including methods that include a human assessor in the loop.

===Past tracks===
* Chemical Track - '''Goal:''' to develop and evaluate technology for large scale search in [[chemistry]]-related documents, including academic papers and patents, to better meet the needs of professional searchers, and specifically [[patent search]]ers and chemists.
* [[Crowdsourcing]] Track - '''Goal:''' to provide a collaborative venue for exploring [[crowdsourcing]] methods both for evaluating search and for performing search tasks. 
* [[TREC Genomics|Genomics Track]] - '''Goal:''' to study the retrieval of [[Genomics|genomic]] data, not just gene sequences but also supporting documentation such as research papers, lab reports, etc. Last ran on TREC 2007.
* [[Enterprise search|Enterprise Track]] - '''Goal:''' to study search over the data of an organization to complete some task. Last ran on TREC 2008.
* Entity Track - '''Goal:''' to perform entity-related search on Web data. These search tasks (such as finding entities and properties of entities) address common information needs that are not that well modeled as ad hoc document search.
* [[Cross-language information retrieval|Cross-Language]] Track - '''Goal:''' to investigate the ability of retrieval systems to find documents topically regardless of source language.
* [[Federated search|FedWeb]] Track - '''Goal:''' to select best resources to forward a query to, and merge the results so that most relevant are on the top.
* Federated Web Search Track - '''Goal:''' to investigate techniques for the selection and combination of search results from a large number of real on-line web search services.
* Filtering Track - '''Goal:''' to binarily decide retrieval of new incoming documents given a stable [[information need]].
* HARD Track - '''Goal:''' to achieve High Accuracy Retrieval from Documents by leveraging additional information about the searcher and/or the search context.
* Interactive Track - '''Goal:''' to study user [[Human-computer interaction|interaction]] with text retrieval systems.
* Knowledge Base Acceleration Track - '''Goal:''' to develop techniques to dramatically improve the efficiency of (human) knowledge base curators by having the system suggest modifications/extensions to the KB based on its monitoring of the data streams.
* Legal Track - '''Goal:''' to develop search technology that meets the needs of lawyers to engage in effective [[discovery (law)|discovery]] in digital document collections.
* Medical Records Track - '''Goal:''' to explore methods for searching unstructured information found in patient medical records. 
* [[Microblog]] Track - '''Goal:''' to examine the nature of real-time information needs and their satisfaction in the context of microblogging environments such as Twitter. 
* Novelty Track - '''Goal:''' to investigate systems' abilities to locate new (i.e., non-redundant) information.
* [[Question answering|Question Answering]] Track - '''Goal:''' to achieve more [[information retrieval]] than just [[document retrieval]] by answering factoid, list and definition-style questions.
* Robust Retrieval Track - '''Goal:''' to focus on individual topic effectiveness.
* [[Relevance feedback|Relevance Feedback]] Track - '''Goal:''' to further deep evaluation of relevance feedback processes.
* Session Track - '''Goal:''' to develop methods for measuring multiple-query sessions where information needs drift or get more or less specific over the session.
* [[Spam (electronic)|Spam]] Track - '''Goal:''' to provide a standard evaluation of current and proposed [[spam filter]]ing approaches.
* Temporal Summarization Track - '''Goal:''' to develop systems that allow users to efficiently monitor the information associated with an event over time.
* [[Terabyte]] Track - '''Goal:''' to investigate whether/how the [[information retrieval|IR]] community can scale traditional IR test-collection-based evaluation to significantly large collections.
* [[Video search engine|Video]] Track - '''Goal:''' to research in automatic segmentation, [[index (search engine)|index]]ing, and content-based retrieval of [[digital video]].
:In 2003, this track became its own independent evaluation named [[TRECVID]].
* Web Track - '''Goal:''' to explore information seeking behaviors common in general web search.

===Related events===
In 1997, a Japanese counterpart of TREC was launched (first workshop in 1999), called [http://research.nii.ac.jp/ntcir/ NTCIR] ([[National Institute of Informatics|NII]] Test Collection for IR Systems), and in 2000, a European counterpart was launched, called [http://www.clef-campaign.org/ CLEF] (Cross Language Evaluation Forum).

== Conference contributions to search effectiveness==

NIST claims that within the first six years of the workshops, the effectiveness of retrieval systems approximately doubled.&lt;ref&gt;[http://trec.nist.gov/overview.html From TREC homepage: "... effectiveness approximately doubled in the first six years of TREC"]&lt;/ref&gt; The conference was also the first to hold large-scale evaluations of non-English documents, speech, video and retrieval across languages. Additionally, the challenges have inspired a large body of [http://trec.nist.gov/pubs.html publications]. Technology first developed in TREC is now included in many of the world's commercial [[search engine]]s.  An independent report by RTII found that "about one-third of the improvement in web search engines from 1999 to 2009 is attributable to TREC. Those enhancements likely saved up to 3 billion hours of time using web search engines. ... Additionally, the report showed that for every $1 that NIST and its partners invested in TREC, at least $3.35 to $5.07 in benefits were accrued to U.S. information retrieval researchers in both the private sector and academia."
&lt;ref&gt;{{cite web|url=http://rti.org/page.cfm?objectid=75E125DC-5056-B100-31A5A6BDE897DE6D |title=NIST Investment Significantly Improved Search Engines |publisher=Rti.org |date= |accessdate=2012-01-19}}&lt;/ref&gt;
&lt;ref&gt;http://www.nist.gov/director/planning/upload/report10-1.pdf&lt;/ref&gt;

While one study suggests that the state of the art for ad hoc search has not advanced substantially in the past decade,&lt;ref&gt;Timothy G. Armstrong, Alistair Moffat, William Webber, Justin Zobel.  Improvements that don't add up: ad hoc retrieval results since 1998.  CIKM 2009.  ACM.&lt;/ref&gt; it is referring just to search for topically relevant documents in small news and web collections of a few gigabytes.  There have been advances in other types of ad hoc search in the past decade.  For example, test collections were created for known-item web search which found improvements from the use of anchor text, title weighting and url length, which were not useful techniques on the older ad hoc test collections.  In 2009, a new billion-page web collection was introduced, and spam filtering was found to be a useful technique for ad hoc web search, unlike in past test collections.

The test collections developed at TREC are useful not just for (potentially) helping researchers advance the state of the art, but also for allowing developers of new (commercial) retrieval products to evaluate their effectiveness on standard tests.  In the past decade, TREC has created new tests for enterprise e-mail search, genomics search, spam filtering, e-Discovery, and several other retrieval domains.

TREC systems often provide a baseline for further research.  Examples include:
* [[Hal Varian]], Chief Economist at [[Google]], says ''Better data makes for better science. The history of information retrieval illustrates this principle well," and describes TREC's contribution.&lt;ref&gt;[http://googleblog.blogspot.com/2008/03/why-data-matters.html Why Data Matters]&lt;/ref&gt;
* TREC's Legal track has influenced the e-Discovery community both in research and in evaluation of commercial vendors.&lt;ref&gt;[http://blogs.the451group.com/information_management/2009/01/29/standards-in-e-discovery-%E2%80%93-walking-the-walk/ The 451 Group: Standards in e-Discovery -- walking the walk]&lt;/ref&gt;
* The [[IBM]] researcher team building [[IBM Watson]] (aka [[DeepQA]]), which beat the world's best [[Jeopardy!]] players,&lt;ref&gt;[http://www-03.ibm.com/press/us/en/presskit/27297.wss IBM and Jeopardy! Relive History with Encore Presentation of Jeopardy!: The IBM Challenge]&lt;/ref&gt; used data and systems from TREC's QA Track as baseline performance measurements.&lt;ref&gt;[http://www.aaai.org/AITopics/articles&amp;columns/Ferrucci-Watson2010.pdf David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya A. Kalyanpur, Adam Lally, J. William Murdock, Eric Nyberg, John Prager, Nico Schlaefer, and Chris Welt. '''Building Watson:  An Overview of the DeepQA Project''']&lt;/ref&gt;

== Participation ==
The conference is made up of a varied, international group of researchers and developers.&lt;ref&gt;{{cite web|url=https://wiki.ir-facility.org/index.php/Participants |title=Participants - IRF Wiki |publisher=Wiki.ir-facility.org |date=2009-12-01 |accessdate=2012-01-19}}&lt;/ref&gt;&lt;ref&gt;http://trec.nist.gov/pubs/trec17/papers/LEGAL.OVERVIEW08.pdf&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://trec.nist.gov/pubs/trec17/appendices/million.query.results.html |title=Text REtrieval Conference (TREC) TREC 2008 Million Query Track Results |publisher=Trec.nist.gov |date= |accessdate=2012-01-19}}&lt;/ref&gt; In 2003, there were 93 groups from both academia and industry from 22 countries participating.

==References==
{{reflist}}

== External links ==
*[http://trec.nist.gov/ TREC website at NIST]
*[http://www.nist.gov/itl/div894/894.02/related_projects/tipster/ TIPSTER]
*[http://www.amazon.com/TREC-Experiment-Evaluation-Information-Electronic/dp/0262220733/ The TREC book (at Amazon)]

[[Category:Information retrieval organizations]]
[[Category:Computational linguistics]]
[[Category:Natural language processing]]
[[Category:Computer science competitions]]</text>
      <sha1>l9hlm7l5f15vosa497m85t8tkz5tewu</sha1>
    </revision>
  </page>
  <page>
    <title>European Summer School in Information Retrieval</title>
    <ns>0</ns>
    <id>22254915</id>
    <revision>
      <id>748286815</id>
      <parentid>735906504</parentid>
      <timestamp>2016-11-07T12:27:04Z</timestamp>
      <contributor>
        <username>Gianmaria.silvello</username>
        <id>9355814</id>
      </contributor>
      <comment>/* ESSIR Editions */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4970" xml:space="preserve">The '''European Summer School in Information Retrieval''' ('''ESSIR''') is a scientific event founded in 1990, which starts off a series of Summer Schools to provide high quality teaching of information retrieval on advanced topics. ESSIR is typically a week-long event consisting of guest lectures and seminars from invited lecturers who are recognized experts in the field.
The aim of ESSIR is to give to its participants a common ground in different aspects of '''[[Information Retrieval]] (IR)'''. Maristella Agosti in 2008 stated that: "''The term IR identifies the activities that a person – the user – has to conduct to choose, from a collection of documents, those that can be of interest to him to satisfy a specific and contingent information need.''"&lt;ref&gt;Agosti, M.: "Information Access using the Guide of User Requirements". In: ''Information Access through Search Engines and Digital Libraries''. Agosti, M. ed., Springer-Verlag Berlin Heidelberg, pp. 1-12, (2008).&lt;/ref&gt;

IR is a discipline with many facets and at the same time influences and is influenced by many other scientific disciplines. Indeed, IR ranges from [[computer science]] to [[information science]] and beyond; moreover, a large number of IR methods and techniques are adopted and absorbed by several technologies. The IR core methods and techniques are those for designing and developing IR systems, Web search engines, and tools for information storing and querying in Digital Libraries. IR core subjects are: system architectures, algorithms, formal theoretical models, and evaluation of the diverse systems and services that implement functionalities of storing and retrieving documents from multimedia document collections, and over wide area networks such as the [[Internet]].

ESSIR aims to give a deep and authoritative insight of the core IR methods and subjects along these three dimensions and also for this reason it is intended for researchers starting out in IR, for industrialists who wish to know more about this increasingly important topic and for people working on topics related to management of information on the Internet.

Two books have been prepared as readings in IR from editions of ESSIR, the first one is ''Lectures on Information Retrieval'',&lt;ref&gt;Agosti, M., Crestani, F. and Pasi, G. (Eds): "Lectures on Information Retrieval". Revised Lectures of Third European Summer-School, ESSIR 2000 Varenna, Italy, September 11–15, 2000. LNCS Vol. 1980, Springer-Verlag, Berlin Heidelberg, 2001.&lt;/ref&gt; the second one is ''Advanced Topics in Information Retrieval''.&lt;ref&gt;Melucci, M., and Baeza-Yates, R. (Eds): "Advanced Topics in Information Retrieval". The Information Retrieval Series, Vol. 33, Springer-Verlag, Berlin Heidelberg, 2011.&lt;/ref&gt;

== ESSIR Editions ==
ESSIR series started in 1990 coming out from the successful experience of the Summer School in Information Retrieval (SSIR) conceived and designed by [http://www.dei.unipd.it/~agosti/ Maristella Agosti], [[University of Padua]], Italy and [[Nick Belkin]], [[Rutgers University]], U.S.A., for an Italian audience in 1989.

{| class="wikitable" border="1"
|-
! Edition
! Web Site
! Location
! Organiser(s)
|-
|  10th
|  [http://mklab.iti.gr/essir2015/ ESSIR 2015]
|  Thessaloniki, Greece
|  Ioannis (Yiannis) Kompatsiaris, Symeon Papadopoulos, Theodora Tsikrika, and Stefanos Vrochidis
|-
|  9th
|  [http://www.ugr.es/~essir2013/ ESSIR 2013]
|  Granada, Spain
|  Juan M. Fernadez-Luna and Juan F. Huete
|-
|  8th
|  [http://essir.uni-koblenz.de/ ESSIR 2011]
|  Koblenz, Germany
|  Sergej Sizov and Steffen Staab
|-
|  7th
|  [http://essir2009.dei.unipd.it/ ESSIR 2009]
|  Padua, Italy
|  Massimo Melucci and Ricardo Baeza-Yates
|-
|  6th
|  [http://www.dcs.gla.ac.uk/essir2007/ ESSIR 2007]
|  Glasgow, Scotland, United Kingdom
|  Iadh Ounis and Keith van Rijsbergen
|-
|  5th
|  [http://www.cdvp.dcu.ie/ESSIR2005/ ESSIR 2005]
|  Dublin, Ireland
|  Alan Smeaton
|-
|  4th
|  [http://www-clips.imag.fr/mrim/essir03/main_essir.html ESSIR 2003]
|  Aussois (Savoie), France
|  Catherine Berrut and Yves Chiaramella
|-
|  3rd
|  [http://www.itim.mi.cnr.it/Eventi/essir2000/index.htm ESSIR 2000]
|  Varenna, Italy
|  Maristella Agosti, Fabio Crestani, and Gabriella Pasi
|-
|  2nd
|  [http://www.dcs.gla.ac.uk/essir/ ESSIR 1995]
|  Glasgow, United Kingdom
|  Keith van Rijsbergen
|-
|  1st
|  [http://ims.dei.unipd.it/websites/essir/essir1990.html ESSIR 1990]
|  Brixen, Italy
|  Maristella Agosti
|}

==Notes==
{{reflist}}

==External links==
* [http://ims.dei.unipd.it/websites/essir/home.html ESSIR presentation page of the IMS Research Group]
* [http://ims.dei.unipd.it IMS Research Group, Department of Information Engineering – University of Padua, Italy]
* [http://www.dei.unipd.it/ Department of Information Engineering – University of Padua, Italy]
* [http://www.unipd.it/en/index.htm University of Padua, Italy]

[[Category:Information retrieval organizations]]
[[Category:Summer schools]]</text>
      <sha1>5ct5fbviqxkqbuw9icnck9giksv98sk</sha1>
    </revision>
  </page>
  <page>
    <title>Clearinghouse for Networked Information Discovery and Retrieval</title>
    <ns>0</ns>
    <id>45638306</id>
    <revision>
      <id>666879159</id>
      <parentid>666705116</parentid>
      <timestamp>2015-06-14T09:13:04Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor />
      <comment>/* References */Removed invisible unicode characters + other fixes, removed: ‎ using [[Project:AWB|AWB]] (11140)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3490" xml:space="preserve">The '''Clearinghouse for Networked Information Discovery and Retrieval''' or '''CNIDR''' was an organization funded by the U.S. [[National Science Foundation]] from 1993 to 1997 and based at the Microelectronics Center of North Carolina (MCNC) in [[Research Triangle Park]].&lt;ref&gt;National Science Foundation, [http://www.nsf.gov/awardsearch/showAward?AWD_ID=9216963 Award Abstract #9216963: Clearinghouse for Network Information Discovery Retrieval]&lt;/ref&gt;&lt;ref&gt;Brett, George. [http://grantome.com/grant/NSF/CNS-9315306 Clearinghouse for Networked Information Discovery and Retrieval (CNIDR)]&lt;/ref&gt;  CNIDR was active in the research and development of open source software and open standards, centered on information discovery and retrieval, in the emerging Internet.

Among the software developed at CNIDR were Isite, an open source [[Z39.50]] implementation and successor to the free version of [[Wide area information server|WAIS]],&lt;ref&gt;Gamiel, Kevin and Nassar, Nassib.  1995.  Structural components of the Isite information system.  In ''Z39.50 Implementation Experiences'', P. Over, R. Denenberg, W. E. Moen, and L. Stovel, Eds.  National Institute of Standards and Technology Special Publication 500-229, US Department of Commerce, Gaithersburg, MD, 71-74.&lt;/ref&gt;&lt;ref&gt;Nebert, Douglas D. and Fullton, James.  [http://www.csdl.tamu.edu/DL95/papers/nebert/nebert.html Use of the ISite Z39.50 software to search and retrieve spatially-referenced data]&lt;/ref&gt;&lt;ref&gt;[http://inkdroid.org/tmp/www-talk/8133.html CNIDR Announces Isite v1.00 Integrated Information System]&lt;/ref&gt;&lt;ref&gt;[http://isite.awcubed.com/Isite.html The Isite Information System]&lt;/ref&gt;&lt;ref&gt;[http://www.loc.gov/z3950/mums.html Library of Congress Search Form]&lt;/ref&gt; and [[Isearch]], an open source text retrieval system.  CNIDR staff were involved in the development of open standards in the [[Internet Engineering Task Force]], the Z39.50 Implementors Group and [[Dublin Core]].&lt;ref&gt;[https://www.ietf.org/meeting/past.html IETF Past Meetings]&lt;/ref&gt;&lt;ref&gt;[http://www.loc.gov/z3950/agency/zig/meetings/output.html ZIG Meeting Output]&lt;/ref&gt;&lt;ref&gt;[http://dublincore.org/workshops/dc1/ DC1: OCLC/NCSA Metadata Workshop: The Essential Elements of Network Object Description]&lt;/ref&gt;

CNIDR collaborated with the [[United States Patent and Trademark Office|U.S. Patent and Trademark Office (USPTO)]] to develop the USPTO's first Internet-based patent search systems.  One of these provided full text searching and images of medical patents related to the research and treatment of HIV/AIDS and issued by the US, Japanese and European patent offices.  Another system, known as the US Patent Bibliographic Database, provided searching of "front page" bibliographic information for all US patents since 1976.&lt;ref&gt;Miller, Annetta.  1994.  [http://www.newsweek.com/new-online-aids-database-186740 "A New Online Aids Database."]  In ''Newsweek'', November 13, 1994.&lt;/ref&gt;&lt;ref&gt;[http://www.pubzpro.com/Pubz/#!search/a/6105 MCNC and U.S. Patent Office Launch Internet AIDS Library]&lt;/ref&gt;&lt;ref&gt;Kawakami, Alice K.  [http://www.istl.org/98-summer/article5.html "Patents and Patent Searching."]&lt;/ref&gt;&lt;ref&gt;[http://www2.iastate.edu/~cyberstacks/hyb_t_7.htm Patents and Trademarks]&lt;/ref&gt;

==References==
{{Reflist}}

[[Category:Information retrieval organizations]]
[[Category:Internet Standards]]
[[Category:Internet protocols]]
[[Category:Internet search engines]]
[[Category:Organizations established in 1992]]
[[Category:Computer-related organizations]]</text>
      <sha1>7ta0k9oeyj6ptgo03n32l6tlzifsa6q</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Yandex</title>
    <ns>14</ns>
    <id>31871742</id>
    <revision>
      <id>666734407</id>
      <parentid>664470321</parentid>
      <timestamp>2015-06-13T08:12:11Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <comment>added [[Category:Information retrieval organizations]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="495" xml:space="preserve">{{commonscat|Yandex}}
{{Cat main|Yandex}}

[[Category:Wikipedia categories named after information technology companies]]
[[Category:Wikipedia categories named after companies of Russia]]
[[Category:Web portals]]
[[Category:Internet search engines]]
[[Category:Internet companies of Russia]]
[[Category:Companies listed on NASDAQ]]
[[Category:Internet in Russia]]
[[Category:Internet properties established in 1997]]
[[Category:Russian websites]]
[[Category:Information retrieval organizations]]</text>
      <sha1>plohxsti2uz2mb9oqdzn1052x0hhtxj</sha1>
    </revision>
  </page>
  <page>
    <title>Automatic Content Extraction</title>
    <ns>0</ns>
    <id>33675011</id>
    <revision>
      <id>743084246</id>
      <parentid>700387121</parentid>
      <timestamp>2016-10-07T18:19:47Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>/* Topics and Exercises */ remove HTML comment</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3209" xml:space="preserve">{{Multiple issues|
{{citation style|date=December 2011}}
{{technical|date=October 2012}}
{{abbreviations|date=October 2012}}
}}
'''Automatic Content Extraction (ACE)''' is a research program for developing advanced [[Information extraction]] [[technologies]] convened by the [[National Institute of Standards and Technology | NIST]] from 1999 to 2008, succeeding [[Message Understanding Conference | MUC]] and preceding [http://www.nist.gov/tac/ Text Analysis Conference]. 

==Goals and Efforts==
In general objective, the ACE program is motivated by and addresses the same issues as the MUC program that preceded it. The ACE program, however, defines the research objectives in terms of the target objects (i.e., the entities, the relations, and the events) rather than in terms of the words in the text. For example, the so-called “named entity” task, as defined in MUC, is to identify those words (on the page) that are names of entities. In ACE, on the other hand, the corresponding task is to identify the entity so named. This is a different task, one that is more abstract and that involves inference more explicitly in producing an answer. In a real sense, the task is to detect things that “aren’t there”.

While the ACE program is directed toward extraction of information from [[Sound|audio]] and [[image]] sources in addition to pure text, the research effort is restricted to information extraction from text. The actual [[transduction (machine learning)|transduction]] of audio and image data into text is not part of the ACE research effort, although the processing of [[Speech recognition | ASR]] and [[Optical character recognition | OCR]] output from such transducers is.

The effort involves:
* defining the research tasks in detail,
* collecting and annotating data needed for training, development, and evaluation,
* supporting the research with evaluation tools and [[research workshop]]s.

==Topics and exercises==
Given a text in [[natural language]], the ACE challenge is to detect:
# '''entities''' mentioned in the text, such as: persons, organizations, locations, facilities, weapons, vehicles, and geo-political entities.
# '''relations''' between entities, such as: person A is the manager of company B. Relation types include: role, part, located, near, and social.
# '''events''' mentioned in the text, such as: interaction, movement, transfer, creation and destruction.

The program relates to [[English language|English]], [[Arabic language|Arabic]] and [[Chinese language|Chinese]] texts.

The ACE corpus is one of the standard benchmarks for testing new information extraction [[algorithm]]s.

==References==
* George Doddington@NIS T, Alexis Mitchell@LD C, Mark Przybocki@NIS T, Lance Ramshaw@BB N, Stephanie Strassel@LD C, Ralph Weischedel@BB N. [http://www.citeulike.org/user/erelsegal-halevi/article/10003935 The automatic content extraction (ACE) program–tasks, data, and evaluation.] 2004

==External links==
* [http://www.itl.nist.gov/iaui/894.02/related_projects/muc/ MUC] - ACE's predecessor.
* [http://projects.ldc.upenn.edu/ace/ ACE] (LDC)
* [http://www.itl.nist.gov/iad/894.01/tests/ace/ ACE] (NIST)

[[Category:Information retrieval organizations]]</text>
      <sha1>5oczipw7dviu4b8aiw6gwx2mltidht5</sha1>
    </revision>
  </page>
  <page>
    <title>Information Retrieval Facility</title>
    <ns>0</ns>
    <id>16635934</id>
    <revision>
      <id>735904927</id>
      <parentid>731672421</parentid>
      <timestamp>2016-08-23T21:41:54Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <minor />
      <comment>cap, layout</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8860" xml:space="preserve">{{Advert|date=May 2012}}

[[Image:IRF logo 350x350.png|thumb|200px|right|IRF logo]]

The '''Information Retrieval Facility''' ('''IRF'''), founded 2006 and located in [[Vienna]], [[Austria]], was a research platform for networking and collaboration for professionals in the field of [[information retrieval]]. It ceased operations in 2012.

The IRF had members in the following categories:

* Researchers in [[information retrieval]] (IR) or related scientific areas
* Industrial/corporate information management professionals
* Patent authorities and governmental institutions
* Students of one of the above

==The Scientific Board==
'''Maristella Agosti''', Professor, [http://www.dei.unipd.it/wdyn/?IDsezione=1 Department of Information Engineering, University of Padova]

'''Gerhard Budin''', Director of the [http://transvienna.univie.ac.at/forschung/professuren/dr-gerhard-budin/ Center of Translation Studies at the University of Vienna],
Director of the [http://www.oeaw.ac.at/icltt/ Department of Corpuslinguistics and Text Technology, Austrian Academy of Sciences]

'''Jamie Callan''', Professor, [http://www.cs.cmu.edu/~callan/Bio.html Language Technologies Institute, CMU, Carnegie Mellon University]

'''Yves Chiaramella''', Professor Emeritus, [http://www-clips.imag.fr/mrim/User/yves.chiaramella/ Department of Computer Science and Applied Mathematics, Joseph Fourier University]

'''Kilnam Chon''', Professor, Computer Science Department, [http://cosmos.kaist.ac.kr/salab/professor/index02.html Korea Advanced Institute of Science and Technology (KAIST)]

'''W. Bruce Croft''', Distinguished Professor, [http://ciir.cs.umass.edu/personnel/croft.html Department of Computer Science and Director Center for Intelligent IR University of Massachusetts Amherst]

'''Hamish Cunningham''', Research Professor, [http://www.dcs.shef.ac.uk/~hamish/ Computer Science Department University Sheffield]

'''Norbert Fuhr''', Chairman of the Scientific Board, Professor, [http://www.is.informatik.uni-duisburg.de/staff/fuhr.html Institute of Informatics and Interactive Systems University Duisburg-Essen]

'''David Hawking''', Science Leader, Project Leader, [http://es.csiro.au/people/Dave/ CSIRO ICT Centre]

'''Noriko Kando''', Professor, [http://www.nii.ac.jp/index.shtml.en Software Engineering Research, Software Research Division, National Institute of Informatics (NII)]

'''Arcot Desai Narasimhalu''', Associate Dean, [http://www.sis.smu.edu.sg/faculty/infosys/arcotdesai.asp School of Information Systems Singapore Management University]

'''John Tait''', Chief Scientific Officer of the IRF, [http://www.johntait.net/ Until July 2007 Professor of Intelligent Information Systems and Associate Dean of the School of Computing and Technology]

'''Benjamin T'sou''', Director, [http://www.cityu.edu.hk/ Language Information Sciences Research Centre, City University of Hong Kong]

'''[[C. J. van Rijsbergen|C.J. van Rijsbergen]]''',
[http://www.dcs.gla.ac.uk/~keith/ Dept. Computer Science at the University of Glasgow]

==Scientific goals==

* Modelling innovative and specialised information retrieval systems for global patent document collections.
* Investigating and developing an adequate technical infrastructure that allows interactive experimentation with formal, mathematical retrieval concepts for very large-scale document collections.&lt;
* Studying the usability of multi modal user-interfaces to very large-scale information retrieval systems.
* Integrating real users with actual information needs into the research process of modelling information retrieval systems to allow accurate performance evaluation.
* Ability to create different views of patent data depending on the focus of the information need.
* Defining standardised methods for benchmarking the information retrieval process in patent document collections.
* Ability to handle text and non-text parts of a patent in a coherent manner.
* Designing, experimenting and evaluating search engines able to retrieve structured and semi-structured documents in very large-scale patent collections.
* Integrating the temporal dimension of patent documents in retrieval strategies.
* Improving effectiveness and precision of patent retrieval, based on ontologies and natural-language understanding techniques.
* Refining IR methods that allow unstructured querying by exploiting available structure within the patent documents.
* Formal (mathematical) identification and specification of relevant business information needs in the field of intellectual property information.
* Investigating efficient scaling mechanisms for information retrieval taking into account the characteristics of patent data.
* Investigating and experimenting with computing architectures for very high-capacity information management.
* Establishing an open [[eScience]] platform that enables a standardised and easy way of creating and performing IR experiments on a common research infrastructure.
* Discovering and investigating novel use cases and business applications deriving from intellectual property information.
* Enabling the formal information retrieval, natural language and semantic processing research to grow into the field of applied sciences in the global, industrial context.
* Development and integration of different information access methods.
* Research on effective methods for interactive information retrieval.

==Semantic supercomputing==
Current technologies to extract concepts from unstructured documents are extremely computational intensive. To allow interactive experimentation with rich and huge text corpora, the IRF has built a high performance computing environment, into which the latest technological advances have been implemented:

* multi-node clusters (currently 80 cores, up to 1024)
* highest speed interconnect technology
* single system image with large compound memory (currently 320 GB, up to 4 TB)
* fully integrated configurable computing (currently 4 FPGA cores, up to 256)

The combination of these HPC features to accelerate text mining represents the IRF implementation of semantic supercomputing.

==The World Patent Corpus==
The IRF aims to bring state-of-the-art information retrieval technology to the community of patent information professionals. We expect information retrieval (IR) technology to become the focus of information technology very soon. All industry sectors can profit from applying modern and future text mining processes to the special requirements of patent research. Although all ideas and concepts are universally applicable to all sorts of intellectual property information, patents require the most sophistication, and confront us with challenging technical and organisational problems. 
The entire body of patent-related documents possibly constitutes the largest corpus of compound documents, making it a rewarding target for text mining scientists and end-users alike. What’s more, patents have become a crucial issue, in particular for large global corporations and universities. The industrial users of patent data are among the most demanding and important information professionals. As a consequence, they could benefit the most from technology that relieves the burden of researching the large body of patent information.

== Research collections ==
The IRF provides a number of test data collections that have either been developed by the IRF, by one of its members or by third parties. These data collections can be used freely for scientific experimentations.

The MAtrixware REsearch Collection ([[MAREC]]) is the first standardised patent data corpus for research purposes. It consists of 19 million patent documents in different languages, normalised to a highly specific XML format. The collection has been developed by Matrixware for the IRF.

The ClueWeb09 collection is a 25 terabyte dataset of about 1 billion web pages crawled in January and February, 2009. It has been created by the Language Technologies Institute at [[Carnegie Mellon University]] to support research on information retrieval and related human language technologies.

==References==
* [http://www.iwr.co.uk/information-world-review/analysis/2231880/patent-medicine-info-retrievers?page=2 Patent medicine for information retrievers, Information World Review]
* [http://ecir2008.dcs.gla.ac.uk/industry.html The IRF and its Role in Professional Information Research, ECIR 2008]

==External links==
* [http://www.ir-facility.org/ Official site: ir-facility.org]
* [https://www.youtube.com/watch?v=XpXtRu0XfeA YouTube: The future of information retrieval Part1] 
* [https://www.youtube.com/watch?v=dRaTeTaHBsI YouTube: The future of information retrieval Part2]

[[Category:Organizations established in 2006]]
[[Category:Computer science organizations]]
[[Category:Information retrieval organizations]]
[[Category:Education in Vienna]]</text>
      <sha1>5u10mb7icqvy0aoyxicwzhrvaxlcvk4</sha1>
    </revision>
  </page>
  <page>
    <title>ChemRefer</title>
    <ns>0</ns>
    <id>11242818</id>
    <revision>
      <id>750662252</id>
      <parentid>696391061</parentid>
      <timestamp>2016-11-21T03:14:37Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 4 sources and tagging 0 as dead. #IABot (v1.2.7.1)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3153" xml:space="preserve">{{Orphan|date=February 2009}}
{{Infobox website
| name = Chemrefer
| logo = [[Image:Chemrefer.png]]
| screenshot = 
| caption = 
| url = http://www.chemrefer.com
| commercial = Yes
| type = [[Search engine]]
| language = English
| registration = Not Applicable
| owner = ChemRefer Limited
| author = William James Griffiths
| launch date = 2006
| current status = Offline
| revenue = 
}}
'''ChemRefer''' is a service that allows searching of freely available and full-text chemical and pharmaceutical literature that is published by authoritative sources.&lt;ref&gt;{{citation|journal=Science Articles |title= Science News Forum|publisher= SciScoop |date=May 19, 2006|url= http://www.sciscoop.com/story/2006/5/19/95844/6293}}&lt;/ref&gt;

Features include basic and advanced search options, [[mouseover]] detailed view, an integrated chemical structure drawing and search tool, downloadable [[toolbar]], customized [[RSS]] feeds, and newsletter.

ChemRefer is primarily of use to readers who do not have subscriptions for accessing restricted chemical literature, and to publishers who offer either [[Open access (publishing)|open access]] or [[hybrid open access journal]]s and seek to attract further subscriptions by publicly releasing part of their archive.

==See also==
*[[Google Scholar]]
*[[Windows Live Academic]]
*[[BASE (search engine)|BASE]]
*[[PubMed]]

==References==
{{reflist}}

==External links==
===Recommendations &amp; reviews===
*[https://web.archive.org/web/20060902072725/http://www.rowland.harvard.edu/resources/library/lnn_archive/031706.php Cited as an "Internet Site of the Week"] by the library of the [[Rowland Institute for Science]] at [[Harvard University]]
*[https://web.archive.org/web/20070804051550/http://infoweb.nrl.navy.mil:80/index.cfm?i=156 Recommended in the list of chemical literature databases] by the library of the [[United States Naval Research Laboratory]]
*[https://web.archive.org/web/20070212122105/http://www.mta.ca:80/library/subject_chemistry.html Recommended in the list of chemical literature databases] by the library of [[Mount Allison University]]
*[http://depth-first.com/articles/2007/01/15/chemrefer-free-direct-access-to-the-primary-literature Review of ChemRefer] at Depth-First chemoinformatics magazine
*[https://web.archive.org/web/20080917155607/http://recherche-technologie.wallonie.be:80/fr/particulier/menu/revue-athena/l-annuaire-de-liens/internet/moteurs-de-recherche/www-chemrefer-com.html?PROFIL=PART Recommended in the list of chemical literature databases] by the Technology Research Portal, Belgium
*[http://www.certh.gr/0E9BF53C.en.aspx Recommended in the list of chemical literature databases] by the Centre for Research and Technology, Thessaloniki

===Background===
*[http://www.reactivereports.com/56/56_0.html Interview with William James Griffiths] at Reactive Reports chemistry magazine
*[http://www.earlham.edu/~peters/fos/overview.htm Open access overview] by Professor Peter Suber, Earlham College

[[Category:Scholarly search services]]
[[Category:Chemistry literature]]
[[Category:Information retrieval systems]]
[[Category:Open access projects]]

{{searchengine-website-stub}}</text>
      <sha1>8750da31coonzcsb326q2qohodeyxt4</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Music search engines</title>
    <ns>14</ns>
    <id>28073505</id>
    <revision>
      <id>747503928</id>
      <parentid>747464621</parentid>
      <timestamp>2016-11-02T19:24:39Z</timestamp>
      <contributor>
        <username>Bamyers99</username>
        <id>12311825</id>
      </contributor>
      <comment>Undid revision 747464621 by [[Special:Contributions/182.186.123.39|182.186.123.39]] ([[User talk:182.186.123.39|talk]]) not the place to add links</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="210" xml:space="preserve">[[Category:Information retrieval systems]]
[[Category:Music software|Search engines]]
[[Category:Internet search engines]]
[[Category:Online music and lyrics databases]]
[[Category:Music information retrieval]]</text>
      <sha1>lm87mi1jyt8pwrtn8mc27meskbh80mo</sha1>
    </revision>
  </page>
  <page>
    <title>Find</title>
    <ns>0</ns>
    <id>1486231</id>
    <revision>
      <id>751905534</id>
      <parentid>751905302</parentid>
      <timestamp>2016-11-28T12:49:14Z</timestamp>
      <contributor>
        <username>TwoTwoHello</username>
        <id>17114440</id>
      </contributor>
      <minor />
      <comment>Reverted 3 edits by [[Special:Contributions/125.22.43.11|125.22.43.11]] ([[User talk:125.22.43.11|talk]]) to last revision by ClueBot NG. ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="17697" xml:space="preserve">{{other uses}}
{{refimprove|date=June 2016}}
{{lowercase|title=find}} 
In [[Unix-like]] and some other [[operating system]]s, &lt;code&gt;'''find'''&lt;/code&gt; is a [[command-line utility]] that [[Search engine (computing)|searches]] one or more [[directory tree]]s of a [[file system]], locates [[Computer file|file]]s based on some [[user (computing)|user]]-specified criteria and applies a user-specified action on each matched file. The possible search criteria include a [[pattern matching|pattern]] to match against the [[filename]] or a time range to match against the modification time or access time of the file. By default, &lt;code&gt;find&lt;/code&gt; returns a list of all files below the current [[working directory]].

The related &lt;code&gt;[[locate (Unix)|locate]]&lt;/code&gt; programs use a database of indexed files obtained through &lt;code&gt;find&lt;/code&gt; (updated at regular intervals, typically by &lt;code&gt;[[cron]]&lt;/code&gt; job) to provide a faster method of searching the entire file system for files by name.

==History==
&lt;code&gt;find&lt;/code&gt; appeared in [[Version 5 Unix]] as part of the [[PWB/UNIX|Programmer's Workbench]] project, and was written by Dick Haight alongside ''cpio'',&lt;ref name="reader"&gt;{{cite techreport |first1=M. D. |last1=McIlroy |authorlink1=Doug McIlroy |year=1987 |url=http://www.cs.dartmouth.edu/~doug/reader.pdf |title=A Research Unix reader: annotated excerpts from the Programmer's Manual, 1971–1986 |series=CSTR |number=139 |institution=Bell Labs}}&lt;/ref&gt; which were designed to be used together.&lt;ref&gt;{{Cite web|title = libarchive/libarchive|url = https://github.com/libarchive/libarchive/wiki/FormatCpio|website = GitHub|accessdate = 2015-10-04}}&lt;/ref&gt;

==Find syntax==
{{expand section|date=August 2008}}
&lt;source lang="bash"&gt;
$ find [-H] [-L] [-P] path... [expression]
&lt;/source&gt;
The three options control how the &lt;code&gt;find&lt;/code&gt; command should treat symbolic links. The default behaviour is never to follow symbolic links. This can be explicitly specified using the -P flag. The -L flag will cause the &lt;code&gt;find&lt;/code&gt; command to follow symbolic links. The -H flag will only follow symbolic links while processing the command line arguments. These flags are not available with some older versions of &lt;code&gt;find&lt;/code&gt;.

At least one path must precede the expression. &lt;code&gt;find&lt;/code&gt; is capable of interpreting [[Wildcard character|wildcards]] internally and commands must be constructed carefully in order to control [[Glob (programming)|shell globbing]].

Expression elements are whitespace-separated and evaluated from left to right. They can contain logical elements such as AND (&amp;#x2011;and or &amp;#x2011;a) and OR (&amp;#x2011;or &amp;#x2011;o) as well as more complex predicates.

The [[GNU Find Utilities|GNU]] &lt;code&gt;find&lt;/code&gt; has a large number of additional features not specified by POSIX.

==POSIX protection from infinite output==
Real-world file systems often contain looped structures created through the use of [[hard link|hard]] or [[symbolic link|soft links]]. The [[POSIX|POSIX standard]] requires that
{{Quotation|
The &lt;code&gt;find&lt;/code&gt; utility shall detect infinite loops; that is, entering a previously visited
directory that is an ancestor of the last file encountered. When it detects an infinite
loop, &lt;code&gt;find&lt;/code&gt; shall write a diagnostic message to standard error and shall either recover
its position in the hierarchy or terminate.
}}

==Operators==
Operators can be used to enhance the expressions of the find command. Operators are listed in order of decreasing precedence:
* '''( expr )''' - forces precedence;
* '''! expr''' - true if expr is false;
* '''expr1 expr2''' (or '''expr1 -a expr2''') - AND. expr2 is not evaluated if expr1 is false;
* '''expr1 -o expr2''' - OR. expr2 is not evaluated if expr1 is true.

&lt;source lang="bash"&gt;
$ find . -name 'fileA_*' -o -name 'fileB_*'
&lt;/source&gt;
This command searches the current working directory tree for files whose names start with "fileA_" or "fileB_".

&lt;source lang="bash"&gt;
$ find . -name 'foo.cpp' '!' -path '.svn'
&lt;/source&gt;
This command searches the current working directory tree except the subdirectory tree ".svn" for files whose name is "foo.cpp". We quote the &lt;code&gt;!&lt;/code&gt; so that it's not interpreted by the shell as the history substitution character.

==Type filter explanation==
Various type filters are supported by &lt;code&gt;find&lt;/code&gt;. They are activated using the configuration switch:
&lt;source lang="bash"&gt;
$ find -type x
&lt;/source&gt;
where x may be any of:
* '''b''' - [[Device file|block device (buffered)]];
* '''c''' - [[Device file|character device (unbuffered)]];
* '''d''' - '''[[Directory (computing)|directory]]''';
* '''f''' - '''[[regular file]]''';
* '''l''' - [[symbolic link]]. This is never true if the -L option or the -follow operator is in effect, unless the symbolic link is broken. If you want to search for symbolic links when -L is in effect, use -xtype (though that is a GNU extension);
* '''p''' - [[named pipe]];
* '''s''' - [[Unix domain socket|socket]];
* '''D''' - [[Doors (computing)|door]].

The configuration switches listed in bold are most commonly used.

==Examples==
{{howto|section|date=September 2016}}

===From the current working directory===
&lt;source lang="bash"&gt;
$ find . -name 'my*'
&lt;/source&gt;
This searches the current working directory tree for files whose names start with ''my''. The single quotes avoid the [[shell (computing)|shell]] expansion—without them the shell would replace ''my*'' with the list of files whose names begin with ''my'' in the current working directory. In newer versions of the program, the directory may be omitted, and it will imply the current working directory.

===Regular files only===
&lt;source lang="bash"&gt;
$ find . -name 'my*' -type f
&lt;/source&gt;
This limits the results of the above search to only regular files, therefore excluding directories, special files, symbolic links, etc. ''my*'' is enclosed in single quotes (apostrophes) as otherwise the shell would replace it with the list of files in the current working directory starting with ''my''…

===Commands===
The previous examples created listings of results because, by default, &lt;code&gt;find&lt;/code&gt; executes the &lt;code&gt;-print&lt;/code&gt; action. (Note that early versions of the &lt;code&gt;find&lt;/code&gt; command had no default action at all; therefore the resulting list of files would be discarded, to the bewilderment of users.)

&lt;source lang="bash"&gt;
$ find . -name 'my*' -type f -ls
&lt;/source&gt;
This prints extended file information.

===Search all directories===
&lt;source lang="bash"&gt;
$ find / -name myfile -type f -print
&lt;/source&gt;
This searches every directory for a regular file whose name is ''myfile'' and prints it to the screen. It is generally not a good idea to look for files this way. This can take a considerable amount of time, so it is best to specify the directory more precisely. Some operating systems may mount dynamic file systems that are not congenial to &lt;code&gt;find&lt;/code&gt;. More complex filenames including characters special to the shell may need to be enclosed in single quotes.

===Search all but one subdirectory tree===
&lt;source lang="bash"&gt;
$ find / -path excluded_path -prune -o -type f -name myfile -print
&lt;/source&gt;
This searches every directory except the subdirectory tree ''excluded_path'' (full path including the leading /) that is pruned by the &lt;code&gt;-prune&lt;/code&gt; action, for a regular file whose name is ''myfile''.

===Specify a directory===
&lt;source lang="bash"&gt;
$ find /home/weedly -name myfile -type f -print
&lt;/source&gt;
This searches the ''/home/weedly'' directory tree for regular files named ''myfile''. You should always specify the directory to the deepest level you can remember.

===Search several directories===
&lt;source lang="bash"&gt;
$ find local /tmp -name mydir -type d -print
&lt;/source&gt;
This searches the ''local'' subdirectory tree of the current working directory and the ''/tmp'' directory tree for directories named ''mydir''.

===Ignore errors===
If you're doing this as a user other than root, you might want to ignore permission denied (and any other) errors. Since errors are printed to [[stderr]], they can be suppressed by redirecting the output to /dev/null. The following example shows how to do this in the bash shell:
&lt;source lang="bash"&gt;
$ find / -name myfile -type f -print 2&gt; /dev/null
&lt;/source&gt;

If you are a [[C shell|csh]] or [[tcsh]] user, you cannot redirect [[stderr]] without redirecting [[stdout]] as well. You can use sh to run the &lt;code&gt;find&lt;/code&gt; command to get around this:
&lt;source lang="bash"&gt;
$ sh -c "find / -name myfile -type f -print 2&gt; /dev/null"
&lt;/source&gt;
An alternate method when using [[C shell|csh]] or [[tcsh]] is to pipe the output from [[stdout]] and [[stderr]] into a [[grep]] command. This example shows how to suppress lines that contain permission denied errors.
&lt;source lang="bash"&gt;
$ find . -name myfile |&amp; grep -v 'Permission denied'
&lt;/source&gt;

===Find any one of differently named files===
&lt;source lang="bash"&gt;
$ find . \( -name '*jsp' -o -name '*java' \) -type f -ls
&lt;/source&gt;
The &lt;code&gt;-ls&lt;/code&gt; operator prints extended information, and the example finds any regular file whose name ends with either 'jsp' or 'java'. Note that the parentheses are required. TIn many shells the parentheses must be escaped with a backslash (&lt;code&gt;\(&lt;/code&gt; and &lt;code&gt;\)&lt;/code&gt;) to prevent them from being interpreted as special shell characters. The &lt;code&gt;-ls&lt;/code&gt; operator is not available on all versions of &lt;code&gt;find&lt;/code&gt;.

===Execute an action===
&lt;source lang="bash"&gt;
$ find /var/ftp/mp3 -name '*.mp3' -type f -exec chmod 644 {} \;
&lt;/source&gt;
This command changes the [[File system permissions|permissions]] of all regular files whose names end with ''.mp3'' in the directory tree ''/var/ftp/mp3''. The action is carried out by specifying the statement &lt;code&gt;-exec [[chmod]] 644 {} \;&lt;/code&gt; in the command. For every regular file whose name ends in &lt;code&gt;.mp3&lt;/code&gt;, the command &lt;code&gt;chmod 644 {}&lt;/code&gt; is executed replacing &lt;code&gt;{}&lt;/code&gt; with the name of the file. The semicolon (backslashed to avoid the shell interpreting it as a command separator) indicates the end of the command. Permission &lt;code&gt;644&lt;/code&gt;, usually shown as &lt;code&gt;rw-r--r--&lt;/code&gt;, gives the file owner full permission to read and write the file, while other users have read-only access. In some shells, the &lt;code&gt;{}&lt;/code&gt; must be quoted. The trailing ";" is customarily quoted with a leading "\", but could just as effectively be enclosed in single quotes.

Note that the command itself should *not* be quoted; otherwise you get error messages like
&lt;source lang="console"&gt;
find: echo "mv ./3bfn rel071204": No such file or directory
&lt;/source&gt;
which means that &lt;code&gt;find&lt;/code&gt; is trying to run a file called 'echo "mv ./3bfn rel071204"' and failing.

If you will be executing over many results, it is more efficient to use a variant of the exec primary that collects filenames up to ARG_MAX and then executes COMMAND with a list of filenames.

&lt;source lang="bash"&gt;
$ find . -exec COMMAND {} +
&lt;/source&gt;
This will ensure that filenames with whitespaces are passed to the executed COMMAND without being split up by the shell.

===Delete files and directories===
The &lt;code&gt;-delete&lt;/code&gt; action is a GNU extension, and using it turns on &lt;code&gt;-depth&lt;/code&gt;. So, if you are testing a find command with &lt;code&gt;-print&lt;/code&gt; instead of &lt;code&gt;-delete&lt;/code&gt; in order to figure out what will happen before going for it, you need to use &lt;code&gt;-depth -print&lt;/code&gt;.

Delete empty files and print the names (note that &lt;code&gt;-empty&lt;/code&gt; is a vendor unique extension from GNU &lt;code&gt;find&lt;/code&gt; that may not be available in all &lt;code&gt;find&lt;/code&gt; implementations):
&lt;source lang="bash"&gt;
$ find . -empty -delete -print
&lt;/source&gt;

Delete empty regular files:
&lt;source lang="bash"&gt;
$ find . -type f -empty -delete
&lt;/source&gt;

Delete empty directories:
&lt;source lang="bash"&gt;
$ find . -type d -empty -delete
&lt;/source&gt;

Delete empty files named 'bad':
&lt;source lang="bash"&gt;
$ find . -name bad -empty -delete
&lt;/source&gt;

Warning. — The &lt;code&gt;-delete&lt;/code&gt; action should be used with conditions such as &lt;code&gt;-empty&lt;/code&gt; or &lt;code&gt;-name&lt;/code&gt;:
&lt;source lang="bash"&gt;
$ find . -delete # this deletes all in .
&lt;/source&gt;

===Search for a string===
This command will search all files from the /tmp directory tree for a string:
&lt;source lang="bash"&gt;
$ find /tmp -type f -exec grep 'search string' '{}' /dev/null \+
&lt;/source&gt;
The &lt;tt&gt;[[/dev/null]]&lt;/tt&gt; argument is used to show the name of the file before the text that is found. Without it, only the text found is printed. 
GNU &lt;code&gt;grep&lt;/code&gt; can be used on its own to perform this task:
&lt;source lang="bash"&gt;
$ grep -r 'search string' /tmp
&lt;/source&gt;

Example of search for "LOG" in jsmith's home directory tree:
&lt;source lang="bash"&gt;
$ find ~jsmith -exec grep LOG '{}' /dev/null \; -print
/home/jsmith/scripts/errpt.sh:cp $LOG $FIXEDLOGNAME
/home/jsmith/scripts/errpt.sh:cat $LOG
/home/jsmith/scripts/title:USER=$LOGNAME
&lt;/source&gt;

Example of search for the string "ERROR" in all XML files in the current working directory tree:
&lt;source lang="bash"&gt;
$ find . -name "*.xml" -exec grep "ERROR" /dev/null '{}' \+ 
&lt;/source&gt;
The double quotes (" ") surrounding the search string and single quotes (&lt;nowiki&gt;' '&lt;/nowiki&gt;) surrounding the braces are optional in this example, but needed to allow spaces and some other special characters in the string. Note with more complex text (notably in most popular shells descended from `sh` and `csh`) single quotes are often the easier choice, since ''double quotes do not prevent all special interpretation''. Quoting filenames which have English contractions demonstrates how this can get rather complicated, since a string with an apostrophe in it is easier to protect with double quotes:
&lt;source lang="bash"&gt;
$ find . -name "file-containing-can't" -exec grep "can't" '{}' \; -print
&lt;/source&gt;

===Search for all files owned by a user===
&lt;source lang="bash"&gt;
$ find . -user &lt;userid&gt;
&lt;/source&gt;

===Search in case insensitive mode===
Note that &lt;code&gt;-iname&lt;/code&gt; is not in the standard and may not be supported by all implementations.
&lt;source lang="bash"&gt;
$ find . -iname 'MyFile*'
&lt;/source&gt;

If the &lt;code&gt;-iname&lt;/code&gt; switch is not supported on your system then workaround techniques may be possible such as:
&lt;source lang="bash"&gt;
$ find . -name '[mM][yY][fF][iI][lL][eE]*'
&lt;/source&gt;

This uses [[Perl]] to build the above command for you (though in general this kind of usage is dangerous, since special characters are not properly quoted before being fed into the standard input of `sh`):
&lt;source lang="bash"&gt;
$ echo 'MyFile*' | perl -pe 's/([a-zA-Z])/[\L\1\U\1]/g;s/(.*)/find . -name \1/' | sh
&lt;/source&gt;

===Search files by size===
Searching files whose size is between 100 kilobytes and 500 kilobytes:
&lt;source lang="bash"&gt;
$ find . -size +100k -a -size -500k
&lt;/source&gt;

Searching empty files:
&lt;source lang="bash"&gt;
$ find . -size 0k
&lt;/source&gt;

Searching non-empty files:
&lt;source lang="bash"&gt;
$ find . ! -size 0k
&lt;/source&gt;

===Search files by name and size ===
&lt;source lang="bash"&gt;
$ find /usr/src ! \( -name '*,v' -o -name '.*,v' \) '{}' \; -print
&lt;/source&gt;
This command will search the /usr/src directory tree. All files that are of the form '*,v' and '.*,v' are excluded. Important arguments to note are in the [[tooltip]] that is displayed on mouse-over.

&lt;source lang="bash" enclose="div"&gt;
for file in `find /opt \( -name error_log -o -name 'access_log' -o -name 'ssl_engine_log' -o -name 'rewrite_log' -o
 -name 'catalina.out' \) -size +300000k -a -size -5000000k`; do 
 cat /dev/null &gt; $file
done
&lt;/source&gt;
The units should be one of [bckw], 'b' means 512-byte blocks, 'c' means byte, 'k' means kilobytes and 'w' means 2-byte words. The size does not count indirect blocks, but it does count blocks in sparse files that are not actually allocated.

==Related utilities==
* &lt;code&gt;[[locate (Unix)|locate]]&lt;/code&gt; is a Unix search tool that searches a prebuilt database of files instead of directory trees of a file system. This is faster than &lt;code&gt;find&lt;/code&gt; but less accurate because the database may not be up-to-date.
* &lt;code&gt;[[grep]]&lt;/code&gt; is a command-line utility for searching plain-text data sets for lines matching a regular expression and by default reporting matching lines on [[standard output]].
* &lt;code&gt;[[tree (Unix)|tree]]&lt;/code&gt; is a command-line utility that recursively lists files found in a directory tree, indenting the filenames according to their position in the file hierarchy.
* [[GNU Find Utilities]] (also known as findutils) is a [[GNU package]] which contains implementations of the tools &lt;code&gt;find&lt;/code&gt; and [[xargs]].
* [[BusyBox]] is a utility that provides several stripped-down Unix tools in a single executable file, intended for embedded operating systems with very limited resources. It also provides a version of &lt;code&gt;find&lt;/code&gt;.
* &lt;code&gt;[[dir (command)|dir]]&lt;/code&gt; has the /s option that recursively searches for files or directories.

==See also==
* [[mdfind]], a similar utility that utilizes metadata for [[Mac OS X]] and [[Darwin (operating system)|Darwin]]
* [[List of Unix programs]]
* [[List of DOS commands]]
* [[Filter (higher-order function)]]
* [[find (command)]], a DOS and Windows command that is very different from UNIX &lt;code&gt;find&lt;/code&gt;

==References==
{{reflist}}

==External links==
* {{man|cu|find|SUS|find files}}
* [https://www.gnu.org/software/findutils/manual/html_mono/find.html Official webpage for GNU find]
* [http://www.librebyte.net/en/gnulinux/command-find-25-practical-examples/ Command find – 25 practical examples]

{{Unix commands}}

[[Category:Information retrieval systems]]
[[Category:Standard Unix programs]]
[[Category:Unix SUS2008 utilities]]</text>
      <sha1>0mf3e3nk5lzt7ntvyk9esydy85iifxh</sha1>
    </revision>
  </page>
  <page>
    <title>Statistically improbable phrase</title>
    <ns>0</ns>
    <id>2724706</id>
    <revision>
      <id>757758056</id>
      <parentid>757664538</parentid>
      <timestamp>2017-01-01T15:43:47Z</timestamp>
      <contributor>
        <username>Derek R Bullamore</username>
        <id>698799</id>
      </contributor>
      <comment>Filling in 1 references using [[WP:REFLINKS|Reflinks]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3643" xml:space="preserve">A '''statistically improbable phrase''' ('''SIP''') is a phrase or set of words that occurs more frequently in a document (or collection of documents) than in some larger [[Text corpus|corpus]].&lt;ref&gt;{{cite web|url=http://courses.cms.caltech.edu/cs145/2011/wikipedia.pdf |title=SIPping Wikipedia |website=Courses.cms.caltech.edu |accessdate=2017-01-01}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://www.plagiarismtoday.com/2012/07/03/how-long-should-a-statistically-improbably-phrase-be/|title=How Long Should a Statistically Improbably Phrase Be?|author=Jonathan Bailey|date=3 July 2012|work=Plagiarism Today}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|url=http://bioinformatics.oxfordjournals.org/content/26/11/1453|title=Identifying duplicate content using statistically improbable phrases|first1=Mounir|last1=Errami|first2=Zhaohui|last2=Sun|first3=Angela C.|last3=George|first4=Tara C.|last4=Long|first5=Michael A.|last5=Skinner|first6=Jonathan D.|last6=Wren|first7=Harold R.|last7=Garner|date=1 June 2010|publisher=|journal=Bioinformatics|volume=26|issue=11|pages=1453–1457|accessdate=1 January 2017|via=bioinformatics.oxfordjournals.org|doi=10.1093/bioinformatics/btq146|pmid=20472545|pmc=2872002}}&lt;/ref&gt; [[Amazon.com]] uses this concept in determining keywords for a given book or chapter, since keywords of a book or chapter are likely to appear disproportionately within that section.&lt;ref&gt;{{cite web|url=http://www.amazon.com/gp/search-inside/sipshelp.html|title=What are Statistically Improbable Phrases?|accessdate=2007-12-18|publisher=[[Amazon.com]]}}&lt;/ref&gt;&lt;ref&gt;{{cite news|url=http://www.washingtonpost.com/wp-dyn/content/article/2005/08/29/AR2005082901873.html|title=Amazon's Vital Statistics Show How Books Stack Up|last=Weeks|first=Linton|work=[[The Washington Post]]|date=August 30, 2005|accessdate=September 8, 2015}}&lt;/ref&gt; [[Christian Rudder]] has also used this concept with data from [[Online dating service|online dating profiles]] and [[Twitter]] posts to determine the phrases most characteristic of a given race or gender in his book ''Dataclysm''.&lt;ref&gt;{{cite book |last=Rudder |first=Christian |date=2014 |title=Dataclysm: Who We Are When We Think No One's Looking |location=New York |publisher=Crown Publishers |page= |isbn=978-0-385-34737-2}}&lt;/ref&gt;

==Example== 
In a document about [[computer]]s, the most common word is likely to be the word "the", but since "the" is the most commonly used word in the English language, it is likely that any given document will have the word "the" used very frequently.  However, a word like "program" might occur in the document at a much higher rate than its average rate in the English language.  Hence, it is a word unlikely to occur in any given document, but ''did'' occur in the document given.  "Program" would be a statistically improbable phrase.

The statistically improbable phrases of Darwin's ''[[On the Origin of Species]]'' are: ''temperate productions, genera descended, transitional gradations, unknown progenitor, fossiliferous formations, our domestic breeds, modified offspring, doubtful forms, closely allied forms, profitable variations, enormously remote, transitional grades, very distinct species'' and ''mongrel offspring''.&lt;ref&gt;[http://crookedtimber.org/2005/04/02/sociologically-improbable-phrases/ Sociologically Improbable Phrases] Crooked Timber April 2005&lt;/ref&gt;

==See also==
* [[Googlewhack]] – A pair of words occurring on a single webpage, as indexed by Google
* [[tf-idf]] – A statistic used in information retrieval and text mining

==References==
{{Reflist}}

{{Amazon}}

[[Category:Amazon.com]]
[[Category:Bookselling]]
[[Category:Information retrieval systems]]</text>
      <sha1>5vrsp5klmle3t1tfbclc0y4sunbj508</sha1>
    </revision>
  </page>
  <page>
    <title>Indexing Service</title>
    <ns>0</ns>
    <id>4047242</id>
    <revision>
      <id>740025932</id>
      <parentid>735633616</parentid>
      <timestamp>2016-09-18T16:29:50Z</timestamp>
      <contributor>
        <username>Entalpia2</username>
        <id>11842605</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6973" xml:space="preserve">{{distinguish|Indexing and abstracting service}}
{{Use dmy dates|date=February 2011}}
{{Infobox Windows component
| name                = Indexing Service
| screenshot          = Indexing Service Query Form.PNG
| screenshot_size     = 300px
| caption             = The Indexing Service Query Form, used to query Indexing Service catalogs, hosted in [[Microsoft Management Console]].
| type                = [[Desktop search]]
| service_name        = Indexing Service
| service_description = Indexes contents and properties of files on local and remote computers; provides rapid access to files through flexible querying language.
| replaced_by         = [[Windows Search]]
| included_with       = [[Windows NT 4.0#Option Pack|Windows NT 4.0 Option Pack]]&lt;ref name="MIS-Intro" /&gt;&lt;br/&gt;[[Windows 2000]]&lt;ref name="MIS-v3" /&gt;&lt;br/&gt;[[Windows XP]]&lt;ref name="TnC-144" /&gt;&lt;br/&gt;[[Windows Server 2003]]&lt;ref name="TnC-144" /&gt;&lt;br/&gt;[[Windows Server 2008]]&lt;ref name="WIS-Install2008" /&gt;
}}

'''Indexing Service''' (originally called '''Index Server''') was a [[Windows service]] that maintained an index of most of the [[Computer file|files]] on a computer to improve searching performance on PCs and corporate [[computer network]]s. It updated indexes without user intervention. In [[Windows 7]], it has been replaced by a newer [[Windows Search]] indexer. The [[IFilter]] plugins to extend the indexing capabilities to more file formats and protocols are compatible between the legacy Indexing Service and the newer Windows Search indexer.

== History ==
Indexing Service was a [[desktop search]] service included with [[Windows NT 4.0#Option Pack|Windows NT 4.0 Option Pack]]&lt;ref name="MIS-Intro" /&gt; as well as [[Windows 2000]] and later.&lt;ref name="MIS-v3" /&gt;&lt;ref name="TnC-144" /&gt;&lt;ref name="WIS-What" /&gt; The first incarnation of the indexing service was shipped in August 1996&lt;ref name="MIS-Intro" /&gt; as a content search system for Microsoft's web server software, [[Internet Information Services]].{{Citation needed|date=February 2011}} Its origins, however, date further back to Microsoft's [[Cairo (operating system)|Cairo operating system]] project, with the component serving as the Content Indexer for the [[Object File System]]. Cairo was eventually shelved, but the content indexing capabilities would go on to be included as a standard component of later Windows desktop and server operating systems, starting with [[Windows 2000]], which includes Indexing Service 3.0.{{Citation needed|date=February 2011}}

In [[Windows Vista]], the content indexer was replaced with the [[Windows Search]] indexer which was enabled by default. Indexing Service is still included with Windows Server 2008 but is not installed or running by default.&lt;ref name="WIS-Install2008" /&gt;

Indexing Service has been deprecated in Windows 7 and Windows Server 2008 R2.&lt;ref&gt;{{cite web|title=Deprecated Features for Windows 7 and Windows Server 2008 R2|url=http://technet.microsoft.com/en-us/library/ee681698%28WS.10%29.aspx|work=Windows 7 Technical Library|publisher=Microsoft Corporation|accessdate=8 November 2011|location=Indexing Service|date=October 16, 2009}}&lt;/ref&gt; It has been removed from [[Windows 8]].

== Search interfaces ==

Comprehensive searching is available after initial building of the index, which can take up to hours or days, depending on the size of the specified directories, the speed of the hard drive, user activity, indexer settings and other factors. Searching using Indexing service works also on [[Uniform Naming Convention|UNC]] paths and/or mapped network drives if the sharing server indexes appropriate directory and is aware of its sharing.

Once the indexing service has been turned on and has built its index it can be searched in three ways. The search option available from the [[Start Menu]] on the [[Microsoft windows|Windows]] [[Taskbar]] will use the indexing service if it is enabled and will even accept complex queries. Queries can also be performed using either the ''Indexing Service Query Form'' in the [[Microsoft Management Console#Common snap-ins|Computer Management snap-in]] of Microsoft Management Console, or, alternatively, using third-party applications such as 'Aim at File' or 'Grokker Desktop'.

Microsoft Index Server 2.0 does not detect changes to a catalog if the data is located on a [[Volume Mount Point|mounted partition]]. It does not support mounted volumes because of technical limitations in the file system.&lt;ref&gt;{{cite web
 | url = http://support.microsoft.com/kb/319506
 | title = INFO: Index Server Does Not Support Mounted Volumes (Revision: 1.0)
 | work = Microsoft Support
 | publisher = 10 May 2002
 | accessdate = 1 February 2011
}}&lt;/ref&gt;

== References ==
{{Reflist|refs=
&lt;ref name = "MIS-Intro"&gt;{{Cite web
  |url = http://msdn.microsoft.com/en-us/library/ms951563.aspx
  |title = Introduction to Microsoft Index Server
  |work = [[Microsoft Developer Network]]
  |publisher = Microsoft Corporation
  |date = 15 October 1997
  |accessdate = 1 February 2011
  |first1 = Krishna
  |last1 = Nareddy
  }}&lt;/ref&gt;
&lt;ref name = "MIS-v3"&gt;{{Cite web
  |url = http://msdn.microsoft.com/en-us/library/ms689644.aspx
  |title = Indexing Service Version 3.0
  |work = [[Microsoft Developer Network]]
  |publisher = Microsoft Corporation
  |date =
  |accessdate = 1 February 2011
  |first1 =
  |last1 =
  }}&lt;/ref&gt;
&lt;ref name = "WIS-What"&gt;{{Cite web
  |url = http://msdn.microsoft.com/en-us/library/ms689718.aspx
  |title = What is Indexing Service?
  |work = [[Microsoft Developer Network]]
  |publisher = Microsoft Corporation
  |date =
  |accessdate = 1 February 2011
  |first1 =
  |last1 =
  }}&lt;/ref&gt;
&lt;ref name="WIS-Install2008"&gt;{{Cite web
  |url = http://support.microsoft.com/kb/954822
  |title = How to install and configure the Indexing Service on a Windows Server 2008-based computer (Revision: 3.0)
  |work = Microsoft Support
  |publisher = Microsoft Corporation
  |date = 3 May 2010
  |accessdate = 1 February 2011
  }}&lt;/ref&gt;
&lt;ref name="TnC-144"&gt;{{Cite book
  |url = http://www.microsoft.com/downloads/en/details.aspx?FamilyId=1B6ACF93-147A-4481-9346-F93A4081EEA8&amp;displaylang=en
  |format = Microsoft Word
  |title = Threats and Countermeasures: Security Settings in Windows Server 2003 and Windows XP
  |edition = 2.0
  |publisher = Microsoft Corporation
  |page = 144
  |date=December 2005
  |first1 = Mike
  |last1  = Danseglio
  |first2 = Kurt
  |last2  = Dillard
  |first3 = José
  |last3  = Maldonado
  |first4 = Paul
  |last4  = Robichaux
  |editor1-first = Reid
  |editor1-last  = Bannecker
  |editor2-first = John
  |editor2-last  = Cobb
  |editor3-first = Jon
  |editor3-last  = Tobey
  |editor4-first = Steve
  |display-editors = 3 |editor4-last  = Wacker
  }}&lt;/ref&gt;
}}

{{Microsoft Windows components}}
{{DEFAULTSORT:Indexing Service}}
[[Category:Windows communication and services]]
[[Category:Desktop search engines|Desktop search engines]]
[[Category:Information retrieval systems]]
[[Category:Windows components]]</text>
      <sha1>akrag5u0pwinq19lntocw8v51f832z7</sha1>
    </revision>
  </page>
  <page>
    <title>MAREC</title>
    <ns>0</ns>
    <id>24979660</id>
    <revision>
      <id>666715167</id>
      <parentid>525865765</parentid>
      <timestamp>2015-06-13T03:58:02Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>move to Category:Information retrieval systems</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3401" xml:space="preserve">{{other uses}}
The '''MA'''trixware '''RE'''search '''C'''ollection ('''MAREC''') is a standardised patent data corpus available for research purposes. MAREC seeks to represent patent documents of several languages in order to answer specific research questions.&lt;ref&gt;Merz C., (2003) A Corpus Query Tool For Syntactically Annotated Corpora Licentiate Thesis, The University of Zurich, Department of Computation linguistic, Switzerland&lt;/ref&gt;&lt;ref&gt;Biber D., Conrad S., and Reppen R. (2000) Corpus Linguistics: Investigating Language Structure and Use. Cambridge University Press, 2nd edition&lt;/ref&gt; It consists of 19 million patent documents in different languages, normalised to a highly specific [[XML]] schema.

MAREC is intended as raw material for research in areas such as [[information retrieval]], [[natural language processing]] or [[machine translation]], which require large amounts of complex documents.&lt;ref&gt;Manning, C. D. and Schütze, H. (2002) Foundations of statistical natural language processing Cambridge, MA, Massachusetts Institute of Technology (MIT)  ISBN 0-262-13360-1.&lt;/ref&gt; The collection contains documents in 19 languages, the majority being English, German and French, and about half of the documents include full text.

In MAREC, the documents from different countries and sources are normalised to a common XML format with a uniform patent numbering scheme and citation format. The standardised fields include dates, countries, languages, references, person names, and companies as well as subject classifications such as [[International Patent Classification|IPC]] codes.&lt;ref&gt;European Patent Office (2009) [http://documents.epo.org/projects/babylon/eponet.nsf/0/1AFC30805E91D074C125758A0051718A/$File/guidelines_2009_complete_en.pdf Guidelines for examination in the European Patent Office], Published by European Patent Office, Germany (April 2009)&lt;/ref&gt;

MAREC is a comparable corpus, where many documents are available in similar versions in other languages. A comparable corpus can be defined as consisting of texts that share similar topics – news text from the same time period in different countries, while a parallel corpus is defined as a collection of documents with aligned translations from the source to the target language.&lt;ref&gt;Järvelin A. , Talvensaari T. , Järvelin Anni, (2008) Data driven methods for improving mono- and cross-lingual IR performance in noisy environments, Proceedings of the second workshop on Analytics for noisy unstructured text data, (Singapore)&lt;/ref&gt; Since the patent document refers to the same “invention” or “concept of idea” the text is a translation of the invention, but it does not have to be a direct translation of the text itself – text parts could have been removed or added for clarification reasons.

The 19,386,697 XML files measure a total of 621 GB and are hosted by the [[Information Retrieval Facility]]. Access and support are free of charge for research purposes.

== Use Cases ==
* MAREC is used in the [[Patent Language Translations Online (PLuTO)]] project.

== References ==
{{Reflist}}

== External links ==
* [http://www.ir-facility.org/prototypes/marec User guide and statistics]
* [http://ir-facility.org Information Retrieval Facility]

[[Category:Corpora]]
[[Category:Information retrieval systems]]
[[Category:Machine translation]]
[[Category:Natural language processing]]
[[Category:XML]]</text>
      <sha1>n66or125eserscibgq6yq690rl4fw0g</sha1>
    </revision>
  </page>
  <page>
    <title>Comparison of enterprise search software</title>
    <ns>0</ns>
    <id>41829011</id>
    <revision>
      <id>750936293</id>
      <parentid>749627004</parentid>
      <timestamp>2016-11-22T10:15:02Z</timestamp>
      <contributor>
        <username>Elise lowry</username>
        <id>29603353</id>
      </contributor>
      <comment>/* General information */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="56167" xml:space="preserve">{{Cleanup-list|date=June 2014}}

The following tables compare the major [[List of enterprise search vendors|enterprise search software]] vendors in their classes.

== General information ==

{| class="wikitable sortable"
|-
! Product !! formerly k.a. !! Vendor !! [[Software release life cycle|Stable release]] !! Update!! Platforms !! API !! Target Customer !! Software License !! Open source !! Multilingual !! Website
|-
|Mindbreeze InSpire
|Mindbreeze InSpire
|Mindbreeze 
|2016 Summer Release
|October 17th, 2016
|Windows Server Linux
|REST, SOAP, .NET, Java, Push API
|Information Insight, Knowledge management for departments/organizations, Big Data Search &amp; Analytics
|?
|No
|Yes (including languages like CJK)
|www.mindbreeze.com 
|-
| [[Lookeen Desktop Search#Lookeen Server|Lookeen Server]]&lt;ref&gt;[http://www.lookeen-server.com/produkt/overview Lookeen Server Enterprise Search]&lt;/ref&gt; || Lookeen Server || [[Axonic Informationssysteme GmbH]] || 1.3.1.1118 || April 2014 || Windows Server || [[.NET Framework|.NET]] || ? || ? || {{no}} || {{yes}} || http://www.lookeen-server.com
|-
| intergator || intergator Enterprise Search || [[intergator|interface projects GmbH]] || 5.3 || March 2016 || Windows Server, Linux Server || Java, Groovy und XML/JSON || Enterprise Search,Knowledge-Management, Content Analytics, Big Data || {{Yes}} || {{No}} || {{Yes}} || http://www.intergator.de 
|-
| Coveo Enterprise Search || Coveo Platform || [[Coveo|Coveo Solutions Inc]] || 7.0 || February 6, 2014 || Windows || [[REST]], [[SOAP]], [[.NET Framework|.NET]] || Web customer service, customer interaction hubs || ? || {{no}} || {{yes|Yes, multilingual user interfaces}} || http://www.coveo.com/en/advanced-enterprise-search
|-
| 3RDi Search || 3RDi Search || The Digital Group Inc || 1.0 || NA || Generic || Supported || Enterprise Search, Knowledge Management, Big Data, BI, Analytics || Commercial || {{no}} || {{Yes}} || http://www.3rdisearch.com
|-
| Endeca Guided Search || ? || [[Oracle Corporation|Oracle]] || 6.2.2 || March 2012 || ? || ? || ? || ? || ? || ? || http://www.oracle.com/us/products/applications/commerce/endeca/endeca-guided-search/overview/index.html
|-
| EXALEAD CloudView || ? || [[Dassault Systèmes]] || R2014 || July 2013 || Windows Server, Linux Server || Push API (PAPI) || ? || ? || {{no}} || {{yes|Yes, &lt;br /&gt;125 languages supported}} || http://www.3ds.com/products-services/exalead/
|-
|Datafari
|
|France Labs
|2.2.1
|April 2016
|Linux Server, Windows (for test)
|REST
|Enterprise Search, Knowledge management, Big Data, BI, Analytics
|No
|Yes
|Yes
|&lt;nowiki&gt;http://www.datafari.com/en&lt;/nowiki&gt;
|-
| FAST for SharePoint 2010 (F4SP) || [[Fast Search &amp; Transfer]] || [[Microsoft]] || 2010 (SP1) || ? || Windows Server, SharePoint || [[.NET Framework|.NET]] || ? || ? || {{no}} || ? || ?
|-
| SharePoint 2013 || [[Fast Search &amp; Transfer]] || [[Microsoft]] || 2010 (SP1) || ? || Windows Server, SharePoint || [[.NET Framework|.NET]] || ? || ? || {{no}} || ? || ?
|-
| [[Google Search Appliance]] (GSA) || ? || [[Google]] || 7.4 || March 2015 || ? || [[.NET Framework|.NET]], [[Java]] || ? || ? || ? || {{yes}} || https://www.google.com/enterprise/search/products/gsa.html
|-
| HP IDOL&lt;ref&gt;[http://www8.hp.com/us/en/software-solutions/information-data-analytics-idol/ HP IDOL]&lt;/ref&gt; || . || [[Hewlett Packard Enterprise|HPE BigData]] || 11 || March 2016 || Windows, HP-UX, Linux, Solaris || [[SOAP]], [[REST]] || big data and analytics, Enterprise search, Video, Image or Audio analytics, Knowledge-Management, Info-Governance || Yes, licensed by volume. Starting on 250Gb of Metadata || No || {{yes}} || http://www8.hp.com/us/en/software-solutions/information-data-analytics-idol/
|-
| Haven Search OnDemand&lt;ref&gt;[http://search.havenondemand.com/ Haven Search OnDemand]&lt;/ref&gt; || . || [[Hewlett Packard Enterprise|HPE BigData]] || 20160329|| March 2016 || SAAS Offering || [[REST]] || ? || Consumption-based pricing. || No || {{yes}} || http://search.havenondemand.com
|-
|-
| [[Funnelback]] Search || Panoptic Search || [[Funnelback]] || 15.6 || June 2016 || SaaS Offering, Windows server, Linux server || [[REST]] || Enterprise search, website search, vertical search || Document- and server-based licensing || {{no}} || {{yes|Yes.  Multi-lingual support includes indexing, querying and localised UIs}} || https://www.funnelback.com
|-
| IBM Infosphere Data Explorer || [[Vivisimo|Vivisimo Velocity]] || [[IBM]] || ? || ? || ? || ? || big data and analytics projects || ? || ? || {{yes}} || http://www-01.ibm.com/software/data/information-optimization/
|-
| [[Swiftype]] Search || [[Swiftype]] || [[Swiftype]] || ? || September 2014 || Windows and Linux, MacOS || [[REST]] APIs || websites and mobile applications || ? No{{No}} || ? || https://swiftype.com
|-
| Lucidworks Fusion || N/A || [[Lucidworks|Lucidworks Inc.]] || 2.1.4 || March 2016 || Windows, Linux, MacOS || [[REST]] APIs || Enterprise search, online retail, search-based data analytics || Licensed by CPU cores || {{No}} ||{{yes}}|| https://lucidworks.com/products/fusion/

|-
| Perceptive Search/ISYS || Enterprise Server || [[Lexmark|Lexmark Perceptive Software]] || 4.2 || ? || Windows, Linux, Solaris, Mac OS, HP-UX and AIX || [[REST]], [[SOAP]] || ? || ? || ? || {{yes}} || http://www.perceptivesoftware.com/products/perceptive-search
|-
| Secure Enterprise Search (SES) || ? || [[Oracle Corporation|Oracle]] || 11.2.2.2 || January 2014 || Windows, Linux (Oracle, RedHat and SUSE, 32-bit and 64 bit), Solaris || ? || ? || ? || ? || ? || http://www.oracle.com/technetwork/search/oses/documentation/ses-096384.html
|-
| RAVN Connect ||   || RAVN Systems || 3.3 || February 2014 || Windows Server, Linux Server || REST API || ? || ? || {{no}} || {{yes}} || http://www.ravn.co.uk/capabilities/enterprise-search/
|}

== Features ==

=== Content Collection &amp; Indexing ===

This section compares the ability of the products to collect and index content, both textual and non textual, from different data source types and document types (formats).

==== Indexing and connectivity ====

This is about indexing pipeline tools and processes; included connectors, support for connectors, etc.

===== Web-based =====

{| class="wikitable sortable"
|-
!  !! Description !! Lookeen Server&lt;ref&gt;[http://www.lookeen-server.com/produkt/overview Lookeen Server Enterprise Search]&lt;/ref&gt;!! HP IDOL&lt;ref&gt;[http://www.ndm.net/archiving/HP-Autonomy/information-connectivity Autonomy Information Connectivity]&lt;/ref&gt;!! Coveo for Advanced Enterprise Search&lt;ref&gt;[http://www.coveo.com/en/platform-features-connect#connectorsSection Coveo connectors]&lt;/ref&gt;!! Endeca Guided Search !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Swiftype Search !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search&lt;ref&gt;[http://www.oracle.com/technetwork/search/oses/overview/ses11222ds-1969734.pdf SES 11.2.2.2 Datasheet - Oracle]&lt;/ref&gt;!! RAVN Connect !! intergator !! Funnelback Search
|-
| [[HTTP]] || For crawling of Web servers. || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || Yes || ? || Yes || {{yes}} || ? || ? || {{yes}} || {{yes}} ||  {{yes}}
|-
| [[HTTPS]] || For crawling of secured Web servers. || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || Yes || ? || Yes || {{yes}} || {{yes}} || {{yes}} || {{yes}} || {{yes}} || {{yes}}  
|-
| [[XML]] || For indexing any XML-compliant data source. || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || Yes || ? || Yes || {{yes}} || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}} 
|-
|}

===== File-based =====

{| class="wikitable sortable"
|-
!  !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search 6.2.2 !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator !! Funnelback Search
|-
| [[NetWare File System|Netware file systems]]  || For incremental indexing of Netware file systems. || ? || {{yes}} || ? || ? || ? || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}} || {{no}}
|-
| [[Samba]]/[[Unix File System|Unix file systems]] ||  || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| [[Windows File System|Windows file systems]], Windows NT Filesystems (NTFS) ||  || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}
|-
|}

===== Archiving/Directory =====

{| class="wikitable sortable"
|-
!  !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search&lt;ref&gt;[http://onlinehelp.coveo.com/en/ces/7.0/Administrator/what_connectors_are_available_with_ces.htm What Connectors Are Available With Coveo]&lt;/ref&gt;!! Endeca Guided Search !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator !! Funnelback Search
|-
| LDAP || For indexing a company directory stored on a LDAP (v2 or v3) server. || ? ||  {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || {{yes}} || ? || ? || ? || ? || {{yes}} || {{yes}} || {{yes}}
|-
| [[Microsoft Active Directory]] || Supports Microsoft Active Directory. || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}} || {{yes}}
|-
| [[Symantec Enterprise Vault]] || . || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}} || {{no}}
|-
|}

===== Messaging =====

{| class="wikitable sortable"
|-
!  !! Description !! Lookeen Server || HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator
|-
| Akonix || . || ? || {{yes}} || ? || ? || ? || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || ?|| ?
|-
| [[Facetime]] || . || ? || {{yes}}|| ? || ? || ? || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || ?|| ?
|-
| [[IBM Lotus Connections]] || . || ? || ? || {{yes}} || ? || {{yes}} || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}}
|-
| [[IMAP]] || For indexing e-mail messages and attached files stored on an IMAP server. || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}}
|-
| [[IBM Lotus Notes]] || For indexing e-mail messages and attached files stored on a Lotus notes server. || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes &lt;br /&gt; (trigram:CLN)}} 
|?|| ? || ? || {{yes}} || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}}
|-
| [[Microsoft Exchange Server|Microsoft Exchange]] || Ability to retrieve and index e-mail messages and attached files &lt;br /&gt; (Mailboxes, Public Folders). || {{yes}} || {{yes}} || {{yes|Yes, &lt;br /&gt; Exchange 2003/2007/2010/2013 Servers}} || ? || {{yes}} || {{yes|Yes &lt;br /&gt; (trigram:CXG)}} 
|Yes|| ? || ? || {{yes|Yes &lt;br /&gt;(via Adhere Solutions partner)}} || ? || ? || ? || {{yes|Yes, &lt;br /&gt; Exchange 2003 Servers}} || {{yes}}|| {{yes}}
|-
| [[Microsoft Exchange Server|Microsoft Exchange Online]] || . || {{yes}} || ? || {{yes}} || ? || {{yes}} || . 
|Yes (IMAP)|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}}
|-
| [[NNTP]] || For real-time indexing of Usenet news groups. || ? ||  ? || ? || ? || ? || {{yes}} 
|?|| ? || ? || ? || ? || ? || ? || ? || ?|| {{yes}}
|-
| [[Gmail]] || . || ? || ? || {{yes}} || ? || {{yes}} || . 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}}
|-
|}

===== [[Content management system|CMS]], [[Document management system|DMS]] &amp; Social =====

{| class="wikitable sortable"
|-
!  !! Description !! Lookeen Server !! HP IDOL&lt;ref&gt;[http://www8.hp.com/us/en/software-solutions/asset/software-asset-viewer.html?asset=1997065&amp;module=1970565&amp;docname=4AA5-8715ENW&amp;page=1970341 KeyView IDOL - Product Brief]&lt;/ref&gt;!! Coveo for Advanced Enterprise Search&lt;ref&gt;[http://onlinehelp.coveo.com/en/ces/7.0/Administrator/what_connectors_are_available_with_ces.htm What connectors are available with Coveo]&lt;/ref&gt;!! Endeca Guided Search !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator
|-
| [[Alfresco (software)|Alfresco]] || . || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes, but no native support}} 
|Yes|| ? || ? || {{yes|Yes (via Incentro partner)}} || ? || {{yes}} || ? || ?  || {{yes}}|| {{yes}}
|-
| [[EMC Documentum|EMC Documentum Server]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes &lt;br /&gt; (trigram:CDO)}} 
|Yes|| ? || ? || {{yes}} || ? || ? || ? || {{yes}}  || {{yes}}|| {{yes}}
|-
| [[EPiServer|EpiServer]] || . || ? || {{yes}} || {{yes}} || ? || ? || {{no}} 
|?|| ? || ? || ? || ? || {{no}} || ? || ?  || ?|| {{yes}}
|-
| IBM Content Manager || . || ? || {{yes}} || ? || ? || {{yes}} || ? 
|?|| ? || ? || {{yes|Yes &lt;br /&gt;(via Adhere Solutions partner)}} || ? || ? || ? || ?  || {{yes}}|| {{yes}}
|-
| IBM FileNet || . || ? || {{yes}} || ? || ? || {{yes}} || ? 
|Yes (P8)|| ? || ? || {{yes|Yes &lt;br /&gt;(via Adhere Solutions partner)}} || ? || ? || ? || ?  || {{yes}}|| {{yes}}
|-
| [[WebSphere|IBM WebSphere]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes, Informatica PowerCenter 9.x connectivity}} 
|?|| ? || ? || {{yes|Yes (via Adhere Solutions partner)}} || ? || ? || ? || ? || {{yes}}|| {{yes}}
|-
| Jalios || . || ? || ? || ? || ? || {{yes}} || {{yes|Yes &lt;br /&gt; (trigram:CJA)}} 
|?|| ? || ? || ? || ? || ? || ? || ?  || ?|| ?
|-
| [[SharePoint]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes &lt;br /&gt; (trigram:CXG)}} 
|Yes|| ? || ? || {{yes|Microsoft SharePoint Portal Server &lt;br /&gt; Microsoft SharePoint Services}} || ? || {{yes}} || ? || ?  || {{yes}}|| {{yes}}
|-
| SharePoint Online || . || ? || ? || {{yes}} || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}}
|-
| [[Dropbox (service)|Dropbox]] || . || ? || ? || {{yes}} || ? || ? || ? 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || ?  || {{yes}}|| {{yes}}
|-
| Windows File Share || . || {{yes}} || ? || {{yes}} || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || ?  || {{yes}}|| {{yes}}
|-
| [[Google Docs]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}}
|-
| Jive || . || ? || ? || {{yes}} || ? || {{yes}} || ? 
|?|| ? || ? || ? || ? || {{yes}} || ? || ? || ?|| {{yes}}
|-
| [[Plumtree Software|Plumtree]] || . || ? || ? || {{yes}} || ? || {{yes}} || ? 
|?|| ? || ? || ? || ? || {{no}} || ? || ? || ?|| {{yes}}
|-
| Lithium || . || ? || ? || {{yes}} || ? || ? || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| ?
|-
| [[Confluence]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || ? 
|Yes|| ? || ? || {{yes|Yes (via partner)}} || ? || {{yes}} || ? || ?  || ? || {{yes}} 
|-
| [[Twitter]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || ?  || {{yes}}|| {{yes}} 
|-
| [[Facebook]] || . || ? || {{yes}} || ? || ? || {{yes}} || ? 
|No|| ? || ? || ? || ? || ? || ? || ?  || {{yes}}|| ? 
|-
| [[LinkedIn]] || . || ? || {{yes}} || ? || ? || {{yes}} || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| ? 
|-
|}

===== Databases =====

{| class="wikitable sortable"
|-
!  !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search&lt;ref&gt;[http://www.arnoldit.com/search-wizards-speak/coveo.html Coveo Solutions Inc., An Interview with Laurent Simoneau]&lt;/ref&gt;!! Endeca Guided Search !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator !! Funnelback Search
|-
| [[IBM DB2]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes, &lt;br /&gt; ETL:Informatica PowerCenter &lt;br /&gt; DB2 for Linux}} 
|Yes|| ? || ? || {{yes}} || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| [[JDBC]] ||  || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes, &lt;br /&gt; translates [[SQL database]] fields &lt;br /&gt; into XML documents &lt;br /&gt; than are then indexed together &lt;br /&gt;  with the document metadata}} 
|Yes|| ? || ? || Yes || ? || {{yes}} || ? || ?  || {{yes}}|| {{yes}} || {{yes}}
|-
| [[Microsoft SQL Server]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes, &lt;br /&gt; ETL:Informatica PowerCenter }} 
|Yes|| ? || ? || {{yes}} || ? || {{yes}} || ? || ?  || {{yes}}|| {{yes}} || {{yes}}
|-
| [[MySQL]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || ? 
|Yes|| ? || ? || {{yes}} || ? || {{yes}} || ? || ?  || {{yes}}|| {{yes}} || {{yes}}
|-
| [[ODBC]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes, &lt;br /&gt; ETL:Informatica PowerCenter }} 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| [[Oracle RDBMS]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes, &lt;br /&gt; ETL:Informatica PowerCenter }} 
|Yes|| ? || ? || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes}}|| {{yes}} || {{?}}
|-
| [[Sybase]] || . || ? || {{yes}} || ? || ? || {{yes}} || {{yes|Yes, &lt;br /&gt; ETL:Informatica PowerCenter }} 
|?|| ? || ? || {{yes}} || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{no}}
|-
|}

===== [[Customer relationship management|CRM]], [[Enterprise resource planning|ERP]], [[Product lifecycle management|PLM]], [[Business intelligence|BI]] =====

{| class="wikitable sortable"
|-
!  !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance&lt;ref&gt;[https://www.google.com/enterprise/marketplace/viewListing?productListingId=3905631+7212827882376498737 IBM DB2 Content Manager OD Connector for Google Search Appliance, by Adhere Solutions]&lt;/ref&gt;!! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator
|-
| [[Salesforce.com|Salesforce]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes &lt;br /&gt; (trigram:CSF)}} 
|Yes (via partner)|| ? || ? || {{yes|Yes (via partner)}} || ? || {{yes}} || ? || ? || {{yes}} || ?
|-
| [[SAP Business Suite|SAP]] || . || ? || {{yes}} || ? || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}} 
|-
| [[Siebel Systems|Siebel]]/Oracle || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes, ETL:Informatica }} 
|?|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}} || {{yes}} 
|-
| [[Microsoft Dynamics]] || . || ? || ? || {{yes}} || ? || ? || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}} 
|-
| [[Business Objects|Business Object]] || . || ? || {{yes}} || ? || ? || {{yes}} || ? 
|?|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}} || {{yes}} 
|-
| [[IBM]] [[Cognos]] || . || ? || {{yes}} || ? || ? || ? || ? 
|?|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}} || {{yes}} 
|-
| [[Informatica|Informatica PowerCenter]] || . || ? || ? || ? || ? || ? || {{yes|Yes &lt;br /&gt; (trigram:CJA)}} 
|?|| ? || ? || ? || ? || ? || ? || ? || ? || ? 
|-
| [[MicroStrategy]] || . || ? || {{yes}} || ? || ? || {{yes}} || ? 
|?|| ? || ? || ? || ? || ? || ? || {{yes|Yes (via partners)}} || ? || ? 
|-
| [[Windchill (software)|PTC Windchill]] || . || ? || {{yes}} || ? || ? || ? || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || ? ||{{yes}}
|-
| [[ENOVIA MatrixOne|ENOVIA]] || Support for MatrixOne/Enovia data. || ? || {{yes}} || ? || ? || ? || {{yes|Yes &lt;br /&gt; (trigram:CEN)}} 
|?|| ? || ? || {{yes|Yes (via partner)}} || ? || ? || ? || ? || ?|| {{yes}}
|-
|}

===== Miscellaneous =====

{| class="wikitable sortable"
|-
!  !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search 6.2.2  !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator
|-
| [[Jira (software)|Jira]] || . || ? || ? || {{yes}} || ? || ? || ? 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || ?|| {{yes}}
|-
| [[GitHub]] || . || ? || ? || ? || ? || ? || ? 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || ?|| ?
|-
| [[Slack (software)|Slack]] || . || ? || ? || ? || ? || ? || ? 
|?|| ? || ? || ? || ? || {{yes}} || ? || ? || ?|| ?
|-
| [[Mantis Bug Tracker|Mantis]] || . || ? || ? || {{yes}} || ? || ? || ? 
|Yes|| ? || ? || ? || ? || ? || ? || ? || ?|| {{yes}}
|-
| [[JD Edwards]]/Oracle || EnterpriseOne, World. || ? || {{yes}} || ? || ? || ? || {{yes|Yes &lt;br /&gt;ETL:Informatica PowerCenter }} 
|?|| ? || ? || ? || ? || ? || ? || ? || ?|| ?
|-
|-
| [[PeopleSoft]]/Oracle || . || ? || {{yes}} || ? || ? || ? || {{yes|Yes &lt;br /&gt;ETL:Informatica PowerCenter}} 
|?|| ? || ? || ? || ? || ? || ? || {{coming soon}} || ?|| ?
|-
|}

==== Supported Formats ====

{| class="wikitable sortable"
|-
! File type !! Description !! Lookeen Server !! HP IDOL  !! Coveo for Advanced Enterprise Search&lt;ref&gt;[http://onlinehelp.coveo.com/en/CES/7.0/User/Supported_File_Formats.htm Coveo Platform 7, Supported File Formats]&lt;/ref&gt;!! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView&lt;ref&gt;[http://3ds.exalead.com/software/common/pdfs/products/cloudview/Exalead-Connectors-and-Formats.pdf Exalead Cloudview Connectors + Formats]&lt;/ref&gt;
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator !! Funnelback Search
|-
| Adobe [[PDF]] || Includes Adobe Acrobat or other PDF documents. || {{yes}} || {{yes}} || {{yes|Yes, &lt;br /&gt; Version 1.0 to 1.7}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}
|-
| Web pages || [[HTML]], [[XHTML]], etc. || {{yes}} || {{yes|Yes, &lt;br /&gt; versions v.3, 4, 5}} || {{yes|.asp, .aspx, .cgi, .col, &lt;br /&gt; .dochtml, .dothtml, .fphtml, &lt;br /&gt; .hta, .htm, .html, .jsp, .php, &lt;br /&gt; .pothtml, .ppthtml, .shtm, &lt;br /&gt; .shtml, .xlshtml}} || ? || {{yes}} || {{yes|Yes, &lt;br /&gt; versions v.4.01 and above, and [[XHTML]]}} 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| [[XML]] || Extensible Markup Language (.xml) || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes, &lt;br /&gt; any [[Document type definition|DTD]]}} 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| Text || Raw text. || {{yes}} || {{yes}} || {{yes|.ascx, .bat, .cmd, .config, &lt;br /&gt; .csv, .dic, .exc, .inf, .ini, &lt;br /&gt; .js, .jsl, .log, .nfo, .scp, &lt;br /&gt; .sdl, .sln, .txt, .vbdproj, &lt;br /&gt; .vbs, .vdp, .vdproj, .vjp, &lt;br /&gt; .vjsproj, .vjsprojdata, &lt;br /&gt; .wsdl, .wsf, .wtx, .xsd &lt;br /&gt; ANSI, ASCII, Unicode}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| [[Microsoft Excel]] || Microsoft Excel Charts (.xls), &lt;br /&gt; Microsoft Excel XML (.xlsx, .xltm, .xltx) &lt;br /&gt; Others (.xlam, .xlb, .xlm, .xlsm) || {{yes}} || {{yes}} || {{yes|&lt;br /&gt; Version 5.0, 95(7.0), 97, 2000, XP, 2003, 2007, 2010. &lt;br /&gt; Indexes Excel 2010 attachments.}} || {{yes}} || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}
|-
| [[Microsoft Word]] || Microsoft Word (.doc) &lt;br /&gt; Microsoft Word XML (.docx, .dotx, .dotm) || {{yes}} || {{yes|Yes &lt;br /&gt; for Mac, Windows (multiple versions)}} || {{yes|Version 6.0, 6.0 (for MAC), 95 (7.0), 97, 98 (for MAC), 2000, &lt;br /&gt; XP, 2003, 2007, 2010 &lt;br /&gt; Indexes Word 2010 attachments.}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}
|-
| [[Microsoft PowerPoint]] || .pot, .potm, .potx, .ppam,  &lt;br /&gt; .pps, .ppsm, .ppsx, .ppt,  &lt;br /&gt; .pptm, .pptx || {{yes}} || {{yes}} || {{yes|Yes, &lt;br /&gt; Indexes PowerPoint 2010 attachments.}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}
|-
| [[Microsoft Access]] || MDB || ? || {{yes}} || ? || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}
|-
| [[Microsoft Project]] || MPP || ? || {{yes}} || ? || ? || {{yes}} || ? 
|?|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}} || {{no}}
|-
| [[Microsoft Visio]] || VSD || ? || {{yes|Yes &lt;br /&gt; (multiple versions)}} || ? || ? || ? || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}
|-
| [[Microsoft Outlook]] || Message, archives, and templates (.msg, .oft, .pst) || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}
|-
| [[MIME|MIME documents]] || Multipurpose Internet Mail Extension. || {{yes}} || {{yes|Yes, &lt;br /&gt; Microsoft Outlook Express Mac and PC (multiple versions) (.eml)}} || {{yes|.email, .eml, .ews, .mime &lt;br /&gt; MIME converter available with CES 7.0.5935+}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}
|-
| [[StarOffice|Sun StarOffice]] || . || ? || ? || ? || ? || {{yes}} || ? 
|?|| ? || ? || ? || ? || ? || ? || {{yes}} || ?|| {{yes}} || {{no}}
|-
| Lotus 1-2-3 || . || ? || ? || ? || ? || ? || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}
|-
| Lotus Freehand || . || ? || ? || ? || ? || ? || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}
|-
| [[Corel WordPerfect]] || Corel WordPerfect Linux (.wps) &lt;br /&gt; Corel WordPerfect Macintosh (.wps) &lt;br /&gt; Corel WordPerfect Windows (.wo) &lt;br /&gt; Corel WordPerfect Windows (.wpd) || ? || {{yes}} || ? || ? || ? || {{yes|Yes, &lt;br /&gt; version 6 and 7}} 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}
|-
| Archive files || . || ? || {{yes|7Z, DMG, HQX, BIZIP2, GZ, ISO, JAR, &lt;br /&gt; EMX, BIN, BKF, CAB, LZH/LHA &lt;br /&gt; ZIP, RAR, RTFD, TAR, Z, UUE &lt;br /&gt; multiple versions.}} || ? || ? || {{yes}} || {{yes|RAR, ZIP}} 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| RTF || Rich Text Format || {{yes}} || {{yes|Yes, &lt;br /&gt;multiple versions.}} || ? || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| Image files &lt;br /&gt; (text extraction) ||  || {{yes}} || ? || {{yes|Yes, &lt;br /&gt; .bmp, .jpeg, .max, .pcx/.dcx, &lt;br /&gt; .pdf, .png, .tiff, .tiff-fx &lt;br /&gt; requires the Optical Character Recognition (OCR) module.}} || ? || ? || ? 
|Yes (via partner)|| ? || ? || ? || ? || ? || ? || ?  || {{yes}}|| {{yes}} || {{no}}
|-
| Images &lt;br /&gt; (metadata extraction) || Creation of thumbnail. || {{yes}} || {{yes}} || {{yes|.bmp, .emf, .exif, .gif, &lt;br /&gt; .icon, .jpeg, .png, .tiff, &lt;br /&gt; .wmf}} || ? || {{yes}} || {{yes|Yes, &lt;br /&gt; JPEG, PNG, GIF, PNG}} 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| Audio &lt;br /&gt; (text extraction) ||  || ? || ? || {{yes|.aif, .aifc, .aiff, .asf, .au, &lt;br /&gt; .cda .mid, .midi, .mp1, .mp3, &lt;br /&gt; .mpga, .rmi, .snd, .wav , .wma &lt;br /&gt; Requires the Coveo Audio Video Search (CAVS) module.}} || ? || ? || ? 
|Yes (via partner)|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}
|-
| Audio (metadata) || Creation of thumbnail. || ? || ? || ? || ? || {{yes}} || {{yes|Yes, &lt;br /&gt; MP3, OGG.}} 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| Video ||  || ? || {{yes|.264, video/3gpp, .3mm, .4mp ... &lt;br /&gt; .avi, .m1v, .mov, .mp2, ...}} || {{yes|.avi, .m1v, .mov, .mp2, .mp2v, &lt;br /&gt; .mpa, .mpeg, .mpg, .mpv2 .qt, &lt;br /&gt; .rec, .rm, .rnx, .wm, .wmv &lt;br /&gt; Requires the Coveo Audio Video Search (CAVS) module.}} || ? || ? || ? 
|Yes (metadata)|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes|Yes. Metadata extraction.}}
|-
| MacroMedia Flash || . || ? || ? || ? || ? || {{yes|Yes, &lt;br /&gt; MacroMedia Flash text section and hypertext links}} || ? 
|Yes|| ? || ? || ? || ? || ? || ? || ? || ?|| {{yes}} || {{no}}
|-
| Autovue 2D &amp; 3D CAD files || . || ? || ? || ? || ?  || ? || ? 
|?|| ? || ? || ? || ? || ? || ? || {{yes|Yes, &lt;br /&gt; Open CAD files directly inside}} Autovue || ?|| {{yes}} || {{no}}
|-
| AutoCAD Drawing || AutoCAD Drawing (DWF), AutoCAD Drawing Exchange (DXF) || ? || {{yes|Yes, &lt;br /&gt; (multiple versions)}} || ? || ? || ? || {{yes}} 
|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}
|-
| [[CATIA]] versions 4 and 5 || Drawing document (CATDrawing), Part document (CATPart), &lt;br /&gt;  Assembly document (CATProduct), Model (V4 only) || ? || {{yes}} || ? || ? || ? || {{yes|Yes, &lt;br /&gt; any 5 version}} 
|?|| ? || ? || ? || ? || ? || ? || ? || ?|| {{yes}} || {{no}}
|-
| [[SolidWorks]] v1 || Drawing (.slddrw), Assembly (.sldasm), &lt;br /&gt; Part (.sldprt) || ? ||{{yes}} || ? || ? || ? || {{yes|Yes, &lt;br /&gt; 2003 to 2013 releases}} 
|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}
|-
| [[Pro/ENGINEER]] || Part (.prt), Assembly (.asm) || ? || ? || ? || ? || ? || {{yes|Yes, &lt;br /&gt; any release from R16 to 2001, from WF1 to WF5}} 
|?|| ? || ? || ? || ? || ? || ? || ? || ?|| {{yes}} || {{no}}
|-
| CAD Open formats || . || ? || ? || ? || ? || ? || {{yes|Yes, &lt;br /&gt; [[IGES]] version 5.2 and 5.3, [[STEP-File]]}} 
|Yes (metadata from the DWG CAD format)|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes|Yes. Metadata from DWG CAD}}
|-
|}

==== Text Analytics ====

{| class="wikitable sortable"
|-
! Linguistics !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance&lt;ref&gt;[http://www.bainsight.com/resources/Google-vs-Microsoft-FAST-Search-Whitepaper.pdf Microsoft and Google - BA Insight]&lt;/ref&gt;!! IBM Infosphere Data Explorer !! Lucidworks Fusion!! Perceptive Search !! Secure Enterprise Search&lt;ref&gt;[http://docs.oracle.com/cd/E14507_01/admin.1112/e14130.pdf Oracle Secure Enterprise Search Administrator's Guide]&lt;/ref&gt;!! RAVN Connect !! intergator !! Funnelback Search
|-
| Language detection || Ability to identify the languages at indexing time. || ? || ? || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| Synonyms/stemming  || Ability to treat as synonyms variations of keywords. || ? || ? || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| {{yes}} || ? || {{yes}} || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| [[Entity extraction|Named Entity Extraction]] || Ability to automatically extract entities such as persons, locations and organizations from indexed content. || ? || {{yes|Yes  &lt;br /&gt; named "Eduction"}} || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} || ? || {{yes|Yes &lt;br/&gt;With automatic entity linking}}|| {{yes}} || {{yes}}
|-
| Stop words || Ability to exclude stop words (e.g. 'an', 'the') in order to improve relevance. || {{yes}} || ? || {{yes}} || {{yes}} || {{yes}} || ? 
|Yes|| ? || ? || {{yes}} || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
|}

==== Audio &amp; Video Analytics ====

{| class="wikitable sortable"
|-
! Multimedia !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search 6.2.2  !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator
|-
| Audio analytics || Ability to understand topics being discussed, genders and emotional tones of speech, music, etc. || ? || ? || ? || ? || ? || ? 
|?|| {{no}} || ? || ? || ? || ? || ? || ? || {{yes|Yes &lt;br/&gt; speech, topics, music, keyword spotting }}|| ? 
|-
| Video analytics || Ability to understand the content of the video without relying on metadata (e.g. key framing, facial identification, logo recognition, etc.) || ? || {{yes}} || ? || ? || ? || ? 
|?|| {{no}} || ? || ? || ? || ? || ? || ? || {{yes}}|| ? 
|-
| Image analytics || Ability to detect patterns in image (e.g. faces, bodies, gender, age range, expression, etc.) || ? || {{yes}} || ? || ? || ? || ? 
|Yes (via partner)|| ? || ? || ? || ? || ? || ? || ? || ?|| {{yes}}
|-
|}

=== Search Experience ===

This section compares the ability of the products to :
* Enable the user to enter and execute the query
* Present the data to the user within seconds after the query is parsed and processed so that the user can find what he seeks quickly and act on it.

==== Search Language ====

{| class="wikitable sortable"
|-
! Query Parser !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search&lt;ref&gt;[http://onlinehelp.coveo.com/en/ces/7.0/User/search_prefix_and_operators.htm Search Prefixes and Operators]&lt;/ref&gt;!! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect&lt;ref&gt;[http://www.ravn.co.uk/wp-content/uploads/2014/02/CORE-Whitepaper-W.V.pdf RAVN Connect, CORE White paper]&lt;/ref&gt;!! intergator !! Funnelback Search &lt;ref&gt;https://docs.funnelback.com/query_language_help.html&lt;/ref&gt;
|-
| Wildcard search || Does the system use the asterisk ("*") and question mark ("?") character as a wildcard? || {{yes}} || ? || {{yes}} || ? || {{yes}} || ? 
|Yes|| ? || ? || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}
|-
| Fuzzy search || Does the system offer phonetic and approximate spelling search? (distinctions of syntax and semantics) || ? || ? || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}
|-
| Exact phrase search || Does the system enable to find words as a phrase? || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| [[Proximity search (text)|Proximity search]] || Support for advanced proximity operators - NEAR, BEFORE, AFTER. || ? || ? || {{yes}} || ? || ? || ? 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| Range search || Ability to match all terms which are lexically between square brackets ("[]") and curly braces ("{}"). || ? || ? || ? || ? || {{yes}} || ? 
|?|| ? || ? || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| Boosting a term || Automatic bigram and trigram relevancy boosting. || ? || ? || {{yes}} || ? || ? || ? 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| Boolean search || Does the system interprets Boolean operators? || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}
|-
| Graph search || Does the system keep relationships between fields and allows searching for them (while enabling full-text search)? || ? || ? || ? || ? || ? || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}
|-
|}

==== Usability ====

===== Search Query =====

This is the process of searching (querying).

{| class="wikitable sortable"
|-
!  !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search&lt;ref&gt;[http://www.coveo.com/en/news-releases/Coveo-Reports-Accelerated-Demand-for-Search-and-Relevance-Technology Coveo Reports Accelerated Demand for Search &amp; Relevance Technology in 2013]&lt;/ref&gt;!! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator !! Funnelback Search
|-
| Auto-complete || Does the system provide an automatic query guidance in the search box while typing? || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}&lt;ref&gt;https://docs.funnelback.com/auto_completion_collection_cfg.html&lt;/ref&gt;
|-
| Spell-checking || Does the system checks if the words in the query are spelled correctly and suggest corrections? || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes}}|| {{yes}}|| {{yes}} &lt;ref&gt;https://docs.funnelback.com/spelling_suggestions.html&lt;/ref&gt;
|-
| [[Federated search]] || The ability to send the same query simultaneously to several searchable sources. || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || ? 
|Yes|| {{yes}} || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} || {{yes}}|| {{yes}}|| {{yes}}&lt;ref&gt;https://docs.funnelback.com/ui_modern_extra_searches_collection_cfg.html&lt;/ref&gt;
|-
| Advanced search page || Does the system allow users to perform complex and sophisticated queries? || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || {{yes}} || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}}|| {{yes}}
|-
|}

===== Result-list =====

This is the process of scanning the content of any document directly from the result lists.

{| class="wikitable sortable"
|-
!  !! Description !! Lookeen Server !! HP IDOL&lt;ref&gt;[http://www.ndm.net/archiving/pdf/20130902_PI_B_HP_AUTN_IDOL10_web.pdf Autonomy KeyView IDOL - Product Brief - Ndm.net]&lt;/ref&gt;!! Coveo for Advanced Enterprise Search !! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance&lt;ref&gt;[http://static.googleusercontent.com/media/www.google.com/fr//support/enterprise/static/gsa/docs/admin/70/gsa_doc_set/quick_start/quick_start.pdf Getting the Most from Your Google Search Appliance]&lt;/ref&gt;!! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect!! intergator
|-
| Relevance ranking || Ability to find the highest quality and most relevant documents and bring them to the top of a search results list. || ? || ? || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| {{yes}} || {{yes}} || {{yes|Yes &lt;br /&gt; Google Site Search factors in more than 100 variables &lt;br /&gt; for each query}} || ? || {{yes}} || {{yes}} || ? || {{yes}}|| {{yes}}
|-
| Find similar || Ability to find similar links. || {{yes}} || ? || ? || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}}
|-
| Hit highlighting || Ability to highlight query key terms within the document in search result. || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| {{no}} || {{no}} || {{yes}} || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}}
|-
| Summarization &lt;br /&gt; (view as HTML) || Does the system offer content preview in the search result, so that users can judge relevance of results? || {{yes}} || {{yes}} || ? || ? || {{yes}} || {{yes}} 
|Yes|| {{partial|*only available for Office file types (Powerpoint, Excel, Visio, etc.)}} || {{partial}} || {{yes|Yes &lt;br /&gt; converts over 220 file formats into HTML}} || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}}
|-
|  || Does the system enable to copy/paste from within the preview? || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| {{no}} || {{no}} || {{yes}} || ? || ? || ? || ? || {{yes}}|| {{yes}}
|-
| Thumbnails and preview || Ability to generate thumbnails for a large amount of different file types. || ? || {{yes|Yes &lt;br /&gt; 100+ doc types}} || {{yes}} || ? || {{yes}} || {{yes|Yes, but low resolution for CATProducts &lt;br /&gt; no thumbnails for Pro/E assemblies}} 
|Yes|| {{partial|*only available for Office file types (.[[DOCX|DOCx]], .[[PPTX|PPTx]]) &lt;br /&gt; First page thumbnail preview}} || {{partial}} || ? || ? || ? || ? || ? || {{yes}}|| {{yes}}
|-
| Full document graphical preview || Ability to access the content of any document without having to open a windows client application. || ? || ? || ? || ? || {{yes}} || {{yes|Yes &lt;br /&gt; regardless of the file's original application}} 
|Yes|| {{partial|*only available for PowerPoint file types}} || {{yes}} || ? || ? || ? || ? || ? || {{yes|Yes &lt;br/&gt; Asynchronous loading}}|| {{yes}}
|-
| Document comparison || Ability to compare. || ? ||{{yes|Yes &lt;br /&gt; for version management, signature identification, among other features}} || ? || ? || {{yes}} || {{yes}} 
|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}}
|-
| Clustering || Ability to dynamically organize search results into groups. || ? || {{yes}} || ? || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || {{yes|Yes &lt;br /&gt; group search results by topic}} || ? || ? || ? || ? || {{yes}}|| {{yes}}
|-
| Sort by fields || Ability to sort all results by order of date or other attribute. || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || {{yes}} || ? || ? || ? || {{yes|Yes, &lt;br /&gt; „hard sort‟}} || {{yes}}|| {{yes}}
|-
|}

==== Faceted Navigation ====

This is the process of browsing the content by narrowing search results quickly in clicking filters that refine results based on related categories, so that users extract more meaning and insight from the content.

{| class="wikitable sortable"
|-
! [[Faceted Search|Facets]] !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search&lt;ref&gt;[http://onlinehelp.coveo.com/en/ces/7.0/user/about_facets.htm Coveo Facets]&lt;/ref&gt;!! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator
|-
| Multiple filters || Does the system enable the user to filter results in selecting multiple facet values? || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || {{yes}} || {{yes}} || ? || {{yes}}|| {{yes}}
|-
| Facet values and counts || Ability to display the term and the number of documents containing that term in the search results. || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}}
|-
| [[Faceted classification|Hierarchical]]- and range facets || . || ? || ? || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}}
|-
| Date, number and string types || Ability to filter by date/time, number and string data types. || {{yes}} || ? || {{yes}} || ? || {{yes}} || ? 
|Yes|| {{yes}} || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}}
|-
|}

==== Social and collaborative ====

This is the process of asking social network.

{| class="wikitable sortable"
|-
!  !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010&lt;ref&gt;[http://www.hcsolutions.at/produkte/ontolica/Documents/Ontolica_2010_feature_matrix.pdf Search Solution for Microsoft SharePoint]&lt;/ref&gt;!! SharePoint 2013&lt;ref&gt;[http://www.slideshare.net/SurfRay/new-sharepoint-server-2013-search-features Introduction to SharePoint 2013 Search]&lt;/ref&gt;!! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search&lt;ref&gt;[http://www.perceptivesoftware.com/images/psi_ds_perceptiveenterprisesearch.pdf Perceptive Enterprise Search - Product Datasheet]&lt;/ref&gt;!! Secure Enterprise Search !! RAVN Connect !! intergator !! Funnelback Search
|-
| Search result tagging || Ability to improve relevancy by creating or adding existing tags. || ? || ? || {{yes}} || ? || {{yes}} || {{yes}} 
|No|| {{yes}} || ? || ? || ? || ? || ? || {{yes}} || {{yes|Yes&lt;br/&gt;Real time}}|| {{yes}} || {{no|No. Deprecated in v14.0.1}}
|-
| Tag searching || Ability to search for a tag. || ? || ? || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}
|-
| Personalization/ Audience targeting || Ability to deliver more accurate targeted results. || ? || {{yes| Yes, &lt;br /&gt; browse histories, content contributors, and interactions, etc}}  || {{yes}} || ? || {{yes}} || ? 
|Yes|| {{yes}} || ? || {{yes| Yes, &lt;br /&gt; source, date, metadata and entities biaising }} || ? || ? || {{yes}} || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| Expertise location || Ability to find experts in users organization by searching on related keywords. || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes &lt;br /&gt; with [[Software development kit|SDK]]}} 
|Yes|| ? || ? || {{yes}} || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| Saved search || Ability to save searches. || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes|Yes &lt;br /&gt; with [[Software development kit|SDK]]}} 
|No|| {{no}} || {{no}} || ? || {{yes}} || ? || {{yes}} || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| Saved alerts || Ability to save alerts in order to notified when new content matching your queries has been added to the system. || ? || ? || ? || ? || {{yes}} || ? 
|Yes|| ? || {{no}} || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{no}}
|-
| Saved RSS feeds || Ability to save RSS feeds. || ? || ? || {{yes}} || ? || {{yes}} || ? 
|No|| ? || {{no}} || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
|}

==== Mobile support ====

{| class="wikitable sortable"
|-
!  !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari
!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator !! Funnelback Search
|-
| Mobile search || Does the system support mobile device access and search? || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}} || {{yes}}
|-
| Mobile UI || Does the system detect the user device (desktop, smartphone, tablet, etc.) and adapt itself based on it?  || ? || ? || {{yes}} || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes|Yes. Admin UI and Default Public UI use responsive designs.}}
|-
| Geolocation || Does the system enable from the end-user’s geolocation to provide additional context to filter? || ? || ? || {{yes}} || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| ? || {{yes}}
|-
| Compatibility || Is the system compatible with iOS. || ? || ? || {{yes}} || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || ? || ? || ? || ?|| {{yes}} || {{yes}}
|-
|  || Is the system compatible with Android? || ? || ? || {{yes}} || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || ? || ? || ? || ?|| {{yes}} || {{yes}}
|-
|  || Is the system compatible with Windows Phone? || ? || ? || {{yes}} || ? || {{yes}} || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || ?|| {{yes}} || ?
|-
|}

=== Administration &amp; Architecture ===

This section compares the flexibility in the underlying architecture, application development, the scalability and the administrative services of the products.

==== Management &amp; Search analytics ====

This table is about the ability to report on usage and activity (most popular queries, documents not found, etc.)

{| class="wikitable sortable"
|-
! [[Search analytics]] !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search&lt;ref&gt;[http://docs.oracle.com/cd/E35643_01/Workbench.211/pdf/WorkbenchUserGuide.pdf Endeca® Workbench: User's Guide - Oracle Documentation]&lt;/ref&gt;!! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator !! Funnelback Search
|-
| Web based administration interfaces &lt;br /&gt; (HTML) || . || ? || ? || {{yes}} || {{yes}} || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| Search statistics || Does the system collect search statistics? || ? || ? || {{yes}} || ? || {{yes}} || ? 
|Yes|| {{yes}} || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}
|-
| Search reports || Does the system enable to report search statistics? || ? || ? || {{yes}} || {{yes}} || {{yes}} || ? 
|Yes|| {{yes}} || ? || ? || ? || ? || ? || {{yes|Yes, &lt;br /&gt; most popular queries}} || {{yes}}|| {{yes}} || {{yes}}
|-
| Portal usage reports || Does the system enable to report on portal usage? || ? || ? || ? || {{yes|Yes, &lt;br /&gt; popular navigation}} || {{yes}} || {{yes|Yes &lt;br /&gt; with [[Software development kit|SDK]]}} 
|?|| {{no}} || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}
|-
| Document storage reports || . || ? || ? || {{yes}} || ? || {{yes}} || ? 
|Yes|| {{no}} || ? || ? || ? || ? || ? || {{yes|Yes, &lt;br /&gt; for documents not found}} || ?|| {{yes}} || {{yes}}
|-
| Custom reports || . || ? || ? || ? || ? || {{yes}} || {{yes|Yes &lt;br /&gt; with [[Software development kit|SDK]]}} 
|Yes|| {{no}} || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| Click Scoring || Ability to improve relevancy by enabling to track which results are most often clicked. || ? || ? || ? || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
|}

==== Interface flexibility ====

This is about tools to customize the interface,so that it adds value to any industry or business process.

{| class="wikitable sortable"
|-
!  !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect!! intergator !! Funnelback Search
|-
| Standard based open interface || Does the system support all client platforms? || ? || {{yes|Yes, &lt;br /&gt; HTTP and XML/JSON}} || ? || {{yes}} || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || {{yes|Yes, &lt;br /&gt; XML, JSON and HTTP}} || ? || ? || {{yes}}|| {{yes}} || {{yes|Yes.  HTML, JSON, XML, RSS and OpenSearch.}}
|-
| Page Layout Helper || Ability to change easily to global attributes (logo, fonts, header, and footer) and to the look of the Search Box and Search Results. || ? || ? || ? || ? || {{yes}} || ? 
|Yes|| {{no|No, action menu on search results not configurable}} || ? || {{yes}} || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{no}}
|-
| Stylesheet editor || Ability to make more extensive changes using [[XSLT]] stylesheet. || ? || ? || {{yes}} || ? || {{yes}} || {{yes|Yes &lt;br /&gt; Full customization with CSS or Java API &amp; XML}}  || {{yes}}
|Yes|| {{yes}} || ? || {{yes|Yes &lt;br /&gt;XSLT stylesheet editor}} || ? || {{yes}} || ? || {{yes}} || {{yes}}|| {{yes|Yes. Full results customisation via [[FreeMarker]]&lt;ref&gt;https://docs.funnelback.com/freemarker.html&lt;/ref&gt; and CSS/JS. Cached copies of XML templated via XSLT&lt;ref&gt;https://docs.funnelback.com/xslt_processing.html&lt;/ref&gt;}}
|-
|}

==== Scalability ====

{| class="wikitable sortable"
|-
!  !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search&lt;ref&gt;[http://onlinehelp.coveo.com/en/ces/7.0/Administrator/coveo_scalability_model.htm Scalability]&lt;/ref&gt;!! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect!! intergator
|-
| Index capacity || How many total documents can be indexed into the system? || up to 10 Million || ? || ? || ? || ? || ? 
|?|| Up to 100 Million || Up to 100 Million || ? || ? || ? || ? || ? || Scalable to billions|| Scalable to billions
|-
| Indexing rate || How rapidly documents can be added or reprocessed into the index? || real time || ? || ? || ? || ? || ? 
|Depends on setup and hardware|| ? || ? || ? || ? || ? || ? || ? || per requirement, scalable || ?
|-
| Query-processing speed || How many queries per second ([[Queries per second|QPS]]) the engine can process? || ? || 2,000 &lt;br /&gt; across all indexed data with sub-second response times || ? || ? || ? || ? 
|Depends on setup and hardware|| ? || ? || ? || ? || ? || ? || scalable || || Depands on the datasource. (sub)-second times 
|-
|}

==== Platform readiness ====

{| class="wikitable sortable"
|-
! [[Search analytics]] !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator
|-
| General system requirement || Minimum of available disk space. || 100 MB || ? || ? || ? || ? || 25 GB 
|?|| ? || ? || ? || ? || ? || ? || ? || 10 GB || 25 GB
|-
|  || Minimum RAM. || 8GB || ? || ? || ? || ? || 8 GB 
|?|| ? || ? || ? || ? || ? || ? || ? || 8 GB || 4 GB
|-
|  || Hard drives to store the data files. || ? || ? || ? || ? || ? || SCSI, SAS, SAN, over FC or SSD disks &lt;br /&gt; (as opposed to SATA disks) 
|?|| ? || ? || ? || ? || ? || ? || ? || optimised for throughput&lt;br/&gt; SCSI, SAS, SAN || SCSI, SAS, SAN, SSD, HDD
|-
|}

=== Vendor Intangibles ===

This section compares each software investment.

{| class="wikitable sortable"
|-
! Investment !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect || intergator
|-
| Hardware costs || Total costs of servers. || 0 || ? || ? || ? || ? || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || ? || Depands on the base licence
|-
| Installation costs || . || ? ||? || ? || ? || ? || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || ?|| ?
|-
| License costs || . || ? ||? || ? || ? || ? || ? 
|0|| ? || ? || 500,000 Documents – $28,387 &lt;br /&gt; 1 million documents – $66,236 &lt;br /&gt; Upgrade from 1 million to 2 million documents – $1,971 &lt;br /&gt; 2 Million documents – $123,010 &lt;br /&gt; 2 Million documents – $123,010 &lt;br /&gt; 3 Million documents – $113, 548 &lt;br /&gt; 3 Million documents – $158,967 &lt;br /&gt; 5 Million documents – $433,766 &lt;br /&gt; 10 Million documents – $305,066 &lt;br /&gt; 10 Million – $423,913 &lt;br /&gt; 15 Million documents – $533,896 &lt;br /&gt; 15 Million documents with hot backup – $615,053 &lt;br /&gt; 30 Million documents with hot backup – $993,548 || ? || ? || ? || ? || ?|| ?
|-
| Annual Maintenance || Annual Maintenance fees. || ? || ? || ? || ? || ? || ? 
|Per server|| ? || ? || ? || ? || ? || ? || ?|| ?|| ?
|-
|}

== References ==

{{reflist}}

== First version ==

[[Category:Information retrieval systems]]</text>
      <sha1>nh4dm2u5yxeajo2zl139f5mcmeiyv0f</sha1>
    </revision>
  </page>
  <page>
    <title>Greenpilot</title>
    <ns>0</ns>
    <id>26926858</id>
    <revision>
      <id>666861313</id>
      <parentid>622945390</parentid>
      <timestamp>2015-06-14T05:52:42Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>Category:Information retrieval systems</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6902" xml:space="preserve">{{COI|date=April 2010}}
The online portal '''Greenpilot''' is a service provided by the German National Library of Medicine, ZB MED.

The project is funded by the German Research Foundation ([[Deutsche Forschungsgemeinschaft]]) and gets its technical support from  [[Averbis]] Ltd. The portal first went online May 29, 2009 and currently runs in the updated beta version. In the context of the 'Germany - Land of Ideas' (Deutschland - Land der Ideen) initiative under the patronage of the [[President of Germany]] [[Horst Köhler]] the ZB MED was awarded the distinction 'Selected Landmark 2009' (Ausgewählter Ort 2009).&lt;ref&gt;[http://idw-online.de/pages/de/news315583 Pressemitteilung im Informationsdienst Wissenschaft vom 15. Mai 2009 ]&lt;/ref&gt;

==Objective==
The Greenpilot portal is a [[digital library]] specialised in the fields of Nutritional, Agricultural and Environmental Sciences. It aims to provide researchers in the three fields with a collection of scientific literature which is easy to access and of high quality. Especially the [[gray literature]] is often difficult to find and retrieve for the average user so Greenpilot also aims to make access to these sources easier. The service addresses itself not only to scientists and students but also to the broadly interested public. Greenpilot has been modelled after the corresponding digital library for Medicine, Medpilot,&lt;ref&gt;[http://www.medpilot.de/ Medpilot portal]&lt;/ref&gt; also a project of the German National Library of Medicine. The ZB MED has chosen the slogan 'Greenpilot - all about life and science' as a motto. In Greenpilot scientifically relevant databases, library catalogues and websites can be searched by entering a search term and the results are presented in a standardised web interface.

==Technical Background==
Greenpilot is a search engine based on intuitive search engine technology. The portal's software was developed in the programming language [[Perl]]. The search engine technology is based upon the 'Averbis Search Platform' software developed by the Averbis Ltd. and uses the [[open source]] software [[Lucene]]. Functionally this is an expert search engine which centres around the intelligent semantic connection of search terms by means of a standardised vocabulary. This is made possible by Averbis's MSI software which provides:

* semantic search optimised for the fields of Medicine and Life Sciences
* a contextual analysis of texts taking synonyms and compounds into account
* multilingual and cross-language search
* linking of lay and expert vocabulary
The search results are generated from a search index.

Additionally a [[metasearch]] can be conducted in order to search other databases not contained in the index. This search is based upon individual results from the specific database searched.

==Contents==
The Greenpilot portal integrates various scientifically relevant information resources under a uniform search interface. These resources are diverse and encompass national and international expert databases, library catalogues of national libraries with a focus on specific topics, full text documents from [[open access (publishing)|open access]] journals as well as information contained on about one thousand scientifically relevant websites selected for Greenpilot.
The following is a list of sources from November 2009:&lt;ref&gt;[http://www.greenpilot.de/beta2/app/misc/help/8cafcf93601eb861aaef86b5ce99ecdc/Datenbanken List of databases in Greenpilot]&lt;/ref&gt;

===Library Catalogues===
* Catalogue of the German National Library of Medicine (ZB MED Nutrition. Environment. Agriculture)
* Catalogue of the German National Library of Medicine (ZB MED Medicine. Health)
* Catalogue of the Bonn University Library
* Library catalogues of scientifically relevant departments within the collective library network (GBV)
* Catalogue of the Federal Ministry of Food, Agriculture and Consumer Protection (BMELV)
* Catalogue of the Johann Heinrich von Thünen-Institut (vTI), Federal Research Institute for Rural Areas, Forestry and Fisheries
* Catalogue of the Julius Kühn-Institut, Federal Research Centre for Cultivated Plants
* Catalogue of the Friedrich Löffler-Institut, Federal Research Institute for Animal Health
* Catalogue of the Max Rubner-Institut, Federal Research Institute for Nutrition and Food
* Catalogue of the Federal Institute for Risk Assessment
* Catalogue of the Leibniz Institute for Marine Science (IFM-GEOMAR)
* Catalogue of the Leibniz Institute for Plant Genetics and Crop Plant Research (IPK-Plant Genetics and Crop Plant)
* Catalogue of the Leibniz Institute for Plant Biochemistry (IPB-Plant Chemistry)
* Catalogue of the special collection inshore and deep-sea fishery
* Catalogue of the University of Veterinary Medicine Hannover (TiHo-Veterinary Sciences)
* Catalogue of the German National Library of Economics (ZBW)

===Bibliographic databases===
* AGRIS (1975–2008), FAO ( Food and Agriculture Organization of the United Nations)
* VITIS-VEA, Viticulture and Enology Abstracts
* Medline (2004–2009)
* UFORDAT, Environmental Research Database (UBA)
* ULIDAT, Environmental Literature Database (UBA)
* ELFIS, International Information System for the Agricultural Sciences and Technology

===Relevant Internet Sources===
* Reviewed list of [[URL]]s selected by the ZB MED Nutrition. Environment. Agriculture
* Open Access journals with full text documents

===Metasearch===
* GetInfo, the knowledge portal for Technical Science provided by the Library for Technical Sciences (TIB) and the professional information centres FIZ Technik Frankfurt, FIZ Karlsruhe and FIZ CHEMIE Berlin.
* ECONIS, Catalogue of the German National Library of Economics (ZBW).

==Other Features==

===Search and results page===
* Search and advanced search
* Context sensitive help function
* [[Truncation]] and [[Boolean function]]s
* Personalised refining of search results by filtering for a specific document type, language or database
* [[Bookmark]]s

===Document ordering===
* Ordering directly from the results page is made possible by using the document delivery service of the ZB MED or the Electronic Journals Library ([[Elektronische Zeitschriftenbibliothek]]).

===Personalisation===
* My Greenpilot: a feature requiring the user to sign up for an account. The service is free of charge and offers an overview of ordered documents as well as enabling individual managing of customer data.

==See also==
*[[List of digital library projects]]
*[[vascoda]]

==References==
&lt;references /&gt;

==External links==
* [http://www.greenpilot.de Greenpilot website]
* [http://www.zbmed.de/home.html?lang=en Website of the German National Library of Medicine, ZB MED]
* [http://www.land-of-ideas.org Germany - Land of Ideas website]

{{coord missing|Germany}}

[[Category:Libraries in Germany]]
[[Category:Information retrieval systems]]
[[Category:Internet search engines]]</text>
      <sha1>3oam3b5gw8os229s7xq51p1hp9iwiz4</sha1>
    </revision>
  </page>
  <page>
    <title>Poliqarp</title>
    <ns>0</ns>
    <id>2398780</id>
    <revision>
      <id>666861518</id>
      <parentid>607634522</parentid>
      <timestamp>2015-06-14T05:53:44Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>Category:Information retrieval systems</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="943" xml:space="preserve">'''Poliqarp''' is an [[open source]] [[search engine]] designed to process [[text corpus|text corpora]], among others the [[National Corpus of Polish]] created at the Institute of Computer Science, [[Polish Academy of Sciences]].

==Features==
* Custom [[query language]]
* Two-level [[regular expressions]]:
** operating at the level of characters in words
** operating at the level of words in statements/paragraphs
* Good performance
* Compact corpus representation (compared to similar projects)
* Portability across operating systems: [[Linux]]/[[BSD]]/[[Win32]]
* Lack of portability across [[endianness]] (current release works only on little endian devices)

==External links==
* [http://www.korpus.pl/index.php?lang=en&amp;page=welcome Polish corpus website (in English)]
* [http://poliqarp.sourceforge.net/ Project website on SourceForge]
* [http://poliqarp.suxx.pl/ Search plugin for Firefox]

[[Category:Information retrieval systems]]</text>
      <sha1>j87cyjiu7n2wkh0vayn6d2n7vnyzlji</sha1>
    </revision>
  </page>
  <page>
    <title>Database search engine</title>
    <ns>0</ns>
    <id>7330158</id>
    <revision>
      <id>754379935</id>
      <parentid>689538597</parentid>
      <timestamp>2016-12-12T10:08:00Z</timestamp>
      <contributor>
        <username>The Transhumanist</username>
        <id>1754504</id>
      </contributor>
      <comment>/* See also */ lists of related topics</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1943" xml:space="preserve">A '''database search engine''' is a [[search engine]] that operates on material stored in a digital [[database]].

== Search engines ==

Categories of search engine software include: 
* Web search or full-text search (e.g. [[Lucene]]).
* [[Database]] or [[structured data]] search (e.g. [[Dieselpoint]]).
* Mixed or [[enterprise search]] (e.g. [[Google Search Appliance]]). 

The largest online directories, such as [[Google]] and [[Yahoo]], utilize thousands of computers to process billions of website documents using [[web crawlers]] or [[spiders (software)]], returning results for thousands of searches per second. Processing high query volumes requires software to run in a distributed environment with redundancy.

== Components ==

Searching for textual content in [[databases]] or [[structured data]] formats (such as [[XML]] and [[Comma-separated values|CSV]]) presents special challenges and opportunities which specialized search engines resolve. [[Databases]] allow logical queries such as the use of multi-field [[Boolean logic]], while full-text searches do not. "Crawling" (a human by-eye search) is not necessary to find information stored in a database because the data is already structured. [[index (datatbase)|Indexing]] the data allows for faster searches.

Database search engines are usually included with major database software products. 

== Applications ==

Database search technology is used by large public and private entities including government database services, e-commerce companies, online advertising platforms, telecommunications service providers and other consumers with a need to access information in large repositories.

==See also==

*[[Outline of search engines]]
*[[List of search engines]]

==External links==
* [http://www.searchtools.com/info/database-search.html Searching for Text Information in Databases]

{{DEFAULTSORT:Search Engine Technology}}
[[Category:Information retrieval systems]]</text>
      <sha1>ekhagsu3irs74gw0504opjysdgbfhn2</sha1>
    </revision>
  </page>
  <page>
    <title>Search engine (computing)</title>
    <ns>0</ns>
    <id>27804</id>
    <revision>
      <id>744163945</id>
      <parentid>722592416</parentid>
      <timestamp>2016-10-13T14:12:39Z</timestamp>
      <contributor>
        <ip>199.15.104.190</ip>
      </contributor>
      <comment>Lisa wichkoski</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5310" xml:space="preserve">{{multiple issues|
{{more footnotes|date=August 2014}}
{{one source|date=August 2014}}
}}

A '''search engine''' is an [[information retrieval|information retrieval system]] designed to help find information stored on a [[computer system]]. The search results are usually presented in a list and are commonly called ''hits''. Search engines help to minimize the time required to find information and the amount of information which must be consulted, akin to other techniques for managing [[information overload]]. {{Citation needed|date=December 2007}}

The most public, visible form of a search engine is a [[Web search engine]] which searches for information on the [[World Wide Web]].

==How search engines work==
Search engines provide an [[interface (computer science)|interface]] to a group of items that enables users to specify criteria about an item of interest and have the engine find the matching items. The criteria are referred to as a [[Web search query|search query]]. In the case of text search engines, the search query is typically expressed as a set of words that identify the desired [[concept]] that one or more [[document]]s may contain.&lt;ref&gt;Voorhees, E.M. [http://www.indexnist.gov/itl/iad/894.02/works/papers/nlp_ir.ps Natural Language Processing and Information Retrieval]. National Institute of Standards and Technology. March 2000.&lt;/ref&gt; There are several styles of search query [[syntax]] that vary in strictness. It can also switch names within the search engines from previous sites.  Whereas some text search engines require users to enter two or three words separated by [[Whitespace (computer science)|white space]], other search engines may enable users to specify entire documents, pictures, sounds, and various forms of [[natural language]]. Some search engines apply improvements to search queries to increase the likelihood of providing a quality set of items through a process known as [[query expansion]].

[[Image:search-engine-diagram-en.svg|right|thumb|Index-based search engine]]

The list of items that meet the criteria specified by the query is typically sorted, or ranked. Ranking items by relevance (from highest to lowest) reduces the time required to find the desired information. [[probability|Probabilistic]] search engines rank items based on measures of [[String metric|similarity]] (between each item and the query, typically on a scale of 1 to 0, 1 being most similar) and sometimes [[popularity]] or [[authority]] (see [[Bibliometrics]]) or use [[relevance feedback]]. [[Boolean logic|Boolean]] search engines typically only return items which match exactly without regard to order, although the term ''boolean search engine'' may simply refer to the use of boolean-style syntax (the use of operators [[Logical_conjunction|AND]], [[Logical_disjunction|OR]], NOT, and [[Exclusive nor gate|XOR]]) in a probabilistic context.

To provide a set of matching items that are sorted according to some criteria quickly, a search engine will typically collect [[metadata]] about the group of items under consideration beforehand through a process referred to as [[Index (search engine)|indexing]]. The index typically requires a smaller amount of [[computer storage]], which is why some search engines only store the indexed information and not the full content of each item, and instead provide a method of navigating to the items in the [[Serpent (album)|search engine result page]]. Alternatively, the search engine may store a copy of each item in a [[cache (computing)|cache]] so that users can see the state of the item at the time it was indexed or for archive purposes or to make repetitive processes work more efficiently and quickly.

Other types of search engines do not store an index. Crawler, or spider type search engines (a.k.a. real-time search engines) may collect and assess items at the time of the search query, dynamically considering additional items based on the contents of a starting item (known as a seed, or seed URL in the case of an Internet crawler). [[Meta search engine]]s store neither an index nor a cache and instead simply reuse the index or results of one or more other search engines to provide an aggregated, final set of results.

==Types of search engines==

; By source

*[[Desktop search]]
*[[Federated search]]
*[[Human search engine]]
*[[Metasearch engine]]
*[[Multisearch]]
*[[Search aggregator]]
*[[Web search engine]]

; By content type

*[[Full text search]]
*[[Image search]]
*[[Video search engine]]

; By interface

*[[Incremental search]]
*[[Instant answer]]
*[[Semantic search]]
*[[Selection-based search]]

; By topic

*[[Bibliographic database]]
*[[Enterprise search]]
*[[Medical literature retrieval]]
*[[Vertical search]]

==See also==
{{Portal|Computer Science}}
{{div col|colwidth=30em}}
*[[Automatic summarization]]
*[[Emanuel Goldberg]] (inventor of early search engine)
*[[Index (search engine)]]
*[[Inverted index]]
*[[List of search engines]]
*[[List of enterprise search vendors]]
*[[Search engine optimization]]
*[[Search suggest drop-down list]]
* [[Solver (computer science)]]
*[[Spamdexing]]
*[[SQL]]
*[[Text mining]]
{{div col end}}

==References==
{{Reflist}}
{{Internet search}}

{{Authority control}}
{{DEFAULTSORT:Search Engine (Computing)}}
[[Category:Information retrieval systems]]</text>
      <sha1>bi2y6wshmxtidx95o72za2ne8whsvpl</sha1>
    </revision>
  </page>
  <page>
    <title>Sørensen–Dice coefficient</title>
    <ns>0</ns>
    <id>9701718</id>
    <revision>
      <id>753370759</id>
      <parentid>753370690</parentid>
      <timestamp>2016-12-06T19:41:17Z</timestamp>
      <contributor>
        <username>Nnescio</username>
        <id>12634958</id>
      </contributor>
      <comment>/* Formula */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8611" xml:space="preserve">The '''Sørensen–Dice index''', also known by other names (see [[Sørensen–Dice_coefficient#Name|Name]], below), is a [[statistic]] used for comparing the similarity of two [[Sample (statistics)|samples]]. It was independently developed by the [[botanist]]s [[Thorvald Sørensen]]&lt;ref&gt;{{cite journal |last=Sørensen |first=T. |year=1948 |title=A method of establishing groups of equal amplitude in [[plant sociology]] based on similarity of species and its application to analyses of the vegetation on Danish commons |journal=[[Kongelige Danske Videnskabernes Selskab]] |volume=5 |issue=4 |pages=1–34 |doi= }}&lt;/ref&gt; and [[Lee Raymond Dice]],&lt;ref&gt;{{cite journal |last=Dice |first=Lee R. |title=Measures of the Amount of Ecologic Association Between Species |jstor=1932409 |journal=Ecology |volume=26 |issue=3 |year=1945 |pages=297–302 |doi=10.2307/1932409 }}&lt;/ref&gt; who published in 1948 and 1945 respectively.
The Sørensen–Dice is also known as [[F1 score]] or Dice similarity coefficient (DSC).

==Name==
The index is known by several other names, usually '''Sørensen index''' or '''Dice's coefficient'''. Both names also see "similarity coefficient", "index", and other such variations. Common alternate spellings for Sørensen are Sorenson, Soerenson index and Sörenson index, and all three can also be seen with the –sen ending.

Other names include:
*[[Jan Czekanowski|Czekanowski]]'s binary (non-quantitative) index&lt;ref name ="gallagher"/&gt;

==Formula==
Sørensen's original formula was intended to be applied to presence/absence data, and is

:&lt;math&gt; QS =  \frac{2 |X \cap Y|}{|X|+ |Y|}&lt;/math&gt;

where |''X''| and |''Y''| are the numbers of elements in the two samples. Based on what is written here,

:&lt;math&gt; DSC = \frac{2 TP}{2 TP + FP + FN}&lt;/math&gt;,

as compared with the Jaccard index, which omits true negatives from both the numerator and the denominator. QS is the quotient of similarity and ranges between 0 and&amp;nbsp;1.&lt;ref&gt;http://www.sekj.org/PDF/anbf40/anbf40-415.pdf&lt;/ref&gt; It can be viewed as a similarity measure over sets.

Similarly to the [[Jaccard index]], the set operations can be expressed in terms of vector operations over binary vectors ''A'' and ''B'':

:&lt;math&gt;s_v = \frac{2 | A \cdot B |}{| A |^2 + | B |^2} &lt;/math&gt;

which gives the same outcome over binary vectors and also gives a more general similarity metric over vectors in general terms.

For sets ''X'' and ''Y'' of keywords used in [[information retrieval]], the coefficient may be defined as twice the shared information (intersection) over the sum of cardinalities :&lt;ref&gt;{{cite book |last=van Rijsbergen |first=Cornelis Joost |year=1979
|title=Information Retrieval
|url=http://www.dcs.gla.ac.uk/Keith/Preface.html |publisher=Butterworths |location=London |isbn=3-642-12274-4 }}&lt;/ref&gt;

When taken as a string similarity measure, the coefficient may be calculated for two strings, ''x'' and ''y'' using [[bigram]]s as follows:&lt;ref&gt;{{cite conference |last=Kondrak |first=Grzegorz |author2=Marcu, Daniel |author3= Knight, Kevin  |year=2003
|title=Cognates Can Improve Statistical Translation Models
|booktitle=Proceedings of HLT-NAACL 2003: Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics
|pages=46–48 |url=http://aclweb.org/anthology/N/N03/N03-2016.pdf}}&lt;/ref&gt;

:&lt;math&gt;s = \frac{2 n_t}{n_x + n_y}&lt;/math&gt;

where ''n''&lt;sub&gt;''t''&lt;/sub&gt; is the number of character bigrams found in both strings, ''n''&lt;sub&gt;''x''&lt;/sub&gt; is the number of bigrams in string ''x'' and ''n''&lt;sub&gt;''y''&lt;/sub&gt; is the number of bigrams in string ''y''. For example, to calculate the similarity between:

:&lt;code&gt;night&lt;/code&gt;
:&lt;code&gt;nacht&lt;/code&gt;

We would find the set of bigrams in each word:
:{&lt;code&gt;ni&lt;/code&gt;,&lt;code&gt;ig&lt;/code&gt;,&lt;code&gt;gh&lt;/code&gt;,&lt;code&gt;ht&lt;/code&gt;}
:{&lt;code&gt;na&lt;/code&gt;,&lt;code&gt;ac&lt;/code&gt;,&lt;code&gt;ch&lt;/code&gt;,&lt;code&gt;ht&lt;/code&gt;}

Each set has four elements, and the intersection of these two sets has only one element: &lt;code&gt;ht&lt;/code&gt;.

Inserting these numbers into the formula, we calculate, ''s''&amp;nbsp;=&amp;nbsp;(2&amp;nbsp;·&amp;nbsp;1)&amp;nbsp;/&amp;nbsp;(4&amp;nbsp;+&amp;nbsp;4)&amp;nbsp;=&amp;nbsp;0.25.

==Difference from Jaccard ==
This coefficient is not very different in form from the [[Jaccard index]].  However, since it doesn't satisfy the triangle inequality, it can be considered a [[Metric (mathematics)#Generalized metrics|semimetric]] version of the Jaccard index.&lt;ref name ="gallagher"/&gt;

The function ranges between zero and one, like Jaccard. Unlike Jaccard, the corresponding difference function

:&lt;math&gt;d = 1 -  \frac{2 | X \cap Y |}{| X | + | Y |} &lt;/math&gt;

is not a proper distance metric as it does not possess the property of [[triangle inequality]].&lt;ref name ="gallagher"&gt;Gallagher, E.D., 1999. [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.9.1334&amp;rep=rep1&amp;type=pdf COMPAH Documentation], University of Massachusetts, Boston&lt;/ref&gt; The simplest counterexample of this is given by the three sets {a}, {b}, and {a,b}, the distance between the first two being 1, and the difference between the third and each of the others being one-third. To satisfy the triangle inequality, the sum of ''any'' two of these three sides must be greater than or equal to the remaining side. However, the distance between {a} and {a,b} plus the distance between {b} and {a,b} equals 2/3 and is therefore less than the distance between {a} and {b} which is 1.

==Applications==
The Sørensen–Dice coefficient is useful for ecological community data (e.g. Looman &amp; Campbell, 1960&lt;ref&gt;[http://links.jstor.org/sici?sici=0012-9658%28196007%2941%3A3%3C409%3AAOSK%28F%3E2.0.CO%3B2-1 Looman, J. and Campbell, J.B. (1960) Adaptation of Sorensen's K (1948) for estimating unit affinities in prairie vegetation. Ecology 41 (3): 409–416.]&lt;/ref&gt;). Justification for its use is primarily  empirical rather than theoretical (although it can be justified  theoretically as the intersection of two [[fuzzy set]]s&lt;ref&gt;[http://dx.doi.org/10.1007/BF00039905 Roberts, D.W. (1986) Ordination on the basis of fuzzy set theory. Vegetatio 66 (3): 123–131.]&lt;/ref&gt;). As compared to [[Euclidean distance]], Sørensen distance retains sensitivity in more heterogeneous data sets and gives less weight to outliers.&lt;ref&gt;McCune, Bruce &amp; Grace, James (2002) Analysis of Ecological Communities. Mjm Software Design; ISBN 0-9721290-0-6.&lt;/ref&gt; Recently the Dice score (and its variations, e.g. logDice taking a logarithm of it) has become popular in computer [[lexicography]] for measuring the lexical association score of two given words.&lt;ref&gt;[http://nlp.fi.muni.cz/raslan/2008/raslan08.pdf#page=14 Rychlý, P. (2008) A lexicographer-friendly association score. Proceedings of the Second Workshop on Recent Advances in Slavonic Natural Language Processing RASLAN 2008: 6–9]&lt;/ref&gt; It is also commonly used in [[Image segmentation]], in particular for comparing algorithm output against reference masks in medical applications{{Citation needed|reason=Some seminal works need to be cited to show how Dice coefficient is used|date=December 2016}}.

==Abundance version==
The expression is easily extended to [[Abundance (ecology)|abundance]] instead of presence/absence of species. This quantitative version is known by several names:
* Quantitative Sørensen–Dice index&lt;ref name ="gallagher"/&gt;
* Quantitative Sørensen index&lt;ref name ="gallagher"/&gt;
* Quantitative Dice index&lt;ref name ="gallagher"/&gt;
* [[Bray–Curtis dissimilarity|Bray–Curtis similarity]] (1 minus the ''Bray-Curtis dissimilarity'')&lt;ref name ="gallagher"/&gt;
* [[Jan Czekanowski|Czekanowski]]'s quantitative index&lt;ref name ="gallagher"/&gt;
* Steinhaus index&lt;ref name ="gallagher"/&gt;
* [[E. C. Pielou|Pielou]]'s percentage similarity&lt;ref name ="gallagher"/&gt;
* 1 minus the [[Hellinger distance]]&lt;ref&gt;{{cite journal |first=J. Roger |last=Bray |first2=J. T. |last2=Curtis |year=1957 |title=An Ordination of the Upland Forest Communities of Southern Wisconsin |journal=Ecological Monographs |volume=27 |issue=4 |pages=326–349 |doi=10.2307/1942268 }}&lt;/ref&gt;

==See also==
* [[Correlation]]
* [[Jaccard index]]
* [[Hamming distance]]
* [[Mantel test]]
* [[Morisita's overlap index]]
* [[Most frequent k characters]]
* [[Overlap coefficient]]
* [[Renkonen similarity index]] (due to [[Olavi Renkonen]])
* [[Tversky index]]
* [[Universal adaptive strategy theory (UAST)]]

==References==
{{reflist}}

==External links==
{{Wikibooks|Algorithm implementation|Strings/Dice's coefficient|Dice's coefficient}}

{{DEFAULTSORT:Sorensen-Dice coefficient}}
[[Category:Information retrieval evaluation]]
[[Category:String similarity measures]]
[[Category:Measure theory]]</text>
      <sha1>3pwfykke2qfsyvf38zrfx4m8vbiqu5q</sha1>
    </revision>
  </page>
  <page>
    <title>Cranfield experiments</title>
    <ns>0</ns>
    <id>20289869</id>
    <revision>
      <id>667052035</id>
      <parentid>667051957</parentid>
      <timestamp>2015-06-15T14:30:26Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <comment>removed [[Category:Information retrieval]]; added [[Category:Information retrieval evaluation]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1161" xml:space="preserve">The '''Cranfield experiments''' were computer information retrieval experiments conducted by [[Cyril W. Cleverdon]] at [[Cranfield University]] in the 1960s, to evaluate the efficiency of indexing systems.&lt;ref&gt;Cleverdon, C. W. (1960). ASLIB Cranfield research project on the comparative efficiency of indexing systems. ASLIB Proceedings, XII, 421-431.&lt;/ref&gt;&lt;ref&gt;Cleverdon, C. W. (1967). The Cranfield tests on index language devices. Aslib Proceedings, 19(6), 173-194.&lt;/ref&gt;&lt;ref&gt;Cleverdon, C. W., &amp; Keen, E. M. (1966). Factors determining the performance of indexing systems. Vol. 1: Design, Vol. 2: Results. Cranfield, UK: Aslib Cranfield Research Project. 
&lt;/ref&gt;

They represent the prototypical evaluation model of [[information retrieval]] systems, and this model has been used in large-scale information retrieval evaluation efforts such as the [[Text Retrieval Conference]] (TREC).

==See also==
*[[ASLIB]]
*[[Information history]]

==References==
{{Reflist}}

==External links==
* [http://ir.dcs.gla.ac.uk/resources/test_collections/cran/ Cranfield 1400 corpus]

[[Category:Experiments]]
[[Category:Information retrieval evaluation]]


{{database-stub}}</text>
      <sha1>ipyf44584c6axuscve827yocbtdwsxf</sha1>
    </revision>
  </page>
  <page>
    <title>Controlled vocabulary</title>
    <ns>0</ns>
    <id>1850719</id>
    <revision>
      <id>753986026</id>
      <parentid>752248354</parentid>
      <timestamp>2016-12-10T05:37:44Z</timestamp>
      <contributor>
        <username>Rjlabs</username>
        <id>325437</id>
      </contributor>
      <comment>/* See also */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="16719" xml:space="preserve">{{refimprove|date=June 2012}}

'''Controlled vocabularies''' provide a way to organize knowledge for subsequent retrieval.  They are used in [[subject indexing]] schemes, [[subject heading]]s, [[thesauri]],&lt;ref&gt;[https://web.archive.org/web/20101204132228/http://www.imresources.fit.qut.edu.au:80/vocab/ Controlled Vocabularies]  Links to examples of thesauri and classification schemes.&lt;/ref&gt;&lt;ref&gt;[https://web.archive.org/web/20090314094707/http://www.fao.org/aims/kos_list_type.htm Controlled Vocabularies]  Links to examples of thesauri and classification schemes used in the domain of Agriculture, Fisheries, Forestry etc.&lt;/ref&gt; [[Taxonomy (general)|taxonomies]] and other forms of [[knowledge organization system]]s. Controlled vocabulary schemes mandate the use of predefined, authorised terms that have been preselected by the designers of the schemes, in contrast to natural language vocabularies, which have no such restriction.

== In library and information science ==

In [[library and information science]] controlled vocabulary is a carefully selected list of [[word (linguistics)|word]]s and [[phrase]]s, which are used to [[Tag (metadata)|tag]] units of information (document or work) so that they may be more easily retrieved by a search.&lt;ref&gt;Amy Warner, [http://www.ischool.utexas.edu/~i385e/readings/Warner-aTaxonomyPrimer.html A taxonomy primer].&lt;/ref&gt;&lt;ref&gt;Karl Fast, Fred Leise and Mike Steckel, [http://boxesandarrows.com/what-is-a-controlled-vocabulary/]&lt;/ref&gt; Controlled vocabularies solve the problems of [[homographs]], [[synonyms]] and [[polyseme]]s by a [[bijection]] between concepts and authorized terms. In short, controlled vocabularies reduce ambiguity inherent in normal human languages where the same concept can be given different names and ensure consistency.

For example, in the [[Library of Congress Subject Headings]] (a subject heading system that uses a controlled vocabulary), authorized terms—subject headings in this case—have to be chosen to handle choices between variant spellings of the same word (American versus British), choice among scientific and popular terms (''cockroach'' versus ''Periplaneta americana''), and choices between synonyms (''automobile'' versus ''car''), among other difficult issues.

Choices of authorized terms are based on the principles of ''user warrant'' (what terms users are likely to use), ''literary warrant'' (what terms are generally used in the literature and documents), and ''structural warrant'' (terms chosen by considering the structure, scope of the controlled vocabulary).

Controlled vocabularies also typically handle the problem of [[homographs]], with qualifiers. For example, the term ''pool'' has to be qualified to refer to either ''swimming pool'' or the game ''pool'' to ensure that each authorized term or heading refers to only one concept.

There are two main kinds of controlled vocabulary tools used in libraries: subject headings and thesauri. While the differences between the two are diminishing, there are still some minor differences.

Historically subject headings were designed to describe books in library catalogs by catalogers while thesauri were used by indexers to apply index terms to documents and articles. Subject headings tend to be broader in scope describing whole books, while thesauri tend to be more specialized covering very specific disciplines. Also because of the card catalog system, subject headings tend to have terms that are in indirect order (though with the rise of automated systems this is being removed), while thesaurus terms are always in direct order. Subject headings also tend to use more pre-coordination of terms such that the designer of the controlled vocabulary will combine various concepts together to form one authorized subject heading. (e.g., children and terrorism) while thesauri tend to use singular direct terms. Lastly thesauri list not only equivalent terms but also narrower, broader terms and related terms among various authorized and non-authorized terms, while historically most subject headings did not.

For example, the [[Library of Congress Subject Heading]] itself did not have much syndetic structure until 1943, and it was not until 1985 when it began to adopt the thesauri type term "[[Hypernym|Broader term]]" and "[[Hyponym|Narrow term]]".

The [[terminology|terms]] are chosen and organized by trained professionals (including librarians and information scientists) who possess expertise in the subject area. Controlled vocabulary terms can accurately describe what a given document is actually about, even if the terms themselves do not occur within the document's text. Well known subject heading systems include the [[Library of Congress Subject Headings|Library of Congress system]], [[Medical Subject Headings|MeSH]], and [[Sears Subject Headings|Sears]]. Well known thesauri include the [[Art and Architecture Thesaurus]] and the [[Education Resources Information Center|ERIC]] Thesaurus.

Choosing authorized terms to be used is a tricky business, besides the areas already considered above, the designer has to consider the specificity of the term chosen, whether to use direct entry, inter consistency and stability of the language. Lastly the amount of pre-co-ordinate (in which case the degree of enumeration versus synthesis becomes an issue) and post co-ordinate in the system is another important issue.

Controlled vocabulary elements (terms/phrases) employed as [[Tag (metadata)|tags]], to aid in the content identification process of documents, or other information system entities (e.g. DBMS, Web Services) qualifies as [[metadata]].

== Indexing languages ==

There are three main types of indexing languages.

* Controlled indexing language – only approved terms can be used by the indexer to describe the document
* [[Natural language]] indexing language – any term from the document in question can be used to describe the document
* Free indexing language – any term (not only from the document) can be used to describe the document

When indexing a document, the indexer also has to choose the level of indexing exhaustivity, the level of detail in which the document is described. For example, using low indexing exhaustivity, minor aspects of the work will not be described with index terms. In general the higher the indexing exhaustivity, the more terms indexed for each document.

In recent years [[free text search]] as a means of access to documents has become popular. This involves using natural language indexing with an indexing exhaustively set to maximum (every word in the text is ''indexed''). Many studies have been done to compare the efficiency and effectiveness of free text searches against documents that have been indexed by experts using a few well chosen controlled vocabulary descriptors.

Controlled vocabularies are often claimed to improve the accuracy of free text searching, such as to reduce [[Relevance (Information Retrieval)|irrelevant]] items in the retrieval list. These irrelevant items ([[false positives]]) are often caused by the inherent ambiguity of [[natural language]]. Take the English word [[football (word)|''football'']] for example. ''Football'' is the name given to a number of different [[team sport]]s. Worldwide the most popular of these team sports is [[Football (soccer)|association football]], which also happens to be called ''[[soccer]]'' in several countries. The word ''football'' is also applied to [[rugby football]] ([[rugby union]] and [[rugby league]]), [[American football]], [[Australian rules football]], [[Gaelic football]], and [[Canadian football]]. A search for ''football'' therefore will retrieve documents that are about several completely different sports. Controlled vocabulary solves this problem by [[Tag (metadata)|tagging]] the documents in such a way that the ambiguities are eliminated.

Compared to free text searching, the use of a controlled vocabulary can dramatically increase the performance of an information retrieval system, if performance is measured by precision (the percentage of documents in the retrieval list that are actually [[relevance|relevant]] to the search topic).

In some cases controlled vocabulary can enhance recall as well, because unlike natural language schemes, once the correct authorized term is searched, you don't need to worry about searching for other terms that might be synonyms of that term.

However, a controlled vocabulary search may also lead to unsatisfactory [[Recall (information retrieval)|recall]], in that it will fail to retrieve some documents that are actually relevant to the search question.

This is particularly problematic when the search question involves terms that are sufficiently tangential to the subject area such that the indexer might have decided to tag it using a different term (but the searcher might consider the same). Essentially, this can be avoided only by an experienced user of controlled vocabulary whose understanding of the vocabulary coincides with the way it is used by the indexer.

Another possibility is that the article is just not tagged by the indexer because indexing exhaustivity is low. For example, an article might mention football as a secondary focus, and the indexer might decide not to tag it with "football" because it is not important enough compared to the main focus. But it turns out that for the searcher that article is relevant and hence recall fails. A free text search would automatically pick up that article regardless.

On the other hand, free text searches have high exhaustivity (you search on every word) so it has potential for high recall (assuming you solve the problems of synonyms by entering every combination) but will have much lower precision.

Controlled vocabularies are also quickly out-dated and in fast developing fields of knowledge, the authorized terms available might not be available if they are not updated regularly. Even in the best case scenario, controlled language is often not as specific as using the words of the text itself. Indexers trying to choose the appropriate index terms might misinterpret the author, while a free text search is in no danger of doing so, because it uses the author's own words.

The use of controlled vocabularies can be costly compared to free text searches because human experts  or expensive automated systems are necessary to index each entry.  Furthermore, the user has to be familiar with the controlled vocabulary scheme to make best use of the system. But as already mentioned, the control of synonyms, homographs can help increase precision.

Numerous methodologies have been developed to assist in the creation of controlled vocabularies, including [[faceted classification]], which enables a given data record or document to be described in multiple ways.

==Applications==
Controlled vocabularies, such as the [[Library of Congress Subject Headings]],  are an essential component of [[bibliography]], the study and classification of books. They were initially developed in [[library and information science]]. In the 1950s, government agencies  began to develop controlled vocabularies for the burgeoning journal literature in specialized fields; an example is the [[Medical Subject Headings]] (MeSH) developed by the [[United States National Library of Medicine|U.S. National Library of Medicine]]. Subsequently, for-profit firms (called Abstracting and indexing services) emerged to index the fast-growing literature in every field of knowledge. In the 1960s, an online bibliographic database industry developed based on dialup [[X.25]] networking. These services were seldom made available to the public because they were difficult to use; specialist librarians called search intermediaries handled the searching job. In the 1980s, the first [[full text]] databases appeared; these databases contain the full text of the index articles as well as the bibliographic information. Online bibliographic databases have migrated to the Internet and are now publicly available; however, most are proprietary and can be expensive to use. Students enrolled in colleges and universities may be able to access some of these services without charge; some of these services may be accessible without charge at a public library.

In large organizations, controlled vocabularies may be introduced to improve [[technical communication]]. The use of controlled vocabulary ensures that everyone is using the same word to mean the same thing.  This consistency of terms is one of the most important concepts in [[technical writing]] and [[knowledge management]], where effort is expended to use the same word throughout a [[document]] or [[organization]] instead of slightly different ones to refer to the same thing.

Web searching could be dramatically improved by the development of a controlled vocabulary for describing Web pages; the use of such a vocabulary could culminate in a [[Semantic Web]], in which the content of Web pages is described using a machine-readable [[metadata]] scheme. One of the first proposals for such a scheme is the [[Dublin Core]] Initiative. An example of a controlled vocabulary which is usable for [[Web indexing|indexing web pages]] is [[Polythematic Structured Subject Heading System|PSH]].

It is unlikely that a single metadata scheme will ever succeed in describing the content of the entire Web.&lt;ref&gt;Cory Doctorow, [http://www.well.com/~doctorow/metacrap.htm Metacrap].&lt;/ref&gt; To create a Semantic Web, it may be necessary to draw from two or more metadata systems to describe a Web page's contents. The [[eXchangeable Faceted Metadata Language]] (XFML) is designed to enable controlled vocabulary creators to publish and share metadata systems. XFML is designed on [[faceted classification]] principles.&lt;ref&gt;Mark Pilgrim, [http://petervandijck.com/xfml/ eXchangeable Faceted Metadata Language].&lt;/ref&gt;

Controlled vocabularies of the [[Semantic Web]] define the concepts and relationships (terms) used to describe a field of interest or area of concern. For instance, to declare a person in a machine-readable format, a vocabulary is needed that has the formal definition of “Person”, such as the Friend of a Friend ([[FOAF]]) vocabulary, which has a Person class that defines typical properties of a person including, but not limited to, name, honorific prefix, affiliation, email address, and homepage, or the Person vocabulary of [[Schema.org]].&lt;ref&gt;{{cite web |url=http://schema.org/Person |title=The Person vocabulary of Schema.org |accessdate=13 March 2015}}&lt;/ref&gt; Similarly, a book can be described using the Book vocabulary of [[Schema.org]]&lt;ref&gt;{{cite web |url=http://schema.org/Book |title=The Book vocabulary of Schema.org |accessdate=13 March 2015}}&lt;/ref&gt; and general publication terms from the [[Dublin Core]] vocabulary,&lt;ref&gt;{{cite web |url=http://dublincore.org/documents/dces/ |title=Dublin Core Metadata Element Set, Version 1.1 |accessdate=13 March 2015}}&lt;/ref&gt; an event with the Event vocabulary of [[Schema.org]],&lt;ref&gt;{{cite web |url=http://schema.org/Event |title=The Event vocabulary of Schema.org |accessdate=13 March 2015}}&lt;/ref&gt; and so on.

To use machine-readable terms from any controlled vocabulary, web designers can choose from a variety of annotation formats, including RDFa, [[Microdata (HTML)|HTML5 Microdata]], or [[JSON-LD]] in the markup, or [[Resource Description Framework|RDF]] serializations (RDF/XML, Turtle, N3, TriG, TriX) in external files.

==See also==
*[[Authority control]]
*[[Controlled natural language]]
*[[IMS VDEX|IMS Vocabulary Definition Exchange]]
*[[Named-entity recognition]]
*[[Nomenclature]]
*[[Ontology (computer science)]]
*[[Terminology]]
*[[Thesaurus]]
*[[Universal Data Element Framework]]
*[[Vocabulary-based transformation]]

==References==
{{Reflist|2}}

==External links==
* [http://www.controlledvocabulary.com/ controlledvocabulary.com] — explains how controlled vocabularies are useful in describing images and information for classifying content in electronic databases.
* [http://www.photo-keywords.com/ photo-keywords.com/] — useful guides to creating and editing your own controlled vocabulary suitable for image cataloging.
* [http://www.niso.org/standards/resources/Z39-19.html ANSI/NISO Z39.19 - 2005 Guidelines for the Construction, Format, and Management of Monolingual Controlled Vocabularies]

{{Lexicography}}

[[Category:Information retrieval techniques]]
[[Category:Library cataloging and classification]]
[[Category:Knowledge representation]]
[[Category:Technical communication]]
[[Category:Semantic Web]]
[[Category:Ontology (information science)]]
[[Category:Controlled vocabularies]]
[[Category:Information science]]</text>
      <sha1>peb0omh8lyh05h6afppxoruwpxfgqa7</sha1>
    </revision>
  </page>
  <page>
    <title>Stemming</title>
    <ns>0</ns>
    <id>30874683</id>
    <revision>
      <id>741820710</id>
      <parentid>741819972</parentid>
      <timestamp>2016-09-29T21:33:52Z</timestamp>
      <contributor>
        <username>Jim Carnicelli</username>
        <id>2186481</id>
      </contributor>
      <minor />
      <comment>/* Suffix-stripping algorithms */ Added link to "Lemmatisation"</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="27719" xml:space="preserve">{{about||the skiing technique|Stem (skiing)|the climbing technique|Glossary of climbing terms#stem}}
{{Expert needed|date=October 2010}}
In [[linguistic morphology]] and [[information retrieval]], '''stemming''' is the process of reducing inflected (or sometimes derived) words to their [[word stem]], base or [[root (linguistics)|root]] form—generally a written word form. The stem need not be identical to the [[morphological root]] of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root. [[Algorithm]]s for stemming have been studied in [[computer science]] since the 1960s. Many [[search engine]]s treat words with the same stem as [[synonym]]s as a kind of [[query expansion]], a process called conflation.

Stemming programs are commonly referred to as stemming algorithms or stemmers.

==Examples==
A stemmer for English, for example, should identify the [[string literal|string]] "cats" (and possibly "catlike", "catty" etc.) as based on the root "cat", and "stems", "stemmer", "stemming", "stemmed" as based on "stem". A stemming algorithm reduces the words "fishing", "fished", and "fisher" to the root word, "fish". On the other hand, "argue", "argued", "argues", "arguing", and "argus" reduce to the stem "argu" (illustrating the case where the stem is not itself a word or root) but "argument" and "arguments" reduce to the stem "argument".&lt;!-- using the Porter algorithm --&gt;

==History==
The first published stemmer was written by [[Julie Beth Lovins]] in 1968.&lt;ref&gt;{{cite journal |first=Julie Beth |last=Lovins |year=1968 |title=Development of a Stemming Algorithm |journal=Mechanical Translation and Computational Linguistics |volume=11 |pages=22–31 }}&lt;/ref&gt; This paper was remarkable for its early date and had great influence on later work in this area.

A later stemmer was written by [[Martin Porter]] and was published in the July 1980 issue of the journal ''Program''. This stemmer was very widely used and became the de facto standard algorithm used for English stemming. Dr. Porter received the [[Tony Kent Strix award]] in 2000 for his work on stemming and information retrieval.

Many implementations of the Porter stemming algorithm were written and freely distributed; however, many of these implementations contained subtle flaws. As a result, these stemmers did not match their potential. To eliminate this source of error, Martin Porter released an official [[free software]] (mostly [[BSD licenses|BSD]]-licensed) implementation&lt;ref&gt;http://tartarus.org/~martin/PorterStemmer/&lt;/ref&gt; of the algorithm around the year 2000. He extended this work over the next few years by building [[Snowball programming language|Snowball]], a framework for writing stemming algorithms, and implemented an improved English stemmer together with stemmers for several other languages.

==Algorithms==
There are several types of stemming algorithms which differ in respect to performance and accuracy and how certain stemming obstacles are overcome.

A simple stemmer looks up the inflected form in a [[lookup table]]. The advantages of this approach are that it is simple, fast, and easily handles exceptions. The disadvantages are that all inflected forms must be explicitly listed in the table: new or unfamiliar words are not handled, even if they are perfectly regular (e.g. iPads ~ iPad), and the table may be large. For languages with simple morphology, like English, table sizes are modest, but highly inflected languages like Turkish may have hundreds of potential inflected forms for each root.

A lookup approach may use preliminary part-of-speech tagging to avoid overstemming.&lt;ref&gt;Yatsko, V. A.; [http://yatsko.zohosites.com/y-stemmer.html ''Y-stemmer'']&lt;/ref&gt;

===The production technique===

The lookup table used by a stemmer is generally produced semi-automatically. For example, if the word is "run", then the inverted algorithm might automatically generate the forms "running", "runs", "runned", and "runly". The last two forms are valid constructions, but they are unlikely.

===Suffix-stripping algorithms===
Suffix stripping algorithms do not rely on a lookup table that consists of inflected forms and root form relations. Instead, a typically smaller list of "rules" is stored which provides a path for the algorithm, given an input word form, to find its root form. Some examples of the rules include:
* if the word ends in 'ed', remove the 'ed'
* if the word ends in 'ing', remove the 'ing'
* if the word ends in 'ly', remove the 'ly'

Suffix stripping approaches enjoy the benefit of being much simpler to maintain than brute force algorithms, assuming the maintainer is sufficiently knowledgeable in the challenges of linguistics and morphology and encoding suffix stripping rules. Suffix stripping algorithms are sometimes regarded as crude given the poor performance when dealing with exceptional relations (like 'ran' and 'run'). The solutions produced by suffix stripping algorithms are limited to those [[lexical category|lexical categories]] which have well known suffixes with few exceptions. This, however, is a problem, as not all parts of speech have such a well formulated set of rules. [[Lemmatisation]] attempts to improve upon this challenge.

Prefix stripping may also be implemented. Of course, not all languages use prefixing or suffixing.

====Additional algorithm criteria====
Suffix stripping algorithms may differ in results for a variety of reasons. One such reason is whether the algorithm constrains whether the output word must be a real word in the given language. Some approaches do not require the word to actually exist in the language lexicon (the set of all words in the language). Alternatively, some suffix stripping approaches maintain a database (a large list) of all known morphological word roots that exist as real words. These approaches check the list for the existence of the term prior to making a decision. Typically, if the term does not exist, alternate action is taken. This alternate action may involve several other criteria. The non-existence of an output term may serve to cause the algorithm to try alternate suffix stripping rules.

It can be the case that two or more suffix stripping rules apply to the same input term, which creates an ambiguity as to which rule to apply. The algorithm may assign (by human hand or stochastically) a priority to one rule or another. Or the algorithm may reject one rule application because it results in a non-existent term whereas the other overlapping rule does not. For example, given the English term ''friendlies'', the algorithm may identify the ''ies'' suffix and apply the appropriate rule and achieve the result of ''friendl''. ''friendl'' is likely not found in the lexicon, and therefore the rule is rejected.

One improvement upon basic suffix stripping is the use of suffix substitution. Similar to a stripping rule, a substitution rule replaces a suffix with an alternate suffix. For example, there could exist a rule that replaces ''ies'' with ''y''. How this affects the algorithm varies on the algorithm's design. To illustrate, the algorithm may identify that both the ''ies'' suffix stripping rule as well as the suffix substitution rule apply. Since the stripping rule results in a non-existent term in the lexicon, but the substitution rule does not, the substitution rule is applied instead. In this example, ''friendlies'' becomes ''friendly'' instead of ''friendl''.

Diving further into the details, a common technique is to apply rules in a cyclical fashion (recursively, as computer scientists would say). After applying the suffix substitution rule in this example scenario, a second pass is made to identify matching rules on the term ''friendly'', where the ''ly'' stripping rule is likely identified and accepted. In summary, ''friendlies'' becomes (via substitution) ''friendly'' which becomes (via stripping) ''friend''.

This example also helps illustrate the difference between a rule-based approach and a brute force approach. In a brute force approach, the algorithm would search for ''friendlies'' in the set of hundreds of thousands of inflected word forms and ideally find the corresponding root form ''friend''. In the rule-based approach, the three rules mentioned above would be applied in succession to converge on the same solution. Chances are that the rule-based approach would be slower, as lookup algorithms have a direct access to the solution, while rule-based should try several options, and combinations of them, and then choose which result seems to be the best.

===Lemmatisation algorithms===
A more complex approach to the problem of determining a stem of a word is [[lemmatisation]]. This process involves first determining the [[part of speech]] of a word, and applying different normalization rules for each part of speech. The part of speech is first detected prior to attempting to find the root since for some languages, the stemming rules change depending on a word's part of speech.

This approach is highly conditional upon obtaining the correct lexical category (part of speech). While there is overlap between the normalization rules for certain categories, identifying the wrong category or being unable to produce the right category limits the added benefit of this approach over suffix stripping algorithms. The basic idea is that, if the stemmer is able to grasp more information about the word being stemmed, then it can apply more accurate normalization rules (which unlike suffix stripping rules can also modify the stem).

===Stochastic algorithms===
[[Stochastic]] algorithms involve using probability to identify the root form of a word. Stochastic algorithms are trained (they "learn") on a table of root form to inflected form relations to develop a probabilistic model. This model is typically expressed in the form of complex linguistic rules, similar in nature to those in suffix stripping or lemmatisation. Stemming is performed by inputting an inflected form to the trained model and having the model produce the root form according to its internal ruleset, which again is similar to suffix stripping and lemmatisation, except that the decisions involved in applying the most appropriate rule, or whether or not to stem the word and just return the same word, or whether to apply two different rules sequentially, are applied on the grounds that the output word will have the highest probability of being correct (which is to say, the smallest probability of being incorrect, which is how it is typically measured).

Some lemmatisation algorithms are stochastic in that, given a word which may belong to multiple parts of speech, a probability is assigned to each possible part. This may take into account the surrounding words, called the context, or not. Context-free grammars do not take into account any additional information. In either case, after assigning the probabilities to each possible part of speech, the most likely part of speech is chosen, and from there the appropriate normalization rules are applied to the input word to produce the normalized (root) form.

===''n''-gram analysis===
Some stemming techniques use the [[n-gram]] context of a word to choose the correct stem for a word.&lt;ref name="CEUR Proceedings"&gt;{{cite journal|last1=McNamee|first1=Paul|title=Exploring New Languages with HAIRCUT at CLEF 2005|journal=CEUR Workshop Proceedings|date=September 21–22, 2005|volume=1171|url=http://ceur-ws.org/Vol-1171/CLEF2005wn-adhoc-McNamee2005.pdf|accessdate=3/6/15}}&lt;/ref&gt;

===Hybrid approaches===
Hybrid approaches use two or more of the approaches described above in unison. A simple example is a suffix tree algorithm which first consults a lookup table using brute force. However, instead of trying to store the entire set of relations between words in a given language, the lookup table is kept small and is only used to store a minute amount of "frequent exceptions" like "ran =&gt; run". If the word is not in the exception list, apply suffix stripping or lemmatisation and output the result.

===Affix stemmers===
In [[linguistics]], the term [[affix]] refers to either a [[prefix]] or a [[suffix]]. In addition to dealing with suffixes, several approaches also attempt to remove common prefixes. For example, given the word ''indefinitely'', identify that the leading "in" is a prefix that can be removed. Many of the same approaches mentioned earlier apply, but go by the name '''affix stripping'''. A study of affix stemming for several European languages can be found here.&lt;ref&gt;Jongejan, B.; and Dalianis, H.; ''Automatic Training of Lemmatization Rules that Handle Morphological Changes in pre-, in- and Suffixes Alike'', in the ''Proceeding of the ACL-2009, Joint conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Singapore, August 2–7, 2009'', pp. 145-153
[http://www.aclweb.org/anthology/P/P09/P09-1017.pdf]&lt;/ref&gt;

===Matching algorithms===
Such algorithms use a stem database (for example a set of documents that contain stem words). These stems, as mentioned above, are not necessarily valid words themselves (but rather common sub-strings, as the "brows" in "browse" and in "browsing"). In order to stem a word the algorithm tries to match it with stems from the database, applying various constraints, such as on the relative length of the candidate stem within the word (so that, for example, the short prefix "be", which is the stem of such words as "be", "been" and "being", would not be considered as the stem of the word "beside").

==Language challenges==
While much of the early academic work in this area was focused on the English language (with significant use of the Porter Stemmer algorithm), many other languages have been investigated.&lt;ref&gt;Dolamic, Ljiljana; and Savoy, Jacques; [http://clef.isti.cnr.it/2007/working_notes/DolamicCLEF2007.pdf ''Stemming Approaches for East European Languages (CLEF 2007)'']&lt;/ref&gt;&lt;ref&gt;Savoy, Jacques; [http://portal.acm.org/citation.cfm?doid=1141277.1141523 ''Light Stemming Approaches for the French, Portuguese, German and Hungarian Languages''], ACM Symposium on Applied Computing, SAC 2006, ISBN 1-59593-108-2&lt;/ref&gt;&lt;ref&gt;Popovič, Mirko; and Willett, Peter (1992); [http://onlinelibrary.wiley.com/doi/10.1002/%28SICI%291097-4571%28199206%2943:5%3C384::AID-ASI6%3E3.0.CO;2-L/abstract ''The Effectiveness of Stemming for Natural-Language Access to Slovene Textual Data''], Journal of the [[American Society for Information Science]], Volume 43, Issue 5 (June), pp. 384–390&lt;/ref&gt;&lt;ref&gt;[http://staff.science.uva.nl/~mdr/Publications/Files/clef2005-proc-adhoc.pdf ''Stemming in Hungarian at CLEF 2005'']&lt;/ref&gt;&lt;ref&gt;Viera, A. F. G. &amp; Virgil, J. (2007); [http://InformationR.net/ir/12-3/paper315.html ''Uma revisão dos algoritmos de radicalização em língua portuguesa''], Information Research, 12(3), paper 315&lt;/ref&gt;

Hebrew and Arabic are still considered difficult research languages for stemming. English stemmers are fairly trivial (with only occasional problems, such as "dries" being the third-person singular present form of the verb "dry", "axes" being the plural of "axe" as well as "axis"); but stemmers become harder to design as the morphology, orthography, and character encoding of the target language becomes more complex. For example, an Italian stemmer is more complex than an English one (because of a greater number of verb inflections), a Russian one is more complex (more noun [[declension]]s), a Hebrew one is even more complex (due to [[nonconcatenative morphology]], a writing system without vowels, and the requirement of prefix stripping: Hebrew stems can be two, three or four characters, but not more), and so on.

===Multilingual stemming===
Multilingual stemming applies morphological rules of two or more languages simultaneously instead of rules for only a single language when interpreting a search query. Commercial systems using multilingual stemming exist{{Citation needed|date=October 2013}}.

==Error metrics==
There are two error measurements in stemming algorithms, overstemming and understemming. Overstemming is an error where two separate inflected words are stemmed to the same root, but should not have been—a [[false positive]]. Understemming is an error where two separate inflected words should be stemmed to the same root, but are not—a [[false negative]]. Stemming algorithms attempt to minimize each type of error, although reducing one type can lead to increasing the other.

For example, the widely used Porter stemmer stems "universal", "university", and "universe" to "univers". This is a case of overstemming: though these three words are etymologically related, their modern meanings are in widely different domains, so treating them as synonyms in a search engine will likely reduce the relevance of the search results.

An example of understemming in the Porter stemmer is "alumnus" → "alumnu", "alumni" → "alumni", "alumna"/"alumnae" → "alumna".  This English word keeps Latin morphology, and so these near-synonyms are not conflated.

==Applications==
Stemming is used as an approximate method for grouping words with a similar basic meaning together. For example, a text mentioning "daffodils" is probably closely related to a text mentioning "daffodil" (without the s). But in some cases, words with the same morphological stem have [[idiom]]atic meanings which are not closely related: a user searching for "marketing" will not be satisfied by most documents mentioning "markets" but not "marketing".

===Information retrieval===
Stemmers are common elements in [[Information Retrieval|query systems]] such as [[World Wide Web|Web]] [[search engine]]s. The effectiveness of stemming for English query systems were soon found to be rather limited, however, and this has led early [[information retrieval]] researchers to deem stemming irrelevant in general.&lt;ref&gt;Baeza-Yates, Ricardo; and Ribeiro-Neto, Berthier (1999); ''Modern Information Retrieval'', ACM Press/Addison Wesley&lt;/ref&gt; An alternative approach, based on searching for [[n-gram]]s rather than stems, may be used instead. Also, stemmers may provide greater benefits in other languages than English.&lt;ref&gt;Kamps, Jaap; Monz, Christof; de Rijke, Maarten; and Sigurbjörnsson, Börkur (2004); ''Language-Dependent and Language-Independent Approaches to Cross-Lingual Text Retrieval'', in Peters, C.; Gonzalo, J.; Braschler, M.; and Kluck, M. (eds.); ''Comparative Evaluation of Multilingual Information Access Systems'', Springer Verlag, pp. 152–165&lt;/ref&gt;&lt;ref&gt;Airio, Eija (2006); ''Word Normalization and Decompounding in Mono- and Bilingual IR'', Information Retrieval '''9''':249–271&lt;/ref&gt;

===Domain Analysis===
Stemming is used to determine domain vocabularies in [[domain analysis]].
&lt;ref&gt;Frakes, W.; Prieto-Diaz, R.; &amp; Fox, C. (1998); ''DARE: Domain Analysis and Reuse Environment'', Annals of Software Engineering (5), pp. 125-141&lt;/ref&gt;

===Use in commercial products===
Many commercial companies have been using stemming since at least the 1980s and have produced algorithmic and lexical stemmers in many languages.&lt;ref&gt;[http://www.dtsearch.co.uk/language.htm ''Language Extension Packs''], dtSearch&lt;/ref&gt;&lt;ref&gt;[http://technet2.microsoft.com/Office/en-us/library/87065c9d-d39d-479d-909b-02160ec6d7791033.mspx?mfr=true ''Building Multilingual Solutions by using Sharepoint Products and Technologies''], Microsoft Technet&lt;/ref&gt;

The [[Snowball (programming language)|Snowball]] stemmers have been compared with commercial lexical stemmers with varying results.&lt;ref&gt;[http://clef.isti.cnr.it/2003/WN_web/19.pdf CLEF 2003: Stephen Tomlinson compared the Snowball stemmers with the Hummingbird lexical stemming (lemmatization) system]&lt;/ref&gt;&lt;ref&gt;[http://clef.isti.cnr.it/2004/working_notes/WorkingNotes2004/21.pdf CLEF 2004: Stephen Tomlinson "Finnish, Portuguese and Russian Retrieval with Hummingbird SearchServer"]&lt;/ref&gt;

[[Google search]] adopted word stemming in 2003.&lt;ref&gt;[http://www.google.com/support/bin/static.py?page=searchguides.html&amp;ctx=basics#stemming ''The Essentials of Google Search''], Web Search Help Center, [[Google|Google Inc.]]&lt;/ref&gt; Previously a search for "fish" would not have returned "fishing". Other software search algorithms vary in their use of word stemming. Programs that simply search for substrings obviously will find "fish" in "fishing" but when searching for "fishes" will not find occurrences of the word "fish".

==See also==
* [[Root (linguistics)]] - linguistic definition of the term "root"
* [[Stem (linguistics)]] - linguistic definition of the term "stem"
* [[Morphology (linguistics)]]
* [[Lemma (morphology)]] - linguistic definition
* [[Lemmatization]]
* [[Lexeme]]
* [[Inflection]]
* [[Derivation (linguistics)|Derivation]] - stemming is a form of reverse derivation
* [[Natural language processing]] - stemming is generally regarded as a form of NLP
* [[Text mining]] - stemming algorithms play a major role in commercial NLP software
* [[Computational linguistics]]
* [[Snowball (programming language)]] - designed for creating stemming algorithms

{{Natural Language Processing}}

==References==
{{reflist|2}}

==Further reading==
{{refbegin|2}}
* Dawson, J. L. (1974); ''Suffix Removal for Word Conflation'', Bulletin of the Association for Literary and Linguistic Computing, 2(3): 33–46
* Frakes, W. B. (1984); ''Term Conflation for Information Retrieval'', Cambridge University Press
* Frakes, W. B. &amp; Fox, C. J. (2003); ''Strength and Similarity of Affix Removal Stemming Algorithms'', SIGIR Forum, 37: 26–30
* Frakes, W. B. (1992); ''Stemming algorithms, Information retrieval: data structures and algorithms'', Upper Saddle River, NJ: Prentice-Hall, Inc.
* Hafer, M. A. &amp; Weiss, S. F. (1974); ''Word segmentation by letter successor varieties'', Information Processing &amp; Management 10 (11/12), 371–386
* Harman, D. (1991); ''How Effective is Suffixing?'', Journal of the American Society for Information Science 42 (1), 7–15
* Hull, D. A. (1996); ''Stemming Algorithms&amp;nbsp;– A Case Study for Detailed Evaluation'', JASIS, 47(1): 70–84
* Hull, D. A. &amp; Grefenstette, G. (1996); ''A Detailed Analysis of English Stemming Algorithms'', Xerox Technical Report
* Kraaij, W. &amp; Pohlmann, R. (1996); ''Viewing Stemming as Recall Enhancement'', in Frei, H.-P.; Harman, D.; Schauble, P.; and Wilkinson, R. (eds.); ''Proceedings of the 17th ACM SIGIR conference held at Zurich, August 18–22'', pp.&amp;nbsp;40–48
* Krovetz, R. (1993); ''Viewing Morphology as an Inference Process'', in ''Proceedings of ACM-SIGIR93'', pp.&amp;nbsp;191–203
* Lennon, M.; Pierce, D. S.; Tarry, B. D.; &amp; Willett, P. (1981); ''An Evaluation of some Conflation Algorithms for Information Retrieval'', Journal of Information Science, 3: 177–183
* Lovins, J. (1971); ''[http://www.eric.ed.gov/sitemap/html_0900000b800c571a.html Error Evaluation for Stemming Algorithms as Clustering Algorithms]'', JASIS, 22: 28–40
* Lovins, J. B. (1968); ''Development of a Stemming Algorithm'', Mechanical Translation and Computational Linguistics, 11, 22—31
* Jenkins, Marie-Claire; and Smith, Dan (2005); [http://www.uea.ac.uk/polopoly_fs/1.85493!stemmer25feb.pdf ''Conservative Stemming for Search and Indexing'']
* Paice, C. D. (1990); ''[http://www.comp.lancs.ac.uk/computing/research/stemming/paice/article.htm Another Stemmer]'', SIGIR Forum, 24: 56–61
* Paice, C. D. (1996) ''[http://www3.interscience.wiley.com/cgi-bin/abstract/57804/ABSTRACT Method for Evaluation of Stemming Algorithms based on Error Counting]'', JASIS, 47(8): 632–649
* Popovič, Mirko; and Willett, Peter (1992); [http://onlinelibrary.wiley.com/doi/10.1002/%28SICI%291097-4571%28199206%2943:5%3C384::AID-ASI6%3E3.0.CO;2-L/abstract ''The Effectiveness of Stemming for Natural-Language Access to Slovene Textual Data''], Journal of the [[American Society for Information Science]], Volume 43, Issue 5 (June), pp.&amp;nbsp;384–390
* Porter, Martin F. (1980); ''[http://telemat.det.unifi.it/book/2001/wchange/download/stem_porter.html An Algorithm for Suffix Stripping]'', Program, 14(3): 130–137
* Savoy, J. (1993); ''[http://www3.interscience.wiley.com/cgi-bin/abstract/10049824/ABSTRACT?CRETRY=1&amp;SRETRY=0 Stemming of French Words Based on Grammatical Categories]'' Journal of the American Society for Information Science, 44(1), 1–9
* Ulmschneider, John E.; &amp; Doszkocs, Tamas (1983); ''[http://www.eric.ed.gov/sitemap/html_0900000b8007ea83.html A Practical Stemming Algorithm for Online Search Assistance]'', Online Review, 7(4), 301–318
* Xu, J.; &amp; Croft, W. B. (1998); ''[http://portal.acm.org/citation.cfm?doid=267954.267957 Corpus-Based Stemming Using Coocurrence of Word Variants]'', ACM Transactions on Information Systems, 16(1), 61–81
{{refend}}

==External links==
*[http://opennlp.apache.org/index.html Apache OpenNLP] includes Porter and Snowball stemmers
* [http://smile-stemmer.appspot.com SMILE Stemmer] - free online service, includes Porter and Paice/Husk' Lancaster stemmers (Java API)
* [http://code.google.com/p/ir-themis/ Themis] - open source IR framework, includes Porter stemmer implementation (PostgreSQL, Java API)
* [http://snowballstem.org Snowball] - free stemming algorithms for many languages, includes source code, including stemmers for five romance languages
* [http://www.iveonik.com/blog/2011/08/snowball-stemmers-on-csharp-free-download/ Snowball on C#] - port of Snowball stemmers for C# (14 languages)
* [http://snowball.tartarus.org/wrappers/guide.html Python bindings to Snowball API]
* [http://locknet.ro/archive/2009-10-29-ann-ruby-stemmer.html Ruby-Stemmer] - Ruby extension to Snowball API
* [http://pecl.php.net/package/stem/ PECL] - PHP extension to the Snowball API
* [http://www.oleandersolutions.com/stemming.html Oleander Porter's algorithm] - stemming library in C++ released under BSD
* [http://www.cs.waikato.ac.nz/~eibe/stemmers/index.html Unofficial home page of the Lovins stemming algorithm] - with source code in a couple of languages
* [http://www.tartarus.org/~martin/PorterStemmer/index.html Official home page of the Porter stemming algorithm] - including source code in several languages
* [http://www.comp.lancs.ac.uk/computing/research/stemming/index.htm Official home page of the Lancaster stemming algorithm] - Lancaster University, UK
* [https://www.uea.ac.uk/computing/word-stemming/ Official home page of the UEA-Lite Stemmer ] - University of East Anglia, UK
* [http://www.comp.lancs.ac.uk/computing/research/stemming/general/index.htm Overview of stemming algorithms]
* [http://code.google.com/p/ptstemmer/ PTStemmer] - A Java/Python/.Net stemming toolkit for the Portuguese language
* [http://mazko.github.com/jssnowball/ jsSnowball] - open source JavaScript implementation of Snowball stemming algorithms for many languages
* [http://trimc-nlp.blogspot.com/2013/08/snowball-stemmer-for-java.html Snowball Stemmer] - implementation for Java
* [http://hlt.di.fct.unl.pt/luis/hindi_stemmer/ hindi_stemmer] - open source stemmer for Hindi
* [http://hlt.di.fct.unl.pt/luis/czech_stemmer/ czech_stemmer] - open source stemmer for Czech
* [http://www.comp.leeds.ac.uk/eric/sawalha08coling.pdf Comparative Evaluation of Arabic Language Morphological Analysers and Stemmers]
* [https://github.com/rdamodharan/tamil-stemmer Tamil Stemmer]

{{FOLDOC}}

[[Category:Linguistic morphology]]
[[Category:Natural language processing]]
[[Category:Tasks of natural language processing]]
[[Category:Computational linguistics]]
[[Category:Information retrieval techniques]]</text>
      <sha1>7ib34q4dkhfzsu7zebl7z5r22jxxem7</sha1>
    </revision>
  </page>
  <page>
    <title>Uncertain inference</title>
    <ns>0</ns>
    <id>25962276</id>
    <revision>
      <id>711417671</id>
      <parentid>704833825</parentid>
      <timestamp>2016-03-22T19:41:52Z</timestamp>
      <contributor>
        <username>Sietse Snel</username>
        <id>94775</id>
      </contributor>
      <minor />
      <comment>/* Further work */ fix deprecated reference syntax</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4193" xml:space="preserve">'''Uncertain inference''' was first described by [[C. J. van Rijsbergen]]&lt;ref&gt;{{cite | author=C. J. van Rijsbergen | title=A non-classical logic for information retrieval | publisher=The Computer Journal | pages=481–485 | year=1986}}&lt;/ref&gt; as a way to formally define a query and document relationship in [[Information retrieval]]. This formalization is a [[logical consequence|logical implication]] with an attached measure of uncertainty.

==Definitions==
Rijsbergen proposes that the measure of [[uncertainty]] of a document ''d'' to a query ''q'' be the probability of its logical implication, i.e.:

:&lt;math&gt;P(d \to q)&lt;/math&gt;

A user's query can be interpreted as a set of assertions about the desired document. It is the system's task to [[inference|infer]], given a particular document, if the query assertions are true. If they are, the document is retrieved.
In many cases the contents of documents are not sufficient to assert the queries. A [[knowledge base]] of facts and rules is needed, but some of them may be uncertain because there may be a probability associated to using them for inference. Therefore, we can also refer to this as ''plausible inference''. The [[plausibility]] of an inference &lt;math&gt;d \to q&lt;/math&gt; is a function of the plausibility of each query assertion. Rather than retrieving a document that exactly matches the query we should rank the documents based on their plausibility in regards to that query.
Since ''d'' and ''q'' are both generated by users, they are error prone; thus &lt;math&gt;d \to q&lt;/math&gt; is uncertain. This will affect the plausibility of a given query.

By doing this it accomplishes two things:
* Separate the processes of revising probabilities from the logic
* Separate the treatment of relevance from the treatment of requests

[[Multimedia]] documents, like images or videos, have different inference properties for each datatype. They are also different from text document properties. The framework of plausible inference allows us to measure and combine the probabilities coming from these different properties.

Uncertain inference generalizes the notions of [[autoepistemic logic]], where truth values are either known or unknown, and when known, they are true or false.

==Example==
If we have a query of the form:

:&lt;math&gt;q = A \wedge B \wedge C&lt;/math&gt;

where A, B and C are query assertions, then for a document D we want the probability:

:&lt;math&gt;P (D \to (A \wedge B \wedge C))&lt;/math&gt;

If we transform this into the [[conditional probability]] &lt;math&gt;P ((A \wedge B \wedge C) | D)&lt;/math&gt; and if the query assertions are independent we can calculate the overall probability of the implication as the product of the individual assertions probabilities.

==Further work==
Croft and Krovetz&lt;ref&gt;{{cite | title=Interactive retrieval office documents | url=http://doi.acm.org/10.1145/45410.45435 | author1=W. B. Croft | author2=R. Krovetz | year=1988 }}&lt;/ref&gt; applied uncertain inference to an information retrieval system for office documents they called ''OFFICER''. In office documents the independence assumption is valid since the query will focus on their individual attributes. Besides analysing the content of documents one can also query about the author, size, topic or collection for example. They devised methods to compare document and query attributes, infer their plausibility and combine it into an overall rating for each document. Besides that uncertainty of document and query contents also had to be addressed.

[[Probabilistic logic network]]s is a system for performing uncertain inference; crisp true/false truth values are replaced not only by a probability, but also by a confidence level, indicating the certitude of the probability.

[[Markov logic network]]s allow uncertain inference to be performed; uncertainties are computed using the [[maximum entropy principle]], in analogy to the way that [[Markov chain]]s describe the uncertainty of [[finite state machine]]s.

== See also ==
* [[Fuzzy logic]]
* [[Probabilistic logic]]
* [[Plausible reasoning]]
* [[Imprecise probability]]

==References==
{{reflist}}

[[Category:Fuzzy logic]]
[[Category:Information retrieval techniques]]
[[Category:Inference]]</text>
      <sha1>lr2gvhbvry14nyg136z3xz3vdmsrui9</sha1>
    </revision>
  </page>
  <page>
    <title>Thesaurus (information retrieval)</title>
    <ns>0</ns>
    <id>39000674</id>
    <revision>
      <id>708228933</id>
      <parentid>708217770</parentid>
      <timestamp>2016-03-04T11:31:49Z</timestamp>
      <contributor>
        <username>AnomieBOT</username>
        <id>7611264</id>
      </contributor>
      <minor />
      <comment>Dating maintenance tags: {{Refimprove section}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9190" xml:space="preserve">{{about|thesauri used to support indexing, tagging or searching for information|thesauri used in general/literary applications|Thesaurus|the Clare Fischer album|Thesaurus (album)}}

In the context of [[information retrieval]], a '''thesaurus''' (plural: "thesauri") is a form of controlled vocabulary that seeks to dictate semantic manifestations of [[metadata]] in the indexing of content objects. A thesaurus serves to minimise semantic ambiguity by ensuring uniformity and consistency in the storage and retrieval of the manifestations of content objects. ANSI/NISO Z39.19-2005 defines a content object as "any item that is to be described for inclusion in an information retrieval system, website, or other source of information".&lt;ref&gt;ANSI &amp; NISO 2005, Guidelines for the Construction, Format, and Management of Monolingual Controlled Vocabularies, NISO, Maryland, U.S.A, p.11&lt;/ref&gt; The thesaurus aids the assignment of preferred terms to convey semantic metadata associated with the content object.&lt;ref&gt;ANSI &amp; NISO 2005, Guidelines for the Construction, Format, and Management of Monolingual Controlled Vocabularies, NISO, Maryland, U.S.A, p.12&lt;/ref&gt;

A thesaurus serves to guide both an indexer and a searcher in selecting the same preferred term or combination of preferred terms to represent a given subject. [[ISO 25964]], the international standard for information retrieval thesauri, defines a thesaurus as a “controlled and structured vocabulary in which concepts are represented by terms, organized so that relationships between concepts are made explicit, and preferred terms are accompanied by lead-in entries for synonyms or quasi-synonyms.”

A thesaurus is composed by at least three elements: 1-a list of words (or terms), 2-the relationship amongst the words (or terms), indicated by their hierarchical relative position (e.g. parent/broader term; child/narrower term, synonym, etc.), 3-a set of rules on how to use the thesaurus.

== History ==
Wherever there have been large collections of information, whether on paper or in computers, scholars have faced a challenge in pinpointing the items they seek. The use of classification schemes to arrange the documents in order was only a partial solution. Another approach was to index the contents of the documents using words or terms, rather than classification codes. In the 1940s and 1950s some pioneers, such as [[Calvin Mooers]], Charles L. Bernier, [http://pubs.acs.org/cen/priestley/recipients/1951crane.html Evan J. Crane] and [[Hans Peter Luhn]], collected up their index terms in various kinds of list that they called a “thesaurus” (by analogy with the well known thesaurus developed by [[Peter Roget]]).&lt;ref&gt;Roberts, N. The pre-history of the information retrieval thesaurus. ''Journal of Documentation'', 40(4), 1984, p.271-285.&lt;/ref&gt; The first such list put seriously to use in information retrieval was the thesaurus developed in 1959 at the E I Dupont de Nemours Company.&lt;ref&gt;Aitchison, J. and Dextre Clarke, S. The thesaurus: a historical viewpoint, with a look to the future. ''Cataloging &amp; Classification Quarterly'', 37 (3/4), 2004, p.5-21.&lt;/ref&gt;&lt;ref&gt;Krooks, D.A. and Lancaster, F.W. The evolution of guidelines for thesaurus construction. ''Libri'', 43(4), 1993, p.326-342.&lt;/ref&gt;

The first two of these lists to be published were the ''Thesaurus of ASTIA Descriptors'' (1960) and the ''Chemical Engineering Thesaurus'' of the American Institute of Chemical Engineers (1961), a descendant of the Dupont thesaurus. More followed, culminating in the influential ''Thesaurus of Engineering and Scientific Terms'' (TEST) published jointly by the Engineers Joint Council and the US Department of Defense in 1967. TEST did more than just serve as an example; its Appendix 1 presented ''Thesaurus rules and conventions'' that have guided thesaurus construction ever since.
Hundreds of thesauri have been produced since then, perhaps thousands. The most notable innovations since TEST have been:
(a)	Extension from monolingual to multilingual capability; and 
(b)	Addition of a conceptually organized display to the basic alphabetical presentation.

Here we mention only some of the national and international standards that have built steadily on the basic rules set out in TEST:

* [[UNESCO]] ''Guidelines for the establishment and development of monolingual thesauri''. 1970 (followed by later editions in 1971 and 1981)
* DIN 1463 ''Guidelines for the establishment and development of monolingual thesauri''. 1972 (followed by later editions)
* ISO 2788 ''Guidelines for the establishment and development of monolingual thesauri''. 1974 (revised 1986)
* ANSI ''American National Standard for Thesaurus Structure, Construction, and Use''. 1974 (revised 1980 and superseded by ANSI/NISO Z39.19-1993)
* ISO 5964 ''Guidelines for the establishment and development of multilingual thesauri''. 1985
* ANSI/NISO Z39.19 ''Guidelines for the construction, format, and management of monolingual thesauri''. 1993 (revised 2005 and renamed ''Guidelines for the construction, format, and management of monolingual controlled vocabularies''.)
* ISO 25964 ''Thesauri and interoperability with other vocabularies''. Part 1 (''Thesauri for information retrieval'' published 2011; Part 2 (''Interoperability with other vocabularies'') published 2013.

The most clearly visible trend across this history of thesaurus development has been from the context of small-scale isolation to a networked world.&lt;ref&gt;Dextre Clarke, Stella G. and Zeng, Marcia Lei. [http://www.niso.org/publications/isq/2012/v24no1/clarke/ From ISO 2788 to ISO 25964: the evolution of thesaurus standards towards interoperability and data modeling] ''Information standards quarterly'', 24(1), 2012, p.20-26.&lt;/ref&gt; Access to information was notably enhanced when thesauri crossed the divide between monolingual and multilingual applications. More recently, as can be seen from the titles of the latest ISO and NISO standards, there is a recognition that thesauri need to work in harness with other forms of vocabulary or knowledge organization system, such as subject heading schemes, classification schemes, taxonomies and ontologies. The official website for ISO 25964 gives more information, including a reading list.&lt;ref&gt;''[http://www.niso.org/schemas/iso25964/ ISO 25964 – the international standard for thesauri and interoperability with other vocabularies.]'' National Information Standards Organization, 2013.&lt;/ref&gt;

== Purpose ==
{{refimprove section|small=z|date=March 2016}}
In information retrieval, a thesaurus can be used as a form of controlled vocabulary to aid in the indexing of appropriate metadata for information bearing entities. A thesaurus helps with expressing the manifestations of a concept in a prescribed way, to aid in improving [[precision and recall]]. This means that the semantic conceptual expressions of information bearing entities are easier to locate due to uniformity of language. Additionally, a thesaurus is used for maintaining a hierarchical listing of terms; usually single words or bound phrases that aid the indexer in narrowing the terms and limiting semantic ambiguity.

The [[Art and Architecture Thesaurus|Art &amp; Architecture Thesaurus]], for example, is used by countless museums around the world, to catalogue their collections. [[AGROVOC]], the thesaurus of the UN’s [[Food and Agriculture Organization]], is used to index and/or search its AGRIS database of worldwide literature on agricultural research.

== Structure ==
{{refimprove section|small=z|date=March 2016}}
Information retrieval thesauri are formally organized so that existing relationships between concepts are made clear. For example, “citrus fruits” might be linked to the broader concept of “fruits”, and the narrower ones of “oranges”, “lemons”, etc. When the terms are displayed online, the links between them make it very easy to surf around the thesaurus, selecting useful terms for a search. When a single term could have more than one meaning, like tables (furniture) or tables (data), these are listed separately so that the user can choose which concept to search for and avoid retrieving irrelevant results. For any one concept, all the known synonyms are listed, such as “mad cow disease”, “bovine spongiform encephalopathy”, “BSE”, etc. The idea is to guide all the indexers and all the searchers to use the same term for the same concept, so that search results will be as complete as possible. If the thesaurus is multilingual, equivalent terms in other languages are shown too. Following international standards, concepts are generally arranged hierarchically within facets or grouped by themes or topics. Unlike a general thesaurus used for literary purposes, information retrieval thesauri typically focus on one discipline, subject or field of study.

== See also ==
* [[Controlled vocabulary]]
* [[ISO 25964]]
* [[Thesaurus]]

== References ==
{{Reflist}}

== External links ==
* [http://www.niso.org/schemas/iso25964/ Official site for ISO 25964] 
* [http://www.taxonomywarehouse.com/ Taxonomy Warehouse]

[[Category:Information retrieval techniques]]
[[Category:Thesauri]]</text>
      <sha1>9xhiigcku4d2fjctrwju5z0oyf6eues</sha1>
    </revision>
  </page>
  <page>
    <title>Search suggest drop-down list</title>
    <ns>0</ns>
    <id>23344134</id>
    <revision>
      <id>762009683</id>
      <parentid>756061903</parentid>
      <timestamp>2017-01-26T03:23:31Z</timestamp>
      <contributor>
        <username>Connor Behan</username>
        <id>2139896</id>
      </contributor>
      <minor />
      <comment>ce</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8636" xml:space="preserve">A '''search suggest drop-down list''' is a [[Query language|query]] feature used in [[computing]] to show the searcher [[Computer shortcut|shortcut]]s, while the query is typed into a [[text box]]. Before the query is complete, a[[drop-down list]] with the suggested completions appears to provide options to select. The suggested queries then enable the searcher to complete the required search quickly. As a form of [[Autocomplete|autocompletion]], the suggestion list is distinct from [[web browsing history|search history]] in that it attempts to be predictive even when the user is searching for the first time. Data may come from popular searches, sponsors, geographic location or other sources.&lt;ref&gt;{{cite web|url=http://www.thingsontop.com/googles-new-search-suggestions-may-kill-your-website-158.html|title=Google's new search suggestions may kill your website|first=Vegard|last=Sandvoid|publisher=Things On Top|date=2008-12-14|accessdate=2016-08-03}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://diegobasch.com/search-for-obama-on-facebook-and-you-get-romney|title=Search for Obama on Facebook and you get Romney|first=Diego|last=Basch|date=2012-09-19|accessdate=2016-08-03}}&lt;/ref&gt;&lt;ref name="se-land"&gt;{{cite web|url=http://searchengineland.com/how-google-instant-autocomplete-suggestions-work-62592|title=How Google Instant's autocomplete suggestions work|first=Danny|last=Sullivan|publisher=Search Engine Land|date=2011-04-06|accessdate=2016-08-03}}&lt;/ref&gt; These lists are used by [[operating system]]s, [[web browsers]] and various [[website]]s, particularly [[search engine]]s. Search suggestions are common with a 2014 survey finding that over 80% of [[e-commerce]] websites included them.&lt;ref&gt;{{cite web|url=https://www.smashingmagazine.com/2014/08/the-current-state-of-e-commerce-search/|title=The current state of e-commerce search|first=Christian|last=Holt|publisher=Smash Magazine|date=2014-08-18|accessdate=2016-08-03}}&lt;/ref&gt;

The [[Computer science|computing science]] of [[syntax]] and [[algorithm]]s are used to form search results from a [[database]]. [[Content management system]]s and frequent searches can assist [[Software engineering|software engineers]] in [[Optimization (computer science)|optimizing]] more refined queries with methods of parameters and subroutines. Suggestions can be results for the current query or related queries by words, time and dates, categories and [[Tag (metadata)|tags]]. The suggestion list may be reordered by other options, as [[Enumeration|enumerative]], [[Hierarchical organization|hierarchical]] or [[Faceted classification|faceted]].

Although not the first deployment of search suggestions, [[Google Suggest]] is one of the most prominent. Four years before it was considered stable, the feature was developed in 2004 by Google engineer Kevin Gibbs and the name was chosen by [[Marissa Mayer]].&lt;ref&gt;{{cite web|url=http://allthingsd.com/20130823/nearly-a-decade-later-the-autocomplete-origin-story-kevin-gibbs-and-google-suggest/|title=Nearly a Decade Later, the Autocomplete Origin Story: Kevin Gibbs and Google Suggest|first=Liz|last=Gannes|publisher=All Things D|date=2013-08-23|accessdate=2016-08-03}}&lt;/ref&gt; Google, and other large search companies, maintain a blacklist that prevents the display of queries that could be interpreted as violating their [[social responsibility]]. Despite this, the company regularly receives complaints that several popular suggestions, or suggestions whose positions have been inflated by [[Internet bot|bots]], should be added to this list.&lt;ref name="se-land" /&gt;&lt;ref&gt;{{cite web|url=https://utopiaordystopia.com/2015/02/22/truth-and-prediction-in-the-dataclysm/|title=Truth and Prediction in the Dataclysm|first=Rick|last=Searle|publisher=Utopia or Dystopia|date=2015-02-22|accessdate=2016-08-03}}&lt;/ref&gt; The [[Electronic Frontier Foundation]]'s [[Jillian York]] has criticized [[Apple Computers|Apple]]'s blacklist for including words that are merely provocative.&lt;ref&gt;{{cite web|url=http://www.thedailybeast.com/articles/2013/07/16/the-apple-kill-list-what-your-iphone-doesn-t-want-you-to-type.html|title=The Apple 'Kill List': What your iPhone doesn't want you to type|first=Michael|last=Keller|publisher=The Daily Beast|date=2013-07-16|accessdate=2016-08-03}}&lt;/ref&gt;

One example of a project using suggested queries to expose societal attitudes was a 2013 ad series called ''The Autocomplete Truth'' by [[UN Women]]. The campaign showed several gender stereotypes being displayed as popular searches by Google Suggest.&lt;ref&gt;{{cite web|url=http://www.adweek.com/adfreak/after-viral-success-inequality-ads-creators-say-they-will-expand-campaign-153363|title=After viral succes of inequality ads, creators say they will expand campaign|first=David|last=Griner|publisher=Ad Week|date=2013-10-24|accessdate=2016-08-03}}&lt;/ref&gt; Another was a story by [[Bad Astronomy]] that revealed a distrustful perspective on scientists in the suggestion box.&lt;ref&gt;{{cite web|url=http://www.slate.com/blogs/bad_astronomy/2013/12/04/search_engine_bias_scientists_are.html|title="Scientists are..."|first=Phil|last=Plait|publisher=Slate|date=2013-12-04|accessdate=2016-08-03}}&lt;/ref&gt; Additionally, cases related to [[libel]] laws have posited that suggestions may inspire people to associate specific names with specific alleged crimes when they would not have otherwise.&lt;ref name="japan"&gt;{{cite web|url=http://www.tamingthebeast.net/blog/online-world/google-autocomplete-angst.htm|title=Some Folks *Really* Hate Autocomplete|first=Michael|last=Bloch|publisher=Taming The Beast|date=2012-03-27|accessdate=2016-08-03}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|url=http://ijlit.oxfordjournals.org/content/23/3/261.full|title=Search engine liability for autocomplete suggestions: personality, privacy and the power of the algorithm|first1=Stavroula|last1=Karapapa|first2=Maurizio|last2=Borghi|journal=International Journal of Law and Information Technology|volume=23|pages=261-289|year=2015|accessdate=2016-08-03}}&lt;/ref&gt;

Some users have criticized the fact that suggestion-enabled text boxes, unlike the [[web forms]] of static HTML, send data about each keystroke to a central server.&lt;ref&gt;{{cite web|url=http://thekeesh.com/2011/08/who-does-facebook-think-you-are-searching-for/|title=Who does Facebook think you are searching for?|first=Jeremy|last=Keeshin|publisher=The Keesh|date=2011-08-18|accessdate=2016-08-03}}&lt;/ref&gt; Such data has the potential to [[keystroke dynamics|identify specific people]]. This has caused at least one [[Mozilla Firefox]] developer to opine that "users mostly dislike search suggestions".&lt;ref&gt;{{cite web|url=https://bugzilla.mozilla.org/show_bug.cgi?id=1189719|title=Recall and display search history within main browser UI|first=Richard|last=Newman|publisher=Mozilla|date=2015-08-25|accessdate=2016-08-03}}&lt;/ref&gt; Apart from the privacy debate, some users have expressed negative reception over the usefulness of search autocompletion.&lt;ref name="japan" /&gt;&lt;ref&gt;{{cite web|url=http://arnoldit.com/wordpress/2012/09/10/google-autocomplete-is-smart-help-a-hindrance/|title=Google Autocomplete: Is Smart Help A Hindrance?|first=Stephen|last=Arnold|publisher=Beyond Search|date=2012-09-09|accessdate=2016-08-03}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://www.quora.com/How-do-very-strange-stupid-auto-complete-statements-appear-while-searching-on-Google-Do-people-actually-do-these-kind-of-searches-or-are-they-pun-intended|title=How do very strange/stupid auto-complete statements appear while searching on Google? Do people actually do these kind of searches or are they pun intended?|first=Seshal|last=Jain|publisher=Quora|date=2015-05-02|accessdate=2016-08-03}}&lt;/ref&gt; Specifically, the sudden appearance of a suggestion box in some programs has been compared to the behaviour of a [[pop-up ad]].&lt;ref&gt;{{cite web|url=http://martesmartes.blogspot.com/2008/07/disabling-openoffices-stupid.html|title=Disabling Open Office's Stupid Autocomplete|first=Jeff|last=Martens|publisher=Martes-Martes|date=2008-07-09|accessdate=2016-08-03}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://mikesmithers.wordpress.com/2012/01/28/turning-off-code-completion-in-sqldeveloper-a-grumpy-old-man-fights-back/|title=Turning off code completion in SQLDeveloper &amp;mdash; A grumpy old man fights back|first=Mike|last=Smithers|publisher=The Anti-Kyte|date=2012-01-28|accessdate=2016-08-03}}&lt;/ref&gt;

==See also==
*[[Autocomplete]]
*[[Search engine (computing)]]
*[[Search box]]
*[[Search algorithm]]
*[[Censorship by Google#Search suggestions|Censorship by Google § Search suggestions]]

==References==
{{Reflist}}

{{DEFAULTSORT:Search Suggest Drop-Down List}}
[[Category:Information retrieval techniques]]</text>
      <sha1>sg1khoj7kpgqw7315poot0npjijws35</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Concordances (publishing)</title>
    <ns>14</ns>
    <id>43967942</id>
    <revision>
      <id>731145894</id>
      <parentid>724690765</parentid>
      <timestamp>2016-07-23T08:31:48Z</timestamp>
      <contributor>
        <username>Uanfala</username>
        <id>11049176</id>
      </contributor>
      <comment>+[[Category:Corpus linguistics]]; ±[[Category:Linguistics]]→[[Category:Textual criticism]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="260" xml:space="preserve">{{Cat main|Concordance (publishing)}}
[[Category:Biblical studies]]
[[Category:Index (publishing)]]
[[Category:Textual criticism]]
[[Category:Reference works]]
[[Category:Information retrieval techniques]]
[[Category:Hypertext]]
[[Category:Corpus linguistics]]</text>
      <sha1>sja3ebiyoqmljcnfqw3eo8gn7ujov48</sha1>
    </revision>
  </page>
  <page>
    <title>Latent semantic mapping</title>
    <ns>0</ns>
    <id>11989095</id>
    <revision>
      <id>666733714</id>
      <parentid>570019893</parentid>
      <timestamp>2015-06-13T08:02:49Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <comment>removed [[Category:Information retrieval]]; added [[Category:Information retrieval techniques]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1930" xml:space="preserve">'''Latent semantic mapping (LSM)''' is a data-driven framework to model globally meaningful relationships implicit in large volumes of (often textual) data. It is a generalization of [[latent semantic analysis]]. In information retrieval, LSA enables retrieval on the basis of conceptual content, instead of merely matching words between queries and documents.

LSM was derived from earlier work on latent semantic analysis.  There are 3 main characteristics of latent semantic analysis: Discrete entities, usually in the form of words and documents, are mapped onto continuous vectors, the mapping involves a form of global correlation pattern, and dimensionality reduction is an important aspect of the analysis process. These constitute generic properties, and have been identified as potentially useful in a variety of different contexts.  This usefulness has encouraged great interest in LSM. The intended product of latent semantic mapping, is a data-driven framework for modeling relationships in large volumes of data.

[[Mac OS X v10.5]] and later includes a [[Software framework|framework]] implementing latent semantic mapping.&lt;ref&gt;[http://developer.apple.com/documentation/TextFonts/Reference/LatentSemanticMapping/index.html API Reference: Latent Semantic Mapping Framework Reference&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;

== See also ==
* [[Latent semantic analysis]]

== Notes ==
{{reflist}}

== References ==
* {{cite journal
 | url=http://ieeexplore.ieee.org/iel5/79/32367/01511825.pdf
 | title=Latent semantic mapping [information retrieval]
 | author=Bellegarda, J.R.
 | date=2005
}}
* {{cite conference
 | url=https://www.securecms.com/ICASSP2006/Tutorial_06.asp
 | title=Latent semantic mapping: Principles and applications
 | author=J. Bellegarda
 | booktitle=ICASSP 2006
 | date=2006
}}

[[Category:Information retrieval techniques]]
[[Category:Natural language processing]]


{{semantics-stub}}
{{compu-stub}}</text>
      <sha1>131pnwdwdah5a9fif8huxzey1byao0g</sha1>
    </revision>
  </page>
  <page>
    <title>Literature-based discovery</title>
    <ns>0</ns>
    <id>31149053</id>
    <revision>
      <id>666859024</id>
      <parentid>654885635</parentid>
      <timestamp>2015-06-14T05:29:24Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>Category:Information retrieval techniques</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3200" xml:space="preserve">'''Literature-based discovery''' refers to the use of papers and other [[Academic publishing|academic publications]] (the "literature") to find new relationships between existing knowledge (the "discovery"). The technique was pioneered by [[Don R. Swanson]] in the 1980s and has since seen widespread use. 

Literature-based discovery does not generate new knowledge through laboratory experiments, as is customary for [[empirical]] sciences. Instead it seeks to connect existing knowledge from empirical results by bringing to light relationships that are implicated and "neglected".&lt;ref&gt;{{cite journal | last1 = Swanson | first1 = Don | year = 1988 | title = Migraine and Magnesium: Eleven Neglected Connections | url = | journal = Perspectives in Biology and Medicine | volume = 31 | issue = 4| pages = 526–557 | doi=10.1353/pbm.1988.0009}}&lt;/ref&gt; It is marked by [[empiricism]] and [[rationalism]] in concert or [[consilience]].

==Swanson linking==
[[File:Swanson linking.jpg|thumb|Swanson linking example diagram]]
''Swanson linking'' is a term proposed in 2003&lt;ref&gt;Stegmann J, Grohmann G. Hypothesis generation guided by co-word clustering. Scientometrics. 2003;56:111–135. As quoted by Bekhuis&lt;/ref&gt; that refers to connecting two pieces of knowledge previously thought to be unrelated.&lt;ref&gt;{{cite journal|last=Bekhuis|first=Tanja|title=Conceptual biology, hypothesis discovery, and text mining: Swanson's legacy|publisher=BioMed Central Ltd.|year=2006|pmc=1459187|pmid=16584552|doi=10.1186/1742-5581-3-2|volume=3|journal=Biomed Digit Libr|pages=2}}&lt;/ref&gt; For example, it may be known that illness A is caused by chemical B, and that drug C is known to reduce the amount of chemical B in the body. However, because the respective articles were published separately from one another (called "disjoint data"), the relationship between illness A and drug C may be unknown. ''Swanson linking'' aims to find these relationships and report them.

==See also==
*[[Arrowsmith System]]
*[[Implicature]]
*[[Latent semantic indexing]]
*[[Metaphor]]

==References==
* Chen, Ran; Hongfei Lin &amp; Zhihao Yang (2011). "Passage retrieval based hidden knowledge discovery from biomedical literature." ''Expert Systems with Applications: An International Journal'' (August, 2011), vol. 38, no. 8, pp.&amp;nbsp;9958–9964.
*:  '''Abstract''': [...] automatic extraction of the implicit biological relationship from biomedical literature contributes to building the biomedical hypothesis that can be explored further experimentally. This paper presents a passage retrieval based method which can explore the hidden connection from MEDLINE records. [...] Experimental results show this method can significantly improve the hidden knowledge discovery performance. @ [http://portal.acm.org/citation.cfm?id=1967763.1968003&amp;coll=DL&amp;dl=GUIDE&amp;CFID=23143258&amp;CFTOKEN=52033794 ACM DL]

; Further readings
* [[Patrick Wilson (librarian)|Wilson, Patrick]] (1977). ''Public Knowledge, Private Ignorance: Toward a Library and Information Policy''. Greenwood Publishing Group. p.&amp;nbsp;156. ISBN 0-8371-9485-7.

; Footnotes
{{reflist}}

[[Category:Information retrieval techniques]]
[[Category:Medical research]]


{{science-stub}}</text>
      <sha1>ou3v0a629slw4sic5bh7zhhn1npizvt</sha1>
    </revision>
  </page>
  <page>
    <title>Index term</title>
    <ns>0</ns>
    <id>6118940</id>
    <revision>
      <id>740030891</id>
      <parentid>733603028</parentid>
      <timestamp>2016-09-18T17:05:41Z</timestamp>
      <contributor>
        <username>Narky Blert</username>
        <id>22041646</id>
      </contributor>
      <comment>/* In web search engines */ Link to DAB page repaired</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4471" xml:space="preserve">An '''index term''', '''subject term''', '''subject heading''', or '''descriptor''', in [[information retrieval]], is a term that captures the essence of the topic of a document. Index terms make up a [[controlled vocabulary]] for use in [[bibliographic record]]s. They are an integral part of bibliographic control, which is the function by which libraries collect, organize and disseminate documents. They are used as keywords to retrieve documents in an information system, for instance, a catalog or a [[search engine]].  A popular form of keywords on the web are [[tag (metadata)|tags]] which are directly visible and can be assigned by non-experts. Index terms can consist of a word, phrase, or alphanumerical term. They are created by analyzing the document either manually with [[subject indexing]] or automatically with [[Index (search engine)|automatic indexing]] or more sophisticated methods of keyword extraction. Index terms can either come from a controlled vocabulary or be freely assigned.

Keywords are stored in a [[Index (search engine)|search index]]. Common words like [[article (grammar)|articles]] (a, an, the) and conjunctions (and, or, but) are not treated as keywords because it's inefficient. Almost every English-language site on the Internet has the article "''the''", and so it makes no sense to search for it. The most popular search engine, [[Google]] removed [[stop words]] such as "the" and "a" from its indexes for several years, but then re-introduced them, making certain types of precise search possible again.

The term "descriptor" was coined by [[Calvin Mooers]] in 1948. It is in particular used about a preferred term from a [[Thesaurus (information retrieval)|thesaurus]].

The [[Simple Knowledge Organization System]] language (SKOS) provides a way to express index terms with [[Resource Description Framework]] for use in the context of [[Semantic Web]].&lt;ref name="auto"&gt;{{cite book|last=Svenonius|first=Elaine|author-link=Elaine Svenonius|title=The intellectual foundation of information organization|date=2009|publisher=MIT Press|location=Cambridge, Mass.|isbn=9780262512619|edition=1st MIT Press pbk.}}&lt;/ref&gt;

==In web search engines==
Most [[web search engine]]s are designed to search for words anywhere in a document—the title, the body, and so on. This being the case, a keyword can be any term that exists within the document. However, priority is given to words that occur in the title, words that recur numerous times, and words that are explicitly assigned as keywords within the coding.&lt;ref&gt;Cutts, Matt. (2010, March 4). ''How search works.'' Retrieved from https://www.youtube.com/watch?v=BNHR6IQJGZs&lt;/ref&gt; Index terms can be further refined using [[Boolean algebra|Boolean operators]] such as "AND, OR, NOT." "AND" is normally unnecessary as most search engines infer it. "OR" will search for results with one search term or another, or both. "NOT" eliminates a word or phrase from the search, getting rid of any results that include it. Multiple words can also be enclosed in quotation marks to turn the individual index terms into a specific index ''phrase''. These modifiers and methods all help to refine search terms, to better maximize the accuracy of search results.&lt;ref&gt;CLIO. ''Keyword search''. Columbia University Libraries. Retrieved from http://www.columbia.edu/cu/lweb/help/clio/keyword.html&lt;/ref&gt;

==Author keywords==
Many journals and databases provides access (also) to index terms made by authors to the articles being published or represented. The relative quality of indexer-provided index terms and author provided index terms is of interest to research in information retrieval. The quality of both kinds of indexing terms depends, of course, on the qualifications of provider. In general authors have difficulties providing indexing terms that characterizes his document ''relative'' to the other documents in the database. Author keywords are an integral part of literature.&lt;ref name="auto"/&gt;

==Examples==
*[[Canadian Subject Headings]] (CSH)
*[[Library of Congress Subject Headings]] (LCSH)
*[[Medical Subject Headings]] (MeSH)
*[[Polythematic Structured Subject Heading System]] (PSH)
*[[Subject Headings Authority File]] (SWD)

==See also==
*[[Dynamic keyword insertion]]
*[[Tag cloud]]
*[[Keyword density]]
*[[Search engine optimization]]
*[[Tag (metadata)]]
*[[Subject (documents)]]

==References==
{{reflist}}

{{Authority control}}
[[Category:Information retrieval techniques]]</text>
      <sha1>2wst6ho99lixeoo7mfmjfl4oeluwr1h</sha1>
    </revision>
  </page>
  <page>
    <title>Proximity search (text)</title>
    <ns>0</ns>
    <id>1934622</id>
    <revision>
      <id>755821189</id>
      <parentid>694535384</parentid>
      <timestamp>2016-12-20T11:23:12Z</timestamp>
      <contributor>
        <username>Feminist</username>
        <id>25530780</id>
      </contributor>
      <minor />
      <comment>/* Usage in commercial search engines */[[Talk:Bing (search engine)#Requested move 13 December 2016]], replaced: [[Bing]] → [[Bing (search engine)|Bing]] (2) using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7226" xml:space="preserve">In [[natural language processing|text processing]], a '''proximity search''' looks for documents where two or more separately matching term occurrences are within a specified [[string distance|distance]], where distance is the number of intermediate words or characters. In addition to proximity, some implementations may also impose a constraint on the word order, in that the order in the searched text must be identical to the order of the search query. Proximity searching goes beyond the simple matching of words by adding the constraint of proximity and is generally regarded as a form of advanced search.

For example, a search could be used to find "red brick house", and match phrases such as "red house of brick" or "house made of red brick". By limiting the proximity, these phrases can be matched while avoiding documents where the words are scattered or spread across a page or in unrelated articles in an anthology.

== Rationale ==
The basic linguistic assumption of proximity searching is that the proximity of the words in a document implies a [[semantic relation|relationship]] between the words. Given that authors of documents try to formulate sentences which contain a single idea, or cluster of related ideas within neighboring sentences or organized into paragraphs, there is an inherent, relatively high, probability within the document structure that words used together are related. On the other hand, when two words are on the opposite ends of a book, the probability of a relationship between the words is relatively weak. By limiting search results to only include matches where the words are within the specified maximum proximity, or distance, the search results are assumed to be of higher relevance than the matches where the words are scattered.

Commercial internet search engines tend to produce too many matches (known as recall) for the average search query. Proximity searching is one method of reducing the number of pages matches, and to improve the relevance of the matched pages by using word proximity to assist in ranking. As an added benefit, proximity searching helps combat [[spamdexing]] by avoiding webpages which contain dictionary lists or shotgun lists of thousands of words, which would otherwise rank highly if the search engine was heavily biased toward [[word frequency]].

== Boolean syntax and operators ==
Note that a proximity search can designate that only some keywords must be within a specified distance. Proximity searching can be used with other search syntax and/or controls to allow more articulate search queries. Sometimes query operators like NEAR, NOT NEAR, FOLLOWED BY, NOT FOLLOWED BY, SENTENCE or FAR are used to indicate a proximity-search limit between specified keywords: for example, "brick NEAR house".

== Usage in commercial search engines ==
In regards to implicit/automatic versus explicit proximity search, as of November 2008, most Internet [[search engine]]s only implement an implicit proximity search functionality. That is, they automatically rank those search results higher where the user keywords have a good "overall proximity score" in such results. If only two keywords are in the search query, this has no difference from an explicit proximity search which puts a NEAR operator between the two keywords. However, if three or more than three keywords are present, it is often important for the user to specify which subsets of these keywords expect a proximity in search results. This is useful if the user wants to do a [[prior art]] search (e.g. finding an existing approach to complete a specific task, finding a document that discloses a system that exhibits a procedural behavior collaboratively conducted by several components and links between these components).

[[Web search engine]]s which support proximity search via an explicit proximity operator in their query language include  [[Walhello]], [[Exalead]], [[Yandex]], [[Yahoo!]], [[Altavista]], and [[Bing (search engine)|Bing]]:
* When using the [[Walhello]] search-engine, the proximity can be defined by the number of characters between the keywords.&lt;ref&gt;[http://www.walhello.com/aboutgl.html "About Walhello"], visited 23 December 2009&lt;/ref&gt;
* The search engine Exalead allows the user to specify the required proximity, as the maximum number of words between keywords. The syntax is &lt;tt&gt;(keyword1 NEAR/n keyword2)&lt;/tt&gt; where n is the number of words.&lt;ref&gt;[http://www.exalead.com/search/web/search-syntax/#proximity_search "Web Search Syntax"], visited 23 December 2009&lt;/ref&gt;
* [[Yandex]] uses the syntax &lt;tt&gt;keyword1 /n keyword2&lt;/tt&gt; to search for two keywords separated by at most &lt;math&gt;n - 1&lt;/math&gt; words, and supports a few other variations of this syntax.&lt;ref&gt;[http://help.yandex.ru/search/?id=481939 Yandex help page on query language] (in Russian)&lt;/ref&gt;
* [[Yahoo!]] and [[Altavista]] both support an undocumented NEAR operator.&lt;ref&gt;[http://search.yahoo.com/search?p=site%3Awww.rfc-editor.org+inurl%3Arfc2606+guidance+NEAR+additional "Successful Yahoo! proximity query"] (22 Feb 2010)&lt;/ref&gt;&lt;ref&gt;[http://search.yahoo.com/search?p=site%3Awww.rfc-editor.org+inurl%3Arfc2606+guidance+NEAR+unused "Unsuccessful Yahoo! proximity query"] (22 Feb 2010)&lt;/ref&gt; The syntax is &lt;tt&gt;keyword1 NEAR keyword2&lt;/tt&gt;.
* [[Google Search]] supports AROUND(#).&lt;ref&gt;[http://www.guidingtech.com/16116/google-search-little-known-around-operator/ "GuidingTech: Meet Google Search's Little Known AROUND Operator"]&lt;/ref&gt;&lt;ref&gt;[http://www.netforlawyers.com/content/google-offers-proximity-search-around-connector-0015/ "Google Offers Proximity Search"] (8 Feb 2011)&lt;/ref&gt;
* [[Bing (search engine)|Bing]] supports NEAR.&lt;ref&gt;[http://www.howtogeek.com/106751/how-to-use-bings-advanced-search-operators-8-tips-for-better-searches/ "How to Use Bing’s Advanced Search Operators"]&lt;/ref&gt; The syntax is &lt;tt&gt;keyword1 near:n keyword2&lt;/tt&gt; where n=the number of maximum separating words.

Ordered search within the [[Google]] and [[Yahoo!]] search engines is possible using the asterisk (*) full-word [[Wildcard character|wildcard]]s: in Google this matches one or more words,&lt;ref&gt;[http://www.google.com/support/websearch/bin/answer.py?answer=136861 "More Google Search Help" visited 23 December 2009]&lt;/ref&gt; and an in Yahoo! Search this matches exactly one word.&lt;ref&gt;[http://www.searchengineshowdown.com/features/yahoo/review.html "Review of Yahoo! Search", by Search Engine Showdown, visited 23 December 2009]&lt;/ref&gt;  (This is easily verified by searching for the following phrase in both Google and Yahoo!: "addictive * of biblioscopy".)

To emulate unordered search of the NEAR operator can be done using a combination of ordered searches.  For example, to specify a close co-occurrence of "house" and "dog", the following search-expression could be specified: "house dog" OR "dog house" OR "house * dog" OR "dog * house" OR "house * * dog" OR "dog * * house".

== See also ==
* [[Compound term processing]]
* [[Edit distance]]
* [[Information retrieval]]
* [[Search engine]]
* [[Search engine indexing]] - how texts are indexed to support proximity search
* [[Semantic proximity]]

== Notes ==
{{Reflist}}

[[Category:Information retrieval techniques]]
[[Category:Internet search algorithms]]</text>
      <sha1>i3qh3jimn39edq902cn23uoo1g1ycd3</sha1>
    </revision>
  </page>
  <page>
    <title>Cluster labeling</title>
    <ns>0</ns>
    <id>25202953</id>
    <revision>
      <id>752038667</id>
      <parentid>751539775</parentid>
      <timestamp>2016-11-29T04:25:18Z</timestamp>
      <contributor>
        <username>Ryk72</username>
        <id>20425983</id>
      </contributor>
      <comment>Disambiguated: [[fusion]] → [[wikt:fusion]]; ce; spc</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10522" xml:space="preserve">In [[natural language processing]] and [[information retrieval]], '''cluster labeling''' is the problem of picking descriptive, human-readable labels for the clusters produced by a [[document clustering]] algorithm; standard clustering algorithms do not typically produce any such labels. Cluster labeling algorithms examine the contents of the documents per cluster to find a labeling that summarize the topic of each cluster and distinguish the clusters from each other.

==Differential cluster labeling==
Differential cluster labeling labels a cluster by comparing term [[probability distribution|distributions]] across clusters, using techniques also used for [[feature selection]] in [[document classification]], such as [[mutual information]] and [[Pearson's chi-squared test|chi-squared feature selection]].  Terms having very low frequency are not the best in representing the whole cluster and can be omitted in labeling a cluster.  By omitting those rare terms and using a differential test, one can achieve the best results with differential cluster labeling.&lt;ref&gt;Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schutze. ''Introduction to Information Retrieval''. Cambridge: Cambridge UP, 2008. ''Cluster Labeling''. Stanford Natural Language Processing Group. Web. 25 Nov. 2009. &lt;http://nlp.stanford.edu/IR-book/html/htmledition/cluster-labeling-1.html&gt;.&lt;/ref&gt;

===Pointwise mutual information===

{{Main article|Pointwise mutual information}}

In the fields of [[probability theory]] and [[information theory]], mutual information measures the degree of dependence of two [[random variables]].  The mutual information of two variables {{mvar|X}} and {{mvar|Y}} is defined as:

&lt;math&gt;I(X, Y) = \sum_{x\in X}{ \sum_{y\in Y} {p(x, y)log_2\left(\frac{p(x, y)}{p_1(x)p_2(y)}\right)}}&lt;/math&gt;

where ''p(x, y)'' is the [[joint probability|joint probability distribution]] of the two variables, ''p&lt;sub&gt;1&lt;/sub&gt;(x)'' is the probability distribution of X, and ''p&lt;sub&gt;2&lt;/sub&gt;(y)'' is the probability distribution of Y.

In the case of cluster labeling, the variable X is associated with membership in a cluster, and the variable Y is associated with the presence of a term.&lt;ref&gt;Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schutze. ''Introduction to Information Retrieval''. Cambridge: Cambridge UP, 2008. ''Mutual Information''. Stanford Natural Language Processing Group. Web. 25 Nov. 2009. &lt;http://nlp.stanford.edu/IR-book/html/htmledition/mutual-information-1.html&gt;.&lt;/ref&gt;  Both variables can have values of 0 or 1, so the equation can be rewritten as follows:

&lt;math&gt;I(C, T) = \sum_{c\in {0, 1}}{ \sum_{t\in {0, 1}} {p(C = c, T = t)log_2\left(\frac{p(C = c, T = t)}{p(C = c)p(T = t)}\right)}}&lt;/math&gt;

In this case, ''p(C = 1)'' represents the probability that a randomly selected document is a member of a particular cluster, and ''p(C = 0)'' represents the probability that it isn't.  Similarly, ''p(T = 1)'' represents the probability that a randomly selected document contains a given term, and ''p(T = 0)'' represents the probability that it doesn't.  The [[joint probability|joint probability distribution function]] ''p(C, T)'' represents the probability that two events occur simultaneously.  For example, ''p(0, 0)'' is the probability that a document isn't a member of cluster ''c'' and doesn't contain term ''t''; ''p(0, 1)'' is the probability that a document isn't a member of cluster ''c'' and does contain term ''t''; and so on.

===Chi-Squared Selection===
{{Main article|Pearson's chi-squared test}}
The Pearson's chi-squared test can be used to calculate the probability that the occurrence of an event matches the initial expectations.  In particular, it can be used to determine whether two events, A and B, are [[statistically independent]].  The value of the chi-squared statistic is:

&lt;math&gt;X^2 = \sum_{a \in A}{\sum_{b \in B}{\frac{(O_{a,b} - E_{a, b})^2}{E_{a, b}}}}&lt;/math&gt;

where ''O&lt;sub&gt;a,b&lt;/sub&gt;'' is the ''observed'' frequency of a and b co-occurring, and ''E&lt;sub&gt;a,b&lt;/sub&gt;'' is the ''expected'' frequency of co-occurrence.

In the case of cluster labeling, the variable A is associated with membership in a cluster, and the variable B is associated with the presence of a term.  Both variables can have values of 0 or 1, so the equation can be rewritten as follows:

&lt;math&gt;X^2 = \sum_{a \in {0,1}}{\sum_{b \in {0,1}}{\frac{(O_{a,b} - E_{a, b})^2}{E_{a, b}}}}&lt;/math&gt;

For example, ''O&lt;sub&gt;1,0&lt;/sub&gt;'' is the observed number of documents that are in a particular cluster but don't contain a certain term, and ''E&lt;sub&gt;1,0&lt;/sub&gt;'' is the expected number of documents that are in a particular cluster but don't contain a certain term.
Our initial assumption is that the two events are independent, so the expected probabilities of co-occurrence can be calculated by multiplying individual probabilities:&lt;ref&gt;Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schutze. ''Introduction to Information Retrieval''. Cambridge: Cambridge UP, 2008. ''Chi2 Feature Selection''. Stanford Natural Language Processing Group. Web. 25 Nov. 2009. &lt;http://nlp.stanford.edu/IR-book/html/htmledition/feature-selectionchi2-feature-selection-1.html&gt;.&lt;/ref&gt;

''E&lt;sub&gt;1,0&lt;/sub&gt; = N * P(C = 1) * P(T = 0)''

where N is the total number of documents in the collection.

==Cluster-Internal Labeling==
Cluster-internal labeling selects labels that only depend on the contents of the cluster of interest. No comparison is made with the other clusters.
Cluster-internal labeling can use a variety of methods, such as finding terms that occur frequently in the centroid or finding the document that lies closest to the centroid.

===Centroid Labels===
{{Main article|Vector space model}}
A frequently used model in the field of [[information retrieval]] is the vector space model, which represents documents as vectors.  The entries in the vector correspond to terms in the [[vocabulary]]. Binary vectors have a value of 1 if the term is present within a particular document and 0 if it is absent. Many vectors make use of weights that reflect the importance of a term in a document, and/or the importance of the term in a document collection. For a particular cluster of documents, we can calculate the [[centroid]] by finding the [[arithmetic mean]] of all the document vectors.  If an entry in the centroid vector has a high value, then the corresponding term occurs frequently within the cluster.  These terms can be used as a label for the cluster.
One downside to using centroid labeling is that it can pick up words like "place" and "word" that have a high frequency in written text, but have little relevance to the contents of the particular cluster.

===Contextualized centroid labels===
A simple, cost-effective way of overcoming the above limitation is to embed the centroid terms with the highest weight in a graph structure that provides a context for their interpretation and selection.&lt;ref&gt;Francois Role, Moahmed Nadif. [http://dl.acm.org/citation.cfm?id=2574675 Beyond cluster labeling: Semantic interpretation of clusters’ contents using a graph representation.] Knowledge-Based Systems, Volume 56, January, 2014: 141-155&lt;/ref&gt;
In this approach, a term-term co-occurrence matrix referred as &lt;math&gt;T_k&lt;/math&gt; is first built for each cluster &lt;math&gt;S_k&lt;/math&gt;. Each cell represents the number of times term &lt;math&gt;i&lt;/math&gt; co-occurs with term &lt;math&gt;j&lt;/math&gt; within a certain window of text (a sentence, a paragraph, etc.)
In a second stage, a similarity matrix &lt;math&gt;T_k^{sim}&lt;/math&gt; is obtained by multiplying &lt;math&gt;T_k&lt;/math&gt; with its transpose. We have &lt;math&gt;T_k^{sim}=T_k' T_k=(t_{{sim}_{ij}})&lt;/math&gt;. Being the dot product of two normalized vectors &lt;math&gt;\tilde{t}_{i}&lt;/math&gt; and &lt;math&gt;\tilde{t}_{j}&lt;/math&gt;, &lt;math&gt;t_{{sim}_{ij}}&lt;/math&gt; denotes the cosine similarity between terms &lt;math&gt;i&lt;/math&gt; and &lt;math&gt;j&lt;/math&gt;. The so obtained &lt;math&gt;T_k^{sim}&lt;/math&gt; can then be used as the weighted adjacency matrix of a term similarity graph. The centroid terms are part of this graph, and they thus can be interpreted and scored by inspecting the terms that surround them in the graph.

===Title labels===
An alternative to centroid labeling is title labeling.  Here, we find the document within the cluster that has the smallest [[Euclidean distance]] to the centroid, and use its title as a label for the cluster.  One advantage to using document titles is that they provide additional information that would not be present in a list of terms.  However, they also have the potential to mislead the user, since one document might not be representative of the entire cluster.

===External knowledge labels===
Cluster labeling can be done indirectly using external knowledge such as pre-categorized knowledge such as the one of Wikipedia.&lt;ref&gt;David Carmel, Haggai Roitman, Naama Zwerdling. [http://portal.acm.org/citation.cfm?doid=1571941.1571967 Enhancing cluster labeling using wikipedia.] SIGIR 2009: 139-146&lt;/ref&gt; In such methods, a set of important cluster text features are first extracted from the cluster documents. These features then can be used to retrieve the (weighted) K-nearest categorized documents from which candidates for cluster labels can be extracted. The final step involves the ranking of such candidates. Suitable methods are such that are based on a voting or a fusion process which is determined using the set of categorized documents and the original cluster features.

=== Combining Several Cluster Labelers ===
The cluster labels of several different cluster labelers can be further combined to obtain better labels. 
For example, [[Linear Regression]] can be used to learn an optimal combination of labeler scores.&lt;ref&gt;David Carmel, Haggai Roitman, Naama Zwerdling. [http://portal.acm.org/citation.cfm?doid=1571941.1571967 Enhancing cluster labeling using wikipedia.] SIGIR 2009: 139-146&lt;/ref&gt; A more sophisticated technique is based on a [[wikt:fusion|fusion]] approach and analysis of the cluster labels decision stability of various labelers.&lt;ref&gt;Haggai Roitman, Shay Hummel, Michal Shmueli-Scheuer. [http://dl.acm.org/citation.cfm?id=2609465 A fusion approach to cluster labeling.] SIGIR 2014: 883-886&lt;/ref&gt;

==External links==
* [http://nlp.stanford.edu/IR-book/html/htmledition/hierarchical-clustering-1.html Hierarchical Clustering]
* [http://www.cs.cmu.edu/~callan/Papers/dgo06-puck.pdf Automatically Labeling Hierarchical Clusters]

==References==
&lt;references/&gt;

{{DEFAULTSORT:Cluster Labeling}}
[[Category:Information retrieval techniques]]</text>
      <sha1>hb72the6uunib3ub4tupkrny17f5qfm</sha1>
    </revision>
  </page>
  <page>
    <title>Collaborative filtering</title>
    <ns>0</ns>
    <id>480289</id>
    <revision>
      <id>760205549</id>
      <parentid>758697402</parentid>
      <timestamp>2017-01-15T16:20:37Z</timestamp>
      <contributor>
        <ip>46.217.7.105</ip>
      </contributor>
      <comment>/* Memory-based */ removed comma</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="26610" xml:space="preserve">{{external links|date=November 2013}}
{{Use dmy dates|date=June 2013}}
{{Recommender systems}}
[[File:Collaborative filtering.gif|300px|thumb|

This image shows an example of predicting of the user's rating using [[Collaborative software|collaborative]] filtering. At first, people rate different items (like videos, images, games). After that, the system is making [[prediction]]s about user's rating for an item, which the user hasn't rated yet. These predictions are built upon the existing ratings of other users, who have similar ratings with the active user. For instance, in our case the system has made a prediction, that the active user won't like the video.]]

'''Collaborative filtering''' ('''CF''') is a technique used by [[recommender system]]s.&lt;ref name="handbook"&gt;Francesco Ricci and Lior Rokach and Bracha Shapira, [http://www.inf.unibz.it/~ricci/papers/intro-rec-sys-handbook.pdf Introduction to Recommender Systems Handbook], Recommender Systems Handbook, Springer, 2011, pp. 1-35&lt;/ref&gt; Collaborative filtering has two senses, a narrow one and a more general one.&lt;ref name=recommender&gt;{{cite web|title=Beyond Recommender Systems: Helping People Help Each Other|url=http://www.grouplens.org/papers/pdf/rec-sys-overview.pdf|publisher=Addison-Wesley|accessdate=16 January 2012|page=6|year=2001|last1=Terveen|first1=Loren|last2=Hill|first2=Will|authorlink1=Loren Terveen}}&lt;/ref&gt;  

In the newer, narrower sense, collaborative filtering is a method of making automatic [[prediction]]s (filtering) about the interests of a user by collecting preferences or [[taste (sociology)|taste]] information from [[crowdsourcing|many users]] (collaborating). The underlying assumption of the collaborative filtering approach is that if a person ''A'' has the same opinion as a person ''B'' on an issue, A is more likely to have B's opinion on a different issue ''x'' than to have the opinion on x of a person chosen randomly. For example, a collaborative filtering recommendation system for [[television]] tastes could make predictions about which television show a user should like given a partial list of that user's tastes (likes or dislikes).&lt;ref&gt;[http://www.redbeemedia.com/insights/integrated-approach-tv-vod-recommendations An integrated approach to TV &amp; VOD Recommendations] {{webarchive |url=https://web.archive.org/web/20120606225352/http://www.redbeemedia.com/insights/integrated-approach-tv-vod-recommendations |date=6 June 2012 }}&lt;/ref&gt; Note that these predictions are specific to the user, but use information gleaned from many users. This differs from the simpler approach of giving an [[average]] (non-specific) score for each item of interest, for example based on its number of [[vote]]s.

In the more general sense, collaborative filtering is the process of filtering for information or patterns using techniques involving collaboration among multiple agents, viewpoints, data sources, etc.&lt;ref name="recommender" /&gt;  Applications of collaborative filtering typically involve very large data sets. Collaborative filtering methods have been applied to many different kinds of data including: sensing and monitoring data, such as in mineral exploration, environmental sensing over large areas or multiple sensors; financial data, such as financial service institutions that integrate many financial sources; or in electronic commerce and web applications  where the focus is on user data, etc. The remainder of this discussion focuses on collaborative filtering for user data, although some of the methods and approaches may apply to the other major applications as well.

==Introduction==
The [[internet growth|growth]] of the [[Internet]] has made it much more difficult to effectively [[information extraction|extract useful information]] from all the available [[online information]]. The overwhelming amount of data necessitates  mechanisms for efficient [[information filtering]]. Collaborative filtering is one of the techniques used for dealing with this problem.

The motivation for collaborative filtering comes from the idea that people often get the best recommendations from someone with tastes similar to themselves. Collaborative filtering encompasses techniques for matching people with similar interests and making [[recommender system|recommendations]] on this basis.

Collaborative filtering algorithms often require (1) users' active participation, (2) an easy way  to represent users' interests, and (3) algorithms that are able to match people with similar interests.

Typically, the workflow of a collaborative filtering system is:
# A user expresses his or her preferences by rating items (e.g. books, movies or CDs) of the system. These ratings can be viewed as an approximate representation of the user's interest in the corresponding domain.
# The system matches this user's ratings against other users'  and finds the people with most "similar" tastes.
# With similar users, the system recommends items that the similar users have rated highly but not yet being rated by this user (presumably the absence of rating is often considered as the unfamiliarity of an item)
A key problem of collaborative filtering is how to combine and weight the preferences of user neighbors. Sometimes, users can immediately rate the recommended items. As a result, the system gains an increasingly accurate representation of user preferences over time.

==Methodology==

[[File:Collaborative Filtering in Recommender Systems.jpg|thumb|Collaborative Filtering in Recommender Systems]]

Collaborative filtering systems have many forms, but many common systems can be reduced to two steps:
# Look for users who share the same rating patterns with the active user (the user whom the prediction is for).
# Use the ratings from those like-minded users found in step 1 to calculate a prediction for the active user
This falls under the category of user-based collaborative filtering. A specific application of this is the user-based [[K-nearest neighbor algorithm|Nearest Neighbor algorithm]].

Alternatively, [[item-item collaborative filtering|item-based collaborative filtering]] (users who bought x also bought y), proceeds in an item-centric manner:
# Build an item-item matrix determining relationships between pairs of items
# Infer the tastes of the current user by examining the matrix and matching that user's data
See, for example, the [[Slope One]] item-based collaborative filtering family.

Another form of collaborative filtering can be based on implicit observations of normal user behavior (as opposed to the artificial behavior imposed by a rating task). These systems observe what a user has done together with what all users have done (what music they have listened to, what items they have bought) and use that data to predict the user's behavior in the future, or to predict how a user might like to behave given the chance.  These predictions then have to be filtered through [[business logic]] to determine how they might affect the actions of a business system.  For example, it is not useful to offer to sell somebody a particular album of music if they already have demonstrated that they own that music.

Relying on a scoring or rating system which is averaged across all users ignores specific demands of a user, and is particularly poor in tasks where there is large variation in interest (as in the recommendation of music). However, there are other methods to combat information explosion, such as [[WWW|web]] search and [[data clustering]].

==Types==

===Memory-based===
This approach uses user rating data to compute the similarity between users or items. This is used for making recommendations. This was an early approach used in many commercial systems. It's effective and easy to implement. Typical examples of this approach are neighbourhood-based CF and item-based/user-based top-N recommendations. For example, in user based approaches, the value of ratings user 'u' gives to item 'i' is calculated as an aggregation of some similar users' rating of the item:
:&lt;math&gt;r_{u,i} = \operatorname{aggr}_{u^\prime \in U} r_{u^\prime, i}&lt;/math&gt;

where 'U' denotes the set of top 'N' users that are most similar to user 'u' who rated item 'i'. Some examples of the aggregation function includes:
:&lt;math&gt;r_{u,i} = \frac{1}{N}\sum\limits_{u^\prime \in U}r_{u^\prime, i}&lt;/math&gt;
:&lt;math&gt;r_{u,i} = k\sum\limits_{u^\prime \in U}\operatorname{simil}(u,u^\prime)r_{u^\prime, i}&lt;/math&gt;
:&lt;math&gt;r_{u,i} = \bar{r_u} +  k\sum\limits_{u^\prime \in U}\operatorname{simil}(u,u^\prime)(r_{u^\prime, i}-\bar{r_{u^\prime}} )&lt;/math&gt;

where k is a normalizing factor defined as &lt;math&gt;k =1/\sum_{u^\prime \in U}|\operatorname{simil}(u,u^\prime)| &lt;/math&gt;. and &lt;math&gt;\bar{r_u}&lt;/math&gt; is the average rating of user u for all the items rated by u.

The neighborhood-based algorithm calculates the similarity between two users or items produces a prediction for the user by taking the [[weighted average]] of all the ratings. Similarity computation between items or users is an important part of this approach. Multiple measures, such as [[Pearson product-moment correlation coefficient|Pearson correlation]] and [[Cosine similarity|vector cosine]] based similarity are used for this.

The Pearson correlation similarity of two users x, y is defined as 
:&lt;math&gt; \operatorname{simil}(x,y) = \frac{\sum\limits_{i \in I_{xy}}(r_{x,i}-\bar{r_x})(r_{y,i}-\bar{r_y})}{\sqrt{\sum\limits_{i \in I_{xy}}(r_{x,i}-\bar{r_x})^2\sum\limits_{i \in I_{xy}}(r_{y,i}-\bar{r_y})^2}} &lt;/math&gt;

where I&lt;sub&gt;xy&lt;/sub&gt; is the set of items rated by both user x and user y.

The cosine-based approach defines the cosine-similarity between two users x and y as:&lt;ref name="Breese1999"&gt;John S. Breese, David Heckerman, and Carl Kadie, [http://uai.sis.pitt.edu/displayArticleDetails.jsp?mmnu=1&amp;smnu=2&amp;article_id=231&amp;proceeding_id=14 Empirical Analysis of Predictive Algorithms for Collaborative Filtering], 1998 {{webarchive |url=https://web.archive.org/web/20131019134152/http://uai.sis.pitt.edu/displayArticleDetails.jsp?mmnu=1&amp;smnu=2&amp;article_id=231&amp;proceeding_id=14 |date=19 October 2013 }}&lt;/ref&gt;
:&lt;math&gt;\operatorname{simil}(x,y) = \cos(\vec x,\vec y) = \frac{\vec x \cdot \vec y}{||\vec x|| \times ||\vec y||} = \frac{\sum\limits_{i \in I_{xy}}r_{x,i}r_{y,i}}{\sqrt{\sum\limits_{i \in I_{x}}r_{x,i}^2}\sqrt{\sum\limits_{i \in I_{y}}r_{y,i}^2}}&lt;/math&gt;

The user based top-N recommendation algorithm uses a similarity-based vector model to identify the k most similar users to an active user. After the k most similar users are found, their corresponding user-item matrices are aggregated to identify the set of items to be recommended. A popular method to find the similar users is the [[Locality-sensitive hashing]], which implements the [[Nearest neighbor search|nearest neighbor mechanism]] in linear time.

The advantages with this approach include: the explainability of the results, which is an important aspect of recommendation systems; easy creation and use; easy facilitation of new data; content-independence of the items being recommended; good scaling with co-rated items.

There are also several disadvantages with this approach. Its performance decreases when [[sparsity|data gets sparse]], which occurs frequently with web-related items. This hinders the [[scalability]] of this approach and creates problems with large datasets. Although it can efficiently handle new users because it relies on a [[data structure]], adding new items becomes more complicated since that representation usually relies on a specific [[vector space]]. Adding new items requires inclusion of the new item and the re-insertion of all the elements in the structure.

===Model-based===
Models are developed using [[data mining]], [[machine learning]] algorithms to find patterns based on training data. These are used to make predictions for real data. There are many model-based CF algorithms. These include [[Bayesian networks]], [[Cluster Analysis|clustering models]], [[Latent Semantic Indexing|latent semantic models]] such as [[singular value decomposition]], [[probabilistic latent semantic analysis]], multiple multiplicative factor, [[latent Dirichlet allocation]] and [[Markov decision process]] based models.&lt;ref name="Suetal2009"&gt;Xiaoyuan Su, Taghi M. Khoshgoftaar, [http://www.hindawi.com/journals/aai/2009/421425/ A survey of collaborative filtering techniques], Advances in Artificial Intelligence archive, 2009.&lt;/ref&gt;

This approach has a more holistic goal to uncover latent factors that explain observed ratings.&lt;ref&gt;[http://research.yahoo.com/pub/2435 Factor in the Neighbors: Scalable and Accurate Collaborative Filtering] {{webarchive |url=https://web.archive.org/web/20101023032716/http://research.yahoo.com/pub/2435 |date=23 October 2010 }}&lt;/ref&gt; Most of the models are based on creating a classification or clustering technique to identify the user based on the training set. The number of the parameters can be reduced based on types of [[Principal Component Analysis|principal component analysis]].

There are several advantages with this paradigm. It handles the sparsity better than memory based ones. This helps with scalability with large data sets. It improves the prediction performance. It gives an intuitive rationale for the recommendations.

The disadvantages with this approach are in the expensive model building. One needs to have a tradeoff between prediction performance and scalability. One can lose useful information due to reduction models. A number of models have difficulty explaining the predictions.

===Hybrid===
A number of applications combine the memory-based and the model-based CF algorithms. These overcome the limitations of native CF approaches and improve prediction performance. Importantly, they overcome the CF problems such as sparsity and loss of information. However, they have increased complexity and are expensive to implement.&lt;ref&gt;{{cite journal | url = http://www.sciencedirect.com/science/article/pii/S0020025512002587 | doi=10.1016/j.ins.2012.04.012 | volume=208 | title=Kernel-Mapping Recommender system algorithms | journal=Information Sciences | pages=81–104}}
&lt;/ref&gt; Usually most commercial recommender systems are hybrid, for example, the Google news recommender system.&lt;ref&gt;{{cite web|url=http://dl.acm.org/citation.cfm?id=1242610|title=Google news personalization|publisher=}}&lt;/ref&gt;

==Application on social web==
Unlike the traditional model of mainstream media, in which there are few editors who set guidelines, collaboratively filtered social media can have a very large number of editors, and content improves as the number of participants increases. Services like [[Reddit]], [[YouTube]], and [[Last.fm]] are typical example of collaborative filtering based media.&lt;ref&gt;[http://www.readwriteweb.com/archives/collaborative_filtering_social_web.php Collaborative Filtering: Lifeblood of The Social Web]&lt;/ref&gt;

One scenario of collaborative filtering application is to recommend interesting or popular information as judged by the community. As a typical example, stories appear in the front page of [[Reddit]] as they are "voted up" (rated positively) by the community. As the community becomes larger and more diverse, the promoted stories can better reflect the average interest of the community members.

Another aspect of collaborative filtering systems is the ability to generate more personalized recommendations by analyzing information from the past activity of a specific user, or the history of other users deemed to be of similar taste to a given user. These resources are used as user profiling and helps the site recommend content on a user-by-user basis. The more a given user makes use of the system, the better the recommendations become, as the system gains data to improve its model of that user.

===Problems===
A collaborative filtering system does not necessarily succeed in automatically matching content to one's preferences. Unless the platform achieves unusually good diversity and independence of opinions, one point of view will always dominate another in a particular community. As in the personalized recommendation scenario, the introduction of new users or new items can cause the [[cold start]] problem, as there will be insufficient data on these new entries for the collaborative filtering to work accurately. In order to make appropriate recommendations for a new user, the system must first learn the user's preferences by analysing past voting or rating activities. The collaborative filtering system requires a substantial number of users to rate a new item before that item can be recommended.

==Challenges==

===Data sparsity===
In practice, many commercial recommender systems are based on large datasets. As a result, the user-item matrix used for collaborative filtering could be extremely large and sparse, which brings about the challenges in the performances of the recommendation.

One typical problem caused by the data sparsity is the [[cold start]] problem. As collaborative filtering methods recommend items based on users' past preferences,  new users will need to rate sufficient number of items to enable the system to capture their preferences accurately and thus provides reliable recommendations.

Similarly,  new items also have the same problem. When new items are added to system, they need to be rated by substantial number of users before they could be recommended to users who have similar tastes with the ones rated them. The new item problem does not limit the [[Content-based filtering|content-based recommendation]], because the recommendation of an item is based on its discrete set of descriptive qualities rather than its ratings.

===Scalability===
As the numbers of users and items grow, traditional CF algorithms will suffer serious scalability problems{{Citation needed|date=April 2013}}. For example, with tens of millions of customers &lt;math&gt;O(M)&lt;/math&gt; and millions of items &lt;math&gt;O(N)&lt;/math&gt;, a CF algorithm with the complexity of &lt;math&gt;n&lt;/math&gt; is already too large. As well, many systems need to react immediately to online requirements and make recommendations for all users regardless of their purchases and ratings history, which demands a higher scalability of a CF system. Large web companies such as Twitter use clusters of machines to scale recommendations for their millions of users, with most computations happening in very large memory machines.&lt;ref name="twitterwtf"&gt;Pankaj Gupta, Ashish Goel, Jimmy Lin, Aneesh Sharma, Dong Wang, and Reza Bosagh Zadeh [http://dl.acm.org/citation.cfm?id=2488433 WTF: The who-to-follow system at Twitter], Proceedings of the 22nd international conference on World Wide Web&lt;/ref&gt;

===Synonyms===
[[Synonyms]] refers to the tendency of a number of the same or very similar items to have different names or entries. Most recommender systems are unable to discover this latent association and thus treat these products differently.

For example, the seemingly different items "children movie" and "children film" are actually referring to the same item. Indeed, the degree of variability in descriptive term usage is greater than commonly suspected.{{citation needed|date=September 2013}} The prevalence of synonyms decreases the recommendation performance of CF systems. Topic Modeling (like the [[Latent Dirichlet Allocation]] technique) could solve this by grouping different words belonging to the same topic.{{citation needed|date=September 2013}}

===Gray sheep===
Gray sheep refers to the users whose opinions do not consistently agree or disagree with any group of people and thus do not benefit from collaborative filtering. [[Black sheep]] are the opposite group whose idiosyncratic tastes make recommendations nearly impossible. Although this is a failure of the recommender system, non-electronic recommenders also have great problems in these cases, so black sheep is an acceptable failure.

===Shilling attacks===
In a recommendation system where everyone can give the ratings, people may give lots of positive ratings  for their own items and negative ratings for their competitors. It is often necessary for the collaborative filtering systems to introduce precautions to discourage such kind of manipulations.

===Diversity and the long tail===
Collaborative filters are expected to increase diversity because they help us discover new products. Some algorithms, however, may unintentionally do the opposite. Because collaborative filters recommend products based on past sales or ratings, they cannot usually recommend products with limited historical data. This can create a rich-get-richer effect for popular products, akin to [[positive feedback]]. This bias toward popularity can prevent what are otherwise better consumer-product matches. A [[Wharton School of the University of Pennsylvania|Wharton]] study details this phenomenon along with several ideas that may promote diversity and the "[[long tail]]."&lt;ref&gt;{{cite journal| last1= Fleder | first1= Daniel | first2= Kartik |last2= Hosanagar | title=Blockbuster Culture's Next Rise or Fall: The Impact of Recommender Systems on Sales Diversity|journal=Management Science |date=May 2009|url=http://papers.ssrn.com/sol3/papers.cfm?abstract_id=955984 | doi = 10.1287/mnsc.1080.0974 }}&lt;/ref&gt; Several collaborative filtering algorithms have been developed to promote diversity and the "[[long tail]]" by recommending novel, unexpected,&lt;ref&gt;{{cite journal| last1= Adamopoulos | first1= Panagiotis | first2= Alexander |last2= Tuzhilin | title=On Unexpectedness in Recommender Systems: Or How to Better Expect the Unexpected|journal=ACM Transactions on Intelligent Systems and Technology |date=January 2015|url=http://dl.acm.org/citation.cfm?id=2559952 | doi = 10.1145/2559952}}&lt;/ref&gt; and serendipitous items.&lt;ref&gt;{{cite journal| last1= Adamopoulos | first1= Panagiotis | title=Beyond rating prediction accuracy: on new perspectives in recommender systems|journal=Proceedings of the 7th ACM conference on Recommender systems |date=October 2013|url=http://dl.acm.org/citation.cfm?id=2508073| doi = 10.1145/2507157.2508073}}&lt;/ref&gt;

==Innovations==
{{Prose|date=May 2012}}
* New algorithms have been developed for CF as a result of the [[Netflix prize]].
* Cross-System Collaborative Filtering where user profiles across multiple [[recommender systems]] are combined in a privacy preserving manner.
* [[Robust collaborative filtering]], where recommendation is stable towards efforts of manipulation. This research area is still active and not completely solved.&lt;ref&gt;{{cite web|url=http://dl.acm.org/citation.cfm?id=1297240 |title=Robust collaborative filtering |doi=10.1145/1297231.1297240 |publisher=Portal.acm.org |date=19 October 2007 |accessdate=2012-05-15}}&lt;/ref&gt;

==See also==
{{div col|3}}
* [[Attention Profiling Mark-up Language|Attention Profiling Mark-up Language (APML)]]
* [[Cold start]]
* [[Collaborative model]]
* [[Collaborative search engine]]
* [[Collective intelligence]]
* [[Customer engagement]]
* [[Delegative Democracy]], the same principle applied to voting rather than filtering
* [[Enterprise bookmarking]]
* [[Firefly (website)]], a defunct website which was based on collaborative filtering
* [[Filter bubble]]
* [[Preference elicitation]]
* [[Recommendation system]]
* [[Relevance (information retrieval)]]
* [[Reputation system]]
* [[Robust collaborative filtering]]
* [[Similarity search]]
* [[Slope One]]
* [[Social translucence]]
{{div col end}}

==References==
{{Reflist|30em}}

==External links==
*[http://www.grouplens.org/papers/pdf/rec-sys-overview.pdf ''Beyond Recommender Systems: Helping People Help Each Other''], page 12, 2001
*[http://www.prem-melville.com/publications/recommender-systems-eml2010.pdf Recommender Systems.] Prem Melville and Vikas Sindhwani. In Encyclopedia of Machine Learning, Claude Sammut and Geoffrey Webb (Eds), Springer, 2010.
*[http://arxiv.org/abs/1203.4487 Recommender Systems in industrial contexts - PHD thesis (2012) including a comprehensive overview of many collaborative recommender systems]
*[http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1423975  Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions]{{dead link|date=June 2016|bot=medic}}{{cbignore|bot=medic}}. Adomavicius, G. and Tuzhilin, A. IEEE Transactions on Knowledge and Data Engineering 06.2005
*[https://web.archive.org/web/20060527214435/http://ectrl.itc.it/home/laboratory/meeting/download/p5-l_herlocker.pdf Evaluating collaborative filtering recommender systems] ([http://www.doi.org/ DOI]: [http://dx.doi.org/10.1145/963770.963772 10.1145/963770.963772])
*[http://www.grouplens.org/publications.html GroupLens research papers].
*[http://www.cs.utexas.edu/users/ml/papers/cbcf-aaai-02.pdf Content-Boosted Collaborative Filtering for Improved Recommendations.] Prem Melville, Raymond J. Mooney, and Ramadass Nagarajan. Proceedings of the Eighteenth National Conference on Artificial Intelligence (AAAI-2002), pp.&amp;nbsp;187–192, Edmonton, Canada, July 2002.
*[http://agents.media.mit.edu/projects.html A collection of past and present "information filtering" projects (including collaborative filtering) at MIT Media Lab]
*[http://www.ieor.berkeley.edu/~goldberg/pubs/eigentaste.pdf Eigentaste: A Constant Time Collaborative Filtering Algorithm. Ken Goldberg, Theresa Roeder, Dhruv Gupta, and Chris Perkins. Information Retrieval, 4(2), 133-151. July 2001.]
*[http://downloads.hindawi.com/journals/aai/2009/421425.pdf A Survey of Collaborative Filtering Techniques] Su, Xiaoyuan and Khoshgortaar, Taghi. M
*[http://dl.acm.org/citation.cfm?id=1242610 Google News Personalization: Scalable Online Collaborative Filtering] Abhinandan Das, Mayur Datar, Ashutosh Garg, and Shyam Rajaram. International World Wide Web Conference, Proceedings of the 16th international conference on World Wide Web
*[https://web.archive.org/web/20101023032716/http://research.yahoo.com/pub/2435 Factor in the Neighbors: Scalable and Accurate Collaborative Filtering] Yehuda Koren, Transactions on Knowledge Discovery from Data (TKDD) (2009)
*[http://webpages.uncc.edu/~asaric/ISMIS09.pdf Rating Prediction Using Collaborative Filtering]
*[http://www.cis.upenn.edu/~ungar/CF/ Recommender Systems]
*[http://www2.sims.berkeley.edu/resources/collab/ Berkeley Collaborative Filtering]

{{Authority control}}

{{DEFAULTSORT:Collaborative Filtering}}
[[Category:Collaboration]]
[[Category:Collaborative software]]
[[Category:Collective intelligence]]
[[Category:Information retrieval techniques]]
[[Category:Recommender systems]]
[[Category:Social information processing]]</text>
      <sha1>fdv7lym2htjpz0kqr5bcqk5d7au19pk</sha1>
    </revision>
  </page>
  <page>
    <title>Policy framework</title>
    <ns>0</ns>
    <id>21828505</id>
    <revision>
      <id>666920801</id>
      <parentid>666918345</parentid>
      <timestamp>2015-06-14T16:18:41Z</timestamp>
      <contributor>
        <username>AnomieBOT</username>
        <id>7611264</id>
      </contributor>
      <minor />
      <comment>Dating maintenance tags: {{Globalize}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4351" xml:space="preserve">{{refimprove|date=March 2009}}
{{globalize|date=June 2015}}
A '''policy framework''' is a logical structure that is established to organize policy documentation into groupings and categories that make it easier for employees to find and understand the contents of various [[policy]] documents. Policy frameworks can also be used to help in the planning and development of the policies for an organization.

==Principles==
[[State Services Commission]] of [[New Zealand]] outlines eleven principles of policy framework as below.&lt;ref&gt;http://www.ssc.govt.nz/Documents/policy_framework_for_Government_.htm&lt;/ref&gt;

===Availability===
Government departments should make information available easily, widely and equitably to the people of New Zealand (except where reasons preclude such availability as specified in legislation).....

===Coverage===
Government departments should make the following information increasingly available on an electronic basis:
* all published material or material already in the public domain
* all policies that could be released publicly
* all information created or collected on a statutory basis (subject to commercial sensitivity and privacy considerations)
* all documents that the public may be required to complete
* corporate documentation in which the public would be interested

===Pricing=== 
a) Free dissemination of Government-held information is appropriate where:
* dissemination to a target audience is desirable for a public policy purpose, or
* a charge to recover the cost of dissemination is not feasible or cost-effective

b) Pricing to recover the cost of dissemination is appropriate where:
* there is no particular public policy reason to disseminate the information, and 
* a charge to recover the cost of dissemination is both feasible and cost effective

c) Pricing to recover the cost of transformation is appropriate where:
* pricing to recover the cost of dissemination is appropriate, and
* there is an avoidable cost involved in transforming the information from the form in which it is held into a form preferred by the recipient, where it is feasible and cost-effective to recover in addition to the cost of dissemination

d) Pricing to recover the full costs of information production and dissemination is appropriate where:
* the information is created for the commercial purpose of sale at a profit, and 
* to do so would not breach the other pricing principles

===Ownership===
Government-held information, created or collected by any person employed or engaged by the Crown is a strategic resource 'owned' by the Government as a steward on behalf of the public.

===Stewardship===
Government departments are stewards of Government-held information, and it is their responsibility to implement good information management.

===Collection===
Government departments should only collect information for specified public policy, operational business or legislative purposes.

===Copyright===
Information created by departments is subject to Crown copyright but where wide dissemination is desirable, the Crown should permit use of its copyrights subject to acknowledgement of source.
 
===Preservation===
Government-held information should be preserved only where a public business need, legislative or policy requirement, or a historical or archival reason, exists.

===Quality===
The key qualities underpinning Government-held information include accuracy, relevancy, timeliness, consistency and collection without bias so that the information supports the purposes for which it is collected.

===Integrity===
The integrity of Government-held information will be achieved when:
* all guarantees and conditions surrounding the information are met
* the principles are clear and communicated
* any situation relating to Government-held information is handled openly and consistently
* those affected by changes to Government-held information are consulted on those changes
* those charged as independent guardians of the public interest  (e.g. the Ombudsman) have confidence in the ability of departments to manage the information well
* there are minimum exceptions to the principles.

===Privacy===
The principles of the Privacy Act 1993 apply.

==References==
{{reflist}}

{{DEFAULTSORT:Policy Framework}}
[[Category:Information retrieval techniques]]
[[Category:Government of New Zealand]]</text>
      <sha1>0vbv3fowr1ex45pvsfmr4mfvvbtx97p</sha1>
    </revision>
  </page>
  <page>
    <title>Hashtag</title>
    <ns>0</ns>
    <id>20819040</id>
    <revision>
      <id>762744498</id>
      <parentid>762260923</parentid>
      <timestamp>2017-01-30T14:08:54Z</timestamp>
      <contributor>
        <username>Elmeter</username>
        <id>23744986</id>
      </contributor>
      <comment>/* Adaptations */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="36769" xml:space="preserve">{{Use mdy dates|date=May 2016}}
{{Use American English|date=May 2016}}

[[File:Global Summit to End Sexual Violence in Conflict (14203190979).jpg|thumb|A sign suggesting the use of a #TimeToAct hashtag at a 2014 conference]]

A '''hashtag''' is a type of label or [[Tag (metadata)|metadata tag]] used on [[Social networking service|social network]] and [[microblogging]] services which makes it easier for users to find messages with a specific theme or content. Users create and use hashtags by placing the [[Number sign|hash character &lt;code&gt;#&lt;/code&gt;]] (also known as the number sign or pound sign) in front of a word or unspaced phrase, either in the main text of a message or at the end. Searching for that hashtag will yield each message that has been tagged with it. A hashtag archive is consequently collected into a single stream under the same hashtag.&lt;ref&gt;{{Cite journal|last=Chang|first=Hsia-Ching|last2=Iyer|first2=Hemalata|title=Trends in Twitter Hashtag Applications: Design Features for Value-Added Dimensions to Future Library Catalogues|url=http://muse.jhu.edu/article/485537|journal=Library Trends|volume=61|issue=1|pages=248–258|doi=10.1353/lib.2012.0024|issn=1559-0682}}&lt;/ref&gt; For example, on the [[photo sharing|photo-sharing]] service [[Instagram]], the hashtag ''#bluesky'' allows users to find all the posts that have been tagged using that hashtag.

Because of its widespread use, ''hashtag'' was added to the ''[[Oxford English Dictionary]]'' in June 2014.&lt;ref&gt;{{cite web |title='Hashtag' added to the OED – but # isn't a hash, pound, nor number sign|url=http://www.theregister.co.uk/2014/06/13/hashtag_added_to_the_oed/|work=The Register|date=June 13, 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web |title=New words notes June 2014|url=http://public.oed.com/the-oed-today/recent-updates-to-the-oed/june-2014-update/new-words-notes-june-2014/|work=Oxford English Dictionary|date=June 2014}}&lt;/ref&gt;  The term ''hashtag'' can also refer to the hash symbol itself when used in the context of a hashtag.&lt;ref&gt;{{cite web |title=Oxford English Dictionary – Hash|url=http://www.oed.com/view/Entry/389023#eid301493073|work=Oxford English Dictionary|date=June 2014}}&lt;/ref&gt;

== Origin and use ==
The [[Number sign|pound sign]] or [[Number sign|hash symbol]] was often used in [[information technology]] to highlight a special meaning. (It should be noted that the words "Pound Sign" in the UK refer specifically to currency "£" - extended ASCII character 156 - and not weight.) In 1970 for example, the number sign was used to denote ''immediate'' [[address mode]] in the assembly language of the [[PDP-11]]&lt;ref&gt;{{cite web|url=https://programmer209.wordpress.com/2011/08/03/the-pdp-11-assembly-language/ |title=PDP-11 assembly language |publisher=Programmer209.wordpress.com |date=August 3, 2011 |accessdate=August 25, 2014}}&lt;/ref&gt; when placed next to a symbol or a number. In 1978, [[Brian Kernighan]] and [[Dennis Ritchie]] used ''#'' in the [[C (programming language)|C programming language]] for special keywords that had to be processed first by the [[C preprocessor]].&lt;ref&gt;{{cite book|title=[[The C Programming Language]]|authors=B.W. Kernighan &amp; d. Ritchie|publisher=Prentice Hall|year=1978|pages=86 and 207|isbn=0-13-110163-3}}&lt;/ref&gt; Since before the invention of the hashtag, the pound sign has been called the "hash symbol" in some countries outside of North America.&lt;ref&gt;{{cite book|last1=Bourke|first1=Jane|title=Communication Technology Resource Book|date=2004|publisher=Ready-Ed Publications|pages=19|url=https://books.google.com/books?id=gPNBTmxzpIIC&amp;lpg=PA19&amp;dq=hash%20key%20telephone&amp;pg=PA19#v=onepage&amp;q=hash&amp;f=false|accessdate=November 7, 2014|isbn=978-1-86397-585-8}}&lt;/ref&gt;&lt;ref&gt;{{cite book|last1=Hargraves|first1=Orin|title=Mighty fine words and smashing expressions : making sense of transatlantic English|date=2003|publisher=Oxford Univ. Press|location=Oxford [u.a.]|isbn=978-0-19-515704-8|pages=33, 260|url=https://books.google.com/books?id=dUTdk93cq9UC&amp;lpg=PA260&amp;dq=hash%20telephone&amp;pg=PA260#v=onepage&amp;q=hash%20mark&amp;f=false}}&lt;/ref&gt;

The pound sign appeared and was used by people within [[Internet Relay Chat|IRC]] networks to label groups and topics.&lt;ref&gt;"Channel Scope". Section 2.2. RFC 2811&lt;/ref&gt; Channels or topics that are available across an entire IRC network are prefixed with a hash symbol # (as opposed to those local to a server, which use an [[ampersand]] '&amp;').&lt;ref&gt;{{cite IETF |title=Internet Relay Chat Protocol |rfc=1459 |sectionname=Channels |section=1.3 |page= |last1=Oikarinen |first1=Jarkko |authorlink1=Jarkko Oikarinen |last2=Reed |first2=Darren |authorlink2= |year=1993 |month=May |publisher=[[Internet Engineering Task Force|IETF]] |accessdate=June 3, 2014}}&lt;/ref&gt;

The use of the pound sign in IRC inspired&lt;ref&gt;{{cite web|url=http://www.cmu.edu/homepage/computing/2014/summer/originstory.shtml |title=#OriginStory|publisher=Carnegie Mellon University|date=August 29, 2014}}&lt;/ref&gt; [[Chris Messina (open source advocate)|Chris Messina]] to propose a similar system to be used on Twitter to tag topics of interest on the microblogging network.&lt;ref&gt;{{cite news | url=http://www.nytimes.com/2011/06/12/fashion/hashtags-a-new-way-for-tweets-cultural-studies.html?_r=1&amp;pagewanted=all | title=Twitter's Secret Handshake | work=The New York Times | date=June 10, 2011 | accessdate=July 26, 2011 | author=Parker, Ashley}}&lt;/ref&gt; He posted the first hashtag on Twitter:
{{quote |1=How do you feel about using # (pound) for groups. As in #barcamp [msg]? |author = Chris Messina |source = ("factoryjoe"), August 23, 2007&lt;ref&gt;{{cite web|url = https://twitter.com/factoryjoe/statuses/223115412|title = Twitter post|author = Chris Messina ("factoryjoe")|date = August 23, 2007&lt;!-- 3:25 PM--&gt;}}&lt;/ref&gt; |width  = 50% |align  = center }}
Messina’s suggestion to use the hashtag was not adopted by Twitter, but the practice took off after hashtags were widely used in tweets relating to the [[2007 San Diego forest fires]] in Southern California.&lt;ref&gt;[http://mashable.com/2013/10/08/what-is-hashtag/ What is hashtag?"], Mashable, 8 October 2013&lt;/ref&gt;&lt;ref&gt;https://factoryjoe.com/2007/10/22/twitter-hashtags-for-emergency-coordination-and-disaster-relief/&lt;/ref&gt;

According to Messina, he suggested use of the hashtag to make it easy for "lay" users to search for content and find specific relevant updates; they are for people who do not have the technological knowledge to navigate the site. Therefore, the hashtag “was created organically by Twitter users as a way to categorize messages." &lt;ref&gt;{{Cite journal|last=Scott|first=Kate|date=2015-05-01|title=The pragmatics of hashtags: Inference and conversational style on Twitter|url=http://www.sciencedirect.com/science/article/pii/S037821661500096X|journal=Journal of Pragmatics|volume=81|pages=8–20|doi=10.1016/j.pragma.2015.03.015}}&lt;/ref&gt;

Internationally, the hashtag became a practice of writing style for Twitter posts during the [[2009–2010 Iranian election protests]]; Twitter users inside and outside Iran used both English- and [[Persian language|Persian]]-language hashtags in communications during the events.&lt;ref&gt;{{cite news|title=The story of the hashtag began with Iranians|url=http://www.dw.de/حکایت-هشتگی-که-ایرانیان-آغاز-کردند/g-18012627|accessdate=March 12, 2015|publisher=Deutsche Welle Persian|date=2009}}&lt;/ref&gt;

The first published use of the term "hash tag" was in a blog post by Stowe Boyd, "Hash Tags = Twitter Groupings,"&lt;ref&gt;{{cite web|url=http://stoweboyd.com/post/39877198249/hash-tags-twitter-groupings |title=Stowe Boyd, Hash Tags = Twitter Groupings |publisher=Stoweboyd.com |date= |accessdate=September 19, 2013}}&lt;/ref&gt; on August 26, 2007, according to lexicographer [[Ben Zimmer]], chair of the American Dialect Society's New Words Committee.

Beginning July 2, 2009,&lt;ref&gt;{{cite web|title=Twitter Makes Hashtags More #Useful|url=http://techcrunch.com/2009/07/02/twitter-makes-hashtags-more-useful/|accessdate=December 27, 2015}}&lt;/ref&gt; Twitter began to hyperlink all hashtags in tweets to Twitter search results for the hashtagged word (and for the standard spelling of commonly misspelled words). In 2010, Twitter introduced "[[Twitter#Trending topics|Trending Topics]]" on the Twitter front page, displaying hashtags that are rapidly becoming popular. Twitter has an algorithm to tackle attempts to [[spamming|spam]] the trending list and ensure that hashtags trend naturally.&lt;ref&gt;{{cite web|url=http://www.allisayis.com/the-secret-of-twitters-trending-hashtags-with-insight-and-tips/ |title=The Secret of Twitter's Trending Hashtags With Insight and Tips |publisher=AllISayIs.com |date= |accessdate=December 3, 2014}}&lt;/ref&gt;

Although the hashtag started out most popularly on Twitter as the main social media platform for this use, the use has extended to other social media sites including Instagram, Facebook, Flickr, Tumblr, and Google+.&lt;ref&gt;{{Cite web|url=http://www.cnn.com/2013/06/12/tech/social-media/facebook-hashtags/index.html|title=Facebook finally gets #hashtags - CNN.com|last=Mashable|first=By Christina Warren|website=CNN|access-date=2016-05-16}}&lt;/ref&gt;

In China, microblogs [[Sina Weibo]] and [[Tencent Weibo]] use a double-hashtag #HashName# format, since the lack of spacing between [[Chinese characters]] necessitates a closing tag. In contrast, when using Chinese characters (and [[orthographies]] with similar spacing conventions) on [[Twitter]], users must insert spacing before and after the hashtagged element (e.g. '我 #爱 你' instead of '我#爱你')&lt;ref&gt;{{cite news|last1=Martin|first1=Rick|title=Twitter Rolls Out Hashtag Support for Japanese, Korean, Chinese, and Russian|url=https://www.techinasia.com/twitter-hashtag-languages/|accessdate=March 5, 2015|publisher=Tech in Asia|date=July 13, 2011}}&lt;/ref&gt; or insert a [[zero-width non-joiner]] character before and after the hashtagged element, to retain a linguistically natural appearance, such as '我#爱你'.&lt;ref&gt;{{cite news|last1=International services team|title=Right-to-left languages on Twitter|url=https://blog.twitter.com/2012/right-to-left-languages-on-twitter|accessdate=March 5, 2015|publisher=Twitter|date=April 5, 2012}}&lt;/ref&gt;

== Style ==
On microblogging or social networking sites, hashtags can be inserted anywhere within a sentence, either preceding it, following it as a [[postscript]], or being included as a word within the sentence (e.g. "It is #sunny today").

The quantity of hashtags used in a post or tweet is just as important as the types of hashtags used. It is currently considered acceptable to tag a post once when contributing to a specific conversation. Two hashtags are considered acceptable when adding a location to the conversation. Three hashtags are seen by some as the "absolute maximum", and any contribution exceeding this risks "raising the ire of the community."&lt;ref&gt;{{cite web|title=What is a (#) Hashtag?|url=http://www.hashtags.org/how-to/history/what-is-a-hashtag/|publisher=Hashtags.org|accessdate=February 22, 2014}}&lt;/ref&gt;

As well as frustrating other users, the misuse of hashtags can lead to account suspensions. Twitter warns that adding hashtags to unrelated tweets, or repeated use of the same hashtag without adding to a conversation, could cause an account to be filtered from search, or even suspended.&lt;ref&gt;{{cite web|title=The Twitter Rules|url=https://support.twitter.com/groups/56-policies-violations/topics/236-twitter-rules-policies/articles/18311-the-twitter-rules|publisher=Twitter, Inc.|accessdate=February 22, 2014}}&lt;/ref&gt;{{failed verification|date=August 2014}}

[[Jimmy Fallon]] and [[Justin Timberlake]] performed a sketch parodying the often misused and misunderstood usage of hashtags on ''[[Late Night with Jimmy Fallon]]'' in September 2013.&lt;ref&gt;{{cite web|author=The Tonight Show Starring Jimmy Fallon |url=https://www.youtube.com/watch?v=57dzaMaouXA |title="#Hashtag" with Jimmy Fallon &amp; Justin Timberlake (Late Night with Jimmy Fallon) |publisher=YouTube |date=September 24, 2013 |accessdate=August 25, 2014}}&lt;/ref&gt;

== Function ==

[[File:Seguir hashtags.png|upright=1.3|right|thumb|Search bar in the header of a social networking site, searching for most recent posts containing the hashtag #science]]

Hashtags are mostly used in unmoderated, ad hoc discussion forums; any combination of characters led by a hash symbol is a hashtag, and any hashtag, if promoted by enough individuals, can "trend" and attract more individual users to discussion. On Twitter, when a hashtag becomes extremely popular, it will appear in the "Trending Topics" area of a user's homepage. The trending topics can be organized by geographic area or by all of Twitter. Hashtags are neither registered nor controlled by any one user or group of users. They cannot be "retired" from public usage, meaning that any given hashtag can theoretically be used in perpetuity. They do not contain any set definitions, meaning that a single hashtag can be used for any number of purposes, as chosen by those who make use of them.

Hashtags intended for discussion of a particular event tend to use an obscure wording to avoid being caught up with generic conversations on similar subjects, such as a cake festival using #cakefestival rather than simply #cake. However, this can also make it difficult for topics to become "trending topics" because people often use different spelling or words to refer to the same topic.  In order for topics to trend, there has to be a consensus, whether silent or stated, that the hashtag refers to that specific topic.

Hashtags also function as beacons in order for users to find and "follow" (subscribe) or "list" (organize into public contact lists) other users of similar interest.

In recent years, broadcasters such as [[Channel 4]] have employed the hashtag during the airing of programmes such as [[First Dates]] and [[The Undateables]]. Research has shown that audience numbers go up when individuals can be interactive - by tweeting while viewing a programme on TV.

Hashtags can be used on the social network [[Instagram]], by posting a picture and hashtagging it with its subject. As an example, a photo of oneself and a friend posted to the social network can be hashtagged #bffl or #friends. Instagram has banned certain hashtags, some because they are too generic, such as #photography #iPhone #iphoneography, and therefore do not fulfill a purpose. They have also blocked hashtags that can be linked to illegal activities, such as drug use.&lt;ref&gt;{{cite web|url=http://www.bbc.co.uk/news/technology-24842750 |title=Instagram banned hashtags | date = November 7, 2013|publisher=[[BBC.co.uk]] |accessdate=November 25, 2013}}&lt;/ref&gt; The ban against certain hashtags has a consequential role in the way that particular [[subaltern]] communities are built and maintained on Instagram. Despite Instagram's content policies, users are finding creative ways of maintaining their practices and ultimately circumventing censorship.&lt;ref&gt;Olszanowski, M. (2014). "Feminist Self-Imaging and Instagram: Tactics of Circumventing Sensorship{{sic|hide=y}}". ''Visual Communication Quarterly,'' 21(1), 83-95. Retrieved February 8, 2015, from http://www.tandfonline.com/doi/abs/10.1080/15551393.2014.928154#.VNgGT7DF-7FF-7F&lt;/ref&gt;

Hashtags are also used informally to express context around a given message, with no intent to categorize the message for later searching, sharing, or other reasons. One of the functions of the hashtag is to serve as a reflexive meta-commentary, which contributes to the idea of how written communication in new media can be paralleled to how pragmatic methodology is applied to speech.&lt;ref&gt;{{Cite web|url=http://www.linguistics.fi/julkaisut/SKY2014/Wikstrom.pdf|title=#srynotfunny: Communicative Functions of Hashtags on Twitter|last=Wilkström|first=Peter|date=2014|work=SKY Journal of Linguistics|publisher=|access-date=May 15, 2016}}&lt;/ref&gt;

This can help express contextual cues or offer more depth to the information or message that appears with the hashtag. "My arms are getting darker by the minute. #toomuchfaketan". Another function of the hashtag can be used to express personal feelings and emotions. For example, with "It's Monday!! #excited #sarcasm" in which the adjectives are directly indicating the emotions of the speaker. It can also be used as a disclaimer of the information that the hashtag accompanies, as in, "BREAKING: US GDP growth is back! #kidding". In this case, the hashtag provides an essential piece of information in which the meaning of the utterance is changed entirely by the disclaimer hashtag. This may also be conveyed with #sarcasm, as in the previous example. Self-mockery is another informal function of the hashtag used by writers, as in this tweet: "Feeling great about myself till I met an old friend who now races at the Master's level. Yup, there's today's #lessoninhumility," where the informality of the hashtag provides commentary on the tweet itself.&lt;ref name=":0"&gt;{{Cite web|url=http://www.skase.sk/Volumes/JTL28/pdf_doc/05.pdf|title=The ‘hashtag’: A new word or a new rule?|last=Caleffi|first=Paola-Maria|date=|website=Skase Journal of Theoretical Linguistics|publisher=|access-date=}}&lt;/ref&gt;

== Other uses ==
The feature has been added to other, non-short-message-oriented services, such as the user comment systems on [[YouTube]] and [[Gawker Media]]. In the case of the latter, hashtags for blog comments and directly submitted comments are used to maintain a more constant rate of user activity even when paid employees are not logged into the website.&lt;ref&gt;{{cite web|url = http://gawker.com/5382267/anarchy-in-the-machine-welcome-to-gawkers-open-forums|title = Anarchy in the Machine: Welcome to Gawker's Open Forums|author = Gabriel Snyder|publisher = Gawker|date = October 15, 2009&lt;!-- 3:25 PM--&gt;}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url = http://www.niemanlab.org/2009/10/got-a-tip-gawker-media-opens-tag-pages-to-masses-expecting-chaos/|title = Got a #tip? Gawker Media opens tag pages to masses, expecting "chaos"|author = Zachary M. Seward|publisher = Nieman Journalism Lab|date = October 15, 2009 &lt;!-- 8 a.m. --&gt;}}&lt;/ref&gt; Real-time search aggregators such as the former [[Google Real-Time Search]] also support hashtags in syndicated posts, meaning that hashtags inserted into Twitter posts can be hyperlinked to incoming posts falling under that same hashtag; this has further enabled a view of the "river" of Twitter posts that can result from search terms or hashtags.{{citation needed|date=September 2014}}

== Uses ==

=== Broadcast media ===

The use of hashtags has extended to [[television]]{{nsmdns}}a concept that began rising in prominence in the early 2010s. Broadcasters may display a hashtag as an on-screen [[digital on-screen graphic|bug]], encouraging viewers to participate in a [[backchannel]] of discussion via social media prior to, during, or after the program. [[Television commercial]]s have sometimes contained hashtags for similar purposes.&lt;ref&gt;{{cite web|url = http://www.tvguide.com/News/New-TV-Screen-1032111.aspx|title = New to Your TV Screen: Twitter Hashtags|date = April 21, 2011&lt;!-- 3:25 PM--&gt;|author = Michael Schneider|publisher = TV Guide}}&lt;/ref&gt; Hashtag bugs appear on either corner of the screen, or they may appear at the end of an advertisement.&lt;ref&gt;{{cite web|url = http://mashable.com/2012/12/03/mcdonalds-tv-ad-twitter-hashtag/|title = McDonald's Releases First TV Ad With Twitter Hashtag|date = Dec 3, 2012|author = Todd Wasserman|publisher = Mashable}}&lt;/ref&gt;

While personalities associated with broadcasts, such as hosts and correspondents, also promote their corporate or personal Twitter usernames in order to receive mentions and replies to posts, usage of related or "branded" hashtags alongside Twitter usernames (e.g., [[The Ed Show|#edshow]] as well as [[Ed Schultz|@edshow]]) is increasingly encouraged as a microblogging style in order to "trend" the hashtag (and, hence, the discussion topic) in Twitter and other search engines. Broadcasters also make use of such a style in order to index select posts for live broadcast. Chloe Sladden, Twitter's director of media partnerships, identified two types of television-formatted usage of hashtags: hashtags which identify a series being broadcast (i.e. [[It's Always Sunny in Philadelphia|#SunnyFX]]) and instantaneous, "temporary" hashtags issued by television personalities to gauge topical responses from viewers during broadcasts.&lt;ref&gt;{{cite web|url = http://www.fastcompany.com/1747437/twitter-tv-hashtag-tips-twitters-own-expert|title = Twitter TV Hashtag Tips From Twitter's Own Expert|author = Gregory Ferenstein|date = April 15, 2011|publisher = Fast Company}}&lt;/ref&gt; Some have speculated that hashtags might take the place of (or co-exist with) the [[Nielsen ratings|Nielsen television ratings system]].&lt;ref&gt;{{cite web|url=http://www.ibtimes.com/twitter-chatter-correlates-tv-ratings-good-or-bad-news-nielsen-1144311 |title=Twitter Chatter Correlates With TV Ratings, But Is That Good Or Bad News For Nielsen? |work=International Business Times |date=March 22, 2013 |accessdate=September 19, 2013}}&lt;/ref&gt;

An example of trending "temporary" hashtags garnering viewers during broadcasts is observed on ''[[The Tonight Show]]'' with [[Jimmy Fallon]], a variety [[talk show]] on [[NBC]]. Every Wednesday, Fallon hosts a segment on his show called "Tonight Show Hashtags," which engages viewers by inviting them via Twitter to post humorous stories based on a specific hashtag topic, such as #WhydidIsaythat, #Worstfirstdate, to #Onetimeinclass, reflecting on funny experiences in daily life. By using hashtags, Fallon creates a sense of community and solidarity among his viewers and draws a wider range of viewers through an online platform while they watch a classic, non-interactive television program. Because of its popularity, the "Tonight Show Hashtags" are usually the 'most tweeted hashtag' on Twitter, which promotes the show. By engaging viewers with a lighthearted subject and simple hashtags, Fallon can gauge topical responses from viewers during broadcasts and also use the hashtags to brand his show.{{citation needed|date=April 2016}}

The increased usage of hashtags as brand promotion devices has been compared to the promotion of branded "[[Index term|keywords]]" by [[AOL]] in the late 1990s and early 2000s, as such keywords were also promoted at the end of television commercials and series episodes.&lt;ref&gt;{{cite web|url = http://techcrunch.com/2012/06/10/twitter-hashtag-pages-aol-keywords/|title = Twitter's Hashtag Pages Could Be The New AOL Keywords — But Better|author = Ryan Lawler|date = June 10, 2012|publisher = Techcrunch}}&lt;/ref&gt;

The late-night television comedy [[game show]] [[@midnight]] with [[Chris Hardwick]] on [[Comedy Central]] features a daily game entitled "Hashtag Wars," in which three comedians compete against one another to come up with phrases based on a given hashtag theme.

Some hashtags have become famous worldwide. For instance the slogan "''[[Je suis Charlie]],''" which was first used on Twitter as the hashtag #jesuischarlie and #iamcharlie to indicate solidarity with ''Charlie Hebdo'' offices attacked in Paris, spread to the internet at large.

=== Purchasing ===

Since February 2013 Twitter and [[American Express]] have collaborated to enable users to pay for discounted goods online by tweeting a special hashtag.&lt;ref&gt;{{cite news | first = Kelly | last = Heather | title = Twitter and Amex let you pay with a hashtag | date = February 12, 2013 | url = http://edition.cnn.com/2013/02/11/tech/social-media/twitter-hashtag-purchases/| work = CNN | accessdate = November 25, 2013}}&lt;/ref&gt; American Express members can sync their card with Twitter and pay for offers by tweeting; American Express tweets a response to the member that confirms the purchase.&lt;ref&gt;{{cite web|url=https://sync.americanexpress.com/Twitter/Index |title=Sync with Twitter|publisher=Amex Sync |accessdate=November 25, 2013}}&lt;/ref&gt;

=== Event promotion ===

[[File:Occupy for Rights.JPG|thumb|[[Stencil graffiti]] promoting the hashtag #OccupyForRights]]

Organized real-world events have used hashtags and ad hoc lists for discussion and promotion among participants. Hashtags are used as beacons by event participants in order to find each other, both on Twitter and, in many cases, during actual physical events.

Companies and advocacy organizations have taken advantage of hashtag-based discussions for promotion of their products, services or campaigns.

Political protests and campaigns in the early 2010s, such as [[Occupy Wall Street|#OccupyWallStreet]] and [[2011 Libyan civil war|#LibyaFeb17]], have been organized around hashtags or have made extensive usage of hashtags for the promotion of discussion.

=== Consumer complaints ===
Hashtags are often used by consumers on social media platforms in order to complain about the customer service experience with large companies.  The term "bashtag" has been created to describe situations in which a user refers to a corporate social media hashtag in order to criticise the company or to tell others about poor customer service. For example, in January 2012, [[McDonald's]] created the #McDStories hashtag so that customers could share positive experiences about the restaurant chain. But, the marketing effort was cancelled after two hours when McDonald's received numerous complaint tweets rather than the positive stories they were anticipating.&lt;ref&gt;{{cite news | first = Alexis | last = Akwagyiram | title = Are Twitter and Facebook changing the way we complain? | date = May 17, 2012 | url = http://www.bbc.co.uk/news/uk-18081651 | work = BBC News | accessdate = June 12, 2012}}&lt;/ref&gt;

=== Sentiment analysis ===
The use of hashtags also reveals what feelings or sentiment an author attaches to a statement. This can range from the obvious, where a hashtag directly describes the state of mind, to the less obvious. For example, words in hashtags are the strongest predictor of whether or not a statement is [[sarcasm|sarcastic]].&lt;ref&gt;{{cite journal|last=Maynard|title=Who cares about sarcastic tweets? Investigating the impact of sarcasm on sentiment analysis|journal=Proceedings of the Conference on Language Resources and Evaluation|year=2014}}&lt;/ref&gt;—a difficult [[Artificial Intelligence|AI]] problem.&lt;ref&gt;http://www.huffingtonpost.com/entry/power-yourself-with-viral-marketing-become-a-hashtag_us_57bf13e6e4b06384eb3e7f1d?dxrywr9zcw30rizfr&lt;/ref&gt;

=== Sports ===
The YouTuber Spencer FC used the hashtag for the name and crest of his YouTube-based association football team, [[Hashtag United]].

== In popular culture ==
During the [[2011 Canadian leaders debates|April 2011 Canadian party leader debate]], [[Jack Layton]], then-leader of the [[New Democratic Party of Canada|New Democratic Party]], referred to [[Conservative Party of Canada|Conservative]] Prime Minister [[Stephen Harper]]'s crime policies as "a hashtag fail" (presumably #fail).&lt;ref&gt;{{cite news|url = http://www.theglobeandmail.com/news/politics/jack-laytons-debatable-hashtag-fail/article576224/|title = Jack Layton's debatable 'hashtag' #fail|author = Anna Mehler Paperny|publisher = The Globe and Mail|date = April 13, 2011 &lt;!-- , 6:00 AM EDT --&gt; }}&lt;/ref&gt;&lt;ref&gt;{{cite news|url = http://www.cbc.ca/news/politics/canadavotes2011/story/2011/04/13/cv-debate-twitter.html|title = Canadians atwitter throughout debate|date = April 13, 2011&lt;!-- 3:25 PM--&gt;|publisher = CBC News}}&lt;/ref&gt;

The term "hashtag [[Hip hop music|rap]]", coined by [[Kanye West]],&lt;ref&gt;{{cite web |url = http://blogs.villagevoice.com/music/2010/11/the_ten_best_qu.php|title = The Ten Best Quotes From Kanye West's Epic Hot 97 Interview With Funkmaster Flex|author = Zach Baron|publisher = The Village Voice|date = November 3, 2010}}&lt;/ref&gt; was developed in the 2010s to describe a style of rapping which, according to Rizoh of the ''[[Houston Press]],'' uses "three main ingredients: a metaphor, a pause, and a one-word [[punch line]], often placed at the end of a rhyme".&lt;ref&gt;{{cite web|url = http://blogs.houstonpress.com/rocks/2011/07/a_brief_history_of_hashtag_rap.php|title = A Brief History Of Hashtag Rap|author = Rizoh|publisher = Houston Press|date = July 7, 2011 &lt;!-- at 9:00 AM --&gt; }}&lt;/ref&gt; Rappers [[Nicki Minaj]], [[Big Sean]], [[Drake (rapper)|Drake]], and [[Lil Wayne]] are credited with the popularization of hashtag rap, while the style has been criticized by [[Ludacris]], [[The Lonely Island]],&lt;ref&gt;{{cite web|url = http://www.tucsonweekly.com/TheRange/archives/2013/05/22/the-lonely-island-puts-hashtag-rap-in-its-place-looking-at-you-drake|title = The Lonely Island Puts Hashtag Rap In Its Place (Looking at You, Drake)|author = David Mendez|date = May 22, 2013 &lt;!-- AT 11:43 AM --&gt; |publisher = Tucson Weekly}}&lt;/ref&gt; and various music writers.&lt;ref&gt;{{cite web|url = http://www.joplinglobe.com/enjoy/x1666506743/Jeremiah-Tucker-Hashtag-rap-is-2010s-lamest-trend|title = Jeremiah Tucker: Hashtag rap is 2010's lamest trend|author = Jeremiah Tucker|date = December 17, 2010|publisher = Joplin Globe}}&lt;/ref&gt;

On September 13, 2013, a hashtag, #TwitterIPO, appeared in the headline of a ''[[The New York Times|New York Times]]'' front page article regarding Twitter's [[initial public offering]].&lt;ref&gt;{{cite web
| title = Twitter / nickbilton: My first byline on A1 of the …
| url = https://twitter.com/nickbilton/status/378534272962793472/photo/1
| accessdate = September 14, 2013
 }}&lt;/ref&gt;

[[Bird's Eye]] foods released in 2014 a shaped [[mashed potato]] food that included forms of @-symbols and hashtags, called "Mashtags".&lt;ref&gt;{{cite web|title=Birds Eye launches Mashtags – social media potato shapes|url=http://www.thegrocer.co.uk/fmcg/birds-eye-launches-mashtags-potato-shapes/354514.article|work=The Grocer}}&lt;/ref&gt;

In May 2014, Twitter users began using the hashtag [[YesAllWomen|#YesAllWomen]] to raise awareness about personal experiences of [[sexism]] and [[violence against women]].&lt;ref name="Nytimes"&gt;{{cite news |last=Medina| first=Jennifer | title = Campus Killings Set Off Anguished Conversation About the Treatment of Women | work = [[The New York Times]] | accessdate = September 23, 2014 | date = May 27, 2014 | url =http://www.nytimes.com/2014/05/27/us/campus-killings-set-off-anguished-conversation-about-the-treatment-of-women.html?ref=us&amp;_r=0 }}&lt;/ref&gt;

In September 2014, in response to the "[[blame the victim]]" public reactions to videotaped footage of [[NFL]] player [[Ray Rice]] assaulting his then-fiancée Janay Palmer in the elevator of an [[Atlantic City]] casino, Beverly Gooden shared on Twitter her own story of [[domestic abuse]], using the hashtag #WhyIStayed, and encouraged others to share theirs.&lt;ref&gt;{{cite news|work=Today|title=WhyIStayed: Woman behind Ray Rice-inspired hashtag writes to past self, other abuse victims|author=Gooden, Beverly|date=September 10, 2014| url= http://www.today.com/news/whyistayed-woman-behind-ray-rice-inspired-hashtag-writes-past-self-1D80139011}}&lt;/ref&gt;&lt;ref&gt;{{cite news|work=The Leonard Lopate Show|authors=Lopate, Leonard &amp; Gooden, Beverly|title=#WhyIStayed|date=September 10, 2014}}&lt;/ref&gt;

Hashtags have been used verbally to make a humorous point in informal conversations,&lt;ref&gt;[http://www.macmillandictionary.com/dictionary/british/hashtag]&lt;/ref&gt; such as "I’m hashtag confused!"&lt;ref name=":0" /&gt; In August 2012, British journalist Tom Meltzer reported in ''[[The Guardian]]'' about a new [[hand gesture]] that mimicked the hashtag, sometimes called the "finger hashtag", in which both hands form a [[Peace sign#The V sign|peace sign]], and then the fingers are crossed to form the symbol of a hashtag.&lt;ref&gt;{{cite web |url=https://www.theguardian.com/technology/shortcuts/2012/aug/01/how-to-say-hashtag-fingers |title=How to say 'hashtag' with your fingers |work=[[The Guardian]] |author=Tom Meltzer |date=August 1, 2012 |accessdate=March 20, 2014}}&lt;/ref&gt; The emerging gesture was reported about in ''[[Wired (magazine)|Wired]]'' by [[Nimrod Kamer]],&lt;ref&gt;{{cite web |url=http://www.wired.co.uk/news/archive/2013-03/06/hashtags |title=Finger-Hashtags |work=[[Wired (magazine)|Wired]] |author=[[Nimrod Kamer]] |date=March 2013 |accessdate=March 20, 2014}}&lt;/ref&gt; and during 2013, it was seen on TV as used by [[Jimmy Fallon]], and on ''[[The Colbert Report]],'' among other programs.&lt;ref&gt;{{cite web |url=http://www.dailydot.com/lol/finger-hashtag-jimmy-fallon-twitter/ |title=I invented finger hashtags—and I regret nothing |work=[[The Daily Dot]] |author=[[Nimrod Kamer]] |date=February 26, 2014 |accessdate=March 20, 2014}}&lt;/ref&gt; Writing in 2015, Paola Maria Caleff considered this usage a [[fad]], but noted that people talking the way that they write was a consequence of computer-mediated communication.&lt;ref name=":0" /&gt;

=== Adaptations ===
*Hashflags: In 2010, Twitter introduced "hashflags" during the 2010 World Cup in South Africa.&lt;ref&gt;{{cite web|author=|url=http://www.ryanseacrest.com/2010/06/11/twitter-supports-world-cup-fever-with-hashflags/ |title=Twitter Supports World Cup Fever with Hashflags |publisher=Ryanseacrest.com |date=June 11, 2010 |accessdate=August 5, 2015 |archiveurl=https://web.archive.org/web/20101129201517/http://ryanseacrest.com/2010/06/11/twitter-supports-world-cup-fever-with-hashflags/ |archivedate=November 29, 2010}}&lt;/ref&gt; They reintroduced the feature on June 10, 2014, in time for the 2014 World Cup in Brazil,&lt;ref&gt;{{cite web|url=http://howto.digidefen.se/twitter/What-are-hashflags.php |title=What are Hashflags? |publisher=Howto.digidefen.se |date=June 10, 2014 |accessdate=August 25, 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web|author=Ben Woods |url=http://thenextweb.com/twitter/2014/06/10/twitter-brings-back-hashflags-just-time-world-cup-2014-kick/ |title=Twitter brings back hashflags just in time for World Cup 2014 kick-off |publisher=Thenextweb.com |date=June 10, 2014 |accessdate=August 25, 2014}}&lt;/ref&gt; and then again on April 10, 2015, with UK political party logos for the 2015 UK General Election.&lt;ref&gt;{{cite web|title=Twitter just launched election hashflags|url=http://www.bbc.co.uk/newsbeat/article/32249518/twitter-just-launched-election-hashflags|website=BBC News|accessdate=April 15, 2015}}&lt;/ref&gt; When a user tweets a hashtag consisting of the three letter country code of any of the 32 countries represented in the tournament, Twitter automatically embeds a flag emoticon for that country.
*Cashtags: In 2009, [[StockTwits]] used [[ticker symbol]]s preceded by the [[dollar sign]] to create "cashtags".&lt;ref name=Wong2012&gt;{{cite journal |author=Wong, Matthew |title=VCs and Start-Ups Pin Their Hopes on Pinterest |date=2012-08-17 |work=[[The Wall Street Journal]] |url=http://blogs.wsj.com/venturecapital/2012/08/17/vcs-and-start-ups-pin-their-hopes-on-pinterest/ |accessdate=2013-05-28 }}&lt;/ref&gt;&lt;ref name=Taylor2012&gt;{{cite journal |author=Taylor, Colleen |title=Howard Lindzon on Why He Sold His Twitter Stock, And The 'Hijack' Of StockTwits’ Cashtags [TCTV] |date=2012-07-01 |publisher=[[TechCrunch]] |url=http://techcrunch.com/2012/08/01/howard-lindzon-on-why-he-sold-his-twitter-stock-and-the-hijack-of-stocktwits-cashtags-tctv/ |accessdate=2013-05-09 }}&lt;/ref&gt; In July 2012, Twitter adapted the hashtag style to make company ticker symbols preceded by the dollar sign clickable (as in [[Apple, Inc.|$AAPL]]), a method that Twitter dubbed the "cashtag".&lt;ref&gt;{{cite web|last=Kim |first=Erin |url=http://money.cnn.com/2012/07/31/technology/twitter-cashtag/ |title=Twitter unveils 'cashtags' to track stock symbols – Jul. 31, 2012 |publisher=Money.cnn.com |date=July 31, 2012 |accessdate=November 12, 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web|author= |url=http://www.theverge.com/2012/7/30/3205284/twitter-stock-ticker-cashtag-links-official |title=Twitter makes stock symbol $ 'cashtag' links official, following # and @ |publisher=The Verge |date=July 30, 2012 |accessdate=November 12, 2013}}&lt;/ref&gt; This is intended to allow users to search posts discussing companies and their stocks. This is also used for discussion of currency fluctuations on twitter, eg. using #USDGBP or $USDGBP when mentioning the US Dollar's level expressed in Pounds Sterling.

== References ==

{{Reflist|30em}}

== External links ==

{{Commons category|Hashtags}}

* [//tools.wmflabs.org/hashtags/search/artandfeminism Wikipedia internal hashtag search engine] – for hashtags used in edit summaries

{{Microblogging}}
{{Online social networking}}
{{Web syndication}}
{{authority control}}

[[Category:Hashtags| ]]
[[Category:2010s slang]]
[[Category:Collective intelligence]]
[[Category:Computer jargon]]
[[Category:Information retrieval techniques]]
[[Category:Knowledge representation]]
[[Category:Metadata]]
[[Category:Reference]]
[[Category:Social media]]
[[Category:Web 2.0]]
[[Category:Twitter]]</text>
      <sha1>hm0hmbz5abqu8kstjfsq4z7un4twp2k</sha1>
    </revision>
  </page>
  <page>
    <title>Subject indexing</title>
    <ns>0</ns>
    <id>13200719</id>
    <revision>
      <id>762988353</id>
      <parentid>733746281</parentid>
      <timestamp>2017-01-31T20:13:03Z</timestamp>
      <contributor>
        <ip>50.53.1.33</ip>
      </contributor>
      <comment>unneeded pipe</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="17072" xml:space="preserve">'''Subject indexing''' is the act of describing or [[document classification|classifying]] a [[document]] by [[index term]]s or other symbols in order to indicate what the document is '''[[aboutness|about]],''' to summarize its [[content (media and publishing)|content]] or to increase its [[findability]]. In other words, it is about identifying and describing the '''[[Subject (documents)|subject]]''' of documents. Indexes are constructed, separately, on three distinct levels: terms in a document such as a book; objects in a collection such as a library; and documents (such as books and articles) within a field of knowledge.

Subject indexing is used in [[information retrieval]] especially to create [[bibliographic index]]es to retrieve documents on a particular subject. Examples of academic indexing services are [[Zentralblatt MATH]], [[Chemical Abstracts]] and [[PubMed]]. The index terms were mostly assigned by experts but author keywords are also common.

The process of indexing begins with any analysis of the subject of the document. The indexer must then identify terms which appropriately identify the subject either by extracting words directly from the document or assigning words from a [[controlled vocabulary]].&lt;ref name="Lancaster2003a"&gt;F. W. Lancaster (2003): "Indexing and abstracting in theory and practise". Third edition. London, Facet ISBN 1-85604-482-3. page 6&lt;/ref&gt; The terms in the index are then presented in a systematic order.

Indexers must decide how many terms to include and how specific the terms should be. Together this gives a depth of indexing.

== Subject analysis ==
The first step in indexing is to decide on the subject matter of the document. In manual indexing, the indexer would consider the subject matter in terms of answer to a set of questions such as "Does the document deal with a specific product, condition or phenomenon?".&lt;ref name="Chowdhury2004"&gt;G.G. Chowdhury (2004): "Introduction to modern information retrieval". Third Edition. London, Facet. ISBN 1-85604-480-7. page 71&lt;/ref&gt; As the analysis is influenced by the knowledge and experience of the indexer, it follows that two indexers may analyze the content differently and so come up with different index terms. This will impact on the success of retrieval.

=== Automatic vs. manual subject analysis ===
Automatic indexing follows set processes of analyzing frequencies of word patterns and comparing results to other documents in order to assign to subject categories. This requires no understanding of the material being indexed. This therefore leads to more uniform indexing but this is at the expense of the true meaning being interpreted. A computer program will not understand the meaning of statements and may therefore fail to assign some relevant terms or assign incorrectly. Human indexers focus their attention on certain parts of the document such as the title, abstract, summary and conclusions, as analyzing the full text in depth is costly and time consuming &lt;ref name="Lancaster2003b"&gt;F. W. Lancaster (2003): "Indexing and abstracting in theory and practice". Third edition. London, Facet ISBN 1-85604-482-3. page 24&lt;/ref&gt; An automated system takes away the time limit and allows the entire document to be analyzed, but also has the option to be directed to particular parts of the document.

== Term selection ==
The second stage of indexing involves the translation of the subject analysis into a set of [[keyword (search)|index terms]]. This can involve extracting from the document or assigning from a [[controlled vocabulary]]. With the ability to conduct a [[full text search]] widely available, many people have come to rely on their own expertise in conducting information searches and [[full text search]] has become very popular. Subject indexing and its experts, professional indexers, [[catalogers]], and [[librarians]], remains crucial to information organization and retrieval. These experts understand [[controlled vocabularies]] and are able to find information that cannot be located by [[full text search]]. The cost of expert analysis to create subject indexing is not easily compared to the cost of hardware, software and labor to manufacture a comparable set of full-text, fully searchable materials. With new web applications that allow every user to annotate documents, [[social tagging]] has gained popularity especially in the Web.&lt;ref name="Voss2007"&gt;{{cite conference
|last1=Voss |first1=Jakob
|title=Tagging, Folksonomy &amp; Co - Renaissance of Manual Indexing?
|booktitle=Proceedings of the International Symposium of Information Science
|pages=234–254
|year=2007
|arxiv=cs/0701072
}}&lt;/ref&gt;

One application of indexing, the [[Index (publishing)|book index]], remains relatively unchanged despite the information revolution.

=== Extraction/Derived indexing ===
Extraction indexing involves taking words directly from the document. It uses [[natural language]] and lends itself well to automated techniques where word frequencies are calculated and those with a frequency over a pre-determined threshold are used as index terms. A stop-list containing common words (such as "the", "and") would be referred to and such [[stop words]] would be excluded as index terms.

Automated extraction indexing may lead to loss of meaning of terms by indexing single words as opposed to phrases. Although it is possible to extract commonly occurring phrases, it becomes more difficult if key concepts are inconsistently worded in phrases. Automated extraction indexing also has the problem that, even with use of a stop-list to remove common words, some frequent words may not be useful for allowing discrimination between documents. For example, the term glucose is likely to occur frequently in any document related to diabetes. Therefore use of this term would likely return most or all the documents in the database. Post-co-ordinated indexing where terms are combined at the time of searching would reduce this effect but the onus would be on the searcher to link appropriate terms as opposed to the information professional. In addition terms that occur infrequently may be highly significant for example a new drug may be mentioned infrequently but the novelty of the subject makes any reference significant. One method for allowing rarer terms to be included and common words to be excluded by automated techniques would be a relative frequency approach where frequency of a word in a document is compared to frequency in the database as a whole. Therefore a term that occurs more often in a document than might be expected based on the rest of the database could then be used as an index term, and terms that occur equally frequently throughout will be excluded.
Another problem with automated extraction is that it does not recognise when a concept is discussed but is not identified in the text by an indexable keyword.&lt;ref name="Lamb2008"&gt;J. Lamb (2008): ''[http://www.indexers.org.uk/index.php?id=463 Human or computer produced indexes?]'' [online] Sheffield, Society of Indexers. Accessed 15 January 2009.&lt;/ref&gt;

=== Assignment indexing ===
An alternative is assignment indexing where index terms are taken from a controlled vocabulary. This has the advantage of controlling for [[synonym]]s as the preferred term is indexed and synonyms or related terms direct the user to the preferred term. This means the user can find articles regardless of the specific term used by the author and saves the user from having to know and check all possible synonyms.&lt;ref name="Tenopir"&gt;C. Tenopir (1999): "Human or automated, indexing is important". ''Library Journal'' '''124'''(18) pages 34-38.&lt;/ref&gt; It also removes any confusion caused by [[homograph]]s by inclusion of a qualifying term. A third advantage is that it allows the linking of related terms whether they are linked by hierarchy or association, e.g. an index entry for an oral medication may list other oral medications as related terms on the same level of the hierarchy but would also link to broader terms such as treatment. Assignment indexing is used in manual indexing to improve inter-indexer consistency as different indexers will have a controlled set of terms to choose from. Controlled vocabularies do not completely remove inconsistencies as two indexers may still interpret the subject differently.&lt;ref name="Chowdhury2004" /&gt;

== Index presentation ==
The final phase of indexing is to present the entries in a systematic order. This may involve linking entries. In a pre-coordinated index the indexer determines the order in which terms are linked in an entry by considering how a user may formulate their search. In a post-coordinated index, the entries are presented singly and the user can link the entries through searches, most commonly carried out by computer software. Post-coordination results in a loss of precision in comparison to pre-coordination &lt;ref name="Bodoff1998"&gt;D. Bodoff and A. Kambil, (1998): "Partial coordination. I. The best of pre-coordination and post-coordination." ''Journal of the American Society for Information Science'', '''49'''(14), 1254-1269.&lt;/ref&gt;

== Depth of Indexing ==
Indexers must make decisions about what entries should be included and how many entries an index should incorporate. The depth of indexing describes the thoroughness of the indexing process with reference to exhaustivity and specificity &lt;ref name="Cleveland2001"&gt;D.B. Cleveland and A.D. Cleveland (2001): "Introduction to indexing and abstracting". 3rd Ed. Englewood, libraries Unlimited, Inc. ISBN 1-56308-641-7. page 105&lt;/ref&gt;

=== Exhaustivity ===
An exhaustive index is one which lists all possible index terms. Greater exhaustivity gives a higher [[Recall (information retrieval)|recall]], or more likelihood of all the relevant articles being retrieved, however, this occurs at the expense of [[Precision (information retrieval)|precision]]. This means that the user may retrieve a larger number of irrelevant documents or documents which only deal with the subject in little depth. In a manual system a greater level of exhaustivity brings with it a greater cost as more man hours are required. The additional time taken in an automated system would be much less significant. At the other end of the scale, in a selective index only the most important aspects are covered.&lt;ref name="Weinberg1999"&gt;B.H. Weinberg (1990): "Exhaustivity of indexes: Books, journals, and electronic full texts; Summary of a workshop presented at the 1999 ASI Annual Conference". ''Key Words'', '''7'''(5), pages 1+.&lt;/ref&gt; Recall is reduced in a selective index as if an indexer does not include enough terms, a highly relevant article may be overlooked. Therefore indexers should strive for a balance and consider what the document may be used. They may also have to consider the implications of time and expense.

=== Specificity ===
The specificity describes how closely the index terms match the topics they represent &lt;ref name="Anderson1997"&gt;J.D. Anderson (1997): ''[http://www.niso.org/publications/tr/ Guidelines for indexes and related information retrieval devices]'' [online]. Bethesda, Maryland, Niso Press. 10 December 2008.&lt;/ref&gt; An index is said to be specific if the indexer uses parallel descriptors to the concept of the document and reflects the concepts precisely.&lt;ref name="Cleveland2001b"&gt;D.B. Cleveland and A.D. Cleveland (2001): "Introduction to indexing and abstracting". 3rd Ed. Englewood, libraries Unlimited, Inc. ISBN 1-56308-641-7. page 106&lt;/ref&gt; Specificity tends to increase with exhaustivity as the more terms you include, the narrower those terms will be.

==Indexing theory==
[[Birger Hjørland|Hjørland]] (2011)&lt;ref&gt;Hjørland, Birger (2011). The Importance of Theories of Knowledge: Indexing and Information retrieval as an example. ''Journal of the American Society for Information Science and Technology'', 62(1,), 72-77.&lt;/ref&gt; found that theories of indexing is at the deepest level connected to different theories of knowledge:

'''Rationalist theories of indexing''' (such as Ranganathan's theory) suggest that subjects are constructed logically from a fundamental set of categories. The basic method of subject analysis is then "analytic-synthetic", to isolate a set of basic categories (=analysis) and then to construct the subject of any given document by combining those categories according to some rules (=synthesis). '''Empiricist theories of indexing''' are based on selecting similar documents based on their properties, in particular by applying numerical statistical techniques. '''Historicist and hermeneutical theories of indexing''' suggest that the subject of a given document is relative to a given discourse or domain, why the indexing should reflect the need of a particular discourse or domain. According to hermeneutics is a document always written and interpreted from particular horizon. The same is the case with systems of knowledge organization and with all users searching such systems. Any question put to such a system is put from a particular horizon. All those horizons may be more or less in consensus or in conflict. To index a document is to try to contribute to the retrieval of “relevant” documents by knowing about those different horizons. '''Pragmatic and critical theories of indexing''' (such as Hjørland, 1997)&lt;ref&gt;Hjørland, B. (1997). Information Seeking and Subject Representation. An Activity-theoretical approach to Information Science. Westport &amp; London: Greenwood Press.&lt;/ref&gt; is in agreement with the historicist point of view that subjects are relative to specific discourses but emphasizes that subject analysis should support given goals and values and should consider the consequences of indexing one way or another. These theories believe that indexing cannot be neutral and that it is a wrong goal to try to index in a neutral way. Indexing is an act (and computer based indexing is acting according to the programmers intentions). Acts serve human goals. Libraries and information services also serve human goals, why their indexing should be done in a way that supports these goals as much as possible. At a first glance this looks strange because the goals of libraries and information services is to identify any document or piece of information. Nonetheless is any specific way of indexing always supporting some kind of uses at the expense of other. The documents to be indexed intend to serve some specific purposes in a community. Basically the indexing should intend serving the same purposes. Primary and secondary documents and information services are parts of the same overall social system. In such a system different theories, epistemologies, worldviews etc. may be at play and users need to be able to orient themselves and to navigate among those different views. This calls for a mapping of the different epistemologies in the field and classification of the single document into such a map. Excellent examples of such different paradigms and their consequences for indexing and classification systems are provided in the domain of art by Ørom (2003)&lt;ref&gt;Ørom, Anders (2003). Knowledge Organization in the domain of Art Studies - History, Transition and Conceptual Changes. Knowledge Organization. 30(3/4), 128-143.&lt;/ref&gt; and in music by Abrahamsen (2003).&lt;ref&gt;Abrahamsen, Knut T. (2003). Indexing of Musical Genres. An Epistemological Perspective. Knowledge Organization, 30(3/4), 144-169.&lt;/ref&gt;

The core of indexing is, as stated by Rowley &amp; Farrow&lt;ref name=rowley2000&gt;Rowley, J. E. &amp; Farrow, J. (2000). Organizing Knowledge: An Introduction to Managing Access to Information. 3rd. Alderstot: Gower Publishing Company&lt;/ref&gt; to evaluate a papers contribution to knowledge and index it accordingly. Or, with the words of Hjørland (1992,&lt;ref&gt;Hjørland, Birger (1992). The Concept of "Subject" in Information Science. Journal of Documentation. 48(2), 172-200. http://iva.dk/bh/Core%20Concepts%20in%20LIS/1992JDOC%5FSubject.PDF&lt;/ref&gt; 1997) to index its informative potentials.

"In order to achieve good consistent indexing, the indexer must have a thorough appreciation of the structure of the subject and the nature of the contribution that the document is making to the advancement of knowledge." (Rowley &amp; Farrow, 2000,&lt;ref name=rowley2000/&gt; p.&amp;nbsp;99).

== See also ==
{{Commons category|Subject indexing}}
* [[Indexing and abstracting service]]
* [[Document classification]]
* [[Metadata]]
* [[Overcategorization]]
* [[Thomas of Ireland]], a medieval pioneer in subject indexing

== References ==
{{reflist}}
* {{cite book|author=Fugman, Robert|year=1993|title=Subject analysis and indexing. Theoretical foundation and practical advice|place=Frankfurt/Main|publisher=Index Verlag}}
* {{cite journal|author=Frohmann, B.|year=1990|title=Rules of Indexing: A Critique of [[Mentalism]] in Information Retrieval Theory|journal=Journal of Documentation|volume=46|issue=2|pages=81–101|doi=10.1108/eb026855}}

[[Category:Index (publishing)]]
[[Category:Information science]]
[[Category:Information retrieval techniques]]</text>
      <sha1>41r106836zl90cvp35a7ju4coawnslt</sha1>
    </revision>
  </page>
  <page>
    <title>Probabilistic relevance model</title>
    <ns>0</ns>
    <id>25959000</id>
    <revision>
      <id>714750470</id>
      <parentid>711593428</parentid>
      <timestamp>2016-04-11T16:51:45Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>move to Category:Information retrieval techniques since this is apparently helpful to deriving the algorithm, not evaluating the performance of an algorithm?</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2366" xml:space="preserve">
The '''probabilistic relevance model'''&lt;ref&gt;{{citation | author=S. E. Robertson and  K. S. Jones | title=Relevance weighting of search terms | publisher=Journal of the American Society for Information Science | pages=129–146 | date=May–June 1976 | url=http://portal.acm.org/citation.cfm?id=106783 }}&lt;/ref&gt;&lt;ref name="robertson2009"&gt;{{Cite journal | author=Stephen Robertson and Hugo Zaragoza | title=The Probabilistic Relevance Framework: BM25 and Beyond | date=2009 | url=http://dl.acm.org/citation.cfm?id=1704810 | publisher=Found. Trends Inf. Retr. | volume=3 | issue=4 | pages=333-389 | doi=10.1561/1500000019 }}&lt;/ref&gt; was devised by Robertson and Jones as a framework for [[Statistical model | probabilistic models]] to come. It is a formalism of [[information retrieval]] useful to derive [[ranking function]]s used by [[search engine]]s and  [[web search engine]]s in order to rank matching documents according to their [[Relevance (information retrieval)|relevance]] to a given search query.
 
It makes an estimation of the probability of finding if a document ''d&lt;sub&gt;j&lt;/sub&gt;'' is relevant to a query ''q''. This model assumes that this probability of relevance depends on the query and document representations. Furthermore, it assumes that there is a portion of all documents that is preferred by the user as the answer set for query ''q''. Such an ideal answer set is called ''R'' and should maximize the overall probability of relevance to that user. The prediction is that documents in this set ''R'' are relevant to the query, while documents not present in the set are non-relevant.

&lt;math&gt;sim(d_{j},q) = \frac{P(R|\vec{d}_j)}{P(\bar{R}|\vec{d}_j)}&lt;/math&gt;

==Related models==
There are some limitations to this framework that need to be addressed by further development:
* There is no accurate estimate for the first run probabilities
* Index terms are not weighted
* Terms are assumed mutually independent

To address these and other concerns there are some developed models from the probabilistic relevance framework. The [[Binary Independence Model]] for one, as it is from the same author. The most known derivative of this framework is the [[Probabilistic relevance model (BM25)|Okapi(BM25)]] weighting scheme and its BM25F brother.

==References==
{{reflist}}

[[Category:Information retrieval techniques]]
[[Category:Probabilistic models]]</text>
      <sha1>0y38brzfanbc2ha3sikzt2f15ee8gsu</sha1>
    </revision>
  </page>
  <page>
    <title>Voice search</title>
    <ns>0</ns>
    <id>13667706</id>
    <revision>
      <id>749657486</id>
      <parentid>681513453</parentid>
      <timestamp>2016-11-15T15:11:58Z</timestamp>
      <contributor>
        <username>HB2016</username>
        <id>29664607</id>
      </contributor>
      <minor />
      <comment>Added Siri</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1501" xml:space="preserve">'''Voice search''', also called voice-enabled search, allows the user to use a voice command to search the Internet, or a portable device.  Currently, voice search is commonly used in (in a narrow sense) "directory assistance", or local search. Examples include [[Google 411]], [[Tellme]] directory assistance and [[Yellowpages.com]]'s 1-800-YellowPages. 

In a broader definition, voice search include open-domain keyword query on any information on the Internet, for example in [[Google Voice Search]], [[Cortana (software)|Cortana]], [[Siri]] and [[Amazon Echo]]. Given that voice-based systems are interactive, such systems are also called open-domain [[question answering]] systems. 

Voice search is often interactive, involving several rounds of interaction that allows a system to ask for clarification. Voice search is a type of [[Dialog systems|dialog system]].

== References ==
{{No footnotes|date=April 2009}}
*Ye-Yi Wang, Dong Yu, Yun-Cheng Ju, Alex Acero, An Introduction to Voice Search, IEEE Signal Processing Magazine (Special Issue on Spoken Language Technology), Institute of Electrical and Electronics Engineers, Inc., May 2008
*J. Sherwani, Dong Yu, Tim Paek, Mary Czerwinski, Yun-Cheng Ju, and Alex Acero. 'VoicePedia: Towards Speech-Based Access to Unstructured Information', Proceedings of the 8th Annual Conference of the International Communication Association (Interspeech 2007). Antwerp, Belgium, August, 2007
{{Internet search}}

[[Category:Information retrieval genres]]</text>
      <sha1>pzj9uywjeuw6a3wzu2w0m56dqwxucfs</sha1>
    </revision>
  </page>
  <page>
    <title>Adversarial information retrieval</title>
    <ns>0</ns>
    <id>11486091</id>
    <revision>
      <id>686895664</id>
      <parentid>686778986</parentid>
      <timestamp>2015-10-22T00:57:49Z</timestamp>
      <contributor>
        <username>Bgwhite</username>
        <id>264323</id>
      </contributor>
      <comment>[[WP:CHECKWIKI]] error fix #95. Editor's signature or link to user space. Do [[Wikipedia:GENFIXES|general fixes]] and cleanup if needed. - using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3216" xml:space="preserve">'''Adversarial information retrieval''' ('''adversarial IR''') is a topic in [[information retrieval]] related to strategies for working with a data source where some portion of it has been manipulated maliciously.  Tasks can include gathering, indexing, filtering, retrieving and ranking information from such a data source. Adversarial IR includes the study of methods to detect, isolate, and defeat such manipulation.

On the Web, the predominant form of such manipulation is [[spamdexing|search engine spamming]] (also known as spamdexing), which involves employing various techniques to disrupt the activity of [[web search engines]], usually for financial gain. Examples of spamdexing are [[Google bomb|link-bombing]], [[comment spam (disambiguation)|comment]] or [[referrer spam]], [[spam blog]]s (splogs), malicious tagging.  [[Reverse engineering]] of [[ranking function|ranking algorithms]], [[Ad filtering|advertisement blocking]], [[click fraud]],&lt;ref&gt;Jansen, B. J. (2007) [https://faculty.ist.psu.edu/jjansen/academic/jansen_click_fraud.pdf Click fraud]. IEEE Computer. 40(7), 85-86.&lt;/ref&gt; and [[web content filtering]] may also be considered forms of adversarial [[data manipulation]].&lt;ref&gt;B. Davison, M. Najork, and T. Converse (2006), [http://wayback.archive.org/web/20090320173324/http://www.acm.org/sigs/sigir/forum/2006D/2006d_sigirforum_davison.pdf SIGIR Worksheet Report: Adversarial Information Retrieval on the Web (AIRWeb 2006)]&lt;/ref&gt;

Activities intended to poison the supply of useful data make search engines less useful for users. If search engines are more exclusionary they risk becoming more like directories and less dynamic.

== Topics ==
Topics related to Web spam (spamdexing):

* [[Link spam]]
* [[Keyword spamming]]
* [[Cloaking]]
* Malicious tagging
* Spam related to blogs, including [[spam in blogs|comment spam]], [[spam blog|splogs]], and [[sping|ping spam]]

Other topics:
* [[Click fraud]] detection
* Reverse engineering of  [[search engine]]'s [[ranking]] algorithm
* Web [[content filtering]]
* [[Ad filtering|Advertisement blocking]]
* Stealth [[web crawling|crawling]]
*[[Troll (Internet)]]
* Malicious tagging or voting in [[social networks]]
* [[Astroturfing]]
* [[Sockpuppetry]]

== History ==
The term "adversarial information retrieval" was first coined in 2000 by [[Andrei Broder]] (then Chief Scientist at [[Alta Vista]]) during the Web plenary session at the [[Text Retrieval Conference|TREC]]-9 conference.&lt;ref&gt;D. Hawking and N. Craswell (2004), [http://es.csiro.au/pubs/trecbook_for_website.pdf Very Large Scale Retrieval and Web Search (Preprint version)]&lt;/ref&gt;

== See also ==
*[[Spamdexing]]
*[[Information retrieval]]

== References ==
{{reflist}}

== External links ==
*[http://airweb.cse.lehigh.edu/ AIRWeb]: series of workshops on Adversarial Information Retrieval on the Web
*[http://webspam.lip6.fr/ Web Spam Challenge]: competition for researchers on Web Spam Detection
*[http://wayback.archive.org/web/20100217125910/http://barcelona.research.yahoo.net/webspam/ Web Spam Datasets]: datasets for research on Web Spam Detection

{{DEFAULTSORT:Adversarial Information Retrieval}}
[[Category:Information retrieval genres]]
[[Category:Internet fraud]]</text>
      <sha1>2aitq56yh0wqp2fqf6md6sqxtskjz4p</sha1>
    </revision>
  </page>
  <page>
    <title>Desktop search</title>
    <ns>0</ns>
    <id>1274156</id>
    <revision>
      <id>748763325</id>
      <parentid>748763108</parentid>
      <timestamp>2016-11-10T06:37:06Z</timestamp>
      <contributor>
        <ip>41.130.82.43</ip>
      </contributor>
      <comment>/* Platforms &amp; their histories */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="18453" xml:space="preserve">{{Multiple issues|
{{technical|date=October 2014}}
{{manual||date=October 2016}}
}}
[[File:AdunaAutoFocus5.png|thumb|OSL Desktop Search engines software Aduna AutoFocus 5]]
'''Desktop search''' tools search within a user's own [[computer files]] as opposed to searching the Internet. These tools are designed to find information on the user's PC, including web browser history, e-mail archives, text documents, sound files, images, and video.

One of the main advantages of desktop search programs is that search results are displayed quickly due to the use of proper indexes.

A variety of desktop search programs are now available; see [[List of search engines#Desktop search engines|this list]] for examples.  Most desktop search programs are standalone applications, whereas a few also provide search capabilities in an [[integrated writing environment]] (IWE).

Desktop search emerged as a concern for large firms for two main reasons: untapped productivity and security. On the one hand, users need to be able to quickly find relevant files, but on the other hand, they shouldn't have access to restricted files. According to analyst firm Gartner, up to 80% of some companies' data is locked up inside [[unstructured data]] — the information stored on an end user's PC, the directories (folders) and files they've created on a [[Computer network|network]], documents stored in repositories such as corporate [[intranet]]s and a multitude of other locations.&lt;ref&gt;{{Citation | url = http://www.computerweekly.com/Articles/2006/04/25/215622/security-special-report-who-sees-your-data.htm | title = Security special report: Who sees your data? | newspaper = Computer Weekly | date = 2006-04-25}}.&lt;/ref&gt;  Moreover, many companies have structured or unstructured information stored in older [[file formats]] to which they don't have ready access.

Companies doing business in the [[United States]] are frequently required under regulatory mandates like [[Sarbanes-Oxley]], [[Health Insurance Portability and Accountability Act|HIPAA]] and [[FERPA]] to make sure that access to sensitive information is 100% controlled. This creates a challenge for IT organizations, which may not have a desktop search standard, or lack strict central control over end users [[downloading]] tools from the [[Internet]]. Some consumer-oriented desktop search tools make it possible to generate indexes outside the corporate [[Firewall (computing)|firewall]] and share those indexes with unauthorized users. In some cases, end users are able to index — but not preview — items they should not even know exist.{{Citation needed|date = November 2009}}

Historically, full desktop search comes from the work of [[Apple inc.|Apple Computer's]] [[Apple Advanced Technology Group|Advanced Technology Group]], resulting in the underlying [[AppleSearch]] technology in the early 1990s. It was used to build the [[Sherlock (software)|Sherlock]] search engine and then developed into [[Spotlight (software)|Spotlight]], which brought automated, non-timer-based full indexing into the operating system.

== Technologies ==
Most desktop search engines build and maintain an [[Index (search engine)|index database]] to achieve reasonable performance when searching several [[gigabyte]]s of [[data]]. Indexing usually takes place when the computer is idle and most search applications can be set to suspend indexing if a portable computer is running on batteries, in order to save power. There are notable exceptions, however: Voidtools' Everything Search Engine,&lt;ref&gt;{{cite web|title=Everything Search Engine|url=http://www.voidtools.com/|publisher=voidtools|accessdate=27 December 2013}}&lt;/ref&gt; which performs searches over only filenames &amp;mdash; not the files' contents &amp;mdash; for NTFS volumes only, is able to build its index from scratch in just a few seconds. Another exception is Vegnos Desktop Search Engine,&lt;ref&gt;{{cite web|title=Vegnos|url=http://www.vegnos.com|publisher=Vegnos|accessdate=27 December 2013}}&lt;/ref&gt; which performs searches over filenames and files' contents without building any indices. The benefits to not having indices is that, in addition to not requiring persistent storage, more powerful queries (e.g., [[regular expressions]]) can be issued, whereas indexed search engines are limited to keyword-based queries. An index may also not be up-to-date, when a query is performed. In this case, results returned will not be accurate (that is, a hit may be shown when it is no longer there, and a file may not be shown, when in fact it is a hit). Some products have sought to remedy this disadvantage by building a real-time indexing function into the software. There are disadvantages to not indexing. Namely, the time to complete a query can be significant, and the issued query can also be resource-intensive.

Desktop search tools typically collect three types of information about files:
* file and folder names
* [[metadata]], such as titles, authors, comments in file types such as [[MP3]], [[Portable Document Format|PDF]] and [[JPEG]]
* file content (for supported types of documents only)

To search effectively within documents, the tools need to be able to parse many different types of documents. This is achieved by using filters that interpret selected file formats. For example, a ''Microsoft Office Filter'' might be used to search inside [[Microsoft Office]] documents.

Long-term goals for desktop search include the ability to search the contents of image files, sound files and video by context.&lt;ref&gt;{{cite web|url=http://www.niallkennedy.com/blog/archives/2006/10/video-search.html|title=The current state of video search|author=Niall Kennedy|date=17 October 2006|work=Niall Kennedy|accessdate=24 June 2015}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.niallkennedy.com/blog/archives/2006/10/audio-search.html|title=The current state of audio search|author=Niall Kennedy|date=15 October 2006|work=Niall Kennedy|accessdate=24 June 2015}}&lt;/ref&gt;

The sector attracted considerable attention from the struggle between Microsoft and Google.&lt;ref&gt;{{cite web|url=http://news.bbc.co.uk/1/hi/technology/3952285.stm|title=BBC NEWS - Technology - Search wars hit desktop computers|work=bbc.co.uk|accessdate=24 June 2015}}&lt;/ref&gt; According to market analysts, both companies were attempting to leverage their monopolies (of [[web browser]]s and [[search engine]]s, respectively) to strengthen their dominance. Due to [[Google]]'s complaint that users of Windows Vista cannot choose any competitor's desktop search program over the built-in one, an agreement was reached between [[US Justice Department]] and [[Microsoft]] that [[Windows Vista Service Pack 1]] would enable users to choose between the built-in and other desktop search programs, and select which one is to be the default.&lt;ref&gt;{{cite web|url=http://goebelgroup.com/searchtoolblog/2007/06/20/microsoft-agrees-to-change-vista-desktop-search-tool/|title=SearchMax|work=goebelgroup.com|accessdate=24 June 2015}}&lt;/ref&gt;

As of September, 2011, Google ended life for Google Desktop, a program designed to make it easy for users to search their own PCs for emails, files, music, photos, Web pages and more.&lt;ref&gt;[http://googledesktop.blogspot.com/2011/09/google-desktop-update.html/ "Google Desktop Update" (Sept 2011)]&lt;/ref&gt;

Desktop search products are software alternatives to Windows Desktop and Outlook Search, helping business professional sift through desktop files, emails, attachments, SharePoint data, and more.,&lt;ref&gt;[http://www.brianmadden.com/blogs/brianmadden/archive/2015/03/11/what-do-you-do-for-desktop-search-in-vdi-and-rdsh.aspx  „What do you do for desktop search in VDI and RDSH?“]. Blogpost by Brian Madden on brainmadden.com. Retrieved on March 25, 2015.&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://venturebeat.com/2008/06/02/lookeen-offers-a-new-way-way-for-outlook-users-to-search/|title=Lookeen offers a new way for Outlook users to search|author=Anthony Ha|date=2 June 2008|work=VentureBeat|accessdate=8 March 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.computerworld.com/article/2475293/desktop-apps/x1-rises-again-with-desktop-search-8--virtual-edition.html/|title=X1 rises again with Desktop Search 8, Virtual Edition|author=Robert L. Mitchell|date=8 May 2013|work=Computerworld|accessdate=24 June 2015}}&lt;/ref&gt;

==Platforms &amp; their histories==
There are three main platforms that desktop search falls into. [[Microsoft Windows|Windows]], [[Mac OS|Mac]] OS &amp; [[Linux]]. This article will focus on the history of these search platforms, the features they had, and how those features evolved.

=== Windows ===
Today's [[Windows Search]] replaced WDS ([[Windows Desktop Search]]). WDS, in turn, replaced [[Indexing Service]]. A "a base service that extracts content from files and constructs an indexed catalog to facilitate efficient and rapid searching"&lt;ref&gt;{{cite web|url=https://msdn.microsoft.com/en-us/library/ee805985%28v=vs.85%29.aspx|title=Indexing Service|publisher=Microsoft|work=microsoft.com|accessdate=24 June 2015}}&lt;/ref&gt; Indexing service was originally released in August 1996, it was built in order to speed up manually searching for files on Personal Desktops and Corporate Computer Network. Indexing service helped by using Microsoft web servers to index files on the desired hard drives. Indexing was done by file format. By using terms that users provided, a search was conducted that matched terms to the data within the file formats. The largest issue that Indexing service faced was the fact that every time a file was added, it had to be indexed. This coupled with the fact that the indexing cached the entire index in RAM, made the hardware a huge limitation.&lt;ref&gt;{{cite web|url=https://msdn.microsoft.com/en-us/library/dd582937%28v=office.11%29.aspx|title=Indexing with Microsoft Index Server|publisher=Microsoft|work=microsoft.com|accessdate=24 June 2015}}&lt;/ref&gt; This made indexing large amounts of files require extremely powerful hardware and very long wait times.

In 2003, Windows Desktop Search (WDS) replaced Microsoft Indexing Service. Instead of only matching terms to the details of the file format and file names, WDS brings in content indexing to all Microsoft files and text-based formats such as e-mail and text files. This means, that WDS looked into the files and indexed the content. Thus, when a user searched a term, WDS no longer matched just information such as file format types and file names, but terms, and values stored within those files. WDS also brought "Instant searching" meaning the user could type a character and the query would instantly start searching and updating the query as the user typed in more characters.&lt;ref&gt;{{cite web|url=http://www.microsoft.com/windows/products/winfamily/desktopsearch/technicalresources/techfaq.mspx|archiveurl=https://web.archive.org/web/20110924212903/http://www.microsoft.com/windows/products/winfamily/desktopsearch/technicalresources/techfaq.mspx|title=Windows Search: Technical FAQ|archivedate=24 September 2011|publisher=Microsoft|work=microsoft.com|accessdate=24 June 2015}}&lt;/ref&gt; Windows Search apparently used up a lot of processing power, as Windows Desktop Search would only run if it was directly queried or while the PC was idle. Even only running while directly queried or while the computer was idled, indexing the entire hard drive still took hours. The index would be around 10% of the size of all the files that it indexed. For example, if the indexed files amounted to around 100GB of space, the index would, itself, be 10GB large.

With the release of [[Windows Vista]] came Windows Search 3.1. Unlike its predecessors WDS and Windows Search 3.0, 3.1 could search through both indexed and non indexed locations seamlessly. Also, the [[RAM]] and [[CPU]] requirements were greatly reduced, cutting back indexing times immensely. Windows Search 4.0 is currently running on all PCs with [[Windows 7]] and up.

=== Mac OS ===
Mac OS was the first to implement Desktop Search with its [[AppleSearch]] search engine, allowing users to fully search all documents within their Macintosh computer, including file format types, meta-data on those files, and content within the files. AppleSearch was a [[Client–server model|client/server application]], and as such required a server separate from the main device in order to function. The biggest issue with AppleSearch were its large resource requirements: "AppleSearch requires at least a 68040 processor and 5MB of RAM."&lt;ref&gt;{{cite web|url=http://infomotions.com/musings/tricks/manuscript/1600-0001.html|title=AppleSearch|work=infomotions.com|accessdate=24 June 2015}}&lt;/ref&gt; At the time, a Macintosh computer with these specifications was priced at approximately $1400; equivalent to $2050 in 2015.&lt;ref&gt;{{cite web|url=http://stats.areppim.com/calc/calc_usdlrxdeflator.php|title=Converter of current to real US dollars - using the GDP deflator|author=eduardo casais|work=areppim.com|accessdate=24 June 2015}}&lt;/ref&gt; On top of this, the software itself cost an additional $1400 for a single license.

In 1997, [[Sherlock (software)|Sherlock]] was released alongside Mac OS 8.5. Sherlock (named after the famous fictional detective [[Sherlock Holmes]]) was integrated into Mac OS's file browser&amp;nbsp;– [[Finder (software)|Finder]]. Sherlock extended the desktop search function to the World Wide Web, allowing users to search both locally and externally. Adding additional functions—such as internet access—to Sherlock was relatively simple, as this was done through plugins written as plain text files. Sherlock was included in every release of Mac OS from [[Mac OS 8]], before being deprecated and replaced by [[Spotlight (software)|Spotlight]] and [[Dashboard (Mac OS)|Dashboard]] in [[Mac OS X Tiger|Mac OS X 10.4 Tiger]]. It was officially removed in [[Mac OS X Leopard|Mac OS X 10.5 Leopard]]

[[Spotlight (software)|Spotlight]] was released in 2005 as part of [[Mac OS X Tiger|Mac OS X 10.4 Tiger]]. It is a Selection-based search tool, which means the user invokes a query using only the mouse. Spotlight allows the user to search the Internet for more information about any keyword or phrase contained within a document or webpage, and uses a built-in calculator and Oxford American Dictionary to offer quick access to small calculations and word definitions.&lt;ref&gt;{{cite web|url=http://www.apple.com/pr/library/2005/04/12Apple-to-Ship-Mac-OS-X-Tiger-on-April-29.html|title=Apple - Press Info - Apple to Ship Mac OS X "Tiger" on April 29|work=apple.com|accessdate=24 June 2015}}&lt;/ref&gt; While Spotlight initially has a long startup time, this decreases as the hard disk is indexed. As files are added by the user, the index is constantly updated in the background using minimal CPU &amp; RAM resources.

=== Linux ===
There are a wide range of desktop search options for Linux users, depending upon the skill level of the user, their preference to use desktop tools which tightly integrate into their desktop environment, command-shell functionality (often with advanced scripting options), or browser-based users interfaces to locally running software.  In addition, many users create their own indexing from a variety of indexing packages (e.g. one which does extraction and indexing of PDF/DOC/DOCX/[[OpenDocument|ODT]] documents well, another search engine which works w/ vcard, LDAP, and other directory/contact databases, as well as the conventional &lt;tt&gt;find&lt;/tt&gt; and &lt;tt&gt;locate&lt;/tt&gt; commands.

====Ubuntu====
The [[Ubuntu distribution]] is a popular version of Linux. Strangely enough, Ubuntu didn't have desktop search until Feisty Fawn 7.04. Using [[Tracker (search software)|Tracker]]&lt;ref&gt;{{cite web|url=http://arstechnica.com/information-technology/2007/07/afirst-look-at-tracker-0-6-0/|title=A first look at Tracker 0.6.0|work=Ars Technica|accessdate=24 June 2015}}&lt;/ref&gt; desktop search, the desktop search feature was very similar to Mac OS's AppleSearch and Sherlock. Considering the fact that both are UNIX-based systems. Tracker, released in late 2007, was built to have a relatively low impact on system resources. But unfortunately occasionally had sporadic control over what resources it was using. It not only featured the basic features of file format sorting and meta-data matching, but support for searching through emails and instant messages was added. Years later, in 2014 [[Recoll]]&lt;ref&gt;{{cite web|url=http://www.lesbonscomptes.com/recoll/usermanual/index.html#RCL.INDEXING|title=Recoll user manual|work=lesbonscomptes.com|accessdate=24 June 2015}}&lt;/ref&gt; was added to Linux distributions, it works with other search programs such as Tracker and [[Beagle (software)|Beagle]] to provide efficient full text search. This greatly increased the types of queries that Linux desktop searches could handle as well as file types. A major advantage of Recoll is that it allows for greater customization of what is indexed. For example, Recoll will index the entire hard disk by default, but will and can index just a few select directories instead of wasting time indexing directories you know you will never need to look at. It also allows for more search options, you may actually narrow down what kind of query you want to ask. For example, you could search for just file types or by content.&lt;ref&gt;{{cite web|url=http://archive09.linux.com/feature/114283|title=Linux.com|work=linux.com|accessdate=24 June 2015}}&lt;/ref&gt;

====[[openSUSE]]&lt;ref&gt;http://www.opensuse.org/&lt;/ref&gt;====
&lt;!--TODO! Prior desktop search before KDE 3.5--&gt;
Starting with [[KDE4]], the [[NEPOMUK (software)|NEPOMUK]] was introduced.  It provided the ability to index a wide range of desktop content, email, and use semantic web technologies (e.g. [[Resource Description Framework|RDF]]) to annotate the database.  The introduction faced a few glitches, much of which seemed to be based on the [[triplestore]].  Performance improved (at least for queries) by switching the backend to a stripped own version of the [[Virtuoso]] Open Source Edition, however indexing remained a common user complaint.  
Based on user feedback, the Nepomuk indexing and search has been replaced with the Baloo framework&lt;ref&gt;https://community.kde.org/Baloo&lt;/ref&gt; based on [[Xapian]].

==See also==
*[[List of search engines#Desktop search engines|List of desktop search engines]]

== References ==
{{reflist|2}}

{{Navigationbox Desktopsearch}}

{{DEFAULTSORT:Desktop Search}}
[[Category:Desktop search engines| ]]
[[Category:Information retrieval genres]]</text>
      <sha1>i4l5ki9rr4s02u3wj3ttbxbtlymyq8g</sha1>
    </revision>
  </page>
  <page>
    <title>Temporal information retrieval</title>
    <ns>0</ns>
    <id>35804330</id>
    <revision>
      <id>747933069</id>
      <parentid>666716437</parentid>
      <timestamp>2016-11-05T08:00:13Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* Temporal visualization (T-interfaces) */http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="81943" xml:space="preserve">'''Temporal Information Retrieval (T-IR)''' is an emerging area of research related to the field of [[information retrieval]] (IR) and a considerable number of sub-areas, positioning itself, as an important dimension in the context of the user information needs.

According to [[information theory]] science (Metzger, 2007),&lt;ref name="Metzger2007"&gt;{{cite journal |last=Metzger |first=Miriam |title=Making Sense of Credibility on the Web: Models for Evaluating Online Information and Recommendations for Future Research |journal=Journal of the American Society for Information Science and Technology |volume=58 |issue=13 |pages=2078–2091 |year =2007 |url=http://dl.acm.org/citation.cfm?id=1315940 |doi=10.1002/asi.20672 }}&lt;/ref&gt; timeliness or currency is one of the key five aspects that determine a document’s credibility besides relevance, accuracy, objectivity and coverage. One can provide many examples when the returned search results are of little value due to temporal problems such as obsolete data on weather, outdated information about a given company’s earnings or information on already-happened or invalid predictions.

T-IR, in general, aims at satisfying these temporal needs and at combining traditional notions of document relevance with the so-called temporal relevance. This will enable the return of temporally relevant documents, thus providing a temporal overview of the results in the form of timeliness or similar structures. It also shows to be very useful for query understanding, query disambiguation, query classification, result diversification and so on.

This page contains a list of the most important research in temporal information retrieval (T-IR) and its related sub-areas. As several of the referred works are related with different research areas a single article can be found in more than one different table. For ease of reading the articles are categorized in a number of different sub-areas referring to its main scope, in detail.

== Temporal dynamics (T-dynamics) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Baeza, Y.''' (2002). [http://www.dcs.bbk.ac.uk/webDyn2/proceedings/baeza_yates_web_strucutre.pdf/ Web Structure, Dynamics and Page Quality]. In A. Laendar &amp; A. Oliveira (Eds.), ''In Lecture Notes in Computer Science - SPIRE2002: 9th International Symposium on String Processing and Information Retrieval'' (Vol. 2476/2002, pp.&amp;nbsp;117 – 130). Lisbon, Portugal. September 11–13: Springer Berlin / Heidelberg. || 2002 || SPIRE || T-Dynamics ||
|-
|'''Cho, J., &amp; Garcia-Molina, H.''' (2003). [http://dl.acm.org/citation.cfm?id=857170 Estimating Frequency of Change]. ''In [http://toit.acm.org TOIT: ACM Transactions on Internet Technology]'', 3(3), 256 - 290.|| 2003 || TOIT || T-Dynamics ||
|-
| '''Fetterly, D., Manasse, M., Najork, M., &amp; Wiener, J.''' (2003). [http://dl.acm.org/citation.cfm?id=775246|A Large-Scale Study of the Evolution of Web Pages]]. ''In [http://www2003.org/ WWW2003]: Proceedings of the 12th International World Wide Web Conference'' (pp.&amp;nbsp;669 – 678). Budapest, Hungary. May 20–24: ACM Press. || 2003 || WWW || T-Dynamics ||
|-
| '''Ntoulas, A., Cho, J., &amp; Olston, C.''' (2004). [http://dl.acm.org/citation.cfm?id=988674 What's New on the Web?: the Evolution of the Web from a Search Engine Perspective]. In [http://www2004.org WWW2004]: Proceedings of the 13th International World Wide Web Conference (pp.&amp;nbsp;1 – 12). New York, NY, United States. May 17–22: ACM Press. || 2004 || WWW || T-Dynamics ||
|-
| '''Vlachos, M., Meek, C., Vagena, Z., &amp; Gunopulos, D.''' (2004). [http://portal.acm.org/citation.cfm?id=1007586 Identifying Similarities, Periodicities and Bursts for Online Search Queries]. In [http://www09.sigmod.org/sigmod04/eproceedings/ SIGMOD2004]: Proceedings of the International Conference on Management of Data (pp.&amp;nbsp;131 – 142). Paris, France. June 13–18: ACM Press. || 2004 || SIGMOD || T-Dynamics ||
|-
| '''Beitzel, S. M., Jensen, E. C., Chowdhury, A., Frieder, O., &amp; Grossman, D.''' (2007). [http://dl.acm.org/citation.cfm?id=1190282 Temporal analysis of a very large topically categorized Web query log]. ''In [http://www.asis.org/jasist.html JASIST]: Journal of the American Society for Information Science and Technology'', 58(2), 166 - 178. || 2007 || JASIST || T-Dynamics ||
|-
| '''Jones, R., &amp; Diaz, F.''' (2007). [http://dl.acm.org/citation.cfm?id=1247720 Temporal Profiles of Queries]. ''In [http://tois.acm.org/ TOIS: ACM Transactions on Information Systems]'', 25(3). Article No.: 14. || 2007 || TOIS || TQ-Understanding ||
|-
| '''Bordino, I., Boldi, P., Donato, D., Santini, M., &amp; Vigna, S.''' (2008). [http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=4734022&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fiel5%2F4733896%2F4733897%2F04734022.pdf%3Farnumber%3D4734022 Temporal Evolution of the UK Web]. In [http://compbio.cs.uic.edu/adn-icdm08/ ADN2008]: Proceedings of the 1st International Workshop on Analysis of Dynamic Networks associated to [http://icdm08.isti.cnr.it/ ICDM2008]: IEEE International Conference on Data Mining (pp.&amp;nbsp;909 – 918). Pisa, Italy. December 19: IEEE Computer Society Press. || 2008 || ICDM - ADN || T-Dynamics ||
|-
| '''Adar, E., Teevan, J., Dumais, S. T., &amp; Elsas, J. L.''' (2009). [http://portal.acm.org/citation.cfm?id=1498837 The Web Changes Everything: Understanding the Dynamics of Web Content]. ''In [http://wsdm2009.org/ WSDM2009]: Proceedings of the 2nd ACM International Conference on Web Search and Data Mining'' (pp.&amp;nbsp;282 – 291). Barcelona, Spain. February 9–12: ACM Press. || 2009 || WSDM || T-Dynamics ||
|-
| '''Metzler, D., Jones, R., Peng, F., &amp; Zhang, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1572085 Improving Search Relevance for Implicitly Temporal Queries]. ''In [http://www.sigir2009.org/ SIGIR 2009]: Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;700 – 701). Boston, MA, United States. July 19–23: ACM Press. || 2009 || SIGIR || TQ-Understanding ||
|-
| '''Elsas, J. L., &amp; Dumais, S. T.''' (2010). [http://dl.acm.org/citation.cfm?id=1718489 Leveraging Temporal Dynamics of Document Content in Relevance Ranking]. ''In [http://www.wsdm-conference.org/2010/ WSDM10]: Third ACM International Conference on Web Search and Data Mining'' (pp.&amp;nbsp;1 – 10). New York, United States. February 3–06: ACM Press. || 2010 || WSDM || T-Dynamics ||
|-
| '''Jatowt, A., Kawai, H., Kanazawa, K., Tanaka, K., &amp; Kunieda, K.''' (2010). [http://dl.acm.org/citation.cfm?id=1772835 Analyzing Collective View of Future, Time-referenced Events on the Web]. ''In [http://www2010.org/www/index.html WWW2010]: Proceedings of the 19th International World Wide Web Conference'' (pp.&amp;nbsp;1123 – 1124). Raleigh, United States. April 26–30: ACM Press. || 2010 || WWW || F-IRetrieval ||
|-
| '''Aji, A., Agichtein, E.''' (2010). [http://dl.acm.org/citation.cfm?id=2175298.2175332 Deconstructing Interaction Dynamics in Knowledge Sharing Communities]. ''In [http://sbp.asu.edu/sbp2010/sbp10.html]: Third International Conference on Social Computing, Behavioral-Cultural Modeling, &amp; Prediction'' (pp.&amp;nbsp;273 – 281). Washington DC, United States. March 30–31: Springer-Verlag. || 2010 || SBP || T-Dynamics ||
|-
| '''Kulkarni, A., Teevan, J., Svore, K. M., &amp; Dumais, S. T.''' (2011). [http://portal.acm.org/citation.cfm?id=1935862 Understanding Temporal Query Dynamics]. ''In [http://www.wsdm2011.org/ WSDM2011]: In Proceedings of the 4th ACM International Conference on Web Search and Data Mining'' (pp.&amp;nbsp;167 – 176). Hong Kong, China. February 9–12: ACM Press. || 2011 || WSDM || T-Dynamics ||
|-
| '''Campos, R., Dias, G., &amp; Jorge, A. M.''' (2011). [http://ceur-ws.org/Vol-707/TWAW2011.pdf What is the Temporal Value of Web Snippets?] ''In [http://temporalweb.net/page3/page3.html TWAW 2011]: Proceedings of the 1st International Temporal Web Analytics Workshop associated to [http://www.www2011india.com/ WWW2011]: 20th International World Wide Web Conference''. Hyderabad, India. March 28.: CEUR Workshop Proceedings. || 2011 || WWW - TWAW || T-Dynamics ||
|-
| '''Campos, R., Jorge, A., &amp; Dias, G.''' (2011). [http://ciir.cs.umass.edu/sigir2011/qru/campos+al.pdf Using Web Snippets and Query-logs to Measure Implicit Temporal Intents in Queries]. ''In [http://ciir.cs.umass.edu/sigir2011/qru/ QRU 2011]: Proceedings of the Query Representation and Understanding Workshop associated to [http://www.sigir2011.org/ SIGIR2011]: 34th Annual International ACM SIGIR 2011 Conference on Research and Development in Information Retrieval'', (pp.&amp;nbsp;13 – 16). Beijing, China. July 28. || 2011 || SIGIR - QRU || T-Dynamics ||
|-
| '''Shokouhi, M.''' (2011). [http://dl.acm.org/citation.cfm?id=2010104 Detecting Seasonal Queries by Time-Series Analysis]. In [http://www.sigir2011.org/ SIGIR2011]: ''In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information'' (pp.&amp;nbsp;1171 – 1172). Beijing, China. July 24–28: ACM Press. || 2011 || SIGIR || T-Dynamics ||
|-
| '''Dias, G., Campos, R., &amp; Jorge, A.''' (2011). [http://select.cs.cmu.edu/meetings/enir2011/papers/dias-campos-jorge.pdf Future Retrieval: What Does the Future Talk About?] ''In [http://select.cs.cmu.edu/meetings/enir2011/ ENIR 2011]: Proceedings of the Enriching Information Retrieval Workshop associated to [http://www.sigir2011.org/ SIGIR2011]: 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval''. Beijing, China. July 28. || 2011 || SIGIR - ENIR || F-IRetrieval ||
|-
| '''Campos, R., Dias, G., &amp; Jorge, A. M.''' (2011). [http://dl.acm.org/citation.cfm?id=2051169 An Exploratory Study on the impact of Temporal Features on the Classification and Clustering of Future-Related Web Documents]. ''In L. Antunes, &amp; H. S. Pinto (Eds.), Lecture Notes in Artificial Intelligence - Progress in Artificial Intelligence - [http://epia2011.appia.pt/ EPIA2011]: 15th Portuguese Conference on Artificial Intelligence associated to APPIA: Portuguese Association for Artificial Intelligence'' (Vol. 7026/2011, pp.&amp;nbsp;581 – 596). Lisboa, Portugal. October 10–13: Springer Berlin / Heidelberg. || 2011 || EPIA || F-IRetrieval ||
|-
| '''Jatowt, A., &amp; Yeung, C. M.''' (2011). [http://dl.acm.org/citation.cfm?id=2063759 Extracting Collective Expectations about the Future from Large Text Collections]. ''In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management'' (pp.&amp;nbsp;1259 – 1264). Glasgow, Scotland, UK. October 24 - 28: ACM Press. || 2011 || CIKM || F-IRetrieval ||
|-
| '''Yeung, C.-m. A., &amp; Jatowt, A.''' (2011). [http://dl.acm.org/citation.cfm?id=2063755 Studying How the Past is Remembered: Towards Computational History through Large Scale Text Mining]. ''In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management'' (pp.&amp;nbsp;1231 – 1240). Glasgow, Scotland, UK. October 24–28: ACM Press. || 2011 || CIKM || C-Memory ||
|-
| '''Costa, M., &amp; Silva, M. J., &amp; Couto, F. M.''' (2014). [http://dl.acm.org/citation.cfm?id=2609619 Learning Temporal-Dependent Ranking Models]. ''In Proceedings of the [http://sigir.org/sigir2014/ SIGIR2014]: 37th Annual ACM SIGIR Conference'' (pp.&amp;nbsp;757--766). Gold Coast, Australia. July 6–11: ACM Press. || 2014 || SIGIR || T-RModels ||
|}

== Temporal markup languages (T-MLanguages) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Setzer, A., &amp; Gaizauskas, R.''' (2000). [ftp://ftp.dcs.shef.ac.uk/home/robertg/papers/lrec00-tempann.pdf Annotating Events and Temporal Information in Newswire Texts]. ''In [http://www.xanthi.ilsp.gr/lrec/ LREC2000]: Proceedings of the 2nd International Conference on Language Resources and Evaluation''. Athens, Greece. May 31 - June 2: ELDA. || 2000 || LREC || T-MLanguages ||
|-
| '''Setzer, A.''' (2001). [http://www.andrea-setzer.org.uk/PAPERS/thesis.pdf Temporal Information in Newswire Articles: An Annotation Scheme and Corpus Study]. Sheffield, UK: University of Sheffield. || 2001 || Phd Thesis || T-MLanguages ||
|-
| '''Ferro, L., Mani, I., Sundheim, B., &amp; Wilson, G.''' (2001). [http://www.timeml.org/site/terqas/readings/MTRAnnotationGuide_v1_02.pdf TIDES Temporal Annotation Guidelines]. Version 1.0.2. Technical Report, MITRE Corporation, McLean, Virginia, United States. || 2001 || Technical Report || T-MLanguages ||
|-
| '''Pustejovsky, J., Castaño, J., Ingria, R., Sauri, R., Gaizauskas, R., Setzer, A., et al.''' (2003). TimeML: Robust Specification of Event and Temporal Expression in Text. ''In [http://iwcs.uvt.nl/iwcs5/index.htm IWCS2003]: Proceedings of the 5th International Workshop on Computational Semantics'', (pp.&amp;nbsp;28 – 34). Tilburg, Netherlands. January 15–17. || 2003 || IWCS || T-MLanguages ||
|-
| '''Ferro, L., Gerber, L., Mani, I., Sundheim, B., &amp; Wilson, G.''' (2005). [http://projects.ldc.upenn.edu/ace/docs/English-TIMEX2-Guidelines_v0.1.pdf TIDES 2005 Standard for the Annotation of Temporal Expressions]. Technical Report, MITRE Corporation, McLean, Virginia, United States. || 2005 || Technical Report || T-MLanguages ||
|}

== Temporal taggers (T-taggers) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| [http://www.timeml.org/site/tarsqi/toolkit/manual/ TempEx Module] - [http://www.timeml.org/site/tarsqi/toolkit/index.html Tarsqi Toolkit] - '''Mani, I., &amp; Wilson, G.''' (2000). [[dl.acm.org/citation.cfm?id=1075228|Robust Temporal Processing of News]]. ''In [http://www.cse.ust.hk/acl2000/ ACL2000]: Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics'' (pp.&amp;nbsp;69 – 76). Hong Kong, China. October 1–8: Association for Computational Linguistics. || 2000 || ACL || T-Taggers ||
|-
| [http://www.aktors.org/technologies/annie/ Annie] - [http://gate.ac.uk/download/index.html GATE distribution] - '''Cunningham, H., Maynard, D., Bontcheva, K., &amp; Tablan, V.''' (2002). [http://eprints.aktors.org/90/01/acl-main.pdf GATE: A Framework And Graphical Development Environment For Robust NLP Tools And Applications]. ''In [http://www.aclweb.org/mirror/acl2002/ ACL2002]: Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics'' (pp.&amp;nbsp;168 – 175). Philadelphia, PA, United States. July 6–12: Association for Computational Linguistics. || 2002 || ACL || T-Taggers ||
|-
| [http://www.timeml.org/site/tarsqi/modules/gutime/download.html GUTime] - [http://www.timeml.org/site/tarsqi/toolkit/index.html Tarsqi Toolkit] || 2002 ||  || T-Taggers ||
|-
| [http://dbs.ifi.uni-heidelberg.de/index.php?id=form-downloads HeidelTime] - '''Strötgen, J., &amp; Gertz, M.''' (2010). [http://delivery.acm.org/10.1145/1860000/1859735/p321-strotgen.pdf?ip=188.80.124.88&amp;acc=OPEN&amp;CFID=82473711&amp;CFTOKEN=13661527&amp;__acm__=1337002719_1b05141ffc83e798f400c972756d43ad HeidelTime: High Quality Rule-based Extraction and Normalization of Temporal Expressions]. ''In [http://semeval2.fbk.eu/semeval2.php SemEval2010]: Proceedings of the 5th International Workshop on Semantic Evaluation associated to [http://acl2010.org/ ACL2010]: 41st Annual Meeting of the Association for Computational Linguistics'', (pp.&amp;nbsp;321 – 324). Uppsala, Sweden. July 11–16.|| 2010 || ACL - SemEval || T-Taggers ||
|-
| [http://www.timen.org/ TIMEN] '''Llorens, H., Derczynski, L., Gaizauskas, R. &amp; Saquete, E.''' (2012). [http://www.lrec-conf.org/proceedings/lrec2012/pdf/128_Paper.pdf TIMEN: An Open Temporal Expression Normalisation Resource]. ''In [http://www.lrec-conf.org/lrec2012/ LREC2012]: Proceedings of the 8th International Conference on Language Resources and Evaluation''. Istanbul, Turkey. May 23-25. || 2012 || LREC || T-Taggers ||
|-
| '''Chang, A., &amp; Manning, C.''' (2012). [http://www.lrec-conf.org/proceedings/lrec2012/pdf/284_Paper.pdf SUTIME: A Library for Recognizing and Normalizing Time Expressions]. ''In [http://www.lrec-conf.org/lrec2012/ LREC2012]: Proceedings of the 8th International Conference on Language Resources and Evaluation''. Istanbul, Turkey. May 23-25. || 2012 || LREC || T-Taggers ||
|-
| [http://dbs.ifi.uni-heidelberg.de/index.php?id=form-downloads HeidelTime] - '''Strötgen, J., &amp; Gertz, M.''' (2012). [http://www.springerlink.com/content/64767752451075k8/ Multilingual and cross-domain temporal tagging]. ''In [http://www.springerlink.com/content/1574-020x/ LRE]: Language Resources and Evaluation'', 1 - 30.|| 2012 || LRE || T-Taggers ||
|-
| [http://www.cs.man.ac.uk/~filannim/projects/tempeval-3/ ManTIME] - '''Filannino, M., Brown, G. &amp; Nenadic G.''' (2013). [http://www.aclweb.org/anthology/S/S13/S13-2.pdf#page=89 ManTIME: Temporal expression identification and normalization in the TempEval-3 challenge]. ''In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013)'', 53 - 57, Atlanta, Georgia, June 14-15, 2013.|| 2013 || ACL - SemEval || T-Taggers || [http://www.cs.man.ac.uk/~filannim/projects/tempeval-3/ online demo]
|}

== Temporal indexing (T-indexing) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Alonso, O., &amp; Gertz, M.''' (2006). [http://dl.acm.org/citation.cfm?id=1148170.1148273&amp;coll=DL&amp;dl=GUIDE&amp;CFID=102654836&amp;CFTOKEN=48651941 Clustering of Search Results using Temporal Attributes]. ''In [http://www.sigir.org/sigir2006/ SIGIR 2006]: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;597 – 598). Seattle, Washington, United States. August 6–11: ACM Press. || 2006 || SIGIR || T-Clustering ||
|-
| '''Berberich, K., Bedathur, S., Neumann, T., &amp; Weikum, G.''' (2007). [http://dl.acm.org/citation.cfm?id=1277831 A Time Machine for Text Search]. ''In [http://www.sigir.org/sigir2007 SIGIR 2007]: Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;519 – 526). Amsterdam, Netherlands. July 23–27: ACM Press. || 2007 || SIGIR || W-Archives ||
|-
| '''Jin, P., Lian, J., Zhao, X., &amp; Wan, S.''' (2008). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4739991 TISE: A Temporal Search Engine for Web Contents]. ''In IITA2008: Proceedings of the 2nd International Symposium on Intelligent Information Technology Application'' (pp.&amp;nbsp;220 – 224). Shanghai, China. December 21–22: IEEE Computer Society Press. || 2008 || IITA || T-SEngine ||
|-
| '''Song, S., &amp; JaJa, J.''' (2008). [http://www.umiacs.umd.edu/~joseph/temporal-web-archiving-final-umiacs-tr-2008-08.pdf Archiving Temporal Web Information: Organization of Web Contents for Fast Access and Compact Storage]. Technical Report UMIACS-TR-2008-08, University of Maryland Institute for Advanced Computer Studies, Maryland, MD, United States. || 2008 || Technical Report || W-Archives ||
|-
| '''Pasca, M.''' (2008). [http://dl.acm.org/citation.cfm?id=1363946 Towards Temporal Web Search]. ''In [http://www.acm.org/conferences/sac/sac2008/ SAC2008]: Proceedings of the 23rd ACM Symposium on Applied Computing'' (pp.&amp;nbsp;1117 – 1121). Fortaleza, Ceara, Brazil. March 16–20: ACM Press. || 2008 || SAC || T-QAnswering ||
|-
| '''Alonso, O., Gertz, M., &amp; Baeza-Yates, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1645953.1645968 Clustering and Exploring Search Results using Timeline Constructions]. ''In [http://www.comp.polyu.edu.hk/conference/cikm2009/ CIKM 2009]: Proceedings of the 18th International ACM Conference on Information and Knowledge Management''. Hong Kong, China. November 2–6: ACM Press. || 2009 || CIKM || T-Clustering ||
|-
| '''Arikan, I., Bedathur, S., &amp; Berberich, K.''' (2009). [http://www.wsdm2009.org/arikan_2009_temporal_expressions.pdf Time Will Tell: Leveraging Temporal Expressions in IR]. ''In [http://wsdm2009.org/ WSDM 2009]: Proceedings of the 2nd ACM International Conference on Web Search and Data Mining''. Barcelona, Spain. February 9–12: ACM Press. || 2009 || WSDM ||| T-RModels ||
|-
| '''Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., &amp; Zaragoza, H.''' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. ''In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval'', (pp.&amp;nbsp;41 – 44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||
|-
| '''Anand, A., Bedathur, S., Berberich, K., &amp; Schenkel, R.''' (2010). [http://dl.acm.org/citation.cfm?id=1871437.1871528 Efficient temporal keyword search over versioned text]. ''In [http://www.yorku.ca/cikm10/ CIKM2010]: Proceedings of the 19th ACM international conference on Information and knowledge management'', (pp.&amp;nbsp;699-708). Toronto, Canada. October 26-30. ACM Press. || 2010 || CIKM || W-Archives||
|-
| '''Anand, A., Bedathur, S., Berberich, K., &amp; Schenkel, R.''' (2011). [http://dl.acm.org/citation.cfm?id=2009991 Temporal index sharding for space-time efficiency in archive search]. ''In [http://www.sigir.org/sigir2011/ SIGIR2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'', (pp.&amp;nbsp;545-554). Beijing, China. July 24-28. ACM Press. || 2011 || SIGIR || T-Indexing||
|-
| '''Anand, A., Bedathur, S., Berberich, K., &amp; Schenkel, R.''' (2012). [http://dl.acm.org/citation.cfm?id=2348318 Index Maintenance for Time-Travel Text Search]. ''In [http://www.sigir.org/sigir2012/ SIGIR2012]: Proceedings of the 35th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'', (pp.&amp;nbsp;235 – 243). Portland, United States. August 12-16. ACM Press. || 2012 || SIGIR || W-Archives ||
|}

== Temporal query understanding (TQ-understanding) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Vlachos, M., Meek, C., Vagena, Z., &amp; Gunopulos, D.''' (2004). [http://portal.acm.org/citation.cfm?id=1007586 Identifying Similarities, Periodicities and Bursts for Online Search Queries]]. In [http://www09.sigmod.org/sigmod04/eproceedings/ SIGMOD2004]: Proceedings of the International Conference on Management of Data (pp.&amp;nbsp;131 – 142). Paris, France. June 13–18: ACM Press. || 2004 || SIGMOD || T-Dynamics ||
|-
| '''Beitzel, S. M., Jensen, E. C., Chowdhury, A., Frieder, O., &amp; Grossman, D.''' (2007). [http://dl.acm.org/citation.cfm?id=1190282 Temporal analysis of a very large topically categorized Web query log]]. ''In [http://www.asis.org/jasist.html JASIST]: Journal of the American Society for Information Science and Technology'', 58(2), 166 - 178. || 2007 || JASIST || T-Dynamics ||
|-
| '''Jones, R., &amp; Diaz, F.''' (2007). [http://dl.acm.org/citation.cfm?id=1247720 Temporal Profiles of Queries]]. ''In [http://tois.acm.org/ TOIS: ACM Transactions on Information Systems]'', 25(3). Article No.: 14. || 2007 || TOIS || TQ-Understanding ||
|-
| '''Dakka, W., Gravano, L., &amp; Ipeirotis, P. G.''' (2008). [http://dl.acm.org/citation.cfm?id=1458320 Answering General Time Sensitive Queries]]. ''In [http://www.cikm2008.org/ CIKM 2008]: Proceedings of the 17th International ACM Conference on Information and Knowledge Management'' (pp.&amp;nbsp;1437 – 1438). Napa Valley, California, United States. October 26–30: ACM Press. || 2008 || CIKM || TQ-Understanding ||
|-
| '''Diaz, F.''' (2009). [http://dl.acm.org/citation.cfm?id=1498825 Integration of News Content into Web Results]. ''In [http://wsdm2009.org/ WSDM2009]: Proceedings of the 2nd ACM International Conference on Web Search and Data Mining'' (pp.&amp;nbsp;182 – 191). Barcelona, Spain. February 9–12: ACM Press. || 2009 || WSDM || TQ-Understanding ||
|-
| '''Metzler, D., Jones, R., Peng, F., &amp; Zhang, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1572085 Improving Search Relevance for Implicitly Temporal Queries]. ''In [http://www.sigir2009.org/ SIGIR2009]: Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;700 – 701). Boston, MA, United States. July 19–23: ACM Press. || 2009 || SIGIR || TQ-Understanding ||
|-
| '''König, A.''' (2009). [http://dl.acm.org/citation.cfm?id=1572002 Click-Through Prediction for News Queries]. ''In [http://www.sigir2009.org/ SIGIR2009]: Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;347 – 354). Boston, MA, United States. July 19–23: ACM Press. || 2009 || SIGIR || TQ-Understanding ||
|-
| '''Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., &amp; Yamada, K.''' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. ''In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication'' (pp.&amp;nbsp;166 – 175). Suwon, Republic of Korea. January 14–15: ACM Press. || 2010 || ICIUMC || T-SEngine ||
|-
| '''Dong, A., Chang, Y., Zheng, Z., Mishne, G., Bai, J., Zhang, R., et al.''' (2010). [http://dl.acm.org/citation.cfm?id=1718490 Towards Recency Ranking in Web Search]. In [http://www.wsdm-conference.org/2010/ WSDM2010]: ''In Proceedings of the 3rd ACM International Conference on Web Search and Data Mining'' (pp.&amp;nbsp;11 – 20). New York, United States. February 3–6: ACM Press. || 2010 || WSDM || T-RModels ||
|-
| '''Kanhabua, N., &amp; Nørvåg, K.''' (2010). [http://dl.acm.org/citation.cfm?id=1887796 Determining Time of Queries for Re-Ranking Search Results]. ''In [http://www.ecdl2010.org/ ECDL2010]: Proceedings of The European Conference on Research and Advanced Technology for Digital Libraries''. Glasgow, Scotland. September 6–10: Springer Berlin / Heidelberg. || 2010 || ECDL || TQ-Understanding ||
|-
| '''Zhang, R., Konda, Y., Dong, A., Kolari, P., Chang, Y., &amp; Zheng, Z.''' (2010). [http://dl.acm.org/citation.cfm?id=1870768 Learning Recurrent Event Queries for Web Search]. ''In [http://www.lsi.upc.edu/events/emnlp2010/ EMNLP2010]: Proceedings of the Conference on Empiral Methods in Natural Language Processing'' (pp.&amp;nbsp;1129 – 1139). Massachusetts, United States. October 9–11: Association for Computational Linguistics. || 2010 || EMNLP || TQ-Understanding ||
|-
| '''Kulkarni, A., Teevan, J., Svore, K. M., &amp; Dumais, S. T.''' (2011). [http://portal.acm.org/citation.cfm?id=1935862 Understanding Temporal Query Dynamics]]. ''In [http://www.wsdm2011.org/ WSDM2011]: In Proceedings of the 4th ACM International Conference on Web Search and Data Mining'' (pp.&amp;nbsp;167 – 176). Hong Kong, China. February 9–12: ACM Press. || 2011 || WSDM || T-Dynamics ||
|-
| '''Campos, R.''' (2011). [http://dl.acm.org/citation.cfm?id=2010182 Using k-top Retrieved Web Snippets to Date Temporal Implicit Queries based on Web Content Analysis]. ''In [http://www.sigir2011.org/%20 SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (p.&amp;nbsp;1325). Beijing, China. July 24–28.: ACM Press. || 2011 || SIGIR || TQ-Understanding ||
|-
| '''Campos, R., Jorge, A., &amp; Dias, G.''' (2011). [http://ciir.cs.umass.edu/sigir2011/qru/campos+al.pdf Using Web Snippets and Query-logs to Measure Implicit Temporal Intents in Queries]. ''In [http://ciir.cs.umass.edu/sigir2011/qru/ QRU 2011]: Proceedings of the Query Representation and Understanding Workshop associated to [http://www.sigir2011.org/ SIGIR2011]: 34th Annual International ACM SIGIR 2005 Conference on Research and Development in Information Retrieval'', (pp.&amp;nbsp;13 – 16). Beijing, China. July 28. || 2011 || SIGIR - QRU || T-Dynamics ||
|-
| '''Shokouhi, M.''' (2011). [http://dl.acm.org/citation.cfm?id=2010104 Detecting Seasonal Queries by Time-Series Analysis]. ''In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;1171 – 1172). Beijing, China. July 24–28.: ACM Press. || 2011 || SIGIR || TQ-Understanding ||
|-
| '''Campos, R., Dias, G., Jorge, A., &amp; Nunes, C.''' (2012). [http://dl.acm.org/citation.cfm?id=2169103&amp;CFID=102654836&amp;CFTOKEN=48651941 Enriching Temporal Query Understanding through Date Identification: How to Tag Implicit Temporal Queries?] ''In [http://www.temporalweb.net/ TWAW 2012]: Proceedings of the 2nd International Temporal Web Analytics Workshop associated to [http://www2012.wwwconference.org/ WWW2012]: 20th International World Wide Web Conference'' (pp.&amp;nbsp;41 – 48). Lyon, France. April 17.: ACM - DL. || 2012 || WWW - TWAW || TQ-Understanding ||
|-
| '''Shokouhi, M., &amp; Radinsky, K.''' (2012). [http://dl.acm.org/citation.cfm?id=2348364 Time-Sensitive Query Auto-Completion]. ''In [http://www.sigir.org/sigir2012/ SIGIR 2012]: Proceedings of the 35th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;601 – 610). Portland, United States. August 12–16.: ACM Press. || 2012 || SIGIR || TQ-Understanding ||
|-
| '''Campos, R., Dias, G., Jorge, A., &amp; Nunes, C.''' (2012). [http://dl.acm.org/citation.cfm?id=2398567&amp;dl=ACM&amp;coll=DL&amp;CFID=204979644&amp;CFTOKEN=99312511 GTE: A Distributional Second-Order Co-Occurrence Approach to Improve the Identification of Top Relevant Dates] ''In [http://www.cikm2012.org/ CIKM 2012]: Proceedings of the 21st ACM Conference on Information and Knowledge Management'' (pp.&amp;nbsp;2035 – 2039). Maui, Hawaii, United States. October 29 - November 02.: ACM Press. || 2012 || CIKM || TQ-Understanding ||
|-
| '''Campos, R., Jorge, A., Dias, G., &amp; Nunes, C.''' (2012). [http://dl.acm.org/citation.cfm?id=2457524.2457656 Disambiguating Implicit Temporal Queries by Clustering Top Relevant Dates in Web Snippets] ''In [http://www.fst.umac.mo/wic2012/WI/ WIC 2012]: Proceedings of the 2012 IEEE/WIC/ACM International Joint Conferences on Web Intelligence and Intelligent Agent Technology,'' Vol. 1, (pp.&amp;nbsp;1 – 8). Macau, China. December 04-07. || 2012 || WIC || T-Clustering ||
|}

== Time-aware retrieval/ranking models (T-RModels) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Li, X., &amp; Croft, B. W.''' (2003). [http://dl.acm.org/citation.cfm?doid=956863.956951 Time-Based Language Models]. ''In CIKM 2003: Proceedings of the 12th International ACM Conference on Information and Knowledge Management'' (pp.&amp;nbsp;469 – 475). New Orleans, Louisiana, United States. November 2–8: ACM Press. || 2003 || CIKM || T-RModels ||
|-
| '''Sato, N., Uehara, M., &amp; Sakai, Y.''' (2003). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?reload=true&amp;arnumber=1232026&amp;contentType=Conference+Publications Temporal Information Retrieval in Cooperative Search Engine]. ''In [http://www.dexa.org/previous/dexa2003/cfp/dexa.html DEXA2003]: Proceedings of the 14th International Workshop on Database and Expert Systems Applications'' (pp.&amp;nbsp;215 – 220). Prague, Czech Republic. September 1–5: IEEE. || 2003 || DEXA || T-RModels ||
|-
| '''Berberich, K., Vazirgiannis, M., &amp; Weikum, G.''' (2005). [http://projecteuclid.org/DPubS?verb=Display&amp;version=1.0&amp;service=UI&amp;handle=euclid.im/1150474885&amp;page=record Time-Aware Authority Ranking]. ''In [http://www.tandf.co.uk/journals/journal.asp?issn=1542-7951&amp;linktype=44 IM: Internet Mathematics]'', 2(3), 301 - 332. || 2005 || IM || T-RModels ||
|-
| '''Cho, J., Roy, S., &amp; Adams, R.''' (2005). [http://dl.acm.org/citation.cfm?id=1066220 Page Quality: In Search of an Unbiased Web Ranking]. In [http://cimic.rutgers.edu/~sigmod05/ SIGMOD2005]: Proceedings of the International Conference on Management of Data (pp.&amp;nbsp;551 – 562). Baltimore, United States. June 13–16: ACM Press. || 2005 || SIGMOD || T-RModels ||
|-
| '''Perkiö, J., Buntine, W., &amp; Tirri, H.''' (2005). [http://dl.acm.org/citation.cfm?id=1076171 A Temporally Adaptative Content-Based Relevance Ranking Algorithm]. ''In [http://www.dcc.ufmg.br/eventos/sigir2005/ SIGIR 2005]: Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;647 – 648). Salvador, Brazil. August 15–16: ACM Press. || 2005 || SIGIR || T-RModels ||
|-
| '''Jones, R., &amp; Diaz, F.''' (2007). [http://dl.acm.org/citation.cfm?id=1247720 Temporal Profiles of Queries]. ''In [http://tois.acm.org/ TOIS: ACM Transactions on Information Systems]'', 25(3). Article No.: 14. || 2007 || TOIS || TQ-Understanding ||
|-
| '''Pasca, M.''' (2008). [http://dl.acm.org/citation.cfm?id=1363946 Towards Temporal Web Search]. ''In [http://www.acm.org/conferences/sac/sac2008/ SAC2008]: Proceedings of the 23rd ACM Symposium on Applied Computing'' (pp.&amp;nbsp;1117 – 1121). Fortaleza, Ceara, Brazil. March 16–20: ACM Press. || 2008 || SAC || T-QAnswering ||
|-
| '''Dakka, W., Gravano, L., &amp; Ipeirotis, P. G.''' (2008). [http://dl.acm.org/citation.cfm?id=1458320 Answering General Time Sensitive Queries]. ''In [http://www.cikm2008.org/ CIKM 2008]: Proceedings of the 17th International ACM Conference on Information and Knowledge Management'' (pp.&amp;nbsp;1437 – 1438). Napa Valley, California, United States. October 26–30: ACM Press. || 2008 || CIKM || TQ-Understanding ||
|-
| '''Jin, P., Lian, J., Zhao, X., &amp; Wan, S.''' (2008). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4739991 TISE: A Temporal Search Engine for Web Contents]. ''In IITA2008: Proceedings of the 2nd International Symposium on Intelligent Information Technology Application'' (pp.&amp;nbsp;220 – 224). Shanghai, China. December 21–22: IEEE Computer Society Press. || 2008 || IITA || T-SEngine ||
|-
| '''Arikan, I., Bedathur, S., &amp; Berberich, K.''' (2009). [http://www.wsdm2009.org/arikan_2009_temporal_expressions.pdf Time Will Tell: Leveraging Temporal Expressions in IR]. ''In [http://wsdm2009.org/ WSDM 2009]: Proceedings of the 2nd ACM International Conference on Web Search and Data Mining''. Barcelona, Spain. February 9–12: ACM Press. || 2009 || WSDM ||| T-RModels ||
|-
| '''Zhang, R., Chang, Y., Zheng, Z., Metzler, D., &amp; Nie, J.-y.''' (2009). [http://dl.acm.org/citation.cfm?id=1620899 Search Result Re-ranking by Feedback Control Adjustment for Time-sensitive Query]. ''In [http://www.naaclhlt2009.org/ NAACL2009]: Proceedings of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies'', (pp.&amp;nbsp;165 – 168). Boulder, Colorado, United States. May 31 - June 5. || 2009 || NAACL || T-RModels ||
|-
| '''Metzler, D., Jones, R., Peng, F., &amp; Zhang, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1572085 Improving Search Relevance for Implicitly Temporal Queries]. ''In [http://www.sigir2009.org/ SIGIR 2009]: Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;700 – 701). Boston, MA, United States. July 19–23: ACM Press. || 2009 || SIGIR || TQ-Understanding ||
|-
| '''Alonso, O., Gertz, M., &amp; Baeza-Yates, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1645953.1645968 Clustering and Exploring Search Results using Timeline Constructions]. ''In [http://www.comp.polyu.edu.hk/conference/cikm2009/ CIKM 2009]: Proceedings of the 18th International ACM Conference on Information and Knowledge Management''. Hong Kong, China. November 2–6: ACM Press. || 2009 || CIKM || T-Clustering ||
|-
| '''Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., &amp; Yamada, K.''' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. ''In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication'' (pp.&amp;nbsp;166 – 175). Suwon, Republic of Korea. January 14–15: ACM Press. || 2010 || ICIUMC || T-SEngine ||
|-
| '''Elsas, J. L., &amp; Dumais, S. T.''' (2010). [http://dl.acm.org/citation.cfm?id=1718489 Leveraging Temporal Dynamics of Document Content in Relevance Ranking]. ''In [http://www.wsdm-conference.org/2010/ WSDM10]: Third ACM International Conference on Web Search and Data Mining'' (pp.&amp;nbsp;1 – 10). New York, United States. February 3–06: ACM Press. || 2010 || WSDM || T-Dynamics ||
|-
| '''Aji, A., Wang, Y., Agichtein, E., Gabrilovich, E.''' (2010). [http://dl.acm.org/citation.cfm?id=1871519 Using the Past to Score the Present: Extending Term Weighting Models Through Revision History Analysis] ''In [http://www.cikm2010.org/ CIKM 2010]: Proceedings of the 19th ACM Conference on Information and Knowledge Management'' (pp.&amp;nbsp;629 – 638). Toronto, ON, Canada. October 26 - October 30: ACM Press. || 2010 || CIKM || T-RModels ||
|-
| '''Dong, A., Chang, Y., Zheng, Z., Mishne, G., Bai, J., Zhang, R., et al.''' (2010). [http://dl.acm.org/citation.cfm?id=1718490 Towards Recency Ranking in Web Search]. In [http://www.wsdm-conference.org/2010/ WSDM2010]: ''In Proceedings of the 3rd ACM International Conference on Web Search and Data Mining'' (pp.&amp;nbsp;11 – 20). New York, United States. February 3–6: ACM Press. || 2010 || WSDM || T-RModels ||
|-
| '''Berberich, K., Bedathur, S., Alonso, O., &amp; Weikum, G.''' (2010). [http://www.springerlink.com/content/b193008160713350/ A Language Modeling Approach for Temporal Information Needs]. In C. Gurrin, Y. He, G. Kazai, U. Kruschwitz, S. Little, T. Roelleke, et al. (Eds.), ''In Lecture Notes in Computer Science - Research and Advanced Technology for Digital Libraries, [http://kmi.open.ac.uk/events/ecir2010/ ECIR 2010]: 32nd European Conference on Information Retrieval'' (Vol. 5993/2010, pp.&amp;nbsp;13 – 25). Milton Keynes, UK. March 28–31: Springer Berlin / Heidelberg. || 2010 || ECIR || T-RModels ||
|-
| '''Dong, A., Zhang, R., Kolari, P., Jing, B., Diaz, F., Chang, Y., Zheng, Z., &amp; Zha, H.''' (2010). [http://dl.acm.org/citation.cfm?id=1772725&amp;dl=ACM&amp;coll=DL&amp;CFID=204979644&amp;CFTOKEN=99312511 Time is of the Essence: Improving Recency Ranking Using Twitter Data]. ''In [http://www2010.org/www/index.html WWW2010]: Proceedings of the 19th International World Wide Web Conference'' (pp.&amp;nbsp;331 – 340). Raleigh, United States. April 26–30: ACM Press. || 2010 || WWW || T-RModels ||
|-
| '''Inagaki, Y., Sadagopan, N., Dupret, G., Dong, A., Liao, C., Chang, Y., &amp; Zheng, Z.''' (2010). [http://labs.yahoo.com/files/aaai10_recencyfeature_2.pdf Session Based Click Features for Recency Ranking]. ''In [http://www.aaai.org/Conferences/AAAI/aaai10.php AAAI2010]: Proceedings of the 24th AAAI Conference on Artificial Intelligence'' (pp.&amp;nbsp;331 – 340). Atlanta, United States. June 11–15: AAAI Press. || 2010 || AAAI || T-RModels ||
|-
| '''Dai, N., &amp; Davison, B.''' (2010). [http://dl.acm.org/citation.cfm?id=1835471 Freshness Matters: In Flowers, Food, and Web Authority]. ''In [http://www.sigir2010.org/doku.php SIGIR 2010]: Proceedings of the 33rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;114 – 121). Geneve, Switzerland. July 19–23: ACM Press. || 2010 || SIGIR || T-RModels ||
|-
| '''Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., &amp; Zaragoza, H.''' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. ''In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval'', (pp.&amp;nbsp;41 – 44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||
|-
| '''Kanhabua, N., &amp; Nørvåg, K.''' (2010). [http://dl.acm.org/citation.cfm?id=1887796 Determining Time of Queries for Re-Ranking Search Results]. ''In [http://www.ecdl2010.org/ ECDL2010]: Proceedings of The European Conference on Research and Advanced Technology for Digital Libraries''. Glasgow, Scotland. September 6–10: Springer Berlin / Heidelberg. || 2010 || ECDL || TQ-Understanding ||
|-
| '''Efron, M., &amp; Golovchinsky, G.''' (2011). [http://dl.acm.org/citation.cfm?id=2009916.2009984 Estimation Methods for Ranking Recent Information]. ''In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;495 – 504). Beijing, China. July 24–28.: ACM Press. || 2011 || SIGIR || T-RModels ||
|-
| '''Dai, N., Shokouhi, M., &amp; Davison, B. D.''' (2011). [http://dl.acm.org/citation.cfm?id=2009916.2009933 Learning to Rank for Freshness and Relevance]. ''In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;95 – 104). Beijing, China. July 24–28.: ACM Press. || 2011 || SIGIR || T-RModels ||
|-
| '''Kanhabua, N., Blanco, R., &amp; Matthews, M.''' (2011). [http://dl.acm.org/citation.cfm?id=2010018&amp;dl=ACM&amp;coll=DL&amp;CFID=102654836&amp;CFTOKEN=48651941 Ranking Related News Predictions]. ''In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;755 – 764). Beijing, China. July 24–28: ACM Press. || 2011 || SIGIR || F-IRetrieval ||
|-
| '''Chang, P-T., Huang, Y-C., Yang, C-L., Lin, S-D., &amp; Cheng, P-J.''' (2012). [http://dl.acm.org/citation.cfm?id=2348489 Learning-Based Time-Sensitive Re-Ranking for Web Search]. ''In Proceedings of the [http://www.sigir.org/sigir2012/ SIGIR2012]: 35th Annual International ACM SIGIR 2012 Conference on Research and Development in Information Retrieval'', (pp.&amp;nbsp;1101 – 1102). Portland, United States. August 12 - 16. || 2012 || SIGIR || T-RModels ||
|-
| '''Efron, M.''' (2012). [http://research.microsoft.com/en-us/people/milads/efrontemporalwsv02.pdf Query-Specific Recency Ranking: Survival Analysis for Improved Microblog Retrieval]. ''In [http://research.microsoft.com/en-us/people/milads/taia2012.aspx TAIA 2012]: Proceedings of the Time-Aware Information Access Workshop associated to [http://www.sigir.org/sigir2012/ SIGIR2012]: 35th Annual International ACM SIGIR 2012 Conference on Research and Development in Information Retrieval''. Portland, United States. August 16. || 2012 || SIGIR - TAIA || T-RModels ||
|-
| '''Kanhabua, N., &amp; Nørvåg, K.''' (2012). [http://dl.acm.org/citation.cfm?id=2398667 Learning to Rank Search Results for Time-Sensitive Queries] ''In [http://www.cikm2012.org/ CIKM 2012]: Proceedings of the 21st ACM Conference on Information and Knowledge Management'' (pp.&amp;nbsp;2463 – 2466). Maui, Hawaii, United States. October 29 - November 02.: ACM Press. || 2012 || CIKM || T-RModels ||
|-
| '''Kim G., and Xing E. P.''' (2013). [http://dl.acm.org/citation.cfm?id=2433417 Time-Sensitive Web Image Ranking and Retrieval via Dynamic Multi-Task Regression]. ''In [http://wsdm2013.org/ WSDM2013]: Proceedings of the 6th ACM International Conference on Web Search and Data Mining'' (pp.&amp;nbsp;163 – 172). Rome, Italy. February 4–8: ACM Press. || 2013 || WSDM || T-IRetrieval ||
|-
| '''Costa, M., &amp; Silva, M. J., &amp; Couto, F. M.''' (2014). [http://dl.acm.org/citation.cfm?id=2609619 Learning Temporal-Dependent Ranking Models]. ''In Proceedings of the [http://sigir.org/sigir2014/ SIGIR2014]: 37th Annual ACM SIGIR Conference'' (pp.&amp;nbsp;757--766). Gold Coast, Australia. July 6–11: ACM Press. || 2014 || SIGIR || T-RModels ||
|}

== Temporal clustering (T-clustering) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Shaparenko, B., Caruana, R., Gehrke, J., &amp; Joachims, T.''' (2005). [http://www.cs.cornell.edu/people/tj/publications/shaparenko_etal_05a.pdf Identifying Temporal Paterns and Key Players in Document Collections]. ''In [http://users.cis.fiu.edu/~taoli/workshop/TDM2005/index.html TDM2005]: Proceedings of the Workshop on Temporal Data Mining associated to [http://www.cacs.louisiana.edu/~icdm05/ ICDM2005]'' (pp.&amp;nbsp;165 – 174). Houston, United States. November 27–30: IEEE Press. || 2005 || ICDM - TDM || TDT ||
|-
| '''Alonso, O., &amp; Gertz, M.''' (2006). [http://dl.acm.org/citation.cfm?id=1148170.1148273&amp;coll=DL&amp;dl=GUIDE&amp;CFID=102654836&amp;CFTOKEN=48651941 Clustering of Search Results using Temporal Attributes]. ''In [http://www.sigir.org/sigir2006/ SIGIR 2006]: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;597 – 598). Seattle, Washington, United States. August 6–11: ACM Press. || 2006 || SIGIR || T-Clustering ||
|-
| '''Mori, M., Miura, T., &amp; Shioya, I.''' (2006). [http://dl.acm.org/citation.cfm?id=1249137 Topic Detection and Tracking for News Web Pages]. ''In [http://www.comp.hkbu.edu.hk/~wii06/wi/ WIC2006]: IEEE Main Conference Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence'' (pp.&amp;nbsp;338 – 342). Hong Kong, China. December 18–22: IEEE Computer Society Press. || 2006 || WIC || TDT ||
|-
| '''Alonso, O., Baeza-Yates, R., &amp; Gertz, M.''' (2007). Exploratory Search Using Timelines. ''In ESCHI: Proceedings of the Workshop on Exploratory Search and Computer Human Interaction associated to [http://www.chi2007.org/ CHI2007]: [http://research.microsoft.com/en-us/um/people/ryenw/esi/acceptedposters.html SIGCHI] Conference on Human Factors in Computing Systems''. San Jose, CA, United States. April 29: ACM Press. || 2007 || CHI - ESCHI || T-SEngine ||
|-
| '''Jatowt, A., Kawai, H., Kanazawa, K., Tanaka, K., &amp; Kunieda, K.''' (2009). [http://dl.acm.org/citation.cfm?id=1555420 Supporting Analysis of Future-Related Information in News Archives and the Web]. ''In [http://www.jcdl2009.org JCDL2009]: Proceedings of the Joint Conference on Digital Libraries'' (pp.&amp;nbsp;115 – 124). Austin, United States. June 15–19.: ACM Press. || 2009 || JCDL || F-IRetrieval ||
|-
| '''Campos, R., Dias, G., &amp; Jorge, A.''' (2009). [http://www.ccc.ipt.pt/~ricardo/ficheiros/KDIR2009.pdf Disambiguating Web Search Results By Topic and Temporal Clustering: A Proposal]. In [http://www.kdir.ic3k.org/ KDIR2009]: Proceedings of the International Conference on Knowledge Discovery and Information Retrieval, (pp.&amp;nbsp;292 – 296). Funchal - Madeira, Portugal. October 6–8. || 2009 || KDIR || T-Clustering ||
|-
| '''Alonso, O., Gertz, M., &amp; Baeza-Yates, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1645953.1645968 Clustering and Exploring Search Results using Timeline Constructions]. ''In [http://www.comp.polyu.edu.hk/conference/cikm2009/ CIKM 2009]: Proceedings of the 18th International ACM Conference on Information and Knowledge Management''. Hong Kong, China. November 2–6: ACM Press. || 2009 || CIKM || T-Clustering ||
|-
| '''Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., &amp; Yamada, K.''' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. ''In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication'' (pp.&amp;nbsp;166 – 175). Suwon, Republic of Korea. January 14–15: ACM Press. || 2010 || ICIUMC || T-SEngine ||
|-
| '''Jatowt, A., &amp; Yeung, C. M.''' (2011). [http://dl.acm.org/citation.cfm?id=2063759 Extracting Collective Expectations about the Future from Large Text Collections]. ''In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management'' (pp.&amp;nbsp;1259 – 1264). Glasgow, Scotland, UK. October: ACM Press. || 2011 || CIKM || F-IRetrieval ||
|-
| '''Campos, R., Jorge, A., Dias, G., &amp; Nunes, C.''' (2012). [http://dl.acm.org/citation.cfm?id=2457524.2457656 Disambiguating Implicit Temporal Queries by Clustering Top Relevant Dates in Web Snippets] ''In [http://www.fst.umac.mo/wic2012/WI/ WIC 2012]: Proceedings of the 2012 IEEE/WIC/ACM International Joint Conferences on Web Intelligence and Intelligent Agent Technology,'' Vol. 1, (pp.&amp;nbsp;1 – 8). Macau, China. December 04-07. || 2012 || WIC || T-Clustering ||
|}

== Temporal text classification (T-classification) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Jong, F., Rode, H., &amp; Hiemstra, D.''' (2006). [http://doc.utwente.nl/66448/ Temporal Language Models for the Disclosure of Historical Text]. ''In [http://www.dans.knaw.nl/en AHC2005]: Proceedings of the XVIth International Conference of the Association for History and Computing'' (pp.&amp;nbsp;161 – 168). Amsterdam, Netherlands. September 14–17 || 2005 || AHC || T-Classification ||
|-
| '''Toyoda, M., &amp; Kitsuregawa, M.''' (2006). [http://dl.acm.org/citation.cfm?id=1135777.1135815 What's Really New on the Web? Identifying New Pages from a Series of Unstable Web Snapshots]. ''In [http://www2006.org WWW2006]: Proceedings of the 15th International World Wide Web Conference'' (pp.&amp;nbsp;233 – 241). Edinburgh, Scotland. May 23–26: ACM Press. || 2006 || WWW || T-Classification ||
|-
| '''Nunes, S., Ribeiro, C., &amp; David, G.''' (2007). [http://dl.acm.org/citation.cfm?id=1316924 Using Neighbors to Date Web Documents]. ''In [http://workshops.inf.ed.ac.uk/WIDM2007/ WIDM2007]: Proceedings of the 9th ACM International Workshop on Web Information and Data Management associated to [[www2.fc.ul.pt/cikm2007|CIKM2007]]: 16th International Conference on Knowledge and Information Management'' (pp.&amp;nbsp;129 – 136). Lisboa, Portugal. November 9: ACM Press. || 2007 || CIKM - WIDM || T-Classification ||
|-
| '''Jatowt, A., Kawai, Y., &amp; Tanaka, K.''' (2007). [http://dl.acm.org/citation.cfm?id=1316925 Detecting Age of Page Content]. ''In [http://workshops.inf.ed.ac.uk/WIDM2007/ WIDM2007]: Proceedings of the 8th International Workshop on Web Information and Data Management associated to [http://www2.fc.ul.pt/cikm2007 CIKM2007]: 16th International Conference on Knowledge and Information Management'' (pp.&amp;nbsp;137 – 144). Lisbon. Portugal. November 9.: ACM Press. || 2007 || CIKM - WIDM || T-Classification ||
|-
| '''Kanhabua, N., &amp; Nørvåg, K.''' (2008). [http://dl.acm.org/citation.cfm?id=1429902 Improving Temporal Language Models for Determining Time of Non-timestamped Documents]. ''In Christensen-Dalsgaard, B., Castelli, D., Jurik, B. A., Lippincott, J. (Eds.), In Lecture Notes in Computer Science - Research and Advanced Technology for Digital Libraries, [http://www.ecdl2008.org/ ECDL 2008]: 12th European Conference on Research and Advances Technology for Digital Libraries'' (Vol. 5173/2008, pp.&amp;nbsp;358 – 370). Aarhus, Denmark. September 14–19: Springer Berlin / Heidelberg. || 2008 || ECDL || T-Classification ||
|-
| '''Jatowt, A., &amp; Yeung, C. M.''' (2011). [http://dl.acm.org/citation.cfm?id=2063759 Extracting Collective Expectations about the Future from Large Text Collections]. ''In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management'' (pp.&amp;nbsp;1259 – 1264). Glasgow, Scotland, UK. October: ACM Press. || 2011 || CIKM || F-IRetrieval ||
|-
| '''Strötgen, J., Alonso, O., &amp; Gertz, M.''' (2012). [http://dl.acm.org/citation.cfm?id=2169095.2169102&amp;coll=DL&amp;dl=GUIDE&amp;CFID=102654836&amp;CFTOKEN=48651941 Identification of Top Relevant Temporal Expressions in Documents]. ''In [http://www.temporalweb.net/ TWAW 2012]: Proceedings of the 2nd International Temporal Web Analytics Workshop associated to [http://www2012.wwwconference.org/ WWW2012]: 20th International World Wide Web Conference'' (pp.&amp;nbsp;33 – 40). Lyon, France. April 17: ACM - DL. || 2012 || WWW - TWAW || T-Classification ||
|-
| '''Filannino, M., and Nenadic, G.''' (2014). [http://www.aclweb.org/anthology/W/W14/W14-4502.pdf Mining temporal footprints from Wikipedia]. ''In Proceedings of the First AHA!-Workshop on Information Discovery in Text'' (Dublin, Ireland, August 2014), Association for Computational Linguistics and Dublin City University, pp. 7–13. || 2014 || COLING || T-Classification || [http://www.cs.man.ac.uk/~filannim/projects/temporal_footprints/ online demo]
|}

== Temporal visualization (T-interfaces) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Swan, R., &amp; Allan, J.''' (2000). [http://dl.acm.org/citation.cfm?id=345546 Automatic Generation of Overview Timelines]. ''In [http://www.aueb.gr/conferences/sigir2000/ SIGIR 2000]: Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;49 – 56). Athens, Greece. July 24–28: ACM Press. || 2000 || SIGIR || TDT ||
|-
| '''Swan, R., &amp; Jensen, D.''' (2000). [http://www.cs.cmu.edu/~dunja/.../Swan_TM.pdf TimeMines: Constructing Timelines with Statistical Models of Word Usage]. ''In M. Grobelnik, D. Mladenic, &amp; N. Milic-Frayling (Ed.), [http://www.cs.cmu.edu/~dunja/WshKDD2000.html TM2000]: Proceedings of the Workshop on Text Mining associated to [http://www.sigkdd.org/kdd2000/ KDD2000]: 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining'' (pp.&amp;nbsp;73 – 80). Boston, Massachusetts, United States. August 20–23: ACM Press. || 2000 || KDD - TM || TDT ||
|-
| [https://books.google.com/ngrams Google Ngram Viewer] ||  ||  || T-Interfaces ||
|-
| '''Cousins, S., &amp; Kahn, M.''' (1991). [http://www.sciencedirect.com/science/article/pii/093336579190005V The Visual Display of Temporal Information]. (E. Keravnou, Ed.) ''In AIM: Artificial Intelligence in Medicine'', 3(6), 341 - 357. || 1991 || AIM || T-Interfaces ||
|-
| '''Karam, G. M.''' (1994). [http://dl.acm.org/citation.cfm?id=187157 Visualization Using Timelines]. In T. J. Ostrand (Ed.), ''ISSTA1994: Proceedings of the International Symposium on Software Testing and Analysis associated to SIGSOFT: ACM Special Interest Group on Software Engineering'' (pp.&amp;nbsp;125 – 137). Seattle, Washington, United States. August 17–19: ACM Press. || 1994 || ISSTA || T-Interfaces ||
|-
| '''Plaisant, C., Miiash, B., Rose, A., Widoff, S., &amp; Shneiderman, B.''' (1996). [http://dl.acm.org/citation.cfm?id=238493 LifeLines: Visualizing Personal Histories]. ''In [http://www.sigchi.org/chi96/proceedings/index.htm CHI1996]: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems'' (pp.&amp;nbsp;221 – 227). Vancouver, British Columbia, Canada. April 13–18: ACM Press. || 1996 || CHI || T-Interfaces ||
|-
| '''Toyoda, M., &amp; Kitsuregawa, M.''' (2005). [http://dl.acm.org/citation.cfm?id=1083387 A System for Visualizing and Analyzing the Evolution of the Web with a Time Series of Graphs]. ''In [http://www.ht05.org HT2005]: Proceedings of the 16th ACM Conference on Hypertext and Hypermedia'' (pp.&amp;nbsp;151 – 160). Salzburg, Austria. September 6–9: ACM Press. || 2005 || HT || W-Archives ||
|-
| '''Efendioglu, D., Faschetti, C., &amp; Parr, T.''' (2006). [http://dl.acm.org/authorize?815487 Chronica: a temporal web search engine]. In ''D. Wolber, N. Calder, &amp; ,. C. Brooks (Ed.), [http://www.icwe2006.org/ ICWE2006]: Proceedings of the 6th International Conference on Web Engineering'' (pp.&amp;nbsp;119 – 120). Palo Alto, California, United States. July 11–14: ACM Press. || 2006 || ICWE || W-Archives ||
|-
| '''Catizone, R., Dalli, A., &amp; Wilks, Y.''' (2006). [http://www.lrec-conf.org/proceedings/lrec2006/pdf/702_pdf.pdf Evaluating Automatically Generated Timelines from the Web]. ''In [http://www.lrec-conf.org/lrec2006/ LREC2006]: Proceedings of the 5th International Conference on Language Resources and Evaluation''. Genoa, Italy. May 24–26: ELDA. || 2006 || LREC || T-Interfaces ||
|-
| '''Mori, M., Miura, T., &amp; Shioya, I.''' (2006). [http://dl.acm.org/citation.cfm?id=1249137 Topic Detection and Tracking for News Web Pages]. ''In [http://www.comp.hkbu.edu.hk/~wii06/wi/ WIC2006]: IEEE Main Conference Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence'' (pp.&amp;nbsp;338 – 342). Hong Kong, China. December 18–22: IEEE Computer Society Press. || 2006 || WIC || TDT ||
|-
| '''Alonso, O., Baeza-Yates, R., &amp; Gertz, M.''' (2007). Exploratory Search Using Timelines. ''In ESCHI: Proceedings of the Workshop on Exploratory Search and Computer Human Interaction associated to [http://www.chi2007.org/ CHI2007]: [http://research.microsoft.com/en-us/um/people/ryenw/esi/acceptedposters.html SIGCHI] Conference on Human Factors in Computing Systems''. San Jose, CA, United States. April 29: ACM Press. || 2007 || CHI - ESCHI || T-SEngine ||
|-
| '''Jatowt, A., Kawai, Y., &amp; Tanaka, K.''' (2008). [http://dl.acm.org/citation.cfm?id=1367497.1367736 Visualizing Historical Content of Web pages]]. ''In [http://www2008.org/ WWW2008]: Proceedings of the 17th International World Wide Web Conference'' (pp.&amp;nbsp;1221 – 1222). Beijing, China. April 21–25: ACM Press. || 2008 || WWW || W-Archives ||
|-
| '''Nunes, S., Ribeiro, C., &amp; David, G.''' (2008). [http://dl.acm.org/citation.cfm?id=1822292 WikiChanges - Exposing Wikipedia Revision Activity]. ''In [http://www.wikisym.org/ws2008/ WikiSym2008]: Proceedings of the 4th International Symposium on Wikis''. Porto, Portugal. September 8–10: ACM Press. || 2008 || WikiSym || T-Interfaces ||
|-
| '''Nunes, S., Ribeiro, C., &amp; David, G.''' (2009). [http://epia2009.web.ua.pt/onlineEdition/601.pdf Improving Web User Experience with Document Activity Sparklines]. ''In L. S. Lopes, N. Lau, P. Mariano, &amp; L. Rocha (Ed.), [http://epia2009.web.ua.pt EPIA2009]: Proceedings of the 14th Portuguese Conference on Artificial Intelligence associated to APPIA: Portuguese Association for Artificial Intelligence'', (pp.&amp;nbsp;601 – 604). Aveiro, Portugal. October 12–15. || 2009 || EPIA || T-Interfaces ||
|-
| '''Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., &amp; Yamada, K.''' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. ''In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication'' (pp.&amp;nbsp;166 – 175). Suwon, Republic of Korea. January 14–15: ACM Press. || 2010 || ICIUMC || T-SEngine ||
|-
| '''Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., &amp; Zaragoza, H.''' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. ''In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval'', (pp.&amp;nbsp;41 – 44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||
|-
| '''Khurana, U., Nguyen V., Cheng H., Ahn, J., Chen X., &amp; Shneiderman, B.''' (2011). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&amp;arnumber=6113166 Visual Analysis of Temporal Trends in Social Networks Using Edge Color Coding and Metric Timelines]. ''In [http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=6112285]: Proceedings of the IEEE Social Computing'', (pp.&amp;nbsp;549 – 554). Boston, United States. || 2011 || SocialCom || T-Interfaces ||
|}

== Temporal search engines (T-SEngine) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Alonso, O., &amp; Gertz, M.''' (2006). [http://dl.acm.org/citation.cfm?id=1148170.1148273&amp;coll=DL&amp;dl=GUIDE&amp;CFID=102654836&amp;CFTOKEN=48651941 Clustering of Search Results using Temporal Attributes]. ''In [http://www.sigir.org/sigir2006/ SIGIR 2006]: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;597 – 598). Seattle, Washington, United States. August 6–11: ACM Press. || 2006 || SIGIR || T-Clustering ||
|-
| '''Alonso, O., Baeza-Yates, R., &amp; Gertz, M.''' (2007). Exploratory Search Using Timelines. ''In ESCHI: Proceedings of the Workshop on Exploratory Search and Computer Human Interaction associated to [http://www.chi2007.org/ CHI2007]: [http://research.microsoft.com/en-us/um/people/ryenw/esi/acceptedposters.html SIGCHI] Conference on Human Factors in Computing Systems''. San Jose, CA, United States. April 29: ACM Press. || 2007 || CHI - ESCHI || T-SEngine ||
|-
| '''Jin, P., Lian, J., Zhao, X., &amp; Wan, S.''' (2008). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4739991 TISE: A Temporal Search Engine for Web Contents]. ''In IITA2008: Proceedings of the 2nd International Symposium on Intelligent Information Technology Application'' (pp.&amp;nbsp;220 – 224). Shanghai, China. December 21–22: IEEE Computer Society Press. || 2008 || IITA || T-SEngine ||
|-
| '''Alonso, O., Gertz, M., &amp; Baeza-Yates, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1645953.1645968 Clustering and Exploring Search Results using Timeline Constructions]. ''In [http://www.comp.polyu.edu.hk/conference/cikm2009/ CIKM 2009]: Proceedings of the 18th International ACM Conference on Information and Knowledge Management''. Hong Kong, China. November 2–6: ACM Press. || 2009 || CIKM || T-Clustering ||
|-
| '''Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., &amp; Yamada, K.''' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. ''In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication'' (pp.&amp;nbsp;166 – 175). Suwon, Republic of Korea. January 14–15: ACM Press. || 2010 || ICIUMC || T-SEngine ||
|-
| '''Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., &amp; Zaragoza, H.''' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. ''In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval'', (pp.&amp;nbsp;41 – 44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||
|}

== Temporal question answering (T-QAnswering) ==
{| class="wikitable sortable"
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Pasca, M.''' (2008). [http://dl.acm.org/citation.cfm?id=1363946 Towards Temporal Web Search]. ''In [http://www.acm.org/conferences/sac/sac2008/ SAC2008]: Proceedings of the 23rd ACM Symposium on Applied Computing'' (pp.&amp;nbsp;1117 – 1121). Fortaleza, Ceara, Brazil. March 16–20: ACM Press. || 2008 || SAC || T-QAnswering ||
|}

== Temporal snippets (T-snippets) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Alonso, O., Baeza-Yates, R., &amp; Gertz, M.''' (2009). [http://www.wssp.info/2009/WSSP2009AlonsoBaezaYatesGertz.pdf Effectiveness of Temporal Snippets]. ''In [http://www.wssp.info/2009.html WSSP2009]: Proceedings of the Workshop on Web Search Result Summarization and Presentation associated to [[www2009.org/|WWW2009]]: 18th International World Wide Web Conference''. Madrid, Spain. April 20–24: ACM Press. || 2009 || WWW - WSSP || T-Snippets ||
|-
| '''Alonso, O., Gertz, M., &amp; Baeza-Yates, R.''' (2011). [http://www.springerlink.com/content/u78qu8x10h613471/ Enhancing Document Snippets Using Temporal Information]. ''In R. Grossi, F. Sebastiani, &amp; F. Silvestri (Eds.), Lecture Notes in Computer Science, [http://spire2011.isti.cnr.it/ SPIRE2011]: 18th International Symposium on String Processing and Information Retrieval'' (Vol. 7024, pp.&amp;nbsp;26 – 31). Pisa, Italy. October 17–21.: Springer Berlin / Heidelberg. || 2011 || SPIRE || T-Snippets ||
|-
| '''Svore, K. M., Teevan, J., Dumais, S. T., &amp; Kulkarni, A.''' (2012). [http://dl.acm.org/citation.cfm?id=2348461 Creating Temporally Dynamic Web Search Snippets]. ''In [http://www.sigir.org/sigir2012/ SIGIR2012]: Proceedings of the 35th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'', (pp.&amp;nbsp;1045 – 1046). Portland, United States. August 12-16. ACM Press  || 2012 || SIGIR || T-Snippets ||
|}

== Future information retrieval (F-IRetrieval) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Baeza-Yates, R.''' (2005). [http://www.dcs.vein.hu/CIR/cikkek/searching_the_future.pdf Searching the Future]. ''In S. Dominich, I. Ounis, &amp; J.-Y. Nie (Ed.), MFIR2005: Proceedings of the Mathematical/Formal Methods in Information Retrieval Workshop associated to [http://www.dcc.ufmg.br/eventos/sigir2005/ SIGIR 2005]: 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval''. Salvador, Brazil. August 15–19: ACM Press. || 2005 || SIGIR - MFIR || F-IRetrieval ||
|-
| '''Jatowt, A., Kawai, H., Kanazawa, K., Tanaka, K., &amp; Kunieda, K.''' (2009). [http://dl.acm.org/citation.cfm?id=1555420 Supporting Analysis of Future-Related Information in News Archives and the Web]. ''In [http://www.jcdl2009.org JCDL2009]: Proceedings of the Joint Conference on Digital Libraries'' (pp.&amp;nbsp;115 – 124). Austin, United States. June 15–19.: ACM Press. || 2009 || JCDL || F-IRetrieval ||
|-
| '''Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., &amp; Yamada, K.''' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. ''In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication'' (pp.&amp;nbsp;166 – 175). Suwon, Republic of Korea. January 14–15: ACM Press. || 2010 || ICIUMC || T-SEngine ||
|-
| '''Jatowt, A., Kawai, H., Kanazawa, K., Tanaka, K., &amp; Kunieda, K.''' (2010). [http://dl.acm.org/citation.cfm?id=1772835 Analyzing Collective View of Future, Time-referenced Events on the Web]. ''In [http://www2010.org/www/index.html WWW2010]: Proceedings of the 19th International World Wide Web Conference'' (pp.&amp;nbsp;1123 – 1124). Raleigh, United States. April 26–30: ACM Press. || 2010 || WWW || F-IRetrieval ||
|-
| '''Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., &amp; Zaragoza, H.''' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. ''In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval'', (pp.&amp;nbsp;41 – 44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||
|-
| '''Dias, G., Campos, R., &amp; Jorge, A.''' (2011). [http://select.cs.cmu.edu/meetings/enir2011/papers/dias-campos-jorge.pdf Future Retrieval: What Does the Future Talk About?] ''In [http://select.cs.cmu.edu/meetings/enir2011/ ENIR 2011]: Proceedings of the Enriching Information Retrieval Workshop associated to [http://www.sigir2011.org/ SIGIR2011]: 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval''. Beijing, China. July 28. || 2011 || SIGIR - ENIR || F-IRetrieval ||
|-
| '''Kanhabua, N., Blanco, R., &amp; Matthews, M.''' (2011). [http://dl.acm.org/citation.cfm?id=2010018&amp;dl=ACM&amp;coll=DL&amp;CFID=82290723&amp;CFTOKEN=53881602 Ranking Related News Predictions]. ''In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;755 – 764). Beijing, China. July 24–28: ACM Press. || 2011 || SIGIR || F-IRetrieval ||
|-
| '''Kanazawa, K., Jatowt, A., &amp; Tanaka, K.''' (2011). [http://dl.acm.org/citation.cfm?id=2052362 Improving Retrieval of Future-Related Information in Text Collections]. ''In [http://liris.cnrs.fr/~wi-iat11/WI 2011/ WIC2011]: IEEE Main Conference Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence'' (pp.&amp;nbsp;278 – 283). Lyon, France. August 22–27: IEEE Computer Society Press. || 2011 || WIC || F-IRetrieval ||
|-
| '''Campos, R., Dias, G., &amp; Jorge, A. M.''' (2011). [http://dl.acm.org/citation.cfm?id=2051169 An Exploratory Study on the impact of Temporal Features on the Classification and Clustering of Future-Related Web Documents]. ''In L. Antunes, &amp; H. S. Pinto (Eds.), Lecture Notes in Artificial Intelligence - Progress in Artificial Intelligence - [http://epia2011.appia.pt/ EPIA2011]: 15th Portuguese Conference on Artificial Intelligence associated to APPIA: Portuguese Association for Artificial Intelligence'' (Vol. 7026/2011, pp.&amp;nbsp;581 – 596). Lisboa, Portugal. October 10–13: Springer Berlin / Heidelberg. || 2011 || EPIA || F-IRetrieval ||
|-
| '''Jatowt, A., &amp; Yeung, C. M.''' (2011). [http://dl.acm.org/citation.cfm?id=2063759 Extracting Collective Expectations about the Future from Large Text Collections]. ''In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management'' (pp.&amp;nbsp;1259 – 1264). Glasgow, Scotland, UK. October: ACM Press. || 2011 || CIKM || F-IRetrieval ||
|-
| '''Weerkamp, W., &amp; Rijke, M.''' (2012). [http://research.microsoft.com/en-us/people/milads/taia2012-activities.pdf Activity Prediction: A Twitter-based Exploration]. ''In [http://research.microsoft.com/en-us/people/milads/taia2012.aspx TAIA 2012]: Proceedings of the Time-Aware Information Access Workshop associated to [http://www.sigir.org/sigir2012/ SIGIR2012]: 35th Annual International ACM SIGIR 2012 Conference on Research and Development in Information Retrieval''. Portland, United States. August 16. || 2012 || SIGIR - TAIA || F-IRetrieval ||
|-
| '''Radinski, K., &amp; Horvitz, E.''' (2013). [http://dl.acm.org/citation.cfm?id=2433431 Mining the Web to Predict Future Events]. ''In [http://wsdm2013.org/ WSDM2013]: Proceedings of the 6th ACM International Conference on Web Search and Data Mining'' (pp.&amp;nbsp;255 – 264). Rome, Italy. February 4–8: ACM Press. || 2013 || WSDM || F-IRetrieval ||
|}

== Temporal image retrieval (T-IRetrieval) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Dias, G., Moreno, J. G., Jatowt, A., &amp; Campos, R.''' (2012). [http://link.springer.com/content/pdf/10.1007%2F978-3-642-34109-0_21 Temporal Web Image Retrieval]. In Calderón-Benavides, L., González-Caro, C., Chávez, E., Ziviani, N. (Eds.), ''In Lecture Notes in Computer Science - [http://catic.unab.edu.co/spire/ SPIRE2012]: 19th International Symposium on String Processing and Information Retrieval'' (Vol. 7608/2012, pp.&amp;nbsp;199 – 204). Cartagena de Indias, Colombia. October 21–25: Springer Berlin / Heidelberg. || 2012 || SPIRE || T-IRetrieval ||
|-
| '''Palermo, F., Hays, J., &amp; Efros, A.''' (2012). [http://link.springer.com/content/pdf/10.1007/978-3-642-33783-3_36 Dating Historical Color Images]. In Fitzgibbon, A., Lazebnik, S., Sato, Y., Schmid, C. (Eds.), ''In Lecture Notes in Computer Science - [http://eccv2012.unifi.it/ ECCV2012]: 12th European Conference on Computer Vision'' (Vol. 7577/2012, pp.&amp;nbsp;499 – 512). Firenze, Italy. October 07–13: Springer Berlin / Heidelberg. || 2012 || ECCV || T-IRetrieval ||
|-
| '''Kim, G., &amp; Xing, E. P.''' (2013). [http://dl.acm.org/citation.cfm?id=2433417 Time-Sensitive Web Image Ranking and Retrieval via Dynamic Multi-Task Regression]. ''In [http://wsdm2013.org/ WSDM2013]: Proceedings of the 6th ACM International Conference on Web Search and Data Mining'' (pp.&amp;nbsp;163 – 172). Rome, Italy. February 4–8: ACM Press. || 2013 || WSDM || T-IRetrieval ||
|-
| '''Martin, P., Doucet, A., &amp; Jurie, F.''' (2014). [http://dl.acm.org/citation.cfm?id=2578790 Dating Color Images with Ordinal Classification]. ''In [http://www.icmr2014.org/ ICMR2014]: Proceedings of International Conference on Multimedia Retrieval'' (pp. 447). Glasgow, United Kingdom. April 01-04: ACM Press. || 2014 || ICMR || T-IRetrieval ||
|}

== Collective memory (C-memory) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Surowiecki, J.''' (2004). [http://www.amazon.com/The-Wisdom-Crowds-Collective-Economies/dp/0385503865 The Wisdom of Crowds: Why the Many Are Smarter Than the Few and How Collective Wisdom Shapes Business, Economies, Societies and Nations]. USA: DoubleDay. || 2004 ||  || C-Memory ||
|-
| '''Hall, D., Jurafsky, D., &amp; Manning, C. D.''' (2008). [http://dl.acm.org/citation.cfm?id=1613715.1613763 Studying the History of Ideas using Topic Models]. ''In [http://conferences.inf.ed.ac.uk/emnlp08 EMNLP 2008]: Proceedings of the Conference on Empirical Methods in Natural Language Processing'' (pp.&amp;nbsp;363 – 371). Waikiki, Honolulu, Hawaii. October 25–27: Association for Computational Linguistics. || 2008 || EMNLP || C-Memory ||
|-
| '''Shahaf, D., &amp; Guestrin, C.''' (2010). [http://dl.acm.org/citation.cfm?id=1835884 Connecting the dots between News Articles]]. In [http://www.sigkdd.org/kdd2010/ KDD2010]: Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp.&amp;nbsp;623 – 632). Washington, United States. July 25–28: ACM Press. || 2010 || KDD || C-Memory ||
|-
| '''Takahashi, Y., Ohshima, H., Yamamoto, M., Iwasaki, H., Oyama, S., &amp; Tanaka, K.''' (2011). [http://dl.acm.org/citation.cfm?id=1995980 Evaluating Significance of Historical Entities based on Tempo-spatial Impacts Analysis using Wikipedia Link Structure]]. ''In [http://www.ht2011.org/ HT2011]: Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia'' (pp.&amp;nbsp;83 – 92). Eindhoven, Netherlands. June 6–9: ACM Press. || 2011 || HT || C-Memory ||
|-
| '''Michel, J.-B., Shen, Y. K., Aiden, A. P., Veres, A., Gray, M. K., Team, T. G., et al.''' (2011). [http://www.sciencemag.org/content/331/6014/176 Quantitative Analysis of Culture Using Millions of Digitized Books]. In [http://www.sciencemag.org/ Science], 331(6014), 176 - 182. || 2011 || Science || C-Memory ||
|-
| '''Yeung, C.-m. A., &amp; Jatowt, A.''' (2011). [http://dl.acm.org/citation.cfm?id=2063755 Studying How the Past is Remembered: Towards Computational History through Large Scale Text Mining]]. ''In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management'' (pp.&amp;nbsp;1231 – 1240). Glasgow, Scotland, UK. October 24–28: ACM Press. || 2011 || CIKM || C-Memory ||
|}

== Web archives (W-archives) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| [[List of Web archiving initiatives|List of Web Archive Initiatives]] || 2011 ||  || W-Archives ||
|-
| '''Kahle, B.''' (1997, 03). [http://www.sciamdigital.com/index.cfm?fa=Products.ViewIssuePreview&amp;ISSUEID_CHAR=00B8E369-1805-4A27-A331-9D727FEAC21&amp;ARTICLEID_CHAR=00B10B9E-5F13-40B2-AA51-0A4D5C41549 Preserving the Internet]. ''In [https://www.scientificamerican.com/sciammag/ Scientific American Magazine]'', 276(3), pp.&amp;nbsp;72 – 73. || 1997 || SAM || W-Archives ||
|-
| '''Toyoda, M., &amp; Kitsuregawa, M.''' (2005). [http://dl.acm.org/citation.cfm?id=1083387 A System for Visualizing and Analyzing the Evolution of the Web with a Time Series of Graphs]. ''In [http://www.ht05.org HT2005]: Proceedings of the 16th ACM Conference on Hypertext and Hypermedia'' (pp.&amp;nbsp;151 – 160). Salzburg, Austria. September 6–9: ACM Press. || 2005 || HT || W-Archives ||
|-
| '''Efendioglu, D., Faschetti, C., &amp; Parr, T.''' (2006). [http://dl.acm.org/authorize?815487 Chronica: a temporal web search engine]]. In ''D. Wolber, N. Calder, &amp; ,. C. Brooks (Ed.), [http://www.icwe2006.org/ ICWE2006]: Proceedings of the 6th International Conference on Web Engineering'' (pp.&amp;nbsp;119 – 120). Palo Alto, California, United States. July 11–14: ACM Press. || 2006 || ICWE || W-Archives ||
|-
| '''Jatowt, A., Kawai, Y., Nakamura, S., Kidawara, Y., &amp; Tanaka, K.''' (2006). [http://dl.acm.org/citation.cfm?id=1149969 Journey to the Past: Proposal of a Framework for Past Web Browser]. ''In HT2006: Proceedings of the 17th Conference on Hypertext and Hypermedia'' (pp.&amp;nbsp;135 – 144). Odense, Denmark. August 22–25: ACM Press. || 2006 || HT || W-Archives ||
|-
| '''Adar, E., Dontcheva, M., Fogarty, J., &amp; Weld, D. S.''' (2008). [http://dl.acm.org/citation.cfm?id=1449756 Zoetrope: Interacting with the Ephemeral Web]]. ''In S. B. Cousins, &amp; M. Beaudouin-Lafon (Ed.), [http://www.acm.org/uist/uist2008/ UIST 2008]: Proceedings of the 21st Annual ACM Symposium on User Interface Software and Technology'' (pp.&amp;nbsp;239 – 248). Monterey, CA, United States. October 19–22: ACM Press. || 2008 || UIST || W-Archives ||
|-
| '''Song, S., &amp; JaJa, J.''' (2008). [http://www.umiacs.umd.edu/~joseph/temporal-web-archiving-final-umiacs-tr-2008-08.pdf Archiving Temporal Web Information: Organization of Web Contents for Fast Access and Compact Storage]. Technical Report UMIACS-TR-2008-08, University of Maryland Institute for Advanced Computer Studies, Maryland, MD, United States. || 2008 || Technical Report || W-Archives ||
|-
| '''Gomes, D., Miranda, J., &amp; Costa, M.''' (2011). [http://dl.acm.org/citation.cfm?id=2042590 A Survey on Web Archiving Initiatives]]. ''In [http://www.tpdl2011.org/ TPDL2011]: Proceedings of the 15th international conference on Theory and practice of digital libraries: research and advanced technology for digital libraries'' (pp.&amp;nbsp;408 – 420). Berlin, Germany. September 25–29: Springer-Verlag || 2011 || TPDL || W-Archives ||
|-
| '''Anand, A., Bedathur, S., Berberich, K., &amp; Schenkel, R.''' (2012). [http://dl.acm.org/citation.cfm?id=2348318 Index Maintenance for Time-Travel Text Search]. ''In [http://www.sigir.org/sigir2012/ SIGIR2012]: Proceedings of the 35th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'', (pp.&amp;nbsp;235 – 243). Portland, United States. August 12-16. ACM Press || 2012 || SIGIR || W-Archives ||
||
|-
| '''Costa, M., &amp; Silva, M.J.''' (2012). [http://link.springer.com/chapter/10.1007%2F978-3-642-35063-4_32 Evaluating Web Archive Search Systems]. ''In [http://www.wise2012.cs.ucy.ac.cy/ WISE2012]: Proceedings of the 13th International Conference on Web Information System Engineering'', (pp.&amp;nbsp;440 - 454). Paphos, Cyprus. November 28-30. Springer-Verlag || 2012 || WISE || W-Archives ||
|}

== Topic detection and tracking (TDT) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Allan, J., Carbonell, J., Doddington, G., &amp; Yamron, J.''' (1998). [http://www.cs.pitt.edu/~chang/265/proj10/sisref/1.pdf Topic Detection and Tracking Pilot Study Final Report]. In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, (pp.&amp;nbsp;194 – 218). Lansdowne, Virginia, United States. February. || 1998 || Technical Report || TDT ||
|-
| '''Swan, R., &amp; Allan, J.''' (1999). [http://dl.acm.org/citation.cfm?id=319956 Extracting Significant Time-Varying Features from Text]]. ''In [http://cikmconference.org/1999/ CIKM 1999]]: Proceedings of the 8th International ACM Conference on Information and Knowledge Management'' (pp.&amp;nbsp;38 – 45). Kansas City, Missouri, United States. November 2–6: ACM Press. || 1999 || CIKM || TDT ||
|-
| '''Swan, R., &amp; Jensen, D.''' (2000). [http://www.cs.cmu.edu/~dunja/.../Swan_TM.pdf TimeMines: Constructing Timelines with Statistical Models of Word Usage]. ''In M. Grobelnik, D. Mladenic, &amp; N. Milic-Frayling (Ed.), [http://www.cs.cmu.edu/~dunja/WshKDD2000.html TM2000]: Proceedings of the Workshop on Text Mining associated to [http://www.sigkdd.org/kdd2000/ KDD2000]: 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining'' (pp.&amp;nbsp;73 – 80). Boston, Massachusetts, United States. August 20–23: ACM Press. || 2000 || KDD - TM || TDT ||
|-
| '''Swan, R., &amp; Allan, J.''' (2000). [http://dl.acm.org/citation.cfm?id=345546 Automatic Generation of Overview Timelines]. ''In [http://www.aueb.gr/conferences/sigir2000/ SIGIR 2000]: Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&amp;nbsp;49 – 56). Athens, Greece. July 24–28: ACM Press. || 2000 || SIGIR || TDT ||
|-
| '''Makkonen, J., &amp; Ahonen-Myka, H.''' (2003). [http://www.springerlink.com/content/a5ev5br7wwh5lvyl/ Utilizing Temporal Information in Topic Detection and Tracking]. ''In T. Koch, &amp; I. T. Solvberg (Eds.), In Lecture Notes in Computer Science - Research and Advanced Technology for Digital Libraries, [http://www.ecdl2003.org/ ECDL 2003]: 7th European Conference on Research and Advances Technology for Digital Libraries'' (Vol. 2769/2004, pp.&amp;nbsp;393 – 404). Trondheim, Norway. August 17–22: Springer Berlin / Heidelberg. || 2003 || ECDL || TDT ||
|-
| '''Shaparenko, B., Caruana, R., Gehrke, J., &amp; Joachims, T.''' (2005). [http://www.cs.cornell.edu/people/tj/publications/shaparenko_etal_05a.pdf Identifying Temporal Paterns and Key Players in Document Collections]. ''In [http://users.cis.fiu.edu/~taoli/workshop/TDM2005/index.html TDM2005]: Proceedings of the Workshop on Temporal Data Mining associated to [http://www.cacs.louisiana.edu/~icdm05/ ICDM2005]'' (pp.&amp;nbsp;165 – 174). Houston, United States. November 27–30: IEEE Press. || 2005 || ICDM - TDM || TDT ||
|-
| '''Mori, M., Miura, T., &amp; Shioya, I.''' (2006). [http://dl.acm.org/citation.cfm?id=1249137 Topic Detection and Tracking for News Web Pages]. ''In [http://www.comp.hkbu.edu.hk/~wii06/wi/ WIC2006]: IEEE Main Conference Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence'' (pp.&amp;nbsp;338 – 342). Hong Kong, China. December 18–22: IEEE Computer Society Press. || 2006 || WIC || TDT ||
|-
| '''Kim, P., Myaeng, S.H.''' (2004). [http://dl.acm.org/citation.cfm?id=1039624 Usefulness of Temporal Information Automatically Extracted from News Articles for Topic Tracking]. ''In [http://talip.acm.org/index.htm TALIP]:Journal of ACM Transactions on Asian Language Information Processing'' (pp.&amp;nbsp;227 – 242). New York, United States. || 2004 || TALIP || TDT ||
|}

==References==
{{reflist}}

[[Category:Information retrieval genres]]</text>
      <sha1>gtls71q6am18ikvkrmm46fyvai4ot5p</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Personalized search</title>
    <ns>14</ns>
    <id>38954543</id>
    <revision>
      <id>727440755</id>
      <parentid>666717227</parentid>
      <timestamp>2016-06-29T01:05:28Z</timestamp>
      <contributor>
        <ip>70.51.200.20</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="91" xml:space="preserve">{{catmore}}

[[Category:Information retrieval genres]]
[[Category:Internet search engines]]</text>
      <sha1>bfm95ck6s3gvekknwd6uwnbykr9t0o9</sha1>
    </revision>
  </page>
  <page>
    <title>Legal information retrieval</title>
    <ns>0</ns>
    <id>24997830</id>
    <revision>
      <id>726948118</id>
      <parentid>723242992</parentid>
      <timestamp>2016-06-25T14:36:00Z</timestamp>
      <contributor>
        <username>Krauss</username>
        <id>1222358</id>
      </contributor>
      <comment>/* Notes */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="14823" xml:space="preserve">'''Legal information retrieval''' is the science of [[information retrieval]] applied to legal text, including [[legislation]], [[case law]], and scholarly works.&lt;ref&gt;Maxwell, K.T., and Schafer, B. 2009, p. 1&lt;/ref&gt; Accurate legal information retrieval is important to provide access to the law to laymen and legal professionals. Its importance has increased because of the vast and quickly increasing amount of legal documents available through electronic means.&lt;ref name=Jackson&gt;Jackson et al., p. 60&lt;/ref&gt; Legal information retrieval is a part of the growing field of [[legal informatics]].

== Overview ==

In a legal setting, it is frequently important to retrieve all information related to a specific query. However, commonly used [[boolean search]] methods (exact matches of specified terms) on full text legal documents have been shown to have an average [[recall rate]] as low as 20 percent,&lt;ref name="Blair, D.C. 1985, p.293"&gt;Blair, D.C., and Maron, M.E., 1985, p.293&lt;/ref&gt; meaning that only 1 in 5 relevant documents are actually retrieved. In that case, researchers believed that they had retrieved over 75% of relevant documents.&lt;ref name="Blair, D.C. 1985, p.293"/&gt; This may result in failing to retrieve important or [[precedential]] cases. In some jurisdictions this may be especially problematic, as legal professionals are [[legal ethics|ethically]] obligated to be reasonably informed as to relevant legal documents.&lt;ref&gt;American Bar Association, Model Rules of Professional Conduct Rule 1.1, http://www.abanet.org/cpr/mrpc/rule_1_1.html&lt;/ref&gt;

Legal Information Retrieval attempts to increase the effectiveness of legal searches by increasing the number of relevant documents (providing a high [[recall rate]]) and reducing the number of irrelevant documents (a high [[precision rate]]). This is a difficult task, as the legal field is prone to [[jargon]],&lt;ref&gt;Peters, W. et al. 2007, p. 118&lt;/ref&gt; [[polysemes]]&lt;ref&gt;Peters, W. et al. 2007, p. 130&lt;/ref&gt; (words that have different meanings when used in a legal context), and constant change.

Techniques used to achieve these goals generally fall into three categories: [[boolean search|boolean]] retrieval, manual classification of legal text, and [[natural language processing]] of legal text.

== Problems ==

Application of standard [[information retrieval]] techniques to legal text can be more difficult than application in other subjects. One key problem is that the law rarely has an inherent [[Taxonomy (general)|taxonomy]].&lt;ref name=LOIS1&gt;Peters, W. et al. 2007, p. 120&lt;/ref&gt; Instead, the law is generally filled with open-ended terms, which may change over time.&lt;ref name=LOIS1 /&gt; This can be especially true in [[common law]] countries, where each decided case can subtly change the meaning of a certain word or phrase.&lt;ref&gt;Saravanan, M. et al.  2009, p. 101&lt;/ref&gt;

Legal information systems must also be programmed to deal with law-specific words and phrases. Though this is less problematic in the context of words which exist solely in law, legal texts also frequently use polysemes, words may have different meanings when used in a legal or common-speech manner, potentially both within the same document. The legal meanings may be dependent on the area of law in which it is applied. For example, in the context of European Union legislation, the term "worker" has four different meanings:&lt;ref name="Peters, W. et al. 2007, p. 131"&gt;Peters, W. et al. 2007, p. 131&lt;/ref&gt;

#Any worker as defined in Article 3(a) of [[Directive 89/391/EEC]] who habitually uses display screen equipment as a significant part of his normal work.
#Any person employed by an employer, including trainees and apprentices but excluding domestic servants;
#Any person carrying out an occupation on board a vessel, including trainees and apprentices, but excluding port pilots and shore personnel carrying out work on board a vessel at the quayside;
#Any person who, in the Member State concerned, is protected as an employee under national employment law and in accordance with national practice;

In addition, it also has the common meaning: 
&lt;ol start="5"&gt;
&lt;li&gt;A person who works at a specific occupation.&lt;ref name="Peters, W. et al. 2007, p. 131"/&gt; &lt;/li&gt;
&lt;/ol&gt;

Though the terms may be similar, correct information retrieval must differentiate between the intended use and irrelevant uses in order to return the correct results.

Even if a system overcomes the language problems inherent in law, it must still determine the relevancy of each result. In the context of judicial decisions, this requires determining the precedential value of the case.&lt;ref name=MaxwellA &gt;Maxwell, K.T., and Schafer, B. 2008, p. 8&lt;/ref&gt; Case decisions from senior or [[superior court]]s may be more relevant than those from [[lower court]]s, even where the lower court's decision contains more discussion of the relevant facts.&lt;ref name=MaxwellA  /&gt; The opposite may be true, however, if the senior court has only a minor discussion of the topic (for example, if it is a secondary consideration in the case).&lt;ref name=MaxwellA  /&gt; A information retrieval system must also be aware of the authority of the jurisdiction. A case from a binding authority is most likely of more value than one from a non-binding authority.

Additionally, the intentions of the user may determine which cases they find valuable. For instance, where a legal professional is attempting to argue a specific interpretation of law, he might find a minor court's decision which supports his position more valuable than a senior courts position which does not.&lt;ref name=MaxwellA  /&gt; He may also value similar positions from different areas of law, different jurisdictions, or dissenting opinions.&lt;ref name=MaxwellA /&gt;

Overcoming these problems can be made more difficult because of the large number of cases available. The number of legal cases available via electronic means is constantly increasing (in 2003, US appellate courts handed down approximately 500 new cases per day&lt;ref name=Jackson /&gt;), meaning that an accurate legal information retrieval system must incorporate methods of both sorting past data and managing new data.&lt;ref name=Jackson /&gt;&lt;ref&gt;Maxwell, K.T., and Schafer, B. 2007, p.1&lt;/ref&gt;

== Techniques ==

===Boolean searches===

[[Boolean search]]es, where a user may specify terms such as use of specific words or judgments by a specific court, are the most common type of search available via legal information retrieval systems. They are widely implemented by services such as [[Westlaw]], [[LexisNexis]], and [[Findlaw]].  However, they overcome few of the problems discussed above.

The recall and precision rates of these searches vary depending on the implementation and searches analyzed. One study found a basic boolean search's [[recall rate]] to be roughly 20%, and its precision rate to be roughly 79%.&lt;ref name="Blair, D.C. 1985, p.293"/&gt; Another study implemented a generic search (that is, not designed for legal uses) and found a recall rate of 56% and a precision rate of 72% among legal professionals. Both numbers increased when searches were run by non-legal professionals, to a 68% recall rate and 77% precision rate. This is likely explained because of the use of complex legal terms by the legal professionals.&lt;ref&gt;Saravanan M., et al. 2009, p. 116&lt;/ref&gt;

===Manual classification===

In order to overcome the limits of basic boolean searches, information systems have attempted to classify case laws and statutes into more computer friendly structures. Usually, this results in the creation of an [[ontology]] to classify the texts, based on the way a legal professional might think about them.&lt;ref name="Maxwell, K.T. 2008, p. 2"&gt;Maxwell, K.T., and Schafer, B. 2008, p. 2&lt;/ref&gt; These attempt to link texts on the basis of their type, their value, and/or their topic areas. Most major legal search providers now implement some sort of classification search, such as [[Westlaw]]'s “Natural Language”&lt;ref name=WL&gt;Westlaw Research, http://www.westlaw.com&lt;/ref&gt; or [[LexisNexis]]' Headnote&lt;ref name=LN&gt;Lexis Research, http://www.lexisnexis.com&lt;/ref&gt; searches. Additionally, both of these services allow browsing of their classifications, via Westlaw's West Key Numbers&lt;ref name=WL /&gt; or Lexis' Headnotes.&lt;ref name=LN /&gt; Though these two search algorithms are proprietary and secret, it is known that they employ manual classification of text (though this may be computer-assisted).&lt;ref name="Maxwell, K.T. 2008, p. 2"/&gt;

These systems can help overcome the majority of problems inherent in legal information retrieval systems, in that manual classification has the greatest chances of identifying landmark cases and understanding the issues that arise in the text.&lt;ref name="Maxwell, K.T. 2008, p. 3"&gt;Maxwell, K.T., and Schafer, B. 2008, p. 3&lt;/ref&gt; In one study, ontological searching resulted in a precision rate of 82% and a recall rate of 97% among legal professionals.&lt;ref&gt;Saravanan, M. et al.  2009, p. 116&lt;/ref&gt; The legal texts included, however, were carefully controlled to just a few areas of law in a specific jurisdiction.&lt;ref&gt;Saravanan, M. et al. 2009, p. 103&lt;/ref&gt;

The major drawback to this approach is the requirement of using highly skilled legal professionals and large amounts of time to classify texts.&lt;ref name="Maxwell, K.T. 2008, p. 3"/&gt;&lt;ref&gt;Schweighofer, E. and Liebwald, D. 2008, p. 108&lt;/ref&gt; As the amount of text available continues to increase, some have stated their belief that manual classification is unsustainable.&lt;ref&gt;Maxwell, K.T., and Schafer, B. 2008, p. 4&lt;/ref&gt;

===Natural language processing===

In order to reduce the reliance on legal professionals and the amount of time needed, efforts have been made to create a system to automatically classify legal text and queries.&lt;ref name=Jackson /&gt;&lt;ref name=AshleyA&gt;Ashley, K.D. and Bruninghaus, S. 2009, p. 125&lt;/ref&gt;&lt;ref name=Gelbart&gt;Gelbart, D. and Smith, J.C. 1993, p. 142&lt;/ref&gt; Adequate translation of both would allow accurate information retrieval without the high cost of human classification. These automatic systems generally employ [[Natural Language Processing]] (NLP) techniques that are adapted to the legal domain, and also require the creation of a legal [[ontology]]. Though multiple systems have been postulated,&lt;ref name=Jackson /&gt;&lt;ref name=AshleyA /&gt;&lt;ref name=Gelbart /&gt; few have reported results. One system, “SMILE,” which attempted to automatically extract classifications from case texts, resulted in an [[f-measure]] (which is a calculation of both recall rate and precision) of under 0.3 (compared to perfect f-measure of 1.0).&lt;ref name=AshleyB &gt;Ashley, K.D. and Bruninghaus, S. 2009, p. 159&lt;/ref&gt; This is probably much lower than an acceptable rate for general usage.&lt;ref name=AshleyB /&gt;&lt;ref&gt;Maxwell, K.T., and Schafer, B. 2009, p. 3&lt;/ref&gt;

Despite the limited results, many theorists predict that the evolution of such systems will eventually replace manual classification systems.&lt;ref&gt;Maxwell, K.T., and Schafer, B. 2009, p. 9&lt;/ref&gt;&lt;ref&gt;Ashley, K.D. and Bruninghaus, S. 2009, p. 126&lt;/ref&gt;

== List of retrieval systems ==
Free-to-use law-texts and associated oficial metadata:

* [[LexML Brazil]]
* [http://www.legislation.gov.uk/ legislation.gov.uk]
* [[EUR-Lex#N-Lex|N-Lex]]
* ...

== Notes ==
{{Reflist|2}}

==References==
{{Refbegin}}
*{{cite journal
|author1=Maxwell, K.T. |author2=Schafer, B.
|year       = 2008
|title      = Concept and Context in Legal Information Retrieval
|url        = http://portal.acm.org/citation.cfm?id=1564016
|journal    = Frontiers in Artificial Intelligence and Applications
|volume     = 189
|pages      = 63–72
|publisher  = IOS Press
|accessdate = 2009-11-07
}}
*{{cite journal
|author     = Jackson, P.|year       = 1998
|title      = Information extraction from case law and retrieval of prior cases by partial parsing and query generation
|url        = http://portal.acm.org/citation.cfm?id=288627.288642
|journal    = Conference on Information and Knowledge Management
|pages      = 60–67
|publisher  = ACM
|accessdate = 2009-11-07
|display-authors=etal}}
*{{cite journal
|author1=Blair, D.C. |author2=Maron, M.E.
|year       = 1985
|title      = An evaluation of retrieval effectiveness for a full-text document-retrieval
|url        = http://portal.acm.org/citation.cfm?id=3166.3197&amp;coll=GUIDE&amp;dl=GUIDE&amp;CFID=61732097&amp;CFTOKEN=95519997
|journal    = Communications of the ACM
|volume     = 28
|issue      = 3 
|pages      = 289–299
|publisher  = ACM
|accessdate = 2009-11-07
|doi=10.1145/3166.3197
}}
*{{cite journal
|author     = Peters, W.|year       = 2007
|title      = The structuring of legal knowledge in LOIS
|url        = http://www.springerlink.com/content/d04l7h2507700g45/
|journal    = Artificial Intelligence and Law
|volume     = 15
|issue      = 2
|pages      = 117–135
|publisher  = Springer Netherlands
|accessdate = 2009-11-07
|doi=10.1007/s10506-007-9034-4
|display-authors=etal}}
*{{cite journal
|author     = Saravanan, M.|year       = 2007
|title      = Improving legal information retrieval using an ontological framework 
|url        = http://www.springerlink.com/content/h66412k08h855626/
|journal    = Artificial Intelligence and Law
|volume     = 17
|issue      = 2
|pages      = 101–124
|publisher  = Springer Netherlands
|accessdate = 2009-11-07
|doi=10.1007/s10506-009-9075-y
|display-authors=etal}}
*{{cite journal
|author1=Schweighofer, E.  |author2=Liebwald, D.
|year       = 2007
|title      = Advanced lexical ontologies and hybrid knowledge based systems: First steps to a dynamic legal electronic commentary
|url        = http://www.springerlink.com/content/v62v7131x10413v0/
|journal    = Artificial Intelligence and Law
|volume     = 15
|issue      = 2
|pages      = 103–115
|publisher  = Springer Netherlands
|accessdate = 2009-11-07
|doi=10.1007/s10506-007-9029-1
}}
*{{cite journal
|author1=Gelbart, D.  |author2=Smith, J.C.
|year       = 1993
|title      = FLEXICON: an evaluation of a statistical ranking model adapted to intelligent legal text management
|url        = http://portal.acm.org/citation.cfm?id=158994
|journal    = International Conference on Artificial Intelligence and Law
|pages      = 142–151
|publisher  = ACM
|accessdate = 2009-11-07
}}
*{{cite journal
|author1=Ashley, K.D.  |author2=Bruninghaus, S.
|year       = 2009
|title      = Automatically classifying case texts and predicting outcomes
|url        = http://www.springerlink.com/content/lhg8837331hgu024/
|journal    = Artificial Intelligence and Law
|volume     = 17
|issue      = 2
|pages      = 125–165
|publisher  = Springer Netherlands
|accessdate = 2009-11-07
|doi=10.1007/s10506-009-9077-9
}}
{{Refend}}

{{DEFAULTSORT:Legal Information Retrieval}}
[[Category:Information retrieval genres]]
[[Category:Natural language processing]]
[[Category:Legal research]]</text>
      <sha1>euemo8s2euykm0uhtam4h7l4h8hxaw2</sha1>
    </revision>
  </page>
  <page>
    <title>Human–computer information retrieval</title>
    <ns>0</ns>
    <id>14473878</id>
    <revision>
      <id>751208472</id>
      <parentid>721731904</parentid>
      <timestamp>2016-11-24T02:02:06Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <comment>/* What is HCIR? */ punct., simplify heading</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11019" xml:space="preserve">'''Human-computer information retrieval''' ('''HCIR''') is the study and engineering of [[information retrieval]] techniques that bring human intelligence into the [[search engine|search]] process. It combines the fields of [[human-computer interaction]] (HCI) and information retrieval (IR) and creates systems that improve search by taking into account the human context, or through a multi-step search process that provides the opportunity for human feedback.

== History ==

This term ''human–computer information retrieval'' was coined by Gary Marchionini in a series of lectures delivered between 2004 and 2006.&lt;ref name=march2006&gt;[http://www.asis.org/Bulletin/Jun-06/marchionini.html Marchionini, G. (2006). Toward Human-Computer Information Retrieval Bulletin, in June/July 2006 Bulletin of the American Society for Information Science]&lt;/ref&gt; Marchionini’s main thesis is that "HCIR aims to empower people to explore large-scale information bases but demands that people also take responsibility for this control by expending cognitive and physical energy."

In 1996 and 1998, a pair of workshops at the [[University of Glasgow]] on [[information retrieval]] and [[human–computer interaction]] sought to address the overlap between these two fields. Marchionini notes the impact of the [[World Wide Web]] and the sudden increase in [[information literacy]] – changes that were only embryonic in the late 1990s.

A few workshops have focused on the intersection of IR and HCI. The Workshop on Exploratory Search, initiated by the [[University of Maryland Human-Computer Interaction Lab]] in 2005, alternates between the [[Association for Computing Machinery]] [[Special Interest Group on Information Retrieval]] (SIGIR) and [[CHI (conference)|Special Interest Group on Computer-Human Interaction]] (CHI) conferences. Also in 2005, the [[European Science Foundation]] held an Exploratory Workshop on Information Retrieval in Context. Then, the first Workshop on Human Computer Information Retrieval was held in 2007 at the [[Massachusetts Institute of Technology]].

== Description ==

HCIR includes various aspects of IR and HCI. These include [[exploratory search]], in which users generally combine querying and browsing strategies to foster learning and investigation; information retrieval in context (i.e., taking into account aspects of the user or environment that are typically not reflected in a query); and interactive information retrieval, which Peter Ingwersen defines as "the interactive communication processes that occur during the retrieval of information by involving all the major participants in information retrieval (IR), i.e. the user, the intermediary, and the IR system."&lt;ref name=ingwer1992&gt;[http://vip.db.dk/pi/iri/index.htm Ingwersen, P. (1992). Information Retrieval Interaction. London: Taylor Graham.]&lt;/ref&gt;

A key concern of HCIR is that IR systems intended for human users be implemented and evaluated in a way that reflects the needs of those users.&lt;ref&gt;{{cite web|title=Mira working group (1996). Evaluation Frameworks for Interactive Multimedia Information Retrieval Applications|url=http://www.dcs.gla.ac.uk/mira/}}&lt;/ref&gt;

Most modern IR systems employ a [[ranking|ranked]] retrieval model, in which the documents are scored based on the [[probability]] of the document's [[relevance]] to the query.&lt;ref&gt;Grossman, D. and Frieder, O. (2004). Information Retrieval Algorithms and Heuristics. &lt;/ref&gt; In this model, the system only presents the top-ranked documents to the user. This systems are typically evaluated based on their [[Information retrieval#Average precision of precision and recall|mean average precision]] over a set of benchmark queries from organizations like the [[Text Retrieval Conference]] (TREC).

Because of its emphasis in using human intelligence in the information retrieval process, HCIR requires different evaluation models – one that combines evaluation of the IR and HCI components of the system. A key area of research in HCIR involves evaluation of these systems. Early work on interactive information retrieval, such as Juergen Koenemann and [[Nicholas J. Belkin]]'s 1996 study of different levels of interaction for automatic query reformulation, leverage the standard IR measures of [[Information retrieval#Precision|precision]] and [[Information retrieval#Recall|recall]] but apply them to the results of multiple iterations of user interaction, rather than to a single query response.&lt;ref name=koene1996&gt;[http://sigchi.org/chi96/proceedings/papers/Koenemann/jk1_txt.htm Koenemann, J. and Belkin, N. J. (1996). A case for interaction: a study of interactive information retrieval behavior and effectiveness. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems: Common Ground (Vancouver, British Columbia, Canada, April 13–18, 1996). M. J. Tauber, Ed. CHI '96. ACM Press, New York, NY, 205-212]&lt;/ref&gt; Other HCIR research, such as Pia Borlund's IIR evaluation model, applies a methodology more reminiscent of HCI, focusing on the characteristics of users, the details of experimental design, etc.&lt;ref name=borlund2003&gt;[http://informationr.net/ir/8-3/paper152.html Borlund, P. (2003). The IIR evaluation model: a framework for evaluation of interactive information retrieval systems. Information Research, 8(3), Paper 152]&lt;/ref&gt;

== Goals ==
HCIR researchers have put forth the following goals towards a system where the user has more control in determining relevant results.&lt;ref name=march2006/&gt;&lt;ref name=ipm2013&gt;[https://dl.acm.org/citation.cfm?id=2504017 White, R., Capra, R., Golovchinsky, G., Kules, B., Smith, C., and Tunkelang, D. (2013). Introduction to Special Issue on Human-computer Information Retrieval. Journal of Information Processing and Management 49(5), 1053-1057]&lt;/ref&gt;

Systems should
*no longer only deliver the relevant documents, but must also provide semantic information along with those documents
*increase user responsibility as well as control; that is, information systems require human intellectual effort
*have flexible architectures so they may evolve and adapt to increasingly more demanding and knowledgeable user bases
*aim to be part of information ecology of personal and [[Collective memory|shared memories]] and tools rather than discrete standalone services
*support the entire information life cycle (from creation to preservation) rather than only the dissemination or use phase
*support tuning by end users and especially by information professionals who add value to information resources
*be engaging and fun to use

In short, information retrieval systems are expected to operate in the way that good libraries do. Systems should help users to bridge the gap between data or information (in the very narrow, granular sense of these terms) and knowledge (processed data or information that provides the context necessary to inform the next iteration of an information seeking process). That is, good libraries provide both the information a patron needs as well as a partner in the learning process — the [[information professional]] — to navigate that information, make sense of it, preserve it, and turn it into knowledge (which in turn creates new, more informed information needs).

== Techniques ==

The techniques associated with HCIR emphasize representations of information that use human intelligence to lead the user to relevant results. These techniques also strive to allow users to explore and digest the dataset without penalty, i.e., without expending unnecessary costs of time, mouse clicks, or context shift.

Many [[search engines]] have features that incorporate HCIR techniques. [[Spelling suggestion]]s and [[query expansion|automatic query reformulation]] provide mechanisms for suggesting potential search paths that can lead the user to relevant results. These suggestions are presented to the user, putting control of selection and interpretation in the user’s hands.

[[Faceted search]] enables users to navigate information [[hierarchy|hierarchically]], going from a category to its sub-categories, but choosing the order in which the categories are presented. This contrasts with traditional [[Taxonomy (general)|taxonomies]] in which the hierarchy of categories is fixed and unchanging. [[Faceted classification|Faceted navigation]], like taxonomic navigation, guides users by showing them available categories (or facets), but does not require them to browse through a hierarchy that may not precisely suit their needs or way of thinking.&lt;ref&gt;Hearst, M. (1999). User Interfaces and Visualization, Chapter 10 of Baeza-Yates, R. and Ribeiro-Neto, B., Modern Information Retrieval.&lt;/ref&gt;

[[Combinatorial search#Lookahead|Lookahead]] provides a general approach to penalty-free exploration. For example, various [[web applications]] employ [[Ajax (programming)|AJAX]] to automatically complete query terms and suggest popular searches. Another common example of lookahead is the way in which search engines annotate results with summary information about those results, including both static information (e.g., [[metadata]] about the objects) and "snippets" of document text that are most pertinent to the words in the search query.

[[Relevance feedback]] allows users to guide an IR system by indicating whether particular results are more or less relevant.&lt;ref&gt;Rocchio, J. (1971). Relevance feedback in information retrieval. In: Salton, G (ed), The SMART Retrieval System.&lt;/ref&gt;

Summarization and [[analytics]] help users digest the results that come back from the query. Summarization here is intended to encompass any means of [[aggregate data|aggregating]] or [[data compression|compressing]] the query results into a more human-consumable form. Faceted search, described above, is one such form of summarization. Another is [[cluster analysis|clustering]], which analyzes a set of documents by grouping similar or co-occurring documents or terms. Clustering allows the results to be partitioned into groups of related documents. For example, a search for "java" might return clusters for [[Java (programming language)]], [[Java|Java (island)]], or [[Java (coffee)]].

[[information visualization|Visual representation of data]] is also considered a key aspect of HCIR. The representation of summarization or analytics may be displayed as tables, charts, or summaries of aggregated data. Other kinds of [[information visualization]] that allow users access to summary views of search results include [[tag clouds]] and [[treemapping]].

== Related Areas ==
* [[Exploratory Video Search]]

== References ==

&lt;References/&gt;

==External links==
*{{cite web|url=https://sites.google.com/site/hcirworkshop/ |title=Workshops on Human Computer Information Retrieval}}
*{{cite web|url=http://www.chiir.org/ |title=ACM SIGIR Conference on Human Information Interaction and Retrieval (CHIIR)}}

{{DEFAULTSORT:Human-computer information retrieval}}
[[Category:Information retrieval genres]]
[[Category:Human–computer interaction]]</text>
      <sha1>38ge7ic9rsip6ee4nj1zzxb1e9q2jo0</sha1>
    </revision>
  </page>
  <page>
    <title>Noisy text analytics</title>
    <ns>0</ns>
    <id>6026708</id>
    <revision>
      <id>723138276</id>
      <parentid>715585657</parentid>
      <timestamp>2016-06-01T07:08:15Z</timestamp>
      <contributor>
        <username>I dream of horses</username>
        <id>9676078</id>
      </contributor>
      <minor />
      <comment>/* top */clean up, added [[CAT:O|orphan]] tag using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6330" xml:space="preserve">{{multiple issues|
{{COI|date=December 2015}}
{{notability|date=December 2015}}
{{Orphan|date=June 2016}}
}}
'''Noisy text analytics''' is a process of [[information extraction]] whose goal is to automatically extract structured or semistructured information from [[noisy text|noisy unstructured text data]]. While [[Text analytics]] is a growing and mature field that has great value because of the huge amounts of data being produced, processing of noisy text is gaining in importance because a lot of common applications produce noisy text data. Noisy unstructured text data is found in informal settings such as [[online chat]], [[Text messaging|text messages]], [[e-mail]]s, [[message boards]], [[newsgroups]], [[blogs]], [[wikis]] and [[web pages]]. Also, text produced by processing spontaneous speech using [[automatic speech recognition]] and printed or handwritten text using [[optical character recognition]] contains processing noise. Text produced under such circumstances is typically highly noisy containing spelling errors, [[abbreviation]]s, non-standard words, false starts, repetitions, missing [[punctuation]]s, missing [[letter case]] information, pause filling words such as “um” and “uh” and other texting and [[speech disfluencies]]. Such text can be seen in large amounts in [[contact centre (business)|contact centers]], [[chat room]]s, [[optical character recognition]] (OCR) of text documents, [[short message service]] (SMS) text, etc. Documents with [[historical language]] can also be considered noisy with respect to today’s knowledge about the language. Such text contains important historical, religious, ancient medical knowledge that is useful. The nature of the noisy text produced in all these contexts warrants moving beyond traditional text analysis techniques.

== Techniques for noisy text analysis ==
Missing punctuation and the use of non-standard words can often hinder standard [[natural language processing]] tools such as [[part-of-speech tagging]]
and [[parsing]]. Techniques to both learn from the noisy data and then to be able to process the noisy data are only now being developed.

== Possible source of noisy text ==
* [[World wide web]]: Poorly written text is found in web pages, [[online chat]], [[blogs]], [[wikis]], [[discussion forum]]s, [[newsgroups]]. Most of these data are unstructured and the style of writing is very different from, say, well-written news articles. Analysis for the web data is important because they are sources for market buzz analysis, market review, [[trend estimation]], etc. Also, because of the large amount of data, it is necessary to find efficient methods of [[information extraction]], [[Statistical classification|classification]], [[automatic summarization]] and analysis of these data.
* [[Contact centre (business)|Contact centers]]: This is a general term for help desks, information lines and customer service centers operating in domains ranging from computer sales and support to mobile phones to apparels. On an average a person in the developed world interacts at least once a week with a contact center agent. A typical contact center agent handles over a hundred calls per day. They operate in various modes such as voice, [[online chat]] and [[E-mail]]. The contact center industry produces gigabytes of data in the form of [[E-mails]], chat logs, voice conversation [[Transcription (linguistics)|transcription]]s, customer feedback, etc. A bulk of the contact center data is voice conversations. Transcription of these using state of the art [[automatic speech recognition]] results in text with 30-40% [[word error rate]]. Further, even written modes of communication like online chat between customers and agents and even the interactions over email tend to be noisy. Analysis of contact center data is essential for customer relationship management, customer satisfaction analysis, call modeling, customer profiling, agent profiling, etc., and it requires sophisticated techniques to handle poorly written text.
* Printed Documents: Many libraries, government organizations and national defence organizations have vast repositories of [[hard copy]] documents. To retrieve and process the content from such documents, they need to be processed using [[Optical Character Recognition]]. In addition to printed text, these documents may also contain handwritten annotations. OCRed text can be highly noisy depending on the font size, quality of the print etc. It can range from 2-3% [[word error rate]]s to as high as 50-60% [[word error rate]]s. Handwritten annotations can be particularly hard to decipher, and error rates can be quite high in their presence.
* [[Text messaging|Short Messaging Service]] (SMS): Language usage over computer mediated discourses, like chats, emails and SMS texts, significantly differs from the standard form of the language. An urge towards shorter message length facilitating faster typing and the need for semantic clarity, shape the structure of this non-standard form known as the texting language.

== References ==
*[http://www.springerlink.com/content/ql711884654q/?p=c6beb20b8dfa4389b5e4daf2dd63618e&amp;pi=0 "Special Issue on Noisy Text Analytics - International Journal on Document Analysis and Recognition (2007), Springer, Guest Editors Craig Knoblock, Daniel Lopresti, Shourya Roy and L. Venkata Subramaniam, Vol. 10, No. 3-4, December 2007."]
*[http://arXiv.org/abs/0810.0332 "Wong, W., Liu, W. &amp; Bennamoun, M. Enhanced Integrated Scoring for Cleaning Dirty Texts. In: IJCAI Workshop on Analytics for Noisy Unstructured Text Data (AND), 2007; Hyderabad, India."].
*"L. V. Subramaniam, S. Roy, T. A. Faruquie, S. Negi, A survey of types of text noise and techniques to handle noisy text. In: Third Workshop on Analytics for Noisy Unstructured Text Data (AND), 2009".
&lt;references /&gt;

==See also==
* [[Text analytics]]
* [[Information extraction]]
* [[Computational linguistics]]
* [[Natural language processing]]
* [[Named entity recognition]]
* [[Text mining]]
* [[Automatic summarization]]
* [[Statistical classification]]
* [[Data quality]]

[[Category:Artificial intelligence applications]]
[[Category:Natural language processing]]
[[Category:Computational linguistics]]
[[Category:Information retrieval genres]]
[[Category:Statistical natural language processing]]</text>
      <sha1>mq5obexvm5nhbb56y6g4bcqomz058bz</sha1>
    </revision>
  </page>
  <page>
    <title>Information retrieval applications</title>
    <ns>0</ns>
    <id>13324645</id>
    <revision>
      <id>684413359</id>
      <parentid>666861561</parentid>
      <timestamp>2015-10-06T13:57:36Z</timestamp>
      <contributor>
        <ip>195.251.3.6</ip>
      </contributor>
      <comment>/* General applications of information retrieval */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1476" xml:space="preserve">Areas where [[information retrieval]] techniques are employed include (the entries are in alphabetical order within each category):

==General applications of information retrieval==
* [[Digital libraries]]
*  [[Information filtering]]
** [[Recommender systems]]
*  Media search
** Blog search
** [[Image retrieval]]
** [[3D retrieval]]
** [[Music information retrieval|Music retrieval]]
** News search
** Speech retrieval
** Video retrieval
* [[Search engines]]
** [[Site search]]
** [[Desktop search]]
** [[Enterprise search]]
** [[Federated search]]
** [[Mobile search]]
** [[Social search]]
** [[Web search engine|Web search]]

==Domain specific applications of information retrieval==
* Expert search finding
* Genomic information retrieval
* [[Geographic information retrieval]]
*  Information retrieval for chemical structures
* Information retrieval in [[software engineering]]
* [[Legal information retrieval]]
* [[Vertical search]]

==Other retrieval methods==
Methods/Techniques in which [[information retrieval]] techniques are employed include:
* [[Adversarial information retrieval]]
* [[Automatic summarization]]
**[[Multi-document summarization]]
* [[Compound term processing]]
* [[Cross-language information retrieval|Cross-lingual retrieval]]
* [[Document classification]]
* [[Spam filtering]]
* [[Question answering]]

== See also ==
* [[Information retrieval]]

{{DEFAULTSORT:Information Retrieval Applications}}
[[Category:Information retrieval genres|*]]</text>
      <sha1>pjjmag6bnccmh6l7r85tg4d3ip8smnb</sha1>
    </revision>
  </page>
  <page>
    <title>Multimedia information retrieval</title>
    <ns>0</ns>
    <id>33407925</id>
    <revision>
      <id>756742240</id>
      <parentid>743226569</parentid>
      <timestamp>2016-12-26T15:12:45Z</timestamp>
      <contributor>
        <username>Saeidbk</username>
        <id>28099371</id>
      </contributor>
      <minor />
      <comment>fixed a typo</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6989" xml:space="preserve">{{COI|date=July 2014}}
{{Original research|date=July 2014}}
{{Use dmy dates|date=February 2012}}
'''Multimedia information retrieval''' ('''MMIR''' or '''MIR''') is a research discipline of [[computer science]] that aims at extracting semantic information from [[multimedia]] data sources.&lt;ref name=Eidenberger&gt;H Eidenberger. ''Fundamental Media Understanding'', atpress, 2011, p. 1.&lt;/ref&gt;{{FV|date=July 2014}} Data sources include directly perceivable media such as [[Content (media and publishing)|audio]], [[image]] and [[video]], indirectly perceivable sources such as [[Written language|text]], biosignals as well as not perceivable sources such as bioinformation, stock prices, etc. The methodology of MMIR can be organized in three groups:

# Methods for the summarization of media content ([[feature extraction]]). The result of feature extraction is a description.
# Methods for the filtering of media descriptions (for example, elimination of [[Data redundancy|redundancy]])
# Methods for the [[categorization]] of media descriptions into classes.

== Feature extraction methods ==

Feature extraction is motivated by the sheer size of multimedia objects as well as their redundancy and, possibly, noisiness.&lt;ref name=Eidenberger/&gt;{{rp|2}}{{FV|date=July 2014}} Generally, two possible goals can be achieved by feature extraction:

* Summarization of media content. Methods for summarization include in the audio domain, for example, [[Mel-frequency cepstrum|mel-frequency cepstral coefficients]], Zero Crossings Rate, Short-Time Energy. In the visual domain, color histograms&lt;ref&gt;A Del Bimbo. ''Visual Information Retrieval'', Morgan Kaufmann, 1999.&lt;/ref&gt; such as the [[MPEG-7]] Scalable Color Descriptor can be used for summarization.
* Detection of patterns by [[auto-correlation]] and/or [[cross-correlation]]. Patterns are recurring media chunks that can either be detected by comparing chunks over the media dimensions (time, space, etc.) or comparing media chunks to templates (e.g. face templates, phrases). Typical methods include Linear Predictive Coding in the audio/biosignal domain,&lt;ref&gt;HG Kim, N Moreau, T Sikora.'' MPEG-7 Audio and Beyond", Wiley, 2005.&lt;/ref&gt; texture description in the visual domain and n-grams in text information retrieval.

== Merging and filtering methods ==

Multimedia Information Retrieval implies that multiple channels are employed for the understanding of media content.&lt;ref&gt;MS Lew (Ed.). ''Principles of Visual Information Retrieval'', Springer, 2001.&lt;/ref&gt; Each of this channels is described by media-specific feature transformations. The resulting descriptions have to be merged to one description per media object. Merging can be performed by simple concatenation if the descriptions are of fixed size. Variable-sized descriptions – as they frequently occur in motion description – have to be normalized to a fixed length first.

Frequently used methods for description filtering include [[factor analysis]] (e.g. by PCA), singular value decomposition (e.g. as latent semantic indexing in text retrieval) and the extraction and testing of statistical moments. Advanced concepts such as the [[Kalman filter]] are used for merging of descriptions.

== Categorization methods ==

Generally, all forms of machine learning can be employed for the categorization of multimedia descriptions&lt;ref name=Eidenberger/&gt;{{rp|125}}{{FV|date=July 2014}} though some methods are more frequently used in one area than another. For example, [[hidden Markov models]] are state-of-the-art in [[speech recognition]], while [[dynamic time warping]] – a semantically related method – is state-of-the-art in gene sequence alignment. The list of applicable classifiers includes the following:

* Metric approaches ([[Cluster analysis]], [[vector space model]], [[Minkowski]] distances, dynamic alignment)
* Nearest Neighbor methods ([[K-nearest neighbors algorithm]], K-means, [[self-organizing map]])
* Risk Minimization (Support vector regression, [[support vector machine]], [[linear discriminant analysis]])
* Density-based Methods (Bayes nets, [[Markov process]]es, mixture models)
* Neural Networks ([[Perceptron]], associative memories, spiking nets)
* Heuristics ([[Decision trees]], random forests, etc.)

The selection of the best classifier for a given problem (test set with descriptions and class labels, so-called [[ground truth]]) can be performed automatically, for example, using the [[Weka]] Data Miner.

== Open problems ==

The quality of MMIR Systems&lt;ref&gt;JC Nordbotten. "[http://nordbotten.com/ADM/ADM_book/MIRS-frame.htm Multimedia Information Retrieval Systems]". Retrieved 14 October 2011.&lt;/ref&gt; depends heavily on the quality of the training data. Discriminative descriptions can be extracted from media sources in various forms. Machine learning provides categorization methods for all types of data. However, the classifier can only be as good as the given training data. On the other hand, it requires considerable effort to provide class labels for large databases. The future success of MMIR will depend on the provision of such data.&lt;ref&gt;H Eidenberger. ''Frontiers of Media Understanding'', atpress, 2012.&lt;/ref&gt; The annual [[TRECVID]] competition is currently one of the most relevant sources of high-quality ground truth.

== Related areas ==

MMIR provides an overview over methods employed in the areas of information retrieval.&lt;ref&gt;H Eidenberger. ''Professional Media Understanding'', atpress, 2012.&lt;/ref&gt;&lt;ref&gt;{{cite journal |last=Raieli |first=Roberto ||date= |title=Introducing Multimedia Information Retrieval to libraries |url=http://leo.cineca.it/index.php/jlis/article/view/11530 |journal=JLIS.it |volume=7 |issue=3 |pages=9-42 |doi=10.4403/jlis.it-11530 |access-date=8 October 2016 }}&lt;/ref&gt; Methods of one area are adapted and employed on other types of media. Multimedia content is merged before the classification is performed. MMIR methods are, therefore, usually reused from other areas such as:

* [[Bioinformatics|Bioinformation analysis]]
* [[Biosignal|Biosignal processing]]
* [[Content-based image retrieval|Content-based image and video retrieval]]
* [[Facial recognition system|Face recognition]]
* [[Music information retrieval|Audio and music classification]]
* [[Speech recognition]]
* [[Technical analysis|Technical chart analysis]]
* [[Video Browsing|Video browsing]]
* [[Information retrieval|Text information retrieval]]

The ''Journal of Multimedia Information Retrieval''&lt;ref&gt;"[http://www.springer.com/computer/journal/13735 Journal of Multimedia Information Retrieval]", Springer, 2011, Retrieved 21 October 2011.&lt;/ref&gt; documents the development of MMIR as a research discipline that is independent of these areas. See also ''Handbook of Multimedia Information Retrieval''&lt;ref&gt;H Eidenberger. ''Handbook of Multimedia Information Retrieval'', atpress, 2012.&lt;/ref&gt; for a complete overview over this research discipline.

==References==
{{reflist}}

[[Category:Information retrieval genres]]</text>
      <sha1>5mt2f0afz0ehp4acixhfq0doivk4a29</sha1>
    </revision>
  </page>
  <page>
    <title>Multi-document summarization</title>
    <ns>0</ns>
    <id>6870342</id>
    <revision>
      <id>752729896</id>
      <parentid>749532129</parentid>
      <timestamp>2016-12-02T23:06:57Z</timestamp>
      <contributor>
        <ip>148.240.2.63</ip>
      </contributor>
      <comment>/* Real-life systems */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10323" xml:space="preserve">{{Refimprove|date=January 2016}}
'''Multi-document summarization''' is an automatic procedure aimed at [[information extraction|extraction of information]] from multiple texts written about the same topic. The resulting summary report allows individual users, such as professional information consumers, to quickly familiarize themselves with information contained in a large cluster of documents. In such a way, multi-document summarization systems are complementing the [[news aggregators]] performing the next step down the road of coping with [[information overload]].

==Key benefits==
Multi-[[document summarization]] creates information reports that are both concise and comprehensive.
With different opinions being put together &amp; outlined, every topic is described from multiple perspectives within a single document.
While the goal of a brief summary is to simplify information search and cut the time by pointing to the most relevant source documents, comprehensive multi-document summary should itself contain the required information, hence limiting the need for accessing original files to cases when refinement is required.
Automatic summaries present information extracted from multiple sources algorithmically, without any editorial touch or subjective human intervention, thus making it completely unbiased.

==Technological challenges==
The multi-document summarization task has turned out to be much more complex than [[automatic summarization|summarizing a single document]], even a very large one. This difficulty arises from inevitable thematic diversity within a large set of documents. A good summarization technology aims to combine the main themes with completeness, readability, and conciseness. Document Understanding Conferences,&lt;ref&gt;{{cite web|url=http://www-nlpir.nist.gov/projects/duc/index.html |title=Document Understanding Conferences |website=Nlpir.nist.gov |date=2014-09-09 |accessdate=2016-01-10}}&lt;/ref&gt; conducted annually by [[NIST]], have developed sophisticated evaluation criteria for techniques accepting the multi-document summarization challenge.

An ideal multi-document summarization system does not simply shorten the source texts but presents information organized around the key aspects to represent a wider diversity of views on the topic. When such quality is achieved, an automatic multi-document summary is perceived more like an overview of a given topic. The latter implies that such text compilations should also meet other basic requirements for an overview text compiled by a human. The multi-document summary quality criteria are as follows:
*clear structure, including an outline of the main content, from which it is easy to navigate to the full text sections
*text within sections is divided into meaningful paragraphs
*gradual transition from more general to more specific thematic aspects
*good [[readability]]

The latter point deserves additional note - special care is taken in order to ensure that the automatic overview shows:
*no paper-unrelated "[[communication noise|information noise]]" from the respective documents (e.g., web pages)
*no dangling references to what is not mentioned or explained in the overview
*no text breaks across a sentence
*no semantic [[Redundancy (information theory)|redundancy]].

==Real-life systems==
The multi-document summarization technology is now coming of age - a view supported by a choice of advanced web-based systems that are currently available.
* Ultimate Research Assistant&lt;ref&gt;{{cite web|url=http://ultimate-research-assistant.com/ |title=Generate Research Report |publisher=Ultimate Research Assistant |date= |accessdate=2016-01-10}}&lt;/ref&gt; - performs text mining on Internet search results to help summarize and organize them and make it easier for the user to perform online research. Specific text mining techniques used by the tool include concept extraction, text summarization, hierarchical concept clustering (e.g., automated taxonomy generation), and various visualization techniques, including tag clouds and mind maps. 
* iResearch Reporter&lt;ref&gt;{{cite web|url=http://www.iresearch-reporter.com/ |title=iResearch Reporter service |website=Iresearch-reporter.com |date= |accessdate=2016-01-10}}&lt;/ref&gt; - Commercial Text Extraction and Text Summarization system, free demo site accepts user-entered query, passes it on to Google search engine, retrieves multiple relevant documents, produces categorized, easily  readable natural language summary reports covering multiple documents in retrieved set, all extracts linked to original documents on the Web, post-processing, entity extraction, event and relationship extraction, text extraction, extract clustering, linguistic analysis, multi-document, full text, natural language processing, categorization rules, clustering, linguistic analysis, text summary construction tool set.
* Newsblaster&lt;ref&gt;[http://newsblaster.cs.columbia.edu]  {{webarchive |url=https://web.archive.org/web/20130416065538/http://newsblaster.cs.columbia.edu |date=April 16, 2013 }}&lt;/ref&gt; is a system that helps users find news that is of the most interest to them. The system automatically collects, clusters, categorizes, and summarizes news from several sites on the web ([[CNN]], [[Reuters]], [[Fox News]], etc.) on a daily basis, and it provides users an interface to browse the results.
* NewsInEssence&lt;ref&gt;[http://www.newsinessence.com]  {{webarchive |url=https://web.archive.org/web/20110411005726/http://www.newsinessence.com |date=April 11, 2011 }}&lt;/ref&gt; may be used to retrieve and summarize a cluster of articles from the web. It can start from a [[Uniform Resource Locator|URL]] and retrieve documents that are similar, or it can retrieve documents that match a given set of keywords. NewsInEssence also downloads news articles daily and produces news clusters from them.
* NewsFeed Researcher&lt;ref&gt;{{cite web|url=http://newsfeedresearcher.com |title=News Feed Researcher &amp;#124; General Stuff |website=Newsfeedresearcher.com |date= |accessdate=2016-01-10}}&lt;/ref&gt; is a news portal performing continuous [[automatic summarization]] of documents initially clustered by the [[news aggregators]] (e.g., [[Google News]]). NewsFeed Researcher is backed by a free online engine covering major events related to business, technology, U.S. and international news. This tool is also available in on-demand mode allowing a user to build a summaries on selected topics.
* Scrape This&lt;ref&gt;[http://www.scrapethis.com]  {{webarchive |url=https://web.archive.org/web/20090919054723/http://www.scrapethis.com |date=September 19, 2009 }}&lt;/ref&gt; is like a search engine, but instead of providing links to the most relevant websites based on a query, it scrapes the pertinent information off of the relevant websites and provides the user with a consolidated multi-document summary, along with dictionary definitions, images, and videos.
* JistWeb&lt;ref&gt;[http://www.jastatechnologies.com/productList.html]  {{webarchive |url=https://web.archive.org/web/20130529112318/http://www.jastatechnologies.com/productList.html |date=May 29, 2013 }}&lt;/ref&gt; is a query specific multiple document summariser.
* The Simplish Simplifying &amp; Summarizing tool&lt;ref&gt;{{cite web|url=http://simplish.org/ |title=Simplish Basic english Tool |publisher=The Goodwill Consortium |date= |accessdate=2016-02-12}}&lt;/ref&gt; - performs automatic multi-lingual multi-document summarization. This tool does not need training of any kind and works by generating ideograms that represent the meaning of each sentence and then summarizes using two user-supplied parameters: equivalence (when are two sentences to be considered equivalent) and relevance (how long is the desired summary). 

As auto-generated multi-document summaries increasingly resemble the overviews written by a human, their use of extracted text snippets may one day face [[copyright]] issues in relation to the [[fair use]] copyright concept.

==Bibliography==
* Günes Erkan and Dragomir R. Radev. Lexrank: Graph-based centrality as salience in text summarization. Journal of Artificial Intelligence Research (JAIR), 2004. [http://clair.si.umich.edu/~radev/papers/lprj.pdf]
* Dragomir R. Radev, Hongyan Jing, Malgorzata Styś, and Daniel Tam. Centroid-based summarization of multiple documents. Information Processing and Management, 40:919–938, December 2004. [http://clair.si.umich.edu/~radev/papers/centroid.pdf]
* Kathleen R. McKeown and Dragomir R. Radev. Generating summaries of multiple news articles. In Proceedings, ACM Conference on Research and Development in Information Retrieval SIGIR'95, pages 74–82, Seattle, Washington, July 1995. [http://clair.si.umich.edu/~radev/papers/sigir95.pdf]
* C.-Y. Lin, E. Hovy, "From single to multi-document summarization: A prototype system and its evaluation", In "Proceedings of the ACL", pp.&amp;nbsp;457–464, 2002
*Kathleen McKeown, Rebecca J. Passonneau, David K. Elson, Ani Nenkova, Julia Hirschberg, "Do Summaries Help? A Task-Based Evaluation of Multi-Document Summarization", SIGIR’05, Salvador, Brazil, August 15–19, 2005 [http://www.cs.columbia.edu/~ani/papers/f98-mckeown.pdf]
*R. Barzilay, N. Elhadad, K. R. McKeown, "Inferring strategies for sentence ordering in multidocument news summarization", Journal of Artificial Intelligence Research, v. 17, pp.&amp;nbsp;35–55, 2002
*M. Soubbotin, S. Soubbotin, "Trade-Off Between Factors Influencing Quality of the Summary", Document Understanding Workshop (DUC), Vancouver, B.C., Canada, October 9–10, 2005 [http://duc.nist.gov/pubs/2005papers/freetext.sergei.pdf]
* C Ravindranath Chowdary, and P. Sreenivasa Kumar. "Esum: an efficient system for query-specific multi-document summarization." In ECIR (Advances in Information Retrieval), pp.&amp;nbsp;724–728. Springer Berlin Heidelberg, 2009.

==See also==
* [[Automatic summarization]]
* [[Text mining]]
* [[News aggregators]]

==References==
{{reflist}}

==External links==
*[http://www-nlpir.nist.gov/projects/duc/index.html Document Understanding Conferences]
*[http://www1.cs.columbia.edu/nlp/projects.html Columbia NLP Projects]
*[http://lada.si.umich.edu:8080/clair/nie1/nie.cgi NewsInEssence: Web-based News Summarization]

{{Natural Language Processing}}

{{DEFAULTSORT:Multi-Document Summarization}}
[[Category:Natural language processing]]
[[Category:Information retrieval genres]]</text>
      <sha1>4021eiugthu33xdnlkxwpfj0w9prxfy</sha1>
    </revision>
  </page>
  <page>
    <title>Dragomir R. Radev</title>
    <ns>0</ns>
    <id>31253847</id>
    <revision>
      <id>751100374</id>
      <parentid>719755646</parentid>
      <timestamp>2016-11-23T10:29:51Z</timestamp>
      <contributor>
        <ip>110.92.98.1</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5702" xml:space="preserve">'''Dragomir R. Radev''' is a [[University of Michigan]] computer science professor and [[Columbia University]] computer science adjunct professor working on [[natural language processing]] and [[information retrieval]]. From January 2017 he will join [[Yale University]] as a professor of computer science.
He is currently working on the fields of open domain [[question answering]],  [[multi-document summarization]], and the application of NLP in Bioinformatics and Political Science.

Radev received his PhD in [[Computer Science]] from [[Columbia University]] in 1999. He is the secretary of [http://www.aclweb.org [[Association for Computational Linguistics|ACL]]] (2006–present) and associate editor of [http://www.jair.org JAIR].

== Awards ==
As [[NACLO]] founder, Radev shared the [[Linguistic Society of America]] 2011 [http://www.lsadc.org/info/lsa-awards.cfm ''Linguistics, Language and the Public Award'']. He is the  Co-winner of the [http://polmeth.wustl.edu/about.php?page=awards Gosnell Prize (2006)].

In 2015 he was named a [[fellow]] of the [[Association for Computing Machinery]] "for contributions to natural language processing and computational linguistics."&lt;ref&gt;{{citation|url=http://www.acm.org/press-room/news-releases/2015/fellows-2015|title=ACM Fellows Named for Computing Innovations that Are Advancing Technology in the Digital Age|publisher=[[Association for Computing Machinery]]|year=2015|accessdate=2015-12-10}}.&lt;/ref&gt;

== IOL==
Radev has served as the coach and led the US national team in the [[International Linguistics Olympiad|International Linguistics Olympiad (IOL)]] to several gold medals [http://www.nsf.gov/news/news_summ.jsp?cntn_id=112073][http://www.nsf.gov/news/news_summ.jsp?cntn_id=109891].

== Books ==
* Puzzles in Logic, Languages and Computation (2013) &lt;ref&gt;{{Cite web|url = http://www.springer.com/education+%26+language/linguistics/book/978-3-642-34371-1|title = Puzzles in Logic, Languages and Computation|date = |accessdate = |website = |publisher = |last = |first = }}&lt;/ref&gt;
* Mihalcea and Radev (2011) [http://www.cambridge.org/gb/knowledge/isbn/item5980387/?site_locale=en_GB ''Graph-based methods for NLP and IR'']

== Selected Papers ==
* SIGIR 1995 Generating summaries of multiple news articles
* ANLP 1997 Building a generation knowledge source using internet-accessible newswire
* Computational Linguistics 1998 Generating natural language summaries from multiple on-line sources
* ACL 1998 Learning correlations between linguistic indicators and semantic constraints: Reuse of context dependent descriptions of entities
* ANLP 2000 Ranking suspected answers to natural language questions using predictive annotation
* CIKM 2001 Mining the web for answers to natural language questions
* AAAI 2002 Towards CST-enhanced summarization
* ACL 2003 Evaluation challenges in large-scale multi-document summarization: the Mead project
* Information Processing and Management 2004 Centroid-based summarization of multiple documents
* J. of Artificial Intelligence Research 2004 LexRank: Graph-based lexical centrality as salience in text summarization
* J. of the American Association of Information Science and Technology 2005 Probabilistic question answering on the web
* Communications of the ACM 2005 NewsInEssence: summarizing online news topics
* EMNLP 2007 Semi-supervised classification for extracting protein interaction sentences using dependency parsing
* Bioinformatics 2008 Identifying gene-disease associations using centrality on a literature mined gene-interaction network
* IEEE Intelligent Systems 2008 natural language processing and the web
* NAACL 2009 Generating surveys of scientific paradigms
* Nucleic Acids Research 2009 Michigan molecular interactions r2: from interacting proteins to pathways
* J. of the American Association of Information Science and Technology 2009 Visual overviews for discovering key papers and influences across research fronts
* KDD 2010 Divrank: the interplay of prestige and diversity in information networks
* American J. of Political Science 2010 How to Analyze Political Attention with Minimal Assumptions and Costs
* Arxiv 2011 The effect of linguistic constraints on the large scale organization of language
* J. of Biomedical Semantics 2011 Mining of vaccine-associated ifn-gamma gene interaction networks using the vaccine ontology

==External links==
* [http://www.nsf.gov/news/news_summ.jsp?cntn_id=112073 Team USA Brings Home the Linguistics Gold]
* [http://www.eecs.umich.edu/eecs/about/articles/2011/Radev-LSA11.html Dragomir Radev, Co-Founders Recognized as NACLO Receives Linguistics, Language and the Public Award]
* [http://www.eecs.umich.edu/eecs/about/articles/2010/Radev-Linguistics.html Dragomir Radev Coaches US Linguistics Team to Multiple Wins]
* [http://www.eecs.umich.edu/eecs/about/articles/2009/Radev-ACM-DM.html Dragomir Radev Honored as ACM Distinguished Scientist]
* [http://www.eecs.umich.edu/eecs/etc/news/shownews.cgi?428 Prof. Dragomir Radev Receives Gosnell Prize]

== References ==
{{reflist}}
&lt;!--- After listing your sources please cite them using inline citations and place them after the information they cite. Please see http://en.wikipedia.org/wiki/Wikipedia:REFB for instructions on how to add citations. ---&gt;
*
*
*
*


{{DEFAULTSORT:Radev, Dragomir R.}}
[[Category:Year of birth missing (living people)]]
[[Category:Living people]]

[[Category:Columbia School of Engineering and Applied Science alumni]]
[[Category:American computer scientists]]
[[Category:University of Michigan faculty]]
[[Category:Natural language processing]]
[[Category:Information retrieval researchers]]
[[Category:Fellows of the Association for Computing Machinery]]</text>
      <sha1>hhk5fper3t7ctndjuawin91u1cxj2ma</sha1>
    </revision>
  </page>
  <page>
    <title>Karen Spärck Jones</title>
    <ns>0</ns>
    <id>17212387</id>
    <revision>
      <id>754982191</id>
      <parentid>736905046</parentid>
      <timestamp>2016-12-15T15:59:27Z</timestamp>
      <contributor>
        <ip>50.46.147.154</ip>
      </contributor>
      <comment>/* Karen Spärck Jones Award */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="9141" xml:space="preserve">{{Infobox scientist
| name = Karen Spärck Jones
| image = Karen Spärck.jpg
| caption = Karen Spärck Jones in 2002
| birth_date = {{birth date|1935|8|26|df=y}}
| birth_place = [[Huddersfield]], [[Yorkshire]]
| death_date = {{death date and age|2007|4|4|1935|8|26|df=y}}
| death_place = [[Willingham, Cambridgeshire]]
| residence = United Kingdom
| nationality = British
| field = Computer science
| work_institution = [[University of Cambridge Computer Laboratory]]
| alma_mater = University of Cambridge
| doctoral_advisor = Richard Braithwaite&lt;ref name=odnb&gt;{{cite web|title=Jones, Karen Ida Boalth Spärck (1935–2007), Computer Scientist|url=http://www.oxforddnb.com/view/article/98729|work=Oxford Dictionary of National Biography|publisher=Oxford University Press|accessdate=5 October 2014}}&lt;/ref&gt;
| thesis_title = Synonymy and Semantic Classiﬁcation
| thesis_year = 1964&lt;ref&gt;{{cite book|author=Karen Spärck Jones|title=Synonymy and Semantic Classification (thesis published as a book)|publisher=Edinburgh University Press|series=Edinburgh Information Technology series|volume=1|year=1986}}&lt;/ref&gt;
| doctoral_students = 
| known_for  = work on information retrieval and natural language processing, in particular her probabilistic model of document and text retrieval
| prizes = ACL Lifetime Achievement Award, BCS Lovelace Medal, ACM-AAAI Allen Newell Award, ACM SIGIR Salton Award, American Society for Information Science and Technology’s Award of Merit
| religion = 
| spouse = [[Roger Needham]]
| website = {{URL|http://www.cl.cam.ac.uk/archive/ksj21}}
}}

'''Karen Spärck Jones''' [[Fellow of the British Academy|FBA]] (26 August 1935 – 4 April 2007) was a [[United Kingdom|British]] computer scientist.&lt;ref&gt;{{Cite journal | last1 = Tait | first1 = J. I. | title = Karen Spärck Jones | doi = 10.1162/coli.2007.33.3.289 | journal = Computational Linguistics | volume = 33 | issue = 3 | pages = 289–291 | year = 2007 | pmid =  | pmc = }}&lt;/ref&gt;&lt;ref&gt;{{Cite journal | last1 = Robertson | first1 = S. | last2 = Tait | first2 = J. | doi = 10.1002/asi.20784 | title = Karen Spärck Jones | journal = Journal of the American Society for Information Science and Technology | volume = 59 | issue = 5 | pages = 852 | year = 2008 | pmid =  | pmc = }}&lt;/ref&gt;

==Personal life==
Karen Ida Boalth Spärck Jones was born in [[Huddersfield]], [[Yorkshire]], [[England]]. Her father was Owen Jones, a lecturer in chemistry, and her mother was Ida Spärck, a [[Norway|Norwegian]] who moved to Britain during [[World War II]].  They left Norway on one of the last boats out after the German invasion in 1940.&lt;ref name="odnb" /&gt; Spärck Jones was educated at a grammar school in Huddersfield and then [[Girton College, Cambridge]] from 1953 to 1956, reading History, with an additional final year in Moral Sciences (philosophy). She briefly became a school teacher, before moving into Computer Science.  During her career in Computer Science, she campaigned hard for more women to enter computing.&lt;ref name="odnb" /&gt;   She was married to fellow Cambridge computer scientist [[Roger Needham]] until his death in 2003. She died 4 April 2007 at [[Willingham, Cambridgeshire|Willingham]] in [[Cambridgeshire]].

==Career==
She worked at the Cambridge Language Research Unit from the late 1950s,&lt;ref&gt;{{cite web|url=http://www.cl.cam.ac.uk/misc/obituaries/sparck-jones/|title=Computer Laboratory obituary}}&lt;/ref&gt; then at [[University of Cambridge|Cambridge's]] [[Cambridge University Computer Laboratory|Computer Laboratory]] from 1974, and retired in 2002, holding the post of Professor of Computers and Information, which she was awarded in 1999.&lt;ref name="odnb" /&gt; She continued to work in the Computer Laboratory until shortly before her death. Her main research interests, since the late 1950s, were [[natural language processing]] and [[information retrieval]].&lt;ref name="doi10.1108/eb026526"&gt;{{Cite journal | last1 = Spärck Jones | first1 = K. | authorlink1 = Karen Spärck Jones| doi = 10.1108/eb026526 | title = A Statistical Interpretation of Term Specificity and Its Application in Retrieval | journal = Journal of Documentation | volume = 28 | pages = 11–21 | year = 1972 | url = http://www.emeraldinsight.com/doi/abs/10.1108/eb026526| pmid =  | pmc = }}&lt;/ref&gt;&lt;ref&gt;{{Cite journal | editor1-last = Tait | editor1-first = John I. | title = Charting a New Course: Natural Language Processing and Information Retrieval, Essays in Honour of Karen Spärck Jones| doi = 10.1007/1-4020-3467-9 | series = The Kluwer International Series on Information Retrieval | volume = 16 | year = 2005 | isbn = 1-4020-3343-5 | pmid =  | pmc = }}&lt;/ref&gt; One of her most important contributions was the concept of [[inverse document frequency]] (IDF) weighting in information retrieval, which she introduced in a 1972 paper.&lt;ref name="doi10.1108/eb026526"/&gt;&lt;ref name="idf"&gt;{{Cite journal | last1 = Spärck Jones | first1 = K. | authorlink1 = Karen Spärck Jones| title = Index term weighting | doi = 10.1016/0020-0271(73)90043-0 | journal = Information Storage and Retrieval | volume = 9 | issue = 11 | pages = 619–633 | year = 1973 | pmid =  | pmc = }}&lt;/ref&gt; IDF is used in most search engines today, usually as part of the [[tf-idf]] weighting scheme.&lt;ref&gt;{{Cite book | last1 = Maybury | first1 = M. T. | chapter = Karen Spärck Jones and Summarization | doi = 10.1007/1-4020-3467-9_7 | title = Charting a New Course: Natural Language Processing and Information Retrieval | series = The Kluwer International Series on Information Retrieval | volume = 16 | pages = 99–10 | year = 2005 | isbn = 1-4020-3343-5 | pmid =  | pmc = }}&lt;/ref&gt;

There is an annual [[British Computer Society|BCS]] lecture named in her honour.&lt;ref&gt;{{cite web|title=Karen Spärck Jones lecture|url=http://academy.bcs.org/ksj|work=BCS Academy of Computing|publisher=British Computer Society|accessdate=3 October 2013}}&lt;/ref&gt;

===Honours===
* Fellow of the [[British Academy]], of which she was Vice-President in 2000–02
* Fellow of [[AAAI]]
* Fellow of [[ECCAI]]
* President of the [[Association for Computational Linguistics]] in 1994

===Awards===
* [[Gerard Salton Award]] (1988)
* [[ASIS&amp;T]] Award of Merit (2002)
* [[Association for Computational Linguistics|ACL]] Lifetime Achievement Award (2004) &lt;ref&gt;{{cite web|title=ACL Lifetime Achievement Award Recipients|url=http://aclweb.org/aclwiki/index.php?title=ACL_Lifetime_Achievement_Award_Recipients|website=ACL wiki|publisher=[[Association for Computational Linguistics|ACL]]|accessdate=16 August 2014}}&lt;/ref&gt;
* [[British Computer Society|BCS]] [[Lovelace Medal]] (2007)
* [[ACM - AAAI Allen Newell Award]] (2006)

==Karen Spärck Jones Award==
To commemorate her achievements, the Karen Spärck Jones Award was created in 2008 by the [[British Computer Society|BCS]] and its Information Retrieval Specialist Group (BCS IRSG), which is sponsored by [[Microsoft Research]].&lt;ref&gt;[http://irsg.bcs.org/ksjaward.php Microsoft BCS/BCS IRSG Karen Spärck Jones Award An Award to Commemorate Karen Spärck Jones]&lt;/ref&gt;

The recipients are:
* 2016, [[Jaime Teevan]]
* 2015, [[Jordan Boyd-Graber]], [[Emine Yilmaz]]
* 2014, [[Ryen White]]
* 2013, [[Eugene Agichtein]]
* 2012, [[Diane Kelly(computer scientist)]]
* 2011, No award was made
* 2010, [[Evgeniy Gabrilovich]]
* 2009, [[Mirella Lapata]]

==References==
{{reflist}}

==Further reading==
* [http://spectrum.ieee.org/may07/5063 Computer Science, A Woman's Work], IEEE Spectrum, May 2007

==External links==
*[http://www.cl.cam.ac.uk/misc/obituaries/sparck-jones/video/ Video: Natural Language and the Information Layer, Karen Spärck Jones, March 2007]
*[http://www.cl.cam.ac.uk/misc/obituaries/sparck-jones/ University of Cambridge obituary]
*[http://news.independent.co.uk/people/obituaries/article2441969.ece Obituary], ''[[The Independent]]'', 12 April 2007 {{dead link|date=April 2014}}
*[http://www.telegraph.co.uk/news/main.jhtml?view=DETAILS&amp;grid=&amp;xml=/news/2007/04/12/db1201.xml  Obituary], ''[[The Daily Telegraph]]'', 12 April 2007 {{dead link|date=April 2014}}
*[http://www.timesonline.co.uk/tol/comment/obituaries/article1968942.ece Obituary], ''[[The Times]]'', 22 June 2007 {{subscription required}}

{{s-start}}
{{s-ach}}
{{succession box |
 before=[[Makoto Nagao]] |
 title=ACL Lifetime Achievement Award |
 after=[[Martin Kay]] |
 years=2004}}
{{s-end}}

{{Authority control}}

{{DEFAULTSORT:Sparck Jones, Karen}}
[[Category:1935 births]]
[[Category:2007 deaths]]
[[Category:Alumni of Girton College, Cambridge]]
[[Category:British computer scientists]]
[[Category:Women computer scientists]]
[[Category:Fellows of the British Academy]]
[[Category:Fellows of the Association for the Advancement of Artificial Intelligence]]
[[Category:Fellows of Newnham College, Cambridge]]
[[Category:Fellows of Wolfson College, Cambridge]]
[[Category:Members of the University of Cambridge Computer Laboratory]]
[[Category:People from Huddersfield]]
[[Category:Deaths from cancer in England]]
[[Category:Information retrieval researchers]]
[[Category:British women scientists]]
[[Category:Artificial intelligence researchers]]
[[Category:20th-century women scientists]]</text>
      <sha1>0q0ioemeyo5xj2f2cty45c1k78vmtqi</sha1>
    </revision>
  </page>
  <page>
    <title>W. Bruce Croft</title>
    <ns>0</ns>
    <id>24963451</id>
    <revision>
      <id>737978961</id>
      <parentid>733554283</parentid>
      <timestamp>2016-09-06T05:43:58Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor />
      <comment>/* top */Removed invisible unicode characters + other fixes, replaced: →   (2) using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3683" xml:space="preserve">'''W. Bruce Croft''' is a [[distinguished professor]] of [[computer science]] at the [[University of Massachusetts Amherst]] whose work focuses on [[information retrieval]].&lt;ref&gt;
{{Cite web
  | last = Croft
  | first = W. Bruce
  | title = Biography
  | url=http://ciir.cs.umass.edu/personnel/croftbio.pdf
  | accessdate = November 4, 2009}}&lt;/ref&gt;
He is the founder of the [[Center for Intelligent Information Retrieval]] and served as the editor-in-chief of [[ACM Transactions on Information Systems]] from 1995 to 2002.  He was also a member of the [[United States National Research Council|National Research Council]] [http://sites.nationalacademies.org/CSTB/index.htm Computer Science and Telecommunications Board] from 2000 to 2003. Since 2015, he is the Dean of the College of Information and Computer Sciences at the University of Massachusetts Amherst. He was Chair of the UMass Amherst Computer Science Department from 2001 to 2007.

Bruce Croft formed the [[Center for Intelligent Information Retrieval]] (CIIR) in 1991, since when he and his students have worked with more than 90 industry and government partners on research and technology projects and have produced more than 900 papers. Bruce Croft has made major contributions to most areas of information retrieval, including pioneering work in clustering, passage retrieval, sentence retrieval, and distributed search. One of the most important areas of work for Croft  relates to ranking functions and retrieval models, where he has led the development of one of the major approaches to modeling search: language modelling. In later years, Croft also led the way in the development of feature-based ranking functions. Croft and his research group have also developed a series of search engines: InQuery, the Lemur toolkit, Indri, and Galago. These search engines are open source and offer unique capabilities that are not replicated in other research retrieval platforms source – consequently they are downloaded by hundreds of researchers world wide. As a consequence of his work, Croft is one of the most cited researchers in information retrieval.

==Education==
Croft earned a bachelor's degree with honors in 1973 and a master's degree in computer science in 1974 from [[Monash University]] in [[Melbourne|Melbourne, Australia]].  He earned his Ph.D in computer science from the [[University of Cambridge]] in 1979 and joined the [[University of Massachusetts Amherst|University of Massachusetts, Amherst]] faculty later that year.

==Honors and awards==
Croft has received several prestigious awards, including:
* [[ACM Fellow]] in 1997
* [[American Society for Information Science and Technology]] Research Award in 2000
* [[Gerard Salton Award]] (a lifetime achievement award) from ACM SIGIR in 2003
* [[Tony Kent Strix award|Tony Kent Strix Award]] in 2013
* IEEE Computer Society Technical Achievement Award in 2014
* [http://sigir.org/awards/best-student-paper-awards/ Best Student Paper Award] from SIGIR in 1997 and 2005
* [http://sigir.org/awards/test-of-time-awards/ Test of Time Award] from SIGIR for his papers published in 1990, 1995, 1996, 1998, 2001
* Many other publications are short-listed as the Best Paper Award in SIGIR and CIKM

==References==
&lt;references/&gt;

==External links==
* [http://ciir.cs.umass.edu/personnel/croft.html Faculty homepage]

{{DEFAULTSORT:Croft, W. Bruce}}
[[Category:American computer scientists]]
[[Category:Fellows of the Association for Computing Machinery]]
[[Category:University of Massachusetts Amherst faculty]]
[[Category:Year of birth missing (living people)]]
[[Category:Living people]]
[[Category:Information retrieval researchers]]


{{Compu-bio-stub}}</text>
      <sha1>bch9e61g1i4m4aeq9fk1sbndjhb8ipv</sha1>
    </revision>
  </page>
  <page>
    <title>Norbert Fuhr</title>
    <ns>0</ns>
    <id>37022703</id>
    <revision>
      <id>722307345</id>
      <parentid>668400217</parentid>
      <timestamp>2016-05-27T07:18:12Z</timestamp>
      <contributor>
        <username>KasparBot</username>
        <id>24420788</id>
      </contributor>
      <comment>migrating [[Wikipedia:Persondata|Persondata]] to Wikidata, [[toollabs:kasparbot/persondata/|please help]], see [[toollabs:kasparbot/persondata/challenge.php/article/Norbert Fuhr|challenges for this article]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1975" xml:space="preserve">'''Norbert Fuhr''' (born 1956) is a professor of computer science
and the leader of the Duisburg Information Engineering Group based at
the [[University of Duisburg-Essen]], Germany.

==Education==
His first  degree is in  technical computer science, which he got  from the Electrical
Engineering Department of  the [[Technical University of Darmstadt]] in 1980, and in 1986 he finished his PhD
(Dr.-Ing) in the Computer Science Department of the same university on "Probabilistic
Indexing and Retrieval".&lt;ref name="Fuhr1986"&gt;{{citation
 | author=Fuhr, Norbert
 | publisher=Fachinformationszentrum Karlsruhe
 | title=Probabilistisches Indexing und Retrieval
 | year=1986
}}&lt;/ref&gt;

==Profession==
He held a PostDoc position in Darmstadt until
1991, when he was appointed Associate Professor in the  Computer
Science Department  of  the [[Technical University of Dortmund]]. Since 2002, he is a full professor at the
[[University of Duisburg-Essen]].

==Honors and awards==
Fuhr's dissertation was awarded the  "Gerhard Pietsch Award" of the German Society
of Documentation in 1987. In 2012, he received the  
[[Gerard Salton Award]].&lt;ref name="Fuhr2012"&gt;{{citation
 | author=Fuhr, Norbert
 | journal=SIGIR '12 Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval
 | title=Salton award lecture: information retrieval as engineering science
 | pages=1–2
 | year=2012
 | doi=10.1145/2348283.2348285
}}&lt;/ref&gt;

==References==
{{reflist}}

==External links==
*[http://www.is.inf.uni-due.de/staff/fuhr.html Norbert Fuhr - University of Duisburg-Essen]

{{Authority control}}
{{DEFAULTSORT:Fuhr, Norbert}}
[[Category:German computer scientists]]
[[Category:1956 births]]
[[Category:Living people]]
[[Category:University of Duisburg-Essen faculty]]
[[Category:Technical University of Dortmund faculty]]
[[Category:Technische Universität Darmstadt alumni]]
[[Category:Information retrieval researchers]]


{{Compu-bio-stub}}</text>
      <sha1>che7bny1vjcooclnbx8jn2isuw7k8qv</sha1>
    </revision>
  </page>
  <page>
    <title>KM programming language</title>
    <ns>0</ns>
    <id>525334</id>
    <revision>
      <id>743296284</id>
      <parentid>654949221</parentid>
      <timestamp>2016-10-09T00:57:26Z</timestamp>
      <contributor>
        <username>Jarble</username>
        <id>7226930</id>
      </contributor>
      <comment>Undid revision 654949221 by [[Special:Contributions/182.66.2.206|182.66.2.206]] ([[User talk:182.66.2.206|talk]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1100" xml:space="preserve">{{Infobox programming language
| name = KM
| paradigm = [[knowledge representation]]
| generation =
| year = 
| designer =
| developer = 
| latest_release_version = 
| latest_release_date = 
| turing-complete = 
| typing = 
| implementations = 
| dialects = 
| influenced_by = [[KRL (programming language)|KRL]]
| influenced = 
}}

'''KM''', the '''Knowledge Machine''', is a [[Knowledge frame|frame]]-based language used for [[knowledge representation]] work.

It has first-order logic semantics, and includes machinery for reasoning, including selection by description, unification, classification, and reasoning about actions. Its origins were the Theo language and [[KRL (programming language)|KRL]], and is implemented in [[Lisp (programming language)|Lisp]].

==External links==
* [http://www.cs.utexas.edu/users/mfkb/RKF/km.html KM: The Knowledge Machine]. 
* An Ontology editor for the KM language: [http://www.algo.be/ref-projects.htm#KMgen/ KMgen].

[[Category:Declarative programming languages]]
[[Category:Knowledge representation]]
[[Category:Common Lisp software]]


{{compu-lang-stub}}</text>
      <sha1>seqv6s20syh9zrer9y7b9fvytkhxv3b</sha1>
    </revision>
  </page>
  <page>
    <title>Meta Content Framework</title>
    <ns>0</ns>
    <id>1053030</id>
    <revision>
      <id>713093369</id>
      <parentid>694907063</parentid>
      <timestamp>2016-04-01T22:17:38Z</timestamp>
      <contributor>
        <username>Cyberbot II</username>
        <id>16283967</id>
      </contributor>
      <comment>Rescuing 2 sources. #IABot</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2749" xml:space="preserve">'''Meta Content Framework''' ('''MCF''') is a specification of a [[content format]] for structuring [[metadata]] about [[web site]]s and other [[data]].

==History==
MCF was developed by [[Ramanathan V. Guha]] at [[Apple Advanced Technology Group|Apple Computer's Advanced Technology Group]] between 1995 and 1997. Rooted in [[Knowledge representation and reasoning | knowledge-representation]] systems such as [[CycL]], [[KRL (programming language)| KRL]], and [[Knowledge Interchange Format|KIF]], it sought to describe objects, their attributes, and the relationships between them.&lt;ref name=hammersley&gt;{{Cite book| publisher = O'Reilly| isbn = 978-0-596-00383-8| last = Hammersley| first = Ben| title = Content Syndication with RSS| location = Sebastopol| date = 2003| page=2}}&lt;/ref&gt;

One application of MCF was [[HotSauce]], also developed by Guha while at Apple. It generated a [[3D computer graphics|3D]] [[visualization (graphic)|visualization]] of a web site's table of contents, based on MCF descriptions. By late 1996, a few hundred sites were creating MCF files and Apple HotSauce allowed users to browse these MCF representations in 3D.&lt;ref name=hammersley /&gt;

When the research project was discontinued, Guha left Apple for [[Netscape Communications Corporation|Netscape]], where, in collaboration with [[Tim Bray]], he adapted MCF to use [[XML]]&lt;ref&gt;{{Cite conference
| publisher = W3C
| last = Guha
| first = R V
|author2=Tim Bray
 | title = Meta Content Framework Using XML
| accessdate = 2014-09-14
| date = 1997-06-06
| url = http://www.w3.org/TR/NOTE-MCF-XML/
}}&lt;/ref&gt;&lt;ref&gt;{{Cite web|last1=Guha |first1=R.V. |last2=Bray |first2=Tim |title=Meta Content Framework Using XML |work=Netscape |accessdate=2015-12-12 |date=1997-06-13 |url=http://developer.netscape.com/mcf.html |deadurl=yes |archiveurl=https://web.archive.org/web/19970615144715/http://developer.netscape.com/mcf.html |archivedate=June 15, 1997 }}&lt;/ref&gt; and created the first version of the [[Resource Description Framework]] (RDF).&lt;ref&gt;{{Cite web|last=Andreessen |first=Marc |title=Innovators of the Net: R.V. Guha and RDF |work=Netscape |accessdate=2014-09-14 |date=1999-01-08 |url=http://wp.netscape.com/columns/techvision/innovators_rg.html |deadurl=yes |archiveurl=https://web.archive.org/web/20080205163659/http://wp.netscape.com/columns/techvision/innovators_rg.html |archivedate=February 5, 2008 }}&lt;/ref&gt;

==References==
{{reflist}}

==External links==
*[http://www.textuality.com/mcf/MCF-tutorial.html MCF Tutorial] (using XML syntax)
*[http://www.guha.com/mcf/ Guha MCF site]
*[http://downlode.org/Etext/MCF/towards_a_theory_of_metacontent.html The metacontent concept]

[[Category:Knowledge representation]]
[[Category:Apple Inc. software]]

{{compu-AI-stub}}</text>
      <sha1>l2qkcs9dli230ql43s5oyh3rpfny723</sha1>
    </revision>
  </page>
  <page>
    <title>Logic form</title>
    <ns>0</ns>
    <id>1936537</id>
    <revision>
      <id>750076051</id>
      <parentid>686866750</parentid>
      <timestamp>2016-11-17T18:05:36Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor />
      <comment>1 archive template merged to {{[[template:webarchive|webarchive]]}} ([[User:Green_Cardamom/Webarchive_template_merge|WAM]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3180" xml:space="preserve">'''Logic forms''' are simple, [[first-order logic]] [[knowledge representation]]s of [[natural language]] sentences formed by the conjunction of concept predicates related through shared arguments. Each noun, verb, adjective, adverb, pronoun, preposition and conjunction generates a predicate. Logic forms can be decorated with [[word sense]]s to [[Word sense disambiguation|disambiguate]] the semantics of the word. There are two types of predicates: events are marked with ''e'', and entities are marked with ''x''. The shared arguments connect the subjects and objects of verbs and prepositions together. Example input/output might look like this:
 Input:  '''The Earth provides the food we eat every day.'''
 Output: '''Earth''':n_#1(&lt;span style="color:#008800;"&gt;x1&lt;/span&gt;) '''provide''':v_#2(&lt;span style="color:#888800;"&gt;e1&lt;/span&gt;, &lt;span style="color:#008800;"&gt;x1&lt;/span&gt;, &lt;span style="color:#880000;"&gt;x2&lt;/span&gt;) '''food''':n_#1(&lt;span style="color:#880000;"&gt;x2&lt;/span&gt;) '''we'''(&lt;span style="color:#000088;"&gt;x3&lt;/span&gt;) '''eat''':v_#1(&lt;span style="color:#880088;"&gt;e2&lt;/span&gt;, &lt;span style="color:#000088;"&gt;x3&lt;/span&gt;, &lt;span style="color:#880000;"&gt;x2&lt;/span&gt;; &lt;span style="color:#008888;"&gt;x4&lt;/span&gt;) '''day''':n_#1(&lt;span style="color:#008888;"&gt;x4&lt;/span&gt;)

Logic forms are used in some [[natural language processing]] techniques, such as [[question answering]], as well as in [[inference]] both for [[database]] systems and QA systems.

==Evaluations==
[http://www.senseval.org/ SENSEVAL-3] in 2004 introduced a {{webarchive |url=https://web.archive.org/web/20050902115653/site=http://www.cs.iusb.edu/~vasile/logic/indexLF.html |date=September 2, 2005 |title=Logic Form Identification task }}.

==References==
*{{cite book | author=Vasile Rus | title=Logic Form for WordNet Glosses  | url=http://www.engr.smu.edu/~vasile/rus02.PhDThesis.ps | publisher=Ph.D. thesis, Southern Methodist University | year=2002 }} &lt;!-- Most information in the article derived from Vasile's work --&gt;
*{{cite journal | author=Vasile Rus and Dan Moldovan | title=High performance logic form transformation | journal=International Journal for Tools with Artificial Intelligence. IEEE Computer Society, IEEE Press |date=September 2002 | volume=11| issue =  3 | pages=437–454 | url=http://www.worldscinet.com/ijait/11/1103/S0218213002000976.html}}
*{{cite conference | author=Dan Moldovan and Vasile Rus | url=http://engr.smu.edu/~vasile/acl2001.ps | title=Logic Form transformation of wordNet and its Applicability to question answering | booktitle=Proceedings of ACL 2001, Toulouse, France | year=2001 | pages=}}
*{{cite conference | author=Jerry R. Hobbs | title=Overview of the TACITUS project | booktitle=Computational Linguistics| year=1986 | pages=12(3)}}
*{{cite conference | author=Vasile Rus | url=http://acl.ldc.upenn.edu/acl2004/senseval/pdf/rus.pdf | title=A First Evaluation of Logic Form Identification Systems | booktitle=SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text | year=2004 | pages=|format=PDF}}

[[Category:Natural language processing]]
[[Category:Computational linguistics]]
[[Category:Knowledge representation]]
{{ling-stub}}</text>
      <sha1>2tfugragxl0hvkmai4u5gj63vvpnlgv</sha1>
    </revision>
  </page>
  <page>
    <title>Faceted classification</title>
    <ns>0</ns>
    <id>796657</id>
    <revision>
      <id>713819476</id>
      <parentid>709403952</parentid>
      <timestamp>2016-04-06T01:34:20Z</timestamp>
      <contributor>
        <ip>68.187.232.213</ip>
      </contributor>
      <comment>/* Art and Architecture Thesaurus (AAT) */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="13158" xml:space="preserve">{{mergefrom|Faceted search|date=January 2015}}
'''Faceted classification''' is a [[classification scheme]] used in organizing knowledge into a systematic order. A faceted classification uses semantic categories, either general or subject-specific, that are combined to create the full classification entry. Many library classification systems use a combination of a fixed, enumerative taxonomy of concepts with subordinate facets that further refine the topic.

== Definition ==

There are two primary types of classification used for information organization: enumerative and faceted. An enumerative classification contains a full set of entries for all concepts.&lt;ref name=lcsh&gt;{{Citation
 |publisher = Libraries Unlimited
 |isbn = 1591581540
 |publication-place = Westport, Conn
 |title = Library of Congress subject headings
 |url = http://openlibrary.org/books/OL3311856M/Library_of_Congress_subject_headings
 |author = Lois Mai Chan
 |publication-date = 2005
 |id = 1591581540
 }}&lt;/ref&gt; A faceted classification system uses a set of semantically cohesive categories that are combined as needed to create an expression of a concept. In this way, the faceted classification is not limited to already defined concepts. While this makes the classification quite flexible, it also makes the resulting expression of topics complex.&lt;ref name=sven&gt;{{Citation
        |publisher = MIT Press
        |isbn = 0262194333
        |publication-place = Cambridge, Mass
        |title = The intellectual foundation of information organization
        |url = http://openlibrary.org/books/OL44967M/The_intellectual_foundation_of_information_organization
        |author = Elaine Svenonius
        |publication-date = 2000
        |id = 0262194333
        }}&lt;/ref&gt; To the extent possible, facets represent "clearly defined, mutually exclusive, and collectively exhaustive aspects, properties or characteristics of a class or specific subject".&lt;ref name=taylor&gt;Taylor, A. G. (1992). Introduction to Cataloging and Classification. 8th ed. Englewood, Colorado: Libraries Unlimited.&lt;/ref&gt; Some commonly used general-purpose facets are time, place, and form.&lt;ref name=chan /&gt;

There are few purely faceted classifications; the best known of these is the [[Colon Classification]] of [[S. R. Ranganathan]], a general knowledge classification for libraries. Some other faceted classifications are specific to special topics, such as the Art and Architecture Thesaurus and the faceted classification of occupational safety and health topics created by D. J. Foskett for the International Labour Organization.&lt;ref name=coyle /&gt;

Many library classifications combine the enumerative and faceted classification techniques. The [[Dewey Decimal Classification]], the [[Library of Congress Classification]], and the [[Universal Decimal Classification]] all make use of facets at various points in their enumerated classification schedules. The allowed facets vary based on the subject area of the classification. These facets are recorded as tables that represent recurring types of subdivisions within subject areas. There are general facets that can be used wherever appropriate, such as geographic subdivisions of the topic. Other tables are applied only to specific areas of the schedules. Facets can be combined to create a complex subject statement.&lt;ref name=chan /&gt;

Arlene Taylor describes faceted classification using an analogy: “If one thinks of each of the faces of a cut and polished diamond as a facet for the whole diamond, one can picture a classification notation that has small notations  standing for subparts of the whole topic strung together to create a complete classification notation”.&lt;ref&gt;{{cite book|last1=Taylor|first1=Arlene G.|year=2004|title=The organization of information|location=Westport, CT|publisher=Libraries Unlimited}}&lt;/ref&gt;

Faceted classifications exhibit many of the same problems as classifications based on a hierarchy. In particular, some concepts could belong in more than one facet, so their placement in the classification may appear to be arbitrary to the classifier. It also tends to result in a complex notation because each facet must be distinguishable as recorded.&lt;ref name=sven /&gt;

== Retrieval ==

Search in systems with faceted classification can enable a user to navigate information along multiple paths corresponding to different orderings of the facets. This contrasts with traditional taxonomies in which the hierarchy of categories is fixed and unchanging.&lt;ref name="Star, S.L. 1998"&gt;Star, S.L. (1998, Fall). Grounded classification: grounded theory and faceted classification. [Electronic version]. Library Trends. 47.2, 218.&lt;/ref&gt;  It is also possible to use facets to filter search results to more quickly find desired results.

==Examples of Faceted Classifications==

=== Colon classification for library materials ===
The [[colon classification]] developed by [[S. R. Ranganathan]] is an example of general faceted classification designed to be applied to all library materials. In the Colon Classification system, a book is assigned a set of values from each independent facet.&lt;ref&gt;Garfield, E. (1984, February). A tribute to S.R. Ranganathan, the father of Indian library science. Essays of an Information Scientist, 7, 37-44.&lt;/ref&gt;  This facet formula uses punctuation marks and symbols placed between the facets to connect them. Colon classification was named after its use of the colon as the primary symbol in its notation.&lt;ref&gt;Chan, L.M. (1994). Cataloging and classification.  New York: McGraw-Hill, Inc.&lt;/ref&gt;&lt;ref&gt;[http://www.essessreference.com/servlet/esGetBiblio?bno=000374 Colon Classification (6th Edition)] by Dr. S.R. Ranganathan, published by Ess Publications, Delhi, India.&lt;/ref&gt;

Ranganathan stated that hierarchical classification schemes like the Dewey Decimal Classification (DDC) or the Library of Congress Subject Headings are too limiting and finite to use for modern classification and that many items can pertain information to more than one subject.  He organized his classification scheme into 42 classes.  Each class can be categorized according to particular characteristics, that he called facets.  Ranganathan said that there are five fundamental categories that can be used to demonstrate the facets of a subject: personality, material, energy, space and time.  He called this the PMEST formula:&lt;ref&gt;Ranganathan, S. R (1987). Colon classification, 7th ed. revised and edited by M.A. Gopinath. Bangalore: Sarada Ranganathan Endowment for Library Science, 1987&lt;/ref&gt;  
*Personality is the most specific or focal subject.
*Matter is the substance, properties or materials of the subject.
*Energy includes the processes, operations and activities.
*Space relates to the geographic location of the subject.
*Time refers to the dates or seasons of the subject.

=== Universal Decimal Classification ===
Another example of a faceted classification scheme is the [[Universal Decimal Classification]] (UDC), the UDC is considered to be a complex multilingual classification that can be used in all fields of knowledge.&lt;ref&gt;About universal decimal classification and the udc consortium. (2006). Retrieved November 30, 2013, from http://www.udcc.org/about.htm&lt;/ref&gt;
The Universal Decimal Classification scheme was created at the end of the nineteenth century by Belgian bibliographers [[Paul Otlet]] and [[Henri la Fontaine]]. The goal of their system was to create an index that would be able to record knowledge even if it is stored in non-conventional ways including materials in notebooks and ephemera. They also wanted their index to organize material systematically instead of alphabetically.&lt;ref&gt;Batty, D. (2003). Universal decimal classification.  Encyclopedia of Library and Information Science.&lt;/ref&gt;

The UDC has an overall taxonomy of knowledge that is extended with a number of facets, such as language, form, place and time. Each facet has its own symbol in the notation, such as: "=" for language; "-02" for materials, "[...]" for subordinate concepts.&lt;ref name=chan&gt;{{Cite book     |publisher = The Scarecrow Press, Inc.     |isbn = 978-0-8108-5944-9     |title = Cataloging and classification     |url = http://openlibrary.org/books/OL9558667M/Cataloging_and_Classification     |last = Chan|first=Lois Mai|edition = Third     |publication-date = 2007 |page=321|id = 0810859440}}&lt;/ref&gt;

===Faceted Classification for Occupational Safety and Health===

[[Douglas John Foskett|D. J. Foskett]], a member of the [[Classification Research Group]] in London, developed classification of occupational safety and health materials for the library of the [[International Labour Organization]].&lt;ref name=coyle&gt;{{cite journal|last1=Coyle|first1=Karen|title=A Faceted Classification for Occupational Safety and Health|journal=Special Libraries|date=1975|volume=66|issue=5-6|pages=256–9}}&lt;/ref&gt;&lt;ref name=foskett&gt;{{cite book|last1=Foskett|first1=D. J.|title=Proceedings of the International Conference on Scientific Information|chapter=Construction of a Faceted Classification for a Special Subject|date=1959|publisher=National Science Foundation|isbn=0-309-57421-8|pages=867–888}}&lt;/ref&gt; After a study of the literature in the field, he created the classification with the following facets:

*Facet A: Occupational Safety and Health: General
*Facet B: Special Classes of Workers, Industries
*Facet C: Sources of Hazards: Fire, Machinery, etc.
*Facet D: Industrial Accidents and Diseases
*Facet E: Preventive Measures, Protection
*Facet F: Organisation, Administration

Notation was solely alphabetic, with the sub-facets organized hierarchically using extended codes, such as "g Industrial equipment and processes", "ge Machines".&lt;ref name=foskett /&gt;

===Art and Architecture Thesaurus (AAT)===

While not strictly a classification system, the [[Art and Architecture Thesaurus|AAT]] uses facets similar to those of Ranganathan's Colon Classification:

*Associated Concepts (e.g., philosophy)
*Physical Attributes
*Styles and Periods
*Agents (People/Organizations)
*Activities (similar to Ranganathan's Energy)
*Materials (similar to Ranganathan's Matter)
*Objects (similar to Ranganathan's Personality)&lt;ref name=denton&gt;{{cite web |url=https://www.miskatonic.org/library/facet-web-howto.html|author=William Denton|title=How to Make a Faceted Classification and Put it on the Web}}&lt;/ref&gt;

==Comparison between faceted and single hierarchical classification==
Hierarchical classification refers to the classification of objects using one ''single'' hierarchical taxonomy. Faceted classification may actually employ hierarchy in one or more of its facets, but allows for the use of more than one taxonomy to classify objects.

*Faceted classification systems allow the assignment of multiple classifications to an object, and enable those classifications to be applied by searchers in multiple ways, rather than in a single, predetermined order. Multiple facets may be used as a first step in a search process.&lt;ref name="Categories, Facets—and Browsable Facets?"&gt;Sirovich, Jaimie (2011). Categories, Facets—and Browsable Facets?, from http://www.uxmatters.com/mt/archives/2011/08/categories-facetsand-browsable-facets.php&lt;/ref&gt; For example, one may ''start'' from language or subject.
*Hierarchical classification systems are developed classes that are subdivided from the most general subjects to the most specific.&lt;ref&gt;Reitz, Joan M. (2004). Dictionary for library and information science. Westport, CT: Libraries Unlimited&lt;/ref&gt;
*Faceted classification systems allow for the combination of facets to [[Filter (software)|filter]] the set of objects rapidly. In addition, the facets can be used to address multiple classification criteria.&lt;ref&gt;Godert, Winfried. F. (1991). Facet classification in online retrieval. International Classification, 18, 98-109&lt;/ref&gt;
*A faceted system focuses on the important, essential or persistent characteristics of content objects, helping it to be useful for categorization of fine-grained rapidly changing repositories.
*In faceted classification systems one does not have to know the name of the category into which an object is placed a priori. A controlled vocabulary is presented with the number of documents matching each vocabulary term.
*New facets may be created at any time without disruption of a single hierarchy or reorganizing other facets.
*Faceted classification systems make few assumptions about the scope and organization of the domain. It is difficult to ''break'' a faceted classification schema.&lt;ref&gt;Adkisson, Hiedi P. (2005).  Use of faceted classification.  Retrieved December 1, 2013, from http://www.webdesignpractices.com/navigation/facets.html&lt;/ref&gt;

==See also==
* [[Classification Research Group]]
* [[Controlled vocabulary]]
* [[Findability]]
* [[Folksonomy]]
* [[Information architecture]]
* [[Tag (metadata)]]
* [[Universal Decimal Classification]]

==References==
{{Reflist|colwidth=35em}}

==External links==
* [http://eprints.soton.ac.uk/271488/ How to ''Reuse'' a Faceted Classification and Put It On the ''Semantic'' Web]

[[Category:Knowledge representation]]
[[Category:Library cataloging and classification]]</text>
      <sha1>iidhs17tl23jnqykao6y0c0uqwfk2pc</sha1>
    </revision>
  </page>
  <page>
    <title>Guideline execution engine</title>
    <ns>0</ns>
    <id>2804505</id>
    <revision>
      <id>758777023</id>
      <parentid>722973190</parentid>
      <timestamp>2017-01-07T14:53:42Z</timestamp>
      <contributor>
        <username>Rjwilmsi</username>
        <id>203434</id>
      </contributor>
      <minor />
      <comment>/* Use of third party workflow engine as a guideline execution engine */Journal cites, set missing volume/pages parameter,  using [[Project:AWB|AWB]] (12142)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3959" xml:space="preserve">A '''Guideline Execution Engine''' is a [[computer program]] which can interpret a [[guideline (medical)|clinical guideline]] represented in a computerized format and perform actions towards the user of an [[electronic medical record]].

A Guideline Execution Engine needs to communicate with a host [[Clinical information system]]. [[virtual Medical Record|vMR]] is one possible interface which can be used.

The engine's main function is to manage instances of executed guidelines of individual patients.

Delivering the inferred engine recommendations or impacts to the host Clinical information system  has to carefully respect current workflow of the clinicians (physicians, nurses, clerks, etc.)

== Architecture of Guideline Execution Engine ==
The following modules are generally needed for any engine

* interface to Clinical Information System
* new guidelines loading module
* guideline interpreter module
* clinical events parser
* alert/recommendations dispatch

== Guideline Interchange Format ==

The ''Guideline Interchange Format (GLIF)'' is computer representation format for [[clinical guideline]]s.&lt;ref&gt;{{cite web |url=http://mis.hevra.haifa.ac.il/~morpeleg/Intermed/ |title=Guideline Representation Page: GLIF 2.0, 3.4, 3.5 Specifications |work=Stanford University, School of Medicine, InterMed Collaboratory }}&lt;/ref&gt; Represented guidelines can be executed using a guideline execution engine.

The format has several versions as it has been improved. In 2003 GLIF3 was introduced.

== Use of third party workflow engine as a guideline execution engine ==
Some commercial Electronic Health Record systems use a [[workflow engine]] to execute clinical guidelines. RetroGuide&lt;ref name=eval&gt;{{Cite journal 
| last1 = Huser | first1 = V. 
| last2 = Narus | first2 = S. P. 
| last3 = Rocha | first3 = R. A. 
| doi = 10.1016/j.jbi.2009.06.001 
| title = Evaluation of a flowchart-based EHR query system: A case study of RetroGuide☆ 
| journal = Journal of Biomedical Informatics 
| volume = 43 
| issue = 1 
| pages = 41–50 
| year = 2010 
| pmid = 19560553 
| pmc =2840619 
}}&lt;/ref&gt; and HealthFlow&lt;ref name=hf2010&gt;{{citation|pmc=3079703|title=Implementation of workflow engine technology to deliver basic clinical decision support functionality|journal=BMC Med Res Methodol.|year=2011|volume= 11|page= 43|doi=10.1186/1471-2288-11-43|pmid=21477364|vauthors=Huser V, Rasmussen LV, Oberg R, Starren JB}}&lt;/ref&gt;  are examples of such an approach.

== See also ==

*[[Electronic medical record]]
*[[Clinical practice guideline]]
*[[Medical algorithm]]
*[[Arden syntax]]
*[[Healthcare workflow]]
*[[Glif]]
*[[RetroGuide]]

== References ==
&lt;references/&gt;

== External links ==
*{{cite journal  |vauthors=Wang D, Peleg M, Tu SW, etal |title=Design and implementation of the GLIF3 guideline execution engine |journal=J Biomed Inform |volume=37 |issue=5 |pages=305–18 |date=October 2004 |pmid=15488745 |doi=10.1016/j.jbi.2004.06.002 |url=http://linkinghub.elsevier.com/retrieve/pii/S1532046404000668}} [http://bmir.stanford.edu/file_asset/index.php/940/BMIR-2004-1008.pdf (PDF)]
*{{cite journal  |vauthors=Ram P, Berg D, Tu S, etal |title=Executing clinical practice guidelines using the SAGE execution engine |journal=Stud Health Technol Inform |volume=107 |issue=Pt 1 |pages=251–5 |year=2004 |pmid=15360813 }}
*{{cite journal |vauthors=Tu SW, Campbell J, Musen MA |title=The structure of guideline recommendations: a synthesis |journal=AMIA Annu Symp Proc |volume= |issue= |pages=679–83 |year=2003 |pmid=14728259 |pmc=1480008 }} [http://bmir.stanford.edu/file_asset/index.php/1511/BMIR-2003-0966.pdf (PDF)]
*{{cite journal |vauthors=Tu SW, Musen MA |title=A flexible approach to guideline modeling |journal=Proc AMIA Symp |volume= |issue= |pages=420–4 |year=1999 |pmid=10566393 |pmc=2232509 }} [http://bmir.stanford.edu/file_asset/index.php/211/BMIR-1999-0789.pdf (PDF)]

[[Category:Health informatics]]
[[Category:Knowledge representation]]</text>
      <sha1>013tbzl6f2j90c1vi978tdudqiaa1hk</sha1>
    </revision>
  </page>
  <page>
    <title>CDS ISIS</title>
    <ns>0</ns>
    <id>2889648</id>
    <revision>
      <id>683750557</id>
      <parentid>606951351</parentid>
      <timestamp>2015-10-02T07:48:58Z</timestamp>
      <contributor>
        <username>Ymblanter</username>
        <id>14596827</id>
      </contributor>
      <comment>/* See also */  rm redlinks</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3577" xml:space="preserve">{{Use dmy dates|date=May 2014}}
'''CDS/ISIS''' is a [[software]] package for generalised ''Information Storage and Retrieval systems'' developed, maintained and disseminated by [[UNESCO]]. It was first released in 1985 and since then over 20,000 [[license|licences]] have been issued by UNESCO and a worldwide network of distributors. It is particularly suited to bibliographical applications and is used for the [[library catalog|catalogues]] of many small and medium-sized [[library|libraries]]. Versions have been produced in Arabic, Chinese, English, French, German, Portuguese, Russian and Spanish amongst other languages. UNESCO makes the software available free for non-commercial purposes, though distributors are allowed to charge for their expenses.

CDS/ISIS is an acronym which stands for Computerised Documentation Service / Integrated Set of Information Systems. In 2003 it was stated that "This package is accepted by libraries in the developing countries as a standard software for information system development".&lt;ref&gt;National Science Foundation of Sri Lanka. "CDS ISIS Library Software" [Last Update 10 January 2003.] http://www.nsf.ac.lk/slstic/isis.htm  Accessed 20 June 2007.&lt;/ref&gt;

The original CDS/ISIS ran on an [[IBM]] [[mainframe computer|mainframe]] and was designed in the mid-1970s under Mr Giampaolo Del Bigio for UNESCO's Computerized Documentation System (CDS). It was based on the internal ISIS (Integrated Set of Information Systems) at the [[International Labour Organization]] in Geneva.

In 1985 a version was produced for mini- and microcomputers programmed in Pascal. It ran on an [[IBM PC]] under [[MS-DOS]]&lt;ref&gt;Buxton, Andrew and Hopkinson, Alan. ''The CDS/ISIS handbook''. London: Library Association, 1994&lt;/ref&gt;
. ''Winisis'', the [[Microsoft Windows|Windows]] version,  first demonstrated in 1995, may run on a single [[computer]] or in a [[local area network]]. A ''JavaISIS'' client/server component was designed in 2000, allowing remote [[database management system|database management]] over the [[Internet]] from [[Microsoft Windows|Windows]], [[Linux]] and [[Apple Macintosh|Macintosh]] computers. Furthermore, ''GenISIS'' allows the user to produce [[HTML]] Web forms for CDS/ISIS database searching. The ''ISIS_DLL'' provides an [[API]] for developing CDS/ISIS based applications. The [[OpenIsis]] library, developed independently from 2002 to 2004, provided another [[API]] for developing CDS/ISIS-like applications.

The most recent effort towards a completely renewed [[Free and open-source software|FOSS]], [[Unicode]] implementation of CDS/ISIS is the J-Isis project, developed by UNESCO since 2005 and currently maintained by Mr Jean Claude Dauphin.

== See also ==
* [[IDIS (software)|IDIS]] is a tool for direct data exchange between CDS/ISIS and IDAMS.

== External links ==
* [http://kenai.com/projects/j-isis J-ISIS New UNESCO Java CDS/ISIS Software]
* [http://portal.unesco.org/ci/en/ev.php-URL_ID=2071&amp;URL_DO=DO_TOPIC&amp;URL_SECTION=201.html CDS/ISIS database software (UNESCO)]
* [http://lists.iccisis.org International list hosted from 2010 by the ICCIsis (International Coordination Committee on ISIS)]
* [https://listserv.surfnet.nl/archives/cds-isis.html Archives of CDS-ISIS@NIC.SURFNET.NL (discontinued in 2010)]
* http://openisis.org/ (discontinued)
* http://sourceforge.net/projects/isis (discontinued)
* [http://pecl.php.net/package/isis PHP extension for reading CDS/ISIS databases]

== References ==
{{Reflist}}

[[Category:Knowledge representation]]
[[Category:Proprietary database management systems]]</text>
      <sha1>r96j1m0jielydootm3a3l2m458qk4hh</sha1>
    </revision>
  </page>
  <page>
    <title>Philosophy of information</title>
    <ns>0</ns>
    <id>4522868</id>
    <revision>
      <id>758240131</id>
      <parentid>724075297</parentid>
      <timestamp>2017-01-04T07:52:07Z</timestamp>
      <contributor>
        <username>Libcub</username>
        <id>6307086</id>
      </contributor>
      <minor />
      <comment>/* See also */ deleted redlink</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11341" xml:space="preserve">{{Information science}}
The '''philosophy of information''' ('''PI''') is the area of research that studies conceptual issues arising at the intersection of [[computer science]], [[information science]], [[information technology]], and [[philosophy]].

It includes:

# the critical investigation of the conceptual nature and basic principles of [[information]], including its dynamics, utilisation and sciences
# the elaboration and application of information-theoretic and computational methodologies to philosophical problems.&lt;ref&gt;Luciano Floridi, [http://www.blackwellpublishing.com/pci/downloads/introduction.pdf "What is the Philosophy of Information?"], ''Metaphilosophy'', 2002, (33), 1/2.&lt;/ref&gt;

==History==
The philosophy of information (PI) has evolved from the [[philosophy of artificial intelligence]], [[logic of information]], [[cybernetics]], [[social theory]], [[ethics]] and the study of language and information.

===Logic of information===
The [[logic of information]], also known as the ''logical theory of information'', considers the information content of logical [[sign (semiotics)|sign]]s and expressions along the lines initially developed by [[Charles Sanders Peirce]].

===Cybernetics===
One source for the philosophy of information can be found in the technical work of [[Norbert Wiener]], [[Alan Turing]] (though his work has a wholly different origin and theoretical framework), [[William Ross Ashby]], [[Claude Shannon]], [[Warren Weaver]], and many other scientists working on computing and information theory back in the early 1950s. See the main article on [[Cybernetics]].

Some important work on information and communication was done by [[Gregory Bateson]] and his colleagues.

===Study of language and information===
Later contributions to the field were made by [[Fred Dretske]], [[Jon Barwise]], [[Brian Cantwell Smith]], and others.

The [[Center for the Study of Language and Information|Center for the Study of Language and Information (CSLI)]] was founded at Stanford University in 1983 by philosophers, computer scientists, linguists, and psychologists, under the direction of [[John Perry (philosopher)|John Perry]] and [[Jon Barwise]].

===P.I.===
More recently this field has become known as the philosophy of information. The expression was coined in the 1990s by [[Luciano Floridi]], who has published prolifically in this area with the intention of elaborating a unified and coherent, conceptual frame for the whole subject.{{citation needed|date=April 2015}}

==Definitions of "information"==

The concept ''information'' has been defined by several theorists.

===Peirce===
[[Charles S. Peirce]]'s theory of information was embedded in his wider theory of symbolic communication he called the ''semeiotic'', now a major part of [[semiotics]]. For Peirce, information integrates the aspects of [[sign]]s and [[Expression (mathematics)|expressions]] separately covered by the concepts of [[denotation]] and [[extension (semantics)|extension]], on the one hand, and by [[connotation]] and [[comprehension (logic)|comprehension]] on the other.

=== Shannon and Weaver ===
Claude E. Shannon, for his part, was very cautious: "The word 'information' has been given different meanings by various writers in the general field of information theory. It is likely that at least a number of these will prove sufficiently useful in certain applications to deserve further study and permanent recognition. It is hardly to be expected that a single concept of information would satisfactorily account for the numerous possible applications of this general field." (Shannon 1993, p.&amp;nbsp;180){{full citation needed|date=April 2015}}. Thus, following Shannon, Weaver supported a tripartite analysis of information in terms of (1) technical problems concerning the quantification of information and dealt with by Shannon's theory; (2) semantic problems relating to meaning and truth; and (3) what he called "influential" problems concerning the impact and effectiveness of information on human behaviour, which he thought had to play an equally important role. And these are only two early examples of the problems raised by any analysis of information.

A map of the main senses in which one may speak of information is provided by  [http://plato.stanford.edu/entries/information-semantic/ the Stanford Encyclopedia of Philosophy article]. The previous paragraphs are based on it.

===Bateson===
[[Gregory Bateson]] defined information as "a difference that makes a difference".&lt;ref&gt;[http://plato.acadiau.ca/courses/educ/reid/papers/PME25-WS4/SEM.html Extract from "Steps to an Ecology of Mind"]&lt;/ref&gt; which is based on [[Donald M. MacKay]]: information is a distinction that makes a difference.&lt;ref&gt;The Philosophy of Information.
Luciano Floridi. Chapter 4. Oxford University Press, USA (March 8, 2011) ASIN: 0199232385 [http://www.amazon.com/Philosophy-Information-Luciano-Floridi/dp/0199232385]&lt;/ref&gt;

===Floridi===
According to Luciano Floridi{{citation needed|date=April 2015}}, four kinds of mutually compatible phenomena are commonly referred to as "information": 
*  Information about something (e.g. a train timetable)
*  Information as something (e.g. DNA, or fingerprints)
*  Information for something (e.g. algorithms or instructions)
*  Information in something (e.g. a pattern or a constraint).

The word "information" is commonly used so metaphorically or so abstractly that the meaning is unclear.

==Philosophical directions==

===Computing and philosophy===
Recent creative advances and efforts in [[computing]], such as [[semantic web]], [[ontology engineering]], [[knowledge engineering]], and modern [[artificial intelligence]] provide [[philosophy]] with fertile notions, new and evolving subject matters, methodologies, and models for philosophical inquiry.  While [[computer science]] brings new opportunities and challenges to traditional philosophical studies, and changes the ways philosophers understand foundational concepts in philosophy, further major progress in [[computer science]] would only be feasible when philosophy provides sound foundations for areas such as bioinformatics, software engineering, knowledge engineering, and ontologies.

Classical topics in philosophy, namely, [[mind]], [[consciousness]], [[experience]], [[reasoning]], [[knowledge]], [[truth]], [[morality]] and [[creativity]] are rapidly becoming common concerns and foci of investigation in [[computer science]], e.g., in areas such as agent computing, [[software agents]], and intelligent mobile agent technologies.{{citation needed|date=December 2012}}

According to Luciano Floridi "&lt;ref&gt;Luciano Floridi, [http://www.philosophyofinformation.net/publications/pdf/oppi.pdf ''Open Problems in the Philosophy of Information''] ''Metaphilosophy'' 35.4, 554-582. Revised version of ''The Herbert A. Simon Lecture on Computing and Philosophy'' given at Carnegie Mellon University in 2001, with [http://ethics.sandiego.edu/video/CAP/CMU2001/Floridi/index.html RealVideo]&lt;/ref&gt; one can think of several ways for applying computational methods towards philosophical matters:
# Conceptual experiments in silico: As an innovative extension of an ancient tradition of [[thought experiment]], a trend has begun in philosophy to apply computational [[Computer model|modeling]] schemes to questions in [[logic]], [[epistemology]], [[philosophy of science]], [[philosophy of biology]], [[philosophy of mind]], and so on.
# [[Digital physics#Pancomputationalism or the computational universe theory|Pancomputationalism]]: By this view, computational and informational concepts are considered to be so powerful that given the right level of [[abstraction]], anything in the world could be modeled and represented as a computational system, and any process could be simulated computationally. Then, however, pancomputationalists have the hard task of providing credible answers to the following two questions:
## how can one avoid blurring all differences among systems?
## what would it mean for the system under investigation not to be an [[Information system|informational system]] (or a computational system, if computation is the same as information processing)?

===Information and society===
Numerous philosophers and other thinkers have carried out philosophical studies of the social and cultural aspects of electronically mediated information.

* [[Albert Borgmann]], ''Holding onto Reality: The Nature of Information at the Turn of the Millennium'' (Chicago University Press, 1999)
* [[Mark Poster]], ''The Mode of Information'' (Chicago Press, 1990)
* [[Luciano Floridi]], "The Informational Nature of Reality", ''Fourth International European Conference on Computing and Philosophy'' 2006 (Dragvoll Campus, NTNU Norwegian University for Science and Technology, Trondheim, Norway, 22–24 June 2006).

==See also==
{{col-begin}}
{{col-break}}
* [[Barwise prize]]
* [[Complex system]]
* [[Digital divide]]
* [[Digital philosophy]]
* [[Digital physics]]
* [[Game theory]]
* [[Freedom of information]]
* [[Informatics (academic field)|Informatics]]
{{col-break}}
* [[Information]]
* [[Information art]]
* [[Information ethics]]
* [[Information theory]]
* [[International Association for Computing and Philosophy]]
* [[Logic of information]]
{{col-break}}
* [[Philosophy of artificial intelligence]]
* [[Philosophy of computer science]]
* [[Philosophy of technology]]
* [[Philosophy of thermal and statistical physics]]
* [[Physical information]]
* [[Relational quantum mechanics]]
* [[Social informatics]]
* [[Statistical mechanics]]
{{col-end}}

==Notes==
{{reflist}}

==Further reading==
*[[Luciano Floridi]], "[http://www.blackwellpublishing.com/pci/downloads/introduction.pdf What is the Philosophy of Information?]" ''Metaphilosophy'', 33.1/2: 123-145. Reprinted in T.W. Bynum and J.H. Moor (eds.), 2003. ''CyberPhilosophy: The Intersection of Philosophy and Computing''. Oxford – New York: Blackwell.
*-------- (ed.), 2004. ''[http://www.blackwellpublishing.com/pci/default.htm The Blackwell Guide to the Philosophy of Computing and Information.]'' Oxford - New York: Blackwell.
*Greco, G.M., Paronitti G., Turilli M., and Floridi L., 2005. ''[http://www.wolfson.ox.ac.uk/~floridi/pdf/htdpi.pdf How to Do Philosophy Informationally.]'' ''Lecture Notes on Artificial Intelligence'' 3782, pp.&amp;nbsp;623–634.

== External links ==
{{Library resources box}}
*{{cite SEP |url-id=information |title=Information |last=Adriaans |first=Peter |editor-last=Zalta |editor-first=Edward N. ||date=Autumn  2013}}
*{{cite SEP |url-id=information-semantic |title=Semantic Conceptions of Information |last=Floridi |first=Luciano |editor-last=Zalta |editor-first=Edward N. |date=Spring 2015}}
*[http://web.comlab.ox.ac.uk/oucl/research/areas/ieg/ IEG site], the Oxford University research group on the philosophy of information.
*[[Luciano Floridi]], "[https://web.archive.org/web/20060820223325/http://academicfeeds.friwebteknologi.org/index.php?id=28 Where are we in the philosophy of information?]" [[University of Bergen]], [[Norway]]. Podcast dated 21.06.06.

{{Navboxes
|list=
{{Philosophy topics}}
{{philosophy of language}}
{{philosophy of mind}}
{{philosophy of science}}
}}

[[Category:Philosophy by topic|Inf]]
[[Category:Philosophy of artificial intelligence]]
[[Category:Knowledge representation]]</text>
      <sha1>i952x4zlmiqjgje6zy1ov0v7e39uya8</sha1>
    </revision>
  </page>
  <page>
    <title>Frame language</title>
    <ns>0</ns>
    <id>485226</id>
    <revision>
      <id>758150012</id>
      <parentid>753160858</parentid>
      <timestamp>2017-01-03T19:36:00Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 1 sources and tagging 0 as dead. #IABot (v1.2.7.1)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="22899" xml:space="preserve">{{Duplication|dupe=Frame (artificial intelligence)}}

A '''frame language''' is a technology used for [[knowledge representation]] in [[artificial intelligence]]. Frames are stored as [[Ontology (information science)|ontologies]] of [[Set theory|sets]] and subsets of the [[Frame_(artificial_intelligence)|frame concepts]]. They are similar to class hierarchies in [[object-oriented languages]] although their fundamental design goals are different. Frames are focused on explicit and intuitive representation of knowledge whereas objects focus on [[Encapsulation (object-oriented programming)|encapsulation]] and [[information hiding]]. Frames originated in AI research and objects primarily in [[software engineering]]. However, in practice the techniques and capabilities of frame and object-oriented languages overlap significantly.

==Description==
Early work on Frames was inspired by psychological research going back to the 1930s that indicated people use stored stereotypical knowledge to interpret and act in new cognitive situations.&lt;ref&gt;{{cite book|last=Bartlett|first=F.C.|title=Remembering: A Study in Experimental and Social Psychology|year=1932|publisher=Cambridge University Press|location=Cambridge, England}}&lt;/ref&gt;  The term Frame was first used by [[Marvin Minsky]] as a paradigm to understand visual reasoning and natural language processing.&lt;ref&gt;{{cite book|last=Minsky|first=Marvin|title=The Psychology of Computer Vision|year=1975|publisher=McGraw Hill|location=New York|pages=211–277|editor=Pat Winston|chapter=A Framework for Representing Knowledge}}&lt;/ref&gt; In these and many other types of problems the potential solution space for even the smallest problem is huge. For example, extracting the phonemes from a raw audio stream or detecting the edges of an object. Things which seem trivial to humans are actually quite complex. In fact, how difficult they really were was probably not fully understood until AI researchers began to investigate the complexity of getting computers to solve them.

The initial notion of Frames or Scripts as they were also called is that they would establish the context for a problem and in so doing automatically reduce the possible search space significantly. The idea was also adopted by Schank and Abelson who used it to illustrate how an AI system could process common human interactions such as ordering a meal at a restaurant.&lt;ref&gt;{{cite book|last=Schank|first=Roger|title=Scripts, Plans, Goals, and Understanding|year=1977|publisher=Lawrence Erlbaum|location=Hillsdale, New Jersey|author2=R. P. Abelson}}&lt;/ref&gt;  These interactions were standardized as Frames with slots that stored relevant information about each Frame. Slots are analogous to object properties in object-oriented modeling and to relations in entity-relation models. Slots often had default values but also required further refinement as part of the execution of each instance of the scenario. I.e., the execution of a task such as ordering at a restaurant was controlled by starting with a basic instance of the Frame and then instantiating and refining various values as appropriate. Essentially the abstract Frame represented an object class and the frame instances an object instance. In this early work the emphasis was primarily on the static data descriptions of the Frame. Various mechanisms were developed to define the range of a slot, default values, etc. However, even in these early systems there were procedural capabilities. One common technique was to use "triggers" (similar to the database concept of triggers) attached to slots. A trigger was simply procedural code that was attached to a slot. The trigger could fire either before and/or after a slot value was accessed or modified.

As with object classes, Frames were organized in [[Subsumption relation|subsumption]] hierarchies. For example, a basic frame might be ordering at a restaurant. An instance of that would be Joe goes to McDonalds. A specialization (essentially a [[Subclass (computer science)|subclass]]) of the restaurant frame would be a frame for ordering at a fancy restaurant. The fancy restaurant frame would inherit all the default values from the restaurant frame but also would either add more slots or change one or more of the default values (e.g., expected price range) for the specialized frame.&lt;ref&gt;{{cite book|last=Feigenbaum|first=Edward|title=The Handbook of Artificial Intelligence, Volume III|publisher=Addison-Wesley|isbn=0201118114|pages=216–222|url=https://archive.org/stream/handbookofartific01barr#page/156/mode/2up|author2=Avron Barr|date=September 1, 1986}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last=Bobrow|first=D.G.|author2=Terry Winograd|title=An Overview of KRL: A Knowledge Representation Language|journal=Cognitive Science|year=1977|volume=1|pages=3–46|doi=10.1207/s15516709cog0101_2}}&lt;/ref&gt;

Much of the early Frame language research (e.g. Schank and Abelson) had been driven by findings from experimental psychology and attempts to design knowledge representation tools that corresponded to the patterns humans were thought to use to function in daily tasks. These researchers were less interested in mathematical formality since they believed such formalisms were not necessarily good models for the way the average human conceptualizes the world. The way humans use language for example is often far from truly logical.

Similarly, in linguistics, [[Charles J. Fillmore]] in the mid-1970s started working on his theory of [[Frame semantics (linguistics)|frame semantics]], which later would lead to computational resources like [[FrameNet]].&lt;ref&gt;{{cite news|last=Lakoff|first=George|title=Charles Fillmore, Discoverer of Frame Semantics, Dies in SF at 84: He Figured Out How Framing Works|url=http://www.huffingtonpost.com/george-lakoff/charles-fillmore-discover_b_4807590.html|accessdate=7 March 2014|newspaper=The Huffington Post|date=18 February 2014}}&lt;/ref&gt; Frame semantics was motivated by reflections on human language and human cognition.

Researchers such as [[Ron Brachman]] on the other hand wanted to give AI researchers the mathematical formalism and computational power that were associated with Logic. Their aim was to map the Frame classes, slots, constraints, and rules in a Frame language to set theory and logic.  One of the benefits of this approach is that the validation and even creation of the models could be automated using theorem provers and other automated reasoning capabilities. The drawback was that it could be more difficult to initially specify the model in a language with a formal semantics.

This evolution also illustrates a classic divide in AI research known as the "[[neats vs. scruffies]]". The "neats" were researchers who placed the most value on mathematical precision and formalism which could be achieved via [[First Order Logic]] and [[Set Theory]]. The "scruffies" were more interested in modeling knowledge in representations that were intuitive and psychologically meaningful to humans.&lt;ref&gt;{{cite book|last=Crevier|first=Daniel|title=AI: The Tumultuous Search for Artificial Intelligence|year=1993|publisher=Basic Books|location=New York|isbn=0-465-02997-3|page=168}}&lt;/ref&gt;

The most notable of the more formal approaches was the [[KL-ONE]] language.&lt;ref&gt;{{cite journal|last=Brachman|first=Ron|title=A Structural Paradigm for Representing Knowledge|journal=Bolt, Beranek, and Neumann Technical Report|year=1978|issue=3605}}&lt;/ref&gt; KL-ONE later went on to spawn several subsequent Frame languages. The formal semantics of languages such as KL-ONE gave these frame languages a new type of automated reasoning capability known as the [[Deductive classifier|classifier]]. The classifier is an engine that analyzes the various declarations in the frame language: the definition of sets, subsets, relations, etc. The classifier can then automatically deduce various additional relations and can detect when some parts of a model are inconsistent with each other. In this way many of the tasks that would normally be executed by forward or backward chaining in an inference engine can instead be performed by the classifier.&lt;ref&gt;{{cite journal|last=MacGregor|first=Robert|title=Using a description classifier to enhance knowledge representation|journal=IEEE Expert|date=June 1991|volume=6|issue=3|url=http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=87683&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D87683|accessdate=10 November 2013|doi=10.1109/64.87683|pages=41–46}}&lt;/ref&gt;

This technology is especially valuable in dealing with the Internet. It is an interesting result that the formalism of languages such as KL-ONE can be most useful dealing with the highly informal and unstructured data found on the Internet. On the Internet it is simply not feasible to require all systems to standardize on one data model. It is inevitable that terminology will be used in multiple inconsistent forms. The automatic classification capability of the classifier engine provides AI developers with a powerful toolbox to help bring order and consistency to a very inconsistent collection of data (i.e., the Internet). The vision for an enhanced Internet, where pages are ordered not just by text keywords but by classification of concepts is known as the [[Semantic Web]]. Classification technology originally developed for Frame languages is a key enabler of the Semantic Web.&lt;ref&gt;{{cite journal|last=Berners-Lee |first=Tim |author2=James Hendler |author3=Ora Lassila |title=The Semantic Web A new form of Web content that is meaningful to computers will unleash a revolution of new possibilities |journal=Scientific American |date=May 17, 2001 |url=http://www.cs.umd.edu/~golbeck/LBSC690/SemanticWeb.html |doi=10.1038/scientificamerican0501-34 |volume=284 |pages=34–43 |deadurl=yes |archiveurl=https://web.archive.org/web/20130424071228/http://www.cs.umd.edu/%7Egolbeck/LBSC690/SemanticWeb.html |archivedate=2013-04-24 |df= }}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Horridge|first=Mathew|title=Protégé OWL Tutorial A step-by-step guide to modelling in OWL using the popular Protégé OWL tools.|url=http://130.88.198.11/tutorials/protegeowltutorial/|work=Manchester University|publisher=Manchester University|accessdate=9 December 2013}}&lt;/ref&gt; The "neats vs. scruffies" divide also emerged in Semantic Web research, culminating in the creation of the [[Linking Open Data]] community—their focus was on exposing data on the Web rather than modeling.

==Example==
A simple example of concepts modeled in a frame language is the [[FOAF (ontology)|Friend of A Friend (FOAF) ontology]] defined as part of the Semantic Web as a foundation for social networking and calendar systems. The primary frame in this simple example is a ''Person''. Example slots are the person's ''email'', ''home page, phone,'' etc. The interests of each person can be represented by additional frames describing the space of business and entertainment domains. The slot ''knows'' links each person with other persons. Default values for a person's interests can be inferred by the web of people they are friends of.&lt;ref&gt;{{cite web|title=FOAF|url=http://semanticweb.org/wiki/FOAF|website=http://semanticweb.org|accessdate=7 June 2014}}&lt;/ref&gt;

==Implementations==
The earliest Frame based languages were custom developed for specific research projects and were not packaged as tools to be re-used by other researchers. Just as with [[expert system]] [[inference engine]]s, researchers soon realized the benefits of extracting part of the core infrastructure and developing general purpose frame languages that were not coupled to specific applications. One of the first general purpose frame languages was KRL.&lt;ref&gt;{{cite journal|last=Bobrow|first=D.G.|author2=Terry Winograd|title=An Overview of KRL: A Knowledge Representation Language|journal=Cognitive Science|year=1977|volume=1|pages=3–46|doi=10.1207/s15516709cog0101_2}}&lt;/ref&gt; One of the most influential early Frame languages was [[KL-ONE]]&lt;ref&gt;{{cite journal|last=Brachman|first=Ron|title=A Structural Paradigm for Representing Knowledge|journal=Bolt, Beranek, and Neumann Technical Report|year=1978|issue=3605}}&lt;/ref&gt; KL-ONE spawned several subsequent Frame languages. One of the most widely used successors to KL-ONE was the [[LOOM (ontology)|Loom language]] developed by Robert MacGregor at the [[Information Sciences Institute]].&lt;ref&gt;{{cite journal|last=MacGregor|first=Robert|title=Using a description classifier to enhance knowledge representation|journal=IEEE Expert|date=June 1991|volume=6|issue=3|url=http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=87683&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D87683|accessdate=10 November 2013|doi=10.1109/64.87683|pages=41–46}}&lt;/ref&gt;

In the 1980s Artificial Intelligence generated a great deal of interest in the business world fueled by expert systems. This led to the development of many commercial products for the development of knowledge-based systems. These early products were usually developed in Lisp and integrated constructs such as IF-THEN rules for logical reasoning with Frame hierarchies for representing data. One of the most well known of these early Lisp knowledge-base tools was the [[Knowledge Engineering Environment]] (KEE) from [[IntelliCorp (software)|Intellicorp]]. KEE provided a full Frame language with multiple inheritance, slots, triggers, default values, and a rule engine that supported backward and forward chaining. As with most early commercial versions of AI software KEE was originally deployed in [[Lisp (programming language)|Lisp]] on [[Lisp machine]] platforms but was eventually ported to PCs and Unix workstations.&lt;ref&gt;{{cite journal|last=Mettrey|first=William|title=An Assessment of Tools for Building Large Knowledge-Based Systems|journal=AI Magazine|year=1987|volume= 8| issue = 4|url=http://www.aaai.org/ojs/index.php/aimagazine/article/viewArticle/625}}&lt;/ref&gt;

The research agenda of the [[Semantic Web]] spawned a renewed interest in automatic classification and frame languages.  An example is the [[Web Ontology Language]] (OWL) standard for describing information on the Internet. OWL is a standard to provide a semantic layer on top of the Internet. The goal is that rather than organizing the web using keywords as most applications (e.g. Google) do today the web can be organized by concepts organized in an ontology.

The name of the OWL language itself provides a good example of the value of a Semantic Web. If one were to search for "OWL" using the Internet today most of the pages retrieved would be on the bird [[Owl]] rather than the standard [[Web Ontology Language|OWL]]. With a Semantic Web it would be possible to specify the concept "Web Ontology Language" and the user would not need to worry about the various possible acronyms or synonyms as part of the search. Likewise the user would not need to worry about homonyms crowding the search results with irrelevant data such as information about birds of prey as in this simple example.

In addition to OWL various standards and technologies that are relevant to the Semantic Web and were influenced by Frame languages include [[Ontology Inference Layer|OIL]] and [[DARPA Agent Markup Language|DAML]].  The [[Protégé (software)|Protege]] Open Source software tool from Stanford University provides an ontology editing capability that is built on OWL and has the full capabilities of a classifier.&lt;ref&gt;{{cite web|last=Horridge|first=Mathew|title=Protégé OWL Tutorial A step-by-step guide to modelling in OWL using the popular Protégé OWL tools.|url=http://130.88.198.11/tutorials/protegeowltutorial/|work=Manchester University|publisher=Manchester University|accessdate=9 December 2013}}&lt;/ref&gt;

==Comparison of frames and objects==
Frame languages have a significant overlap with [[object-oriented]] languages. The terminologies and goals of the two communities were different but as they moved from the academic world and labs to the commercial world developers tended to not care about philosophical issues and focused primarily on specific capabilities, taking the best from either camp regardless of where the idea began. What both paradigms have in common is a desire to reduce the distance between concepts in the real world and their implementation in software. As such both paradigms arrived at the idea of representing the primary software objects in taxonomies starting with very general types and progressing to more specific types.

The following table illustrates the correlation between standard terminology from the object-oriented and frame language communities:

{| class="wikitable"
|-
! Frame Terminology !! OO Terminology
|-
| Frame || Object Class
|-
| Slot || Object property or attribute
|-
| Trigger || Accessor and Mutator methods
|-
| Method (e.g. Loom, KEE) || Method
|}

The primary difference between the two paradigms was in the degree that encapsulation was considered a major requirement. For the object-oriented paradigm encapsulation was one of the if not the most critical requirement. The desire to reduce the potential interactions between software components and hence manage large complex systems was a key driver of object-oriented technology. For the frame language camp this requirement was less critical than the desire to provide a vast array of possible tools to represent rules, constraints, and programming logic. In the object-oriented world everything is controlled by methods and the visibility of methods. So for example, accessing the data value of an object property must be done via an accessor method. This method controls things such as validating the data type and constraints on the value being retrieved or set on the property. In Frame languages these same types of constraints could be handled in multiple ways. Triggers could be defined to fire before or after a value was set or retrieved. Rules could be defined that managed the same types of constraints. The slots themselves could be augmented with additional information (called "facets" in some languages) again with the same type of constraint information.

The other main differeniator between frame and OO languages was multiple inheritance (allowing a frame or class to have two or more superclasses). For frame languages multiple inheritance was a requirement.  This follows from the desire to model the world the way humans do, human conceptualizations of the world seldom fall into rigidly defined non-overlapping taxonomies. For many OO languages, especially in the later years of OO, single inheritance was either strongly desired or required. Multiple inheritance was seen as a possible step in the analysis phase to model a domain but something that should be eliminated in the design and implementation phases in the name of maintaining encapsulation and modularity.&lt;ref&gt;{{cite web|title=The Unified Modeling Language|url=http://www.essentialstrategies.com/publications/modeling/uml.htm|work=essentialstrategies.com|publisher=Essential Strategies Inc.|accessdate=10 December 2013|year=1999|quote=In your author’s experience, nearly all examples that appear to require multiple inheritance or multiple type hierarchies can be solved by attacking the model from a different direction.}}&lt;/ref&gt;

Although the early frame languages such as KRL did not include message passing, driven by the demands of developers, most of the later frame languages (e.g. Loom, KEE) included the ability to define messages on Frames.&lt;ref&gt;{{cite journal|last=Mettrey|first=William|title=An Assessment of Tools for Building Large Knowledge-Based Systems|journal=AI Magazine|year=1987|volume= 8| issue = 4|url=http://www.aaai.org/ojs/index.php/aimagazine/article/viewArticle/625}}&lt;/ref&gt;

On the object-oriented side, standards have also emerged that provide essentially the equivalent functionality that frame languages provided, albeit in a different format and all standardized on object libraries. For example, the [[Object Management Group]] has standardized specifications for capabilities such as associating test data and constraints with objects (analogous to common uses for facets in Frames and to constraints in Frame languages such as Loom) and for integrating rule engines.&lt;ref&gt;{{cite web|last=Macgregor|first=Robert|title=Retrospective on Loom|url=http://www.isi.edu/isd/LOOM/papers/macgregor/Loom_Retrospective.html|work=isi.edu|publisher=Information Sciences Institute|accessdate=10 December 2013|date=August 13, 1999}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=OMG Formal Specifications|url=http://www.omg.org/spec/|work=omg.org|publisher=Object Management Group|accessdate=10 December 2013}}&lt;/ref&gt;

==See also==
*[[Description logic]]
* [[Deductive classifier]]
*[[First-order logic]]
*[[Knowledge base]]
*[[Knowledge-based system]]
*[[Ontology language]]
*[[Semantic Networks]]

==References==
{{reflist}}

==Additional References==
* Marvin Minsky, [http://web.media.mit.edu/~minsky/papers/Frames/frames.html A Framework for Representing Knowledge], MIT-AI Laboratory Memo 306, June, 1974.
* Daniel G. Bobrow, Terry Winograd, [ftp://reports.stanford.edu/pub/cstr/reports/cs/tr/76/581/CS-TR-76-581.pdf An Overview of KRL, A Knowledge Representation Language],  Stanford Artificial Intelligence Laboratory Memo AIM 293, 1976.
* R. Bruce Roberts and Ira P. Goldstein, [ftp://publications.ai.mit.edu/ai-publications/pdf/AIM-408.pdf The FRL Primer], 1977
* R. Bruce Roberts and Ira P. Goldstein, [ftp://publications.ai.mit.edu/ai-publications/pdf/AIM-409.pdf The FRL Manual], 1977
* {{cite journal | last1 = Brachman | first1 = R. | last2 = Schmolze | first2 = J. | year = 1985 | title = An overview of the KL-ONE Knowledge Representation System | url = | journal = Cognitive science | volume = 9 | issue = | pages = 171–216 | doi=10.1016/s0364-0213(85)80014-8}}
* {{cite journal | last1 = Fikes | first1 = R. E. | last2 = Kehler | first2 = T. | year = 1985 | title = The role of frame-based representation in knowledge representation and reasoning | url = | journal = Communications of the ACM | volume = 28 | issue = 9| pages = 904–920 | doi=10.1145/4284.4285}}
* Peter Clark &amp; Bruce Porter:  KM - The Knowledge Machine 2.0: Users Manual,  http://www.cs.utexas.edu/users/mfkb/RKF/km.html.
* Peter D. Karp, [http://www.ai.sri.com/pub_list/236 The Design Space of Frame Knowledge Representation Systems], Technical Note 520. [[Artificial Intelligence Center]], [[SRI International]], 1992

==External links==
*[http://www.cs.umbc.edu/771/papers/nebel.html Frame-Based Systems]
*[http://www.ai.sri.com/~gfp/spec/paper/paper.html The Generic Frame Protocol]
*[http://protege.stanford.edu/ The Protégé Ontology Editor]
*[http://www.csee.umbc.edu/courses/771/current/presentations/frames.pdf Intro Presentation to Frame Languages]

[[Category:Artificial intelligence]]
[[Category:Knowledge engineering]]
[[Category:Knowledge representation]]</text>
      <sha1>bveu1rj08lz4c80puhgskjemzpcumhe</sha1>
    </revision>
  </page>
  <page>
    <title>Attempto Controlled English</title>
    <ns>0</ns>
    <id>6520028</id>
    <revision>
      <id>759248865</id>
      <parentid>756632136</parentid>
      <timestamp>2017-01-10T02:13:09Z</timestamp>
      <contributor>
        <username>Peterl</username>
        <id>266404</id>
      </contributor>
      <comment>/* top */ Updated version, added ref</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="17071" xml:space="preserve">{{Refimprove|date=April 2016}}
'''Attempto Controlled English''' ('''ACE''') is a [[controlled natural language]], i.e. a subset of standard [[English grammar|English]] with a restricted syntax and restricted semantics described by a small set of construction and interpretation rules.&lt;ref&gt;{{cite conference |author1=Norbert E. Fuchs |author2=Kaarel Kaljurand |author3=Gerold Schneider | title = Attempto Controlled English Meets the Challenges of Knowledge Representation, Reasoning, Interoperability and User Interfaces | booktitle = FLAIRS 2006 | date = 2006 | url = http://attempto.ifi.uzh.ch/site/publications/papers/FLAIRS0601FuchsN.pdf | format = [[PDF]]}}&lt;/ref&gt; It has been under development at the [[University of Zurich]] since 1995. In 2013, ACE version 6.7 was announced.&lt;ref&gt;http://attempto.ifi.uzh.ch/site/news/&lt;/ref&gt;

ACE can serve as [[knowledge representation]], [[specification language|specification]], and [[query language]], and is intended for professionals who want to use formal notations and formal methods, but may not be familiar with them. Though ACE appears perfectly natural – it can be read and understood by any speaker of English – it is in fact a [[formal language]].

ACE and its related tools have been used in the fields of [[requirements analysis|software specifications]], [[theorem proving]], [[automatic summarization|text summaries]], [[ontologies]], rules, querying, [[health informatics|medical documentation]] and [[planning]].

Here are some simple examples:

# Every woman is a human.
# A woman is a human.
# A man tries-on a new tie. If the tie pleases his wife then the man buys it.

ACE construction rules require that each noun be introduced by a determiner (''a'', ''every'', ''no'', ''some'', ''at least 5'', ...). ACE interpretation rules decide that (1) is interpreted as [[Universal quantification|universally quantified]], while (2) is interpreted as [[Existential quantification|existentially quantified]]. Sentences like "Women are human" do not follow ACE syntax and are consequently not valid.

Interpretation rules resolve the [[Deixis#Anaphoric reference|anaphoric references]] in (3): ''the tie'' and ''it'' of the second sentence refer to ''a new tie'' of the first sentence, while ''his'' and ''the man'' of the second sentence refer to ''a man'' of the first sentence. Thus an ACE text is a coherent entity of anaphorically linked sentences.

The Attempto Parsing Engine (APE) translates ACE texts unambiguously into [[Discourse Representation Theory|discourse representation structures]] (DRS) that use a variant of the language of [[first-order logic]].&lt;ref&gt;{{cite conference |author1=Norbert E. Fuchs |author2=Kaarel Kaljurand |author3=Tobias Kuhn | title = Discourse Representation Structures for ACE 6.6 | booktitle = Technical Report ifi-2010.0010, Department of Informatics, University of Zurich | date = 2010 | url = http://attempto.ifi.uzh.ch/site/pubs/papers/drs_report_66.pdf| format = [[PDF]] }}&lt;/ref&gt; A DRS can be further translated into other [[formal languages]], for instance AceRules with various semantics,&lt;ref&gt;{{cite conference | author = Tobias Kuhn | title = AceRules: Executing Rules in Controlled Natural Language | booktitle = First International Conference on Web Reasoning and Rule Systems (RR 2007) | year = 2007  | url = http://attempto.ifi.uzh.ch/site/pubs/papers/kuhn07acerules.pdf| format = [[PDF]]}}&lt;/ref&gt;   [[Web Ontology Language|OWL]],&lt;ref&gt;{{cite conference |author1=Kaarel Kaljurand |author2=Norbert E. Fuchs | title = Verbalizing OWL in Attempto Controlled English | booktitle = OWL: Experiences and Directions (OWLED 2007) | year = 2007  | url = http://attempto.ifi.uzh.ch/site/pubs/papers/owled2007_kaljurand.pdf | format = [[PDF]]}}&lt;/ref&gt; and [[Semantic Web Rule Language|SWRL]]. Translating an ACE text into (a fragment of) first-order logic allows users to  [[inference|reason]] about the text, for instance to [[formal verification|verify]], to [[formal verification|validate]], and to [[Information retrieval|query]] it.

== ACE in a nutshell ==
{{unreferenced section|date=May 2013}}
As an overview of the current version 6.6 of ACE this section:

* Briefly describes the vocabulary
* Gives an account of the syntax
* Summarises the handling of ambiguity
* Explains the processing of anaphoric references.

=== Vocabulary ===

The vocabulary of ACE comprises:

* Predefined function words (e.g. determiners, conjunctions)
* Predefined phrases (e.g. "it is false that ...", "it is possible that ...")
* Content words (e.g. nouns, verbs, adjectives, adverbs).

=== Grammar ===

The grammar of ACE defines and constrains the form and the meaning of ACE sentences and texts. ACE's grammar is expressed as a set of  [http://attempto.ifi.uzh.ch/site/docs/ace_constructionrules.html  construction rules]. The meaning of sentences is described as a small set of [http://attempto.ifi.uzh.ch/site/docs/ace_interpretationrules.html  interpretation rules]. A [http://attempto.ifi.uzh.ch/site/docs/ace_troubleshooting.html Troubleshooting Guide] describes how to use ACE and how to avoid pitfalls.

==== ACE texts ====

An ACE text is a sequence of declarative sentences that can be anaphorically interrelated. Furthermore, ACE supports questions and commands.

==== Simple sentences ====

A simple sentence asserts that something is the case — a fact, an event, a state.

:The temperature is -2 °C.
:A customer inserts 2 cards. 
:A card and a code are valid.

Simple ACE sentences have the following general structure:

:subject + verb + complements + adjuncts

Every sentence has a subject and a verb. Complements (direct and indirect objects) are necessary for transitive verbs (''insert something'') and ditransitive verbs (''give something to somebody''), whereas adjuncts (adverbs, prepositional phrases) are optional.

All elements of a simple sentence can be elaborated upon to describe the situation in more detail. To further specify the nouns ''customer'' and ''card'', we could add adjectives:

:A trusted customer inserts two valid cards.

possessive nouns and ''of''-prepositional phrases:

:John's customer inserts a card of Mary.

or variables as appositions:

:John inserts a card A.

Other modifications of nouns are possible through relative sentences:

:A customer who is trusted inserts a card that he owns.

which are described below since they make a sentence composite. We can also detail the insertion event, e.g. by adding an adverb:

:A customer inserts some cards manually.

or, equivalently:

:A customer manually inserts some cards.

or, by adding prepositional phrases:

:A customer inserts some cards into a slot.

We can combine all of these elaborations to arrive at:

:John's customer who is trusted inserts a valid card of Mary manually into a slot A.

==== Composite sentences ====

Composite sentences are recursively built from simpler sentences through [[coordination (linguistics)|coordination]], [[subordination (linguistics)|subordination]], [[Quantification (linguistics)|quantification]], and [[negation]]. Note that ACE composite sentences overlap with what linguists call compound sentences and complex sentences.

===== Coordination =====

Coordination by ''and'' is possible between sentences and between phrases of the same syntactic type.

:A customer inserts a card and the machine checks the code.
:There is a customer who inserts a card and who enters a code.
:A customer inserts a card and enters a code.
:An old and trusted customer enters a card and a code.

Note that the coordination of the noun phrases ''a card'' and ''a code'' represents a plural object.

Coordination by ''or'' is possible between sentences, verb phrases, and relative clauses.

:A customer inserts a card or the machine checks the code.
:A customer inserts a card or enters a code.
:A customer owns a card that is invalid or that is damaged.

Coordination by ''and'' and ''or'' is governed by the standard binding order of logic, i.e. ''and'' binds stronger than ''or''. Commas can be used to override the standard binding order. Thus the sentence:

:A customer inserts a VisaCard or inserts a MasterCard, and inserts a code.

means that the customer inserts a VisaCard and a code, or alternatively a MasterCard and a code.

===== Subordination =====

There are four constructs of subordination: relative sentences, ''if-then'' sentences, modality, and sentence subordination.

Relative sentences starting with ''who'', ''which'', and ''that'' allow to add detail to nouns:

:A customer who is trusted inserts a card that he owns.

With the help of ''if-then'' sentences we can specify conditional or hypothetical situations:

:If a card is valid then a customer inserts it.

Note the anaphoric reference via the pronoun ''it'' in the ''then''-part to the noun phrase ''a card'' in the ''if''-part.

Modality allows us to express possibility and necessity:

:A trusted customer can/must insert a card.
:It is possible/necessary that a trusted customer inserts a card.

Sentence subordination comes in various forms:

:It is true/false that a customer inserts a card.
:It is not provable that a customer inserts a card.
:A clerk believes that a customer inserts a card.

===== Quantification =====

Quantification allows us to speak about all objects of a certain class ([[universal quantification]]), or to denote explicitly the existence of at least one object of this class ([[existential quantification]]). The textual occurrence of a universal or existential quantifier opens its scope that extends to the end of the sentence, or in coordinations to the end of the respective coordinated sentence.

To express that all involved customers insert cards we can write

:Every customer inserts a card.

This sentence means that each customer inserts a card that may, or may not, be the same as the one inserted by another customer. To specify that all customers insert the same card — however unrealistic that situation seems — we can write:

:A card is inserted by every customer.

or, equivalently:

:There is a card that every customer inserts.

To state that every card is inserted by a customer we write:

:Every card is inserted by a customer.

or, somewhat indirectly:

:For every card there is a customer who inserts it.

===== Negation =====

Negation allows us to express that something is not the case:

:A customer does not insert a card.
:A card is not valid.

To negate something for all objects of a certain class one uses ''no'':

:No customer inserts more than 2 cards.

or, ''there is no'':

:There is no customer who inserts a card.

To negate a complete statement one uses sentence negation:

:It is false that a customer inserts a card.

These forms of negation are logical negations, i.e. they state that something is provably not the case. Negation as failure states that a state of affairs cannot be proved, i.e. there is no information whether the state of affairs is the case or not.

:It is not provable that a customer inserts a card.

==== Queries ====

ACE supports two forms of queries: ''yes/no''-queries and ''wh''-queries.

''Yes/no''-queries ask for the existence or non-existence of a specified situation. If we specified:

:A customer inserts a card.

then we can ask:

:Does a customer insert a card?

to get a positive answer. Note that interrogative sentences always end with a question mark.

With the help of ''wh''-queries, i.e. queries with query words, we can interrogate a text for details of the specified situation. If we specified:

:A trusted customer inserts a valid card manually in the morning in a bank.

we can ask for each element of the sentence with the exception of the verb.

:Who inserts a card?
:Which customer inserts a card?
:What does a customer insert?
:How does a customer insert a card?
:When does a customer enter a card?
:Where does a customer enter a card?

Queries can also be constructed by a sequence of declarative sentences followed by one interrogative sentence, for example:

:There is a customer and there is a card that the customer enters. Does a customer enter a card?

==== Commands ====

ACE also supports commands. Some examples:

:John, go to the bank!
:John and Mary, wait!
:Every dog, bark!
:A brother of John, give a book to Mary!

A command always consists of a noun phrase (the addressee), followed by a comma, followed by an uncoordinated verb phrase. Furthermore, a command has to end with an exclamation mark.

=== Constraining ambiguity ===

To constrain the ambiguity of full natural language ACE employs three simple means:

* Some ambiguous constructs are not part of the language; unambiguous alternatives are available in their place
* All remaining ambiguous constructs are interpreted deterministically on the basis of a small number of interpretation rules
* Users can either accept the assigned interpretation, or they must rephrase the input to obtain another one.

==== Avoidance of ambiguity ====

In natural language, relative sentences combined with coordinations can introduce ambiguity:

:A customer inserts a card that is valid and opens an account.

In ACE the sentence has the unequivocal meaning that the customer opens an account, as reflected by the paraphrase:

:A card is valid. A customer inserts the card. The customer opens an account.

To express the alternative — though not very realistic — meaning that the card opens an account, the relative pronoun ''that'' must be repeated, thus yielding a coordination of relative sentences:

:A customer inserts a card that is valid and that opens an account.

This sentence is unambiguously equivalent in meaning to the paraphrase:

:A card is valid. The card opens an account. A customer inserts the card.

==== Interpretation rules ====

Not all ambiguities can be safely removed from ACE without rendering it artificial. To deterministically interpret otherwise syntactically correct ACE sentences we use a small set of interpretation rules. For example, if we write:

:A customer inserts a card with a code.

then ''with a code'' attaches to the verb ''inserts'', but not to ''a card''. However, this is probably not what we meant to say. To express that ''the code'' is associated with ''the card'' we can employ the interpretation rule that a relative sentence always modifies the immediately preceding noun phrase, and rephrase the input as:

:A customer inserts a card that carries a code.

yielding the paraphrase:

:A card carries a code. A customer inserts the card.

or — to specify that the customer inserts a card and a code — as:

:A customer inserts a card and a code.

=== Anaphoric references ===

Usually ACE texts consist of more than one sentence:

:A customer enters a card and a code. If a code is valid then SimpleMat accepts a card.

To express that all occurrences of card and code should mean the same card and the same code, ACE provides anaphoric references via the definite article:

:A customer enters a card and a code. If the code is valid then SimpleMat accepts the card.

During the processing of the ACE text, all anaphoric references are replaced by the most recent and most specific accessible noun phrase that agrees in gender and number. As an example of "most recent and most specific", suppose an ACE parser is given the sentence:

:A customer enters a red card and a blue card.

Then:

:The card is correct.

refers to the second card, while:

:The red card is correct.

refers to the first card.

Noun phrases within ''if-then'' sentences, universally quantified sentences, negations, modality, and subordinated sentences cannot be referred to anaphorically from subsequent sentences, i.e. such noun phrases are not "accessible" from the following text. Thus for each of the sentences:

:If a customer owns a card, then they enter it.
:Every customer enters a card.
:A customer does not enter a card.
:A customer can enter a card.
:A clerk believes that a customer enters a card.

we cannot refer to ''a card'' with:

:The card is correct.

Anaphoric references are also possible via personal pronouns:

:A customer enters a card and a code. If it is valid then SimpleMat accepts the card.

or via variables:

:A customer enters a card X and a code Y. If Y is valid then SimpleMat accepts X.

Anaphoric references via definite articles and variables can be combined:

:A customer enters a card X and a code Y. If the code Y is valid then SimpleMat accepts the card X.

Note that proper names like ''SimpleMat'' always refer to the same object.

==See also==
*[[Gellish]]
*[[Natural Language Processing]]
*[[Knowledge Representation]]
*[[Natural language programming]]
*[[Structured English]]
**[[ClearTalk]], another machine-readable knowledge representation language
**[[Inform 7]], a programming language with English syntax

==References==
{{Reflist}}

==External links==
*[http://attempto.ifi.uzh.ch Project Attempto]

[[Category:Controlled English]]
[[Category:Knowledge representation]]
[[Category:Controlled natural languages]]
[[Category:Natural language processing]]
[[Category:Natural language parsing]]</text>
      <sha1>465gj40opcw6alzbradbry3wb03xf12</sha1>
    </revision>
  </page>
  <page>
    <title>Attribute-value system</title>
    <ns>0</ns>
    <id>7512482</id>
    <revision>
      <id>752569793</id>
      <parentid>752524580</parentid>
      <timestamp>2016-12-02T00:42:48Z</timestamp>
      <contributor>
        <username>Ibadibam</username>
        <id>1138432</id>
      </contributor>
      <minor />
      <comment>/* References */ [[MOS:LISTGAP]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5530" xml:space="preserve">An '''attribute-value system''' is a basic [[knowledge representation]] framework comprising a table with columns designating "attributes" (also known as "properties", "predicates," "features," "dimensions," "characteristics", "[[Field (computer science)|fields]]", "headers" or "independent variables" depending on the context) and "[[Row (database)|rows]]" designating "objects" (also known as "entities," "instances," "exemplars," "elements", "[[Record (computer science)|records]]" or "dependent variables"). Each table cell therefore designates the value (also known as "state") of a particular attribute of a particular object.

== Example of attribute-value system==
Below is a sample attribute-value system. It represents 10 objects (rows) and five features (columns). In this example, the table contains only integer values. In general, an attribute-value system may contain any kind of data, numeric or otherwise. An attribute-value system is distinguished from a simple "feature list" representation in that each feature in an attribute-value system may possess a range of values (e.g., feature &lt;math&gt;P_{1}&lt;/math&gt; below, which has domain of {0,1,2}), rather than simply being ''present'' or ''absent'' {{Harv|Barsalou|Hale|1993}}.

:{| class="wikitable" style="text-align:center; width:30%" border="1"
|+ Sample Attribute-Value System
! Object !! &lt;math&gt;P_{1}&lt;/math&gt; !! &lt;math&gt;P_{2}&lt;/math&gt; !! &lt;math&gt;P_{3}&lt;/math&gt; !! &lt;math&gt;P_{4}&lt;/math&gt; !! &lt;math&gt;P_{5}&lt;/math&gt;
|-
! &lt;math&gt;O_{1}&lt;/math&gt;
| 1 || 2 || 0 || 1 || 1
|-
! &lt;math&gt;O_{2}&lt;/math&gt;
| 1 || 2 || 0 || 1 || 1
|-
! &lt;math&gt;O_{3}&lt;/math&gt;
| 2 || 0 || 0 || 1 || 0
|-
! &lt;math&gt;O_{4}&lt;/math&gt;
| 0 || 0 || 1 || 2 || 1
|-
! &lt;math&gt;O_{5}&lt;/math&gt;
| 2 || 1 || 0 || 2 || 1
|-
! &lt;math&gt;O_{6}&lt;/math&gt;
| 0 || 0 || 1 || 2 || 2
|-
! &lt;math&gt;O_{7}&lt;/math&gt;
| 2 || 0 || 0 || 1 || 0
|-
! &lt;math&gt;O_{8}&lt;/math&gt;
| 0 || 1 || 2 || 2 || 1
|-
! &lt;math&gt;O_{9}&lt;/math&gt;
| 2 || 1 || 0 || 2 || 2
|-
! &lt;math&gt;O_{10}&lt;/math&gt;
| 2 || 0 || 0 || 1 || 0
|}

== Other terms used for "attribute-value system"==
Attribute-value systems are pervasive throughout many different literatures, and have been discussed under many different names:
*''Flat data''
*''[[Spreadsheet]]''
*''Attribute-value system'' (Ziarko &amp; Shan 1996)
*''Information system'' ([[Zdzislaw Pawlak|Pawlak]] 1981)
*''Classification system'' (Ziarko 1998)
*''Knowledge representation system'' (Wong &amp; Ziarko 1986)
*''Information table'' (Yao &amp; Yao 2002)
*''Object-predicate table'' (Watanabe 1985)
*''Aristotelian table'' (Watanabe 1985)
*''Simple frames'' {{Harv|Barsalou|Hale|1993}}
*''[[First normal form]]'' database

==See also==
*[[Bayes networks]]
*[[Entity–attribute–value model]]
*[[Joint distribution]]
*[[Knowledge representation]]
*[[wikibooks:Optimal Classification|Optimal classification]] (in Wikibooks)
*[[Rough set]]
*[[Triplestore]]

== References ==
* {{Cite book
 | last1=Barsalou
 | given1=Lawrence W.
 | surname2=Hale
 | given2=Christopher R.
 | year= 1993
 | chapter=Components of conceptual representation: From feature lists to recursive frames
 | editor=Iven Van Mechelen |editor2=James Hampton |editor3=Ryszard S. Michalski |editor4=Peter Theuns
 | title=Categories and Concepts: Theoretical Views and Inductive Data Analysis
 | pages=97–144
 | edition=
 | publisher=Academic Press
 | place=London
 | url=
 | accessdate=
 | ref=harv
 | postscript=&lt;!--None--&gt;}}
*{{cite book
  | last = Pawlak
  | first = Zdzisław
  | authorlink = Zdzislaw Pawlak
  | title = Rough sets: Theoretical Aspects of Reasoning about Data
  | publisher = Kluwer
  | year = 1991
  | location = Dordrecht}}
*{{cite journal
  | last = Ziarko
  | first = Wojciech 
  | last2 = Shan
  | first2 = Ning
  | title = A method for computing all maximally general rules in attribute-value systems
  | journal = Computational Intelligence
  | volume = 12
  | issue = 2
  | pages = 223–234
  | year = 1996
  | doi = 10.1111/j.1467-8640.1996.tb00260.x
  | ref = harv}}
*{{cite journal
  | last = Pawlak
  | first = Zdzisław
  | last2 = Shan
  | first2 = Ning
  | title = Information systems: Theoretical foundations
  | journal = Information Systems
  | volume = 6
  | issue = 3
  | pages = 205–218
  | year = 1981
  | doi = 10.1016/0306-4379(81)90023-5
  | ref = harv}}
*{{cite journal
  | last = Wong
  | first = S. K. M.
  | last2 = Ziarko
  | first2 = Wojciech
  | last3 = Ye
  | first3 = R. Li
  | title = Comparison of rough-set and statistical methods in inductive learning
  | journal = International Journal of Man-Machine Studies
  | volume = 24
  | pages = 53–72
  | year = 1986
  | ref = harv}}
*{{cite conference
  | first = Yao
  | last = J. T.
  |author2=Yao, Y. Y.
  | title = Induction of classification rules by granular computing
  | booktitle = Proceedings of the Third International Conference on Rough Sets and Current Trends in Computing (TSCTC'02)
  | pages = 331–338
  | publisher = Springer-Verlag
  | year = 2002
  | location = London, UK}}
*{{cite book
  | last = Watanabe
  | first = Satosi
  | title = Pattern Recognition: Human and Mechanical
  | publisher = John Wiley &amp; Sons
  | year = 1985
  | location = New York}}
*{{cite conference
  | first = Wojciech
  | last = Ziarko
  | title = Rough sets as a methodology for data mining
  | booktitle = Rough Sets in Knowledge Discovery 1: Methodology and Applications
  | pages = 554–576
  | editor    = Polkowski, Lech |editor2=Skowron, Andrzej
  | publisher = Physica-Verlag
  | year = 1998
  | location = Heidelberg}}

[[Category:Knowledge representation]]
[[Category:Specific models]]</text>
      <sha1>oot0tuusozpfbsjrziue34qgs94ajhf</sha1>
    </revision>
  </page>
  <page>
    <title>Minimum Information Standards</title>
    <ns>0</ns>
    <id>7819348</id>
    <revision>
      <id>757657045</id>
      <parentid>721913703</parentid>
      <timestamp>2016-12-31T23:13:04Z</timestamp>
      <contributor>
        <username>Trappist the monk</username>
        <id>10289486</id>
      </contributor>
      <minor />
      <comment>/* top */ cite repair;</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="13369" xml:space="preserve">{{Multiple issues|
{{confusing|date=January 2010}}
{{essay-like|date=January 2010}}
{{lead rewrite|date=January 2010}}
{{external links|date=September 2012}}
{{more footnotes|date=January 2010}}
{{expert-subject|Computational Biology|date=January 2010}}
}}

The '''minimum information standard''' is a set of guidelines for [[data reporting|reporting]]  [[data]] derived by relevant methods in biosciences. If followed, it ensures that the data can be easily verified, analysed and clearly interpreted by the wider scientific community. Keeping with these recommendations also facilitates the foundation of structuralized databases, public repositories and development of data analysis tools.&lt;ref name="MIFlowCyt: The minimum information about a flow cytometry experiment"&gt;{{cite journal|last=Lee|first=Jamie A. |author2=Spidlen, Josef |author3=Boyce, Keith |author4=Cai, Jennifer |author5=Crosbie, Nicholas |author6=Dalphin, Mark |author7=Furlong, Jeff |author8=Gasparetto, Maura |author9=Goldberg, Michael |author10=Goralczyk, Elizabeth M. |author11=Hyun, Bill |author12=Jansen, Kirstin |author13=Kollmann, Tobias |author14=Kong, Megan |author15=Leif, Robert |author16=McWeeney, Shannon |author17=Moloshok, Thomas D. |author18=Moore, Wayne |author19=Nolan, Garry |author20=Nolan, John |author21=Nikolich-Zugich, Janko |author22=Parrish, David |author23=Purcell, Barclay |author24=Qian, Yu |author25=Selvaraj, Biruntha |author26=Smith, Clayton |author27=Tchuvatkina, Olga |author28=Wertheimer, Anne |author29=Wilkinson, Peter |author30=Wilson, Christopher |author31=Wood, James |author32=Zigon, Robert |author33=Scheuermann, Richard H. |author34=Brinkman, Ryan R. |title=MIFlowCyt: The minimum information about a flow cytometry experiment|journal=Cytometry Part A|date=1 October 2008|volume=73A|issue=10|pages=926–930|doi=10.1002/cyto.a.20623|pmid=18752282|pmc=2773297}}&lt;/ref&gt;&lt;ref name="Minimum information about a microarray experiment (MIAME)—toward standards for microarray data"&gt;{{cite journal|last=Brazma|first=Alvis |author2=Hingamp, Pascal |author3=Quackenbush, John |author4=Sherlock, Gavin |author5=Spellman, Paul |author6=Stoeckert, Chris |author7=Aach, John |author8=Ansorge, Wilhelm |author9=Ball, Catherine A. |author10=Causton, Helen C. |author11=Gaasterland, Terry |author12=Glenisson, Patrick |author13=Holstege, Frank C.P. |author14=Kim, Irene F. |author15=Markowitz, Victor |author16=Matese, John C. |author17=Parkinson, Helen |author18=Robinson, Alan |author19=Sarkans, Ugis |author20=Schulze-Kremer, Steffen |author21=Stewart, Jason |author22=Taylor, Ronald |author23=Vilo, Jaak |author24=Vingron, Martin |title=Minimum information about a microarray experiment (MIAME)—toward standards for microarray data|journal=Nature Genetics|date=30 November 2001|volume=29|issue=4|pages=365–371|doi=10.1038/ng1201-365|pmid=11726920}}&lt;/ref&gt;

The individual '''minimum information standards''' are brought by the communities of cross-disciplinary specialists focused on the problematic of the specific method used in experimental biology.  The standards then provide specifications what information about the experiments ([[metadata]]) is crucial and important to be reported together with the resultant data to make it comprehensive.&lt;ref name="MIFlowCyt: The minimum information about a flow cytometry experiment"/&gt;&lt;ref name="Minimum information about a microarray experiment (MIAME)—toward standards for microarray data"/&gt; The need for this standardization is largely driven by the development of high-throughput experimental methods that provide tremendous amounts of data.  The development of minimum information standards of different methods is since 2008 being harmonized by "Minimum Information about a Biomedical or Biological Investigation" (MIBBI) project.&lt;ref name="Promoting coherent minimum reporting guidelines for biological and biomedical investigations: the MIBBI project"&gt;{{cite journal|last=Taylor|first=Chris F|title=Promoting coherent minimum reporting guidelines for biological and biomedical investigations: the MIBBI project|journal=Nature Biotechnology|year=2008|volume=26|pages=889–896|doi=10.1038/nbt.1411}}&lt;/ref&gt;

==MI Standards==

===MIAME, gene expression microarray===
Minimum Information About a Microarray Experiment (MIAME) describes the Minimum Information About a Microarray Experiment that is needed to enable the interpretation of the results of the experiment unambiguously and potentially to reproduce the experiment and is aimed at facilitating the dissemination of data from microarray experiments.

MIAME contains a number of extensions to cover specific biological domains, including MIAME-env, MIAME-nut and MIAME-tox, covering environmental genomics, nutritional genomics and toxogenomics, respectively

===MINI: Minimum Information about a Neuroscience Investigation===

====MINI: Electrophysiology====
[[Electrophysiology]] is a technology used to study the electrical properties of biological cells and tissues. Electrophysiology typically involves the measurements of voltage change or electric current flow on a wide variety of scales from single ion channel
proteins to whole tissues. This document is a single module, as part of the Minimum Information about a Neuroscience investigation (MINI) family of reporting guideline
documents, produced by community consultation and continually available for public comment. A MINI module represents the minimum information that should be reported about a dataset to facilitate computational access and analysis to allow a reader to interpret and critically evaluate the processes performed and the conclusions reached, and to support their experimental corroboration. In practice a MINI module comprises a checklist of information that should be provided (for example about the protocols employed) when
a data set is described for publication. The full specification of the MINI module can be found here.&lt;ref&gt;Gibson, Frank, Overton, Paul, Smulders, Tom, Schultz, Simon, Eglen, Stephen, Ingram, Colin, Panzeri, Stefano, Bream, Phil, Sernagor, Evelyne, Cunningham, Mark, Adams, Christopher, Echtermeyer, Christoph, Simonotto, Jennifer, Kaiser, Marcus, Swan, Daniel, Fletcher, Marty, and Lord, Phillip. Minimum Information about a Neuroscience Investigation (MINI) Electrophysiology. Available from Nature Precedings &lt;http://hdl.handle.net/10101/npre.2008.1720.1&gt; (2008)&lt;/ref&gt;

===MIARE, RNAi experiment===
Minimum Information About an RNAi Experiment (MIARE) is a [[data reporting]] guideline which describes the minimum information that should be reported about an RNAi experiment to enable the unambiguous interpretation and reproduction of the results.

===MIACA, cell based assay===
Advances in genomics and functional genomics have enabled large-scale analyses of gene and protein function by means of high-throughput cell biological analyses. Thereby, cells in culture can be perturbed in vitro and the induced effects recorded and analyzed. Perturbations can be triggered in several ways, for instance with molecules (siRNAs, expression constructs, small chemical compounds, ligands for receptors, etc.), through environmental stresses (such as temperature shift, serum starvation, oxygen deprivation, etc.), or combinations thereof. The cellular responses to such perturbations are analyzed in order to identify molecular events in the biological processes addressed and understand biological principles.
We propose the Minimum Information About a Cellular Assay (MIACA) for reporting a cellular assay, and CA-OM, the modular cellular assay object model, to facilitate exchange of data and accompanying information, and to compare and integrate data that originate from different, albeit complementary approaches, and to elucidate higher order principles. [http://sourceforge.net/project/showfiles.php?group_id=158121 Documents describing MIACA] are available and provide further information as well as the checklist of terms that should be reported.

===MIAPE, proteomic experiments===
The Minimum Information About a Proteomic Experiment documents describe information which should be given along with a proteomic experiment. The parent document describes the processes and principles underpinning the development of a series of domain specific documents which now cover all aspects of a MS-based proteomics workflow.
{{Details|Minimum Information About a Proteomics Experiment }}

===MIMIx, molecular interactions===
This document has been developed and maintained by the Molecular Interaction worktrack of the HUPO-PSI (www.psidev.info) and describes the Minimum Information about a Molecular Interaction experiment.

===MIAPAR, protein affinity reagents===
The Minimum Information About a Protein Affinity Reagent has been developed and maintained by the Molecular Interaction worktrack of the HUPO-PSI (www.psidev.info)in conjunction with the HUPO Antibody Initiative and a European consortium of binder producers and seeks to encourage users to improve their description of binding reagents, such as antibodies, used in the process of protein identification.

===MIABE, bioactive entities===
The Minimum Information About a Bioactive Entity was produced by representatives from both large pharma and academia who are looking to improve the description of usually small molecules which bind to, and potentially modulate the activity of, specific targets in a living organism. This document encompasses drug-like molecules as well as hebicides, pesticides and food additives. It is primarily maintained through the EMBL-EBI Industry program (www.ebi.ac.uk/industry).

===MIGS/MIMS, genome/metagenome sequences===
This specification is being developed by the [[Genomic Standards Consortium]]

===MIFlowCyt, flow cytometry===

====Minimum Information about a Flow Cytometry Experiment====
The fundamental tenet of any scientific research is that the published results of any study have to be open to independent validation or refutation. The Minimum Information about a Flow Cytometry Experiment (MIFlowCyt) establishes the criteria to record information about the experimental overview, samples, instrumentation and data analysis. It promotes consistent annotation of clinical, biological and technical issues surrounding a flow cytometry experiment by specifying the requirements for data content and by providing a structured framework for capturing information.

More information can be found at:
* The Flow Informatics and Computational Cytometry Socienty (FICCS) [http://wiki.ficcs.org/ficcs/MIFlowCyt MIFlowCyt wiki] page.
* The Bioinformatics Standards for Flow Cytometry [http://flowcyt.sourceforge.net/miflowcyt/ MIFlowCyt web] page.

===MISFISHIE, In Situ Hybridization and Immunohistochemistry Experiments===
{{Emptysection|date=February 2013}}

===MIAPA, Phylogenetic Analysis===
Criteria for Minimum Information About a Phylogenetic Analysis were described in 2006. &lt;ref&gt; {{Cite journal | doi = 10.1089/omi.2006.10.231| title = Taking the First Steps towards a Standard for Reporting on Phylogenies: Minimum Information about a Phylogenetic Analysis (MIAPA)| journal = OMICS: A Journal of Integrative Biology| volume = 10| issue = 2| pages = 231| year = 2006| last1 = Leebens-Mack | first1 = J. | last2 = Vision | first2 = T. | last3 = Brenner | first3 = E. | last4 = Bowers | first4 = J. E. | last5 = Cannon | first5 = S. | last6 = Clement | first6 = M. J. | last7 = Cunningham | first7 = C. W. | last8 = Depamphilis | first8 = C. | last9 = Desalle | first9 = R. | last10 = Doyle | first10 = J. J. | last11 = Eisen | first11 = J. A. | last12 = Gu | first12 = X. | last13 = Harshman | first13 = J. | last14 = Jansen | first14 = R. K. | last15 = Kellogg | first15 = E. A. | last16 = Koonin | first16 = E. V. | last17 = Mishler | first17 = B. D. | last18 = Philippe | first18 = H. | last19 = Pires | first19 = J. C. | last20 = Qiu | first20 = Y. L. | last21 = Rhee | first21 = S. Y. | last22 = Sjölander | first22 = K. | last23 = Soltis | first23 = D. E. | last24 = Soltis | first24 = P. S. | authorlink24 = Pamela S. Soltis| last25 = Stevenson | first25 = D. W. | last26 = Wall | first26 = K. | last27 = Warnow | first27 = T. | last28 = Zmasek | first28 = C. }} &lt;/ref&gt;

===MIAO, ORF===
{{Emptysection|date=February 2013}}

===MIAMET, METabolomics experiment===
{{Emptysection|date=February 2013}}

===MIAFGE, Functional Genomics Experiment===
{{Emptysection|date=February 2013}}

===MIRIAM, Minimum Information Required in the Annotation of Models===
The Minimal Information Required In the Annotation of Models ([[MIRIAM]]), is a set of rules for the curation and annotation of quantitative models of biological systems.

===MIASE, Minimum Information About a Simulation Experiment===
The Minimum Information About a Simulation Experiment ([[MIASE]]) is an effort to standardize the description of simulation experiments in the field of systems biology.

===CIMR, Core Information for Metabolomics Reporting===

==External links==
* [http://mibbi.sourceforge.net/ MIBBI (Minimum Information for Biological and Biomedical Investigations)] A ‘one-stop shop’ for exploring the range of extant projects, foster collaborative development and ultimately promote gradual integration.
* [http://www.biosharing.org BioSharing catalogue]

==References==
{{reflist}}

[[Category:Bioinformatics]]
[[Category:Knowledge representation]]</text>
      <sha1>k8295hbjq6zvbdml4m81o3j9itnexqv</sha1>
    </revision>
  </page>
  <page>
    <title>Microformat</title>
    <ns>0</ns>
    <id>2346998</id>
    <revision>
      <id>760445523</id>
      <parentid>755821224</parentid>
      <timestamp>2017-01-17T01:08:19Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <comment>simplify headings, rm items linked already/integrate items</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="22410" xml:space="preserve">{{About||the photographic miniaturization of documents|Microform|details of microformats used on Wikipedia|:Wikipedia:Microformats}}
A '''microformat''' (sometimes abbreviated '''μF''') is a [[World Wide Web]]-based approach to semantic markup which uses [[HTML]]/[[XHTML]] tags supported for other purposes to convey additional [[metadata]]&lt;ref&gt;{{cite web |url=http://microformats.org/wiki/existing-classes |work=Microformats.org |title=Class Names Across All Microformats |date=2007-09-23 |accessdate=2008-09-06}}&lt;/ref&gt; and other attributes in web pages and other contexts that support (X)HTML, such as [[RSS]]. This approach allows [[software agent|software]] to process information intended for end-users (such as [[Address book|contact information]], [[Geographic coordinate system|geographic coordinates]], calendar events, and similar information) automatically.

Although the content of web pages has been capable of some "automated processing" since the inception of the web, such processing is difficult because the [[markup language|markup tags]] used to display information on the web do not describe what the information means.&lt;ref name="Wharton000"&gt;{{cite web |title=What’s the Next Big Thing on the Web? It May Be a Small, Simple Thing -- Microformats|work=Knowledge@Wharton |publisher=[[Wharton School of the University of Pennsylvania]] |date=2005-07-27 |url=http://knowledge.wharton.upenn.edu/index.cfm?fa=printArticle&amp;ID=1247}}&lt;/ref&gt; Microformats can bridge this gap by attaching [[semantics]], and thereby obviate other, more complicated, methods of automated processing, such as [[natural language processing]] or [[screen scraping]]. The use, adoption and processing of microformats enables data items to be indexed, searched for, saved or cross-referenced, so that information can be reused or combined.&lt;ref name="Wharton000"/&gt;

{{As of | 2013}} microformats allow the encoding and extraction of event details, contact information, social relationships and similar information. Established microformats such as [[hCard]] are published on the web more than alternatives like schema ([[Microdata (HTML)|microdata]]) and [[RDFa]].&lt;ref&gt;{{cite web|url=http://webdatacommons.org/structureddata/index.html#toc2
 |date=2013 |work= section 3.1, "Extraction Results from the November 2013 Common Crawl Corpus" |accessdate=2015-02-21 |title=Web Data Commons – RDFa, Microdata, and Microformat Data Sets}}&lt;/ref&gt;{{failed verification|date=November 2016}}

== Background ==
Microformats emerged around 2005&lt;ref&gt;The ''microformats'' is a community-standard maintained by its Wiki, and [http://microformats.org/wiki/index.php?title=Main_Page&amp;dir=prev&amp;action=history the Wiki arrived ~2005].&lt;/ref&gt; as part of a grassroots movement to make recognizable data items (such as events, contact details or geographical locations) capable of automated processing by software, as well as directly readable by end-users.&lt;ref name="Wharton000"/&gt;&lt;ref&gt;In this context, the definition of "end-user" includes a person reading a web page on a computer screen or mobile device, or an [[assistive technology]] such as a [[screen reader]].&lt;/ref&gt; Link-based microformats emerged first. These include vote links that express opinions of the linked page, which search engines can tally into instant polls.&lt;ref name="Khare000"&gt;{{cite journal |title=Microformats: The Next (Small) Thing on the Semantic Web? |first=Rohit |last=Khare |journal=[[IEEE Internet Computing]] |volume=10 |issue=1 |pages=68–75 |date=January–February 2006 |publisher=[[IEEE Computer Society]] |url=http://csdl2.computer.org/persagen/DLAbsToc.jsp?resourcePath=/dl/mags/ic/&amp;toc=comp/mags/ic/2006/01/w1toc.xml&amp;DOI=10.1109/MIC.2006.13 |doi=10.1109/MIC.2006.13 |accessdate=2008-09-06}}
&lt;/ref&gt;

[[CommerceNet]], a nonprofit organization that promotes [[e-commerce]] on the Internet, has helped sponsor and promote the technology and support the microformats community in various ways.&lt;ref name="Khare000"/&gt; CommerceNet also helped co-found the Microformats.org community site.&lt;ref name="Khare000"/&gt;

Neither CommerceNet nor Microformats.org operates as a [[standards body]]. The microformats community functions through an open [[wiki]], a mailing list, and an Internet relay chat ([[Internet Relay Chat|IRC]]) channel.&lt;ref name="Khare000"/&gt; Most of the existing microformats originated at the Microformats.org wiki and the associated mailing list{{citation needed|date=October 2012}} by a process of gathering examples of web-publishing behaviour, then codifying it. Some other microformats (such as [[nofollow|rel=nofollow]] and [[unAPI]]) have been proposed, or developed, elsewhere.

== Technical overview ==

XHTML and HTML standards allow for the embedding and encoding of semantics within the [[HTML element|attributes of markup tags]]. Microformats take advantage of these standards by indicating the presence of metadata using the following attributes:

; &lt;code&gt;class&lt;/code&gt;
: [[Class (computer programming)|Classname]]

; &lt;code&gt;rel&lt;/code&gt;
: relationship, description of the target address in an anchor-element (&lt;code&gt;&lt;a href=... rel=...&gt;...&lt;/a&gt;&lt;/code&gt;)

; &lt;code&gt;rev&lt;/code&gt;
: reverse relationship, description of the referenced document (in one case, otherwise deprecated in microformats&lt;ref name="uF-rel-faq"&gt;{{cite web |url=http://microformats.org/wiki/rel-faq |title="rel" attribute frequently asked questions |work=Microformats.org |date=2008-08-06 |accessdate=2008-09-06}}&lt;/ref&gt;)

For example, in the text "The birds roosted at &lt;span class="geo"&gt;&lt;span class="latitude"&gt;52.48&lt;/span&gt;, &lt;span class="longitude"&gt;-1.89&lt;/span&gt;&lt;/span&gt;" is a pair of numbers which may be understood, from their context, to be a set of [[geographic coordinate system|geographic coordinates]]. With wrapping in [[Span and div|spans]] (or other HTML elements) with specific class names (in this case &lt;code&gt;geo&lt;/code&gt;, &lt;code&gt;latitude&lt;/code&gt; and &lt;code&gt;longitude&lt;/code&gt;, all part of the [[Geo (microformat)|geo microformat]] specification):

 &lt;syntaxhighlight lang="xml"&gt;The birds roosted at
   &lt;span class="geo"&gt;
     &lt;span class="latitude"&gt;52.48&lt;/span&gt;,
     &lt;span class="longitude"&gt;-1.89&lt;/span&gt;
   &lt;/span&gt;
 &lt;/syntaxhighlight&gt;

software agents can recognize exactly what each value represents and can then perform a variety of tasks such as indexing, locating it on a map and exporting it to a [[GPS]] device.

=== Examples ===
In this example, the contact information is presented as follows:

 &lt;syntaxhighlight lang="xml"&gt;
 &lt;ul&gt;
   &lt;li&gt;Joe Doe&lt;/li&gt;
   &lt;li&gt;The Example Company&lt;/li&gt;
   &lt;li&gt;604-555-1234&lt;/li&gt;
   &lt;li&gt;&lt;a href="http://example.com/"&gt;http://example.com/&lt;/a&gt;&lt;/li&gt;
 &lt;/ul&gt;
 &lt;/syntaxhighlight&gt;

With hCard microformat markup, that becomes:

 &lt;syntaxhighlight lang="xml"&gt;
 &lt;ul class="vcard"&gt;
   &lt;li class="fn"&gt;Joe Doe&lt;/li&gt;
   &lt;li class="org"&gt;The Example Company&lt;/li&gt;
   &lt;li class="tel"&gt;604-555-1234&lt;/li&gt;
   &lt;li&gt;&lt;a class="url" href="http://example.com/"&gt;http://example.com/&lt;/a&gt;&lt;/li&gt;
 &lt;/ul&gt;
 &lt;/syntaxhighlight&gt;

Here, the formatted name (&lt;code&gt;fn&lt;/code&gt;), organisation (&lt;code&gt;org&lt;/code&gt;), telephone number (&lt;code&gt;tel&lt;/code&gt;) and [[Uniform Resource Locator|web address]] (&lt;code&gt;url&lt;/code&gt;) have been identified using specific class names and the whole thing is wrapped in &lt;code&gt;class="vcard"&lt;/code&gt;, which indicates that the other classes form an hCard (short for "HTML [[vCard]]") and are not merely coincidentally named. Other, optional, hCard classes also exist. Software, such as browser plug-ins, can now extract the information, and transfer it to other applications, such as an address book.

&lt;div class="noprint"&gt; &lt;!-- ensures that the following "Live" example dies not carry over to printed mirrors --&gt;

&lt;!-- Note "noprint" div started in previous section
--&gt;=== In-context examples ===
For annotated examples of microformats on live pages, see [[HCard#Live example]] and [[Geo (microformat)#Usage]].
&lt;/div&gt;

== Specific microformats ==
Several microformats have been developed to enable semantic markup of particular types of information. However, only hCard and hCalendar have been ratified, the others remaining as drafts:

* [[hAtom]] (superseded by [[h-entry]] and [[h-feed]]) – for marking up [[Atom (standard)|Atom]] feeds from within standard HTML
* [[hCalendar]] – for events
* [[hCard]] – for contact information; includes:
** adr – for postal addresses
** [[geo (microformat)|geo]] – for geographical coordinates ([[latitude]], [[longitude]])
* hMedia - for audio/video content&lt;ref&gt;[http://microformats.org/wiki/hmedia hMedia · Microformats Wiki&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;&lt;ref&gt;[http://sixrevisions.com/web-development/ultimate-guide-to-microformats-reference-and-examples/ Ultimate Guide to Microformats: Reference and Examples&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;
* hAudio – for audio content
* [[hNews]] -  for news content
* [[hProduct]] – for products
* [[hRecipe]] - for recipes and foodstuffs.
* [[hResume]] – for resumes or [[curriculum vitae|CVs]]
* [[hReview]] – for reviews
* rel-[[directory (file systems)|directory]] – for distributed directory creation and inclusion&lt;ref&gt;[http://microformats.org/wiki/rel-directory rel-directory · Microformats Wiki&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;
* rel-enclosure – for multimedia attachments to web pages&lt;ref&gt;[http://microformats.org/wiki/rel-enclosure rel="enclosure" · Microformats Wiki&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;
* rel-license – specification of copyright license&lt;ref&gt;[http://microformats.org/wiki/rel-license rel="license" · Microformats Wiki&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;
* rel-[[nofollow]], an attempt to discourage third-party content spam (e.g. [[spam in blogs]])
* rel-[[tag (metadata)|tag]] – for decentralized tagging ([[Folksonomy]])&lt;ref&gt;[http://microformats.org/wiki/rel-tag rel="tag" · Microformats Wiki&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;
* [[xFolk]] – for tagged links
* [[XHTML Friends Network]] (XFN) – for social relationships
* [[XOXO (microformat)|XOXO]] – for lists and outlines

== Uses ==
Using microformats within HTML code provides additional formatting and semantic data that applications can use. For example, applications such as [[web crawler]]s can collect data about on-line resources, or desktop applications such as e-mail clients or scheduling software can compile details. The use of microformats can also facilitate "mash ups" such as exporting all of the geographical locations on a web page into (for example) [[Google Maps]] to visualize them spatially.

Several browser extensions, such as [[Operator (extension)|Operator]] for [[Firefox]] and Oomph for [[Internet Explorer]], provide the ability to detect microformats within an HTML document. When hCard or hCalendar are involved, such browser extensions allow microformats to be exported into formats compatible with contact management and calendar utilities, such as [[Microsoft Outlook]]. When dealing with geographical coordinates, they allow the location to be sent to applications such as [[Google Maps]]. [[Yahoo! query language|Yahoo! Query Language]] can be used to extract microformats from web pages.&lt;ref&gt;{{cite web|url=http://developer.yahoo.net/blog/archives/2009/01/wikipedia_w_yql.html|title=Retrieving and displaying data from Wikipedia with YQL|last=Heilman|first=Chris|date=2009-01-19|work=Yahoo Developer Network|publisher=Yahoo|accessdate=2009-01-19}}&lt;/ref&gt; On 12 May 2009 [[Google search|Google]] announced that they would be parsing the hCard, hReview and hProduct microformats, and using them to populate search result pages.&lt;ref name="Rich-Snippets"&gt;{{cite web|url=http://googlewebmastercentral.blogspot.com/2009/05/introducing-rich-snippets.html|title=Introducing Rich Snippets|last=Goel|first=Kavi|author2=Ramanathan V. Guha |author3=Othar Hansson |date=2009-05-12|work=Google Webmaster Central Blog|publisher=Google|accessdate=2009-05-25}}&lt;/ref&gt; They have since extended this to use hCalendar for events&lt;ref name="Google-recipes"&gt;{{cite web|url=http://googlewebmastercentral.blogspot.com/2010/04/better-recipes-on-web-introducing.html|title=Better recipes on the web: Introducing recipe rich snippets|last=Gong|first=Jun|author2=Kosuke Suzuki |author3=Yu Watanabe |date=2010-04-13|publisher=Google|accessdate=17 March 2011}}&lt;/ref&gt; and hRecipe for cookery recipes.&lt;ref name="Google-recipes" /&gt; Similarly, microformats are also processed by [[Bing (search engine)|Bing]]&lt;ref name="Bing"&gt;{{cite web|url=http://www.bing.com/community/site_blogs/b/search/archive/2011/06/02/bing-google-and-yahoo-unite-to-build-the-web-of-objects.aspx|title=Bing Introducing Schema.org: Bing, Google and Yahoo Unite to Build the Web of Objects - Search Blog - Site Blogs - Bing Community|date=2011-06-02|work=[[Bing (search engine)|Bing]]|accessdate=2 June 2011}}&lt;/ref&gt; and [[Yahoo!]].&lt;ref name="YSearch"&gt;{{cite web|url=http://www.ysearchblog.com/2011/06/02/introducing-schema-org-a-collaboration-on-structured-data|title=Introducing schema.org: A Collaboration on Structured Data|date=2011-06-02|accessdate=2 June 2011}}&lt;/ref&gt; Together, these are the world's top three search engines.&lt;ref&gt;{{cite web |url=http://gs.statcounter.com/#search_engine-ww-monthly-201010-201012 |title=Top 5 Search Engines from Oct to Dec 10 &amp;#124; StatCounter Global Stats |author= |work= |publisher=StatCounter |accessdate=17 January 2011}}&lt;/ref&gt;

[[Microsoft]] said they needed to incorporate Microformats into upcoming projects,&lt;ref&gt;{{cite web|url=http://microformats.org/blog/2006/03/20/bill-gates-at-mix06-we-need-microformats |title=Bill Gates at Mix06 – "We need microformats" |date=2006-03-20 |quote=We need microformats and to get people to agree on them. It is going to bootstrap exchanging data on the Web… …we need them for things like contact cards, events, directions… |accessdate=2008-09-06}}&lt;/ref&gt; as did other software companies.

Alex Faaborg summarizes the arguments for putting the responsibility for microformat user interfaces in the web browser rather than making more complicated HTML:&lt;ref&gt;[http://blog.mozilla.com/faaborg/2007/02/04/microformats-part-4-the-user-interface-of-microformat-detection/ Microformats – Part 4: The User Interface of Microformat Detection « Alex Faaborg&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;
* Only the web browser knows what applications are accessible to the user and what the user's preferences are
* It lowers the barrier to entry for web site developers if they only need to do the markup and not handle "appearance" or "action" issues
* Retains backwards compatibility with web browsers that don't support microformats
* The web browser presents a single point of entry from the web to the user's computer, which simplifies security issues

== Evaluation ==
Various commentators have offered review and discussion on the design principles and practical aspects of microformats. Microformats have been compared to other approaches that seek to serve the same or similar purpose.&lt;ref name="criticism000"&gt;{{cite web |title=Criticism |work=Microformats.org |url=http://microformats.org/wiki?title=criticism&amp;oldid=18478 |date=2007-03-24 |accessdate=2007-08-15}}&lt;/ref&gt; From time to time, there is criticism of one, or all, microformats.&lt;ref name="criticism000"/&gt; The spread and use of microformats has been advocated.&lt;ref name="advocacy000"&gt;{{cite web |title=Advocacy |work=Microformats.org |url=http://microformats.org/wiki/advocacy |date=2008-08-27 |accessdate=2007-08-15}}&lt;/ref&gt;&lt;ref name="spread000"&gt;{{cite web |title=Spread Microformats |work=Microformats.org |url=http://microformats.org/wiki/spread-microformats |date=2008-08-29 |accessdate= 2007-08-15}} This includes community resources for marketing microformats such as buttons, banners, wallpaper / desktop screens, logo graphics, etc.&lt;/ref&gt; [[Opera Software]] CTO and [[Cascading Style Sheets|CSS]] creator [[Håkon Wium Lie]] said in 2005 "We will also see a bunch of microformats being developed, and that’s how the [[Semantic Web|semantic web]] will be built, I believe."&lt;ref name="advocacy001"&gt;{{cite web |title=Interview with Håkon Wium Lie |url=http://www.molly.com/2005/03/31/interview-with-hkon-wium-lie/ |first=Molly E. |last=Holzschlag |authorlink=Molly Holzschlag |date=2005-03-31 |work=Molly.com |accessdate=2007-11-18}}&lt;/ref&gt; However, in August 2008 Toby Inkster, author of the "Swignition" (formerly "Cognition") microformat parsing service, pointed out that no new microformat specifications had been published since 2005.&lt;ref name="threeyears"&gt;{{cite web |title=More than three years |url=http://microformats.org/discuss/mail/microformats-discuss/2008-August/012402.html |work=Microformats.org |first=Toby A. |last=Inkster |date=2008-04-22 |accessdate=2008-08-24}}&lt;/ref&gt;

=== Design principles ===
Computer scientist and entrepreneur, [[Rohit Khare]] stated that ''reduce, reuse, and recycle'' is "shorthand for several design principles" that motivated the development and practices behind microformats.&lt;ref name="Khare000"/&gt;{{rp|71–72}} These aspects can be summarized as follows:

*Reduce: favor the simplest solutions and focus attention on specific problems;
*Reuse: work from experience and favor examples of current practice;
*Recycle: encourage modularity and the ability to embed, valid XHTML can be reused in blog posts, RSS feeds, and anywhere else you can access the web.&lt;ref name="Khare000"/&gt;

=== Accessibility ===
Because some microformats make use of title attribute of HTML's {{tag|abbr|open}} element to conceal [[machine-readable data]] (particularly date-times and geographical coordinates) in the "[http://microformats.org/wiki/abbr-design-pattern abbr design pattern]", the plain text content of the element is inaccessible to [[screen reader]]s that expand abbreviations.&lt;ref name="ATF"&gt;{{cite web |url=http://www.webstandards.org/2007/04/27/haccessibility/ | title=hAccessibility | first=James |last=Craig |publisher= [[Web Standards Project]] |date=2007-04-27 |accessdate=2007-08-16}}&lt;/ref&gt; In June 2008 the [[BBC]] announced that it would be dropping use of microformats using the &lt;code&gt;abbr&lt;/code&gt; design pattern because of accessibility concerns.&lt;ref name="BBCabbr"&gt;{{cite web |url=http://www.bbc.co.uk/blogs/radiolabs/2008/06/removing_microformats_from_bbc.shtml |title=Removing Microformats from bbc.co.uk/programmes |first=Michael |last=Smethurst |publisher= [[BBC]] |date=2008-06-23 |accessdate=2008-08-24}}&lt;/ref&gt;

=== Comparison with alternative approaches ===
Microformats are not the only solution for providing "more intelligent data" on the web; alternative approaches are used and are under development. For example, the use of [[XML]] markup and standards of the Semantic Web are cited as alternative approaches.&lt;ref name="Khare000"/&gt; Some contrast these with microformats in that they do not necessarily coincide with the design principles of "reduce, reuse, and recycle", at least not to the same extent.&lt;ref name="Khare000"/&gt;

One advocate of microformats, [[Tantek Çelik]], characterized a problem with alternative approaches: {{cquote|Here's a new language we want you to learn, and now you need to output these additional files on your server. It's a hassle. (Microformats) lower the barrier to entry.&lt;ref name="Wharton000"/&gt;}}

For some applications the use of other approaches may be valid. If the type of data to be described does not map to an existing microformat, [[RDFa]] can embed arbitrary vocabularies into HTML, such as for example domain-specific scientific data such as zoological or chemical data for which there is no microformat. Standards such as W3C's [[GRDDL]] allow microformats to be converted into data compatible with the Semantic Web.&lt;ref name="King007"&gt;{{cite web |title=W3C GRDDL Recommendation Bridges HTML/Microformats and the Semantic Web |work=XML Coverpages |date=2007-09-13 |url=http://xml.coverpages.org/ni2007-09-13-a.html |publisher=[[OASIS (organization)|OASIS]] |accessdate=2007-11-23}}&lt;/ref&gt;

Another advocate of microformats, Ryan King, put the compatibility of microformats with other approaches this way: {{cquote|Microformats provide an easy way for many people to contribute semantic data to the web. With GRDDL all of that data is made available for RDF Semantic Web tools. Microformats and GRDDL can work together to build a better web.&lt;ref name="King007"/&gt;}}

== See also ==
*[[COinS]]
*[[Embedded RDF]]
*[[Intelligent agent]]s
*[[RDFa Lite]]
*[[JSON-LD]]
*[[S5 (file format)]]
*[[Schema.org]]
*[[Simple HTML Ontology Extensions]]
*[[XMDP]]

== Notes ==
{{Reflist|2}}

== References ==
{{Refbegin|2}}
*{{cite book |last=Allsopp |first=John | title=Microformats: Empowering Your Markup for Web 2.0 |date=March 2007 |publisher=[[Apress|Friends of ED]] |isbn=978-1-59059-814-6 |page=368}}
*{{cite book |last=Orchard |first=Leslie M |title=Hacking RSS and Atom |date=September 2005 |publisher=[[John Wiley &amp; Sons]] |isbn=978-0-7645-9758-9 |page=602}}
*{{cite book |last=Robbins |first=Jennifer Niederst |authorlink=Jennifer Niederst Robbins |first2=Tantek|last2=Çelik|authorlink2=Tantek Çelik|first3=Derek|last3=Featherstone|first4=Aaron|last4=Gustafson|title=Web Design In A Nutshell |edition=Third |date=February 2006 |publisher=[[O'Reilly Media]] |isbn=978-0-596-00987-8 |page=826}}

{{Refend}}

== Further reading ==
* {{cite book |last= Suda |first= Brian |title= Using Microformats |date=September 2006 |publisher= [[O'Reilly Media]] |isbn=978-0-596-52821-8 |page=45}}
* Ahmet Soylu, Patrick De Causmaecker, Fridolin Wild [http://www.rintonpress.com/journals/jmmonline.html#v6n1  Ubiquitous Web for Ubiquitous Environments: The Role of Embedded Semantics], article in Journal of Mobile Multimedia, Vol. 6, No.1, pp.&amp;nbsp;26–48, (2010). [https://lirias.kuleuven.be/bitstream/123456789/243944/2/JMM_soylu_et_al_2010.pdf PDF]

== External links ==
{{Commons category|Microformat screenshots}}
* [http://microformats.org/ microformats.org]
* [http://www.digital-web.com/articles/microformats_primer/ Microformats Primer]
* [http://microformatique.com/optimus/ Optimus] microformats parser and validator
* [http://blog.mozilla.com/faaborg/2006/12/11/microformats-part-0-introduction A four-part discussion of Microformats, UI issues, and possible presentation in Firefox 3 by Alex Faaborg of Mozilla]

{{Semantic Web}}
{{Use dmy dates|date=January 2011}}

[[Category:Microformats| ]]
[[Category:Knowledge representation]]
[[Category:Semantic HTML]]
[[Category:Semantic Web]]
[[Category:Web design]]
[[Category:Web development]]</text>
      <sha1>hekueqczn94w75imdifo87pbnb6tc1n</sha1>
    </revision>
  </page>
  <page>
    <title>Fuzzy cognitive map</title>
    <ns>0</ns>
    <id>11270885</id>
    <revision>
      <id>759295748</id>
      <parentid>759295093</parentid>
      <timestamp>2017-01-10T09:47:22Z</timestamp>
      <contributor>
        <username>Kku</username>
        <id>5846</id>
      </contributor>
      <comment>/* Details */ lx</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="14903" xml:space="preserve">[[File:FCMdrug520.png|thumb|right|Rod Tabers FCM depicting eleven factors of the American drug market]]
A '''fuzzy cognitive map''' is a [[cognitive map]] within which the relations between the elements (e.g. concepts, events, project resources) of a "mental landscape" can be used to compute the "strength of impact" of these elements.  Fuzzy cognitive maps were introduced by [[Bart Kosko]].&lt;ref&gt;{{cite journal|author=[[Bart Kosko]]|title=''Fuzzy Cognitive Maps''|journal=International Journal of Man-Machine Studies|volume=24|date=1986|pages=65-75|url=http://sipi.usc.edu/~kosko/FCM.pdf|format=PDF}}&lt;/ref&gt;&lt;ref&gt;[http://sipi.usc.edu/~kosko/Virtual_Worlds_FCM.pdf] {{dead link|date=January 2017}}&lt;/ref&gt;  Ron Axelord introduced Cognitive Maps as a formal way of representing social scientific knowledge and modeling [[decision making]] in social and political systems. Then brought in the computation [[fuzzy logic]].

==Details==
Fuzzy cognitive maps are signed fuzzy [[directed graph|digraph]]s.  They may look at [[first blush]] like [[Hasse diagrams]] but they are not.
[[Spreadsheet]]s or tables are used to map FCMs into [[matrix (Mathematics)|matric]]es for further computation.&lt;ref&gt;{{cite web|url=http://www.FCMappers.net/joomla/index.php?option=com_content&amp;view=article&amp;id=52&amp;Itemid=53 |title=FCMapper - our Fuzzy Cognitive Mapping Software Solution |website=Fcmappers.net |date=2016-01-27 |accessdate=2017-01-09}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.ochoadeaspuru.com/fuzcogmap/index.php |title=Fuzzy Cognitive Maps |website=Ochoadeaspuru.com |date= |accessdate=2017-01-09}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://jfcm.megadix.it/ |title=JFCM - Java Fuzzy Cognitive Maps |website=Jfcm.megadix.it |date= |accessdate=2017-01-09}}&lt;/ref&gt;
FCM is a technique used for causal knowledge acquisition and representation, it supports causal knowledge reasoning process and belong to the neuro-fuzzy system that aim at solving decision making problems, modeling and simulate [[complex system]]s. 
Learning algorithms  have been proposed for training and updating FCMs weights mostly based on ideas coming from the field of [[Artificial Neural Network]]s. Adaptation and learning methodologies used to adapt the FCM model and adjust its weights.  Kosko and Dickerson (Dickerson &amp; Kosko, 1994) suggested the Differential [[Hebbian Learning]] (DHL) to train FCM.&lt;ref&gt;{{cite web|url=http://home.eng.iastate.edu/~julied/publications/FCM96.pdf |title=IEEEBook8.dvi |website=Home.eng.iastate.edu |format=PDF |date= |accessdate=2017-01-09}}&lt;/ref&gt; There have been proposed algorithms based on the initial Hebbian algorithm;&lt;ref&gt;{{cite journal |doi=10.1016/j.ijar.2004.01.001 |title=Active Hebbian learning algorithm to train fuzzy cognitive maps |journal=International Journal of Approximate Reasoning |volume=37 |issue=3 |page=219 |year=2004 |last1=Papageorgiou |first1=E.I. |last2=Stylios |first2=C.D. |last3=Groumpos |first3=P.P. }}&lt;/ref&gt; others algorithms come from the field of [[genetic algorithms]], [[swarm intelligence]]&lt;ref&gt;{{cite journal |doi=10.1007/s10844-005-0864-9 |title=Fuzzy Cognitive Maps Learning Using Particle Swarm Optimization |journal=Journal of Intelligent Information Systems |volume=25 |page=95 |year=2005 |last1=Papageorgiou |first1=Elpiniki I. |last2=Parsopoulos |first2=Konstantinos E. |last3=Stylios |first3=Chrysostomos S. |last4=Groumpos |first4=Petros P. |last5=Vrahatis |first5=Michael N. }}&lt;/ref&gt; and  [[evolutionary computation]].&lt;ref&gt;{{cite book |doi=10.1109/FUZZY.2005.1452465 |chapter=Evolutionary Development of Fuzzy Cognitive Maps |title=The 14th IEEE International Conference on Fuzzy Systems, 2005. FUZZ '05 |pages=619– |year=2005 |last1=Stach |first1=W. |last2=Kurgan |first2=L. |last3=Pedrycz |first3=W. |last4=Reformat |first4=M. |isbn=0-7803-9159-4 }}&lt;/ref&gt; [[Learning algorithms]] are used to overcome the shortcomings that the traditional FCM present i.e. decreasing the human intervention by suggested automated FCM candidates; or by activating only the most relevant concepts every execution time; or by making models more transparent and dynamic.&lt;ref&gt;{{cite journal |doi=10.1016/j.ijhcs.2006.02.009 |title=Unsupervised learning techniques for fine-tuning fuzzy cognitive map causal links |journal=International Journal of Human-Computer Studies |volume=64 |issue=8 |page=727 |year=2006 |last1=Papageorgiou |first1=Elpiniki I. |last2=Stylios |first2=Chrysostomos |last3=Groumpos |first3=Peter P. }}&lt;/ref&gt;

Fuzzy cognitive maps (FCMs) have gained considerable research interest due to their ability in representing structured knowledge and model complex systems in various fields. This growing interest led to the need for enhancement and making more reliable models that can better represent real situations.
A first simple application of FCMs is described in a book&lt;ref name="confusion"&gt;William R. Taylor: ''[http://www.americanconfusion.com/?p=122 Lethal American Confusion] (How Bush and the Pacifists Each Failed in the War on Terrorism)'', 2006, ISBN 0-595-40655-6 (FCM application in chapter 14) {{webarchive |url=https://web.archive.org/web/20070930103802/http://www.americanconfusion.com/?p=122 |date=September 30, 2007 }}&lt;/ref&gt; of William R. Taylor, where the war in Afghanistan and Iraq is analyzed. And in [[Bart Kosko]]'s book ''Fuzzy Thinking'',&lt;ref name="FuzzyThinking"&gt;Bart Kosko: ''Fuzzy Thinking'', 1993/1995, ISBN 0-7868-8021-X (Chapter 12: Adaptive Fuzzy Systems)''&lt;/ref&gt; several Hasse diagrams illustrate the use of FCMs. As an example, one FCM quoted from Rod Taber&lt;ref name="Drugs"&gt;Rod Taber: ''Knowledge Processing with Fuzzy Cognitive Maps'', Expert Systems with Applications, vol. 2, no. 1, 83-87, 1991 ([[:de:Bild:FCMdrug520.png|Hasse diagram]] in German Wikipedia)&lt;/ref&gt; describes 11 factors of the American cocaine market and the relations between these factors. For computations, Taylor uses pentavalent logic (scalar values out of {-1,-0.5,0,+0.5,+1}). That particular map of Taber uses [[trivalent logic]] (scalar values out of {-1,0,+1}). Taber et al.  also illustrate the dynamics of map fusion and give a theorem on the convergence of combination in a related article &lt;ref name='Medical'&gt;{{cite journal |doi=10.1002/int.20185 |title=Quantization effects on the equilibrium behavior of combined fuzzy cognitive maps |journal=International Journal of Intelligent Systems |volume=22 |issue=2 |page=181 |year=2007 |last1=Taber |first1=Rod |last2=Yager |first2=Ronald R. |last3=Helgason |first3=Cathy M. }}&lt;/ref&gt;

While applications in social sciences&lt;ref name="confusion"/&gt;&lt;ref name="FuzzyThinking"/&gt;&lt;ref name="Drugs"/&gt;&lt;ref&gt;Costas Neocleous, Christos Schizas, Costas Yenethlis: ''[http://www.congreso-info.cu/UserFiles/File/Info/Info2006/Ponencias/271.pdf Fuzzy Cognitive Models in Studying Political Dynamics  - The case of the Cyprus problem]'' {{webarchive |url=https://web.archive.org/web/20070929055849/http://www.congreso-info.cu/UserFiles/File/Info/Info2006/Ponencias/271.pdf |date=September 29, 2007 }}&lt;/ref&gt; introduced FCMs to the public, they are used in a much wider range of applications, which all have to deal with creating and using models&lt;ref&gt;Chrysostomos D. Stylios, Voula C. Georgopoulos, Peter P. Groumpos: ''[http://med.ee.nd.edu/MED5/PAPERS/067/067.PDF The Use of Fuzzy Cognitive Maps in Modeling Systems]'' {{webarchive |url=https://web.archive.org/web/20110720011915/http://med.ee.nd.edu/MED5/PAPERS/067/067.PDF |date=July 20, 2011 }}&lt;/ref&gt; of uncertainty and complex processes and systems. Examples:
*In business FCMs can be used for product planning.&lt;ref&gt;Antonie Jetter: ''Produktplanung im Fuzzy Front End'', 2005, ISBN 3-8350-0144-2&lt;/ref&gt;
*In economics, FCMs support the use of [[game theory]] in more complex settings.&lt;ref&gt;Vesa A. Niskanen: ''[http://www.ijicic.org/fic04-20.pdf Application of Fuzzy Linguistic Cognitive Maps to Prisoner's Dilemma]'', 2005, ICIC International pp. 139-152, ISSN 1349-4198 {{webarchive |url=https://web.archive.org/web/20070929040145/http://www.ijicic.org/fic04-20.pdf |date=September 29, 2007 }}&lt;/ref&gt;
* In Medical applications to model systems, provide diagnosis,&lt;ref&gt;{{cite journal |doi=10.1016/S0933-3657(02)00076-3 |pmid=14656490 |title=A fuzzy cognitive map approach to differential diagnosis of specific language impairment |journal=Artificial Intelligence in Medicine |volume=29 |issue=3 |pages=261–78 |year=2003 |last1=Georgopoulos |first1=Voula C |last2=Malandraki |first2=Georgia A |last3=Stylios |first3=Chrysostomos D }}&lt;/ref&gt; develop [[decision support systems]]&lt;ref&gt;{{cite journal |doi=10.1109/TBME.2003.819845 |pmid=14656062 |title=An integrated two-level hierarchical system for decision making in radiation therapy based on fuzzy cognitive maps |journal=IEEE Transactions on Biomedical Engineering |volume=50 |issue=12 |pages=1326–39 |year=2003 |last1=Papageorgiou |first1=E.I. |last2=Stylios |first2=C.D. |last3=Groumpos |first3=P.P. }}&lt;/ref&gt; and [[medical assessment]].&lt;ref&gt;{{cite book |doi=10.1007/978-3-319-11457-6_18 |chapter=Supervisory Fuzzy Cognitive Map Structure for Triage Assessment and Decision Support in the Emergency Department |title=Simulation and Modeling Methodologies, Technologies and Applications |volume=319 |pages=255–69 |series=Advances in Intelligent Systems and Computing |year=2015 |last1=Georgopoulos |first1=Voula C. |last2=Stylios |first2=Chrysostomos D. |isbn=978-3-319-11456-9 }}&lt;/ref&gt;
* In Engineering for [[process modeling|modeling]] and [[Process control|control]]&lt;ref&gt;{{cite web|url=http://content.iospress.com/articles/journal-of-intelligent-and-fuzzy-systems/ifs089?resultNumber=0&amp;totalResults=11&amp;start=0&amp;q=stylios&amp;resultsPageSize=10&amp;rows=10 |title=Fuzzy Cognitive Maps in modeling supervisory control systems - IOS Press |website=Content.iospress.com |date= |accessdate=2017-01-09}}&lt;/ref&gt; mainly of complex systems&lt;ref&gt;{{cite journal |doi=10.1109/TSMCA.2003.818878 |title=Modeling Complex Systems Using Fuzzy Cognitive Maps |journal=IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans |volume=34 |page=155 |year=2004 |last1=Stylios |first1=C.D. |last2=Groumpos |first2=P.P. }}&lt;/ref&gt;
*In project planning FCMs help to analyze the mutual dependencies between project resources.
*In robotics&lt;ref name="FuzzyThinking"/&gt;&lt;ref&gt;Marc Böhlen: ''[http://www.realtechsupport.org/pdf/SpaceRobotics2000.pdf More Robots in Cages]'',&lt;/ref&gt; FCMs support machines to develop fuzzy models of their environments and to use these models to make crisp decisions.
*In computer assisted learning FCMs enable computers to check whether students understand their lessons.&lt;ref&gt;Benjoe A. Juliano, Wylis Bandler: ''Tracing Chains-of-Thought (Fuzzy Methods in Cognitive Diagnosis)'', Physica-Verlag Heidelberg 1996, ISBN 3-7908-0922-5&lt;/ref&gt;
*In [[expert system]]s&lt;ref name="Drugs"/&gt; a few or many FCMs can be aggregated into one FCM in order to process estimates of knowledgeable persons.&lt;ref&gt;W. B. Vasantha Kandasamy, Florentin Smarandache: ''[http://www.gallup.unm.edu/~smarandache/NCMs.pdf Fuzzy Cognitive Maps and Neutrosophic Cognitive Maps]'', 2003, ISBN 1-931233-76-4&lt;/ref&gt;
*In IT project management, a FCM-based methodology helps to success modelling.&lt;ref&gt;{{cite journal |doi=10.1016/j.eswa.2006.01.032 |title=Modelling IT projects success with Fuzzy Cognitive Maps |journal=Expert Systems with Applications |volume=32 |issue=2 |page=543 |year=2007 |last1=Rodriguez-Repiso |first1=Luis |last2=Setchi |first2=Rossitza |last3=Salmeron |first3=Jose L. }}&lt;/ref&gt;

FCMappers&lt;ref&gt;FCMappers - international community for fuzzy cognitive mapping: http://www.FCMappers.net/&lt;/ref&gt; - an international online community for the analysis and the visualization of fuzzy cognitive maps offer support for starting with FCM and also provide an MS-Excel-based tool that is able to check and analyse FCMs. The output is saved as [[Pajek]] file and can be visualized within 3rd party software like Pajek, Visone,... . They also offer to adapt the software to specific research needs. On their webpage you also will find a linklist for interesting scientific articles, related software, institutes, people and projects. The FCMappers have about one thousand registered members worldwide.

Additional FCM software tools, such as Mental Modeler,&lt;ref&gt;{{cite book |doi=10.1109/HICSS.2013.399 |chapter=Mental Modeler: A Fuzzy-Logic Cognitive Mapping Modeling Tool for Adaptive Environmental Management |title=2013 46th Hawaii International Conference on System Sciences |pages=965– |year=2013 |last1=Gray |first1=Steven A. |last2=Gray |first2=Stefan |last3=Cox |first3=Linda J. |last4=Henly-Shepard |first4=Sarah |isbn=978-1-4673-5933-7 }}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.mentalmodeler.com/ |title=Fuzzy Logic Cognitive Mapping |publisher=Mental Modeler |date= |accessdate=2017-01-09}}&lt;/ref&gt; have recently been developed as a decision-support tool for use in [[social science]] research, [[collaborative decision-making]], and [[Natural resource management|natural resource planning]].

==Bipolar Fuzzy Cognitive Maps==
Fuzzy cognitive maps have been further extended to bipolar fuzzy cognitive maps based on bipolar fuzzy sets &lt;ref&gt;Wen-Ran Zhang, 1998, (Yin)(Yang) Bipolar Fuzzy Sets. Proceedings of IEEE World Congress on Computational Intelligence – Fuzz-IEEE, Anchorage, AK, 835-840 &lt;/ref&gt; and bipolar cognitive mapping.&lt;ref&gt;{{cite journal |doi=10.1109/21.24529 |title=Pool2: A generic system for cognitive map development and decision analysis |journal=IEEE Transactions on Systems, Man, and Cybernetics |volume=19 |page=31 |year=1989 |last1=Zhang |first1=W.R. |last2=Chen |first2=S.S. |last3=Bezdek |first3=J.C. }}&lt;/ref&gt;&lt;ref&gt;{{cite journal |doi=10.1109/21.141315 |title=A cognitive-map-based approach to the coordination of distributed cooperative agents |journal=IEEE Transactions on Systems, Man, and Cybernetics |volume=22 |page=103 |year=1992 |last1=Zhang |first1=W.-R. |last2=Chen |first2=S.-S. |last3=Wang |first3=W. |last4=King |first4=R.S. }}&lt;/ref&gt;&lt;ref&gt;{{cite journal |doi=10.1109/TSMCB.2003.810444 |title=Equilibrium relations and bipolar cognitive mapping for online analytical processing with applications in international relations and strategic decision support |journal=IEEE Transactions on Systems, Man and Cybernetics, Part B (Cybernetics) |volume=33 |issue=2 |page=295 |year=2003 |last1=Wen-Ran Zhang }}&lt;/ref&gt;&lt;ref&gt;Wen-Ran Zhang,  2003b, Equilibrium Energy and Stability Measures for Bipolar Decision and Global Regulation. Int’l J. of Fuzzy Sys. Vol. 5, No. 2, 2003, 114-122&lt;/ref&gt; Bipolar fuzzy set theory as an equilibrium-based extension to fuzzy sets is recognized by [[L. A. Zadeh]]. &lt;ref&gt;L. A. Zadeh, 2008, Fuzzy logic. Scholarpedia, 3(3):1766, Created: 10 July 2006, reviewed: 27 March 2007, accepted: 31 March 2008.&lt;/ref&gt;

==See also==
[[Soft Computing]]   

==References==
{{Reflist|30em}}

{{Commons category|Cognitive maps}}

[[Category:Knowledge representation]]
[[Category:Fuzzy logic]]</text>
      <sha1>5gwyovw8cc74i8e1qagkgd27j5hu1at</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Grouping</title>
    <ns>14</ns>
    <id>11284359</id>
    <revision>
      <id>131699395</id>
      <timestamp>2007-05-18T03:10:28Z</timestamp>
      <contributor>
        <username>Grumpyyoungman01</username>
        <id>846078</id>
      </contributor>
      <comment>[[WP:AES|←]]Created page with '[[Category:Knowledge representation]]'</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="37" xml:space="preserve">[[Category:Knowledge representation]]</text>
      <sha1>c7ub5uwhswlhnrrfyh07qu4qoi2x1xv</sha1>
    </revision>
  </page>
  <page>
    <title>Qualification problem</title>
    <ns>0</ns>
    <id>731287</id>
    <revision>
      <id>690772395</id>
      <parentid>614232524</parentid>
      <timestamp>2015-11-15T16:20:14Z</timestamp>
      <contributor>
        <username>RW Dutton</username>
        <id>7102439</id>
      </contributor>
      <comment>/* External links */ Add to Epistemology category (and move to Epistemology stubs) to match Ratification problem page</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1400" xml:space="preserve">{{one source|date=July 2011}}
In [[philosophy]] and [[Artificial intelligence|AI]] (especially, knowledge based systems), the '''qualification problem''' is concerned with the impossibility of listing ''all'' the [[precondition]]s required for a real-world action to have its intended effect. It might be posed as ''how to deal with the things that prevent me from achieving my intended result''. It is strongly connected to, and opposite the [[ramification problem|ramification side]] of, the [[frame problem]]. John McCarthy gives the following motivating example, in which it is impossible to enumerate all the circumstances that may prevent a rowboat from performing its ordinary function:

:"[T]he successful use of a boat to cross a river requires, if the boat is a rowboat, that the oars and rowlocks be present and unbroken, and that they fit each other. Many other qualifications can be added, making the rules for using a rowboat almost impossible to apply, and yet anyone will still be able to think of additional requirements not yet stated."

==See also==
*[[Non-monotonic logic]]
*[[Circumscription (logic)|Circumscription]]

==External links==
* John McCarthy "[http://www-formal.stanford.edu/jmc/circumscription/node1.html Introduction: The Qualification Problem]" 

[[Category:Knowledge representation]]
[[Category:Logic programming]]
[[Category:Epistemology]]

{{epistemology-stub}}</text>
      <sha1>qepck1hjl6vn3r52v3h8hfaokc88qme</sha1>
    </revision>
  </page>
  <page>
    <title>Event calculus</title>
    <ns>0</ns>
    <id>2897680</id>
    <revision>
      <id>725178201</id>
      <parentid>725178163</parentid>
      <timestamp>2016-06-14T01:53:14Z</timestamp>
      <contributor>
        <ip>86.30.201.123</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10676" xml:space="preserve">The '''event calculus''' is a [[logic]]al language for representing and reasoning about events and their effects first presented by [[Robert Kowalski]] and [[Marek Sergot]] in 1986.
It was extended by [[Murray Shanahan]] and [[Rob Miller (Computer Scientist)|Rob Miller]] in the 1990s. Similar to other languages for reasoning about change, the event calculus represents the effects of [[Action (artificial intelligence)|action]]s on [[fluent (artificial intelligence)|fluent]]s. However, [[Event (computing)|event]]s can also be external to the system. In the event calculus, one can specify the value of fluents at some given time points, the events that take place at given time points, and  their effects.

==Fluents and events==

In the event calculus, fluents are [[Reification (knowledge representation)|reified]]. This means that they are not formalized by means of [[Predicate (mathematics)|predicate]]s but by means of [[function (mathematics)|function]]s. A separate predicate &lt;math&gt;HoldsAt&lt;/math&gt; is used to tell which fluents hold at a given time point. For example, &lt;math&gt;HoldsAt(on(box,table),t)&lt;/math&gt; means that the box is on the table at time &lt;math&gt;t&lt;/math&gt;; in this formula, &lt;math&gt;HoldsAt&lt;/math&gt; is a predicate while &lt;math&gt;on&lt;/math&gt; is a function.

Events are also represented as terms. The effects of events are given using the predicates &lt;math&gt;Initiates&lt;/math&gt; and &lt;math&gt;Terminates&lt;/math&gt;. In particular, &lt;math&gt;Initiates(e,f,t)&lt;/math&gt; means that,
if the event represented by the term &lt;math&gt;e&lt;/math&gt; is executed at time &lt;math&gt;t&lt;/math&gt;,
then the fluent &lt;math&gt;f&lt;/math&gt; will be true after &lt;math&gt;t&lt;/math&gt;.
The &lt;math&gt;Terminates&lt;/math&gt; predicate has a similar meaning, with the only difference 
being that &lt;math&gt;f&lt;/math&gt; will be false and not true after &lt;math&gt;t&lt;/math&gt;.

==Domain-independent axioms==

Like other languages for representing actions, the event calculus formalizes the correct evolution of the fluent via formulae telling the value of each fluent after an arbitrary action has been performed. The event calculus solves the [[frame problem]] in a way that is similar to the [[successor state axiom]]s of the [[situation calculus]]: a fluent is true at time &lt;math&gt;t&lt;/math&gt; if and only if it has been made true in the past and has not been made false in the meantime.
 
:&lt;math&gt;HoldsAt(f,t) \leftarrow
[Happens(e,t_1) \wedge Initiates(e,f,t_1) 
\wedge (t_1&lt;t) \wedge \neg Clipped(t_1,f,t)]&lt;/math&gt;

This formula means that the fluent represented by the term &lt;math&gt;f&lt;/math&gt; is true at time &lt;math&gt;t&lt;/math&gt; if:

# an event &lt;math&gt;e&lt;/math&gt; has taken place: &lt;math&gt;Happens(e,t_1)&lt;/math&gt;;
# this took place in the past: &lt;math&gt;t_1&lt;t&lt;/math&gt;;
# this event has the fluent &lt;math&gt;f&lt;/math&gt; as an effect: &lt;math&gt;Initiates(e,f,t_1)&lt;/math&gt;; 
# the fluent has not been made false in the meantime: &lt;math&gt;Clipped(t_1,f,t)&lt;/math&gt;

A similar formula is used to formalize the opposite case in which a fluent is false at a given time. Other formulae are also needed for correctly formalizing fluents before they have been effects of an event. These formulae are similar to the above, but &lt;math&gt;Happens(e,t_1) \wedge Initiates(e,f,t_1)&lt;/math&gt; is replaced by &lt;math&gt;HoldsAt(f,t_1)&lt;/math&gt;.

The &lt;math&gt;Clipped&lt;/math&gt; predicate, stating that a fluent has been made false during an interval, can be axiomatized, or simply taken as a shorthand, as follows:

:&lt;math&gt;Clipped(t_1,f,t_2) \equiv
\exists e,t 
[Happens(e,t) \wedge (t_1 \leq t &lt; t_2) \wedge Terminates(e,f,t)]&lt;/math&gt;

==Domain-dependent axioms==

The axioms above relate the value of the predicates &lt;math&gt;HoldsAt&lt;/math&gt;, &lt;math&gt;Initiates&lt;/math&gt; and &lt;math&gt;Terminates&lt;/math&gt;, but do not specify which fluents are known to be true and which events actually make fluents true or false. This is done by using a set of domain-dependent axioms. The known values of fluents are stated as simple literals &lt;math&gt;HoldsAt(f,t)&lt;/math&gt;. The effects of events are stated by formulae relating the effects of events with their preconditions. For example, if the event &lt;math&gt;open&lt;/math&gt; makes the fluent &lt;math&gt;isopen&lt;/math&gt; true, but only if &lt;math&gt;haskey&lt;/math&gt; is currently true, the corresponding formula in the event calculus is:

:&lt;math&gt;Initiates(e,f,t) \equiv
[ e=open \wedge f=isopen \wedge HoldsAt(haskey, t)] \vee \cdots
&lt;/math&gt;

The right-hand expression of this equivalence is composed of a disjunction: for each event and fluent that can be made true by the event, there is a disjunct saying that &lt;math&gt;e&lt;/math&gt; is actually that event, that &lt;math&gt;f&lt;/math&gt; is actually that fluent, and that the precondition of the event is met.

The formula above specifies the [[truth value]] of &lt;math&gt;Initiates(e,f,t)&lt;/math&gt; for every possible event and fluent. As a result, all effects of all events have to be combined in a single formulae. This is a problem, because the addition of a new event requires modifying an existing formula rather than adding new ones. This problem can be solved by the application of [[Circumscription (logic)|circumscription]] to a set of formulae each specifying one effect of one event:

: &lt;math&gt;Initiates(open, isopen, t) \leftarrow HoldsAt(haskey, t)&lt;/math&gt;
: &lt;math&gt;Initiates(break, isopen, t) \leftarrow HoldsAt(hashammer, t)&lt;/math&gt;
: &lt;math&gt;Initiates(break, broken, t) \leftarrow HoldsAt(hashammer, t)&lt;/math&gt;

These formulae are simpler than the formula above, because each effect of each event can be specified separately. The single formula telling which events &lt;math&gt;e&lt;/math&gt; and fluents &lt;math&gt;f&lt;/math&gt; make &lt;math&gt;Initiates(e,f,t)&lt;/math&gt; true has been replaced by a set of smaller formulae, each one telling the effect of an event on a fluent.
 
However, these formulae are not equivalent to the formula above. Indeed, they only specify sufficient conditions for &lt;math&gt;Initiates(e,f,t)&lt;/math&gt; to be true, which should be completed by the fact that &lt;math&gt;Initiates&lt;/math&gt; is false in all other cases. This fact can be formalized by simply circumscribing the predicate &lt;math&gt;Initiates&lt;/math&gt; in the formula above. It is important to note that this circumscription is done only on the formulae specifying &lt;math&gt;Initiates&lt;/math&gt; and not on the domain-independent axioms. The predicate &lt;math&gt;Terminates&lt;/math&gt; can be specified in the same way &lt;math&gt;Initiates&lt;/math&gt; is.

A similar approach can be taken for the &lt;math&gt;Happens&lt;/math&gt; predicate. The evaluation of this predicate can be enforced by formulae specifying not only when it is true and when it is false:

:&lt;math&gt;Happens(e,t) \equiv
(e=open \wedge t=0) \vee (e=exit \wedge t=1) \vee \cdots&lt;/math&gt;

Circumscription can simplify this specification, as only necessary conditions can be specified:

:&lt;math&gt;Happens(open, 0)&lt;/math&gt;
:&lt;math&gt;Happens(exit, 1)&lt;/math&gt;

Circumscribing the predicate &lt;math&gt;Happens&lt;/math&gt;, this predicate will be false at all points in which it is not explicitly specified to be true. This circumscription has to be done separately from the circumscription of the other formulae. In other words, if &lt;math&gt;F&lt;/math&gt; is the set of formulae of the kind &lt;math&gt;Initiates(e,f,t) \leftarrow \cdots&lt;/math&gt;, &lt;math&gt;G&lt;/math&gt; is the set of formulae &lt;math&gt;Happens(e, t)&lt;/math&gt;, and &lt;math&gt;H&lt;/math&gt; are the domain independent axioms, the correct formulation of the domain is:

:&lt;math&gt;Circ(F; Initiates, Terminates) \wedge
Circ(G; Happens) \wedge H&lt;/math&gt;

==The event calculus as a logic program==

The event calculus was originally formulated as a set of [[Horn clauses]] augmented with [[negation as failure]] and could be run as a [[Prolog]] program. 
In fact, circumscription is one of the several semantics that can be given to negation as failure, and is closely related to the completion semantics (in which "if" is interpreted as "if and only if" &amp;mdash; see [[logic programming]]).

==Extensions and applications==

The original event calculus paper of Kowalski and Sergot focused on applications to database updates and narratives. Extensions of the event 
calculus can also formalize non-deterministic actions, concurrent actions, actions with delayed effects, gradual changes, actions with duration, continuous change, and non-inertial fluents.

Kave Eshghi showed how the event calculus can be used for planning, using [[Abduction (logic)|abduction]] to generate hypothetical events in [[Abductive Logic Programming|abductive logic programming]]. Van Lambalgen and Hamm showed how the event calculus can also be used to give an algorithmic semantics to tense and aspect in natural language using constraint logic programming.

==Reasoning tools==

In addition to Prolog and its variants, several other tools for reasoning using the event calculus are also available:
* [http://www.doc.ic.ac.uk/~mpsha/planners.html Abductive Event Calculus Planners]
* [http://decreasoner.sourceforge.net/ Discrete Event Calculus Reasoner]
* [http://reasoning.eas.asu.edu/ecasp/ Event Calculus Answer Set Programming]
* [https://www.inf.unibz.it/~montali/tools.html Reactive Event Calculus]

==See also==

* [[First-order logic]]
* [[Frame problem]]
* [[Situation calculus]]

==References==
* Brandano, S. (2001) "[http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?isnumber=20130&amp;arnumber=930691&amp;count=35&amp;index=2 The Event Calculus Assessed,]" ''IEEE TIME Symposium'': 7-12.
* Eshghi, K. (1988) "Abductive Planning with Event Calculus," ''ICLP/SLP'': 562-79.
* Kowalski, R. (1992) "Database updates in the event calculus," ''Journal of Logic Programming 12 (162)'': 121-46.
* -------- and M. Sergot (1986) "[http://www.doc.ic.ac.uk/~rak/papers/event%20calculus.pdf A Logic-Based Calculus of Events,]" ''New Generation Computing 4'': 67–95.
* -------- and F. Sadri (1995) "Variants of the Event Calculus," ''ICLP'': 67-81.
* Miller, R., and M. Shanahan (1999) "[http://www.ida.liu.se/ext/epa/ej/etai/1999/016/epapage.html The event-calculus in classical logic — alternative axiomatizations,]" ''[[Electronic Transactions on Artificial Intelligence]]'' 3(1): 77-105.
* Mueller, Erik T. (2015). ''Commonsense Reasoning: An Event Calculus Based Approach (2nd Ed.)''. Waltham, MA: Morgan Kaufmann/Elsevier. ISBN 978-0128014165. (Guide to using the event calculus)
* Shanahan, M. (1997) ''Solving the frame problem: A mathematical investigation of the common sense law of inertia''. MIT Press.
* -------- (1999) "[http://www.springerlink.com/content/1bxk8gd0n6pajxbq/?p=8f3428a89bad4589a949d74b6f0ec98d&amp;pi=0 The Event Calculus Explained,]" Springer Verlag, LNAI (1600): 409-30.
* Van Lambalgen, M., and F. Hamm (2005) ''The proper treatment of events''. Oxford and Boston: Blackwell Publishing.

[[Category:1986 introductions]]
[[Category:Logic in computer science]]
[[Category:Logic programming]]
[[Category:Knowledge representation]]
[[Category:Logical calculi]]</text>
      <sha1>1glfkhixe5v7en3g1zq7ohiwlthnvhh</sha1>
    </revision>
  </page>
  <page>
    <title>Region connection calculus</title>
    <ns>0</ns>
    <id>8489018</id>
    <revision>
      <id>749442283</id>
      <parentid>749442220</parentid>
      <timestamp>2016-11-14T10:20:59Z</timestamp>
      <contributor>
        <username>Widefox</username>
        <id>1588193</id>
      </contributor>
      <minor />
      <comment>/* top */ bold alt article name per MOS</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4849" xml:space="preserve">{{no footnotes|date=November 2016}}
The '''region connection calculus''' ('''RCC''') is intended to serve for qualitative spatial representation and [[Spatial-temporal reasoning|reasoning]]. RCC abstractly describes regions (in [[Euclidean space]], or in a [[topological space]]) by their possible relations to each other. RCC8 consists of 8 basic relations that are possible between two regions:
* disconnected (DC)
* externally connected (EC)
* equal (EQ)
* partially overlapping (PO)
* tangential proper part (TPP)
* tangential proper part inverse (TPPi)
* non-tangential proper part (NTPP)
* non-tangential proper part inverse (NTPPi)
From these basic relations, combinations can be built. For example, proper part (PP) is the union of TPP and NTPP.
[[Image:RCC8.jpg]]

==Composition table==
The composition table of RCC8 are as follows:

&lt;center&gt;
{| class="wikitable" style="text-align:center;" border="1"
! o
! DC || EC || PO || TPP || NTPP || TPPi || NTPPi || EQ
|-
! DC
| * || DC,EC,PO,TPP,NTPP || DC,EC,PO,TPP,NTPP || DC,EC,PO,TPP,NTPP || DC,EC,PO,TPP,NTPP || DC || DC || DC
|-
! EC
| DC,EC,PO,TPPi,NTPPi || DC,EC,PO,TPP,TPPi,EQ || DC,EC,PO,TPP,NTPP || EC,PO,TPP,NTPP || PO,TPP,NTPP || DC,EC || DC || EC
|-
! PO
| DC,EC,PO,TPPi,NTPPi || DC,EC,PO,TPPi,NTPPi || * || PO,TPP,NTPP || PO,TPP,NTPP || DC,EC,PO,TPPi,NTPPi || DC,EC,PO,TPPi,NTPPi || PO
|-
! TPP
| DC || DC,EC || DC,EC,PO,TPP,NTPP || TPP,NTPP || NTPP || DC,EC,PO,TPP,TPPi,EQ || DC,EC,PO,TPPi,NTPPi || TPP
|-
! NTPP
| DC || DC || DC,EC,PO,TPP,NTPP || NTPP || NTPP || DC,EC,PO,TPP,NTPP|| * || NTPP
|-
! TPPi
| DC,EC,PO,TPPi,NTPPi || EC,PO,TPPi,NTPPi || PO,TPPi,NTPPi || PO,TPP,TPPi,EQ || PO,TPP,NTPP || TPPi,NTPPi || NTPPi || TPPi
|-
! NTPPi
| DC,EC,PO,TPPi,NTPPi || PO,TPPi,NTPPi || PO,TPPi,NTPPi || PO,TPPi,NTPPi || PO,TPP,NTPP,TPPi,NTPPi,EQ|| NTPPi || NTPPi || NTPPi
|-
! EQ
|  DC || EC || PO || TPP || NTPP || TPPi || NTPPi || EQ
|}
&lt;/center&gt;

*  "*" denotes the universal relation.

==Examples==

The RCC8 calculus is intended for reasoning about spatial configurations. Consider the following example: two houses are connected via a road. Each house is located on an own property. The first house possibly touches the boundary of the property; the second one surely does not. What can we infer about the relation of the second property to the road? 

The spatial configuration can be formalized in RCC8 as the following [[constraint network]]:

 house1 DC house2
 house1 {TPP, NTPP} property1
 house1 {DC, EC} property2
 house1 EC road
 house2 { DC, EC } property1
 house2 NTPP property2
 house2 EC road
 property1 { DC, EC } property2
 road { DC, EC, TPP, TPPi, PO, EQ, NTPP, NTPPi } property1
 road { DC, EC, TPP, TPPi, PO, EQ, NTPP, NTPPi } property2

Using the RCC8 [[composition table]] and the [[path-consistency algorithm]], we can refine the network in the following way:
 road { PO, EC } property1
 road { PO, TPP } property2

That is, the road either overlaps with the second property, or is even (tangential) part of it.

Other versions of the region connection calculus include RCC5 (with only five basic relations - the distinction whether two regions touch each other are ignored) and RCC23 (which allows reasoning about convexity).

==RCC8 use in GeoSPARQL==

RCC8 has been partially{{Clarify|date=January 2016}} implemented in [[GeoSPARQL]] as described below:
[[File:Region_Connection_Calculus_8_Relations_and_Open_Geospatial_Consortium_relations.svg|thumb|center|700px|alt=A graphical representation of Region Connection Calculus (RCC: Randell, Cui and Cohn, 1992) and the links to the equivalent naming by the Open Geospatial Consortium (OGC) with their equivalent URIs.|A graphical representation of Region Connection Calculus (RCC: Randell, Cui and Cohn, 1992) and the links to the equivalent naming by the Open Geospatial Consortium (OGC) with their equivalent URIs.]]

==References==
* Randell, D. A., Cui, Z. and Cohn, A. G.:  [http://wenxion.net/ac/randell92spatial.pdf A spatial logic based on regions and connection], Proc. 3rd Int. Conf. on Knowledge Representation and Reasoning, Morgan Kaufmann, San Mateo, pp. 165–176, 1992.
* Anthony G. Cohn, Brandon Bennett, John Gooday, Micholas Mark Gotts: Qualitative Spatial Representation and Reasoning with the Region Connection Calculus. GeoInformatica, 1, 275–316, 1997.
* J. Renz: [http://www.springerlink.com/content/d5g7fcjkd0q2/ Qualitative Spatial Reasoning with Topological Information]. Lecture Notes in Computer Science 2293, Springer Verlag, 2002.
* T. Dong: [http://www.jstor.org/stable/41217909?seq=1#page_scan_tab_contents A COMMENT ON RCC: FROM RCC TO RCC⁺⁺]. Journal of Philosophical Logic, Vol 34, No. 2, pp. 319--352 

[[Category:Reasoning]]
[[Category:Knowledge representation]]
[[Category:Constraint programming]]
[[Category:Computational topology]]
[[Category:Logical calculi]]</text>
      <sha1>bxvwgbwl7cs8wlde1qcukmnwqff8jic</sha1>
    </revision>
  </page>
  <page>
    <title>Digital curation</title>
    <ns>0</ns>
    <id>16702334</id>
    <revision>
      <id>754518343</id>
      <parentid>752422419</parentid>
      <timestamp>2016-12-13T02:41:45Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 1 sources and tagging 0 as dead. #IABot (v1.2.7.1)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11750" xml:space="preserve">'''Digital curation''' is the selection,&lt;ref name="ALA, Scime" &gt;{{Cite web
  |title=The Content Strategist as Digital Curator
  |author=Erin Scime
  |date=8 December 2009
  |publisher=[[A List Apart]]
  |url=http://www.alistapart.com/articles/content-strategist-as-digital-curator/
}}&lt;/ref&gt; [[Preservation (library and archival science)|preservation]], maintenance,  collection and [[archiving]] of [[Digital data|digital]] assets.&lt;ref name="paper"&gt;{{Cite book | last1 = Rusbridge | first1 = C. | last2 = Buneman | first2 = P. | authorlink2 = Peter Buneman| last3 = Burnhill | first3 = P. | last4 = Giaretta | first4 = D. | last5 = Ross | first5 = S. | last6 = Lyon | first6 = L. | last7 = Atkinson | first7 = M. | authorlink7 = Malcolm Atkinson| chapter = The Digital Curation Centre: A Vision for Digital Curation | doi = 10.1109/LGDI.2005.1612461 | title = 2005 IEEE International Symposium on Mass Storage Systems and Technology | pages = 31 | year = 2005 | url = http://eprints.erpanet.org/82/01/DCC_Vision.pdf| isbn = 0-7803-9228-0 | pmid =  | pmc = }}&lt;/ref&gt;&lt;ref name="dccdefn"&gt;{{cite web |title=What is Digital Curation? |publisher=[[Digital Curation Centre]] |url=http://www.dcc.ac.uk/about/what |accessdate=2008-04-01}}&lt;/ref&gt;&lt;ref&gt;{{cite web |title=Digital curation |author=Elizabeth Yakel |publisher=Emerald Group Publishing |year=2007 |url=http://www.ingentaconnect.com/content/mcb/164/2007/00000023/00000004/art00003 |accessdate=2008-04-01}}&lt;/ref&gt;
Digital curation establishes, maintains and adds value to repositories of digital data for present and future use.&lt;ref name="dccdefn"/&gt; This is often accomplished by [[archivist]]s, librarians, scientists, historians, and scholars. Enterprises are starting to use digital curation to improve the quality of information and data within their operational and strategic processes.&lt;ref&gt;E. Curry, A. Freitas, and S. O'Riáin, [http://3roundstones.com/led_book/led-curry-et-al.html "The Role of Community-Driven Data Curation for Enterprises,"] in Linking Enterprise Data, D. Wood, Ed. Boston, MA: Springer US, 2010, pp. 25-47.&lt;/ref&gt; Successful digital curation will mitigate digital obsolescence, keeping the information accessible to users indefinitely. 

The term ''[[curator|curation]]'' in the past commonly referred to museum and library professionals. It has since been applied to interaction with [[social media]] including compiling digital images, web links and movie files.

==Approaches==
===Create new representation===
For some topics, knowledge is embodied in forms that have not been conducive to print, such as how choreography of dance or of the motion of skilled workers or artisans is difficult to encode. New digital approaches such as 3d holograms and other computer-programmed expressions are developing. 

For mathematics, it seems possible for a new common language to be developed that would express mathematical ideas in ways that can be digitally stored, linked, and made accessible. The [[Global Digital Mathematics Library]] is a project to define and develop such a language.

===Convert print resources===
The process of converting printed resources into digital collections has been epitomized to some degree by librarians and related specialists. For example,
The [[Digital Curation Centre]] is claimed to be a "world leading centre of expertise in digital information curation"&lt;ref name="Digital Curation Centre"&gt;{{cite web|last=Digital Curation Centre|title=About the DCC|url=http://www.dcc.ac.uk/about-us|work=Website|publisher=Digital Curation Centre|accessdate=6 March 2013}}&lt;/ref&gt; that assists higher education research institutions in such conversions. The DCC, based in the UK, began operations in early 2004 and suggests the following as a general outline of their approach to digital curation:

* Conceptualize: Consider what digital material you will be creating and develop storage options. Take into account websites, publications, email, among other types of digital output.
* Create: Produce digital material and attach all relevant metadata, typically the more metadata the more accessible the information.
* Access and use: Determine the level of accessibility for the range of digital material created. Some material may be accessible only by password and other material may be freely accessible to the public.
* Appraise and select: Consult the mission statement of the institution or private collection and determine what digital data is relevant. There may also be legal guidelines in place that will guide the decision process for a particular collection.
* Dispose: Discard any digital material that is not deemed necessary to the institution.
* Ingest: Send digital material to the predetermined storage solution. This may be an archive, repository or other facility.
* Preservation action: Employ measures to maintain the integrity of the digital material.
* Reappraise: Reevaluate material to ensure that is it still relevant and is true to its original form.
* Store: Secure data within the predetermined storage facility.
* Access and reuse: Routinely check that material is still accessible for the intended audience and that the material has not been compromised through multiple uses.
* Transform: If desirable or necessary the material may be transferred into a different digital format.

==="Sheer curation"===
''Sheer curation'' is an approach to digital curation where curation activities are quietly integrated into the normal work flow of those creating and managing data and other digital assets. The word sheer is used to emphasize the lightweight and virtually transparent nature of these curation activities. The term ''sheer curation'' was coined by Alistair Miles in the ImageStore project,&lt;ref&gt;[http://imageweb.zoo.ox.ac.uk/wiki/index.php/The_ImageStore_Project The ImageStore Project - ImageWeb&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; and the UK Digital Curation Centre's SCARP project.&lt;ref&gt;[http://www.dcc.ac.uk/scarp/ Digital Curation Centre: DCC SCARP Project&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; The approach depends on curators having close contact or 'immersion' in data creators' working practices. An example is the case study of a neuroimaging research group by Whyte et al., which explored ways of building its digital curation capacity around the apprenticeship style of learning of neuroimaging researchers, through which they share access to datasets and re-use experimental procedures.&lt;ref&gt;Whyte, A., Job, D., Giles, S. and Lawrie, S. (2008) '[http://www.ijdc.net/index.php/ijdc/article/view/74/53 Meeting Curation Challenges in a Neuroimaging Group]', The International Journal of Digital Curation Issue 1, Volume 3, 2008&lt;/ref&gt;      

Sheer curation depends on the hypothesis that good data and digital asset management at the point of creation and primary use is also good practice in preparation for sharing, publication and/or [[long-term preservation]] of these assets. Therefore, sheer curation attempts to identify and promote tools and good practices in local data and digital asset management in specific domains, where those tools and practices add immediate value to the creators and primary users of those assets. Curation can best be supported by identifying existing practices of sharing, stewardship and re-use that add value, and augmenting them in ways that both have short-term benefits, and in the longer term reduce risks to digital assets or provide new opportunities to sustain their long-term accessibility and re-use value.    

The aim of sheer curation is to establish a solid foundation for other curation activities which may not directly benefit the creators and primary users of digital assets, especially those required to ensure long-term preservation. By providing this foundation, further curation activities may be carried out by specialists at appropriate institutional and organisation levels, whilst causing the minimum of interference to others.

A similar idea is ''curation at source'' used in the context of Laboratory Information Management Systems [[Laboratory information management system|LIMS]]. This refers more specifically to automatic recording of [[metadata]] or information about data at the point of capture, and has been developed to apply semantic web techniques to integrate laboratory instrumentation and documentation systems.&lt;ref&gt;Frey, J. [http://www.allhands.org.uk/2008/programme/jeremyfrey.cfm 'Sharing and Collaboration' keynote presentation at UK e-Science All Hands Meeting], 8–11 September 2008, Edinburgh&lt;/ref&gt; Sheer curation and curation-at-source can be contrasted with post hoc [[digital preservation]], where a project is initiated to preserve a collection of digital assets that have already been created and are beyond the period of their primary use.

===Channelisation===
''Channelisation'' is curation of digital assets on the web, often by brands and media companies, into continuous flows of content, turning the user experience from a lean-forward interactive medium, to a lean-back passive medium. The curation of content can be done by an independent third party, that selects media from any number of on-demand outlets from across the globe and adds them to a playlist to offer a digital "channel" dedicated to certain subjects, themes, or interests so that the end user would see and/or hear a continuous stream of content.

==Challenges==
* Storage format evolution and obsolescence&lt;ref name=ijdcpw200711&gt;{{cite web|title=Digital Preservation Theory and Application: Transcontinental Persistent Archives Testbed Activity |publisher=The International Journal of Digital Curation |url=http://www.ijdc.net/./ijdc/article/view/43/50 |author=Paul Watry |date=November 2007 |accessdate=2008-04-01 |deadurl=yes |archiveurl=https://web.archive.org/web/20080315030030/http://www.ijdc.net:80/ijdc/article/view/43/50 |archivedate=2008-03-15 |df= }}&lt;/ref&gt; 
* Rate of creation of new data and data sets
* Maintaining accessibility to data through links and search results
* Comparability of [[semantic]] and [[ontology|ontological]] definitions of data sets&lt;ref name="ijdcpw200711"/&gt;

===Responses===
* Specialized research institutions&lt;ref&gt;[http://www.dcc.ac.uk/ Digital Curation Centre]&lt;/ref&gt;&lt;ref&gt;[http://www.dpconline.org/ Digital Preservation Coalition]&lt;/ref&gt;
* Academic courses
* Dedicated symposia&lt;ref&gt;[http://www.ils.unc.edu/digccurr2007/ DigCCurr 2007 - an international symposium on Digital Curation, April 18-20, 2007]&lt;/ref&gt;&lt;ref&gt;[http://stardata.nrf.ac.za/nadicc 1st African Digital Management and Curation Conference and Workshop - Date: 12-13 February 2008]&lt;/ref&gt;
* Peer reviewed technical and industry journals&lt;ref&gt;[http://www.ijdc.net/ International Journal of Digital Curation]&lt;/ref&gt;

==See also==
* [[Biocurator|Biocuration]]
* [[Curator]]
* [[Data curation]]
* [[Data format management]]
* [[Digital artifactual value]]
* [[Digital asset management]]
* [[Digital obsolescence]]
* [[International Society for Biocuration]]

==References==
{{reflist|33em}}

==External links==
*[https://www.youtube.com/watch?v=pbBa6Oam7-w Animations introducing digital preservation and curation]
*[http://www.alistapart.com/articles/content-strategist-as-digital-curator/ Content Strategist as Digital Curator], A List Apart Journal, December 2009
*[http://www.dcc.ac.uk/ Digital Curation Centre]
*[http://journals.tdl.org/jodi/article/view/229/183 Digital Curation and Trusted Repositories: Steps Toward Success]*[http://www.digcur-education.org DigCurV] A project funded by the European Commission to establish a curriculum framework for vocational training in digital curation.

[[Category:Archival science]]
[[Category:Databases]]
[[Category:Knowledge representation]]
[[Category:Digital libraries]]
[[Category:Digital preservation]]</text>
      <sha1>iht2tsgyvr5lzqmabyzwncvh8gy27j3</sha1>
    </revision>
  </page>
  <page>
    <title>Ronald J. Brachman</title>
    <ns>0</ns>
    <id>853832</id>
    <revision>
      <id>760061969</id>
      <parentid>744809531</parentid>
      <timestamp>2017-01-14T19:15:17Z</timestamp>
      <contributor>
        <ip>96.248.115.152</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4640" xml:space="preserve">{{Infobox scientist
| name              = Ronald Jay Brachman
| image             = &lt;!--(filename only)--&gt;
| image_size        = 
| alt               = 
| caption           = 
| birth_date        = {{Birth year and age|1949}} 
| birth_place       = 
| death_date        = &lt;!-- {{Death date and age|YYYY|MM|DD|YYYY|MM|DD}} (death date then birth date) --&gt;
| death_place       = 
| resting_place             = 
| resting_place_coordinates = &lt;!-- {{Coord|LAT|LONG|type:landmark|display=inline,title}} --&gt;
| residence         = 
| citizenship       = 
| nationality       = 
| fields            = 
| workplaces        = [[Harvard University]]&lt;br&gt;[[Yahoo! Research]]&lt;br&gt;[[AT&amp;T Corporation]]&lt;br&gt;[[DARPA]]
| alma_mater        = [[Harvard University]]&lt;br&gt;[[Princeton University]]
| thesis_title      = A structural paradigm for representing knowledge
| thesis_url        = https://books.google.com/books?id=ThS-HAAACAAJ
| thesis_year       = 1977
| doctoral_advisor  = William A. Woods
| academic_advisors = 
| doctoral_students = 
| notable_students  = 
| known_for         = 
| author_abbrev_bot = 
| author_abbrev_zoo = 
| influences        = 
| influenced        = 
| awards            = 
| signature         = &lt;!--(filename only)--&gt;
| signature_alt     = 
| website           = {{URL|www.brachman.org}}&lt;br&gt;{{URL|research.yahoo.com/Ron_Brachman}}
| footnotes         = 
| spouse            = 
}}'''Ronald Jay "Ron" Brachman''' (born 1949) is the director of the Jacobs Technion-Cornell Institute at [[Cornell Tech]].&lt;ref&gt;{{Cite web|url=http://tech.cornell.edu/news/ron-brachman-joins-the-jacobs-technion-cornell-institute-at-cornell-tech-as|title=Ron Brachman Joins the Jacobs Technion-Cornell Institute at Cornell Tech as the New Director|website=Cornell Tech|access-date=2016-05-25}}&lt;/ref&gt; Previously, he was the Chief Scientist of Yahoo! and head of [[Yahoo! Labs]].  Prior to that, he was the Associate Head of Yahoo! Labs and Head of Worldwide Labs and Research Operations.

==Education==
Brachman earned his [[Bachelor of Engineering|B.S.E.E.]] degree from [[Princeton University]], and his [[Master of Science|S.M.]] and [[Doctor of Philosophy|Ph.D.]] degrees from [[Harvard University]].

==Career==
Prior to working at Yahoo!, Brachman worked at [[DARPA]] as the Director of the [[Information Processing Techniques Office]] (IPTO), one of DARPA's eight offices at the time.  While at IPTO, he helped develop [[DARPA]]'s Cognitive Systems research efforts. Before that, he worked at [[AT&amp;T Corporation|AT&amp;T]] [[Bell Labs|Bell Laboratories]] ([[Murray Hill, New Jersey]]) as the Head of the [[Artificial Intelligence]] Principles Research Department (2004) and Director of the Software and Systems Research Laboratory.  When AT&amp;T split with Lucent in 1996, he became Communications Services Research Vice President and was one of the founders of [[AT&amp;T Labs]].

He is considered by some to be the godfather{{citation needed|date=August 2012}} of [[Description Logic]], the logic-based [[knowledge representation]] [[Semantics (computer science)|formalism]] underlying the [[Web Ontology Language]] OWL.]

==Publications==
He is the co-author with [[Hector Levesque]] of a popular book on [[knowledge representation and reasoning]]&lt;ref&gt;{{cite book |author1=Reiter, Ray |author2=Brachman, Ronald J. |author3=Levesque, Hector J. |title=Knowledge representation |publisher=MIT Press |location=Cambridge, Mass |year=1992 |pages= |isbn=0-262-52168-7 |oclc= |doi= |accessdate=}}&lt;/ref&gt;&lt;ref&gt;{{cite book |author1=Levesque, Hector J. |author2=Brachman, Ronald J. |title=Knowledge representation and reasoning |publisher=Elsevier/Morgan Kaufmann |location=Amsterdam |year=2004 |pages= |isbn=1-55860-932-6 |oclc= |doi= |accessdate=}}&lt;/ref&gt; and many scientific papers.&lt;ref name="microsoft"&gt;{{AcademicSearch|9029466}}&lt;/ref&gt;&lt;ref name="dblp"&gt;{{DBLP|name=Ronald J. Brachman}}&lt;/ref&gt;&lt;ref&gt;Ronald J. Brachman (1983) "What IS-A is and isn't. An Analysis of Taxonomic Links in [[Semantic network|Semantic Networks]]"; ''IEEE Computer'', 16 (10); October.&lt;/ref&gt;

==References==
{{reflist}}

== External links ==
* [http://www.cc.gatech.edu/events/dr-ronald-brachman-yahoo-research-distinguished-guest-lecture External biography]

{{DEFAULTSORT:Brachman, Ronald J.}}
[[Category:Living people]]
[[Category:Artificial intelligence researchers]]
[[Category:Knowledge representation]]
[[Category:Harvard University alumni]]
[[Category:Princeton University alumni]]
[[Category:Fellow Members of the IEEE]]
[[Category:Fellows of the Association for the Advancement of Artificial Intelligence]]
[[Category:Yahoo! employees]]
[[Category:1959 births]]


{{compu-bio-stub}}</text>
      <sha1>e6ik8u1v2uwu55qxpbwdfw6ehqmkwop</sha1>
    </revision>
  </page>
  <page>
    <title>BCM Classification</title>
    <ns>0</ns>
    <id>17952329</id>
    <revision>
      <id>745787482</id>
      <parentid>742924097</parentid>
      <timestamp>2016-10-23T08:49:12Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 1 sources and tagging 0 as dead. #IABot (v1.2.5)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3855" xml:space="preserve">The '''British Catalogue of Music Classification''' (BCM Classification)&lt;ref&gt;The British Catalogue of Music Classification / compiled for the Council of the British National Bibliography Ltd. by E. J. Coates, F.L.A.  London : Council of the British National Bibliography, 1960&lt;/ref&gt; is a [[faceted classification]] that was commissioned from E. J. Coates by the Council of the British National Bibliography to organize the content of the British Catalogue of Music.&lt;ref&gt;{{cite web|url=https://archive.org/details/britishcatalogue001781mbp |title=Internet Archive: Details: The British Catalogue Of Music 1960 |publisher=Archive.org |date= |accessdate=2016-10-06}}&lt;/ref&gt; The published schedule (1960) was considerably expanded by Patrick Mills of the British Library up until its use was abandoned in 1998. Entries in the catalogue were organized by BCM classmark from the catalogue's inception in 1957 until 1982. From that year the British Catalogue of Music (which from 1974 onward was published by [[The British Library]]) was organized instead by [[Dewey Decimal Classification]] number, though BCM classmarks continued to be added to entries up to the 1998 annual cumulation.

The schedule is divided into two main parts: A-B representing Musical literature and C-Z representing Music — Scores and Parts.  There are also seven auxiliary tables dealing with various sub-arrangements, sets of ethnic/locality subdivisions and chronological reference points.

The notation is retroactive using uppercase alphabetic characters omitting I and O, with the addition of slash / and parentheses ( ) which have specific anteriorizing functions.  Retroactive notation requires that the classifier combines terms in reverse schedule order. This has the benefit of producing a compact notation by removing the need for facet indicators.

The schedule at A (Music Literature) parallels that from the Scores and Parts schedules thus Choral Music is at D while books about Choral Music are at AD; Harp Music is at TQ so books on harp music are at ATQ.  The schedule at B accommodates books about specific composers and music in non-European traditions.

As a fully faceted scheme after the ideas of [[S. R. Ranganathan]], BCM class numbers are capable of being chain-indexed, allowing index access to each step of the hierarchy.

BCM classification had a strong influence on Russell Sweeney's so-called Phoenix Dewey 780 schedule&lt;ref&gt;DDC Dewey Decimal Classification : proposed revision of 780 music / prepared under the direction of Russell Sweeney and John Clews with assistance from Winton E. Mathews, Jr. Albany N.Y. : Forest Press, 1980. ISBN 0-910608-25-3&lt;/ref&gt; which in turn influenced the 780 Music schedule in the 20th edition of [[Dewey Decimal Classification]].  The music schedule of the second edition of the Bliss Classification&lt;ref&gt;{{cite web|url=http://library.music.indiana.edu/tech_s/mla/facacc.rev |title=Archived copy |accessdate=2008-06-15 |deadurl=yes |archiveurl=https://web.archive.org/web/20080512000152/http://library.music.indiana.edu:80/tech_s/mla/facacc.rev |archivedate=2008-05-12 |df= }}&lt;/ref&gt; is also strongly influenced by BCM.

This classification system is still in use at a number of libraries, including the [[State Library of Western Australia]]&lt;ref&gt;{{cite web|url=http://www.slwa.wa.gov.au/find/guides/music/general_information/british_catalogue_of_music_classification_scheme |title=(accessed 2015-12-17) |publisher=Slwa.wa.gov.au |date=2013-08-20 |accessdate=2016-10-06}}&lt;/ref&gt; and the Library at [[Edith Cowan University]].&lt;ref&gt;{{cite web|url=http://ecu.au.libguides.com/c.php?g=410622&amp;p=2797056 |title=(accessed 2015-12-17) |publisher=Ecu.au.libguides.com |date=2016-08-05 |accessdate=2016-10-06}}&lt;/ref&gt;

==References==
{{Reflist}}

[[Category:Library cataloging and classification]]
[[Category:Knowledge representation]]</text>
      <sha1>thhham0uws0efjceg0xja3z3fs7yi03</sha1>
    </revision>
  </page>
  <page>
    <title>Semantic knowledge management</title>
    <ns>0</ns>
    <id>20298912</id>
    <revision>
      <id>723876921</id>
      <parentid>721184068</parentid>
      <timestamp>2016-06-05T20:53:16Z</timestamp>
      <contributor>
        <username>Dcirovic</username>
        <id>11795905</id>
      </contributor>
      <minor />
      <comment>/* References */refs using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1429" xml:space="preserve">{{Orphan|date=February 2009}}
'''Semantic knowledge management''' is a set of practices that seeks to classify content so that the knowledge it contains may be immediately accessed and transformed for delivery to the desired audience, in the required format. This classification of content is semantic in its nature &amp;ndash; identifying content by its type or meaning within the content itself and via external, descriptive metadata – and is achieved by employing [[XML]] technologies.

The specific outcomes of these practices are:

* Maintain content for multiple audiences together in a single document 
* Transform content into various delivery formats without re-authoring  
* Search for content more effectively 
* Involve more [[subject-matter expert]]s in the creation of content without reducing quality 
* Reduce production costs for delivery formats 
* Reduce the manual administration of getting the right knowledge to the right people 
* Reduce the cost and time to localize content

==References==
{{refbegin}}
* {{cite book|title=Semantic Knowledge Management: Integrating Ontology Management, Knowledge Discovery, and Human Language Technologies|author1=John Davies |author2=Marko Grobelnik |author3=Dunja Mladenic |isbn=3-540-89164-1|year=2008}}
{{refend}}

== Notable semantic knowledge management systems ==
*Learn eXact
*Thinking Cap LCMS
*Thinking Cap LMS
*Xyleme LCMS

[[Category:Knowledge representation]]</text>
      <sha1>iaq3799265yhaugi453hvn3lj48dep4</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Knowledge representation software</title>
    <ns>14</ns>
    <id>20966976</id>
    <revision>
      <id>547524181</id>
      <parentid>438532343</parentid>
      <timestamp>2013-03-28T23:53:35Z</timestamp>
      <contributor>
        <username>KLBot2</username>
        <id>14393296</id>
      </contributor>
      <minor />
      <comment>Bot: Migrating 1 interwiki links, now provided by [[Wikidata]] on [[:d:Q8575278]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="71" xml:space="preserve">[[Category:Knowledge representation]]
[[Category:Application software]]</text>
      <sha1>k88o449d8lbigckqvooogblmot8m1fd</sha1>
    </revision>
  </page>
  <page>
    <title>User modeling</title>
    <ns>0</ns>
    <id>12781902</id>
    <revision>
      <id>753810701</id>
      <parentid>723169565</parentid>
      <timestamp>2016-12-09T09:20:08Z</timestamp>
      <contributor>
        <username>Kku</username>
        <id>5846</id>
      </contributor>
      <minor />
      <comment>link [[adaptive system]] using [[:en:User:Edward/Find link|Find link]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="12978" xml:space="preserve">'''User modeling''' is the subdivision of [[human–computer interaction]] which describes the
process of building up and modifying a conceptual understanding of the user. The main goal of user modeling is customization and [[Adaptation (computer science)|adaptation of systems]] to the user's specific needs. The system needs to "say the 'right' thing at the 'right' time in the 'right' way".&lt;ref name=Fischer&gt;{{Citation
  | last1 = Fischer | first1 = Gerhard
  | title = User Modeling in Human-Computer Interaction
  | journal = User Modeling and User-Adapted Interaction 11
  | pages = 65–68
  | year = 2001 }}&lt;/ref&gt; To do so it needs an internal representation of the user. Another common purpose is modeling specific kinds of users, including modeling of their skills and declarative knowledge, for use in automatic software-tests.&lt;ref name=JohnsonTaatgen&gt; {{Citation
  | last1 = Johnson | first1 = Addie
  | last2=Taatgen | first2 = Niels
  | chapter = User Modeling
  | title = Handbook of human factors in Web design
  | pages = 424–439
  | publisher = Lawrence Erlbaum Associates
  | year = 2005 }}&lt;/ref&gt; User-models can thus serve as a cheaper alternative to [[user testing]].

== Background ==

A user model is the collection and categorization of personal data associated with a specific user. Therefore, it is the basis for any adaptive changes to the system's behavior. Which data is included in the model depends on the purpose of the application. It can include personal information such as users' names and ages, their interests, their skills and knowledge, their goals and plans, their preferences and their dislikes or data about their behavior and their interactions with the system.

There are different design patterns for user models, though often a mixture of them is used.&lt;ref name=JohnsonTaatgen /&gt;&lt;ref&gt;{{Citation
  | last1 = Hothi | first1 = Jatinder
  | last2=Hall | first2 = Wendy
  | title = An Evaluation of Adapted Hypermedia Techniques Using Static User Modelling
  | journal = Proceedings of the 2nd Workshop on Adaptive Hypertext and Hypermedia
  | place = Southampton University, Electronics and Computer Science University Road, Southampton, Hampshire, UK
  | year = June 1998
  | url = http://wwwis.win.tue.nl/ah98/Hothi/Hothi.html }}&lt;/ref&gt;
* '''Static user models'''
:Static user models are the most basic kinds of user models. Once the main data is gathered they are normally not changed again, they are static. Shifts in users' preferences are not registered and no learning algorithms are used to alter the model.
* '''Dynamic user models'''
:Dynamic user models allow a more up to date representation of users. Changes in their interests, their learning progress or interactions with the system are noticed and influence the user models. The models can thus be updated and take the current needs and goals of the users into account.
* '''Stereotype based user models '''
:Stereotype based user models are based on [[Demographics|demographic statistics]]. Based on the gathered information users are [[Classification_in_machine_learning|classified]] into common stereotypes. The system then adapts to this stereotype. The application therefore can make assumptions about a user even though there might be no data about that specific area, because demographic studies have shown that other users in this stereotype have the same characteristics. Thus, stereotype based user models mainly rely on statistics and do not take into account that personal attributes might not match the stereotype. However, they allow predictions about a user even if there is rather little information about him or her.
* '''Highly adaptive user models'''
:Highly adaptive user models try to represent one particular user and therefore allow a very high adaptivity of the system. In contrast to stereotype based user models they do not rely on demographic statistics but aim to find a specific solution for each user. Although users can take great benefit from this high adaptivity, this kind of model needs to gather a lot of information first.

== Data gathering ==

Information about users can be gathered in several ways. There are three main methods:

* '''Asking for specific facts while (first) interacting with the system'''&lt;ref name=JohnsonTaatgen /&gt;
:Mostly this kind of data gathering is linked with the registration process. While registering users are asked for specific facts, their likes and dislikes and their needs. Often the given answers can be altered afterwards.
* '''Learning users' preferences by observing and interpreting their interactions with the system'''&lt;ref name=JohnsonTaatgen /&gt;
:In this case users are not asked directly for their personal data and preferences, but this information is derived from their behavior while interacting with the system. The ways they choose to accomplish a tasks, the combination of things they takes interest in, these observations allow inferences about a specific user. The application dynamically learns from observing these interactions. Different [[machine learning]] algorithms may be used to accomplish this task.
* '''A hybrid approach which asks for explicit feedback and alters the user model by adaptive learning'''&lt;ref name=Montaner&gt;{{Citation
  | last = Montaner | first = Miguel
  | last2 = López | first2 = Beatriz
  | last3 = De La Rosa | first3 = Josep Lluís
  | title = A Taxonomy of Recommender Agents on the Internet,
  | journal = Artif. Intell. Rev.
  | volume = 19
  | pages = 285–330
  | year = 2003 }}&lt;/ref&gt;
:This approach is a mixture of the ones above. Users have to answer specific questions and give explicit feedback. Furthermore, their interactions with the system are observed and the derived information are used to automatically adjust the user models.

Though the first method is a good way to quickly collect main data it lacks the ability to automatically adapt to shifts in users' interests. It depends on the users' readiness to give information and it is unlikely that they are going to edit their answers once the registration process is finished. Therefore, there is a high likelihood that the user models are not up to date. However, this first method allows the users to have full control over the collected data about them. It is in their decision which information they are willing to provide. This possibility is missing in the second method. Adaptive changes in a system that learns users' preferences and needs only by interpreting their behavior might appear a bit opaque to the users, because they cannot fully understand and reconstruct why the system behaves the way it does.&lt;ref name=Montaner /&gt; Moreover, the system is forced to collect a certain amount of data before it is able to predict the users' needs with the required accuracy. Therefore, it takes a certain learning time before a user can benefit from adaptive changes. However, afterwards these automatically adjusted user models allow a quite accurate adaptivity of the system. The hybrid approach tries to combine the advantages of both methods. Through collecting data by directly asking its users it gathers a first stock of information which can be used for adaptive changes. By learning from the users' interactions it can adjust the user models and reach more accuracy. Yet, the designer of the system has to decide, which of these information should have which amount of influence and what to do with learned data that contradicts some of the information given by a user.

== System adaptation ==

Once a system has gathered information about a user it can evaluate that data by preset analytical algorithm and then start to adapt to the user's needs. These adaptations may concern every aspect of the system's behavior and depend on the system's purpose. Information and functions can be presented according to the user's interests, knowledge or goals by displaying only relevant features, hiding information the user does not need, making proposals what to do next and so on. One has to distinguish between [[Adaptation (computer science)#Adaptivity_and_adaptability|adaptive and adaptable systems]].&lt;ref name=Fischer /&gt; In an adaptable system the user can manually change the system's appearance, behavior or functionality by actively selecting the corresponding options. Afterwards the system will stick to these choices. In an [[adaptive system]] a dynamic adaption to the user is automatically performed by the system itself, based on the built user model. Thus, an adaptive system needs ways to interpret information about the user in order to make these adaptations. One way to accomplish this task is implementing rule-based filtering. In this case a set of IF... THEN... rules is established that covers the [[knowledge base]] of the system.&lt;ref name=JohnsonTaatgen /&gt; The IF-conditions can check for specific user-information and if they match the THEN-branch is performed which is responsible for the adaptive changes. Another approach is based on [[collaborative filtering]].&lt;ref name=JohnsonTaatgen /&gt;&lt;ref name=Montaner /&gt; In this case information about a user is compared to that of other users of the same systems. Thus, if characteristics of the current user match those of another, the system can make assumptions about the current user by presuming that he or she is likely to have similar characteristics in areas where the model of the current user is lacking data. Based on these assumption the system then can perform adaptive changes.

== Usages ==

* [[Adaptive hypermedia]]: In an adaptive hypermedia system the displayed content and the offered hyperlinks are chosen on basis of users' specific characteristics, taking their goals, interests, knowledge and abilities into account. Thus, an adaptive hypermedia system aims to reduce the "lost in hyperspace" syndrome by presenting only relevant information.
* [[Adaptive educational hypermedia]]: Being a subdivision of adaptive hypermedia the main focus of adaptive educational hypermedia lies on education, displaying content and hyperlinks corresponding to the user's knowledge on the field of study.
* [[Intelligent tutoring system]]: Unlike adaptive educational hypermedia systems intelligent tutoring systems are stand-alone systems. Their aim is to help students in a specific field of study. To do so, they build up a user model where they store information about abilities, knowledge and needs of the user. The system can now adapt to this user by presenting appropriate exercises and examples and offering hints and help where the user is most likely to need them. 
* [[Expert systems]]: Expert systems are computer systems that emulate the decision-making ability of a human expert in order to help the user solving a problem in a specific area. Step by step they ask questions to identify the current problem and to find a solution. User models can be used to adapt to the current user's knowledge, differentiating between experts and novices. The system can assume, that experienced users are able to understand and answer more complex questions than someone who is new to the topic. Therefore, it can adjust the used vocabulary and the type of question which are presented to the user, thus reducing the steps needed to find a solution.
* [[Recommender system]]: The basic idea of recommender systems is to present a selection of items to the user which best fit his or her needs. This selection can be based on items the user has bookmarked, rated, bought, recently viewed, etc. Recommender systems are often used in [[e-commerce]] but may also cover areas like social networks, websites, news, etc.
* [[Usability testing|User-Simulation]]: Since user modeling allows the system to hold an internal representation of a specific user, different types of users can be simulated by artificially modeling them. Common types are "experts" or "novices" on the scope of the system or the usage of the system. Based on these characteristics user tests can be simulated.

== Standards==
A certain number of representation formats and standards are available for representing the users in computer systems,&lt;ref&gt;Nabeth Thierry (2005), [http://www.fidis.net/resources/fidis-deliverables/identity-of-identity/#c1753: Models], FIDIS Deliverable, October 2005. &lt;/ref&gt; such as:
* [[IMS-LIP]] (IMS &amp;ndash; Learner Information Packaging, used in [[e-learning]]) 
* [[HR-XML Standards|HR-XML]] (used in [[human resource management]])
* [[JXDM]] (Justice with the Global Justice Extensible Markup)
* [[Europass]] (the Europass online CV)

== See also ==

* [[Personalization]]
* [[Cognitive model]]
* [[User profile]]
* [[Identity management]]

== References ==
&lt;references/&gt;

== External references ==
* [http://www.umuai.org/ User Modeling and User-Adapted Interaction (UMUAI)] The Journal of Personalization Research 
* [http://www.cs.cmu.edu/~bej/cogtool/ CogTool Project at CMU]
* [http://www.iit.demokritos.gr/um2007/ UserModeling conference 2007]

[[Category:Knowledge representation]]</text>
      <sha1>7a6scjhwzvfjrjsit4m3bxz6fl8blww</sha1>
    </revision>
  </page>
  <page>
    <title>Upper ontology</title>
    <ns>0</ns>
    <id>3200382</id>
    <revision>
      <id>760467692</id>
      <parentid>758192259</parentid>
      <timestamp>2017-01-17T04:30:48Z</timestamp>
      <contributor>
        <username>RoyCullum</username>
        <id>7873718</id>
      </contributor>
      <minor />
      <comment>Changed "an" to "and"</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="43954" xml:space="preserve">{{multiple issues|
{{more footnotes|date=February 2011}}
{{essay-like|date=October 2010}}
}}

In [[information science]], an '''upper ontology''' (also known as a '''top-level ontology''' or '''foundation ontology''') is an [[Ontology (computer science)|ontology]] (in the sense used in information science) which consists of very general terms (such as "object", "property", "relation") that are common across all domains.  An important function of an upper ontology is to support broad [[semantic interoperability]] among a large number of domain-specific ontologies by providing a common starting point for the formulation of definitions. Terms in the domain ontology are ranked "under" the terms in the upper ontology, and the former stand to the latter in subclass relations.

A number of upper ontologies have been proposed, each with its own proponents. Each upper ontology can be considered as a computational implementation of [[natural philosophy]], which itself is a more empirical method for investigating the topics within the philosophical discipline of [[physical ontology]].

[[Library classification]] systems predate upper ontology systems. Though library classifications organize and categorize knowledge using general concepts that are the same across all knowledge domains, neither system is a replacement for the other.

==Development==

Any standard foundational ontology is likely to be contested among different groups, each with their own idea of "what exists". One factor exacerbating the failure to arrive at a common approach has been the lack of open-source applications that would permit the testing of different ontologies in the same computational environment.  The differences have thus been debated largely on theoretical grounds, or are merely the result of personal preferences. Foundational ontologies can however be compared on the basis of adoption for the purposes of supporting interoperability across domain ontologies.

No particular upper ontology has yet gained widespread acceptance as a [[de facto]] standard.  Different organizations have attempted to [[#Available ontologies|define standards]] for specific domains.  The '[[Process Specification Language]]' (PSL) created by the [[National Institute for Standards and Technology]] (NIST) is one example.

Another important factor leading to the absence of wide adoption of any existing upper ontology is the complexity. Some upper ontologies -- [[Cyc]] is often cited as an example in this regard -- are very large, ranging up to thousands of elements (classes, relations), with complex interactions among them and with a complexity similar to that of a human [[natural language]], and the learning process can be even longer than for a natural language because of the unfamiliar format and logical rules.  The motivation to overcome this learning barrier is largely absent because of the paucity of publicly accessible examples of use.  As a result, those building domain ontologies for local applications tend to create the simplest possible domain-specific ontology, not related to any upper ontology.  Such domain ontologies may function adequately for the local purpose, but they are very time-consuming to relate accurately to other domain ontologies. 

To solve this problem some genuinely top level ontologies have been developed, which are deliberately designed to have minimal overlap with any domain ontologies. Examples are [[Basic Formal Ontology]] and the [[Domain Ontology for Linguistic and Cognitive Engineering | DOLCE]] (see below).

===Arguments for the infeasibility of an upper ontology===
{{unreferenced section|date=December 2016}}
Historically, many attempts in many societies{{which?|date=December 2016}} have been made to impose or define a single set of concepts as more primal, basic, foundational, authoritative, true or rational than all others. A common objection{{By whom?|date=December 2016}} to such attempts points out that humans lack the sort of  transcendent perspective - or ''[[God's eye view]]'' - that would be required to achieve this goal. Humans are bound by language or culture, and so lack the sort of objective perspective from which to observe the whole terrain of concepts and derive any one standard.

Another objection is the problem of formulating definitions. Top level ontologies are designed to maximize support for interoperability across a large number of terms. Such ontologies must therefore consist of terms expressing very general concepts, but such concepts are so basic to our understanding that there is no way in which they can be defined, since the very process of definition implies that a less basic (and less well understood) concept is defined in terms of concepts that are more basic and so (ideally) more well understood. Very general concepts can often only be elucidated, for example by means of examples, or paraphrase. 

* There is no self-evident way of dividing the world up into [[concept]]s, and certainly no non-controversial one
* There is no neutral ground that can serve as a means of translating between specialized (or "lower" or "application-specific") ontologies
* Human [[language]] itself is already an arbitrary approximation of just one among many possible conceptual maps.  To draw any ''necessary correlation'' between [[English language|English]] words and any number of intellectual concepts we might like to represent in our ontologies is just asking for trouble. ([[WordNet]], for instance, is successful and useful precisely because it does not pretend to be a general-purpose upper ontology; rather, it is a tool for semantic / syntactic / linguistic disambiguation, which is richly embedded in the particulars and peculiarities of the English language.)
* Any hierarchical or topological representation of concepts must begin from some ontological, [[epistemology|epistemological]], linguistic, cultural, and ultimately pragmatic perspective.  Such pragmatism does not allow for the exclusion of politics between persons or groups, indeed it requires they be considered as perhaps more basic primitives than any that are represented.

Those{{who?|date=December 2016}} who doubt the feasibility of general purpose ontologies are more inclined to ask “what specific purpose do we have in mind for this conceptual map of entities and what practical difference will this ontology make?”  This pragmatic philosophical position surrenders all hope of devising the encoded ontology version of “everything that is the case,” ([[Wittgenstein]], [[Tractatus Logico-Philosophicus]]).

Finally there are objections similar to those against [[artificial intelligence]]{{From whom?|date=December 2016}}.  Technically, the complex concept acquisition and the social / linguistic interactions of human beings suggests any axiomatic foundation of "most basic" concepts must be cognitive, biological or otherwise difficult to characterize since we don't have axioms for such systems.  Ethically, any general-purpose ontology could quickly become an actual tyranny by recruiting adherents into a political program designed to propagate it and its funding means, and possibly defend it by violence.  Historically, inconsistent and irrational belief systems have proven capable of commanding obedience to the detriment or harm of persons both inside and outside a society that accepts them.  How much more harmful would a consistent rational one be, were it to contain even one or two basic assumptions incompatible with human life?

===Arguments for the feasibility of an upper ontology===
{{unreferenced section|date=November 2014}}
Many of those who doubt the possibility of developing wide agreement on a common upper ontology fall into one of two traps:
# they assert that there is no possibility of universal agreement on any conceptual scheme; but they argue that a practical common ontology does not need to have universal agreement, it only needs a large enough user community (as is the case for human languages) to make it profitable for developers to use it as a means to general interoperability, and for third-party developer to develop utilities to make it easier to use; and
# they point out that developers of data schemes find different representations congenial for their local purposes; but they do not demonstrate that these different representation are in fact logically inconsistent.

In fact, different representations of assertions about the real world (though not philosophical models), if they accurately reflect the world, must be logically consistent, even if they focus on different aspects of the same physical object or phenomenon.  If any two assertions about the real world are logically inconsistent, one or both must be wrong, and that is a topic for experimental investigation, not for ontological representation.  In practice, representations of the real world are created as and known to be approximations to the basic reality, and their use is circumscribed by the limits of error of measurements in any given practical application.  Ontologies are entirely capable of representing approximations, and are also capable of representing situations in which different approximations have different utility.  Objections based on the different ways people perceive things attack a simplistic, impoverished view of ontology.  The objection that there are logically incompatible models of the world are true, but in an upper ontology those different models can be represented as different theories, and the adherents of those theories can use them in preference to other theories, while preserving the logical consistency of the ''necessary'' assumptions of the upper ontology.  The ''necessary'' assumptions provide the logical vocabulary with which to specify the meanings of all of the incompatible models.  It has never been demonstrated that incompatible models cannot be properly specified with a common, more basic set of concepts, while there are examples of incompatible theories that can be logically specified with only a few basic concepts.

Many of the objections to upper ontology refer to the problems of life-critical decisions or non-axiomatized  problem areas such as law or medicine or politics that are difficult even for humans to understand.  Some of these objections do not apply to physical objects or standard abstractions that are defined into existence by human beings and closely controlled by them for mutual good, such as standards for electrical power system connections or the signals used in traffic lights.  No single general [[metaphysics]] is required to agree that some such standards are desirable.  For instance, while time and space can be represented many ways, some of these are already used in interoperable artifacts like maps or schedules.

Objections to the feasibility of a common upper ontology also do not take into account the possibility of forging agreement on an ontology that contains all of the ''primitive'' ontology elements that can be combined to create any number of more specialized concept representations.  Adopting this tactic permits effort to be focused on agreement only on a limited number of ontology elements. By agreeing on the meanings of that inventory of basic concepts, it becomes possible to create and then accurately and automatically interpret an infinite number of concept representations as combinations of the basic ontology elements.  Any domain ontology or database that uses the elements of such an upper ontology to specify the meanings of its terms will be automatically and accurately interoperable with other ontologies that use the upper ontology, even though they may each separately define a large number of domain elements not defined in other ontologies.  In such a case, proper interpretation will require that the logical descriptions of domain-specific elements be transmitted along with any data that is communicated; the data will then be automatically interpretable because the domain element descriptions, based on the upper ontology, will be properly interpretable by any system that can properly use the upper ontology.  In effect elements in different domain ontologies can be *translated* into each other using the common upper ontology. An upper ontology based on such a set of primitive elements can include alternative views, provided that they are logically compatible.  Logically incompatible models can be represented as alternative theories, or represented in a specialized extension to the upper ontology.  The proper use of alternative theories is a piece of knowledge that can itself be represented in an ontology.  Users that develop new domain ontologies and find that there are semantic primitives needed for their domain but missing from the existing common upper ontology can add those new primitives by the accepted procedure, expanding the common upper ontology as necessary.

Most proponents{{Who|date=April 2015}} of an upper ontology argue that several good ones may be created with perhaps different emphasis.  Very few are actually arguing to discover just one within natural language or even an academic field.  Most are simply standardizing some existing communication.  Another view advanced is that there is almost total overlap of the different ways that upper ontologies have been formalized, in the sense that different ontologies focus on a different aspect of the same entities, but the different views are complementary and not contradictory to each other; as a result, an internally consistent ontology that contains all the views, with means of translating the different views into the other, is feasible.  Such an ontology has not thus far been constructed, however, because it would require a large project to develop so as to include all of the alternative views in the separately developed upper ontologies, along with their translations.  The main barrier to construction of such an ontology is not the technical issues, but the reluctance of funding agencies to provide the funds for a large enough consortium of developers and users.

Several common arguments against upper ontology can be examined more clearly by separating issues of concept definition (ontology), language (lexicons), and facts (knowledge).  For instance, people have different terms and phrases for the same concept.  However, that does not necessarily mean that those people are referring to different concepts.  They may simply be using different language or idiom.  Formal ontologies typically use linguistic labels to refer to concepts, but the terms that label ontology elements mean no more and no less than what their axioms say they mean.  Labels are similar to variable names in software, evocative rather than definitive.  The proponents of a common upper ontology point out that the meanings of the elements (classes, relations, rules) in an ontology depend only on their [[logical form]], and not on the labels, which are usually chosen merely to make the ontologies more easily usable by their human developers.  In fact, the labels for elements in an ontology need not be words - they could be, for example, images of instances of a particular type, or videos of an action that is represented by a particular type.  It cannot be emphasized too strongly that words are *not* what are represented in an ontology, but entities in the real world, or abstract entities (concepts) in the minds of people.  Words are not equivalent to ontology elements, but words *label* ontology elements.  There can be many words that label a single concept, even in a single language (synonymy), and there can be many concepts labeled by a single word (ambiguity).  Creating the mappings between human language and the elements of an ontology is the province of Natural Language Understanding.  But the ontology itself stands independently as a logical and computational structure.  For this reason, finding agreement on the structure of an ontology is actually easier than developing a controlled vocabulary, because all different interpretations of a word can be included, each *mapped* to the same word in the different terminologies.

A second argument is that people believe different things, and therefore can't have the same ontology.  However, people can assign different truth values to a particular assertion while accepting the validity of certain underlying claims, facts, or way of expressing an argument with which they disagree. (Using, for instance, the issue/position/argument form.) This objection to upper ontologies ignores the fact that a single ontology can represent different belief systems, representing them as different belief systems, without taking a position on the validity of either.

Even arguments about the existence of a thing require a certain sharing of a concept, even though its existence in the real world may be disputed.  Separating belief from naming and definition also helps to clarify this issue, and show how concepts can be held in common, even in the face of differing belief.  For instance, [[wiki]] as a medium may permit such confusion but disciplined users can apply [[dispute resolution]] methods to sort out their conflicts.  It is also argued that most people share a common set of "semantic primitives", fundamental concepts, to which they refer when they are trying to explain unfamiliar terms to other people.  An ontology that includes representations of those semantic primitives could in such a case be used to create logical descriptions of any term that a person may wish to define logically.   That ontology would be one form of upper ontology, serving as a logical "interlingua" that can translate ideas in one terminology to its [[logical equivalence|logical equivalent]] in another terminology.

Advocates{{Who|date=April 2015}} argue that most disagreement about the viability of an upper ontology can be traced to the conflation of ontology, language and knowledge, or too-specialized areas of knowledge: many people, or agents or groups will have areas of their respective internal ontologies that do not overlap.  If they can cooperate and share a conceptual map at all, this may be so very useful that it outweighs any disadvantages that accrue from sharing.  To the degree it becomes harder to share concepts the deeper one probes, the more valuable such sharing tends to get.  If the problem is as basic as opponents of upper ontologies claim, then, it also applies to a group of humans trying to cooperate, who might need machine assistance to communicate easily.

If nothing else, such ontologies are implied by [[machine translation]], used when people cannot practically communicate.  Whether "upper" or not, these seem likely to proliferate.

==Available upper ontologies==

===Basic Formal Ontology (BFO)===
{{main|Basic Formal Ontology}}
The Basic Formal Ontology (BFO) framework developed by [[Barry Smith (academic and ontologist)|Barry Smith]] and his associates consists of a series of sub-ontologies at different levels of granularity. The ontologies are divided into two varieties: relating to continuant entities such as three-dimensional enduring objects, and occurrent entities (primarily) processes conceived as unfolding in successive phases through time. BFO thus incorporates both three-dimensionalist and four-dimensionalist perspectives on reality within a single framework. Interrelations are defined between the two types of ontologies in a way which gives BFO the facility to deal with both static/spatial and dynamic/temporal features of reality. A continuant domain ontology descending from BFO can be conceived as an inventory of entities existing at a time. Each occurrent ontology can be conceived as an inventory of processes unfolding through a given interval of time. Both BFO itself and each of its extension sub-ontologies can be conceived as a window on a certain portion of reality at a given level of granularity. More than [http://ifomis.uni-saarland.de/bfo/users 200 extension ontologies] of BFO have been created, applying the BFO architecture to different domains through the strategy of downward population. The Cell Ontology, for example, populates downward from BFO by importing the BFO branch terminating with object, and defining a cell as a subkind of object. Other examples of ontologies extending BFO are the [[Ontology for Biomedical Investigations]] (OBI) and the ontologies of the [[OBO Foundry|Open Biomedical Ontologies Foundry]]. In addition to these examples, BFO and extensions are increasingly being use in defense and security domains, for example in the [http://milportal.org AIRS framework]. BFO serves as the upper level of the Sustainable Development Goals (SDG) Interface Ontology developed by the [http://uneplive.unep.org/portal United Nations Environment Programme]. BFO has been documented in the textbook [http://mitpress.mit.edu/building-ontologies Building Ontologies with Basic Formal Ontology], published by MIT Press in 2015.

===BORO===
{{main|BORO}}
Business Objects Reference Ontology is an upper ontology designed for developing ontological or semantic models for large complex operational applications that consists of a top ontology as well as a process for constructing the ontology.  It is built upon a series of clear [[metaphysical choices]]  to provide a solid (metaphysical) foundation. A key choice was for an [[Extension (metaphysics)|extensional]] (and hence, [[Spacetime|four-dimensional]]) [[ontology]] which provides it a simple [[criteria of identity]]. Elements of it have appeared in a number of standards. For example, the ISO standard, [[ISO 15926]] – Industrial automation systems and integration – was heavily influenced by an early version. The [[IDEAS Group|IDEAS]] (International Defence Enterprise Architecture Specification for exchange) standard is based upon BORO, which in turn was used to develop [[DODAF]] 2.0.

===CIDOC Conceptual Reference Model===
{{main|CIDOC Conceptual Reference Model}}
Although "CIDOC object-oriented Conceptual Reference Model" (CRM) is a [[Ontology (information science)#Domain ontologies and upper ontologies|domain ontology]], specialised to the purposes of representing cultural heritage, a subset called CRM Core is a generic upper ontology, including:&lt;ref&gt;{{cite web|title=Graphical Representation of core CRM form|url=http://www.cidoc-crm.org/cidoc_core_graphical_representation/graphical_representation.html|publisher=[[CIDOC]]}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Definition of the CIDOC Conceptual Reference Model, Version 5.0.4|url=http://www.cidoc-crm.org/html/5.0.4/cidoc-crm.html#_Toc310250785|publisher=[[CIDOC]]|date=November 2011}}&lt;/ref&gt;
* Space-Time – title/identifier, place, era/period, time-span, relationship to persistent items
* Events – title/identifier, beginning/ending of existence, participants (people, either individually or in groups), creation/modification of things (physical or conceptional), relationship to persistent items
* Material Things – title/identifier, place, the information object the material thing carries, part-of relationships, relationship to persistent items
* Immaterial Things – title/identifier, information objects (propositional or symbolic), conceptional things, part-of relationships

A persistent item is a physical or conceptional item that has a persistent identity recognized within the duration of its existence by its identification rather than by its continuity or by observation. A persistent item is comparable to an endurant.&lt;br/&gt;A propositional object is a set of statements about real or imaginary things.&lt;br/&gt;A symbolic object is a sign/symbol or an aggregation of signs or symbols.

===COSMO===
[[Common Semantic Model|COSMO]] (COmmon Semantic MOdel, available at http://micra.com/COSMO/COSMO.owl) is an ontology that was initiated as a project of the COSMO working group of the Ontology and taxonomy Coordinating Working Group, with the goal of developing a foundation ontology that can serve to enable broad general [[Semantic Interoperability]]. The current version is an OWL ontology, but a Common-Logic compliant version is anticipated in the future. The ontology and explanatory files are available at the COSMO site. The goal of the COSMO working group was to develop a foundation ontology by a collaborative process that will allow it to represent all of the basic ontology elements that all members feel are needed for their applications. The development of COSMO is fully open, and any comments or suggestions from any sources are welcome. After some discussion and input from members in 2006, the development of the COSMO has been continued primarily by Patrick Cassidy, the chairman of the COSMO Working Group. Contributions and suggestions from any interested party are still welcome and encouraged. Many of the types (OWL classes) in the current COSMO have been taken from the OpenCyc OWL version 0.78, and from the SUMO. Other elements were taken from other ontologies (such as BFO and DOLCE), or developed specifically for COSMO. Development of the COSMO initially focused on including representations of all of the words in the [[Longman Dictionary of Contemporary English]] (LDOCE) controlled [[defining vocabulary]] (2148 words). These words are sufficient to define (linguistically) all of the entries in the LDOCE. It is hypothesized that the ontological representations of the concepts represented by those terms will be sufficient to specify the meanings of any specialized ontology element, thereby serving as a basis for general [[Semantic Interoperability]].  Interoperability via COSMO is enabled by using the COSMO (or an ontology derived from it) as an interlingua by which other domain ontologies can be translated into each other's terms and thereby accurately communicate. As new domains are linked into COSMO, additional semantic primitives may be recognized and added to its structure.  The current (January 2016) OWL version of COSMO has over 8000 types (OWL classes), over 1000 relations, and over 3000 restrictions.  The COSMO itself (COSMO.owl) and other related and explanatory files can be obtained at http://micra.com/COSMO.

===Cyc===
{{main|Cyc}}
A well-known and quite comprehensive ontology available today is [[Cyc]], a proprietary system under development since 1986, consisting of a foundation ontology and several domain-specific ontologies (called ''microtheories''). A subset of that ontology has been released for free under the name [[Cyc#OpenCyc|OpenCyc]], and a more or less unabridged version is made available for free non-commercial use under the name [[Cyc#ResearchCyc|ResearchCyc]].

=== DOLCE ===
Descriptive Ontology for Linguistic and Cognitive Engineering (DOLCE) is the first module of the WonderWeb foundational ontologies library,&lt;ref&gt;http://www.loa-cnr.it/old/Papers/D18.pdf&lt;/ref&gt; developed by Nicola Guarino and his associates at the Laboratory for Applied Ontology (LOA). As implied by its acronym, DOLCE has a clear ''cognitive bias'', in that it aims at capturing the ontological categories underlying [[natural language]] and human [[common sense]]. DOLCE, however, does not commit to a strictly [[referentialist]] metaphysics related to the intrinsic nature of the world. Rather, the categories it introduces are thought of as cognitive artifacts, which are ultimately depending on human perception, cultural imprints and social conventions. In this sense, they intend to be just ''descriptive'' (vs ''prescriptive'') notions, that assist in making already formed conceptualizations explicit.

===General Formal Ontology (GFO)===
{{main|General formal ontology}}
The general formal ontology (GFO), developed by Heinrich Herre and his colleagues of the research group Onto-Med in [[Leipzig]], is a realistic ontology integrating processes and objects. It attempts to include many aspects of recent philosophy, which is reflected both in its taxonomic tree and its axiomatizations. GFO allows for different axiomatizations of its categories (such as the existence of [[atomic time-interval]]s vs. [[dense time]]). The basic principles of GFO are published in the Onto-Med Report Nr. 8 and in "General Formal Ontology (GFO): A Foundational Ontology for Conceptual Modelling".&lt;ref&gt;http://www.onto-med.de/Archiv/ontomed2002/en/publications/scientific-reports/om-report-no8.pdf&lt;/ref&gt;&lt;ref&gt;http://www.onto-med.de/publications/2010/gfo-basic-principles.pdf&lt;/ref&gt;

Two GFO specialties, among others, are its account of persistence and its time model. Regarding persistence, the distinction between endurants (objects) and perdurants (processes) is made explicit within GFO by the introduction of a special category, a persistent&lt;!-- [sic]?, not persistAnt? --&gt;.&lt;ref&gt;http://www.onto-med.de/en/theories/gfo/part1/node20.html&lt;/ref&gt; A persistant&lt;!-- [sic], not persistEnt --&gt; is a special category with the intention that its instances "remain identical" (over time). With respect to time, time intervals are taken as primitive in GFO, and time-points (called "time boundaries") as derived. Moreover, time-points may coincide, which is convenient for modelling instantaneous changes.

===gist===

gist is developed and supported by Semantic Arts.  gist (not an acronym – it means to get the essence of) is a “minimalist upper ontology”.  gist is targeted at enterprise information systems, although it has been applied to healthcare delivery applications.  
The major attributes of gist are:
# it is small (there are 140 classes and 127 properties)
# it is comprehensive (most enterprises will not find the need to create additional primitive classes, but will find that most of their classes can be defined and derived from gist)
# it is robust – all the classes descend from 12 primitive classes, which are mostly mutually disjoint.  This aids a great deal in subsequent error detection.  There are 1342 axioms, and it uses almost all of the DL constructs (it is SROIQ(D) )
# it is concrete – most upper ontologies start with abstract philosophical concepts that users must commit to in order to use the ontology.  Gist starts with concrete classes that most people already do, or reasonably could agree with, such as Person, Organization, Document, Time, UnitOfMeasure and the like) 
# it is unambiguous – ambiguous terms (such as “term”) have been removed as they are often overloaded and confused.  Also terms that frequently have different definitions at different enterprises (such as customer and order) have been removed, also to reduce ambiguity.
# it is understandable – in addition to being built on concrete, generally understood primitives, it is extremely modular.  The 140 classes are implemented in 18 modular ontologies, each can easily be understood in its entirety, and each imports only the other modules that it needs. 
gist has been used to build Enterprise Ontologies for a number of major commercial and governmental agencies including:  Procter &amp; Gamble, Sentara Healthcare, Washington State Department of Labor &amp; Industries, LexisNexis, Sallie Mae and two major Financial Services firms.
gist is freely available with a Creative Commons share alike license.  There are 18 small ontologies that make up gist.  Gist can be downloaded all at once by loading or importing gistCore at gist7.   
gist is actively maintained, and has been in use for 10 years. As of May 2015 it is at version 7.1.1.&lt;ref&gt;{{cite web|url=http://semanticarts.com/gist|title=gist home page|author=Semantic Arts}}&lt;/ref&gt;

gist was the subject of a paper exploring how to bridge modeling differences between ontologies &lt;ref&gt;{{cite web|url=http://www.researchgate.net/profile/Anthony_Cohn/publication/221235042_Utility_Ontology_Development_with_Formal_Concept_Analysis/links/0912f50cbb29adba1f000000.pdf#page=163|title=Complexity of Reasoning with Expressive Ontology Mappings|author1=Chiara Ghidini |author2=Luciano Serafini |author3=Segio Tessaris }}&lt;/ref&gt;
In a paper describing the OQuaRE methodology for evaluating ontologies, the gist unit of measure ontology scored the highest in the manual evaluation against 10 other unit of measure ontologies,  and scored above average in the automated evaluation.  The authors stated "This ontology could easily be tested and validated, its knowledge could be effectively reused and adapted for different specified environments" &lt;ref&gt;{{cite web|url=http://www.acs.org.au/__data/assets/pdf_file/0015/14118/JRPIT43.2.159.pdf|title=OQuaRE: A SQuaRE-based Approach for Evaluating the Quality of Ontologies|author1=Astrid Duque-Ramos  |author2=Jesualdo Tomas Fernandez-Breis |lastauthoramp=yes }}&lt;/ref&gt;

===IDEAS===
The upper ontology developed by the [[IDEAS Group]] is [[higher-order]], [[extensional]] and [[4D ontology|4D]]. It was developed using the [[BORO Method]]. The IDEAS ontology is not intended for reasoning and inference purposes; its purpose is to be a precise model of business.

===ISO 15926===
{{main|ISO 15926}}
ISO 15926 is an International Standard for the representation of process plant life-cycle information. This representation is specified by a generic, conceptual data model that is suitable as the basis for implementation in a shared database or data warehouse. The data model is designed to be used in conjunction with reference data: standard instances that represent information common to a number of users, process plants, or both. The support for a specific life-cycle activity depends on the use of appropriate reference data in conjunction with the data model. To enable integration of life-cycle information the model excludes all information constraints that are appropriate only to particular applications within the scope.
ISO 15926-2 defines a generic model with 201 entity types. It has been prepared by Technical Committee ISO/TC 184, Industrial automation systems and integration, Subcommittee SC 4, Industrial data.

===MarineTLO===
MarineTLO is an upperontology for the marine domain (also applicable to the terrestrial domain), developed by the Information Systems Laboratory at the Institute of Computer Science,
Foundation for Research and Technology - Hellas ([[FORTH-ICS]]).
Its purpose is to tackle the need for having integrated sets of facts about marine species,
and thus to assist research about species and [[biodiversity]].
It provides a unified and coherent core model for schema mapping which enables formulating and
answering queries which cannot be answered by any individual source.&lt;ref&gt;{{cite web|url=http://www.ics.forth.gr/isl/MarineTLO|title=MarineTLO - A Top Level Ontology for the Marine/Biodiversity Domain|work=forth.gr|accessdate=22 April 2015}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|author=Tzitzikas, Y. and Alloca, C. and Bekiari, C. and Marketakis, Y. and Fafalios, P. and Doerr, M. and Minadakis, N. and Patkos, T. and Candela, L.|title=Integrating Heterogeneous and Distributed Information about Marine Species through a Top Level Ontology|url=http://link.springer.com/chapter/10.1007/978-3-319-03437-9_29|location=Institute of Computer Science, FORTH-ICS, Greece|publisher=Springer|year=2013|pages=289–301|doi=10.1007/978-3-319-03437-9_29|journal=Communications in Computer and Information Science}}&lt;/ref&gt;

===PROTON===
PROTON (PROTo ONtology) is a basic [[subsumption hierarchy]] which provides coverage of most of the upper-level concepts   necessary for semantic annotation, indexing, and retrieval.{{citation needed|date=November 2014}}

===SUMO (Suggested Upper Merged Ontology)===
{{main|Suggested Upper Merged Ontology}}
The [[Suggested Upper Merged Ontology]] (SUMO) is another comprehensive ontology project.  It includes an [[Standard upper ontology|upper ontology]], created by the [[IEEE]] working group P1600.1 (originally by [[Ian Niles]] and [[Adam Pease]]). It is extended with many domain ontologies and a complete set of links to WordNet. It is open source.

===UMBEL===
{{main|UMBEL}}
Upper Mapping and Binding Exchange Layer ([[UMBEL]]) is an ontology of 28,000 reference concepts that maps to a simplified subset of the [[OpenCyc]] ontology, that is intended to provide a way of linking the precise OpenCyc ontology with less formal ontologies.&lt;ref&gt;{{cite web|url=http://www.mkbergman.com/441/the-role-of-umbel-stuck-in-the-middle-with-you/|title=The Role of UMBEL: Stuck in the Middle with You . . .|author=Mike Bergman|accessdate=2010-10-26}}&lt;/ref&gt; It also has formal mappings to [[Wikipedia]], [[DBpedia]], [[PROTON Ontology|PROTON]] and [[GeoNames]]. It has been developed and maintained as [[open source]] by Structured Dynamics.

===UFO (Unified Foundational Ontology)===
The Unified Foundational Ontology (UFO), developed by Giancarlo Guizzardi and associates, incorporating developments from GFO, DOLCE and the Ontology of Universals underlying OntoClean in a single coherent foundational ontology. The core categories of UFO (UFO-A) have been completely formally characterized in Giancarlo Guizzardi's Ph.D. thesis and further extended at the Ontology and Conceptual Modelling Research Group (NEMO) in Brazil with cooperators from Brandenburg University of Technology (Gerd Wagner) and Laboratory for Applied Ontology (LOA). UFO-A has been employed to analyze structural conceptual modeling constructs such as object types and taxonomic relations, associations and relations between associations, roles, properties, datatypes and weak entities, and parthood relations among objects. More recent developments incorporate an ontology of events in UFO (UFO-B), as well as an ontology of social and intentional aspects (UFO-C). The combination of UFO-A, B and C has been used to analyze, redesign and integrate reference conceptual models in a number of complex domains such as, for instance, Enterprise Modeling, Software Engineering, Service Science, Petroleum and Gas, Telecommunications, and Bioinformatics. Another recent development aimed towards a clear account of services and service-related concepts, and provided for a commitment-based account of the notion of service (UFO-S),&lt;ref&gt;Nardi, J. C., Falbo, R. D. A., Almeida, J. P. A., Guizzardi, G., Pires, L. F., van Sinderen, M. J., &amp; Guarino, N. (2013, September). "Towards a commitment-based reference ontology for services". In Enterprise Distributed Object Computing Conference (EDOC), 2013 17th IEEE International (pp. 175-184). IEEE.&lt;/ref&gt;
UFO is the foundational ontology for [[OntoUML]], an ontology modeling language.

===WordNet===
{{main|WordNet}}
[[WordNet]], a freely available database originally designed as a [[semantic network]] based on [[psycholinguistic]] principles, was expanded by addition of definitions and is now also viewed as a [[dictionary]]. It qualifies as an upper ontology by including the most general concepts as well as more specialized concepts, related to each other not only by the [[subsumption relation]]s, but by other semantic relations as well, such as part-of and cause. However, unlike Cyc, it has not been formally axiomatized so as to make the logical relations between the concepts precise. It has been widely used in [[Natural language processing]] research.

===YAMATO (Yet Another More Advanced Top Ontology)===
YAMATO is developed by Riichiro Mizoguchi, formerly at the Institute of Scientific and Industrial Research of the [[University of Osaka]], and now at the [[Japan Advanced Institute of Science and Technology]]. Major features of YAMATO are:
# an advanced description of quality, attribute, property, and quantity,&lt;ref&gt;http://www.ei.sanken.osaka-u.ac.jp/hozo/onto_library/YAMATO101216.pdf&lt;/ref&gt; 
# an ontology of representation,&lt;ref&gt;{{cite journal|url=http://link.springer.com/article/10.1007/BF03040960#page-1|title=Part 3: Advanced course of ontological engineering|work=springer.com|accessdate=22 April 2015|doi=10.1007/BF03040960|volume=22|pages=193–220}}&lt;/ref&gt; 
# an advanced description of processes and events,&lt;ref&gt;{{cite journal | doi = 10.3233/AO-2009-0067 }}&lt;/ref&gt; 
# the use of a theory of roles.&lt;ref&gt;{{cite journal | last1 = Mizoguchi | first1 = R. | last2 = Sunagawa | first2 = E. | last3 = Kozaki | first3 = K. | last4 = Kitamura | first4 = Y. | year = | title = A Model of Roles within an Ontology Development Tool: Hozo | url = http://iospress.metapress.com/content/w67u25284x0l206v/ | journal = J. of Applied Ontology | volume = 2 | issue = 2| pages = 159–179 }}&lt;/ref&gt;

YAMATO has been extensively used for developing other, more applied, ontologies such as a medical ontology,&lt;ref&gt;http://ceur-ws.org/Vol-833/paper9.pdf&lt;/ref&gt; an ontology of gene,&lt;ref&gt;http://ceur-ws.org/Vol-897/session1-paper05.pdf&lt;/ref&gt; an ontology of learning/instructional theories,&lt;ref&gt;{{cite web|url=http://edont.qee.jp/omnibus/doku.php|title=start    [OMNIBUS project]|date=6 December 2014|work=qee.jp|accessdate=22 April 2015}}&lt;/ref&gt; an ontology of sustainability science,&lt;ref&gt;{{cite web|url=http://link.springer.com/article/10.1007%2Fs11625-008-0063-z|title=Toward knowledge structuring of sustainability science based on ontology engineering|work=springer.com|accessdate=22 April 2015}}&lt;/ref&gt; and an ontology of the cultural domain.

== Upper/Foundational Ontology tools==

===ONSET===
{{unreferenced section|date=November 2014}}
ONSET, the foundational ontology selection and explanation tool, assists the domain ontology developer in selecting the most appropriate foundational ontology. The domain ontology developer provides the requirements/answers one or more questions, and ONSET computes the selection of the appropriate foundational ontology and explains why. The current version (v2 of 24 April 2013) includes DOLCE, BFO, GFO, SUMO, YAMATO and GIST.

===ROMULUS===
{{unreferenced section|date=November 2014}}
ROMULUS is a foundational ontology repository aimed at improving semantic interoperability. Currently there are three foundational ontologies in the repository: DOLCE, BFO and GFO. Features of ROMULUS include:
# It provides a high-level view of the foundational ontologies with only the most general concepts common to all implemented foundational ontologies. 
# Foundational ontologies in ROMULUS are modularised.
# Foundational ontology mediation has been performed. This includes alignment, mapping, merging, searchable metadata and an interchangeability method for foundational ontologies. 
# ROMULUS provides detailed taxonomies of each foundational ontology to allow easy browsing of foundational ontologies. 
# ROMULUS allows you to download each foundational ontology module including the integrated foundational ontologies. 
# Searchable metadata of each foundational ontology is available. 
# A comparison of the included foundational ontologies is available.

==See also==
* [[Authority control]]
* [[Formal ontology]]
* [[Foundations of mathematics]]
* [[Knowledge Organization Systems]]
* [[Library classification]]
* [[Ontology (information science)]]
* [[Physical ontology]]
* [[Process ontology]]
* [[Semantic interoperability]]
* [[Commonsense knowledge]]

==References==
{{Reflist}}

==External links==
{{External links|date=April 2015}}
* [http://www.onto-med.de/ontologies/gfo General Formal Ontology (GFO) homepage]
* [http://www.loa.istc.cnr.it/ Laboratory of Applied Ontology (LOA) homepage]
* [http://proton.semanticweb.org/ PROTON Ontology]
* [http://ontolog.cim3.net/cgi-bin/wiki.pl?UpperOntologySummit Upper Ontology Summit (March 2006)]
* [http://ontogenesis.knowledgeblog.org/740 What is an upper level ontology?] Knowledge Blog article, 2010.
* [http://www.ics.forth.gr/isl/MarineTLO/ The MarineTLO ontology] What, Why, Who, Current applications, How to exploit it, Documents and Publications, Provide feedback.
* [http://www.thezfiles.co.za/ROMULUS/Onset/webonset.html ONSET]
* [http://www.thezfiles.co.za/ROMULUS/ ROMULUS]
{{Computable knowledge}}

{{DEFAULTSORT:Upper Ontology (Information Science)}}
[[Category:Knowledge representation]]
[[Category:Technical communication]]
[[Category:Information science]]
[[Category:Ontology (information science)]]</text>
      <sha1>ds4gwz7ptxy53smetwh9ifeqx6liy1c</sha1>
    </revision>
  </page>
  <page>
    <title>Framing (social sciences)</title>
    <ns>0</ns>
    <id>10438439</id>
    <revision>
      <id>762657019</id>
      <parentid>762076333</parentid>
      <timestamp>2017-01-30T01:10:43Z</timestamp>
      <contributor>
        <username>Matthiaspaul</username>
        <id>13467261</id>
      </contributor>
      <comment>/* See also */ +links</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="81728" xml:space="preserve">{{globalize|date=July 2010}}

In the [[social sciences]], '''framing''' comprises a set of concepts and theoretical perspectives on how individuals, groups, and societies, organize, perceive, and communicate about [[reality]]. Framing involves [[social construction]] of a [[social phenomenon]] – by [[mass media]] sources, political or social movements, political leaders, or other actors and organizations. Participation in a language community necessarily influences an individual's ''[[perception]]'' of the meanings attributed to words or phrases. Politically, the language communities of [[advertising]], [[religion]], and mass media are highly contested, whereas framing in less-sharply defended [[speech community|language communities]] might evolve imperceptibly and organically over [[cultural]] time frames, with fewer overt modes of disputation.  

Framing itself can be framed in one of two ways, depending on whether one chooses to emphasise processes of [[cognition|thought]] or processes of interpersonal [[communication]]. ''Frames in thought'' consist of the mental representations, interpretations, and simplifications of reality. ''Frames in communication'' consist of the communication of frames between different actors.&lt;ref name="Druckman2001"&gt;{{cite journal | last1 = Druckman | first1 = J.N. | year = 2001 | title = The Implications of Framing Effects for Citizen Competence | url = | journal = Political Behavior | volume = 23 | issue = 3| pages = 225–256 | doi=10.1023/A:1015006907312}}&lt;/ref&gt;

One can view framing in communication as positive or negative – depending on the audience and what kind of information is being presented. Framing might also be understood as being either ''equivalence frames'', which represent logically equivalent alternatives portrayed in different ways (see [[framing effect (psychology)|framing effect]]) or as ''emphasis frames'', which simplify reality by focusing on a subset of relevant aspects of a situation or issue.&lt;ref name="Druckman2001" /&gt; In the case of  "equivalence frames", the information being presented is based on the same facts, but the "frame" in which it is presented changes, thus creating a reference-dependent perception.

The effects of framing can be seen in many journalism applications. With the same information being used as a base, the "frame" surrounding the issue can change the reader's perception without having to alter the actual facts. In the context of politics or mass-media communication, a frame defines the packaging of an element of [[rhetoric]] in such a way as to encourage certain interpretations and to discourage others.  For political purposes, framing often presents facts in such a way that implicates a problem that is in need of a solution. Members of political parties attempt to frame issues in a way that makes a solution favoring their own political leaning appear as the most appropriate course of action for the situation at hand.&lt;ref name="van der Pas"&gt;{{cite journal|last=van der Pas|first=D.|title=Making Hay While the Sun Shines: Do Parties Only Respond to Media Attention When The Framing is Right?|journal=Journal of Press/Politics|year=2014|volume=19|issue=1|pages=42–65|doi=10.1177/1940161213508207}}&lt;!--|accessdate=6 March 2014--&gt;&lt;/ref&gt;

In [[social theory]], framing is a [[Schema (psychology)|schema]] of [[interpretation (logic)|interpretation]], a collection of [[Anecdotal evidence|anecdotes]] and [[stereotype]]s, that individuals rely on to understand and respond to events.&lt;ref name="Goffman1974"&gt;
Goffman, E. (1974). Frame analysis: An easy on the organization of experience. Cambridge, MA: Harvard University Press. 
&lt;/ref&gt; In other words, people build a series of mental "filters" through biological and cultural influences. They then use these filters to make sense of the world. The choices they then make are influenced by their creation of a frame.

Framing is also a key component of [[sociology]], the study of social interaction among humans.  Framing is an integral part of conveying and processing data on a daily basis.  Successful framing techniques can be used to reduce the ambiguity of intangible topics by contextualizing the information in such a way that recipients can connect to what they already know.

== Explanation ==
When one seeks to explain an event, the understanding often depends on the frame referred to. If a friend rapidly closes and opens an eye, we will respond very differently depending on whether we attribute this to a purely "physical" frame (they blinked) or to a social frame (they winked).

Though the former might result from a speck of dust (resulting in an involuntary and not particularly meaningful reaction), the latter would imply a voluntary and meaningful action (to convey humor to an accomplice, for example). Observers will read events seen as purely physical or within a frame of "nature" differently from those seen as occurring with social frames. But we do not look at an event and then "apply" a frame to it. Rather, individuals constantly project into the world around them the interpretive frames that allow them to make sense of it; we only shift frames (or realize that we have habitually applied a frame) when incongruity calls for a frame-shift. In other words, we only become aware of the frames that we always already use when something forces us to replace one frame with another.&lt;ref&gt;
This example borrowed from Clifford Geertz: ''Local Knowledge: Further Essays in Interpretive Anthropology'' (1983), Basic Books 2000 paperback: ISBN 0-465-04162-0
&lt;/ref&gt;&lt;ref&gt;
Goffman offers the example of the woman bidding on a mirror at an auction who first examines the frame and surface for imperfections, and then "checks" herself in the mirror and adjusts her hat. See Goffman, Erving. ''Frame Analysis: An essay on the organization of experience''. Boston: Northeastern University Press, 1986. ISBN 0-930350-91-X, page 39. In each case the mirror represents more than simply a physical object.
&lt;/ref&gt;

Framing is so effective because it is a heuristic, or mental shortcut that may not always yield desired results; and is seen as a 'rule of thumb'. According to Susan T. Fiske and Shelley E. Taylor, human beings are by nature "cognitive misers", meaning they prefer to do as little thinking as possible.&lt;ref&gt;Fiske, S. T., &amp; Taylor, S. E. (1991). Social cognition (2nd ed.). New York: McGraw-Hill&lt;/ref&gt; Frames provide people a quick and easy way to process information. Hence, people will use the previously mentioned mental filters (a series of which is called a schema) to make sense of incoming messages. This gives the sender and framer of the information enormous power to use these schemas to influence how the receivers will interpret the message.&lt;ref name="EntmanRobertTree"&gt;Entman,Robert "Tree Beard". Framing: Toward Clarification of a Fractured Paradigm. Journal of Communication; Autumn 1993, 43, 4, p.51&lt;/ref&gt;

Though some consider framing to be synonymous with [[Agenda-setting theory|agenda setting]], other scholars state that there is a distinction. According to an article written by Donald H. Weaver, framing selects certain aspects of an issue and makes them more prominent in order to elicit certain interpretations and evaluations of the issue, whereas agenda setting introduces the issue topic to increase its salience and accessibility.&lt;ref&gt;{{Cite journal|last=Weaver|first=David H.|title=Thoughts on Agenda Setting, Framing, and Priming|journal=Journal of Communication|volume=57}}&lt;/ref&gt;

==Framing effect in communication research==
In the field of communication, framing defines how news media coverage shapes [[mass opinion]]. [[Richard Vatz|Richard E. Vatz's]] discourse on creation of rhetorical meaning relates directly to framing, although he references it little.  To be specific, framing effects refer to behavioral or attitudinal strategies and/or outcomes that are due to how a given piece of information is being framed in [[public discourse]]. Today, many volumes of the major communication journals contain papers on media frames and framing effects.&lt;ref&gt;Scheufele, D. A. &amp; Iyengar, S. (forthcoming). The state of framing research: A call for new directions. In K. kENSKI, &amp; K. H. Jamieson (Eds.), The Oxford Handbook of political communication theories. New York: Oxford University Press.&lt;/ref&gt; Approaches used in such papers can be broadly classified into two groups: studies of framing as the dependent variable and studies of framing as the independent variable.&lt;ref&gt;Tewksbury &amp; Scheufele (2009). News framing theory and research, In J. Bryant, &amp; M. B. Oliver (Eds.) Media effects: Advances in theory and research, New York: Routledge.&lt;/ref&gt; The former usually deals with ''frame building'' (i.e. how frames create societal discourse about an issue and how different frames are adopted by journalists) and latter concerns ''frame setting'' (i.e. how media framing influences an audience).

===Frame building===
Frame building is related to at least three areas: journalist norms, political actors, and cultural situations. It assumes that several media frames compete to set one frame regarding an issue, and one frame finally gains influence because it resonates with [[popular culture]], fits with media practices, or is heavily sponsored by [[elite]]s. 
First, in terms of practices of news production, there are at least five aspects of news work that may influence how journalists frame a certain issue: larger societal norms and values, organizational pressures and constraints, external pressures from [[interest group]]s and other [[policy maker]]s, professional routines, and ideological or political orientations of journalists. The second potential influence on frame building comes from elites, including interest groups, government bureaucracies, and other political or corporate actors. Empirical studies show that these influences of elites seem to be strongest for issues in which journalists and various players in the policy arena can find shared narratives. Finally, cultural contexts of a society are also able to establish frame. Goffman&lt;ref name="Goffman1974"/&gt; assumes that the meaning of a frame has implicit cultural roots. This context dependency of media frame has been described as 'cultural resonance'&lt;ref&gt;Gamson, W. A. &amp; Modigliani, A. (1987) The changing culture of affirmative action. Research in Political Sociology, 3, 137-177&lt;/ref&gt; or 'narrative fidelity'.&lt;ref name="SnowBenford1988"&gt;Snow, D. A., &amp; Benford, R. D. (1988). Ideology, frame resonance, and participant mobilization. In B. Klandermans, H. Kriesi, &amp; S. Tarrow (Eds.), International social movement research. Vol 1, From structure on action: Comparing social movement research across cultures (pp. 197-217). Greenwich, CT: JAI Press.&lt;/ref&gt;

===Frame setting===
When people are exposed to a novel news frame, they will accept the constructs made applicable to an issue, but they are significantly more likely to do so when they have existing schema for those constructs. This is called the applicability effect. That is, when new frames invite people to apply their existing schema to an issue, the implication of that application depends, in part, on what is in that schema. Therefore, generally, the more the audiences know about issues, the more effective are frames.

There are a number of levels and types of framing effects that have been examined. For example, scholars have focused on attitudinal and behavioral changes, the degrees of perceived importance of the issue, voting decisions, and opinion formations. Others are interested in psychological processes other than applicability. For instance, Iyengar&lt;ref&gt;Iyengar, S. (1991). Is anyone responsible? How television frames political issues. Chicago: University of Chicago Press.&lt;/ref&gt; suggested that news about social problems can influence attributions of causal and treatment responsibility, an effect observed in both cognitive responses and evaluations of political leaders, or other scholars looked at the framing effects on receivers' evaluative processing style and the complexity of audience members' thoughts about issues.

==In mass communication research==
News media frame all news items by emphasizing specific values, facts, and other considerations, and endowing them with greater apparent applicability for making related judgments.  News media promotes particular definitions, interpretations, evaluations and recommendations.&lt;ref name=Entman1993&gt;{{cite journal|last=Entman|first=R.M.|title=Framing: Toward clarification of a fractured paradigm|journal=Journal of Communication|year=1993|volume=43|issue=4|pages=51–58|doi=10.1111/j.1460-2466.1993.tb01304.x}}&lt;/ref&gt;&lt;ref name=NelsonClawsonOxley1997&gt;{{cite journal|last=Nelson|first=T.E.|author2=Clawson, R.A. |author3=Oxley, Z.M. |title=Media framing of a civil liberties conflict and its effect on tolerance|journal=American Political Science Review|year=1997|volume=91|issue=3|pages=567–583|doi=10.2307/2952075}}&lt;/ref&gt;

===Foundations in mass communication research===

Anthropologist [[Gregory Bateson]] first articulated the concept of framing in his 1972 book ''[[Steps to an Ecology of Mind]]''.  A frame, Bateson wrote, is "a spatial and temporal bounding of a set of interactive messages."&lt;ref name=Bateson1972&gt;{{cite book|last=Bateson|first=G.|title=Steps to an Ecology of Mind|year=1972|publisher=Ballantine Books|location=New York}}&lt;/ref&gt;

====Sociological roots of media framing research====

Media framing research has both sociological and psychological roots.  Sociological framing focuses on "the words, images, phrases, and presentation styles" that communicators use when relaying information to recipients.&lt;ref name="Druckman2001" /&gt; Research on frames in sociologically driven media research generally examines the influence of "social norms and values, organizational pressures and constraints, pressures of interest groups, journalistic routines, and ideological or political orientations of journalists" on the existence of frames in media content.&lt;ref name=Scheufele2000&gt;{{cite journal|last=Scheufele|first=D.A.|title=Agenda-setting, priming, and framing revisited: Another look at cognitive effects of political communication|journal=Mass Communication &amp; Society|year=2000|volume=3|issue=2&amp;3|pages=297–316|doi=10.1207/S15327825MCS0323_07}}&lt;/ref&gt;

[[Todd Gitlin]], in his analysis of how the news media trivialized the student [[New Left]] movement during the 1960s, was among the first to examine media frames from a sociological perspective.  Frames, Gitlin wrote, are "persistent patterns of cognition, interpretations, and presentation, of selection [and] emphasis ... [that are] largely unspoken and unacknowledged ... [and] organize the world for both journalists [and] for those of us who read their reports."&lt;ref name=Gitlin1980&gt;{{cite book|last=Gitlin|first=T.|title=The Whole World is Watching: Mass Media in the Making and Unmaking of the New Left|year=1980|publisher=University of California Press|location=Berkeley, CA}}&lt;/ref&gt;

====Psychological roots of media framing research====

Research on frames in psychologically driven media research generally examines the effects of media frames on those who receive them.  For example, Iyengar explored the impact of episodic and thematic news frames on viewers' attributions of responsibility for political issues including crime, terrorism, poverty, unemployment, and racial inequality.&lt;ref name=Iyengar1991&gt;{{cite book|last=Iyengar|first=S.|title=Is Anyone Responsible? How Television Frames Political Issues|year=1991|publisher=University of Chicago Press|location=Chicago}}&lt;/ref&gt; According to Iyengar, an episodic news frame "takes the form of a case study or event-oriented report and depicts public issues in terms of concrete instances," while a thematic news frame "places public issues in some more general abstract context ... directed at general outcomes or conditions."&lt;ref name=Entman1993 /&gt;&lt;ref name=Iyengar1991 /&gt; Iyengar found that the majority of television news coverage of poverty, for example, was episodic.&lt;ref name=Iyengar1991 /&gt;  In fact, in a content analysis of six years of television news, Iyengar found that the typical news viewer would have been twice as likely to encounter episodic rather than thematic television news about poverty.&lt;ref name=Iyengar1991 /&gt;  Further, experimental results indicate participants who watched episodic news coverage of poverty were more than twice as likely as those who watched thematic news coverage of poverty to attribute responsibility of poverty to the poor themselves rather than society.&lt;ref name=Iyengar1991 /&gt;  Given the predominance of episodic framing of poverty, Iyengar argues that television news shifts responsibility of poverty from government and society to the poor themselves.&lt;ref name=Iyengar1991 /&gt;  After examining content analysis and experimental data on poverty and other political issues, Iyengar concludes that episodic news frames divert citizens' attributions of political responsibility away from society and political elites, making them less likely to support government efforts to address those issue and obscuring the connections between those issues and their elected officials' actions or lack thereof.&lt;ref name=Iyengar1991 /&gt;

===Clarifying and distinguishing a "fractured paradigm"===

Perhaps because of their use across the social sciences, frames have been defined and used in many disparate ways.  Entman called framing "a scattered conceptualization" and "a fractured paradigm" that "is often defined casually, with much left to an assumed tacit understanding of the reader."&lt;ref name=Entman1993 /&gt; In an effort to provide more conceptual clarity, Entman suggested that frames "select some aspects of a perceived reality and make them more salient in a communicating text, in such a way as to promote a particular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation for the item described."&lt;ref name=Entman1993 /&gt;

Entman's&lt;ref name=Entman1993 /&gt;  conceptualization of framing, which suggests frames work by elevating particular pieces of information in salience, is in line with much early research on the psychological underpinnings of framing effects (see also Iyengar,&lt;ref name=Iyengar1991 /&gt;  who argues that accessibility is the primary psychological explanation for the existence of framing effects).  Wyer and Srull&lt;ref name=WyerSrull1984 /&gt;  explain the construct of accessibility thus:
# People store related pieces of information in "referent bins" in their long-term memory.&lt;ref name=WyerSrull1984&gt;{{cite book|last=Wyer, Jr.|first=R.S.|title=Social Cognition: The Ontario Symposium|year=1984|publisher=Lawrence Erlbaum|location=Hillsdale, NJ|author2=Srull, T.K.|editor=E.T. Higgins |editor2=N.A. Kuiper |editor3=M.P Zanna (Eds.)|chapter=Category Accessibility: Some theoretic and empirical issues concerning the processing of social stimulus information}}&lt;/ref&gt; 
# People organize "referent bins" such that more frequently and recently used pieces of information are stored at the top of the bins and are therefore more accessible.&lt;ref name=WyerSrull1984 /&gt; 
# Because people tend to retrieve only a small portion of information from long-term memory when making judgments, they tend to retrieve the most accessible pieces of information to use for making those judgments.&lt;ref name=WyerSrull1984 /&gt;

The argument supporting accessibility as the psychological process underlying framing can therefore be summarized thus: Because people rely heavily on news media for public affairs information, the most accessible information about public affairs often comes from the public affairs news they consume.  The argument supporting accessibility as the psychological process underlying framing has also been cited as support in the debate over whether framing should be subsumed by [[agenda-setting theory]] as part of the second level of agenda setting.  McCombs and other agenda-setting scholars generally agree that framing should be incorporated, along with [[Priming (media)|priming]], under the umbrella of agenda setting as a complex model of media effects linking media production, content, and audience effects.&lt;ref name=Kosicki1993&gt;{{cite journal|last=Kosicki|first=G.M.|title=Problems and opportunities in Agenda-setting research|journal=Journal of Communication|year=1993|volume=43|issue=2|pages=100–127|doi=10.1111/j.1460-2466.1993.tb01265.x}}&lt;/ref&gt;&lt;ref name=McCombsShaw1993&gt;{{cite journal|last=McCombs|first=M.E.|author2=Shaw, D.L.|title=The evolution of agenda-setting research: Twenty-five years in the marketplace of ideas|journal=Journal of Communication|year=1993|volume=43|issue=2|pages=58–67|doi=10.1111/j.1460-2466.1993.tb01262.x}}&lt;/ref&gt;&lt;ref name=McCombsLlamasLopez-EscobarRey1997 /&gt;  Indeed, McCombs, Llamas, Lopez-Escobar, and Rey justified their attempt to combine framing and agenda-setting research on the assumption of parsimony.&lt;ref name=McCombsLlamasLopez-EscobarRey1997&gt;{{cite journal|last=McCombs|first=M.F.|author2=Llamas, J.P. |author3=Lopez-Escobar, E. |author4=Rey, F. |title=Candidate images in Spanish elections: Second-level agenda-setting effects|journal=Journalism &amp; Mass Communication Quarterly|year=1997|volume=74|pages=703–717|doi=10.1177/107769909707400404|issue=4}}&lt;/ref&gt;

Scheufele, however, argues that, unlike agenda setting and priming, framing does not rely primarily on accessibility, making it inappropriate to combine framing with agenda setting and priming for the sake of parsimony.&lt;ref name=Scheufele2000 /&gt; Empirical evidence seems to vindicate Scheufele's claim.  For example, Nelson, Clawson, and Oxley empirically demonstrated that applicability, rather than their salience, is key.&lt;ref name="NelsonClawsonOxley1997" /&gt; By operationalizing accessibility as the response latency of respondent answers where more accessible information results in faster response times, Nelson, Clawson, and Oxley demonstrated that accessibility accounted for only a minor proportion of the variance in framing effects while applicability accounted for the major proportion of variance.&lt;ref name="NelsonClawsonOxley1997" /&gt; Therefore, according to Nelson and colleagues, "frames influence opinions by stressing specific values, facts, and other considerations, endowing them with greater apparent relevance to the issue than they might appear to have under an alternative frame."&lt;ref name="NelsonClawsonOxley1997" /&gt;

In other words, while early research suggested that by highlighting particular aspects of issues, frames make certain considerations more accessible and therefore more likely to be used in the judgment process,&lt;ref name=Entman1993 /&gt;&lt;ref name=Iyengar1991 /&gt;  more recent research suggests that frames work by making particular considerations more applicable and therefore more relevant to the judgment process.&lt;ref name="NelsonClawsonOxley1997" /&gt;&lt;ref name=Scheufele2000 /&gt;

===Equivalency versus emphasis: two types of frames in media research===

Chong and Druckman suggest framing research has mainly focused on two types of frames: equivalency and emphasis frames.&lt;ref name=ChongDruckman2007&gt;{{cite journal|last=Chong|first=D.|author2=Druckman, J.N. |title=Framing theory|journal=Annual Review of Political Science|year=2007|volume=10|pages=103–126|doi=10.1146/annurev.polisci.10.072805.103054}}&lt;/ref&gt;  Equivalency frames offer "different, but logically equivalent phrases," which cause individuals to alter their preferences.&lt;ref name="Druckman2001" /&gt; Equivalency frames are often worded in terms of "gains" versus "losses."  For example, Kahneman and Tversky asked participants to choose between two "gain-framed" policy responses to a hypothetical disease outbreak expected to kill 600 people.&lt;ref name=KahnemanTversky1984&gt;{{cite journal |last=Kahneman |first=D.|author2=Tversky, A.|title=Choices, values, and frames|journal=American Psychologist |year=1984|volume=39|issue=4|pages=341–350 |doi=10.1037/0003-066X.39.4.341}}&lt;/ref&gt;  Response A would save 200 people while Response B had a one-third probability of saving everyone, but a two-thirds probability of saving no one.  Participants overwhelmingly chose Response A, which they perceived as the less risky option. Kahneman and Tversky asked other participants to choose between two equivalent "loss-framed" policy responses to the same disease outbreak.  In this condition, Response A would kill 400 people while Response B had a one-third probability of killing no one but a two-thirds probability of killing everyone.  Although these options are  mathematically identical to those given in the "gain-framed" condition, participants overwhelmingly chose Response B, the risky option.  Kahneman and Tversky, then, demonstrated that when phrased in terms of potential gains, people tend to choose what they perceive as the less risky option (i.e., the sure gain).  Conversely, when faced with a potential loss, people tend to choose the riskier option.&lt;ref name=KahnemanTversky1984 /&gt;

Unlike equivalency frames, emphasis frames offer "qualitatively different yet potentially relevant considerations" which individuals use to make judgments.&lt;ref name=ChongDruckman2007 /&gt; For example, Nelson, Clawson, and Oxley exposed participants to a news story that presented the [[Ku Klux Klan]]'s plan to hold a rally.&lt;ref name="NelsonClawsonOxley1997" /&gt;  Participants in one condition read a news story that framed the issue in terms of public safety concerns while participants in the other condition read a news story that framed the issue in terms of free speech considerations.  Participants exposed to the public safety condition considered public safety applicable for deciding whether the Klan should be allowed to hold a rally and, as expected, expressed lower tolerance of the Klan's right to hold a rally.&lt;ref name="NelsonClawsonOxley1997" /&gt;  Participants exposed to the free speech condition, however, considered free speech applicable for deciding whether the Klan should be allowed to hold a rally and, as expected, expressed greater tolerance of the Klan's right to hold a rally.&lt;ref name="NelsonClawsonOxley1997" /&gt;

==Framing effect in psychology and economics==
[[File:Daniel KAHNEMAN.jpg|thumb|180px|[[Daniel Kahneman]]]]
{{Main|Framing effect (psychology)}}
[[Amos Tversky]] and [[Daniel Kahneman]] have shown that framing can affect the outcome (i.e. the choices one makes) of choice problems, to the extent that several of the classic axioms of [[rational choice]] do not hold.&lt;ref name="TverskyKahneman1981"&gt;{{cite journal | last1 = Tversky | first1 = Amos | last2 = Kahneman | first2 = Daniel | year = 1981 | title = The Framing of Decisions and the Psychology of Choice | url = | journal = Science | volume = 211 | issue = 4481| pages = 453–458 | doi = 10.1126/science.7455683 | pmid = 7455683 }}&lt;/ref&gt; This led to the development of [[prospect theory]] as an alternative to rational choice theory.&lt;ref&gt;Econport. "Decision-Making Under Uncertainty - Advanced Topics: An Introduction to Prospect Theory". (EconPort is an economics digital library specializing in content that emphasizes the use of experiments in teaching and research.) [http://www.econport.org/econport/request?page=man_ru_advanced_prospect]&lt;/ref&gt;

The context or framing of problems adopted by decision-makers results in part from extrinsic manipulation of the decision-options offered, as well as from forces intrinsic to decision-makers, e.g., their norms, habits, and unique [[temperament]].

===Experimental demonstration===
Tversky and Kahneman (1981) demonstrated systematic [[preference reversal|reversals of preference]] when the same problem is presented in different ways, for example in the Asian disease problem. Participants were asked to "imagine that the U.S. is preparing for the outbreak of an unusual Asian disease, which is expected to kill 600 people. Two alternative programs to combat the disease have been proposed. Assume the exact scientific estimate of the consequences of the programs are as follows."

The first group of participants was presented with a choice between programs:
In a group of 600 people,
* Program A: "200 people will be saved"
* Program B: "there is a 1/3 probability that 600 people will be saved, and a 2/3 probability that no people will be saved"

72 percent of participants preferred program A (the remainder, 28%, opting for program B).

The second group of participants was presented with the choice between the following:
In a group of 600 people,
* Program C: "400 people will die"
* Program D: "there is a 1/3 probability that nobody will die, and a 2/3 probability that 600 people will die"

In this decision frame, 78% preferred program D, with the remaining 22% opting for program C.

Programs A and C are identical, as are programs B and D. The change in the decision frame between the two groups of participants produced a preference reversal: when the programs were presented in terms of lives saved, the participants preferred the secure program, A (= C). When the programs were presented in terms of expected deaths, participants chose the gamble D (= B).&lt;ref&gt;{{Cite journal
| last = Entman
| first = R. M.
| year = 1993
| contribution = Framing: Toward Clarification of a Fractured Paradigm
| periodical = [[Journal of Communication]]
| volume = 43
| issue = 4
| pages = 51–58 [pp. 53–54] |doi=10.1111/j.1460-2466.1993.tb01304.x
| postscript = &lt;!--None--&gt;
}}&lt;/ref&gt;

===Absolute and relative influences===
Framing effects arise because one can frequently frame a decision using multiple [[scenario]]s, wherein one may express benefits either as a relative risk reduction (RRR), or as absolute risk reduction (ARR). Extrinsic control over the cognitive distinctions (between [[risk tolerance]] and [[Incentive|reward anticipation]]) adopted by decision makers can occur through altering the presentation of [[relative risk]]s and [[Three degrees of comparison|absolute]] benefits.

People generally prefer the absolute certainty inherent in a positive framing-effect, which offers an assurance of gains. When decision-options appear framed as a ''likely gain'', risk-averse choices predominate.

A shift toward risk-seeking behavior occurs when a decision-maker frames decisions in negative terms, or adopts a negative framing effect.

In [[Decision-making|medical decision making]], [[framing bias]] is best avoided by using absolute measures of efficacy.&lt;ref name="pmid21792695"&gt;{{cite journal|vauthors=Perneger TV, Agoritsas T | title=Doctors and Patients' Susceptibility to Framing Bias: A Randomized Trial | journal=J Gen Intern Med | year= 2011 | volume= 26| issue= 12| pages= 1411–7| pmid=21792695 | doi=10.1007/s11606-011-1810-x | pmc= 3235613| url= }}&lt;/ref&gt;

===Frame-manipulation research===
Researchers have found&lt;ref name="TverskyKahneman1981" /&gt; that framing decision-problems in a positive light generally results in less-risky choices; with negative framing of problems, riskier choices tend to result. According to [[behavioral economics|behavioral economist]]s{{Citation needed|date=November 2007}}:

*positive framing effects (associated with [[risk aversion]]) result from presentation of options as sure (or absolute) gains
*negative framing effects (associated with a preference shift toward choosing riskier options) result from options presented as the relative likelihood of losses

Researchers have found{{Citation needed|date=October 2007}} that framing-manipulation invariably affects subjects, but to varying degrees. Individuals proved risk averse when presented with value-increasing options; but when faced with value decreasing contingencies, they tended towards increased risk-taking. Researchers {{Who|date=September 2008}} found that variations in decision-framing achieved by manipulating the options to represent either a gain or as a loss altered the risk-aversion preferences of decision-makers.

In one study, 57% of the subjects chose a medication when presented with benefits in relative terms, whereas only 14.7% chose a medication whose benefit appeared in absolute terms. Further questioning of the patients suggested that, because the subjects ignored the underlying risk of disease, they perceived benefits as greater when expressed in relative terms.&lt;ref&gt;[http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=8271086&amp;dopt=Abstract The framing effect of relative and absolute risk. [J Gen Intern Med. 1993&amp;#93; - PubMed Result&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;

===Theoretical models===
Researchers have proposed&lt;ref&gt;Chong, D. and Druckman, J. N. (2007): Framing Theory, Annual Review of Political Science, vol. 10&lt;/ref&gt;&lt;ref&gt;Price, V., Tewksburg, D. and Powers, E. (1997): Switching Trains of Thought: The Impact of News Frames on Readers' Cognitive Responses, Communication Research, Vol. 24 No. 5 s. 481 - 506&lt;/ref&gt; various models explaining the '''framing effect''':

*cognitive theories, such as the [[fuzzy-trace theory]], attempt to explain the framing-effect by determining the amount of cognitive processing effort devoted to determining the value of potential gains and losses.
*[[prospect theory]] explains the framing-effect in functional terms, determined by preferences for differing perceived values, based on the assumption that people give a greater weighting to losses than to equivalent gains.
*[[motivation]]al theories explain the framing-effect in terms of [[hedonic]] forces affecting individuals, such as fears and wishes—based on the notion that negative emotions evoked by potential losses usually out-weigh the emotions evoked by hypothetical gains.
*cognitive [[Cost-benefit analysis|cost-benefit]] trade-off theory defines choice as a compromise between desires, either as a preference for a correct decision or a preference for minimized cognitive effort. This model, which dovetails elements of cognitive and motivational theories, postulates that calculating the value of a sure gain takes much less cognitive effort than that required to select a risky gain.

===Neuroimaging===
Cognitive [[neuroscientist]]s have linked the framing-effect to neural activity in the [[amygdala]], and have identified another brain-region, the orbital and medial [[prefrontal cortex]] (OMPFC), that appears to moderate the role of [[emotion]] on decisions. Using [[functional magnetic resonance imaging]] (fMRI) to monitor brain-activity during a financial decision-making task, they observed greater activity in the OMPFC of those research subjects less susceptible to the framing-effect.&lt;ref&gt;{{cite journal | last1 = De Martino | first1 = B. | last2 = Kumaran | first2 = D. | last3 = Seymour | first3 = B. | last4 = Dolan | first4 = R. J. | year = 2006 | title = Frames, biases, and rational decision-making in the human brain | url = | journal = Science | volume = 313 | issue = 5787| pages = 684–687 | doi = 10.1126/science.1128356 | pmid = 16888142 | pmc = 2631940 }}&lt;/ref&gt;

==Framing theory and frame analysis in sociology==
Framing theory and frame analysis provide a broad theoretical approach that analysts have used in [[communication studies]], [[journalism|news]] (Johnson-Cartee, 1995), politics, and [[social movement]]s (among other applications).

According to some sociologists, the "social construction of collective action frames" involves "public discourse, that is, the interface of media discourse and interpersonal interaction; persuasive communication during mobilization campaigns by movement organizations, their opponents and countermovement organizations; and consciousness raising during episodes of collective action."&lt;ref&gt;
Bert Klandermans. 1997. ''The Social Psychology of Protest''. Oxford: Blackwell, page 45
&lt;/ref&gt;

===History===
[[diction|Word-selection]] or diction has been a component of [[rhetoric]] since time immemorial. But most commentators attribute the concept of framing to the work of [[Erving Goffman]] on [[frame analysis]] and point especially to his 1974 book, ''Frame analysis: An essay on the organization of experience''. Goffman used the idea of frames to label "schemata of interpretation" that allow individuals or groups "to locate, perceive, identify, and label" events and occurrences, thus rendering meaning, organizing experiences, and guiding actions.&lt;ref&gt;
Erving Goffman (1974). ''Frame Analysis: An essay on the organization of experience''. Cambridge: Harvard University Press, 1974, page 21.
&lt;/ref&gt;
Goffman's framing concept evolved out of his 1959 work, ''[[The Presentation of Self in Everyday Life]]'', a commentary on the [[management]] of [[Impression management|impression]]s. These works arguably depend on [[Kenneth Boulding]]'s concept of image.&lt;ref&gt;
Kenneth Boulding: ''The Image: Knowledge in Life and Society'', University of Michigan Press, 1956)
&lt;/ref&gt;

===Social movements===
Sociologists have utilized framing to explain the process of [[social movement]]s.&lt;ref name="SnowBenford1988" /&gt;
Movements act as carriers of beliefs and ideologies (compare [[meme]]s). In addition, they operate as part of the process of constructing meaning for participants and opposers (Snow &amp; Benford, 1988). Sociologists deem the mobilization of mass-movements "successful" when the frames projected align with the frames of participants to produce resonance between the two parties. Researchers of framing speak of this process as ''frame re-alignment''.

===Frame-alignment===
Snow and Benford (1988) regard frame-alignment as an important element in social mobilization or movement. They argue that when individual frames become linked in congruency and complementariness, "frame alignment" occurs,&lt;ref name="Snowetal1986"&gt;
Snow, D. A., Rochford, E. B., Worden, S. K., &amp; Benford, R. D. (1986). Frame alignment processes, micromobilization, and movement participation. American Sociological Review, 51, page 464
&lt;/ref&gt;
producing "frame resonance", a catalyst in the process of a group making the transition from one frame to another (although not all framing efforts prove successful). The conditions that affect or constrain framing efforts include the following:

*"The robustness, completeness, and thoroughness of the framing effort". Snow and Benford (1988) identify three core framing-tasks, and state that the degree to which framers attend to these tasks will determine participant mobilization. They characterize the three tasks as the following:
*#diagnostic framing for the identification of a problem and assignment of blame
*#prognostic framing to suggest solutions, strategies, and tactics to a problem
*#motivational framing that serves as a call to arms or rationale for action
*The relationship between the proposed frame and the larger [[belief system|belief-system]]; centrality: the frame cannot be of low hierarchical significance and salience within the larger belief system. Its range and interrelatedness, if the framer links the frame to only one core belief or value that, in itself, has a limited range within the larger belief system, the frame has a high degree of being discounted.
*Relevance of the frame to the realities of the participants; a frame must seem relevant to participants and must also inform them. Empirical credibility or testability can constrain relevancy: it relates to participant experience, and has narrative fidelity, meaning that it fits in with existing cultural myths and narrations.
*[[Protest cycle|Cycles of protest]] (Tarrow 1983a; 1983b); the point at which the frame emerges on the timeline of the current era and existing preoccupations with social change. Previous frames may affect efforts to impose a new frame.

Snow and Benford (1988) propose that once someone has constructed proper frames as described above, large-scale changes in society such as those necessary for social movement can be achieved through frame-alignment.

====Types====
Frame-alignment comes in four forms: frame bridging, frame amplification, frame extension and frame transformation.

#''Frame bridging'' involves the "linkage of two or more ideologically congruent but structurally unconnected frames regarding a particular issue or problem" (Snow et al., 1986, p.&amp;nbsp;467). It involves the linkage of a movement to "unmobilized [''[[sic]]''] sentiment pools or public opinion preference clusters" (p.&amp;nbsp;467) of people who share similar views or grievances but who lack an organizational base.
#''Frame amplification'' refers to "the clarification and invigoration of an interpretive frame that bears on a particular issue, problem, or set of events" (Snow et al., 1986, p.&amp;nbsp;469). This interpretive frame usually involves the invigorating of values or beliefs.
#''Frame extensions'' represent a movement's effort to incorporate participants by extending the boundaries of the proposed frame to include or encompass the views, interests, or sentiments of targeted groups (Snow et al., 1986, p.&amp;nbsp;472).
#''Frame transformation'' becomes necessary when the proposed frames "may not resonate with, and on occasion may even appear antithetical to, conventional lifestyles or rituals and extant interpretive frames" (Snow et al., 1986, p.&amp;nbsp;473).

When this happens, the securing of participants and support requires new values, new meanings and understandings. Goffman (1974, p.&amp;nbsp;43–44) calls this "keying", where "activities, events, and biographies that are already meaningful from the standpoint of some primary framework, in terms of another framework" (Snow et al., 1986, p.&amp;nbsp;474) such that they are seen differently. Two types of frame transformation exist:

#Domain-specific transformations, such as the attempt to alter the status of groups of people, and
#Global interpretive frame-transformation, where the scope of change seems quite radical—as in a change of [[world view|world-views]], total conversions of thought, or uprooting of everything familiar (for example: moving from [[communism]] to [[market capitalism]], or vice versa; [[religious conversion]], etc.).

==Frame analysis as rhetorical criticism==
Although the idea of language-framing had been explored earlier by [[Kenneth Burke]] (terministic screens), political communication researcher [[Jim A. Kuypers]] first published work advancing [[frame analysis]] (framing analysis) as a rhetorical perspective in 1997. His approach begins inductively by looking for themes that persist across time in a text (for Kuypers, primarily news narratives on an issue or event) and then determining how those themes are framed. Kuypers's work begins with the assumption that frames are powerful rhetorical entities that "induce us to filter our perceptions of the world in particular ways, essentially making some aspects of our multi-dimensional reality more noticeable than other aspects. They operate by making some information more salient than other information...."&lt;ref&gt;
Jim A. Kuypers, "Framing Analysis" in ''Rhetorical Criticism: Perspectives in Action'', edited by J.A. Kuypers,  Lexington Press, 2009. p. 181.&lt;/ref&gt;

In his 2009 essay "Framing Analysis" in ''Rhetorical Criticism: Perspectives in Action''&lt;ref&gt;
''Rhetorical Criticism: Perspectives in Action''&lt;/ref&gt; and his 2010 essay "Framing Analysis as a Rhetorical Process",&lt;ref&gt;Kuypers, Jim A. "Framing Analysis as a Rhetorical Process," Doing News Framing Analysis.  Paul D'Angelo and  Jim A. Kuypers, eds. (New York: Routeledge, 2010).&lt;/ref&gt; Kuypers offers a detailed conception for doing framing analysis from a rhetorical perspective. According to Kuypers, "Framing is a process whereby communicators, consciously or unconsciously, act to construct a point of view that encourages the facts of a given situation to be interpreted by others in a particular manner. Frames operate in four key ways: they define problems, diagnose causes, make moral judgments, and suggest remedies. Frames are often found within a narrative account of an issue or event, and are generally the central organizing idea."&lt;ref&gt;Jim A. Kuypers, ''Bush's War: Media Bias and Justifications for War in a Terrorist Age'', Rowman &amp; Littlefield Publishers, Inc., 2009.&lt;/ref&gt; Kuypers's work is based on the premise that framing is a rhetorical process and as such it is best examined from a rhetorical point of view. Curing the problem is not rhetorical and best left to the observer.

==Rhetorical framing in politics==

===Semiotic analysis of 2016 Republican primaries===

Framing is used to construct, refine, and deliver messages. Framing in politics is essential to getting your message across to the masses. Frames are mental structures that shape the way we view the world (Lakoff, Don't Think of an Elephant! Know Your Values and Frame the Debate 2004).&lt;ref name=Lakoff&gt;In Don't Think of an Elephant! Know Your Values and Frame the Debate, by George Lakoff, 144. Chelsea Green Publishing, 2004.&lt;/ref&gt; Reframing is used particularly well by both conservatives and liberals in the political arena, so well that they have news anchors and commentators discussing the ideas, supplied phrases and framing (Lakoff, Don't Think of an Elephant! Know Your Values and Frame the Debate 2004).&lt;ref name=Lakoff/&gt; 

The neoconservatives in the Bush Administration and the Pentagon viewed the 9/11 attack as an opportunity to go to war in the Middle East and finally take out Saddam Hussain. The Bush administration sold the war by convincing the nation that Iraq had WMDs and collected supportive evidence that they had Secretary of State Colin Powell present at the United Nations. The War on Terror was the label assigned by the Bush administration to its national security policy, launched in response to the attacks of 9/11 (Lewis 2009).&lt;ref name="Lewis 2009"&gt;Lewis, Stephen D. Reese and Seth C. "Framing the War on Terror The internalization of policy in the US press." Journalism, 2009: 777–797.&lt;/ref&gt; The cultural construction and political rationale supporting this slogan represent a powerful organizing principle that has become a widely accepted framing, laying the groundwork for the invasion of Iraq (Lewis 2009).&lt;ref name="Lewis 2009" /&gt; 

The challenge of political violence has grown with new means of global coordination and access to weapons of mass destruction. The Bush administration's response to this threat, following the now iconic policy reference point of 11 September 2001, has had far-ranging implications for national security strategy, relations with the world community, and civil liberties (Lewis 2009).&lt;ref name="Lewis 2009" /&gt; Labeled the 'War on Terror', the policy was framed within a phrase now part of the popular lexicon, becoming a natural and instinctive shorthand. More than phrases though, frames are 'organizing principles that are socially shared and persistent over time, that work symbolically to meaningfully structure the social world' (Lewis 2009).&lt;ref name="Lewis 2009" /&gt; As a particularly powerful organizing principle, the War on Terror created a supportive political climate for what has been called the biggest US foreign policy blunder in modern times: the invasion of Iraq. Thus, in the scope and consequences of its policy-shaping impact, the War on Terror may be the most important frame in recent memory. (Lewis 2009)

In the now well-known evolution of the administration's policy, influential neoconservatives within the administration had advocated regime change in Iraq for some time, but the events of 9/11 gave them a compelling way to fast-track their ideas and justify a new policy of preemptive war, fist in Afghanistan and then in Iraq. The National Strategy for Combating Terrorism defined the attacks of 9/11 as 'acts of war against the United States of America and its allies, and against the very idea of civilized society'. It identified the enemy as terrorism, an 'evil' threatening our 'freedoms and our way of life. The related National Security Strategy of the United States of America clearly divides 'us' from 'them', linking terrorism to rogue states that 'hate the United States and everything for which it stands (Lewis 2009).&lt;ref name="Lewis 2009" /&gt; Presenting himself as God's agent, Bush's Manichean struggle pitted the USA and its leader against the evildoers (Lewis 2009).&lt;ref name="Lewis 2009" /&gt; 

This argument is being played out in the 2016 Republican primaries, especially by Donald Trump. Trump has portrayed the Syrian refugees as foot soldiers for ISIS, coming to America to kill us in our main streets. Trump's rhetoric appears to be working; many middle class Americans are consuming his rhetoric.{{citation needed|date=June 2016}} The Americans that are supporting Trump and the Republicans in general, many of them are working class and the Republican agenda although it appears to be in their favor it is not. Framing their message to say one thing and mean something completely different is what the conservatives have become masters at. The 2016 Republican primary has been a knock down fight since it started in August 2015. Donald Trump has approached this contest as if Vince McMahon were the promoter and the rest of the field are a bunch of jobbers (persons who are paid to lose). Trump was inducted into the World Wrestling Entertainment (WWE) Hall of Fame in 2003. Even his attacks on Megan Kelly from FOX News are straight out of the WWE's playbook. Roland Barthes analyzed wrestling and boxing in his book ''Mythologies''.
&lt;blockquote&gt;''This public knows very well the distinction between wrestling and boxing; it knows that boxing is a Jansenist sport, based on a demonstration of excellence. One can bet on the outcome of a boxing-match: with wrestling, it would make no sense. A boxing- match is a story which is constructed before the eyes of the spectator; in wrestling, on the contrary, it is each moment which is intelligible, not the passage of time... The logical conclusion of the contest does not interest the wrestling-fan, while on the contrary a boxing-match always implies a science of the future. In other words, wrestling is a sum of spectacles, of which no single one is a function: each moment imposes the total knowledge of a passion which rises erect and alone, without ever extending to the crowning moment of a result.'' (Legum 2015)&lt;ref&gt;{{cite web|author=Legum, Judd|title=This French Philosopher Is The Only One Who Can Explain The Donald Trump Phenomenon|work=thinkprogress.org|date=September 14, 2015|url=http://thinkprogress.org/politics/2015/09/14/3701084/donald-trump/|accessdate=April 23, 2016}}&lt;/ref&gt;&lt;/blockquote&gt;

==Applications==

===Finance===
Preference reversals and other associated phenomena are of wider relevance within behavioural economics, as they contradict the predictions of [[rational choice]], the basis of traditional economics. Framing biases affecting investing, lending, borrowing decisions make one of the themes of [[behavioral finance]].

===Law===
[[Edward Zelinsky]] has shown that framing effects can explain some observed behaviors of legislators.&lt;ref&gt;[[Edward Zelinsky|Zelinsky, Edward A.]]. 2005. Do Tax Expenditures Create Framing Effects? Volunteer Firefighters, Property Tax Exemptions, and the Paradox of Tax Expenditure Analysis. ''Virginia Tax Review'' 24. [http://www.allbusiness.com/accounting/3584666-1.html]&lt;/ref&gt;

===Media===
The role framing plays in the effects of media presentation has been widely discussed, with the central notion that associated perceptions of factual information can vary based upon the presentation of the information.

====News media examples====
In ''Bush's War: Media Bias and Justifications for War in a Terrorist Age,''&lt;ref&gt;Jim A. Kuypers, ''Bush's War: Media Bias and Justifications for War in a Terrorist Age'' (Lanham, MD: Rowman and Littlefield, 2006),&lt;/ref&gt;[[Jim A. Kuypers]] examined the differences in framing of the war on terror between the Bush administration and the U.S. Mainstream News between 2001 and 2005.  Kuypers looked for common themes between presidential speeches and press reporting of those speeches, and then determined how the president and the press had framed those themes.  By using a rhetorical version of framing analysis, Kuypers determined that the U.S. news media advanced frames counter to those used by the Bush administration:

{{quote| the press actively contested the framing of the War on Terror as early as eight weeks following 9/11. This finding stands apart from a collection of communication literature suggesting the press supported the President or was insufficiently critical of the President's efforts after 9/11. To the contrary, when taking into consideration how themes are framed, [Kuypers] found that the news media framed its response in such a way that it could be viewed as supporting the idea of some action against terrorism, while concommitantly opposing the initiatives of the President.  The news media may well relay what the president says, but it does not necessarily follow that it is framed in the same manner; thus, an echo of the theme, but not of the frame.  The present study demonstrates, as seen in Table One [below], that shortly after 9/11 the news media was beginning to actively counter the Bush administration and beginning to leave out information important to understanding the Bush Administration's conception of the War on Terror.  In sum, eight weeks after 9/11, the news media was moving beyond reporting political opposition to the President—a very necessary and invaluable press function—and was instead actively choosing themes, and framing those themes, in such a way that the President's focus was opposed, misrepresented, or ignored.&lt;ref&gt;Jim A. Kuypers, Stephen D. Cooper, Matthew T. Althouse, "George W. Bush, The American Press, and the Initial Framing of the War on Terror after 9/11," ''The George W. Bush Presidency: A Rhetorical Perspective,'' Robert E. Denton, ed. (Lanham, MD: Lexington Books, 2012), 89-112.&lt;/ref&gt;}}

Table One: Comparison of President and News Media Themes and Frames 8 Weeks after 9/11&lt;ref&gt;Jim A. Kuypers, Stephen D. Cooper, Matthew T. Althouse, "George W. Bush, "The American Press, and the Initial Framing of the War on Terror after 9/11," ''The George W. Bush Presidency: A Rhetorical Perspective,'' Robert E. Denton, ed. (Lanham, MD: Lexington Books, 2012), 105.&lt;/ref&gt;

{| class="wikitable"
|-
! Themes !! President's Frame !! Press Frame
|-
| Good v. Evil || Struggle of good and evil || Not mentioned
|-
| Civilization v. Barbarism || Struggle of civilization v. barbarism || Not mentioned
|-
| Nature of Enemy ||Evil, implacable, murderers || Deadly, indiscriminant

Bush Administration
|-
| Nature of War || Domestic/global/enduring

War
 || Domestic/global/longstanding

War or police action
|-
| Similarity to Prior Wars || Different Kind of War || WWII or Vietnam?
|-
| Patience || Not mentioned || Some, but running out
|-
| International Effort || Stated || Minimally reported
|-

|}

In 1991 Robert M. Entman published findings&lt;ref&gt;Entman, R. M. (1991), Symposium Framing U.S. Coverage of International News: Contrasts in Narratives of the KAL and Iran Air Incidents. Journal of Communication, 41: 6–27. {{DOI|10.1111/j.1460-2466.1991.tb02328.x}}&lt;/ref&gt; surrounding the differences in media coverage between [[Korean Air Lines Flight 007]] and [[Iran Air Flight 655]]. After evaluating various levels of media coverage, based on both amount of airtime and pages devoted to similar events, Entman concluded that the frames the events were presented in by the media were drastically different:

{{quote| By de-emphasizing the agency and the victims and by the choice of graphics and adjectives, the news stories about the U.S. downing of an Iranian plane called it a technical problem, while the Soviet downing of a Korean jet was portrayed as a moral outrage… [T]he contrasting news frames employed by several important U.S. media outlets in covering these two tragic misapplications of military force. For the first, the frame emphasized the moral bankruptcy and guilt of the perpetrating nation, for the second, the frame de-emphasized the guilt and focused on the complex problems of operating military high technology. }}

Differences in coverage amongst various media outlets:

{| class="wikitable"
|-
! Amounts of Media coverage dedicated to each event !! Korean Air !! Iran Air
|-
| Time Magazine and Newsweek || 51 pages || 20 pages
|-
| CBS || 303 minutes || 204 minutes
|-
| New York Times || 286 stories || 102 stories
|}

In 1988 Irwin Levin and Gary Gaeth did a study on the effects of framing attribute information on consumers before and after consuming a product (1988). In this study they found that in a study on beef. People who ate beef labeled as 75% lean rated it more favorably than people whose beef was labelled 25% fat.

===Politics===

Linguist and rhetoric scholar [[George Lakoff]] argues that, in order to persuade a political audience of one side of and argument or another, the facts must be presented through a rhetorical frame.  It is argued that, without the frame, the facts of an argument become lost on an audience, making the argument less effective.  The rhetoric of politics uses framing to present the facts surrounding an issue in a way that creates the appearance of a problem at hand that requires a solution.  Politicians using framing to make their own solution to an exigence appear to be the most appropriate compared to that of the opposition.&lt;ref name="van der Pas" /&gt;  Counter-arguments become less effective in persuading an audience once one side has framed an argument, because it is argued that the opposition then has the additional burden of arguing the frame of the issue in addition to the issue itself.

Framing a political issue, a political party or a political opponent is a [[strategy|strategic]] goal in [[politics]], particularly in the [[United States of America]]. Both the [[Democratic Party (United States)|Democratic]] and [[Republican Party (United States)|Republican]] political parties compete to successfully harness its power of persuasion. According to the ''[[New York Times]]'':

{{quote|Even before the [[United States presidential election, 2004|election]], a new political word had begun to take hold of the party, beginning on the [[West Coast of the United States|West Coast]] and spreading like a virus all the way to the inner offices of the [[United States Capitol|Capitol]]. That word was 'framing.' Exactly what it means to 'frame' issues seems to depend on which Democrat you are talking to, but everyone agrees that it has to do with choosing the language to define a debate and, more important, with fitting individual issues into the contexts of broader story lines.|&lt;ref name="framingwars"&gt;
[http://www.nytimes.com/2005/07/17/magazine/17DEMOCRATS.html?pagewanted=1&amp;ei=5070&amp;en=e3e686efd4fa97c5&amp;ex=1183608000 The Framing Wars. ''[[New York Times]]'' 17 July 2005]&lt;/ref&gt;}}

Because framing has the ability to alter the public's perception, politicians engage in battles to determine how issues are framed. Hence, the way the issues are framed in the media reflects who is winning the battle. For instance, according to Robert Entman, professor of Communication at George Washington University, in the build-up to the Gulf War the conservatives were successful in making the debate whether to attack sooner or later, with no mention of the possibility of not attacking. Since the media picked up on this and also framed the debate in this fashion, the conservatives won.&lt;ref name="EntmanRobertTree" /&gt;

One particular example of [[George Lakoff|Lakoff's]] work that attained some degree of fame was his advice to rename&lt;ref&gt;[[Walter Olson]], [http://www.overlawyered.com/2005/07/some_framing_advice.html Overlawyered weblog], 2005-07-18&lt;/ref&gt; [[trial lawyer]]s (unpopular in the United States) as "public protection attorneys". Though Americans have not generally adopted this suggestion, the [[Association of Trial Lawyers of America]] did rename themselves the "American Association of Justice", in what the [[Chamber of Commerce]] called an effort to hide their identity.&lt;ref&gt;[[Al Kamen]], [http://www.washingtonpost.com/wp-dyn/content/article/2007/01/16/AR2007011601429_pf.html "Forget Cash -- Lobbyists Should Set Support for Lawmakers in Stone"], ''[[Washington Post]]'', 2007-01-17&lt;/ref&gt;

The ''[[New York Times]]'' depicted similar intensity among Republicans:

{{quote|In one recent memo, titled 'The 14 Words Never to Use,' [[Frank Luntz|[Frank] Luntz]] urged conservatives to restrict themselves to phrases from what he calls ... the 'New American Lexicon.' Thus, a smart Republican, in Luntz's view, never advocates '[[oil drilling|drilling for oil]]'; he prefers 'exploring for energy.' He should never criticize the 'government,' which cleans our streets and pays our firemen; he should attack '[[Washington, D.C.|Washington]],' with its ceaseless thirst for taxes and regulations. 'We should never use the word [[outsourcing]],' Luntz wrote, 'because we will then be asked to defend or end the practice of allowing companies to ship American jobs overseas.'|&lt;ref name="framingwars"/&gt;}}

From a political perspective, framing has widespread consequences. For example, the concept of framing links with that of [[agenda setting theory|agenda-setting]]: by consistently invoking a particular frame, the framing party may effectively control discussion and perception of the issue. [[Sheldon Rampton]] and [[John Stauber]] in ''[[Trust Us, We're Experts]]'' illustrate how [[Public Relations|public-relations]] (PR) firms often use language to help frame a given issue, structuring the questions that then subsequently emerge. For example, one firm advises clients to use "bridging language" that uses a strategy of answering questions with specific terms or ideas in order to shift the discourse from an uncomfortable topic to a more comfortable one.&lt;ref&gt;
Rampton, Sheldon and Stauber, John. ''Trust Us, We're Experts!'' Putnam Publishing, New York, NY, 2002. Page 64.&lt;/ref&gt;
Practitioners of this strategy might attempt to draw attention away from one frame in order to focus on another. As Lakoff notes, "On the day that [[George W. Bush]] took office, the words "tax relief" started coming out of the White House."&lt;ref name="Lakoff2004"&gt;{{Cite book|last=Lakoff|first=George|title=Don't think of an elephant!: know your values and frame the debate|year=2004|publisher=Chelsea Green Publishing|isbn=978-1-931498-71-5|page=56}}&lt;/ref&gt;
By refocusing the structure away from one frame ("tax burden" or "tax responsibilities"), individuals can set the agenda of the questions asked in the future.

[[Cognitive linguistics|Cognitive linguists]] point to an example of framing in the phrase "[[tax cut|tax relief]]". In this frame, use of the concept "relief" entails a concept of (without mentioning the benefits resulting from) taxes putting strain on the citizen:

{{quote|The current tax code is full of inequities. Many single moms face higher marginal tax rates than the wealthy. Couples frequently face a higher tax burden after they marry. The majority of Americans cannot deduct their charitable donations. Family farms and businesses are sold to pay the death tax. And the owners of the most successful small businesses share nearly half of their income with the government. President Bush's tax cut will greatly reduce these inequities. It is a fair plan that is designed to provide tax relief to everyone who pays income taxes.|&lt;ref&gt;[http://georgewbush-whitehouse.archives.gov/news/reports/taxplan.html The President's Agenda for Tax Relief] retrieved 3 July 2007.&lt;/ref&gt;}}

Alternative frames may emphasize the concept of taxes as a source of infrastructural support to businesses:

{{quote|The truth is that the wealthy have received more from America than most Americans—not just wealth but the infrastructure that has allowed them to amass their wealth: banks, the Federal Reserve, the stock market, the Securities and Exchange Commission, the legal system, federally sponsored research, patents, tax supports, the military protection of foreign investments, and much much more. American taxpayers support the infrastructure of wealth accumulation. It is only fair that those who benefit most should pay their fair share.|&lt;ref&gt;[http://www.cognitivepolicyworks.com/resource-center/planning-tools/framing-tutorials/simple-framing/ Cognitive Policy Works/Rockridge Institute: Simple Framing]&lt;/ref&gt;}}

Frames can limit debate by setting the vocabulary and [[metaphor]]s through which participants can comprehend and discuss an issue. They form a part not just of political discourse, but of [[cognition]]. In addition to generating new frames, politically oriented framing research aims to increase public awareness of the connection between framing and reasoning.

====Examples====
*The initial response of the [[George W. Bush administration|Bush administration]] to the [[September 11, 2001 attacks|assault of September 11, 2001]] was to frame the acts of [[Counterterrorism|terror]] as [[crime]]. This framing was replaced within hours by a war metaphor, yielding the "[[War on Terrorism|War on Terror]]". The difference between these two framings is in the implied response. Crime connotes bringing criminals to justice, putting them on trial and sentencing them, whereas as [[war]] implies enemy territory, military action and war powers for government.&lt;ref name="Lakoff2004" /&gt;&lt;ref&gt;{{Cite journal|last=Zhang|first=Juyan |title=Beyond anti-terrorism: Metaphors as message strategy of post-September-11 U.S. public diplomacy |journal=Public Relations Review|year=2007|volume=33|issue=1|pages=31–39|doi=10.1016/j.pubrev.2006.11.006}}&lt;/ref&gt;
*The term "escalation" to describe an increase in American troop-levels in [[Iraq]] in 2007 implied that the United States deliberately increased the scope of conflict in a provocative manner and possibly implies that U.S. strategy entails a long-term military presence in Iraq, whereas [[Iraq War troop surge of 2007|"surge"]] framing implies a powerful but brief, transitory increase in intensity.&lt;ref&gt;[http://www.alternet.org/waroniraq/48059/ "It's Escalation, Stupid." ''Alternet''] retrieved 3 July 2007&lt;/ref&gt;
*The "bad apple" frame, as in the proverb "one bad [[apple]] spoils the barrel". This frame implies that removing one underachieving or corrupt official from an [[institution]] will solve a given problem; an opposing frame presents the same problem as systematic or structural to the institution itself—a source of infectious and spreading rot.&lt;ref&gt;[http://www.huffingtonpost.com/bruce-budner/the-rumsfeld-dilemma-dem_b_29550.html "The Rumsfeld Dilemma: Demand an Exit Strategy, Not a Facelift"] by Bruce Budner, in ''The Huffington Post'' 15 September 2006&lt;/ref&gt;
*The "[[taxpayers]] money" frame, rather than [[government spending|public or government funds]], which implies that individual taxpayers have a claim or right to set [[government policy]] based upon their payment of tax rather than their status as [[citizen]]s or [[voters]] and that taxpayers have a right to control public funds that are the shared property of all citizens and also privileges individual self-interest above group interest.{{Citation needed|date=April 2009}}
*The "collective property" frame, which implies that property owned by individuals is really owned by a collective in which those individuals are members. This collective can be a territorial one, such as a nation, or an abstract one that does not map to a specific territory.
*Program-names that may describe only the intended effects of a program but may also imply their effectiveness. These include the following:
**"[[Foreign aid]]"&lt;ref&gt;[http://hij.sagepub.com/cgi/content/abstract/12/2/120 "Is It All in a Word? The Effect of Issue Framing on Public Support for U.S. Spending on HIV/AIDS in Developing Countries."] by Sara Bleich. Retrieved 2007-07-03
&lt;/ref&gt; (which implies that spending money will aid foreigners, rather than harm them)
**"[[Social security]]" (which implies that the program can be relied on to provide security for a society)
**"[[Stabilisation policy]]" (which implies that a policy will have a stabilizing effect).
* Based on [[opinion polling]] and [[focus group]]s, [[ecoAmerica]], a nonprofit environmental marketing and messaging firm, has advanced the position that [[global warming]] is an ineffective framing due to its identification as a leftist advocacy issue. The organization has suggested to government officials and environmental groups that alternate formulations of the issues would be more effective.&lt;ref&gt;[http://www.nytimes.com/2009/05/02/us/politics/02enviro.html "Seeking to Save the Planet, With a Thesaurus"] article by John M. Broder in ''[[The New York Times]]'' May 1, 2009&lt;/ref&gt;
*In her 2009 book ''Frames of War'', [[Judith Butler]] argues that the justification within liberal-democracies for war, and atrocities committed in the course of war, (referring specifically to the current war in Iraq and to [[Abu Ghraib torture and prisoner abuse|Abu Ghraib]] and [[Guantanamo Bay detention camp|Guantanamo Bay]]) entails a framing of the (especially Muslim) 'other' as pre-modern/primitive and ultimately not human in the same way as citizens within the liberal order.&lt;ref&gt;Butler, J. (2009), ''Frames of War'', London: Verso.&lt;/ref&gt;

==See also==
{{div col|3}}
*[[Anecdotal value]]
*[[Alternative facts]]
*[[Argumentation theory]]
*[[Bias]]
*[[Choice architecture]]
*[[Code word (figure of speech)]]
*[[Communication theory]]
*[[Connotation]]
*[[Cultural bias]]
*[[Decision making]]
*[[Definition of the situation]]
*[[Demagoguery]]
*[[Domain of discourse]]
*[[Echo chamber (media)]]
*[[Fallacy of many questions]]
*[[Figure of speech]]
*[[Filter bubble]]
*[[Freedom of speech]]
*[[Freedom of the press|Free press]]
*[[Idea networking]]
*[[Language and thought]]
*[[Meme]]
*[[Newspeak]]
&lt;!--* [[Political frame]] - self ref after merge/redirect--&gt;
*[[Power word]]
*[[Overton window]]
*[[Political correctness]]
*[[Rhetorical device]]
*[[Semantics]]
*[[Semantic domain]]
*[[Social heuristics]]
*[[Sophism]]
*[[Spin doctor]]
*[[Stovepiping]]
*''[[Thought Reform (book)]]''
*[[Trope (linguistics)|Trope]]
*[[Unspeak]] (book)
*[[Virtue word]]
{{div col end}}

==References==
{{reflist|colwidth=30em}}
Levin, Irwin P., and Gary J. Gaeth. "How Consumers Are Affected By The Framing Of Attribute Information Before And After Consuming The Product." Journal of Consumer Research 15.3 (1988): 374. Print.

==Further reading==
*[[Bernard Baars|Baars, B]]. ''A cognitive theory of consciousness'', NY: [[Cambridge University Press]] 1988, ISBN 0-521-30133-5.
*[[Kenneth E. Boulding|Boulding, Kenneth E.]] (1956). The Image: Knowledge in Life and Society. Michigan University Press.
* {{cite journal | last1 = Carruthers | first1 = P. | authorlink = Peter Carruthers (philosopher) | year = 2003 | title = On Fodor's Problem | url = | journal = Mind and Language | volume = 18 | issue = 5| pages = 502–523 | doi = 10.1111/1468-0017.00240 }}
*Clark, A. (1997), Being There: Putting Brain, Body, and World Together Again, Cambridge, MA: MIT Press.
*Cutting, Hunter and Makani Themba Nixon (2006). Talking the Walk: A Communications Guide for Racial Justice: AK Press
*[[Daniel Dennett|Dennett, D.]] (1978), Brainstorms, Cambridge, MA: MIT Press.
*Fairhurst, Gail T. and Sarr, Robert A. 1996. ''The Art of Framing: Managing the Language of Leadership.'' USA: Jossey-Bass, Inc.
*Feldman, Jeffrey. (2007), ''Framing the Debate: Famous Presidential Speeches and How Progressives Can Use Them to Control the Conversation (and Win Elections)''. Brooklyn, NY: Ig Publishing.
*[[Jerry Fodor|Fodor, J.A.]] (1983), The Modularity of Mind, Cambridge, MA: MIT Press.
*Fodor, J.A. (1987), "Modules, Frames, Fridgeons, Sleeping Dogs, and the Music of the Spheres", in Pylyshyn (1987).
*Fodor, J.A. (2000), The Mind Doesn't Work That Way, Cambridge, MA: MIT Press.
*Ford, K.M. &amp; Hayes, P.J. (eds.) (1991), Reasoning Agents in a Dynamic World: The Frame Problem, New York: JAI Press.
*[[Erving Goffman|Goffman, Erving]]. 1974. ''Frame Analysis: An Essay on the Organization of Experience.'' London: Harper and Row.
*Goffman, E. (1974). Frame Analysis. Cambridge: Harvard University Press.
*Goffman, E. (1959). Presentation of Self in Everyday Life. New York: Doubleday.
*Goodman, N. (1954), Fact, Fiction, and Forecast, Cambridge, MA: Harvard University Press.
*{{cite journal | last1 = Hanks | first1 = S. | last2 = McDermott | first2 = D. | year = 1987 | title = Nonmonotonic Logic and Temporal Projection | url = | journal = Artificial Intelligence | volume = 33 | issue = 3| pages = 379–412 | doi = 10.1016/0004-3702(87)90043-9 }}
*Haselager, W.F.G. (1997). Cognitive science and folk psychology: the right frame of mind. London: Sage
*{{cite journal | last1 = Haselager | first1 = W.F.G. | last2 = Van Rappard | first2 = J.F.H. | year = 1998 | title = Connectionism, Systematicity, and the Frame Problem | url = | journal = Minds and Machines | volume = 8 | issue = 2| pages = 161–179 | doi = 10.1023/A:1008281603611 }}
*Hayes, P.J. (1991), "Artificial Intelligence Meets David Hume: A Reply to Fetzer", in Ford &amp; Hayes (1991).
*Heal, J. (1996), "Simulation, Theory, and Content", in Theories of Theories of Mind, eds. P. Carruthers &amp; P. Smith, Cambridge: Cambridge University Press, pp.&amp;nbsp;75–89.
*Johnson-Cartee, K. (2005). News narrative and news framing: Constructing political reality. Lanham, MD: Rowman &amp; Littlefield.
*[[Diana Kendall|Kendall, Diana]], ''Sociology In Our Times'', Thomson Wadsworth, 2005, ISBN 0-534-64629-8 [https://books.google.com/books?vid=ISBN0534646298&amp;id=kzU-gtx2VfoC&amp;pg=PA531&amp;lpg=PA531&amp;dq=%22Resource+Mobilization%22&amp;sig=NgTePMtdl2stO7V2FofPqeZuP5I&amp;hl=en Google Print, p.531]
*Klandermans, Bert. 1997. ''The Social Psychology of Protest.'' Oxford: Blackwell.
*[[George Lakoff|Lakoff, G.]] &amp; Johnson, M. (1980), Metaphors We Live By, Chicago: University of Chicago Press.
*Leites, N. &amp; Wolf, C., Jr. (1970). Rebellion and authority. Chicago: Markham Publishing Company.
*{{cite journal | last1 = Martino | first1 = De | year = 2006 | title = Frames, Biases, and Rational Decision-Making in the Human Brain | url = | journal = Science | volume = 313 | issue = 5787| pages = 684–687 | doi = 10.1126/science.1128356 | pmid = 16888142 | last2 = Kumaran | first2 = D | last3 = Seymour | first3 = B | last4 = Dolan | first4 = RJ | pmc = 2631940 }}
*McAdam, D., McCarthy, J., &amp; Zald, M. (1996). Introduction: Opportunities, Mobilizing Structures, and Framing Processes—Toward a Synthetic, Comparative Perspective on Social Movements. In D. McAdam, J. McCarthy &amp; M. Zald (Eds.), Comparative Perspectives on Social Movements; Political Opportunities, Mobilizing Structures, and Cultural Framings (pp.&amp;nbsp;1–20). New York: Cambridge University Press.
*McCarthy, J. (1986), "Applications of Circumscription to Formalizing Common Sense Knowledge", [[Artificial Intelligence (journal)|Artificial Intelligence]], vol. 26(3), pp.&amp;nbsp;89–116.
*McCarthy, J. &amp; Hayes, P.J. (1969), "Some Philosophical Problems from the Standpoint of Artificial Intelligence", in Machine Intelligence 4, ed. D.Michie and B.Meltzer, Edinburgh: Edinburgh University Press, pp.&amp;nbsp;463–502.
*McDermott, D. (1987), "We've Been Framed: Or Why AI Is Innocent of the Frame Problem", in Pylyshyn (1987).
*Mithen, S. (1987), ''The Prehistory of the Mind'', London: Thames &amp; Hudson.
*{{cite journal | last1 = Nelson | first1 = T. E. | last2 = Oxley | first2 = Z. M. | last3 = Clawson | first3 = R. A. | year = 1997 | title = Toward a psychology of framing effects | url = | journal = Political Behavior | volume = 19 | issue = 3| pages = 221–246 | doi = 10.1023/A:1024834831093 }}
*{{cite journal | last1 = Pan | first1 = Z. | last2 = Kosicki | first2 = G. M. | year = 1993 | title = Framing analysis: An approach to news discourse | url = | journal = Political Communication | volume = 10 | issue = 1| pages = 55–75 | doi = 10.1080/10584609.1993.9962963 }}
*Pan. Z. &amp; Kosicki, G. M. (2001). Framing as a strategic action in public deliberation. In S. D. Reese, O. H. Gandy, Jr., &amp; A. E. Grant (Eds.), Framing public life: Perspectives on media and our understanding of the social world, (pp.&amp;nbsp;35–66). Mahwah, NJ: Lawrence Erlbaum Associates.
*Pan, Z. &amp; Kosicki, G. M. (2005). Framing and the understanding of citizenship. In S. Dunwoody, L. B. Becker, D. McLeod, &amp; G. M. Kosicki (Eds.), Evolution of key mass communication concepts, (pp.&amp;nbsp;165–204). New York: Hampton Press.
*[[Zenon Pylyshyn|Pylyshyn, Zenon W.]] (ed.) (1987), The Robot's Dilemma: The Frame Problem in Artificial Intelligence, Norwood, NJ: Ablex.
*Stephen D. Reese, Oscar H. Gandy and August E. Grant. (2001). [https://books.google.com/books?id=I0BlAAAAMAAJ&amp;q=journalist+subject:%22Reporters+and+reporting%22&amp;dq=journalist+subject:%22Reporters+and+reporting%22&amp;lr=&amp;client=firefox-a&amp;pgis=1 ''Framing Public Life: Perspectives on Media and Our Understanding of the Social World.''] Maywah, New Jersey: Lawrence Erlbaum. ISBN 978-0-8058-3653-0
*Russell, S. &amp; Wefald, E. (1991), Do the Right Thing: Studies in Limited Rationality, Cambridge, MA: MIT Press.
*{{cite journal | last1 = Scheufele | first1 =  DA| authorlink = Scheufele | last2 = Dietram | first2 = A.  | year = 1999 | title = Framing as a theory of media effects | url = | journal = [[Journal of Communication]] | volume = 49 | issue = 1| pages = 103–122 | doi = 10.1111/j.1460-2466.1999.tb02784.x }}
*Shanahan, Murray P. (1997), ''Solving the Frame Problem: A Mathematical Investigation of the Common Sense Law of Inertia'', Cambridge, MA: MIT Press. ISBN 0-262-19384-1
*Shanahan, Murray P. (2003), "The Frame Problem", in ''The Macmillan Encyclopedia of Cognitive Science'', ed. L.Nadel, Macmillan, pp.&amp;nbsp;144–150.
*[[Herbert A. Simon|Simon, Herbert]] (1957), ''Models of Man, Social and Rational: Mathematical Essays on Rational Human Behavior in a Social Setting'', New York: John Wiley. {{OCLC|165735}}
*{{cite journal | last1 = Snow | first1 = D. A. | last2 = Benford | first2 = R. D. | year = 1988 | title = Ideology, frame resonance, and participant mobilization | url = | journal = International Social Movement Research | volume = 1 | issue = | pages = 197–217 }}
*{{cite journal | last1 = Snow | first1 = D. A. | last2 = Rochford | first2 = E. B. | last3 = Worden | first3 = S. K. | last4 = Benford | first4 = R. D. | year = 1986 | title = Frame alignment processes, micromobilization, and movement participation | url = | journal = American Sociological Review | volume = 51 | issue = 4| pages = 464–481 | doi = 10.2307/2095581 }}
*{{cite journal | last1 = Sperber | first1 = D. | last2 = Wilson | first2 = D. | year = 1996 | title = Fodor's Frame Problem and Relevance Theory | url = | journal = Behavioral and Brain Sciences | volume = 19 | issue = 3| pages = 530–532 | doi = 10.1017/S0140525X00082030 }}
*[[Sidney Tarrow|Tarrow, S.]] (1983a). "Struggling to Reform: social Movements and policy change during cycles of protest". Western Societies Paper No. 15. Ithaca, NY: Cornell University.
*Tarrow, S. (1983b). "Resource mobilization and cycles of protest: Theoretical reflections and comparative illustrations". Paper presented at the Annual Meeting of the [[American Sociological Association]], Detroit, August 31–September 4.
*Triandafyllidou, A. and Fotiou, A. (1998), [http://www.socresonline.org.uk/3/1/2.html "Sustainability and Modernity in the European Union: A Frame Theory Approach to Policy-Making"], Sociological Research Online, vol. 3, no. 1.
*[[Charles Tilly|Tilly, C.]], Tilly, L., &amp; Tilly, R. (1975). ''The rebellious century, 1830–1930''. Cambridge, MA: Cambridge University Press.
*Turner, R. H., &amp; Killian, L. M. (1972). ''Collective Behavior''. Englewood Cliffs, NJ: Prentice-Hall.
* "Rational Choice and the Framing of Decisions", A.Tversky, D.Kahneman, ''Journal of Business'', 1986, vol.59, no.4, pt.2.
*{{cite journal | last1 = Wilkerson | first1 = W.S. | year = 2001 | title = Simulation, Theory, and the Frame Problem | url = | journal = [[Philosophical Psychology (journal)|Philosophical Psychology]] | volume = 14 | issue = 2| pages = 141–153 | doi = 10.1080/09515080120051535 }}
*[[Charles Arthur Willard|Willard, Charles Arthur]]. ''Liberalism and the Social Grounds of Knowledge'' Chicago: University of Chicago Press, 199

==External links==
*[http://www.nytimes.com/2005/07/17/magazine/17DEMOCRATS.html The Framing Wars. ''New York Times'' 17 July 2005]
*Curry, Tom. 2005. [http://www.msnbc.msn.com/id/7640262/page/2/ "Frist chills talk of judges deal (Page 2)."] "The question in the poll was not '''framed''' as a matter of whether nominee ought to get an up-or-down vote. And that '''framing''' of the issue, Republican strategists believe, is the most advantageous one..."; [[MSNBC]]
*[http://www.ccbi.cmu.edu/reprints/Gonzalez_JOEP2005-decision-making.pdf CMU.edu (pdf)] - 'The Framing effect and risky decision: Examining cognitive functions with fMRI', C. Gonzalez, et al., ''[[Journal of Economic Psychology]]'' (2005)
*[http://hbswk.hbs.edu/item/5488.html HBS.edu] - 'Fixing Price Tag Confusion'(interview), Sean Silverthorne (December 11, 2006)
*[http://www.msnbc.msn.com/id/14170927/ "'Framing effect' influences decisions: Emotions play a role in decision-making when information is too complex"], Charles Q. Choi, [[MSNBC]] (August 3, 2006)

{{Media culture}}
{{Media manipulation}}
{{Propaganda}}
{{World view}}

{{DEFAULTSORT:Framing (Social Sciences)}}
[[Category:Cognitive biases]]
[[Category:Framing (social sciences)| ]]
[[Category:Knowledge representation]]
[[Category:Propaganda techniques]]
[[Category:Prospect theory]]
[[Category:Social constructionism]]</text>
      <sha1>1pptdn7t6g9wo5p7u6sui3iy7xmia0g</sha1>
    </revision>
  </page>
  <page>
    <title>AGRIS</title>
    <ns>0</ns>
    <id>23241698</id>
    <revision>
      <id>742012625</id>
      <parentid>731646017</parentid>
      <timestamp>2016-10-01T03:36:21Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 1 sources and tagging 2 as dead. #IABot (v1.2.4)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10009" xml:space="preserve">'''AGRIS''' (International System for Agricultural Science and Technology) is a global public domain database with more than 8 million structured bibliographical records on agricultural science and technology. The database is maintained by [[CIARD]], and its content is provided by more than 150 participating institutions from 65 countries. The AGRIS Search system,&lt;ref&gt;{{cite web|url=http://agris.fao.org |title=agris.fao.org |publisher=agris.fao.org |date= |accessdate=2016-03-14}}&lt;/ref&gt; allows scientists, researchers and students to perform sophisticated searches using keywords from the [[AGROVOC]] thesaurus, specific journal titles or names of countries, institutions, and authors.

== Early AGRIS years ==
As [[information management]] flourished in the 1970s, the AGRIS [[metadata]] [[Text corpus|corpus]] was developed to allow its users to have free access to knowledge available in agricultural science and technology. AGRIS was developed to be an international cooperative system to serve both developed and [[developing countries]].

With the advent of the Internet, along with the promises offered by [[open access (publishing)|open access]] publishing, there was growing awareness that the management of agricultural science and technology information, would have various facets: [[Technical standard|standards]] and methodologies for [[interoperability]] and facilitation of knowledge exchange; tools to enable information management specialists to process data; information and knowledge exchange across countries. Common [[interoperability]] criteria were thus adopted in its implementation, and the AGRIS AP [[metadata]] was accordingly created in order to allow exchange and retrieval of Agricultural information Resources.&lt;ref&gt;{{cite web|url=http://www.fao.org/docrep/008/ae909e/ae909e00.htm |title=The AGRIS Application Profile for the International Information System on Agricultural Sciences and Technology Guidelines on Best Practices for Information Object Description |publisher=Fao.org |date= |accessdate=2013-07-09}}&lt;/ref&gt;

== AGRIS 2.0 ==
AGRIS covers the wide range of subjects related to agriculture science and technology, including forestry, animal husbandry, aquatic sciences and fisheries, human nutrition, and extension. Its content includes unique grey literature such as unpublished scientific and technical reports, theses, conference papers, government publications, and more. A growing number (around 20%) of bibliographical records have a corresponding full text document on the web which can easily be retrieved by Google.

On 5th December 2013 AGRIS 2.0 was released. AGRIS 2.0 is at the same time:

# A collaborative network of more than 150 institutions from 65 countries, maintained by FAO of the UN, promoting free access to agricultural information.
# A multilingual bibliographic database for agricultural science, fuelled by the AGRIS network, containing more than 8 million records largely enhanced with AGROVOC, FAO’s multilingual thesaurus covering all areas of interest to FAO, including food, nutrition, agriculture, fisheries, forestry, environment etc.
# A mash-up web application that links the bibliographic AGRIS knowledge to related resources on the web using the [[Linked Open Data]] methodology. An AGRIS mashup page (e.g. http://agris.fao.org/agris-search/search.do?recordID=QM2008000025 ) is a web page where an AGRIS resource is displayed together with relevant knowledge extracted from external data sources (as the World Bank, DBPedia, and Nature). The availability of external data sources is not under AGRIS control. Thus, if an external data source is temporary unreachable, it won’t be displayed in AGRIS mashup pages.

Access to the AGRIS Repository is provided through the AGRIS Search Engine.&lt;ref&gt;{{cite web|url=http://agris.fao.org/ |title=agris.fao.org |publisher=agris.fao.org |date= |accessdate=2013-07-09}}&lt;/ref&gt;  As such, it:
# enables retrieval of bibliographic records contained in the AGRIS Repository,
# allows users to perform either full-text or fielded, parametric and assisted queries.

AGRIS data was converted to RDF and the resulting linked dataset created some 200 million triples.
AGRIS is also registered in the Data Hub at http://thedatahub.org/dataset/agris

The AGRIS partners contributing to the AGRIS Database use several formats for exchanging data, including simple DC, from [[OAI-PMH]] systems.
The AGRIS AP format is anyway adopted directly by:
# Open Archive Initiative (OAI) partners: Scielo, Viikki Science Library
# BIBSYS, Norway, National Library of Portugal, Wageningen UR Library.
# ''National networks'': NARIMS&lt;ref&gt;[http://www.arc.sci.eg/ arc.sci.eg]&lt;/ref&gt; in [[Egypt]], [[PhilAgriNet]] in [[Philippines]], [[KAINet]] in [[Kenya]], NAC in [[Thailand]], GAINS in [[Ghana]].
# ''National institutional repositories'': Russia, Belarus, Uruguay, Spain, Iran.
# ''Information service providers'': [[Wolters Kluwer]], [[NISC]], CGIR, [[CGIAR]], [[Agriculture Network Information Center|AgNIC]], GFIS.
# ''Database systems/tools'': AgriOceanDspace,&lt;ref&gt;{{cite web|url=http://aims.fao.org/agriocean-dspace |title=AgriOcean DSpace &amp;#124; Agricultural Information Management Standards (AIMS) |publisher=Aims.fao.org |date= |accessdate=2013-07-09}}&lt;/ref&gt; NewGenlib, WebAGRIS, NERAKIN, AgriDrupal.&lt;ref&gt;{{cite web|url=http://aims.fao.org/tools/agridrupal |title=AgriDrupal &amp;#124; Agricultural Information Management Standards (AIMS) |publisher=Aims.fao.org |date= |accessdate=2013-07-09}}&lt;/ref&gt;

=== AGRIS under the CIARD umbrella ===
Falling under the umbrella of CIARD,&lt;ref&gt;{{cite web|url=http://www.ciard.net |title=What is CIARD? &amp;#124; Coherence in Information for Agricultural Research for Development |publisher=Ciard.net |date= |accessdate=2013-07-09}}&lt;/ref&gt; a joint initiative co-led by the CGIAR,&lt;ref&gt;{{cite web|last=Rijsberman |first=Frank |url=http://www.cgiar.org |title=CGIAR Home |publisher=Cgiar.org |date=2013-07-04 |accessdate=2013-07-09}}&lt;/ref&gt; GFAR&lt;ref&gt;{{cite web|url=http://www.egfar.org |title=EGFAR web Site |publisher=Egfar.org |date= |accessdate=2013-07-09}}&lt;/ref&gt; and [[FAO]], the new AGRIS aims to promote the sharing and management of agricultural science and technology information through the use of common [[Technical standard|standards]] and methodologies. These will incorporate [[Web 2.0]] features, in order to make the search experience as comprehensive, intuitive and far-reaching as possible for users of the new AGRIS.

Furthermore, the new AGRIS will also leverage the data and infrastructure of one of CIARD's projects: the CIARD RING. An acronym standing for Routemap to Information Nodes and Gateways (RING), the CIARD RING project is led by [[Global Forum on Agricultural Research|GFAR]] and it aims to:

* give an overview of the current offer of information services in ARD; as well as
* support those who want to implement new services.

A directory of ARD (Agricultural Research for Development) information services will allow the monitoring, describing and classifying of existing services, whilst benchmarking them against [[interoperability]] criteria, to ensure for maximum outreach and global availability.

== See also ==
* [[Agricultural Information Management Standards]]
* [[AgMES]]
* [[Agricultural Ontology Service]]
* [[AGROVOC]]
* [[Information management]]
* [[IMARK]]
* [[Disciplinary repository]]

== References ==
{{Reflist|2}}

== Other publications ==
* [http://f1000research.com/articles/4-432/v2 Discovering, Indexing and Interlinking Information Resources (F1000research 2015)]
* [http://f1000research.com/articles/4-110/v1 AGRIS: providing access to agricultural research data exploiting open data on the web (F1000research 2015)]
* [http://iospress.metapress.com/content/l15562xk70234n79/fulltext.pdf Migrating bibliographic datasets to the Semantic Web: The AGRIS case (Semantic Web journal 2013)]{{dead link|date=October 2016 |bot=InternetArchiveBot |fix-attempted=yes }}
* [https://web.archive.org/web/20140908155657/http://eprints.rclis.org/21112/1/2013_EFITA%20Pushing_Pulling.pdf Pushing, Pulling, Harvesting, Linking - Rethinking Bibliographic Workflows for the Semantic Web (EFITA-2013)] 
* [http://www.fao.org/documents/advanced_s_result.asp?FORM_C=AND&amp;SERIES=339 Agricultural Information and Knowledge Management Papers]
* [http://www.fao.org/docrep/008/ae909e/ae909e00.htm The AGRIS Application Profile for the International Information System on Agricultural Sciences and Technology Guidelines on Best Practices for Information Object Description]
* [http://www.fao.org/kce/consultations/coaim/coaim-2000/en/ Consultations on Agricultural Management (COAIM)]{{dead link|date=October 2016 |bot=InternetArchiveBot |fix-attempted=yes }}
* [http://www.fao.org/docrep/x7936e/x7936e00.htm First Consultation on Agricultural Information Management (2000 COAIM Report)]
* [http://www.fao.org/docrep/meeting/005/y7963e/Y7963e00.htm#Top Report on the Second Consultation on Agricultural Information Management (2002 COAIM Report)]
* [http://departments.agri.huji.ac.il/economics/gelb-agris-10.pdf#AGRIS 1968-1994: Insights and Lessons]

== External links ==
* [http://agris.fao.org '''AGRIS''']
* [http://agris.fao.org/agris-search/agrisMap.do AGRIS network map]
* [http://agrovoc.fao.org/axis/services/SKOSWS?wsdl AGROVOC Web services]
* [http://aims.fao.org/ Agricultural Information Management Standards (AIMS)]
* [http://www.fao.org Food and Agriculture Organization of the United Nations (FAO) Web site]
* [http://www.ciard.net Coherence in Information for Agricultural Research for Development (CIARD) Wed site]
* [http://www.fao.org/nems/rss/rss_nems_results.asp?owner=615&amp;status=10&amp;dateto=31/12/2006&amp;lang=en&amp;sites=1 RSS feed of news and events]

{{DEFAULTSORT:Agris}}
[[Category:Agricultural organizations]]
[[Category:Interoperability]]
[[Category:Knowledge representation]]
[[Category:Metadata]]
[[Category:Food and Agriculture Organization]]
[[Category:Public domain databases]]
[[Category:Agricultural databases]]</text>
      <sha1>eo2zrivj83obp8krdrnpktaawg07kd5</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Knowledge representation languages</title>
    <ns>14</ns>
    <id>23890667</id>
    <revision>
      <id>733574189</id>
      <parentid>722189610</parentid>
      <timestamp>2016-08-08T19:14:16Z</timestamp>
      <contributor>
        <username>Jarble</username>
        <id>7226930</id>
      </contributor>
      <comment>Another addition to the list</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="221" xml:space="preserve">== See also ==

* [[:Category:Constraint programming languages]]
* [[:Category:Domain-specific programming languages]]

[[Category:Knowledge representation]]
[[Category:Engineered languages]]
[[Category:Markup languages]]</text>
      <sha1>44y3tklg8zhfl7wsyu8n9na341tm8sj</sha1>
    </revision>
  </page>
  <page>
    <title>Cognitive map</title>
    <ns>0</ns>
    <id>1385766</id>
    <revision>
      <id>759296219</id>
      <parentid>748493035</parentid>
      <timestamp>2017-01-10T09:53:01Z</timestamp>
      <contributor>
        <username>Kku</username>
        <id>5846</id>
      </contributor>
      <comment>/* Parallel map theory */ lx</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="16158" xml:space="preserve">A '''cognitive map''' (sometimes called a [[mental map]] or [[mental model]]) is a type of [[mental representation]] which serves an individual to acquire, code, store, recall, and decode information about the relative locations and attributes of phenomena in their everyday or metaphorical spatial environment. The concept was introduced by [[Edward C. Tolman|Edward Tolman]] in 1948.&lt;ref name="pmid18870876"&gt;{{cite journal |last=Tolman |first=Edward C. |authorlink=Edward C. Tolman |title=Cognitive maps in rats and men |journal=[[Psychological Review]] |volume=55 |issue=4 |pages=189–208 |date=July 1948 |pmid=18870876| doi=10.1037/h0061626}}&lt;/ref&gt; The term was later generalized by some researchers, especially in the field of [[operations research]], to refer to a kind of [[semantic network]] representing an individual's personal knowledge or [[Schema (psychology)|schemas]].&lt;ref&gt;{{cite journal |last=Eden |first=Colin |date=July 1988 |title=Cognitive mapping |journal=[[European Journal of Operational Research]] |volume=36 |issue=1 |pages=1–13 |doi=10.1016/0377-2217(88)90002-1 |quote=In the practical setting of work in with a team of busy managers cognitive mapping is a tool for building interest from all team members in the problem solving activity. [...] The cycle of ''problem construction'', ''making sense'', ''defining the problem'', and declaring a ''portfolio of solutions'', which I have discussed elsewhere (Eden, 1982) is the framework that guides the process of working with teams. Thus building and working with the cognitive maps of each individual is primarily aimed at helping each team member reflectively 'construct' and 'make sense' of the situation they believe the team is facing. (pp. 7–8)}}&lt;/ref&gt;&lt;ref&gt;{{cite journal |last1=Fiol |first1=C. Marlene |last2=Huff |first2=Anne Sigismund |date=May 1992 |title=Maps for managers: Where are we? Where do we go from here? |journal=[[Journal of Management Studies]] |volume=29 |issue=3 |pages=267–285 |doi=10.1111/j.1467-6486.1992.tb00665.x |quote=For geographers, a map is a means of depicting the world so that people understand where they are and where they can go. For cognitive researchers, who often use the idea of a 'map' as an analogy, the basic idea is the same. Cognitive maps are graphic representations that locate people in relation to their information environments. Maps provide a frame of reference for what is known and believed. They highlight some information and fail to include other information, either because it is deemed less important, or because it is not known. (p. 267)}}&lt;/ref&gt;&lt;ref&gt;{{cite book |last1=Ambrosini |first1=Véronique |last2=Bowman |first2=Cliff |date=2002 |chapter=Mapping successful organizational routines |editor1-last=Huff |editor1-first=Anne Sigismund |editor2-last=Jenkins |editor2-first=Mark |title=Mapping strategic knowledge |location=London; Thousand Oaks, CA |publisher=[[Sage Publications]] |pages=19–45 |isbn=0761969497 |oclc=47900801 |quote=We shall not explain here what cognitive maps are about as this has been done extensively elsewhere (Huff, 1990). Let us just say that cognitive maps are the representation of an individual's personal knowledge, of an individual's own experience (Weick and Bougon, 1986), and they are ways of representing individuals' views of reality (Eden et al., 1981). There are various types of cognitive maps (Huff, 1990). (pp. [//books.google.com/books?id=LE95fcRz_IcC&amp;pg=PA21 21–22])}}&lt;/ref&gt;

== Overview ==

Cognitive maps have been studied in various fields, such as psychology, education, archaeology, planning, geography, cartography, architecture, landscape architecture, urban planning, management and history.&lt;ref&gt;{{cite book |title=Conspiracy nation: the politics of paranoia in Postwar America |last=Knight |first=Peter |year=2002 |publisher=[[New York University Press]] |location=New York and London |isbn=0814747353}}&lt;/ref&gt;{{Page needed|date=August 2016}} As a consequence, these mental models are often referred to, variously, as cognitive  maps, [[mental map]]s, [[Behavioral script|script]]s, [[Schema (psychology)|schemata]], and [[Frame of reference|frames of reference]].

Cognitive maps serve the construction and accumulation of spatial knowledge, allowing the "[[mind's eye]]" to visualize images in order to reduce [[cognitive load]], enhance [[recollection|recall]] and [[learning]] of information. This type of spatial thinking can also be used as a metaphor for non-spatial tasks, where people performing non-spatial tasks involving [[memory]] and imaging use spatial knowledge to aid in processing the task.&lt;ref&gt;{{cite journal |last=Kitchin |first=Robert M. |title=Cognitive maps: what are they and why study them? |journal=[[Journal of Environmental Psychology]] |year=1994 |volume=14 |issue=1 |pages=1–19 |doi=10.1016/S0272-4944(05)80194-X}}&lt;/ref&gt;

The [[neural correlate]]s of a cognitive map have been speculated to be the [[place cell]] system in the [[hippocampus]]&lt;ref name="O'Keefe"&gt;{{cite book |last1=O'Keefe |first1=John |authorlink1=John O'Keefe (neuroscientist) |last2=Nadel |first2=Lynn |authorlink2=Lynn Nadel |date=1978 |title=The hippocampus as a cognitive map |location=Oxford; New York |publisher=[[Clarendon Press]]; [[Oxford University Press]] |isbn=0198572069 |oclc=4430731 |url=http://www.cognitivemap.net/}}&lt;/ref&gt; and the recently discovered [[grid cells]] in the [[entorhinal cortex]].&lt;ref name="pmid16675704"&gt;{{cite journal |last1=Sargolini |first1=Francesca |last2=Fyhn |first2=Marianne |last3=Hafting |first3=Torkel |last4=McNaughton |first4=Bruce L. |last5=Witter |first5=Menno P. |last6=Moser |first6=May-Britt |last7=Moser |first7=Edvard I. |title=Conjunctive representation of position, direction, and velocity in entorhinal cortex |journal=[[Science (journal)|Science]] |volume=312 |issue=5774 |pages=758–762 |date=May 2006 |pmid=16675704 |doi=10.1126/science.1125572 |bibcode=2006Sci...312..758S }}&lt;/ref&gt;

== Neurological basis ==

Cognitive mapping is believed to largely be a function of the hippocampus. The hippocampus is connected to the rest of the brain in such a way that it is ideal for integrating both spatial and nonspatial information. Connections from the [[postrhinal cortex]] and the medial entorhinal cortex provide spatial information to the hippocampus. Connections from the [[perirhinal cortex]] and lateral entorhinal cortex provide nonspatial information. The integration of this information in the hippocampus makes the hippocampus a practical location for cognitive mapping, which necessarily involves combining information about an object's location and its other features.&lt;ref name="Manns"&gt;{{cite journal |last1=Manns |first1=Joseph R. |last2=Eichenbaum |first2=Howard |authorlink2=Howard Eichenbaum |date=October 2009 |title=A cognitive map for object memory in the hippocampus |journal=[[Learning &amp; Memory]] |volume=16 |issue=10 |pages=616–624 |doi=10.1101/lm.1484509 |pmc=2769165 |pmid=19794187 }}&lt;/ref&gt;

O'Keefe and Nadel were the first to outline a relationship between the hippocampus and cognitive mapping.&lt;ref name="O'Keefe" /&gt; Many additional studies have shown additional evidence that supports this conclusion.&lt;ref name=Moser&gt;{{cite journal |last1=Moser |first1=Edvard I. |authorlink1=Edvard Moser |last2=Kropff |first2=Emilio |last3=Moser |first3=May-Britt |authorlink3=May-Britt Moser |date=2008 |title=Place cells, grid cells, and the brain's spatial representation system |journal=[[Annual Review of Neuroscience]] |volume=31 |pages=69–89 |doi=10.1146/annurev.neuro.31.061307.090723 |pmid=18284371 |url=http://www.annualreviews.org/eprint/7t2VcSrTYa8V8yACMweG/full/10.1146/annurev.neuro.31.061307.090723}}&lt;/ref&gt; Specifically, [[pyramidal cells]] ([[place cells]], [[boundary cell]]s, and [[grid cells]]) have been implicated as the neuronal basis for cognitive maps within the hippocampal system.

Numerous studies by O'Keefe have implicated the involvement of place cells. Individual place cells within the hippocampus correspond to separate locations in the environment with the sum of all cells contributing to a single map of an entire environment. The strength of the connections between the cells represents the distances between them in the actual environment. The same cells can be used for constructing several environments, though individual cells' relationships to each other may differ on a map by map basis.&lt;ref name="O'Keefe" /&gt; The possible involvement of place cells in cognitive mapping has been seen in a number of mammalian species, including rats and macaque monkeys.&lt;ref name=Moser /&gt; Additionally, in a study of rats by Manns and Eichenbaum, pyramidal cells from within the hippocampus were also involved in representing object location and object identity, indicating their involvement in the creation of cognitive maps.&lt;ref name=Manns /&gt; However, there has been some dispute as to whether such studies of mammalian species indicate the presence of a cognitive map and not another, simpler method of determining one's environment.&lt;ref name=Bennet /&gt;

While not located in the hippocampus, grid cells from within the medial entorhinal cortex have also been implicated in the process of [[path integration]], actually playing the role of the path integrator while place cells display the output of the information gained through path integration.&lt;ref name=McNaughton&gt;{{cite journal |last1=McNaughton |first1=Bruce L. |last2=Battaglia |first2=Francesco P. |last3=Jensen |first3=Ole |last4=Moser |first4=Edvard I. |authorlink4=Edvard Moser |last5=Moser |first5=May-Britt |authorlink5=May-Britt Moser |date=August 2006 |title=Path integration and the neural basis of the 'cognitive map' |journal=[[Nature Reviews Neuroscience]] |volume=7 |issue=8 |pages=663–678 |doi=10.1038/nrn1932 |pmid=16858394 }}&lt;/ref&gt; The results of path integration are then later used by the hippocampus to generate the cognitive map.&lt;ref name=Jacobs /&gt; The cognitive map likely exists on a circuit involving much more than just the hippocampus, even if it is primarily based there. Other than the medial entorhinal cortex, the presubiculum and parietal cortex have also been implicated in the generation of cognitive maps.&lt;ref name=Moser /&gt;

=== Parallel map theory ===

There has been some evidence for the idea that the cognitive map is represented in the [[hippocampus]] by two separate maps. The first is the bearing map, which represents the environment through self-movement cues and [[gradient]] cues. The use of these [[vector (mathematics)|vector]]-based cues creates a rough, 2D map of the environment. The second map would be the sketch map that works off of positional cues. The second map integrates specific objects, or [[landmark]]s, and their relative locations to create a 2D map of the environment. The cognitive map is thus obtained by the integration of these two separate maps.&lt;ref name=Jacobs /&gt;

==Generation==

The cognitive map is generated from a number of sources, both from the [[visual system]] and elsewhere. Much of the cognitive map is created through self-generated movement [[sensory cue|cues]]. Inputs from senses like vision, [[proprioception]], olfaction, and hearing are all used to deduce a person's location within their environment as they move through it. This allows for path integration, the creation of a vector that represents one's position and direction within one's environment, specifically in comparison to an earlier reference point. This resulting vector can be passed along to the hippocampal place cells where it is interpreted to provide more information about the environment and one's location within the context of the cognitive map.&lt;ref name=Jacobs&gt;{{cite journal |last1=Jacobs |first1=Lucia F. |last2=Schenk |first2=Françoise |date=April 2003 |title=Unpacking the cognitive map: the parallel map theory of hippocampal function |journal=[[Psychological Review]] |volume=110 |issue=2 |pages=285–315 |doi=10.1037/0033-295X.110.2.285 |pmid=12747525}}&lt;/ref&gt;

Directional cues and positional landmarks are also used to create the cognitive map. Within directional cues, both explicit cues, like markings on a compass, as well as gradients, like shading or magnetic fields, are used as inputs to create the cognitive map. Directional cues can be used both statically, when a person does not move within his environment while interpreting it, and dynamically, when movement through a gradient is used to provide information about the nature of the surrounding environment. Positional landmarks provide information about the environment by comparing the relative position of specific objects, whereas directional cues give information about the shape of the environment itself. These landmarks are processed by the hippocampus together to provide a graph of the environment through relative locations.&lt;ref name=Jacobs /&gt;

== History ==

The idea of a cognitive map was first developed by [[Edward C. Tolman]]. Tolman, one of the early cognitive psychologists, introduced this idea when doing an experiment involving rats and mazes. In Tolman's experiment, a rat was placed in a cross shaped maze and allowed to explore it. After this initial exploration, the rat was placed at one arm of the cross and food was placed at the next arm to the immediate right. The rat was conditioned to this layout and learned to turn right at the intersection in order to get to the food. When placed at different arms of the cross maze however, the rat still went in the correct direction to obtain the food because of the initial cognitive map it had created of the maze. Rather than just deciding to turn right at the intersection no matter what, the rat was able to determine the correct way to the food no matter where in the maze it was placed.&lt;ref&gt;{{cite book |last=Goldstein |first=E. Bruce |date=2011 |title=Cognitive psychology: connecting mind, research, and everyday experience |edition=3rd |location=Belmont, CA |publisher=[[Wadsworth Cengage Learning]] |isbn=9780840033550 |oclc=658234658 |pages=11–12}}&lt;/ref&gt;

== Criticism ==

In a review, Andrew T.D. Bennett argued that there are no clear evidence for cognitive maps in non-human animals (i.e. cognitive map according to Tolman's definition).&lt;ref name=Bennet&gt;{{cite journal |last=Bennett |first=Andrew T. D. |date=January 1996 |title=Do animals have cognitive maps? |journal=[[The Journal of Experimental Biology]] |volume=199 |issue=Pt 1 |pages=219–224 |pmid=8576693}}&lt;/ref&gt; This argument is based on analyses of studies where it has been found that simpler explanations can account for experimental results. Bennett highlights three simpler alternatives that cannot be ruled out in tests of cognitive maps in non-human animals "These alternatives are (1) that the apparently novel short-cut is not truly novel; (2) that path integration is being used; and (3) that familiar landmarks are being recognised from a new angle, followed by movement towards them."

== Related term ==
{{Refimprove section|date=August 2016}}

A cognitive map is a spatial representation of the outside world that is kept within the mind, until an actual manifestation (usually, a drawing) of this perceived knowledge is generated, a mental map. Cognitive mapping is the implicit, mental mapping the explicit part of the same process. In most cases, a cognitive map exists independently of a mental map, an article covering just cognitive maps would remain limited to theoretical considerations.

In some uses, mental map refers to a practice done by urban theorists by having city dwellers draw a map, from memory, of their city or the place they live. This allows the theorist to get a sense of which parts of the city or dwelling are more substantial or imaginable. This, in turn, lends itself to a decisive idea of how well urban planning has been conducted.

==See also==
* [[Cognitive geography]]
* [[Fuzzy cognitive map]]
* [[Motion perception]]
* [[Repertory grid]]

==References==
{{Reflist|30em}}

== External links ==
* {{commonscat-inline|Cognitive maps}}

{{DEFAULTSORT:Cognitive Map}}
[[Category:Cognitive science]]
[[Category:Mnemonics]]
[[Category:Knowledge representation]]</text>
      <sha1>ew9tbaai6o5ar6dgd0scyvq7e4nehl3</sha1>
    </revision>
  </page>
  <page>
    <title>Vivid knowledge</title>
    <ns>0</ns>
    <id>25154733</id>
    <revision>
      <id>646953751</id>
      <parentid>496915099</parentid>
      <timestamp>2015-02-13T13:48:34Z</timestamp>
      <contributor>
        <username>John of Reading</username>
        <id>11308236</id>
      </contributor>
      <minor />
      <comment>Typo fixing, replaced: Kownledge → Knowledge (3) per the book cover image at Amazon using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2010" xml:space="preserve">'''Vivid knowledge''' refers to a specific kind of [[knowledge representation]].

The idea of a '''vivid knowledge base''' is to get an interpretation mostly straightforward out of it &amp;ndash; it implies the interpretation. Thus, any query to such a [[knowledge base]] can be reduced to a [[database]]-like query.

== Propositional knowledge base ==

A [[Propositional logic|propositional]] [[knowledge base]] KB is '''vivid''' ''iff'' KB is a [[Completeness (knowledge bases)|complete]] and [[consistency (knowledge bases)|consistent]] set of [[Literal (mathematical logic)|literals]] (over some vocabulary).&lt;ref&gt;Knowledge Representation and Reasoning / Ronald J. Brachman, Hector J. Levesque / page 337&lt;/ref&gt;

Such a knowledge base has the property that it as exactly one interpretation, i.e. the interpretation is unique. A check for entailment of a sentence can simply be broken down into its literals and those can be answered by a simple database-like check of KB.

== First-order knowledge base ==

A [[First-order logic|first-order]] knowledge base KB is '''vivid''' ''iff'' for some finite set of positive function-free ground literals KB&lt;sup&gt;+&lt;/sup&gt;,

: KB = KB&lt;sup&gt;+&lt;/sup&gt; ∪ Negations ∪ DomainClosure ∪ UniqueNames,

whereby

: Negations ≔ { ¬p | p is atomic and KB ⊭ p },
: DomainClosure ≔ { (c&lt;sub&gt;i&lt;/sub&gt; ≠ c&lt;sub&gt;j&lt;/sub&gt;) | c&lt;sub&gt;i&lt;/sub&gt;, c&lt;sub&gt;j&lt;/sub&gt; are distinct constants },
: UniqueNames ≔ { ∀x: (x = c&lt;sub&gt;1&lt;/sub&gt;) ∨ (x = c&lt;sub&gt;2&lt;/sub&gt;) ∨ ..., where the c&lt;sub&gt;i&lt;/sub&gt; are all the constants in KB&lt;sup&gt;+&lt;/sup&gt; }.

&lt;ref&gt;Knowledge Representation and Reasoning / Ronald J. Brachman, Hector J. Levesque / page 337&lt;/ref&gt;

All interpretations of a vivid first-order knowledge base are isomorphic.&lt;ref&gt;Knowledge Representation and Reasoning / Ronald J. Brachman, Hector J. Levesque / page 339&lt;/ref&gt;

== See also ==
* [[Closed world assumption]]

{{computable knowledge}}

== References ==

&lt;references/&gt;

[[Category:Knowledge representation]]


{{logic-stub}}
{{database-stub}}</text>
      <sha1>1n6bg1fqv1w2jy99eh3y494ffnh0tq1</sha1>
    </revision>
  </page>
  <page>
    <title>IDIS (software)</title>
    <ns>0</ns>
    <id>2889751</id>
    <revision>
      <id>683750522</id>
      <parentid>499866604</parentid>
      <timestamp>2015-10-02T07:48:33Z</timestamp>
      <contributor>
        <username>Ymblanter</username>
        <id>14596827</id>
      </contributor>
      <comment>/* See also */  rm redlink</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="342" xml:space="preserve">{{Unreferenced|date=December 2009}}

'''IDIS''' is a [[software]] tool for direct [[data exchange]] between [[CDS/ISIS]] and [[IDAMS]]. It is developed, maintained and disseminated by [[UNESCO]].

==See also==
*[[CDS/ISIS]] - database software

{{DEFAULTSORT:Idis (Software)}}
[[Category:Knowledge representation]]


{{network-software-stub}}</text>
      <sha1>pofplfqvfla3agmv5uqailphkleyg60</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Ontology (information science)</title>
    <ns>14</ns>
    <id>26259157</id>
    <revision>
      <id>644097920</id>
      <parentid>608480592</parentid>
      <timestamp>2015-01-25T13:12:39Z</timestamp>
      <contributor>
        <ip>77.56.53.183</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="176" xml:space="preserve">{{Cat main|Ontology (information science)}}
{{Commons category|Ontology}}

[[Category:Knowledge representation]]
[[Category:Metadata]]
[[Category:Ontology|Information science]]</text>
      <sha1>i6ccbiuwms726an8dfvzqsqnot9de04</sha1>
    </revision>
  </page>
  <page>
    <title>E-services</title>
    <ns>0</ns>
    <id>202311</id>
    <revision>
      <id>762028843</id>
      <parentid>762028830</parentid>
      <timestamp>2017-01-26T06:10:01Z</timestamp>
      <contributor>
        <username>Alex Cohn</username>
        <id>60778</id>
      </contributor>
      <minor />
      <comment>Reverted edits by [[Special:Contribs/119.148.44.254|119.148.44.254]] ([[User talk:119.148.44.254|talk]]) to last version by MolBio7</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="37034" xml:space="preserve">The concept of '''e-service''' (short for electronic service) represents one prominent application of utilizing the use of [[Information and communication technology|information and communication technologies]] (ICTs) in different areas. However, providing an exact definition of e-service is hard to come by as researchers have been using different definitions to describe e-service. Despite these different definitions, it can be argued that they all agree about the role of technology in facilitating the delivery of services which make them more of electronic services.

It seems compelling to adopt Rowley (2006)&lt;ref name=autogenerated1&gt;Rowley, J. (2006) An analysis of the e-service literature: towards a research agenda. Internet Research, 16 (3), 339-359&lt;/ref&gt; approach who defines e-services as: “…deeds, efforts or performances whose delivery is mediated by information technology. Such e-service includes the service element of e-tailing, customer support, and service delivery”. This definition reflect three main components- service provider, service receiver and the channels of service delivery (i.e., technology). For example, as concerned to public e-service, public agencies are the service provider and citizens as well as businesses are the service receiver. The channel of service delivery is the third requirement of e-service. Internet is the main channel of e-service delivery while other classic channels (e.g. telephone, call center, public kiosk, mobile phone, television) are also considered.

==Definitions and origin of the term e-service==
Since its conceptual inception in the late 1980s in Europe{{Citation needed|date=October 2012}} and formal introduction in 1993 by the US Government,&lt;ref&gt;Alasem, A. (2009). An Overview of e-Government Metadata Standards and Initiatives based on Dublin Core. Electronic Journal of e-Government, 7(1), 1 – 10&lt;/ref&gt; the term ‘[[E-Government]]’ has now become one of the recognized research domains especially in the context of public policy and now has been rapidly gaining strategic importance in public sector modernization.&lt;ref&gt;Wimmer, M., Codagnone, C. and Janssen, M. (2008) “Future of e-Government Research: 13 research themes identified in the eGovRTD2020 project’. Proceeding of the 41st Hawaii International Conference on System Sciences, USA&lt;/ref&gt; E-service is one of the branches of this domain and its attention has also been creeping up among the practitioners and researchers.&lt;ref&gt;Lӧfstedt, U. (2005) ‘Assessment of current research and some proposals for future direction’, International Journal of Public IS&lt;/ref&gt;

E-service (or eservice) is a highly generic term, usually referring to ‘The provision of services via the Internet (the prefix 'e' standing for ‘electronic’, as it does in many other usages), thus e-Service may also include e-Commerce, although it may also include non-commercial services (online), which is usually provided by the government.’ (Irma Buntantan &amp; G. David Garson, 2004: 169-170; Muhammad Rais &amp; Nazariah, 2003: 59, 70-71).

'E-Service constitutes the online services available on the Internet, whereby a valid transaction of buying and selling (procurement) is possible, as opposed to the traditional websites, whereby only descriptive information are available, and no online transaction is made possible.' (Jeong, 2007).&lt;ref&gt;Jeong Chun Hai @Ibrahim. (2007). ''Fundamental of Development Administration.'' Selangor: Scholar Press. ISBN 978-967-5-04508-0&lt;/ref&gt;

==Importance of E-service ==
Lu (2001)&lt;ref&gt;Lu, J. (2001). Measuring cost/benefits of e-business applications and customer satisfaction”, Proceedings of the 2nd International Web Conference, 29–30 November, Perth, Australia, 139-47&lt;/ref&gt; identifies a number of benefits for e-services, some of these are:

* Accessing a greater customer base
* Broadening market reach
* Lowering of entry barrier to new markets and cost of acquiring new customers
* Alternative communication channel to customers
* Increasing services to customers
* Enhancing perceived company image
* Gaining competitive advantages
* Potential for increasing [[Customer knowledge]]

==Importance and advantages of E-shopping==
*E-shops are open 24 hours a day.
*There is no need to travel to the malls or wait at the checkout counters.
*There is usually a wide selection of goods and services.
*It is easy to compare prices and quality by using the E-shopping tool.
*Price reduction and discounts are electronically conveyed.

==E-service domain==
The term ‘e-service’ has many applications and can be found in many disciplines. The two dominant application areas of e-services are

E-business (or e-commerce): e-services mostly provided by businesses or [NGO|non-government organizations] (NGOs) (private sector).

E-government: e-services provided by government to citizens or business (public sector is the supply side). The use and description of the e-service in this page will be limited to the context of e-government only where of the e-service is usually associated with prefix “public”: Public e-services. In some cases, we will have to describe aspects that are related to both fields like some conferences or journals which cover the concept of “e-Service” in both domains of e-government and [[e-business]].

==Architecture==
Depending on the types of services, there are certain functionalities required in the certain layers of e-service architectural framework, these include but are not limited to – Data layer (data sources), processing layers (customer service systems, management systems, data warehouse systems, integrated customer content systems), exchange layer ([[Enterprise application integration|Enterprise Application Integration]]– EAI), interaction layer ( integrating e-services), and presentation layer (customer interface through which the web pages and e-services are linked).

==E-service quality==
Measuring [[service quality]] and service excellence are important in a competitive organizational environment. The [[SERVQUAL]]- service quality model is one of the widely used tools for measuring quality of the service on various aspects. The five attributes of this model are: reliability, responsiveness, assurance, tangibles, and empathy. The following table summarizes some major of these:

{| class="sortable wikitable"
|-
!'''SERVQUAL&lt;ref&gt;Jiang, J.J.; Klein, G. and Crampton, S.M. (2000). A note on SERVQUAL reliability and validity in information system service quality measurement. Decision Sciences. Atlanta: Summer 2000. Vol. 31, Iss. 3; p. 725&lt;/ref&gt;'''
!'''Kaynama &amp; Black (2000)'''&lt;ref&gt;Kaynama, S. A., and Black, C. I.  (2000). A Proposal to assess the Service Quality of Online Travel Agencies: An Exploratory Study. Journal of Professional Services Marketing (21:1), 63-88&lt;/ref&gt;
!'''Zeithaml (2002)'''&lt;ref&gt;Zeithaml, V. A. (2002). Service Excellence in Electronic Channels. Managing Service Quality (12:3), 2002, 135-138&lt;/ref&gt;
!'''Janda et al. (2002)'''&lt;ref&gt;Janda, S., Trocchia, P. J., and Gwinner, K. (2002). Consumer perceptions of Internet Retail Service Quality. International Journal of Service Industry Management (13:5),  412-431&lt;/ref&gt;
!'''Alawattegama &amp; Wattegama (2008)'''&lt;ref&gt;Alawattegama, L. and Wattegama, C. (2008). Benchmarking Asia Pacific National Telecom Regulatory Authority Websites. LIRNEasia&lt;/ref&gt;
|-
|| Reliability      ||Content      ||Access      ||Access      || Factual information
|-
||Responsiveness      ||Access      ||Ease of navigation      ||Security     || Business information
|-
||Assurance      ||Navigation      ||Efficiency     ||Sensation        || General information
|-
||Tangibles      ||Design      ||Flexibility      ||Information/content      || Consumer‐ related information
|-
||Empathy      ||Response      ||Reliability ||     ||
|-
||     ||Background      ||Personalization      ||     ||
|-
||     ||Personalization     ||Security/privacy      ||     ||
|-
||     ||     ||Responsiveness      ||     ||
|-
||     ||     ||Assurance/trust     ||     ||
|-
||     ||     ||Site aesthetics      ||     ||
|-
||     ||    ||Price knowledge      ||     ||
|}

The [[LIRNEasia]] study on benchmarking national telecom regulator websites focuses on content than on accessibility and ease of use, unlike the other studies mentioned here. Websites are increasingly important portals to government agencies, especially in the context of [[information society]] reforms. Stakeholders, including businesses, investors and even the general public, are interested in information produced by these agencies, and websites can help to increase their transparency and accountability. The quality of its website also demonstrates how advanced a regulatory agency is.

==E-service cost factor==
Some major cost factors are (Lu, 2001):&lt;ref&gt;Lu, J. (2001). Measuring cost/benefits of e-business applications and customer
satisfaction”, Proceedings of the 2nd International Web Conference, 29–30 November, Perth, Australia, 139-47&lt;/ref&gt;

* Expense of setting up applications
* Maintaining applications
* Internet connection
* Hardware/software
* Security concerns
* legal issues
* Training; and
* Rapid technology changes

==Practical examples of e-services in the Developing World==
Information technology is a powerful tool for accelerating economic development. Developing countries have focused on the development of ICT during the last two decades and as a result, it has been recognized that ICT is critical to economy and is as a catalyst of economic development. So, in recent years there seems to have been efforts for providing various e-services in many developing countries since ICT is believed to offer considerable potential for the sustainable development of e-Government and as a result, e-Services.&lt;ref name=autogenerated4&gt;Ndou,V.(2004)E-Government for developing countries: Opportunities and Challenges, EJISDC 18, 1, 1-24&lt;/ref&gt;

Many government agencies in developed countries have taken progressive steps toward the web and ICT use, adding coherence to all local activities on the Internet, widening local access and skills, opening up interactive services for local debates, and increasing the participation of citizens on promotion and management of the territory(Graham and Aurigi, 1997).&lt;ref&gt;Graham, S. and Aurigi, A. (1997) Virtual Cities, Social Polarisation, and the Crisis in Urban Public Space, Journal of Urban Technology, 4, 1, 19-52&lt;/ref&gt;

But the potential for e-government in developing countries remains largely unexploited, even though. ICT is believed to offer considerable potential for the sustainable development of e-government. Different human, organizational and technological factors,
issues and problems pertain in these countries, requiring focused studies and appropriate approaches. ICT, in general, is referred to as an “enabler”, but on the other hand it should also be regarded as a challenge and a peril in itself. The organizations, public or private,which ignore the potential value and use of ICT may suffer pivotal competitive disadvantages. Nevertheless, some e-government initiatives have flourished in developing countries too, e.g. Brazil, India, Chile, etc.&lt;ref name=autogenerated4 /&gt; What the experience in these countries shows, is that governments in the developing world can effectively exploit and appropriate the benefits of ICT, but e-government success entails the accommodation of certain unique conditions,needs and obstacles. The adaptive challenges of e-government go far beyond technology, they call for organizational structures and skills, new forms of leadership, transformation of public-private partnerships (Allen et al., 2001).&lt;ref&gt;Allen, A.B., Juillet, L., Paquet, G. and Roy, J. (2001) E-Governance and Government Online in Canada: Partnerships, People and Prospects, Government Information Quarterly,18, 93-104.)&lt;/ref&gt;

Following are a few examples regarding e-services in some developing countries:

===E-services in Rwanda===
Only a decade after emerging from the fastest genocide of the 20th Century, Rwanda, a small country in Eastern Central Africa,
has become one of the continent’s leaders in, and model on, bridging the digital divide through e-government. Rwanda has undergone a rapid turnaround from one of the most technologically deficient countries only a decade ago to a country
where legislative business is conducted online and wireless access to the Internet is available anywhere in the country. This is
puzzling when viewed against the limited progress made in other comparable developing countries, especially those located in the
same region, sub-Saharan Africa, where the structural and institutional constraints to e-government diffusion are similar.&lt;ref&gt;Mawangi, W.(2006) The social relations of e-government diffusion in developing countries: the case of Rwanda, Proceedings of the 2006 international conference on Digital government research, May 21–24, 2006, San Diego, California&lt;/ref&gt;

===E-services in South Africa===
In South Africa, there continues to be high expectations of government in respect to improved delivery of service and of closer consultation with citizens. Such expectations are not unique to this country, and in this regard there is a need for governments to recognise that the implementation of e-government systems and e-services affords them the opportunity to enhance service delivery and good governance.&lt;ref name=autogenerated3&gt;van Brakel, P.A.(2009) Proceedings of the 11th Annual Conference on World Wide Web Applications, Port Elizabeth, 2–4 September&lt;/ref&gt; The implementation of e-Government has been widely acclaimed in that it provides new impetus to deliver services quickly and efficiently (Evans &amp; Yen, 2006:208).&lt;ref&gt;Evans, D. &amp; Yen, D. C. 2006. e-Government: evolving relationship of citizens and government, domestic, and international development. Government Information Quarterly, 23(2): 207-235.)&lt;/ref&gt; In recognition of these benefits, various arms of the South African government have embarked on a number of e-government programmes for example the [http://www.gov.za/  Batho Pele portal], [[Sars efiling|SARS e-filing]], the [http://www.enatis.com/  e-Natis system], electronic processing of grant applications from remote sites, and a large number of departmental information websites. Also a number of well publicised e-government ventures such as the latter, analysts and researchers consider the state of e-government in South Africa to be at rudimentary stages. There are various factors
which collectively contribute to such an assessment. Amongst these, key factors relate to a lack of a clear strategy to facilitate uptake and adoption of e-government services as well as evaluation frameworks to assess expectations of citizens who are one of the primary user groups of these services.&lt;ref name=autogenerated3 /&gt;

===E-services in Malaysia===
E-Services is one of the pilot projects under the Electronic Government Flagship within the Multimedia Super Corridor ([http://www.mscmalaysia.my/  MSC]) initiative. With E-Services, one can now conduct transactions with Government agencies, such as the Road Transport Department (RTD) and private utility companies such as Tenaga Nasional Berhad ([http://www.tnb.com.my  TNB]) and Telekom Malaysia Berhad ([http://www.tm.net.my  TM]) through various convenient channels such as the eServices kiosks and internet. No more queuing, traffic jams or bureaucratic hassles and one can now conduct transaction at one’s own convenience. Also, Electronic Labour Exchange ([http://www.elx.gov.my  ELX])is one stop-centre for labor market information, as supervised by the Ministry of Human Resource ([http://www.mohr.gov.my  MOHR]), to enable employers and job seekers to communicate on the same platform.

[http://www.esyariah.gov.my/  e-Syariah] is the seventh project under the Electronic Government flagship application of the Multimedia Super Corridor (MSC). A case management system that integrates the processes related to management of cases for the Syariah Courts.

==Challenges to e-services in the Developing World==
The future of e-service is bright but some challenges remain. There are some challenges in e-service, as Sheth &amp; Sharma (2007)&lt;ref&gt;Sheth., J.N. , Sharma, A., (2007). E-Services: A framework for growth.  Journal of Value Chain Management, 1(1/2)&lt;/ref&gt; identify, are:

* Low penetration of ICT especially in the developing countries;
* [[Internet fraud|Fraud]] on the internet space which is estimated around 2.8billion USD
* [[Internet privacy|Privacy]] due the emergence of various types of spyware and security holes, and
* intrusive characteristics of the service (e.g. mobile phones based) as customers may not like to be contacted with the service providers at any time and at any place.

The first challenge and primary obstacle to the e-service platform will be penetration of the internet. In some developing countries, the access to the internet is limited and speeds are also limited. In these cases firms and customers will continue to use traditional platforms. The second issue of concern is fraud on the internet. It is anticipated that the fraud on the e-commerce internet space costs $2.8 billion. Possibility of fraud will continue to reduce the utilization of the internet. The third issue is of privacy. Due to both spyware and security holes in operating systems, there is concern that the transactions that consumers undertake have privacy limitations.  For example, by stealthily following online activities, firms can develop fairly accurate descriptions of customer profiles.  Possibility of privacy violations will reduce the utilizations of the internet. The final issue is that e-service can also become intrusive as they reduce time and location barriers of other forms of contract. For example, firms can contact people through mobile devices at any time and at any place. Customers do not take like the intrusive behavior and may not use the e-service platform. (Heiner and lyer, 2007)&lt;ref&gt;Heiner and lyer (2007) E-Service opportunities and Threats, Journal of value chain management, 1, 11.&lt;/ref&gt;

==Major e-service keywords==
A considerable amount of research efforts already exists on the subject matter exploring different aspects of e-service and e-service delivery ; one worth noting effort is Rowley’s study (2006)&lt;ref name=autogenerated1 /&gt; who did a review study on the e-service literature. The key finding of his study is that there is need to explore dimensions of e-service delivery not focusing only on service quality “In order to understand e-service experiences it is necessary to go beyond studies of e-service quality dimensions and to also take into account the inherent characteristics of e-service delivery and the factors that differentiate one service experience from another.”

Some of the major keywords of e-service as found in the e-government research are as follows:

===Acceptance===
User acceptance of technology is defined according to Morris (1996, referred by Wu 2005, p.&amp;nbsp;1)&lt;ref&gt;Wu, Philip F. (2009). User Acceptance of Emergency Alert Technology: A Case Study. Proceedings of the 6th International ISCRAM Conference – Gothenburg, Sweden&lt;/ref&gt; as “the demonstrable willingness within a user group to employ information technology for the tasks it is designed to support”. This definition can be brought into the context of e-service where acceptance can be defined as the users’ willingness to use e-service or the willingness to decide when and how to use the e-service.

===Accessibility===
Users’ ability to access to the e-service is important theme in the previous literature. For example, Huang (2003)&lt;ref&gt;Huang, C.J. (2003). Usability of E-Government Web Sites for People with Disabilities, In Proceedings of the 36th Hawaii International Conference on System Sciences (HICSS’03), IEEE Computer Society, 2003&lt;/ref&gt; finds that most of the websites in general fail to serve users with disabilities. Recommendation to improve accessibility is evident in previous literature including Jaeger (2006)&lt;ref&gt;Jaeger, P.T. Assessing Section 508 compliance on federal e-government Web sites: A multi-method, user-centered evaluation of accessibility for persons with disabilities. Government Information Quarterly 23 (2006) 169–190&lt;/ref&gt; who suggests the following to improve e-services’ accessibility like: design for accessibility from the outset of website development, Involve users with disabilities in the testing of the site …Focus on the benefits of an accessible Web site to all users.

===Administrative literacy===
According to Grönlund et al. (2007),&lt;ref&gt;Grönlund, Å., Hatakka, M. and Ask, A. (2007) ‘ Inclusion in the E-Service Society – Investigating Administrative Literacy Requirements for Using E-Services’. 6th International Conference (EGOV 2007, Regensburg, Germany), 4656&lt;/ref&gt; for a simple e-service, the needs for knowledge and skills, content and procedures are considerably less. However, in complicated services there are needed to change some prevailed skills, such as replacing verbal skills with skill in searching for information online.

===Benchmarking===
This theme is concerned with establishing standards for measuring e-services or the best practices within the field. This theme also includes the international benchmarking of e-government services (UN reports, EU reports); much critic has been targeting these reports being incomprehensive and useless. According to Bannister (2007)&lt;ref&gt;Bannister F. (2007). The curse of the benchmark: an assessment of the validity and value of e-government comparisons, International Review of Administrative Sciences, 73 (2), 171-188&lt;/ref&gt; “… benchmarks are not a reliable tool for measuring real e-government progress. Furthermore, if they are poorly designed, they risk distorting government policies as countries may chase the benchmark rather than looking at real local and national needs”

===Digital divide===
[[Digital divide]] is considered one of the main barriers to implementing e-services; some people do not have means to access the e-services and some others do not know how to use the technology (or the e-service). According to Helbig et al. (2009),&lt;ref&gt;Helbig, N; Gil-García, J ; Ferro, E (2009). Understanding the complexity of electronic government: Implications from the digital divide literature. Government Information Quarterly, 26(2009), 89–97&lt;/ref&gt; “we suggest E-Government and the digital divide should be seen as complementary social phenomena (i.e., demand and supply). Moreover, a serious e-government digital divide is that services mostly used by social elites."

===E-readiness===
Most of the reports and the established criteria focus on assessing the services in terms of infrastructure and public policies ignoring the citizen participation or [[e-readiness]]. According to by Shalini (2009),&lt;ref&gt;Shalini, R. (2009). Are Mauritians ready for e-Government services?. Government Information Quarterly 26 (2009) 536–539&lt;/ref&gt; “the results of the research project reveal that a high [http://www.eiu.com/site_info.asp?info_name=eiu_2007_e_readiness_rankings&amp;rf=0|e-readiness index] may be only indicating that a country is e-ready in terms of ICT infrastructure and info-structure, institutions, policies, and political commitment, but it is a very poor measure of the e-readiness of citizens. To summarize the findings, it can be said that Mauritius is ready but the Mauritians are not”

``E-readiness, as the Economist Intelligence Unit defines, is the measure of a country’s ability to leverage digital channels for communication, commerce and government in order to further economic and social development. Implied in this measure is the extent to which the usage of communications devices and Internet services creates efficiencies for business and citizens, and the extent to which this usage is leveraged in the development of information and communications technology (ICT) industries. In general terms, the definition of e-readiness is relative,for instance depending on a country in question's priorities and perspective.&lt;ref&gt;GeoSINC International (2002). E-Readiness Guide. Available at http://www.apdip.net/documents/evaluation/e-readiness/geosinc01042002.pdf&lt;/ref&gt;

===Efficiency===
As opposed to effectiveness, efficiency is focused on the internal competence within the government departments when delivering e-services. There is a complaint that researchers focus more on effectiveness “There is an emerging trend seemingly moving away from the efficiency target and focusing on users and governance outcome. While the latter is worthwhile, efficiency must still remain a key priority for eGovernment given the budget constraints compounded in the future by the costs of an ageing population. Moreover, efficiency gains are those that can be most likely proven empirically through robust methodologies”&lt;ref&gt;Codagnone, C.  Undheim T.A (2008).  Benchmarking eGovernment: tools, theory, and practice. European Journal of ePractice. Nº 4 • August 2008&lt;/ref&gt;

===Security===
Security is the most important challenge that faces the implementation of e-services because without a guarantee of privacy and security citizens will not be willing to take up e-government services. These security concerns, such as hacker attacks and the theft of credit card information, make governments hesitant to provide public online services.  According to the GAO report&lt;ref&gt;GAO.(2002). E-Government: Proposal addresses Critical Challenges. U.S General Accounting Office, Govt of the USA&lt;/ref&gt; of 2002 “security concerns present one of the toughest challenges to extending the reach of e-government.The rash of hacker attacks, Web page defacing, and credit card information being posted on electronic bulletin boards can make many federal agency officials—as well as the general public—reluctant to conduct sensitive government transactions involving personal or financial data over the Internet.” By and Large,  Security is one of the major challenges that faces the implementation and development of electronic services. people want to be assured that they are safe when they are conducting online services and that their information will remain secure and confidential

===Stakeholders===
Axelsson et al. (2009)&lt;ref&gt;Axelsson, K, Melin, f, Lindgren, I, (2009) DEVELOPING PUBLIC E-SERVICES FOR SEVERAL STAKEHOLDERS – A MULTIFACETED VIEW OF THE NEEDS FOR AN E-SERVICE. 17th European Conference on Information Systems&lt;/ref&gt; argue that the stakeholder concept-which was originally used in private firms-, can be used in public setting and in the context of e-government. According to them, several scholars have discussed the use of the [[stakeholder theory]] in public settings.&lt;ref&gt;Scholl, H. J. (2001). Applying stakeholder theory to e-government: Benefits and Limits. Kluwer Academic Publishers, Massachusetts&lt;/ref&gt; The stakeholder theory suggests that need to focus on all the involved stakeholder s when designing the e-service; not only on the government and citizens.

===Usability===
Compared to Accessibility, There is sufficient literature that addresses the issue of usability; researchers have developed different models and methods to measure the usability and effectiveness of eGovernment websites. However, But still there is call to improve these measures and make it more compressive&lt;ref&gt;Kaylor, C.,  Deshazo, R. &amp; Eck, D. V. (2001) "Gauging e-government: A report on implementing services among American cities". Government Information Quarterly (GIQ), 18(4), 293 - 307&lt;/ref&gt;

``The word usability has cropped up a few times already in this unit. In the context of biometric identification, usability referred to the smoothness of enrollment and other tasks associated with setting up an identification system. A system that produced few false matches during enrollment of applicants was described as usable. Another meaning of usability is related to the ease of use of an interface. Although this meaning of the term is often used in the context of computer interfaces, there is no reason to confine it to computers.&lt;ref&gt;[http://openlearn.open.ac.uk/mod/resource/view.php?id=211245 Open Learning - OpenLearn - Open University]&lt;/ref&gt;´´

==Social, cultural and ethical implications of e-services==
The perceived effectiveness of e-service can be influenced by public’s view of the social and cultural implications of e-technologies and e-service.

Impacts on individuals’ rights and privacy – as more and more companies and government agencies use technology to collect, store, and make accessible data on individuals, privacy concerns have grown. Some companies monitor their employees' computer usage patterns in order to assess individual or workgroup performance.&lt;ref&gt;Asgarkhani, M. (2002). Strategic Management of Information systems and Technology in an e-World”, Proceedings of the 21st IT Conference, Sri Lanka, pp103-111.&lt;/ref&gt; Technological advancements are also making it much easier for businesses, government and other individuals to obtain a great deal of information about an individual without their knowledge. There is a growing concern&lt;ref name=autogenerated2&gt;Asgarkhani, M. (2002b) “e-Governance in Asia Pacific”, Proceedings of the International Conference on Governance in Asia, Hong Kong.&lt;/ref&gt; that access to a wide range of information can be dangerous within politically corrupt government agencies.

Impact on Jobs and Workplaces - in the early days of computers, management scientists anticipated that computers would replace human decision-makers. However, despite significant technological advances, this prediction is no longer a mainstream concern. At the current time, one of the concerns associated with computer usage in any organization (including governments) is the health risk – such as injuries related to working continuously on a computer keyboard. Government agencies are expected to work with regulatory groups in order to avoid these problems.

Potential Impacts on Society – despite some economic benefits of ICT to individuals, there is evidence that the computer literacy and access gap between the haves and have-nots may be increasing. Education and information access are more than ever the keys to economic prosperity, yet access by individuals in different countries is not equal - this social inequity has become known as the digital divide.

Impact on Social Interaction – advancements in ICT and e-Technology solutions have enabled many government functions to become automated and information to be made available online. This is a concern to those who place a high value on social interaction.

Information Security - technological advancements allow government agencies to collect, store and make data available online to individuals and organizations. Citizens and businesses expect to be allowed to access data in a flexible manner (at any time and from any location). Meeting these expectations comes at a price to government agencies where it concerns managing information – more specifically, ease of access; data integrity and accuracy; capacity planning to ensure the timely delivery of data to remote (possibly mobile) sites; and managing the security of corporate and public information.&lt;ref name=autogenerated2 /&gt;

==E-service awards==
The benefits of e-services in advancing businesses efficiency and in promoting good governance are huge; recognizing the importance of these benefits has resulted in number of international awards that are dedicated to recognize the best designed e-services. In the section, we will provide description of some international awards

===Best online e-service in Europe===
[http://www.epractice.eu/en/awards|The European eGovernment Awards program] started 2003 to recognize the best online public service in Europe. The aim of Awards is to encourage the deployment of e-services and to bring the attention to best practices in the field. The winners of the [http://ec.europa.eu/idabc/en/document/7842|4th European eGovernment Awards] were announced in the award ceremony that took place at the [http://www.egov2009.se/ 5th Ministerial eGovernment Conference] on 19 November 2009 (Sweden); the winners in their respective categories are:

* Category 1. eGovernment supporting the Single Market: EU-OPA, the European Order for Payment Application ({{flag|Austria}} and {{flag|Germany}})
* Category 2a. eGovernment empowering citizens: Genvej ({{flag|Denmark}})
* Category 2b. eGovernment empowering businesses: MEPA, the Public Administration eMarketplace ({{flag|Italy}})
* Category 3. eGovernment enabling administrative efficiency and effectiveness: Licensing of Hunters via the “Multibanco” ATM Network ({{flag|Portugal}})
* Public prize: SMS Information System ({{flag|Turkey}})

===Other awards===

[http://www.ita.gov.om/HMAward/ Sultan Qaboos Award for excellence in eGovernance] {{flag|Oman}}(Started 2009) The award has five categories: Best eContent, Best eService, Best eProject, eEconomy, eReadiness.

[http://www.egovawards.bh/AboutEn.aspx?Id=1|Bahrain eGovernment Excellence Awards] {{flag|Bahrain}}(Started 2007) The program has three categories: Government Awards: Best eContent, Best eService, Best eProject, eEconomy, eEducation, eMaturity Business Awards: Best ICT solution Provider, eEconomy,eEducation Citizen Awards: Best eContent, eCitizen.

[http://www.e-servicesphils.com/esp2010/ Philippines e-Service Awards] {{flag|Philippines}}(Started 2001) Categories: Outstanding Client Application of the Year, Outstanding Customer Application of the year, Groundbreaking Technology of the Year, Most Progressive Homegrown Company of the Year.

==Major journals focusing on e-services==
There are some journals particularly interested for “e-Service “. Some of these are:
* [http://www.igi-global.com/journal/international-journal-services-mobile-applications/1114  International Journal of E-services and Mobile Applications]
* [http://eservicejournal.org/ eService Journal]
* [http://www.palgrave-journals.com/ejis/index.html  European Journal of Information Systems]
* [http://www.misq.org/  MIS Quarterly]
* [http://www.elsevier.com/wps/find/journaldescription.cws_home/505553/description#description  Information &amp; Management]
* [http://www.wiley.com/bw/journal.asp?ref=1350-1917&amp;site=1  Information Systems Journal]
* [http://www.igi-global.com/Bookstore/TitleDetails.aspx?TitleId=1091  International Journal of Electronic Government]
* [http://www.ejeg.com/  Electronic Journal of e-Government]
* [http://www.gvsu.edu/business/ijec/  International Journal of Electronic Commerce]
* [http://www.emeraldinsight.com/Insight/viewContainer.do?containerType=Journal&amp;containerId=11229  Internet Research]
* [http://www.palgrave-journals.com/jit/index.html  Journal Information Technology]
* [http://www.elsevier.com/wps/find/journaldescription.cws_home/525447/description#description  Journal of Strategic Information Systems]
* [http://aisel.aisnet.org/jais/  Journal of the Association for Information Systems]
* [http://www.elsevier.com/wps/find/journaldescription.cws_home/620202/description#description  Government Information Quarterly]
* [http://www.wiley.com/bw/journal.asp?ref=0033-3352  Public Administration Review]

==Major conferences focusing on e-services==

Major conferences considering e-service as one of the themes are:
* [http://ec.europa.eu/information_society/newsroom/cf/itemdetail.cfm?item_id=5649&amp;utm_campaign=isp&amp;utm_medium=rss&amp;utm_source=newsroom&amp;utm_content=tpa-8  eServices in European Civil Registration conference]
* [http://www.iist.unu.edu/I3E/IFIP  Conference on e-Business, e-Services, and e-Society]
* [http://www.eafricaconference.org/  International ICST Conference on e-service]
* [http://www.e-servicesphils.com/esp2010/  E-service Global Sourcing Conference &amp; Exhibition]
* [http://www.hicss.hawaii.edu/hicss_43/minitracks/eg-sin.htm  Annual Hawaii International Conference on Systems Sciences]
* [http://www.egov-conference.org/egov-2011-preview  Electronic Government Conference (EGOV)]
* [http://www.dexa.org/  International Conference on Electronic Government and the Information Systems Perspective (EGOVIS)]
* [http://www.icegov.org/  International Conference on Theory and Practice of Electronic Governance ( ICEGOV)]

==See also==
* [[Electronic services delivery]]
* [[Customer knowledge]]

==References==
{{Reflist}}

==External links==
* [http://www.e-govwatch.org.nz/criteria/e-services_delivery.html  E-services delivery]
* [http://www.computerworld.com/s/article/9005371/Report_Card_The_Best_E_Government_Sites  The Best E-Government Sites]
* [http://egov.infodev.org/en/Section.78.html#citizen-or-business-centric-portals  The World Bank (InfoDev) e-Government toolkit]

[[Category:Digital divide]]
[[Category:E-commerce]]
[[Category:Information technology]]
[[Category:Knowledge representation]]
[[Category:Open government]]
[[Category:Technology in society]]</text>
      <sha1>1uvymloeaak019ayqdda6ucfg918dlk</sha1>
    </revision>
  </page>
  <page>
    <title>Figurative system of human knowledge</title>
    <ns>0</ns>
    <id>464119</id>
    <revision>
      <id>755233275</id>
      <parentid>737857487</parentid>
      <timestamp>2016-12-17T00:17:14Z</timestamp>
      <contributor>
        <username>The Transhumanist</username>
        <id>1754504</id>
      </contributor>
      <comment>/* External links */ English link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="12594" xml:space="preserve">[[File:ENC SYSTEME FIGURE.jpeg|right|200px|thumb|[[Classification chart]] with the original "figurative system of human knowledge" tree, in French.]]

The '''"figurative system of human knowledge"''', sometimes known as '''the tree of Diderot and d'Alembert''', was a tree developed to represent the structure of [[knowledge]] itself, produced for the ''[[Encyclopédie]]'' by [[Jean le Rond d'Alembert]] and [[Denis Diderot]].

The tree was a [[Taxonomy (general)|taxonomy]] of human knowledge, inspired by [[Francis Bacon]]'s ''[[The Advancement of Learning]]''. The three main branches of knowledge in the tree are: "Memory"/[[History]], "Reason"/[[Philosophy]], and "Imagination"/[[Poetry]].

Notable is the fact that [[theology]] is ordered under 'Philosophy'. The historian [[Robert Darnton]] has argued that this categorization of [[religion]] as being subject to human reason, and not a source of knowledge in and of itself ([[revelation]]), was a significant factor in the controversy surrounding the work.&lt;ref&gt;Robert Darnton, "Philosophers Trim the Tree of Knowledge: The Epistemological Strategy of the ''Encyclopedie''," ''The Great Cat Massacre and Other Episodes in French Cultural History'' (New York: Basic Books, Inc., 1984), 191-213.&lt;/ref&gt;  Additionally notice that 'Knowledge of God' is only a few nodes away from 'Divination' and 'Black Magic'.

The original version, in [[French (language)|French]], can be seen in the graphic on the right. An [http://www.hti.umich.edu/d/did/tree.html image of the diagram with English translations superimposed over the French text] is available. Another example of English translation of the tree is available in literature (see the reference by Schwab). Below is a version of it rendered in [[English (language)|English]] as a bulleted outline.

== ''The Tree of Diderot and d'Alembert'' ==
'''"Detailed System of Human Knowledge"'''
from the [[Encyclopédie]].
* [[Understanding]]
:* [[Memory]].
::* [[History]].
:::* [[Sacred history|Sacred]] (History of [[Prophet]]s).
:::* [[History of Christianity|Ecclesiastical]].
:::* [[Civilization#History|Civil]], [[Ancient history|Ancient]] and [[Modern history|Modern]].
::::* [[Civilization#History|Civil History]], properly said. ''(See also: [[Civil society#History|History of civil society]])''
::::* [[Literary History]].
:::::* [[Memoirs]].
:::::* [[Antiquities]]. ''(See also: [[Classical antiquity]])''
:::::* Complete Histories.
:::* [[Natural history|Natural]].
::::* Uniformity of Nature. ''(See: [[Uniformitarianism]])''
:::::* [[Cosmology|Celestial History]].
:::::* History...
::::::* of [[Meteoroid#History|Meteors]].
::::::* of the [[History of the Earth|Earth]] and the [[World Ocean|Sea]] ''(See also: [[Origin of water on Earth]])''
::::::* of [[Minerals]]. ''(See also:  [[Geological history of Earth]])''
::::::* of [[Vegetable]]s. ''(See also: [[History of agriculture]])''
::::::* of [[Animal]]s.  ''(See also: [[Evolutionary history of life]])''
::::::* of the [[Chemical element|Elements]]. ''(See also: [[Classical element]], [[History of alchemy]], and [[History of chemistry]])''
::::* Deviations of Nature.
:::::* [[Celestial object|Celestial Wonders]].
:::::* [[Meteoroid#Frequency of large meteors|Large Meteors]]. ''(See also: [[Asteroid]]s)''
:::::* Wonders of Land and Sea. ''(See: [[Wonders of the World]])''
:::::* [[Mineral#Other properties|Monstrous Mineral]]s.
:::::* Monstrous Vegetables. ''(See: [[Largest organisms#Plants|Largest plants]], [[Poisonous plant]]s, and [[Carnivorous plant]]s)''
:::::* Monstrous Animals. (See: ''[[Largest organisms#Animals|Largest animals]] and [[Predator]]s)''
:::::* Wonders of the Elements. ''(See: [[Natural disaster]]s)''
::::* Uses of Nature (See ''[[Technology]] and [[Applied science]]s)''
:::::* Arts, [[Craft]]s, Manufactures.
::::::* Work and Uses of [[Gold]] and [[Silver]].
:::::::* [[Mint (coin)|Minting]].
:::::::* [[Goldsmith]].
:::::::* Gold Spinning.
:::::::* Gold Drawing.
:::::::* [[Silversmith]]
:::::::* [[Planishing|Planisher]], etc.
::::::* Work and Uses of Precious Stones.
:::::::* [[Lapidary]].
:::::::* [[Diamond cutting]].
:::::::* [[Jewellery|Jeweler]], etc.
::::::* Work and Uses of [[Iron]].
:::::::* Large [[Forge|Forges]].
:::::::* [[Locksmithing|Locksmith]].
:::::::* Tool Making.
:::::::* Armorer.
:::::::* Gun Making, etc.
::::::* Work and Uses of [[Glass]].
:::::::* [[Glass|Glassmaking]].
:::::::* [[Plate-Glass|Plate-Glassmaking]].
:::::::* [[Mirror#Manufacture|Mirror Making]].
:::::::* [[Optician]].
:::::::* [[Glazier]], etc.
::::::* Work and Uses of Skin.
:::::::* [[Tanner (occupation)|Tanner]].
:::::::* [[Chamois leather|Chamois Maker]].
:::::::* Leather Merchant.
:::::::* [[Glove]] Making, etc.
::::::* Work and Uses of [[Stonemasonry|Stone]], [[Plaster#Uses|Plaster]], [[Slate#Uses|Slate]], etc.
:::::::* Practical [[Architecture]].
:::::::* Practical [[Sculpture]].
:::::::* [[Masonry|Mason]].
:::::::* [[Tiler]], etc.
::::::* Work and Uses of [[Silk#Uses|Silk]].
:::::::* Spinning.
:::::::* Milling.
:::::::* Work like.
:::::::* [[Velvet]].
:::::::* Brocaded Fabrics, etc.
::::::* Work and Uses of [[Wool]].
:::::::* Cloth-Making.
:::::::* Bonnet-Making, etc.
::::::* Working and Uses, etc.
:* [[Reason]]
::* [[Philosophy]]
:::* General [[Metaphysics]], or [[Ontology]], or Science of Being in General, of Possibility, of [[Existence]], of Duration, etc.
:::* Science of [[God]].
::::* [[Natural Theology]].
::::* Revealed [[Theology]].
::::* Science of Good and Evil Spirits.
:::::* [[Divination]].
:::::* [[Black Magic]].
:::* Science of Man.
::::* [[Pneumatology]] or Science of the [[Soul]].
:::::* Reasonable.
:::::* Sensible.
::::* [[Logic]].
:::::* Art of [[Outline of thought|Thinking]].
::::::* [[Apprehension (understanding)|Apprehension]].
:::::::* Science of [[Idea]]s
::::::* [[Judgement]].
:::::::* Science of [[Proposition]]s.
::::::* [[Reasoning]].
:::::::* [[Inductive reasoning|Induction]].
::::::* [[Reasoning#Logical_reasoning_methods_and_argumentation|Method]].
:::::::* Demonstration.
::::::::* [[Analysis]].
::::::::* [[:wikt:-synthesis|Synthesis]].
:::::* Art of Remembering.
::::::* [[Memory]].
:::::::* Natural.
:::::::* [[Art of memory|Artificial]].
::::::::* Prenotion.
::::::::* Emblem.
::::::* Supplement to Memory.
:::::::* [[Writing]].
:::::::* [[Printing]].
::::::::* [[Alphabet]].
::::::::* Cipher.
:::::::::* Arts of [[Writing]], Printing, [[Reading (process)|Reading]], Deciphering.
::::::::::* [[Orthography]].
:::::* Art of [[Communication]]
::::::* Science of the Instrument of [[Discourse]].
:::::::* [[Grammar]].
::::::::* [[Sign]]s.
:::::::::* [[Gesture]].
::::::::::* [[Mime|Pantomime]].
::::::::::* Declamation.
:::::::::* Characters.
::::::::::* [[Ideogram]]s.
::::::::::* [[Hieroglyphics]].
::::::::::* [[Heraldry]] or Blazonry.
::::::::* [[Prosody (linguistics)|Prosody]].
::::::::* Construction.
::::::::* [[Syntax]].
::::::::* [[Philology]].
::::::::* Critique.
:::::::* [[Pedagogy]].
::::::::* [[Curriculum|Choice of Studies]].
::::::::* [[Teaching method|Manner of Teaching]].
::::::* Science of Qualities of [[Discourse]].
:::::::* [[Rhetoric]].
:::::::* Mechanics of [[Poetry]].
::::* [[Outline of ethics|Ethics]].
:::::* [[Contemporary ethics|General]].
::::::* General Science of [[Good and evil|Good and Evil]], of duties in general, of [[Virtue]], of the necessity of being Virtuous, etc.
:::::* [[Outline of ethics#Branches of ethics|Particular]].
::::::* Science of [[Law]]s or [[Jurisprudence]].
:::::::* [[Natural law|Natural]].
:::::::* [[Economic forces|Economic]]. ''(See also [[commercial law]])''
:::::::* [[Politics|Political]]. ''(See also [[political law]])''
::::::::* [[Domestic politics|Internal]] and [[International politics|External]]. ''(See also [[foreign policy]])''
::::::::* [[Commerce]] on Land and [[Maritime industry|Sea]].
:::* [[Natural science|Science of Nature]]
::::* [[Metaphysics]] of Bodies or, General Physics, of Extent, of Impenetrability, of Movement, of Word, etc.
::::* [[Outline of mathematics|Mathematics]].
:::::* [[Pure mathematics|Pure]].
::::::* [[Outline of arithmetic|Arithmetic]].
:::::::* [[Number|Numeric]].
:::::::* [[Algebra]].
::::::::* [[Elementary algebra|Elementary]].
::::::::* [[Infinitesimal]].
:::::::::* [[Differential algebra|Differential]].
:::::::::* [[Integral]].
::::::* [[Outline of geometry|Geometry]].
:::::::* Elementary (Military Architecture, Tactics).
:::::::* Transcendental (Theory of Courses).
:::::* Mixed.
::::::* [[Mechanics]].
::::::::* [[Statics]].
:::::::::* Statics, properly said.
:::::::::* [[Hydrostatics]].
::::::::* [[Dynamics (mechanics)|Dynamics]].
:::::::::* Dynamics, properly said.
:::::::::* [[Ballistics]].
:::::::::* [[Hydrodynamics]].
::::::::::* [[Hydraulics]].
::::::::::* [[Navigation]], Naval Architecture.
::::::* Geometric [[Astronomy]].
:::::::* [[Cosmography]].
::::::::* [[Celestial cartography|Uranography]].
::::::::* [[Geography]].
::::::::* [[Hydrography]].
:::::::* [[Chronology]].
:::::::* [[Gnomon]]ics.
::::::* [[Optics]].
:::::::* Optics, properly said.
:::::::* [[Dioptrics]], Perspective.
:::::::* [[Catoptrics]].
::::::* [[Acoustics]].
::::::* [[Pneumatics]].
::::::* Art of Conjecture. [[probability|Analysis of Chance]].
:::::* Physicomathematics.
::::* Particular Physics.
:::::* [[Outline of zoology|Zoology]].
::::::* [[Anatomy]].
:::::::* Simple.
:::::::* [[Comparative anatomy|Comparative]].
::::::* [[Physiology]].
::::::* [[Outline of medicine|Medicine]].
:::::::* Hygiene.
::::::::* [[Hygiene]], properly said.
::::::::* Cosmetics (Orthopedics).
::::::::* Athletics (Gymnastics).
:::::::* Pathology.
:::::::* Semiotics.
:::::::* Treatment.
::::::::* Diete.
::::::::* [[Surgery]].
::::::::* Pharmacy.
::::::* [[Veterinary medicine|Veterinary Medicine]].
::::::* [[Horse care|Horse Management]].
::::::* [[Hunting]].
::::::* [[Outline of fishing|Fishing]].
::::::* [[Falconry]].
:::::* Physical [[Astronomy]].
::::::* [[Astrology]].
:::::::* Judiciary Astrology.
:::::::* Physical Astrology.
:::::* [[Meteorology]].
:::::* [[Cosmology]].
::::::* Uranology.
::::::* [[Aerology]].
::::::* [[Geology]].
::::::* [[Hydrology]].
:::::* [[Botany]].
::::::* [[Agriculture]].
::::::* [[Gardening]].
:::::* [[Mineralogy]].
:::::* [[Chemistry]].
::::::* Chemistry, properly said, ([[Pyrotechnics]], Dyeing, etc.).
::::::* [[Metallurgy]].
::::::* [[Alchemy]].
::::::* Natural Magic.
:* Imagination.
::* [[Poetry]].
:::* Sacred, Profane.
::::* Narrative.
:::::* [[Epic poetry|Epic Poem]]
:::::* [[Madrigal (poetry)|Madrigal]]
:::::* [[Epigram]]
:::::* [[Novel]], etc.
::::* [[Drama|Dramatic]]
:::::* [[Tragedy]]
:::::* [[Comedy]]
:::::* [[Pastoral]], etc.
::::* Parable
:::::* [[Allegory]]
(NOTE: THIS NEXT BRANCH SEEMS TO BELONG TO BOTH THE NARRATIVE AND DRAMATIC TREE AS DEPICTED BY THE LINE DRAWN CONNECTING THE TWO.)
::::* [[Outline of music|Music]]
:::::* [[Music theory|Theoretical]]
:::::* Practical ''(see also [[musical technique]])''
:::::** [[Instrumental]]
:::::** [[vocal music|Vocal]]
::::* [[Outline of painting|Painting]]
::::* [[Outline of sculpture|Sculpture]]
::::* [[Engraving]]

== See also ==
* [[Classification chart]]
* [[Instauratio magna]]
* [[Propædia]]
* [[Pierre Mouchon]]

== References ==
{{reflist}}

== Further reading ==
* Robert Darnton, "Epistemological angst: From encyclopedism to advertising," in Tore Frängsmyr, ed., ''The structure of knowledge: classifications of science and learning since the Renaissance'' (Berkeley, CA: Office for the History of Science and Technology, University of California, Berkeley, 2001).
* Adams, David (2006) 'The Système figuré des Connaissances humaines and the structure of Knowledge in the Encyclopédie',  in Ordering the World, ed. Diana Donald and Frank O'Gorman, London: Macmillan, p.&amp;nbsp;190-215. 
* ''Preliminary discourse to the Encyclopedia of Diderot'', Jean Le Rond d'Alembert, translated by Richard N. Schwab, 1995. ISBN 0-226-13476-8

==External links==

* [http://quod.lib.umich.edu/d/did/tree.html The ''Tree'' translated into English]
* [http://artfl.uchicago.edu/cactus/ ESSAI D'UNE DISTRIBUTION GÉNÉALOGIQUE DES SCIENCES ET DES ARTS PRINCIPAUX, published as a fold-out frontispiece in volume 1 of Pierre Mouchon, ''Table analytique et raisonnée des matieres contenues dans les XXXIII volumes in-folio du Dictionnaire des sciences, des arts et des métiers, et dans son supplément'', Paris, Panckoucke 1780.]

{{DEFAULTSORT:Figurative System Of Human Knowledge}}
[[Category:Taxonomy]]
[[Category:Age of Enlightenment]]
[[Category:Trees (data structures)]]
[[Category:Knowledge representation]]</text>
      <sha1>2ni1slkgn6gv6qjg3a08pb6samuqoem</sha1>
    </revision>
  </page>
  <page>
    <title>Darwin Core Archive</title>
    <ns>0</ns>
    <id>29824007</id>
    <revision>
      <id>689756392</id>
      <parentid>620216008</parentid>
      <timestamp>2015-11-09T05:45:01Z</timestamp>
      <contributor>
        <ip>2602:306:3651:8E50:2527:DDD9:A609:BBB</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3819" xml:space="preserve">{{Orphan|date=January 2011}}

'''Darwin Core Archive''' (DwC-A) is a [[biodiversity informatics]] data standard that makes use of the [[Darwin Core]] terms to produce a single, self-contained dataset for species occurrence or checklist data. Essentially it is a set of text (CSV) files with a simple descriptor (meta.xml) to inform others how your files are organized. The format is defined in the Darwin Core Text Guidelines.&lt;ref name="dwc-text"&gt;[http://rs.tdwg.org/dwc/terms/guides/text/index.htm Darwin Core Text Guidelines]&lt;/ref&gt; It is the preferred format for publishing data to the [[GBIF]] network. 

__TOC__

==Darwin Core==
The Darwin Core standard has been used to mobilise the vast majority of specimen occurrence and observational records within the GBIF network.&lt;ref name="gbif-dwca"&gt;[http://www.gbif.org/resources/2552 GBIF Darwin Core Archive, How-to Guide]&lt;/ref&gt; The [[Darwin Core]] standard was originally conceived to facilitate the discovery, retrieval, and integration of information about modern biological specimens, their spatio-temporal occurrence, and their supporting evidence housed in collections (physical or digital).

The Darwin Core today is broader in scope. It aims to provide a stable, standard reference for sharing information on biological diversity. As a glossary of terms, the Darwin Core provides stable semantic definitions with the goal of being maximally reusable in a variety of contexts. This means that Darwin Core may still be used in the same way it has historically been used, but may also serve as the basis for building more complex exchange formats, while still ensuring interoperability through a common set of terms.

==Archive Format==
{{unreferenced section|date=December 2010}}
The central idea of an archive is that its data files are logically arranged in a star-like manner, with one core data file surrounded by any number of ’extensions’. Each extension record (or ‘extension file row’) points to a record in the core file; in this way, many extension records can exist for each single core record.

Details about recommended extensions can be found in their respective subsections and will be extensively documented in the GBIF registry, which will catalogue all available extensions.

Sharing entire datasets instead of using pageable web services like DiGIR and TAPIR allows much simpler and more efficient data transfer. For example, retrieving 260,000 records via TAPIR takes about nine hours, issuing 1,300 http requests to transfer 500 MB of XML-formatted data. The exact same dataset, encoded as DwC-A and zipped, becomes a 3 MB file. Therefore, GBIF highly recommends compressing an archive using ZIP or GZIP when generating a DwC-A. 

An archive requires stable identifiers for core records, but not for extensions. For any kind of shared data it is therefore necessary to have some sort of local record identifiers. It’s good practice to maintain – with the original data – identifiers that are stable over time and are not being reused after the record is deleted. If you can, please provide globally unique identifiers instead of local ones.

===Archive Descriptor===
To be completed.

&lt;!--
===Data Files===
To be completed.
--&gt;

===Dataset Metadata===
A Darwin Core Archive should contain a file containing metadata describing the whole dataset. The [[Ecological Metadata Language]] (EML) is the most common format for this, but simple Dublin Core files are being used too.

==References==
{{reflist}}

==External links==
* [http://rs.tdwg.org/dwc/terms/index.htm Darwin Core Quick Reference Guide]
* [[Biodiversity Information Standards]] (TDWG)
* [[Global Biodiversity Information Facility]] (GBIF)
* [[Biodiversity informatics]]

[[Category:Bioinformatics]]
[[Category:Knowledge representation]]
[[Category:Interoperability]]</text>
      <sha1>fb4ho39he1qckw8kou2ph0vq8qicyxz</sha1>
    </revision>
  </page>
  <page>
    <title>Geopolitical ontology</title>
    <ns>0</ns>
    <id>20250365</id>
    <revision>
      <id>739443231</id>
      <parentid>739443177</parentid>
      <timestamp>2016-09-14T18:31:39Z</timestamp>
      <contributor>
        <username>Tango303</username>
        <id>18434560</id>
      </contributor>
      <comment>/* Definitions and examples */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="14743" xml:space="preserve">The '''FAO geopolitical ontology''' is an [[Ontology (information science)|Ontology]] developed by the [[FAO|Food and Agriculture Organization of the United Nations (FAO)]] to describe, manage and exchange data related to geopolitical entities such as countries, territories, regions and other similar areas.

==Definitions and examples==
An [[ontology (information science)|ontology]] is a kind of dictionary that describes information  in a certain domain using concepts and relationships. It is often implemented using [[Web Ontology Language|OWL]] (Web Ontology Language), an [[XML]]-based standard language  that can be interpreted by computers.

* A ''Concept'' is defined as abstract knowledge. For example, in the geopolitical ontology a [[United Nations list of Non-Self-Governing Territories|non-self-governing territory]] or a [[geographical region|geographical group]] are concepts. Concepts are explicitly implemented in the ontology with individuals and classes:
** An ''individual'' is defined as an object perceived from the real world. In the geopolitical domain   [[Ethiopia]] or the [[least developed countries]] group are individuals.
** A ''class'' is defined as a set of individuals sharing common properties. In the geopolitical domain, [[Ethiopia]], [[Republic of Korea]] or [[Italy]] are individuals of the class ''self-governing'' territory; and [[least developed countries]] is an individual of the class ''special group''.
* Relationships between concepts are explicitly implemented by:
** ''[[Object (computer science)|Object]] properties'' between individuals of two classes. For example, ''has member'' and ''is in group'' properties, as shown in Figure 1.
** ''[[Datatype]] properties'' between individuals and literals or [[XML]] datatypes. For example, the individual [[Afghanistan]] has the datatype property ''CodeISO3'' with the value "AFG".
** ''Restrictions'' in classes and/or properties. For example, the property ''official English name'' of the class ''self-governing'' territory has been restricted to have only ''one'' value, this means that a self-governing territory (or country) can only have one internationally recognized official English name.&lt;ref&gt;Official names of countries from [http://www.fao.org/faoterm/nocs/pages/homeNocs.jsp?members=allC&amp;lang=en&amp;lang2=en FAO terminology database]&lt;/ref&gt;

[[Image:Concepts November 19 2008 v 2.png||thumb|600px|center|Figure 1. An example of concepts and relationship in the geopolitical ontology.]]

The advantage of describing information in an ontology is that it enables to acquire domain knowledge by defining hierarchical structures of classes, adding individuals, setting object properties and datatype properties, and assigning restrictions.

==FAO ontology==
The geopolitical ontology provides names in seven languages (Arabic, Chinese, French, English, Spanish, Russian and Italian) and identifiers in various international coding systems ([[ISO 3166-1 alpha-2|ISO2]], [[ISO 3166-1 alpha-3|ISO3]], [[AGROVOC]], [[Food and Agriculture Organization Corporate Statistical Database|FAOSTAT]], [http://www.fao.org/faoterm/index.asp?lang=EN FAOTERM], [[Global Administrative Unit Layers (GAUL)|GAUL]], [[UN]], [[List of UNDP country codes|UNDP]] and [[DBPedia]]ID codes) for territories and groups. Moreover, the [[FAO]] geopolitical ontology tracks historical changes from 1985 up until today;&lt;ref&gt;Country or area code changes since 1982:  [http://unstats.un.org/unsd/methods/m49/m49chang.htm United Nations Statistics Division - country or area codes added or changed]&lt;/ref&gt; provides [[geolocation]] (geographical coordinates); implements relationships among [[countries]] and countries, or countries and groups, including properties such as ''has border with'', ''is predecessor of'', ''is successor of'', ''is administered by'', ''has members'', and ''is in group''; and disseminates country statistics including country area, land area, agricultural area, [[GDP]] or [[population]].

The FAO geopolitical ontology provides a structured description of data sources. This includes: source name, source identifier, source creator and source's update date. Concepts are described using the [[Dublin Core]] vocabulary (http://purl.org/dc/elements/1.1/description).

In summary, the main objectives of the FAO geopolitical ontology are:

* To provide the most updated geopolitical information (names, codes, relationships, statistics)
* To track historical changes in geopolitical information
* To improve information management and facilitate standardized data sharing of geopolitical information
* To demonstrate the benefits of the geopolitical ontology to improve [[interoperability]] of [[corporate]] [[information systems]]

It is possible to '''download''' the FAO geopolitical ontology in [http://aims.fao.org/geopolitical.owl OWL] and [http://www.fao.org/countryprofiles/geoinfo/geopolitical/resource/ RDF] formats. Documentation is available in the [[FAO Country Profiles]] [http://www.fao.org/countryprofiles/geoinfo.asp?lang=en Geopolitical information] web page.

==Features of the FAO ontology==
The geopolitical ontology contains :

*Area types:&lt;ref&gt;When an area (territory or group) changed but kept the same name, the ontology differentiates the two areas by sub-fixing the name of the obsolete one with the year (e.g. “FAO 2006”). The year indicates the beginning of validity of that particular area.&lt;/ref&gt;
**Territories: [[List of sovereign states|self-governing]], [[United Nations list of Non-Self-Governing Territories|non-self-governing]], [[Disputed area|disputed]], other.&lt;ref&gt;The area type ''Other'' is used for [[Antarctica]] which has no government and belongs to no country. See also [[Antarctica#Politics|Politics in Antarctica]].&lt;/ref&gt;
**Groups: [[organizations]], [[subregion|geographic]], [[economic union|economic]] and special groups.&lt;ref&gt;Special groups term is used for non-economical or greographical territory groups like the [[Small Island Developing States]], [[Landlocked countries|Land Locked Countries]], Low Income Food Deficit Countries, [[Least Developed Countries]], etc.&lt;/ref&gt;
*Names &lt;ref&gt;UN official names: [http://unstats.un.org/unsd/geoinfo/uncsgnreports.htm Reports of the United Nations Conference on the Standardization of Geographical Names]&lt;/ref&gt; (official, short and names for lists) in Arabic, Chinese, English, French, Spanish, Russian and Italian.
*International codes: UN code – M49, [[ISO 3166]] Alpha-2 and Alpha-3, [[List of UNDP country codes|UNDP code]], [[Global Administrative Unit Layers (GAUL)|GAUL]] code, FAOSTAT, [[AGROVOC]] FAOTERM and [[DBPedia]]ID.
*Coordinates: maximum [[latitude]], minimum [[latitude]], maximum [[longitude]], minimum [[longitude]].
*Basic country statistics: country area, land area, agricultural area, GDP, population.
*Currency names and codes.
*Adjectives of nationality.
*Relations:
**Groups membership.
**Neighbours (land [[border]]), administration of [[United Nations list of Non-Self-Governing Territories|non-self-governing]].
**Historic changes: predecessor, successor, valid since,&lt;ref&gt;The value of the datatype property "validSince" is the first year of validity  of a territory or group. The geopolitical ontology traces back historic changes only until 1985. Therefore if an area has a validSince = 1985, this indicates that the area is valid at least since 1985.&lt;/ref&gt; valid until.&lt;ref&gt;The value of the datatype property "validUntil" is the last year of validity of the territory or group. In case the area is currently valid, this value is set by default to 9999.&lt;/ref&gt;

==Implementation into  OWL==
The [[FAO]] geopolitical ontology is implemented in [[Web Ontology Language|OWL]]. It consists of classes, properties, individuals and restrictions. Table 1 shows all classes, gives a brief description and lists some individuals that belong to each class. Note that the current version of the geopolitical ontology does not provide individuals of the class "disputed" territories. Table 2 and Table 3 illustrate datatype properties and object properties.

&lt;!-- Deleted image removed: [[Image:Class and instances in the geopolitical ontology v 1.png||thumb|667px|center|Table 1. Classes and instances in the geopolitical ontology.]] --&gt;

&lt;!-- Deleted image removed: [[Image:Datatype properties in the geopolitical ontology v 1.png||thumb|667px|center|Table 2. Datatype properties in the geopolitical ontology.]] --&gt;

[[Image:Object properties in the geopolitical ontology v 1.png||thumb|674px|center|Table 3. Object properties in the geopolitical ontology.]]

== Geopolitical ontology in Linked Open Data ==
&lt;!-- Deleted image removed: [[File:Geopol LOD.png|thumb|200px|left|Figure 2. RDF version of FAO geopolitical ontology]]  --&gt;

The FAO Geopolitical ontology is embracing the [http://linkeddata.org W3C Linked Open Data (LOD) initiative] and released its RDF version of the geopolitical ontology in March 2011. 
The term 'Linked Open Data' refers to a set of best practices for publishing and connecting structured data on the Web. The key technologies that support Linked Data are URIs, HTTP and RDF.

The RDF version of the geopolitical ontology is compliant with all [http://www.w3.org/DesignIssues/LinkedData.html Linked data principles] to be included in the [http://richard.cyganiak.de/2007/10/lod/ Linked Open Data cloud], as explained in the following.

==Resolvable http:// URIs ==
Every resource in the OWL format of the FAO Geopolitical Ontology has a unique URI. Dereferenciation was implemented to allow for three different URIs to be assigned to each resource as follows:  
* URI identifying the non-information  resource
* Information resource with an RDF/XML representation
* Information resource with an HTML representation
In addition the current URIs used for OWL format needed to be kept to allow for backwards compatibility for other systems that are using them. Therefore, the new URIs for the FAO Geopolitical Ontology in LOD were carefully created, using  “Cool URIs for Semantic Web”  and considering other good practices for URIs, such as DBpedia URIs.

==New URIs==
The URIs of the geopolitical ontology need to be permanent, consequently all transient information, such as year, version, or format was avoided in the definition of the URIs. 
The new URIs can be accessed at
http://www.fao.org/countryprofiles/geoinfo/geopolitical/resource/ 
For example, for the resource “Italy” the URIs are the following: 
;http://www.fao.org/countryprofiles/geoinfo/geopolitical/resource/Italy
: identifies the non-information resource. 
;http://www.fao.org/countryprofiles/geoinfo/geopolitical/data/Italy
: identifies the resource with an RDF/XML representation. 
;http://www.fao.org/countryprofiles/geoinfo/geopolitical/page/Italy
:identifies the information resource with an HTML representation.
In addition, “owl:sameAs” is used to map the new URIs to the OWL representation.

==Dereferencing URIs==
When a non-information resource is looked up without any specific representation format, then the server needs to redirect the request to information resource with an HTML representation. 
For example, to retrieve the resource “Italy” (http://www.fao.org/countryprofiles/geoinfo/geopolitical/resource/Italy), which is a non-information resource, the server redirects to the html page of “Italy” (http://www.fao.org/countryprofiles/geoinfo/geopolitical/page/Italy).

==At least 1000 triples in the datasets==
The total number of triple statements in FAO Geopolitical Ontology is 22,495. 
At least 50 links to a dataset already in the current LOD Cloud:  
FAO Geopolitical Ontology has 195 links to [http://www.dbpedia.org DBpedia], which is already part of the LOD Cloud.

==Access to the entire dataset==
FAO Geopolitical Ontology provides the entire dataset as a RDF dump. It is available at http://www.fao.org/countryprofiles/geoinfo/geopolitical/data

The RDF version of the FAO Geopolitical Ontology has been already registered in CKAN (http://ckan.net/package/fao-geopolitical-ontology) and it was requested to add it into the LOD Cloud.

==Example of use==
[[Image:Geopolitical Ontology in Country Profiles August 12 2009 v 1.png||thumb|200px|right|Figure 3. a website of introducing the geopolitical ontology in FAO Country Profiles.]]

The [[FAO Country Profiles]] is an information retrieval tool which groups the FAO's vast archive of information on its global activities in [[agriculture]] and [[rural development]] in one single area and catalogues it exclusively by country.

The [[FAO Country Profiles]] system provides access to country-based heterogeneous data sources.&lt;ref&gt;[http://www.fao.org/countryprofiles/resources.asp Inventory of data sources used in the FAO country profiles]&lt;/ref&gt; By using the  geopolitical ontology in the system, the following benefits are expected:&lt;ref&gt;[http://semanticweb.com/integrating-country-based-heterogeneous-data-at-the-united-nations-fao-s-geopolitical-ontology-and-services_b10681 Integrating country-based heterogeneous data at the United Nations: FAO's geopolitical ontology and services.]&lt;/ref&gt;

* Enhanced system functionality for content aggregation and synchronization from the multiple source repositories.
* Improved information access and browsing through comparison of data in neighbor countries and groups.

Figure 3 shows a page in the [[FAO Country Profiles]] where the geopolitical ontology is described.

==See also==
*[[Agricultural Information Management Standards]]
*[[AGROVOC]]
*[[Country code]]
*[[FAO Country Profiles]]
*[[Global Administrative Unit Layers (GAUL)|Global Administrative Unit Layers]] (GAUL)
*[[International Organization for Standardization]] (ISO)

==References==
{{reflist|2}}

==External links==
*[http://aims.fao.org/geopolitical.owl Geopolitical ontology in OWL format]
*[http://www.fao.org/countryprofiles/geoinfo/geopolitical/resource/ Geopolitical ontology in RDF format]
*[http://www.fao.org/countryprofiles/geoinfo.asp?lang=en Geopolitical information in the FAO Country Profiles]
*[http://www.slideshare.net/faoaims/faos-geopolitical-ontology-and-services FAO’s Geopolitical Ontology and Services] (Slides about FAO's geopolitical ontology)
*[http://www.fao.org/countryprofiles/default.asp?lang=en FAO Country Profiles]
*[http://www.fao.org/faoterm FAO Terminology] (FAOTERM)
*[http://faostat.fao.org FAOSTAT]
*[http://unstats.un.org/unsd/methods/m49/m49.htm UN Statistics Division - M49 codes]
*[http://www.iso.org/iso/english_country_names_and_code_elements ISO - Maintenance Agency for ISO 3166 country codes]

{{DEFAULTSORT:Geopolitical Ontology}}
[[Category:Ontology]]
[[Category:Ontology (information science)]]
[[Category:Knowledge representation]]
[[Category:Country codes]]</text>
      <sha1>kcob9xzfjyi0byprqme5i688lmi4fd8</sha1>
    </revision>
  </page>
  <page>
    <title>ISO 15926</title>
    <ns>0</ns>
    <id>4724116</id>
    <revision>
      <id>737691065</id>
      <parentid>720501244</parentid>
      <timestamp>2016-09-04T12:47:25Z</timestamp>
      <contributor>
        <ip>217.123.55.71</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="16610" xml:space="preserve">The '''ISO 15926''' is a standard for [[data integration]], sharing, exchange, and hand-over between [[computer system]]s.

The title, "''Industrial automation systems and integration&amp;mdash;Integration of life-cycle data for process plants including oil and gas production facilities''",  is regarded too narrow by the present ISO 15926 developers. Having developed a [[generic data model]] and Reference Data Library for process plants, it turned out that this subject is already so wide, that actually any state information may be modelled with it.

== History ==
In 1991 a European Union [[European Strategic Program on Research in Information Technology|ESPRIT]]-, named ProcessBase, started. The focus of this research project was to develop a [[data model]] for lifecycle information of a facility that would suit the requirements of the process industries. At the time that the project duration had elapsed, a consortium of companies involved in the process industries had been established: [[EPISTLE]] (European Process Industries STEP Technical Liaison Executive). Initially individual companies were members, but later this changed into a situation where three national consortia were the only members: PISTEP (UK), POSC/Caesar (Norway), and USPI-NL (Netherlands). (later PISTEP merged into POSC/Caesar, and USPI-NL was renamed to USPI).

EPISTLE took over the work of the ProcessBase project. Initially this work involved a standard called ISO 10303-221 (referred to as "[[ISO_10303|STEP]] AP221"). In that AP221 we saw, for the first time, an Annex M with a list of standard instances of the AP221 data model, including types of objects. These standard instances would be for reference and would act as a knowledge base with knowledge about the types of objects.
In the early nineties EPISTLE started an activity to extend Annex M to become a library of such object classes and their relationships: STEPlib. In the STEPlib activities a group of approx. 100 domain experts from all three member consortia, spread over the various expertises (e.g. Electrical, Piping, Rotating equipment, etc.), worked together to define the "core classes".

The development of STEPlib was extended with many additional classes and relationships between classes and published as [[Open Source]] data. Furthermore, the concepts and relation types from the AP221 and ISO 15926-2 data models were also added to the STEPlib dictionary. This resulted in the development of [[Gellish English]], whereas STEPlib became the [[Gellish English dictionary]]. Gellish English is a structured subset of natural English and is a [[modeling language]] suitable for [[knowledge modeling]], [[product modeling]] and [[data exchange]]. It differs from conventional modeling languages ([[meta language]]s) as used in information technology as it not only defines generic concepts, but also includes an English dictionary. The semantic expression capability of Gellish English was significantly increased by extending the number of relation types that can be used to express knowledge and information.

For modelling-technical reasons POSC/Caesar proposed another standard than [[ISO 10303]], called ISO 15926. EPISTLE (and ISO) supported that proposal, and continued the modelling work, thereby writing Part 2 of ISO 15926. This Part 2 has official ISO IS (International Standard) status since 2003.

POSC/Caesar started to put together their own RDL (Reference Data Library). They added many specialized classes, for example for [[American National Standards Institute|ANSI]] (American National Standards Institute) pipe and pipe fittings. Meanwhile STEPlib continued its existence, mainly driven by some members of USPI. Since it was clear that it was not in the interest of the industry to have two libraries for, in essence, the same set of classes, the Management Board of EPISTLE decided that the core classes of the two libraries shall be merged into Part 4 of ISO 15926. This merging process has been finished. Part 4 should act as reference data for part 2 of ISO 15926 as well as for ISO 10303-221 and replaced its Annex M. On June 5, 2007 ISO 15926-4 was signed off as a TS (Technical Specification).

In 1999 the work on an earlier version of Part 7 started. Initially this was based on [[XML Schema (W3C)|XML Schema]] (the only useful W3C Recommendation available then), but when [[Web Ontology Language|Web Ontology Language (OWL)]] became available it was clear that provided a far more suitable environment for Part 7. Part 7 passed the first ISO ballot by the end of 2005, and an implementation project started. A formal ballot for TS (Technical Specification) was planned for December 2007. However, it was decided then to split Part 7 into more than one part, because the scope was too wide.

== The standard ==
{{External links|date=May 2012}}
ISO 15926 has eleven parts (as of June 2009):

* Part 1 [http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=29556] - Introduction, information concerning engineering, construction and operation of production facilities is created, used and modified by many different organizations throughout a facility's lifetime. The purpose of ISO 15926 is to facilitate integration of data to support the lifecycle activities and processes of production facilities.
* Part 2 [http://www.stanford.edu/group/narratives/classes/08-09/CEE215/ReferenceLibrary/FIATECH%20ISO%2015926/ISO/pack/ISO%2015926%20part2%20pack/ECM4.5/lifecycle_integration_schema.html]- Data Model. a generic 4D model that can support all disciplines, supply chain company types and life cycle stages, regarding information about functional requirements, physical solutions, types of objects and individual objects as well as activities.
* Part 3 - Reference data for geometry and topology.
* Parts 4 [http://data.posccaesar.org/rdl/] - Reference Data, the terms used within facilities for the process industry.
* Part 7 [http://www.15926.org] - Integration of life-cycle data for process plants including oil and gas production facilities - Part 7: Implementation methods for the integration of distributed systems: Template methodology.
* Part 8 [http://www.15926.org] - Integration of life-cycle data for process plants including oil and gas production facilities - Part 8: Implementation methods for the integration of distributed systems: [[Web Ontology Language]] (OWL/RDF) implementation.
* Part 9 (in development)- Implementation standards, with the focus on Façades, standard web servers, web services, and security.
* Part 10 (in development)- Test Methods.
* Part 11 (in development)- Industrial Usage Guidelines.
* Part 12 (in development)- Life cycle integration ontology in [[Web Ontology Language]] (OWL2).
* Part 13 (in development)- Integrated lifecycle asset planning.

=== Description ===
The model and the library are suitable for representing lifecycle information about technical installations and their components.

They can also be used for defining the terms used in product catalogs in [[e-commerce]]. Another, more limited, use of the standard is as a reference classification for harmonization purposes between shared databases and product catalogues that are not based on ISO 15926.

The purpose of ISO 15926 is to provide a [[Lingua franca|Lingua Franca]] for computer systems, thereby integrating the information produced by them. Although set up for the process industries with large projects involving many parties, and involving plant operations and maintenance lasting decades, the technology can be used by anyone willing to set up a proper vocabulary of reference data in line with Part 4.

In Part 7 the concept of Templates is introduced. These are semantic constructs, using Part 2 entities, that represent a small piece of information. These constructs then are mapped to more efficient classes of n-ary relations that interlink the Nodes that are involved in the represented information.

In Part 8 the data model of Part 2 is mapped to OWL, and so are, in concept, the Reference Data of Part 4 and the templates of Part 7. For validation and reasoning purposes all are represented in First-Order Logic as well.

In Part 9 these Node and Template instances are stored in Façades. A Façade is an RDF [[quad store]], set up to a standard schema and an API. Any Façade only stores the data for which the Façade owner is responsible.

Each participating computer system maps its data from its internal format to such ISO-standard Node and Template instances. These are stored in a System Façade, each system its own Façade.

Data can be "handed over" from one Façade to another in cases where data custodianship is handed over (e.g. from a contractor to a plant owner, or from a manufacturer to the owners of the manufactured goods). Hand-over can be for a part of all data, whilst maintaining full referential integrity.

Façades can be set up for the consolidation of data by handing over data produced by various participating computer systems and stored in their System Façades. Examples are: a Façade for a project discipline, a project, a plant).

Documents are user-definable. They are defined in [[XML Schema]] and they are, in essence, only a structure containing cells that make reference to instances of Templates. This represents a view on all lifecycle data: since the data model is a 4D (space-time) model, it is possible to present the data that was valid at any given point in time, thus providing a true historical record. It is expected that this will be used for Knowledge Mining.

Data can be queried by means of [[SPARQL]]. In any implementation a restricted number of Façades can be involved, with different access rights. This is done by means of creating a CPF Server (= Confederation of Participating Façades). An [[Ontology (computer science)|Ontology]] Browser allows for access to one or more Façades in a given CPF, depending on the access rights.

== Projects and applications ==
{{External links|date=May 2012}}
There are a number of projects working on the extension of the ISO 15926 standard in different application areas.

=== Capital-intensive projects ===

Within the application of Capital Intensive projects, some cooperating implementation projects are running:

* The EDRC Project of [http://www.fiatech.org FIATECH] [http://www.fiatech.org/images/stories/projects/Project_Resumes/EDRC_Resume_v8_Sept_13_2013.pdf Capturing Equipment Data Requirements Using ISO 15926 and Assessing Conformance]. [http://techinvestlab.ru/EDRCDemo Example data and videos.]
* The ADI Project of [http://www.fiatech.org FIATECH], to build the tools (which will then be made available in the public domain)
** The tools and deliverables can be seen on the ISO 15926 knowledge base: [http://15926.org]
* The IDS Project of [http://www.posccaesar.org POSC Caesar Association], to define product models required for data sheets
* A joint ADI-IDS project is the [[ISO 15926 WIP]]
* The DEXPI project: The objective of DEXPI is to develop and promote a general standard for the process industry covering all phases of the lifecycle of a (petro-)chemical plant, ranging from specification of functional requirements to assets in operation. See more at [http://www.dexpi.org dexpi.org]

=== Upstream Oil and Gas industry ===

The [[Norwegian Oil Industry Association]] (OLF) has decided to use ISO 15926 (also known as the [[Oil and Gas Ontology]]) as the instrument for integrating data across disciplines and business domains for the [[Upstream (oil industry)|Upstream Oil and Gas industry]]. It is seen as one of the enablers of what has been called the next (or second) generation of [[Integrated operations]], where a better integration across companies is the goal.&lt;ref&gt;{{cite web |url=http://www.olf.no/getfile.php/zKonvertert/www.olf.no/Rapporter/Dokumenter/070919%20IO%20and%20Ontology%20-%20Brosjyre.pdf |title=Integrated Operations and the Oil and Gas Ontology |author=The Norwegian Oil Industry Association (OLF) |accessdate=2009-05-06 }}&lt;/ref&gt;

The following projects are currently running (May 2009):

* The [[Integrated Operations in the High North|Integrated Operations in the High North (IOHN)]] project is working on extending ISO 15926 to handle real-time data transmission and (pre-)processing to enable the next generation of [[Integrated operations|Integrated Operations]].
* The [http://trac.posccaesar.org/wiki/EW Environment Web] project to include environmental reporting terms and definitions  as used in [http://www.epim.no EPIM]'s [http://www.epim.no/default.asp?id=945 EnvironmentWeb] in ISO 15926.

Finalised projects include:

* The [http://trac.posccaesar.org/wiki/IIP Integrated Information Platform (IIP)] project working on establishing a real-time information pipeline based on open standards. It worked among others on:
** [http://www.posccaesar.org/wiki/NcsDdr Daily Drilling Report (DDR)] to including all terms and definitions in ISO 15926. This standard became mandatory on February 1, 2008&lt;ref&gt;{{cite web |url=http://www.npd.no/English/Produkter+og+tjenester/Skjemaer/CDRS_reporting_oct_2007.htm |title=Drilling Reporting to the authorities |author=Norwegian Petroleum Directorate |accessdate=2009-05-05 }}&lt;/ref&gt; for reporting on the [[Norwegian Continental Shelf]] by the [[Norwegian Petroleum Directorate|Norwegian Petroleum Directorate (NPD)]] and [http://www.ptil.no/main-page/category9.html Safety Authority Norway (PSA)]. NPD says that the quality of the reports has improved considerably since.
** [http://www.posccaesar.org/wiki/NcsDpr Daily Production Report (DPR)] to including all terms and definitions in ISO 15926. This standard was tested successfully on the [[Valhall oil field|Valhall]] ([[BP]]-operated) and [http://www.statoilhydro.com/en/ouroperations/explorationprod/ncs/aasgard/pages/default.aspx Åsgard] ([[StatoilHydro]]-operated) fields offshore Norway. The terminology and XML schemata developed have also been included in [http://www.Energistics.org Energistics]’ [[PRODML]] standard.

== Some technical background ==
One of the main requirements was (and still is) that the scope of the data model covers the entire lifecycle of a facility (e.g. oil refinery) and its components (e.g. pipes, pumps and their parts, etc.). Since such a facility over such a long time entails many different types of activities on a myriad of different objects it became clear that a generic and data-driven data model would be required.

A simple example will illustrate this. There are thousands of different types of physical objects in a facility (pumps, compressors, pipes, instruments, fluids, etc). Each of these has many properties. If all combinations would be modelled in a "hard-coded" fashion, the number of combinations would be staggering, and unmanageable.

The solution is a "template" that represents the semantics of: "This object has a property of  X yyyy" (where yyyy is the unit of measure). Any instance of that template refers to the applicable reference data:
* physical object (e.g. my Induction Motor)
* indirect property type (e.g. the class "cold locked rotor time")
* base property type (here: time)
* scale (here: seconds)

Without being able to make reference to those classes, via the Internet, it will be impossible to express this information.

==References==
{{Reflist}}

== External links ==
* [http://15926.org 15926.org]: A forum for ISO 15926 discussions and team collaboration.
* [http://iringug.org/wiki/index.php?title=Main_Page iringug.org]: -An online community of users, companies, and organizations that have common interest in solutions that implement ISO 15926 reference data and protocols.
* [http://iringtoday.com iringtoday.com]: - An online ISO 15926 thought leadership community geared toward engineering management.
* [http://techinvestlab.ru/ISO15926en .15926 Editor] Open source software to view, edit and verify ISO 15926 data.
* [http://wings.buffalo.edu/philosophy/ontology/bfo/west.pdf Against Idiosyncrasy in Ontology Development]: A critical study of ISO 15926 and of the claims made on its behalf.
* [http://www.matthew-west.org.uk/publications/ResponseToBarrySmithCommentsOnISO15926.pdf A Response to "Against Idiosyncrasy in Ontology Development"]: A rebuttal of "Against Idiosyncracy in Ontology Development".

{{ISO standards}}

{{DEFAULTSORT:Iso 15926}}
[[Category:ISO standards|#15926]]
[[Category:Semantic Web]]
[[Category:Knowledge engineering]]
[[Category:Technical communication]]
[[Category:Information science]]
[[Category:Ontology (information science)]]
[[Category:Knowledge representation]]</text>
      <sha1>1opxwaxt2dsp0qlv5rgual17i8e0aeb</sha1>
    </revision>
  </page>
  <page>
    <title>NeOn Toolkit</title>
    <ns>0</ns>
    <id>30724333</id>
    <revision>
      <id>639347773</id>
      <parentid>509051156</parentid>
      <timestamp>2014-12-23T15:33:38Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor />
      <comment>Robot - Moving category Free software programmed in Java to [[:Category:Free software programmed in Java (programming language)]] per [[WP:CFD|CFD]] at [[Wikipedia:Categories for discussion/Log/2014 December 11]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="1450" xml:space="preserve">'''The NeOn Toolkit''' is an open source, multi-platform [[ontology editor]], which supports the development of ontologies in [[Web Ontology Language|OWL]]/[[Resource Description Framework|RDF]]. The editor is based on the [[Eclipse (software)|Eclipse platform]] and provides a set of plug-ins (currently 20 plug-ins are available for the latest version, v2.4) covering a number of ontology engineering activities, including Annotation and Documentation, Modularization and Customization, Reuse, Ontology Evolution, translation&lt;ref name="Espinoza2008"&gt;M. Espinoza, A. Gomez-Perez, and E. Mena. [http://sid.cps.unizar.es/PUBLICATIONS/POSTSCRIPTS/eswc08-localization.pdf Enriching an ontology with multilingual information]. In Proc. of 5th European Semantic Web Conference (ESWC'08), Tenerife, (Spain), June 2008.&lt;/ref&gt; and others. 

The NeOn Toolkit has been developed in the course of the EU-funded NeOn project and is currently maintained and distributed by the NeOn Technologies Foundation.

==References==
&lt;references/&gt;

== External links ==
* [http://www.neon-foundation.org/ NeOn Technologies Foundation]
* [http://neon-toolkit.org/ NeOn Toolkit Website]
* [http://www.neon-project.org/ NeOn Project Website]

[[Category:Knowledge representation]]
[[Category:Free integrated development environments]]
[[Category:Ontology (information science)]]
[[Category:Ontology editors]]
[[Category:Free software programmed in Java (programming language)]]</text>
      <sha1>pnjx3ufvvdpigwnje4f27vmffi7pg20</sha1>
    </revision>
  </page>
  <page>
    <title>Concept map</title>
    <ns>0</ns>
    <id>698226</id>
    <revision>
      <id>757802557</id>
      <parentid>755974258</parentid>
      <timestamp>2017-01-01T21:04:44Z</timestamp>
      <contributor>
        <username>SporkBot</username>
        <id>12406635</id>
      </contributor>
      <minor />
      <comment>Replace template per [[Wikipedia:Templates for discussion/Log/2016 July 2|TFD outcome]]; no change in content</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="11682" xml:space="preserve">{{for|concept maps in [[generic programming]]|Concept (generic programming)}}
[[File:Electricity Concept Map.gif|thumb|An Electricity Concept Map, an example of a concept map]]
{{InfoMaps}}
A '''concept map''' or '''conceptual diagram''' is a [[diagram]] that depicts suggested relationships between [[concept]]s.&lt;ref&gt;Peter J. Hager,Nancy C. Corbin. ''Designing &amp; Delivering: Scientific, Technical, and Managerial Presentations,'' 1997, . 163.&lt;/ref&gt; It is a graphical tool that [[instructional designer]]s, [[engineer]]s, [[Technical communication|technical writers]], and others use to organize and structure [[knowledge]].

A concept map typically represents ideas and information as boxes or circles, which it connects with labeled arrows in a downward-branching hierarchical structure. The relationship between concepts can be articulated in linking phrases such as ''causes'', ''requires'', or ''contributes to''.&lt;ref name=theory&gt;[[Joseph D. Novak]] &amp; Alberto J. Cañas (2006). [http://cmap.ihmc.us/Publications/ResearchPapers/TheoryCmaps/TheoryUnderlyingConceptMaps.htm "The Theory Underlying Concept Maps and How To Construct and Use Them"], [[Institute for Human and Machine Cognition]]. Accessed 24 Nov 2008.&lt;/ref&gt;

The technique for [[Visualization (graphic)|visualizing]] these relationships among different concepts is called ''concept mapping''. Concept maps define the [[Ontology (information science)|ontology]] of computer systems, for example with the [[object-role modeling]] or  [[Unified Modeling Language]] formalism.

== Overview ==
A concept map is a way of representing relationships between [[idea]]s, [[image]]s, or [[word]]s in the same way that a [[sentence diagram]] represents the grammar of a sentence, a road map represents the locations of highways and towns, and a [[circuit diagram]] represents the workings of an electrical appliance. In a concept map, each word or phrase connects to another, and links back to the original idea, word, or phrase. Concept maps are a way to develop logical thinking and study skills by revealing connections and helping students see how individual ideas form a larger whole. An example of the use of concept maps is provided in the context of learning about types of fuel.{{clarify|why is this noteworthy?|date=November 2016}}&lt;ref name="CMP"&gt;[http://www.energyeducation.tx.gov/pdf/223_inv.pdf CONCEPT MAPPING FUELS]. Accessed 24 Nov 2008.&lt;/ref&gt;

Concept maps were developed{{whom|date=November 2016}} to enhance meaningful learning in the sciences{{fact|date=November 2016}}. A well-made concept map grows within a ''context frame'' defined by an explicit "focus question", while a [[mind map]] often has only branches radiating out from a central picture. Some research evidence suggests that the brain stores knowledge as productions (situation-response conditionals) that act on [[declarative memory]] content, which is also referred to as chunks or propositions.&lt;ref&gt;Anderson, J. R., &amp; Lebiere, C. (1998). The atomic components of thought. Mahwah, NJ: Erlbaum.&lt;/ref&gt;&lt;ref&gt;Anderson, J. R., Byrne, M. D., Douglass, S., Lebiere, C., &amp; Qin, Y. (2004). An Integrated Theory of the Mind. Psychological Review, 111(4), 1036&amp;ndash;1050.&lt;/ref&gt; Because concept maps are constructed to reflect organization of the declarative memory system, they facilitate sense-making and meaningful learning on the part of individuals who make concept maps and those who use them.

== Differences from other visualizations ==

'''[[Topic map]]s:'''  Concept maps are rather similar to topic maps in that both allow to concepts or topics via [[graph (data structure)|graphs]]. Among the various schema and techniques for visualizing ideas, processes, and organizations, concept mapping, as developed by [[Joseph D. Novak|Joseph Novak]] is unique in its philosophical basis, which "makes concepts, and propositions composed of concepts, the central elements in the structure of knowledge and construction of meaning."&lt;ref&gt;Novak, J.D. &amp; Gowin, D.B. (1996). Learning How To Learn, Cambridge University Press: New York, p. 7.&lt;/ref&gt;

'''[[mind mapping|Mind maps]]:'''  Both concept maps and topic maps can be contrasted with [[mind mapping]], which is often restricted to radial hierarchies and [[tree structure]]s. Another contrast between concept mapping and mind mapping is the speed and spontaneity when a mind map is created. A mind map reflects what you think about a single topic, which can focus group brainstorming. A concept map can be a map, a system view, of a real (abstract) system or set of concepts. Concept maps are more free form, as multiple hubs and clusters can be created, unlike mind maps, which fix on a single two centered approach.

== History ==
The technique of concept mapping was developed by [[Joseph D. Novak]] and his research team at [[Cornell University]] in the 1970s as a means of representing the emerging science knowledge of students.&lt;ref&gt;{{cite web|url=http://www.ihmc.us/users/user.php?UserID=jnovak|title=Joseph D. Novak|publisher=Institute for Human and Machine Cognition (IHMC)|accessdate=2008-04-06}}&lt;/ref&gt; It has subsequently been used as a tool to increase meaningful learning in the sciences and other subjects as well as to represent the expert knowledge of individuals and teams in education, government and business. Concept maps have their origin  in the learning movement called [[constructivism (learning theory)|constructivism]]. In particular, constructivists hold that learners actively construct knowledge.

Novak's work is based on the cognitive theories of [[David Ausubel]], who stressed the importance of prior knowledge in being able to learn (or ''assimilate'') new concepts: "The most important single factor influencing learning is what the learner already knows. Ascertain this and teach accordingly."&lt;ref&gt;Ausubel, D. (1968) Educational Psychology: A Cognitive View. Holt, Rinehart &amp; Winston, New York.&lt;/ref&gt; Novak taught students as young as six years old to make concept maps to represent their response to focus questions such as "What is water?" "What causes the seasons?" In his book ''Learning How to Learn'', Novak states that a "meaningful learning involves the assimilation of new concepts and propositions into existing cognitive structures."

Various attempts have been made to conceptualize the process of creating concept maps. Ray McAleese, in a series of articles, has suggested that mapping is a process of ''off-loading''. In this 1998 paper, McAleese draws on the work of Sowa&lt;ref&gt;Sowa, J.F., 1983. ''Conceptual structures: information processing in mind and machine'', Addison-Wesley.&lt;/ref&gt; and a paper by Sweller &amp; Chandler.&lt;ref&gt;Sweller, J. &amp; Chandler, P., 1991. Evidence for Cognitive Load Theory. ''Cognition and Instruction'', 8(4), p.351-362.&lt;/ref&gt; In essence, McAleese suggests that the process of making knowledge explicit, using ''nodes'' and ''relationships'', allows the individual to become aware of what they know and as a result to be able to modify what they know.&lt;ref&gt;McAleese,R (1998) '''The Knowledge Arena''' as an Extension to the Concept Map: Reflection in Action, ''Interactive Learning Environments'', '''6,3,p.251-272'''.&lt;/ref&gt; Maria Birbili applies that same idea to helping young children learn to think about what they know.&lt;ref&gt;Birbili, M. (2006) [http://ecrp.uiuc.edu/v8n2/birbili.html "Mapping Knowledge: Concept Maps in Early Childhood Education"], ''Early Childhood Research &amp; Practice'', ''8(2)'', Fall 2006&lt;/ref&gt; The concept of the ''knowledge arena'' is suggestive of a virtual space where learners may explore what they know and what they do not know.

==Use==
[[Image:Conceptmap.png|thumb|450px|Example concept map created using the IHMC CmapTools computer program.]]
Concept maps are used to stimulate the generation of ideas, and are believed to aid [[creativity]].&lt;ref name="theory"/&gt; Concept mapping is also sometimes used for [[brain-storming]]. Although they are often personalized and idiosyncratic, concept maps can be used to communicate complex ideas.

Formalized concept maps are used in [[software design]], where a common usage is [[Unified Modeling Language]] diagramming amongst similar conventions and development methodologies.

Concept mapping can also be seen as a first step in [[ontology (computer science)|ontology]]-building, and can also be used flexibly to represent formal argument.

Concept maps are widely used in education and business. Uses include:
*[[Note taking]] and summarizing gleaning key concepts, their relationships and hierarchy from documents and source materials
*New knowledge creation: e.g., transforming [[tacit knowledge]] into an organizational resource, mapping team knowledge
*Institutional knowledge preservation (retention), e.g., eliciting and mapping expert knowledge of employees prior to retirement
*Collaborative knowledge modeling and the transfer of expert knowledge
*Facilitating the creation of shared vision and shared understanding within a team or organization
*Instructional design: concept maps used as [[David Ausubel|Ausubelian]] "advance organizers" that provide an initial conceptual frame for subsequent information and learning.
*Training: concept maps used as [[David Ausubel|Ausubelian]] "advanced organizers" to represent the training context and its relationship to their jobs, to the organization's strategic objectives, to training goals.
*Communicating complex ideas and arguments
*Examining the symmetry of complex ideas and arguments and associated terminology
*Detailing the entire structure of an idea, [[train of thought]], or line of argument (with the specific goal of exposing faults, errors, or gaps in one's own reasoning) for the scrutiny of others.
*Enhancing [[metacognition]] (learning to learn, and thinking about knowledge)
*Improving language ability
*Assessing learner understanding of learning objectives, concepts, and the relationship among those concepts
*Lexicon development

==See also==
{{list|date=November 2016}}
{{colbegin}}
* [[Argument map]]
* [[Cognitive map]]
* [[Conceptual graphs]]
* [[Conceptual framework]]
* [[Idea networking]]
* [[Knowledge visualization]]
* [[List of concept- and mind-mapping software]]
* [[Mental model]]
* [[Mind map]]
* [[Radial tree]]
* [[Entity-relationship model]]
* [[Nomological network]]
* [[Semantic web]]
* [[Topic Maps]]
* [[Educational psychology]]
* [[Educational technology]]
* [[Morphological analysis (problem-solving)|Morphological analysis]]
* [[Wicked problem]]
* [[Object role modeling]]
* [[Personal knowledge base]]
* [[Semantic network]]
* [[Olog]]
* [[Pathfinder network]]
* [[Sensemaking]]
{{colend}}

==References==
{{reflist|2}}

== Further reading ==
* {{cite book |last= Novak |first= J.D. |title= Learning, Creating, and Using Knowledge: Concept Maps as Facilitative Tools in Schools and Corporations |publisher= Routledge |edition= 2nd |date= 2009 |isbn= 9780415991858 }}
&lt;!-- |publisher= Lawrence Erlbaum Associates |location= Mahwah |date= 1998 |edition= 1st --&gt;
* {{cite book |last1= Novak |first1= J.D. |last2= Gowin |first2= D.B. |title= Learning How to Learn |publisher= Cambridge University Press |location= Cambridge |date= 1984 |isbn= 9780521319263 }}

== External links ==
{{commons|Concept map}}
{{wikiversity|Concept mapping}}
* [http://www.mind-mapping.org/images/walt-disney-business-map.png Example of a concept map from 1957] by Walt Disney.

{{Mindmaps}}

{{DEFAULTSORT:Concept Map}}
[[Category:Concepts]]
[[Category:Constructivism (psychological school)]]
[[Category:Diagrams]]
[[Category:Educational technology]]
[[Category:Graph drawing]]
[[Category:Knowledge representation]]
[[Category:Note-taking]]
[[Category:Visual thinking]]</text>
      <sha1>ec7zlqfjgs2bnpx0wgct2nqhg1nu5ub</sha1>
    </revision>
  </page>
  <page>
    <title>OntoWiki</title>
    <ns>0</ns>
    <id>12105194</id>
    <revision>
      <id>699952064</id>
      <parentid>626614461</parentid>
      <timestamp>2016-01-15T12:54:50Z</timestamp>
      <contributor>
        <username>White gecko</username>
        <id>7656203</id>
      </contributor>
      <comment>Add current version</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2548" xml:space="preserve">{{Infobox Software | name = OntoWiki
| logo = 
| screenshot =&lt;!-- Deleted image removed:  [[Image:OntoWiki-screenshot.jpg|180px]] --&gt;
| caption = 
| founder = Sören Auer
| current maintainer = Sebastian Tramp
| latest_release_version = 0.9.11
| latest_release_date = January 31, 2014
| operating_system = [[Cross-platform]]
| programming language=[[PHP]]
| database=[[MySQL]]
| genre = [[knowledge management system]]
| license = [[GNU General Public License|GPL]]
| website = http://ontowiki.net
}}

'''OntoWiki''' is a free, [[open source software|open-source]] [[semantic wiki]] application, meant to serve as an [[ontology (computer science)|ontology]] editor and a [[knowledge management|knowledge acquisition]] system. It is a web-based application written in [[PHP]] and using either a [[MySQL]] database or a [[Virtuoso Universal Server|Virtuoso triple store]]. OntoWiki is form-based rather than syntax-based, and thus tries to hide as much of the complexity of knowledge representation formalisms from users as possible. OntoWiki is mainly being developed by the [http://aksw.org Agile Knowledge Engineering and Semantic Web (AKSW) research group] at the [[University of Leipzig]], a group also known for the [[DBpedia]] project among others, in collaboration with volunteers around the world.

In 2009 the AKSW research group got a budget of €425,000 from the [[Federal Ministry of Education and Research (Germany)|Federal Ministry of Education and Research of Germany]] for the development of the OntoWiki.&lt;ref&gt;[http://idw-online.de/pages/de/news300375 "OntoWiki" hilft Daten im Web zu verknüpfen] (German)&lt;/ref&gt;

In 2010 OntoWiki became part of the technology stack supporting the [[Framework Programmes for Research and Technological Development#LOD2|LOD2]] (Linked Open Data) project. Leipzig University is one of the consortium members of the project, which is funded by a €6.5m EU grant.&lt;ref&gt;{{cite web |url=http://cordis.europa.eu/fetch?CALLER=PROJ_ICT&amp;ACTION=D&amp;CAT=PROJ&amp;RCN=95562 |publisher=European Commission |title=CORDIS FP7 ICT Projects - LOD2 |date=2010-04-20}}&lt;/ref&gt;

==See also==
* [[Semantic MediaWiki]]
* [[DBpedia]]

== External links ==
* {{official website|http://OntoWiki.net}}
* [https://github.com/AKSW/OntoWiki#ontowiki About page on GitHub]
* [http://blog.aksw.org AKSW blog]

== References ==
&lt;references/&gt;

{{DEFAULTSORT:Ontowiki}}
[[Category:Semantic wiki software]]
[[Category:Free integrated development environments]]
[[Category:Knowledge representation]]
[[Category:Ontology (information science)]]</text>
      <sha1>5ce0sosxwickptm2gu8vxph3jqgeept</sha1>
    </revision>
  </page>
  <page>
    <title>IMARK</title>
    <ns>0</ns>
    <id>31933815</id>
    <revision>
      <id>724770012</id>
      <parentid>712875797</parentid>
      <timestamp>2016-06-11T10:49:23Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <comment>removed [[Category:Development]]; added [[Category:Rural development]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="6144" xml:space="preserve">{{Infobox website
|name=Information Management Resource Kit (IMARK)
|logo = 
|url ={{URL|http://www.imarkgroup.org}}
|type = [[Capacity building]]
|commercial      = No
|registration    = Optional
|language        = English, French, Spanish, Arabic, Chinese
|launch date = 2001
|current status = Online
|screenshot      = }}

The Information Management Resource Kit ('''IMARK''') is a partnership-based [[e-learning]] initiative developed by the [[Food and Agriculture Organization|Food and Agriculture Organization (FAO)]] of the [[United Nations]] and partner organizations to support individuals, institutions and networks world-wide in the effective management of information and agricultural development. IMARK consists of a suite of [[distance learning]] resources and tools on [[information management]].&lt;ref&gt;{{cite web|url=http://www.imarkgroup.org/index_en.asp?m=0 |title=IMARK - Information Management Resource Kit |publisher=Imarkgroup.org |date= |accessdate=2011-06-07}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://usain.org/links2.html |title=Links for Agricultural Librarians |publisher=USAIN |date=2010-05-19 |accessdate=2011-06-07}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://knowledge.cta.int/index.php/en/Dossiers/S-T-Issues-in-Perspective/ICT-for-transforming-research-for-agricultural-and-rural-development/Links/IMARK-FAO |title=IMARK (FAO) / Links / ICT for transforming research for agricultural and rural development / S&amp;T Issues in Perspective / Dossiers / Home - Knowledge for Development |publisher=Knowledge.cta.int |date= |accessdate=2011-06-07}}&lt;/ref&gt;

== About IMARK ==
The [[Food and Agriculture Organization|Food and Agriculture Organization (FAO)]] of the [[United Nations]] initiated a partnership-based [[e-learning]] programme in 2001 to support [[information management]].&lt;ref&gt;http://www.fao.org/rdd/doc/IMARK%20General%20Sheet%20EN%2011-05.pdf&lt;/ref&gt; IMARK is targeted at information professionals in developing countries. Each IMARK curriculum is designed through a consultative process with [[Subject-matter expert|subject matter experts]], field practitioners and representatives from the target audience from around the world. The IMARK initiative is a response to demand for enhanced information and [[knowledge management]] in the effort to achieve the [[Millennium Development Goals|Millennium Development Goals (MDGs)]], especially those related to [[hunger]] and the information society, in the context of bridging the digital divide.&lt;ref&gt;{{cite web|url=http://www.fao.org/rdd/doc/FAO%20Approach%20to%20WSIS2005.pdf |title=FAO's strategies towards the WSIS 2005 |format=PDF |date= |accessdate=2011-06-07}}&lt;/ref&gt;  The development goal of IMARK is to improve the capabilities of people concerned with [[information management]] and [[knowledge sharing]].&lt;ref&gt;{{cite web|url=http://editlib.org/noaccesspresent/26495 |title=Ed/ITLib Digital Library → No Access |publisher=Editlib.org |date=2007-10-15 |accessdate=2011-06-07}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.infodev.org/en/Publication.183.html |title=Quick Guide: ICT and Rural Livelihood Resources at FAO |publisher=infoDev.org |date=2006-09-28 |accessdate=2011-06-07}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.icimod.org/index.php?page=23 |title=Knowledge Management |publisher=Icimod.org |date=2010-12-16 |accessdate=2011-06-07}}&lt;/ref&gt;

=== Objectives and Scope ===
The development goal of IMARK is to improve the overall effectiveness of programmes in agricultural development and [[food security]] by enhancing access to information by key stakeholders.&lt;ref&gt;http://eprints.rclis.org/bitstream/10760/15682/1/FAO%E2%80%99s%20Capacity-Building%20Initiatives%20in%20Accessing,%20Documenting,%20Communicating%20and%20Managing%20Agricultural%20Information.pdf&lt;/ref&gt;

=== Steering Committee ===
IMARK has over 30 partners and collaborating institutions since its inception in 2001, and its activities are coordinated through a Steering Committee whose members include [[Association for Progressive Communications|The Association for Progressive Communications (APC)]], [[Food and Agriculture Organization|Food and Agriculture Organization (FAO)]] of the [[United Nations]], the [[Agence universitaire de la Francophonie|Agence Universitaire de la Francophonie (AUF)]], [[Commonwealth of Learning|Commonwealth of Learning (COL)]], [[Groupe de Recherches et d'Echanges Technologiques|Groupe de Recherches et d'Echanges Technologiques (GRET)]], [[Bibliotheca Alexandrina]] and [[UNESCO]].&lt;ref&gt;{{cite web|url=http://portal.unesco.org/ci/en/ev.php-URL_ID=21458&amp;URL_DO=DO_TOPIC&amp;URL_SECTION=201.html |title=IMARK launches new e-learning module |publisher=Portal.unesco.org |date=2008-01-24 |accessdate=2011-06-07}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.apc.org/en/news/development/world/e-learning-module-developing-electronic-communitie |title=e-Learning module developing electronic communities &amp;#124; Association for Progressive Communications |publisher=Apc.org |date=2006-05-08 |accessdate=2011-06-07}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://webapp.ciat.cgiar.org/ccc/imark.htm |title=CCC: E-Learning Initiatives - IMARK-FAO |publisher=Webapp.ciat.cgiar.org |date= |accessdate=2011-06-07}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.iaald.org/docs/iisast2_report.pdf |title=2nd Expert Consultation IISAST |format=PDF |date= |accessdate=2011-06-07}}&lt;/ref&gt;

== See also ==
* [[E-Learning|E-learning]]
* [[FAO|Food and Agriculture Organization of the United Nations (FAO)]]
* [[Agricultural Information Management Standards|Agricultural Information Management Standards (AIMS)]]

== References ==
{{Reflist|2}}

== External links ==
* [http://www.imarkgroup.org/ Official IMARK Website ]
* [http://www.fao.org Food and Agriculture Organization of the United Nations]

{{DEFAULTSORT:Imark}}
[[Category:Information technology]]
[[Category:Distance education]]
[[Category:Education]]
[[Category:Virtual learning environments]]
[[Category:Learning methods]]
[[Category:Educational technology projects]]
[[Category:Rural development]]
[[Category:Non-profit technology]]
[[Category:Food and Agriculture Organization]]
[[Category:Information technology management]]
[[Category:Knowledge representation]]</text>
      <sha1>90o1pkoer4nyjidt2po9s6f0h4l8oav</sha1>
    </revision>
  </page>
  <page>
    <title>Brand page</title>
    <ns>0</ns>
    <id>34215536</id>
    <revision>
      <id>741997145</id>
      <parentid>685866571</parentid>
      <timestamp>2016-10-01T01:03:30Z</timestamp>
      <contributor>
        <username>Trivialist</username>
        <id>5360838</id>
      </contributor>
      <comment>Copyedit (minor)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7678" xml:space="preserve">A '''brand page''' (also known as a '''page''' or '''fan page'''), in online social networking parlance, is a profile on a social networking website which is considered distinct from an actual [[user profile]] in that it is created and managed by at least one other registered user as a representation of a non-personal [[online identity]]. This feature is most used to represent the brands of organizations associated with, properties owned by, or general interests favored by a user of the hosting network.

While also being potentially manageable by more than one registered user, pages are distinguished from [[Group (online social networking)|groups]] in that pages are usually designed for the managers to direct messages and posts to subscribing users (akin to a [[newsletter]] or [[blog]]) and promote a brand, while groups are usually and historically formed for discussion purposes.

==History==
Prior to 2007, only a few websites made use of non-personal profile pages. [[Last.fm]], established in 2002, used its music recommendation service to automatically generate "artist pages" which serve as portals for biographies, events and artist-related playlists. This approach, however, is not explicitly controlled by artists or music groups because of the automatic nature of artist pages; pages, for example, could be created from erroneous misspellings and miscredits of works which are accepted as-is by the Audioscrobbler recommendation service used by Last.fm. Furthermore, Last.fm has never advertised itself as a social networking service, despite accruing myriad social features since 2002.

The most high-profile usage of this model is [[Facebook]]'s Pages (formerly known as "Fan Page" until 2010) feature, launched in 2007; one could "be a fan of" a page until April 2010, when the parlance was replaced with "Like".&lt;ref&gt;{{cite web|url=http://www.allfacebook.com/2010/04/facebooks-become-a-fan-officially-switches-to-like/ | title=Facebook’s "Become A Fan" Officially Switches To "Like" | author=[[Nick O'Neill]]|publisher = AllFacebook.com|date = April 19, 2010 &lt;!-- 4:37 PM --&gt; }}&lt;/ref&gt; [[Foursquare]], a location-oriented social networking site, launched its "Brands" feature allowing for the creation of specialized brand pages in January 2010 (with [[Intel]] being the first user), but they did not become "self-serve" (controllable by individuals employed by page brand owners) until August 2011.&lt;ref&gt;{{cite web|url = http://blog.foursquare.com/2011/08/02/pages-are-now-self-serve-a-new-home-for-brands-and-organizations-on-foursquare/|title = Pages are now self-serve! A new home for brands and organizations on foursquare.|date = Aug 2, 2011|publisher = Foursquare}}&lt;/ref&gt; [[LinkedIn]], an enterprise-oriented social networking service, launched "Company Pages" in November 2010.&lt;ref&gt;{{cite web|url = http://blog.linkedin.com/2010/11/01/linkedin-company-pages/|title = Recommend your favorite products and services on LinkedIn Company Pages|author = Ryan Roslansky|publisher = LinkedIn|date = November 1, 2010}}&lt;/ref&gt; [[Google+]], the current social networking service operated by [[Google]], launched its own "Pages" feature in October 2011.&lt;ref&gt;{{cite web|url = http://googleblog.blogspot.com/2011/11/google-pages-connect-with-all-things.html|title = Google+ Pages: connect with all the things you care about|publisher = Google|date = 11-07-2011 &lt;!-- 10:01:00 AM --&gt; }}&lt;/ref&gt; On November 19th, 2012, [[Amazon.com|Amazon]] announced Amazon Pages giving brands self-service control over their presence on the site.&lt;ref&gt;{{cite web|url = http://techcrunch.com/2012/11/20/amazon-offers-amazon-pages-for-brands-to-customize-with-their-own-urls-and-amazon-posts-for-social-media-marketing/|title = Amazon Offers ‘Amazon Pages’ For Brands To Customize With Their Own URLs, And ‘Amazon Posts’ For Social Media Marketing|author = TechCrunch|date = November 20, 2012}}&lt;/ref&gt; On 8 December, [[Twitter]] announced that it would roll out "brand pages" as part of a major user interface redesign in 2012.&lt;ref&gt;{{cite web|url = http://advertising.twitter.com/2011/12/let-your-brand-take-flight-on-twitter.html|title = Let your brand take flight on Twitter with enhanced profile pages|publisher = Twitter Advertising Blog|author = TwitterAds|date = December 8, 2011}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url = http://adage.com/article/digital/twitter-joins-facebook-google-launches-brand-pages-marketers/231448/|title = Twitter Joins Facebook, Google, Launches 'Brand Pages' for Marketers|author = Cotton Delo|date = December 8, 2011}}&lt;/ref&gt;

==Features==
Increasingly, brand pages make use of the following features: 
* Header banners
* The ability to post blogs or replies on the brand page in the name of the brand page
* The ability to administer multiple pages
* Photos
* Video
* Maps (including physical location of the page)
* Subscribers
* Other apps

Twitter made use of header banners in their launch of brand pages, and Facebook made use of "cover photos" in their re-design of brand pages in March 2011.

==Uses==
Organizations and brands regularly make use of pages in order to syndicate news and upcoming events, especially off-site blog posts, to subscribing users. Page subscription numbers can also be used as a metric of trust or interest in the associated brand.

Interests can also be indexed as pages, and are often the basis for the formation of mass social movements (i.e., the [[Arab Spring]], [[Occupy Wall Street]]).

===Newsroom accounts===
Pages are also used as newsroom accounts.

A '''[[newsroom]] account''' refers to any microblogging or social networking account branded by or owned by a publishing or broadcasting organization which is dedicated solely to syndicating content from a particular category of content as published on the original website of the organization. Such accounts have come into increased usage by news organizations as means by which:
# A news organization's presence on a social networking or microblogging website is increased
# A news organization can specialize content syndication to selective users who wish to subscribe

News organizations who make use of multiple newsroom accounts typically allow for either online editors or multiple employed authors to edit and update the syndications of newsroom content. Such accounts are typically marked by graphic icons which make use of the brand symbol combined with distinct colors assigned to each account.

Examples of newsroom accounts and pages include the Facebook pages for both ''[[The Guardian]]'' and the newspaper's Technology newsroom.

==Impact==

===Pseudonyms===
The usage of pseudonyms on social networking services, long considered a preserve of user privacy, has been partially affected by the promotion of pseudonyms, as social networking services have encouraged users to create pages for pseudonyms and implemented legal name requirements for user profile registration (i.e., New York resident Stefani Germanotta keeping a separate personal user profile under her legal name while maintaining a fan page under her stage name and pseudonym [[Lady Gaga]]).

===Interest-based connections===
As pages can be created to represent interests, the number of attempts to create vertical social networking services (i.e., [[Ning (website)|Ning]]) has leveled off in the 2010s. [[Social network advertising]] can also be targeted to users based upon their page subscriptions.

==See also==
* [[Fansite]]
* [[Landing page]]

==References==
{{reflist}}

{{Online social networking}}
{{Microblogging}}

{{DEFAULTSORT:Page (online social networking)}}
[[Category:Software features]]
[[Category:Knowledge representation]]
[[Category:Identity management]]</text>
      <sha1>1oqe7q9qll5in86qhrv4iozzhi7kt7w</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Belief revision</title>
    <ns>14</ns>
    <id>36312376</id>
    <revision>
      <id>562888448</id>
      <parentid>519506014</parentid>
      <timestamp>2013-07-04T21:44:04Z</timestamp>
      <contributor>
        <username>SporkBot</username>
        <id>12406635</id>
      </contributor>
      <minor />
      <comment>Replace template per [[Wikipedia:Templates for discussion/Log/2013 June 17|TFD outcome]]; no change in content</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="203" xml:space="preserve">{{Cat main}}
{{see also|WP:WikiProject Philosophy/Resources}}

[[Category:Belief]]
[[Category:Formal epistemology]]
[[Category:Knowledge representation]]
[[Category:Logic]]
[[Category:Logic programming]]</text>
      <sha1>qebihyvwur7tpreobbc3b9gri4n63fz</sha1>
    </revision>
  </page>
  <page>
    <title>Thesaurus</title>
    <ns>0</ns>
    <id>30334</id>
    <revision>
      <id>760342893</id>
      <parentid>742726311</parentid>
      <timestamp>2017-01-16T11:29:07Z</timestamp>
      <contributor>
        <username>Omnipaedista</username>
        <id>8524693</id>
      </contributor>
      <comment>standardized punct.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4682" xml:space="preserve">{{about|thesauri for general or literary applications|thesauri designed for information retrieval|Thesaurus (information retrieval)|the Clare Fischer album|Thesaurus (album)}}
[[File:Historical Thesaurus.jpg|thumb|''Historical Thesaurus of the Oxford English Dictionary'', two-volume set]]
In general usage, a '''thesaurus''' is a [[reference work]] that lists words grouped together according to similarity of meaning (containing [[synonyms]] and sometimes [[antonyms]]), in contrast to a [[dictionary]], which provides [[definitions]] for words, and generally lists them in alphabetical order. The main purpose of such reference works is to help the user "to find the word, or words, by which [an] idea may be most fitly and aptly expressed"&amp;nbsp;– to quote [[Peter Mark Roget]], architect of the best known thesaurus in the English language.&lt;ref name="Roget"&gt;Roget, Peter. 1852. ''Thesaurus of English Language Words and Phrases''.&lt;/ref&gt;

Although including synonyms, a thesaurus should not be taken as a complete list of all the synonyms for a particular word. The entries are also designed for drawing distinctions between similar words and assisting in choosing exactly the right word. Unlike a [[dictionary]], a thesaurus entry does not give the definition of words.

In [[library science]] and [[information science]], thesauri have been widely used to specify domain models.  Recently, thesauri have been implemented with [[Simple Knowledge Organization System]] (SKOS).{{citation needed|date=January 2016}}

== Etymology ==
The word "thesaurus" is derived from 16th-century [[New Latin]], in turn from [[Latin]] ''[[wikt:en:thesaurus#Latin|thēsaurus]]'', which is the [[Latinisation (literature)|Latinisation]] of the [[Ancient Greek|Greek]] {{lang|grc|[[wikt:en:θησαυρός#Ancient Greek|θησαυρός]]}} (''thēsauros''), "treasure, treasury, storehouse".&lt;ref name="Harper"&gt;[http://www.etymonline.com/index.php?term=thesaurus "thesaurus"]. ''[[Online Etymology Dictionary]]''.&lt;/ref&gt; The word ''thēsauros'' is of uncertain etymology. [[Douglas Harper]] derives it from the root of the Greek verb τιθέναι ''tithenai'', "to put, to place."&lt;ref name="Harper" /&gt; [[Robert S. P. Beekes|Robert Beekes]] rejected an [[Proto-Indo-European language|Indo-European]] derivation and suggested a [[Pre-Greek]] suffix {{nowrap|''*-ar&lt;sup&gt;w&lt;/sup&gt;o-''}}.&lt;ref&gt;[[Robert S. P. Beekes|R. S. P. Beekes]], ''Etymological Dictionary of Greek'', Brill, 2009, p. 548.&lt;/ref&gt;

From the 16th to the 19th centuries, the term "thesaurus" was applied to any [[dictionary]] or [[encyclopedia]], as in the ''[[Thesaurus linguae latinae]]'' (1532), and the ''[[Thesaurus linguae graecae]]'' (1572). The meaning "collection of words arranged according to sense" is first attested in 1852 in Roget's title and ''thesaurer'' is attested in [[Middle English]] for "[[treasurer]]".&lt;ref name="Harper" /&gt;

== History ==
[[File:Roget P M.jpg|150px|right|thumb|[[Peter Mark Roget]], author of the first thesaurus.]]
In antiquity, [[Philo of Byblos]] authored the first text that could now be called a thesaurus. In [[Sanskrit]], the [[Amarakosha]] is a thesaurus in verse form, written in the 4th century.

The first modern thesaurus was ''[[Roget's Thesaurus]]'', first compiled in 1805 by [[Peter Mark Roget]], and published in 1852. Since its publication it has never been out of print and is still a widely used work across the English-speaking world.&lt;ref&gt;http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780199254729.001.0001/acprof-9780199254729-chapter-1&lt;/ref&gt; Entries in ''Roget's Thesaurus'' are listed conceptually rather than alphabetically.
Roget described his thesaurus in the foreword to the first edition:

&lt;blockquote&gt;It is now nearly fifty years since I first projected a system of verbal classification similar to that on which the present work is founded. Conceiving that such a compilation might help to supply my own deficiencies, I had, in the year 1805, completed a classed catalogue of words on a small scale, but on the same principle, and nearly in the same form, as the Thesaurus now published.&lt;ref&gt;Lloyd 1982, p. xix{{Full citation needed|date=August 2014}}&lt;/ref&gt;
&lt;/blockquote&gt;

== See also ==
* [[AGRIS]]
* [[Controlled vocabulary]]
* [[Knowledge Organization Systems]]
* [[Ontology (computer science)]]
* [[Simple Knowledge Organisation System]]
* [[ISO 25964]]

== References ==
{{Reflist}}

== External links ==
* {{Wiktionary-inline|thesaurus}}

{{Lexicography}}

[[Category:Thesauri| ]]
[[Category:Information science]]
[[Category:Knowledge representation]]
[[Category:Reference works]]
[[Category:Dictionaries by type]]
[[Category:Lexical semantics]]</text>
      <sha1>arnrb56ay2pva6hlfw0bnlj01byqazw</sha1>
    </revision>
  </page>
  <page>
    <title>Protégé (software)</title>
    <ns>0</ns>
    <id>5007318</id>
    <revision>
      <id>747620753</id>
      <parentid>737605140</parentid>
      <timestamp>2016-11-03T11:43:24Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor />
      <comment>/* top */http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4058" xml:space="preserve">{{Infobox software
| name                   = Protégé
| logo                   = &lt;!-- Image name is enough --&gt;
| logo alt               = 
| screenshot             = &lt;!-- Image name is enough --&gt;
| caption                = 
| screenshot alt         = 
| collapsible            = 
| author                 = 
| developer              = [[Stanford University School of Medicine|Stanford]] Center for Biomedical Informatics Research
| released               = {{Start date and age|1999|11|11|df=yes}}&lt;ref name=versions&gt;{{cite web |title=Protege Desktop Older Versions |website=Protege Wiki |date=24 May 2016 |url=http://protegewiki.stanford.edu/wiki/Protege_Desktop_Old_Versions }}&lt;/ref&gt;
| discontinued           = 
| latest release version = 5.0.0
| latest release date    = {{Start date and age|2016|05|24|df=yes}}&lt;ref name=versions/&gt;
| latest preview version = 
| latest preview date    = &lt;!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} --&gt;
| status                 = Active
| programming language   = [[Java (programming language)|Java]]
| operating system       = Linux, Mac OS X &amp; Windows&lt;ref name=install5&gt;{{cite web |title=Protege Desktop 5.0 Installation Instructions |website=Protege Wiki |url=http://protegewiki.stanford.edu/wiki/Install_Protege5 }}&lt;/ref&gt;
| platform               = Java {{abbr|VM|Virtual Machine}}&lt;ref&gt;{{cite web |title= Protege Desktop Frequently Asked Questions  |website=Protege Wiki |url=http://protegewiki.stanford.edu/wiki/Protege-OWL_4_FAQ }}&lt;/ref&gt;
| size                   = 
| language               = 
| language count         = &lt;!-- Number only --&gt;
| language footnote      = 
| genre                  = [[Ontology (information science)#Editor|Ontology editor]]
| license                = [[BSD licenses#2-clause license ("Simplified BSD License" or "FreeBSD License")|BSD 2-clause]]
| alexa                  = 
| website                = {{URL|protege.stanford.edu}}
| repo                   = {{URL|https://github.com/protegeproject/protege}}
| standard               = 
| AsOf                   = 
}}

'''Protégé''' is a free, open source [[Ontology (computer science)|ontology]] editor and a [[knowledge management]] system. Protégé provides a graphic user interface to define ontologies. It also includes [[deductive classifier]]s to validate that models are consistent and to infer new information based on the analysis of an ontology. Like [[Eclipse (software)|Eclipse]], Protégé is a framework for which various other projects suggest plugins. This application is written in [[Java (programming language)|Java]] and heavily uses [[Swing (Java)|Swing]] to create the user interface. Protégé recently has over 300,000 registered users.&lt;ref&gt;[http://protege.stanford.edu/community.php Protégé Community]&lt;/ref&gt; According to a 2009 book it is "the leading ontological engineering tool".&lt;ref name="SelicGaševic2009"&gt;{{cite book|author1=Dragan Gašević|author2=Dragan Djurić|author3=Vladan Devedžić|title=Model Driven Engineering and Ontology Development|url=https://books.google.com/books?id=s-9yu7ubSykC&amp;pg=PA194|year= 2009|publisher=Springer|isbn=978-3-642-00282-3|pages=194|edition=2nd}}&lt;/ref&gt;

Protégé is being developed at [[Stanford University]] and is made available under the [[BSD licenses#2-clause license ("Simplified BSD License" or "FreeBSD License")|BSD 2-clause license]].&lt;ref&gt;{{cite web |title=protege/license.txt |website=GitHub |url=https://github.com/protegeproject/protege/blob/master/license.txt }}&lt;/ref&gt; Earlier versions of the tool were developed in collaboration with the [[University of Manchester]].

== References ==
&lt;references/&gt;

== External links ==
* {{Official website|http://protege.stanford.edu/}}
* [http://protegewiki.stanford.edu/wiki/Main_Page Protégé wiki]

{{DEFAULTSORT:Protege (software)}}
[[Category:Knowledge representation]]
[[Category:Free integrated development environments]]
[[Category:Ontology (information science)]]
[[Category:Ontology editors]]
[[Category:Free software programmed in Java (programming language)]]


{{programming-software-stub}}</text>
      <sha1>s43hvvok1qqkywvr5fr2skv75uvb4zn</sha1>
    </revision>
  </page>
  <page>
    <title>Spatial–temporal reasoning</title>
    <ns>0</ns>
    <id>3342061</id>
    <revision>
      <id>748137965</id>
      <parentid>741649771</parentid>
      <timestamp>2016-11-06T15:22:28Z</timestamp>
      <contributor>
        <username>MaxEnt</username>
        <id>1190064</id>
      </contributor>
      <comment>fix lead; add unkind section headers</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5491" xml:space="preserve">{{about|spatial-temporal reasoning in information technology|spatial-temporal reasoning in psychology|Spatial visualization ability}}
{{technical|date=October 2012}}
'''Spatial–temporal reasoning''' is an area of [[Artificial Intelligence|artificial intelligence]] which draws from the fields of [[computer science]], [[cognitive science]], and [[cognitive psychology]]. The theoretic goal—on the cognitive side—involves representing and reasoning spatial-temporal knowledge in mind. The applied goal—on the computing side—involves developing high-level control systems of robots for navigating and understanding time and space. 

== Influence from cognitive psychology ==
A convergent result in cognitive psychology is that the connection relation is the first spatial relation that human babies acquire, followed by understanding orientation relations and distance relations. Internal relations among the three kinds of spatial relations can be computationally and systematically explained within the theory of cognitive prism as follows: (1) the connection relation is primitive; (2) an orientation relation is a distance comparison relation: you being in front of me can be interpreted as you are nearer to my front side than my other sides; (3) a distance relation is connection relations using a third object: you being one meter away from me can be interpreted as an object with the maximum extension of one meter can be connected with you and me simultaneously. 

== Fragmentary representations of temporal calculi ==
Without addressing internal relations among spatial relations, AI researchers contributed many fragmentary representations. Examples of temporal calculi include [[Allen's interval algebra]], and Vilain's &amp; Kautz's [[point algebra]]. The most prominent spatial calculi are [[Mereotopology|mereotopological calculi]], [[Andrew U. Frank|Frank]]'s [[cardinal direction calculus]], Freksa's double cross calculus, Egenhofer and Franzosa's [[9-intersection calculus|4- and 9-intersection calculi]], Ligozat's [[flip-flop calculus]], various [[region connection calculus|region connection calculi]] (RCC), and the [[Oriented Point Relation Algebra]]. Recently, spatio-temporal calculi have been designed that combine spatial and temporal information. For example, the [[spatiotemporal constraint calculus]] (STCC) by Gerevini and Nebel combines Allen's interval algebra with RCC-8. Moreover, the [[qualitative trajectory calculus]] (QTC) allows for reasoning about moving objects.

== Quantitative abstraction ==
An emphasis in the literature has been on [[Qualitative reasoning|qualitative]] spatial-temporal reasoning which is based on qualitative abstractions of temporal and spatial aspects of the common-sense background knowledge on which our human perspective of physical reality is based.  Methodologically, qualitative [[Constraint satisfaction|constraint]] calculi restrict the vocabulary of rich mathematical theories dealing with temporal or spatial entities such that specific aspects of these theories can be treated within [[Decidability (logic)|decidable]] fragments with simple qualitative (non-[[Metric (mathematics)|metric]]) languages. Contrary to mathematical or physical theories about space and time, qualitative constraint calculi allow for rather inexpensive reasoning about entities located in space and time.  For this reason, the limited expressiveness of qualitative representation formalism calculi is a benefit if such reasoning tasks need to be integrated in applications.  For example, some of these calculi may be implemented for handling spatial [[Geographic information system|GIS]] queries efficiently and some may be used for navigating, and communicating with, a mobile [[robot]].

== Relation algebra ==
Most of these calculi can be formalized as abstract [[relation algebra]]s, such that reasoning can be carried out at a symbolic level. For computing solutions of a [[constraint network]], the [[Local consistency#Path_consistency|path-consistency algorithm]] is an important tool.

== Software ==
* [http://www.sfbtr8.spatial-cognition.de/de/projekte/reasoning/r4-logospace/research-tools/gqr/ GQR], constraint network solver for calculi like RCC-5, RCC-8, Allen's interval algebra, point algebra, cardinal direction calculus, etc.

== See also ==
*[[Cerebral cortex]]
*[[Diagrammatic reasoning]]
*[[Temporal logic]]
*[[Visual thinking]]
*[[Spatial ability]]

== Notes ==
{{reflist}}

==References==
*J. Renz, B. Nebel, [http://users.rsise.anu.edu.au/~jrenz/papers/renz-nebel-los.pdf Qualitative Spatial Reasoning using Constraint Calculi], in: M. Aiello, I. Pratt-Hartmann, J. van Benthem (eds.): Handbook of Spatial Logics, Springer 2007.
*T. Dong: [http://www.jstor.org/stable/41217909?seq=1#page_scan_tab_contents A COMMENT ON RCC: FROM RCC TO RCC⁺⁺]. Journal of Philosophical Logic, Vol 34, No. 2, pp. 319--352
*M. Vilain, H. Kautz, P. van Beek, [http://www.cs.rochester.edu/~kautz/papers/vilain-kautz-book.pdf Constraint propagation algorithms for temporal reasoning: A Revised Report], 1987.
*T. Dong. [http://www.springer.com/de/book/9783642240577 Recognizing Variable Environment -- The Theory of Cognitive Prism]. Studies in Computational Intelligence, Vol. 388, Springer-Verlag, Berlin Heidelberg, 2012.

{{DEFAULTSORT:Spatial-temporal reasoning}}
[[Category:Cognitive science]]
[[Category:Knowledge representation]]
[[Category:Educational psychology]]
[[Category:Logical calculi]]
[[Category:Reasoning]]</text>
      <sha1>gaewb4asl7rxrfihernjqd8ixvtexqw</sha1>
    </revision>
  </page>
  <page>
    <title>Chow–Liu tree</title>
    <ns>0</ns>
    <id>12680566</id>
    <revision>
      <id>724727874</id>
      <parentid>689740496</parentid>
      <timestamp>2016-06-11T02:58:24Z</timestamp>
      <contributor>
        <username>Dcirovic</username>
        <id>11795905</id>
      </contributor>
      <minor />
      <comment>refs, [[WP:AWB/T|typo(s) fixed]]: so called → so-called using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8558" xml:space="preserve">[[File:Chow-liu.png|thumb|400 px|A first-order dependency tree representing the product on the left.]]

In probability theory and statistics '''Chow–Liu tree''' is an efficient method for constructing a second-[[Orders of approximation|order]] product approximation of a [[joint probability distribution]], first described in a paper by {{Harvtxt|Chow|Liu|1968}}.  The goals of such a decomposition, as with such [[Bayesian networks]] in general, may be either [[data compression]] or [[inference]].

==The Chow–Liu representation==
The Chow–Liu method describes a [[joint probability distribution]] &lt;math&gt;P(X_{1},X_{2},\ldots,X_{n})&lt;/math&gt; as a product of second-order conditional and marginal distributions.  For example, the six-dimensional distribution &lt;math&gt;P(X_{1},X_{2},X_{3},X_{4},X_{5},X_{6})&lt;/math&gt; might be approximated as

:&lt;math&gt;
P^{\prime
}(X_{1},X_{2},X_{3},X_{4},X_{5},X_{6})=P(X_{6}|X_{5})P(X_{5}|X_{2})P(X_{4}|X_{2})P(X_{3}|X_{2})P(X_{2}|X_{1})P(X_{1})
&lt;/math&gt;

where each new term in the product introduces just one new variable, and the product can be represented as a first-order dependency tree, as shown in the figure.  The Chow–Liu algorithm (below) determines which conditional probabilities are to be used in the product approximation.   In general, unless there are no third-order or higher-order interactions, the Chow–Liu approximation is indeed an ''approximation'', and cannot capture the complete structure of the original distribution.  {{Harvtxt|Pearl|1988}} provides a modern analysis of the Chow–Liu tree as a [[Bayesian network]].

==The Chow–Liu algorithm==
Chow and Liu show how to select second-order terms for the product approximation so that, among all such second-order approximations (first-order dependency trees), the constructed approximation &lt;math&gt;P^{\prime}&lt;/math&gt; has the minimum [[Kullback–Leibler distance]] to the actual distribution &lt;math&gt;P&lt;/math&gt;, and is thus the ''closest'' approximation in the classical [[information theory|information-theoretic]] sense. The Kullback–Leibler distance between a second-order product approximation and the actual distribution is shown to be

:&lt;math&gt;
D(P\parallel P^{\prime })=-\sum I(X_{i};X_{j(i)})+\sum
H(X_{i})-H(X_{1},X_{2},\ldots ,X_{n})
&lt;/math&gt;

where &lt;math&gt;I(X_{i};X_{j(i)})&lt;/math&gt; is the [[mutual information]] between variable &lt;math&gt;X_{i}&lt;/math&gt; and its parent &lt;math&gt;X_{j(i)}&lt;/math&gt; and &lt;math&gt;H(X_{1},X_{2},\ldots ,X_{n})&lt;/math&gt; is the [[joint entropy]] of variable set &lt;math&gt;\{X_{1},X_{2},\ldots ,X_{n}\}&lt;/math&gt;.   Since the terms &lt;math&gt;\sum H(X_{i})&lt;/math&gt; and  &lt;math&gt;H(X_{1},X_{2},\ldots ,X_{n})&lt;/math&gt; are independent of the dependency ordering in the tree, only the sum of the pairwise [[mutual information]]s, &lt;math&gt;\sum I(X_{i};X_{j(i)})&lt;/math&gt;, determines the quality of the approximation. Thus, if every branch (edge) on the tree is given a weight corresponding to the mutual information between the variables at its vertices, then the tree which provides the optimal second-order approximation to the target distribution is just the ''maximum-weight tree''. The equation above also highlights the role of the dependencies in the approximation: When no dependencies exist, and the first term in the equation is absent, we have only an approximation based on first-order marginals, and the distance between the approximation and the true distribution is due to the redundancies that are not accounted for when the variables are treated as independent. As we specify second-order dependencies, we begin to capture some of that structure and reduce the distance between the two distributions.

Chow and Liu provide a simple algorithm for constructing the optimal tree; at each stage of the procedure the algorithm simply adds the maximum [[mutual information]] pair to the tree.  See the original paper, {{Harvtxt|Chow|Liu|1968}}, for full details. A more efficient tree construction algorithm for the common case of sparse data was outlined in {{Harvtxt|Meilă|1999}}.

Chow and Wagner proved in a later paper {{Harvtxt|Chow|Wagner|1973}} that the learning of the Chow–Liu tree is consistent given samples (or observations) drawn i.i.d. from a tree-structured distribution. In other words, the probability of learning an incorrect tree decays to zero as the number of samples tends to infinity. The main idea in the proof is the continuity of the mutual information in the pairwise marginal distribution. Recently, the exponential rate of convergence of the error probability was provided.&lt;ref name="Tan"&gt;A Large-Deviation Analysis for the Maximum-Likelihood Learning of Tree Structures. V. Y. F. Tan, A. Anandkumar, L. Tong and A. Willsky. In the International symposium on information theory (ISIT), July 2009.&lt;/ref&gt;

==Variations on Chow–Liu trees==
The obvious problem which occurs when the actual distribution is not in fact a second-order dependency tree can still in some cases be addressed by fusing or aggregating together densely connected subsets of variables to obtain a "large-node" Chow–Liu tree {{Harv|Huang|King|2002}}, or by extending the idea of greedy maximum branch weight selection to non-tree (multiple parent) structures {{Harv|Williamson|2000}}. (Similar techniques of variable substitution and construction are common in the [[Bayes network]] literature, e.g., for dealing with loops.  See {{Harvtxt|Pearl|1988}}.)

Generalizations of the Chow–Liu tree are the so-called [[t-cherry junction trees]]. It is proved that the t-cherry junction trees provide a better or at least as good approximation for a  discrete multivariate probability distribution as the Chow–Liu tree gives.
For the third order t-cherry junction tree see {{Harv|Kovács|Szántai|2010}}, for the ''k''th-order t-cherry junction tree see {{Harv|Szántai|Kovács|2010}}. The second order t-cherry junction tree is in fact the Chow–Liu tree.

==See also==
*[[Bayesian network]]
*[[Knowledge representation]]

==Notes==
{{reflist}}

==References==
{{refbegin|2}}
*{{Citation
 | last=Chow | first=C. K. | last2=Liu | first2=C.N.
 | title=Approximating discrete probability distributions with dependence trees
 | journal=IEEE Transactions on Information Theory
 | volume=IT-14  | issue=3 | year=1968 | pages=462–467 | url= | doi=10.1109/tit.1968.1054142}}.
*{{Citation
 | last=Huang | first=Kaizhu  | last2=King | first2=Irwin
 | last3=Lyu | first3=Michael R.  | year= 2002
 | chapter=Constructing a large node Chow–Liu tree based on frequent itemsets
 |editor1=Wang, Lipo |editor2=Rajapakse, Jagath C. |editor3=Fukushima, Kunihiko |editor4=Lee, Soo-Young |editor5=Yao, Xin | title=Proceedings of the 9th International Conference on Neural Information Processing ({ICONIP}'02)
 | place=[[Singapore]] | url= | accessdate= | pages=498–502}}.
*{{Citation
 | last=Pearl | first=Judea
 | title=Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference
 | publisher=[[Morgan Kaufmann]] | place=[[San Mateo, CA]] | year=1988}}
*{{Citation
 | last=Williamson | first=Jon | year= 2000
 | chapter=Approximating discrete probability distributions with Bayesian networks
 | title=Proceedings of the International Conference on Artificial Intelligence in Science and Technology
 | place=[[Tasmania]] | accessdate= | pages=16–20}}.
*{{Citation
 | last=Meilă | first=Marina | year= 1999
 | chapter=An Accelerated Chow and Liu Algorithm: Fitting Tree Distributions to High-Dimensional Sparse Data
 | title=Proceedings of the Sixteenth International Conference on Machine Learning
 | publisher=Morgan Kaufmann | accessdate= | pages=249–257}}.
*{{Citation
 | last=Chow | first=C. K. | last2=Wagner | first2=T.
 | title=Consistency of an estimate of tree-dependent probability distribution
 | journal=IEEE Transactions on Information Theory
 | volume=IT-19  | issue=3 | year=1973 | pages=369–371 | doi=10.1109/tit.1973.1055013}}.
*{{Citation
 | last=Kovács | first=E. | last2=Szántai | first2=T.
 | title=On the approximation of a discrete multivariate probability distribution using the new concept of t-cherry junction tree
 | journal=Lecture Notes in Economics and Mathematical Systems
 | volume=633, Part 1 | year=2010 | pages= 39–56 | doi=10.1007/978-3-642-03735-1_3}}.
*{{Citation
 | last=Szántai | first=T. | last2=Kovács | first2=E.
 | title=Hypergraphs as a mean of discovering the dependence structure of a discrete multivariate probability distribution
 | journal=Annals of Operations Research | year=2010 | pages= }}.
{{refend}}

{{DEFAULTSORT:Chow-Liu tree}}
[[Category:Knowledge representation]]</text>
      <sha1>9yc8ocambiudu6d36omwvzf340xh86w</sha1>
    </revision>
  </page>
  <page>
    <title>Composite portrait</title>
    <ns>0</ns>
    <id>39089943</id>
    <revision>
      <id>654281071</id>
      <parentid>644274549</parentid>
      <timestamp>2015-03-31T03:26:40Z</timestamp>
      <contributor>
        <ip>69.86.191.173</ip>
      </contributor>
      <comment>deleted links</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3240" xml:space="preserve">[[File:Composite portraiture Galton.jpg|thumb|Composite portraiture, [[Francis Galton]], 1883.]]
'''Composite portraiture''' (also known as [[composite photograph]]s) is a technique invented by Sir [[Francis Galton]] in the 1880s after a suggestion by [[Herbert Spencer]] for registering photographs of human faces on the two eyes to create an "average" photograph of all those in the photographed group.&lt;ref&gt;Benson, P., &amp; Perrett, D. (1991). [http://www.abebooks.com/9781854890368/Photovideo-1854890360/plp Computer averaging and manipulations of faces.] In P. Wombell (ed.), ''Photovideo: Photography in the age of the computer'' (pp. 32–38). London: Rivers Oram Press.&lt;/ref&gt;&lt;ref&gt;Galton, F. (1878). [http://www.galton.org/essays/1870-1879/galton-1879-jaigi-composite-portraits.pdf Composite portraits.] ''Journal of the Anthropological Institute of Great Britain and Ireland, 8'', 132–142.&lt;/ref&gt;

Spencer had suggested using onion paper and line drawings, but Galton devised a technique for multiple exposures on the same photographic plate.  He noticed that these composite portraits were more attractive than any individual member, and this has generated a large body of research on human [[attractiveness]] and [[averageness]] one hundred years later.  He also suggested in a [[Royal Society]] presentation in 1883 that the composites provided an interesting concrete representation of human [[ideal type]]s and [[concept]]s.  He discussed using the technique to investigate characteristics of common types of humanity, such as criminals.  In his mind, it was an extension of the statistical techniques of [[average]]s and [[correlation]].  In this sense, it represents one of the first implementations of [[convolution]] [[factor analysis]] and [[neural network]]s in the understanding of [[knowledge representation]] in the human mind. Galton also suggested that the technique could be used for creating natural types of common objects.

During the late 19th century, English psychometrician [[Sir Francis Galton]] attempted to define [[Physiognomy|physiognomic]] characteristics of health, disease, beauty, and criminality, via a method of composite photography. Galton's process involved  the photographic superimposition of two or more faces by multiple exposures. After averaging together photographs of violent criminals, he found that the composite appeared "more respectable" than any of the faces comprising it; this was likely due to the irregularities of the skin across the constituent images being averaged out in the final blend. With the advent of computer technology during the early 1990s, Galton's composite technique has been adopted and greatly improved using computer graphics software.&lt;ref&gt;Yamaguchi, M. K., Hirukawa, T., &amp; Kanazawa, S. (1995). [http://www.perceptionweb.com/abstract.cgi?id=p240563 Judgment of gender through facial parts.] ''Perception, 24'', 563–575.&lt;/ref&gt;

== References ==
&lt;references /&gt;

==External links==
* [http://www.medienkunstnetz.de/works/composite-fotografie/ Samples of Galton's composites]
* [http://www.compositeportraits.com/ A Visual History of Composite Portraiture in Photography. Edited by Jake Rowland]

[[Category:Portrait art]]
[[Category:Knowledge representation]]</text>
      <sha1>7snvmrfx8otdyp53pws8xsnqocj0kw2</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Dewey Decimal Classification</title>
    <ns>14</ns>
    <id>39327232</id>
    <revision>
      <id>590555073</id>
      <parentid>553986460</parentid>
      <timestamp>2014-01-13T19:47:16Z</timestamp>
      <contributor>
        <ip>63.251.123.2</ip>
      </contributor>
      <comment>I don't see how this is an instance of [[:Category:Science studies]], although Science Studies certainly concerns itself with "Knowledge representation"</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="119" xml:space="preserve">{{catmore}}

[[Category:Knowledge representation]]
[[Category:Library cataloging and classification]]
[[Category:OCLC]]</text>
      <sha1>cqml8oaxohfyvwocpbvzjwe0jmc8iqk</sha1>
    </revision>
  </page>
  <page>
    <title>Retrievability</title>
    <ns>0</ns>
    <id>39585214</id>
    <revision>
      <id>757295412</id>
      <parentid>753458935</parentid>
      <timestamp>2016-12-29T21:59:33Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor />
      <comment>Remove blank line(s) between list items per [[WP:LISTGAP]] to fix an accessibility issue for users of [[screen reader]]s. Do [[WP:GENFIXES]] and cleanup if needed. Discuss this at [[Wikipedia talk:WikiProject Accessibility/LISTGAP]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2050" xml:space="preserve">'''Retrievability''' is a term associated with the ease with which information can be found or retrieved using an information system, specifically a [[search engine]] or [[information retrieval]] system.

A document (or information object) has high retrievability if there are many queries which retrieve the document via the search engine, and the document is ranked sufficiently high that a user would encounter the document. Conversely, if there are few queries that retrieve the document, or when the document is retrieved the documents are not high enough in the ranked list, then the document has low retrievability.

Retrievability can be considered as one aspect of [[findability]].

Applications of retrievability include detecting [[Web search engine#Search engine bias|search engine bias]], measuring algorithmic bias, evaluating the influence of search technology, tuning information retrieval systems and evaluating the quality of documents in a [[text corpus|collection]].

==See also==
* [[Information retrieval]]
* [[Knowledge mining]]
* [[Search engine optimization]]
* [[Findability]]

==References==
*{{cite book|author1=Azzopardi, L.  |author2=Vinay, V.  |lastauthoramp=yes | chapter=Retrievability: an evaluation measure for higher order information access tasks| title=Proceedings of the 17th ACM conference on Information and knowledge management| year=2008| pages=561–570| publisher=ACM| location=Napa Valley, California, USA| series=CIKM '08| doi=10.1145/1458082.1458157| url=http://doi.acm.org/10.1145/1458082.1458157| accessdate=5 June 2013}}
*{{cite book|author1=Azzopardi, L.  |author2=Vinay, V.  |lastauthoramp=yes | chapter=Accessibility in information retrieval| title=Proceedings of the IR research, 30th European conference on Advances in information retrieval| year=2008| pages=482–489| publisher=Springer| location=Glasgow,UK| series=ECIR '08| url=http://dl.acm.org/citation.cfm?id=1793333| accessdate=7 Dec 2016}}

[[Category:Web design]]
[[Category:Knowledge representation]]
[[Category:Information science]]</text>
      <sha1>r5seqt4bw65ssmbyouvswhffhqzb4tl</sha1>
    </revision>
  </page>
  <page>
    <title>Universal Decimal Classification</title>
    <ns>0</ns>
    <id>32129</id>
    <revision>
      <id>761823506</id>
      <parentid>761823485</parentid>
      <timestamp>2017-01-25T00:21:34Z</timestamp>
      <contributor>
        <username>Oshwah</username>
        <id>3174456</id>
      </contributor>
      <minor />
      <comment>Reverted edits by [[Special:Contributions/160.171.136.221|160.171.136.221]] ([[User talk:160.171.136.221|talk]]): Editing tests ([[WP:HG|HG]]) (3.1.22)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="47429" xml:space="preserve">The '''Universal Decimal Classification''' ('''UDC''') is a bibliographic and [[library classification]] developed by the [[Belgium|Belgian]] bibliographers [[Paul Otlet]] and [[Henri La Fontaine]] at the end of the 19th century. They worked with numerous subject specialists, for example, [[Herbert Haviland Field]] at the [[Concilium Bibliographicum]] for Zoology. UDC provides a systematic arrangement of all branches of human knowledge organized as a coherent system in which knowledge fields are related and inter-linked.&lt;ref name="UDC Fact Sheet"&gt;[http://www.udcc.org/index.php/site/page?view=factsheet UDC Fact Sheet], UDC Consortium website&lt;/ref&gt;&lt;ref&gt;[McIlwaine, I. C. "Universal Decimal Classification: a guide to its use. Revised ed. The Hague: UDC Consortium, 2007]&lt;/ref&gt;&lt;ref&gt;[http://www.tandfonline.com/doi/abs/10.1081/E-ELIS3-120043532 McIlwaine, I. C. (2010) Universal Decimal Classification (UDC). In: Encyclopedia of Library and Information Sciences. 3rd ed. New York: Taylor &amp; Francis, 2010. Vol. 1:1, pp. 5432-5439. DOI: 10.1081/E-ELIS3-120043532]&lt;/ref&gt;&lt;ref&gt;Broughton, V: Universal Decimal Classification - chapters 18 and 19. IN: Essential Classification. London: Facet Publishing, 2004, pp. 207-256&lt;/ref&gt;

Originally based on the [[Dewey Decimal Classification]], the UDC was developed as a new analytico-synthetic classification system with a significantly larger vocabulary and syntax that enables very detailed content indexing and information retrieval in large collections.&lt;ref name="UDC History"&gt;[http://www.udcc.org/index.php/site/page?view=about_history UDC History], "About UDC" - UDC Consortium website&lt;/ref&gt;&lt;ref&gt;[http://onlinelibrary.wiley.com/doi/10.1002/%28SICI%291097-4571%28199704%2948:4%3C331::AID-ASI6%3E3.0.CO;2-X/abstract McIlwaine, I. C. (1997) The Universal Decimal Classification: Some factors concerning its origins, development, and influence. Journal of the American Society for Information Science, 48 (4), pp. 331–339]&lt;/ref&gt; In its first edition in 1905, the UDC already included many features that were revolutionary in the context of knowledge classifications: tables of generally applicable (aspect-free) concepts—called common auxiliary tables; a series of special auxiliary tables with specific but re-usable attributes in a particular field of knowledge; an expressive notational system with connecting symbols and syntax rules to enable coordination of subjects and the creation of a documentation language proper. Albeit originally designed as an indexing and retrieval system, due to its logical structure and scalability, UDC has become one of the most widely used knowledge organization systems in libraries, where it is used for either shelf arrangement, content indexing or both.&lt;ref&gt;[http://hdl.handle.net/10150/105685 Slavic, A. (2004) UDC implementation: from library shelves to a structured indexing language. International Cataloguing and Bibliographic Control , 33 3(2004), 60-65.]&lt;/ref&gt; UDC codes can describe any type of document or object to any desired level of detail. These can include textual documents and other media such as [[film]]s, [[video]] and [[sound]] recordings, [[illustration]]s, [[map]]s as well as [[realia (library science)|realia]] such as [[museum]] objects.

Since the first edition in French "Manuel du Répertoire bibliographique universel" (1905), UDC has been translated and published in various editions in 40 languages.&lt;ref&gt;[http://www.udcc.org/index.php/site/page?view=editions UDC Editions], UDC Consortium website&lt;/ref&gt;&lt;ref&gt;[http://hdl.handle.net/10150/106363 Slavic, A. (2004) UDC Translations: a 2004 Survey Report and Bibliography. Extensions &amp; Corrections to the UDC, 26 (2004): 58-80. ]&lt;/ref&gt; UDC Summary, an abridged Web version of the scheme is available in over 50 languages.&lt;ref name="UDCS" /&gt; The classification has been modified and extended over the years to cope with increasing output in all areas of human knowledge, and is still under continuous review to take account of new developments.&lt;ref&gt;[http://www.udcc.org/index.php/site/page?view=major_revisions Major Revisions of the UDC 1993-2013], UDC Consortium website&lt;/ref&gt;&lt;ref&gt;[http://hdl.handle.net/10150/105220 Slavic, A., Cordeiro, M. I. &amp; Riesthuis, G. (2008) Maintenance of the Universal Decimal Classification: overview of the past and preparations for the future. International Cataloguing and Bibliographic Control, 37 (2), 23-29.]&lt;/ref&gt;

== The application of UDC ==

UDC is used in around 150,000 libraries in 130 countries and in many bibliographical services which require detailed content indexing. In a number of countries it is the main classification system for information exchange and is used in all type of libraries: public, school, academic and special libraries.&lt;ref&gt;[http://hdl.handle.net/10150/105579 Slavic, A. (2008) Use of the Universal Decimal Classification: a worldwide survey. Journal of Documentation, 64 (2), 2008: 211-228. ]&lt;/ref&gt;&lt;ref name="UDC Users Worldwide"&gt;[http://www.udcc.org/index.php/site/page?view=users_worldwide  UDC Users Worldwide], UDC Consortium website&lt;/ref&gt;&lt;ref name="UDC Countries"&gt;[http://www.udcc.org/countries.htm UDC Countries], UDC Consortium website&lt;/ref&gt;

UDC is also used in national bibliographies of around 30 countries. Examples of large databases indexed by UDC include:&lt;ref name="Large collections"&gt;[http://www.udcc.org/index.php/site/page?view=collections Collections indexed by UDC], UDC Consortium website&lt;/ref&gt; 
: NEBIS (The Network of Libraries and Information Centers in Switzerland) – 2.6 million records
: COBIB.SI (Slovenian National Union Catalogue) – 3.5 million records
: Hungarian National Union Catalogue (MOKKA) – 2.9 million records
: [[VINITI RAS]] database (All-Russian Scientific and Technical Information Institute of Russian Academy of Science) with 28 million records
: Meteorological &amp; Geoastrophysical Abstracts (MGA) with 600 journal titles
: PORBASE (Portuguese National Bibliography) with 1.5 million records

UDC has traditionally been used for the indexing of scientific articles which was an important source of information of scientific output in the period predating electronic publishing. Collections of research articles in many countries covering decades of scientific output contain UDC codes.  Examples of journal articles indexed by UDC:
:UDC code '''663.12:57.06''' in the article "Yeast Systematics: from Phenotype to Genotype" in the  journal ''Food Technology and Biotechnology'' ({{ISSN|1330-9862}})&lt;ref&gt;[http://www.ftb.pbf.hr/index.php/ftb/article/viewFile/243/241 Example: Journal article indexed by UDC] ({{ISSN|1330-9862}})&lt;/ref&gt;
:UDC code '''37.037:796.56''', provided in the article "The game method as means of interface of technical-tactical and psychological preparation in sports orienteering" in the Russian journal "''Pedagogico-psychological and medico-biological problems of the physical culture and sport''"  ({{ISSN|2070-4798}}).&lt;ref&gt;[http://www.kamgifk.ru/magazin/20_%283%29_2011/20_%283%29_2011_16.pdf Example: Journal article indexed by UDC] ({{ISSN|2070-4798}})&lt;/ref&gt;
:UDC code '''621.715:621.924:539.3''' in the article Residual Stress in Shot-Peened Sheets of AIMg4.5Mn Alloy - in the journal ''Materials and technology'' ({{ISSN|1580-2949}}).&lt;ref&gt;[http://www.docstoc.com/docs/5320753/UDK-Pregledni-znanstveni-lanek-ISSN-MTAEC-M-MI-OVI Example: Journal article indexed by UDC] ({{ISSN|1580-2949}})&lt;/ref&gt;
:
The design of UDC lends itself to machine readability, and the system has been used both with early automatic mechanical sorting devices, and modern library [[OPAC]]s.&lt;ref&gt;[http://hdl.handle.net/10150/105346 Slavic, A. (2006) The level of exploitation of Universal Decimal Classification in library OPACs: a pilot study. Vjesnik bibliotekara Hrvatske, 49(3-4):155-182]&lt;/ref&gt;&lt;ref&gt;[http://hdl.handle.net/10150/105276 Slavic, A. (2006) UDC in subject gateways: experiment or opportunity? Knowledge Organization, 33 2, 67-85.]&lt;/ref&gt; From 1993, a standard version of UDC is maintained and is distributed in a [[database]] format: UDC Master Reference File (UDC MRF) which is updated and released annually.&lt;ref name="UDC MRF"&gt;[http://www.udcc.org/index.php/site/page?view=mrf UDC Master Reference File], UDC Consortium website&lt;/ref&gt; The 2011 version of the MRF (released in 2012) contains over 70,000 classes.&lt;ref name="UDC Fact Sheet"/&gt; In the past full printed editions used to have around 220,000 subdivisions.&lt;ref name="UDCS"&gt;{{cite web
| url         = http://www.udcc.org/udcsummary/php/index.php
| title       = Multilingual Universal Decimal Classification Summary (UDCC Publication No. 088)
| year        = 2012
| work        = Multilingual Universal Decimal Classification Summary
| publisher   = UDC Consortium
| accessdate  = 2012-03-25
| quote = ''Multilingual UDC Summary (2012). Multilingual Universal Decimal Classification Summary. Web resource, v. 1.1. The Hague: UDC Consortium (UDCC Publication No. 088). Available at: http://www.udcc.org/udcsummary/php/index.php ''
}}&lt;/ref&gt;

== UDC structure ==

=== Notation ===
A notation is a code commonly used in classification schemes to represent a class, i.e. a subject and its position in the hierarchy, to enable mechanical sorting and filing of subjects. UDC uses [[Arabic numerals]] arranged decimally. Every number is thought of as a decimal fraction with the initial decimal point omitted, which determines the filing order. An advantage of decimal notational systems is that they are infinitely extensible, and when new subdivisions are introduced, they need not disturb the existing allocation of numbers. For ease of reading, a UDC notation is usually punctuated after every third digit:
{| 
|- 
| style="width:18%; font-weight:bold;" | Notation || style="font-weight:bold;" | Caption (Class description)
|-
| 539.120 ||Theoretical problems of elementary particles physics. Theories and models of fundamental interactions
|-
| 539.120.2 || Symmetries of quantum physics
|-
|539.120.22 ||  Conservation laws
|-
| 539.120.222 ||   Translations. Rotations
|-
| 539.120.224 ||   Reflection in time and space
|-
|539.120.226 ||   Space-time symmetries
|-
| 539.120.23 ||  Internal symmetries
|-
| 539.120.3 || Currents
|-
| 539.120.4 || Unified field theories
|-
|539.120.5 || Strings
|}

In UDC the notation has two features that make the scheme easier to browse and work with: 
* '''hierarchically expressive''' – the longer the notation, the more specific the class: removing the final digit automatically produces a broader class code.
* '''syntactically expressive''' – when UDC codes are combined, the sequence of digits is interrupted by a precise type of punctuation sign which indicates that the expression is a combination of classes rather than a simple class e.g. the colon in 34:32 indicates that there are two distinct notational elements: 34 Law. Jurisprudence and 32 Politics; the closing and opening parentheses and double quotes in the following code 913(574.22)"19"(084.3) indicate four separate notational elements: 913 Regional geography, (574.22) North Kazakhstan (Soltüstik Qazaqstan); "19" 20th century and (084.3) Maps (document form)

=== Basic features and syntax ===
UDC is an analytico-synthetic and [[faceted classification]]. It allows an unlimited combination of attributes of a subject and relationships between subjects to be expressed. UDC codes from different tables can be combined to present various aspects of document content and form, e.g. 94(410)"19"(075) History ''(main subject)'' of United Kingdom ''(place)'' in 20th century ''(time)'', a textbook ''(document form)''. Or: 37:2 Relationship between Education and Religion. Complex UDC expressions can be accurately parsed into constituent elements.

UDC is also a disciplinary classification covering the entire universe of knowledge.&lt;ref name="UDC Subject Coverage"&gt;[http://www.udcc.org/index.php/site/page?view=subject_coverage UDC Subject Coverage], UDC Consortium website&lt;/ref&gt; This type of classification can also be described as ''aspect'' or ''perspective'', which means that concepts are subsumed and placed under the field in which they are studied. Thus, the same concept can appear in different fields of knowledge. This particular feature is usually implemented in UDC by re-using the same concept in various combinations with the main subject, e.g. a code for language in common auxiliaries of language is used to derive numbers for ethnic grouping, individual languages in linguistics and individual literatures. Or, a code from the auxiliaries of place, e.g. ''(410) United Kingdom'', uniquely representing the concept of United Kingdom can be used to express ''911(410) Regional geography of United Kingdom'' and ''94(410) History of United Kingdom''.

=== Organization of classes ===

Concepts are organized in two kinds of tables in UDC:&lt;ref name="UDC Structure"&gt;[http://www.udcc.org/index.php/site/page?view=about_structure UDC Structure and Tables], UDC Consortium website&lt;/ref&gt;

*'''Common auxiliary tables''' (including certain auxiliary signs). These tables contain facets of concepts representing, general recurrent characteristics, applicable over a range of subjects throughout the main tables, including notions such as place, language of the text and physical form of the document, which may occur in almost any subject. UDC numbers from these tables, called common auxiliaries are simply added at the end of the number for the subject taken from the main tables. There are over 15,000 of common auxiliaries in UDC.
*'''The main tables or main schedules''' containing the various disciplines and branches of knowledge, arranged in 9 main classes,  numbered from 0 to 9 (with class 4 being vacant). At the beginning of each class there are also series of special auxiliaries, which express aspects that are recurrent within this specific class. Main tables in UDC contain more than 60,000 subdivisions.

==== Main classes ====
*0 [[Science]] and [[Knowledge]]. [[Organization]]. [[Computer Science]]. [[Information Science]]. [[Documentation]]. [[Librarianship]]. [[Institutions]]. [[Publications]]
*1 [[Outline of philosophy|Philosophy]]. [[Outline of psychology|Psychology]]
*2 [[Outline of religion|Religion]]. [[Outline of theology|Theology]]
*3 [[Outline of social science|Social Sciences]]
*4 ''vacant''
*5 [[Outline of mathematics|Mathematics]]. [[Outline of natural science|Natural Sciences]]
*6 [[Outline of applied science|Applied Sciences]]. [[Outline of medicine|Medicine]], [[Outline of technology|Technology]]
*7 [[The arts|The Arts]]. [[Outline of entertainment|Entertainment]]. [[Outline of sports|Sport]]
*8 [[Outline of linguistics|Linguistics]]. [[Outline of literature|Literature]]
*9 [[Outline of geography|Geography]]. [[Outline of history|History]]

The vacant class 4 is the result of a planned schedule expansion. This class was freed by moving linguistics into class 8 in the 1960s to make space for future developments in the rapidly expanding fields of knowledge; primarily natural sciences and technology.

==== Common auxiliary tables ====
''Common auxiliaries'' are aspect-free concepts that can be used in combination with any other UDC code from the main classes or with other common auxiliaries. They have unique notational representations that makes them stand out in complex expressions. Common auxiliary numbers always begin with a certain symbol known as a facet indicator, e.g. &lt;nowiki&gt;=&lt;/nowiki&gt; (equal sign) always introduces concepts representing the language of a document; (0...) numbers enclosed in parentheses starting with zero always represent a concept designating document form. Thus (075) Textbook and =111 English can be combined to express, e.g.(075)=111 Textbooks in English, and when combined with numbers from the main UDC tables they can be used as follows: 2(075)=111 Religion textbooks in English, 51(075)=111 Mathematics textbooks in English etc.

*=...	Common auxiliaries of language. Table 1c
*(0...)	Common auxiliaries of form. Table 1d
*(1/9)	Common auxiliaries of place. Table 1e
*(=...)	Common auxiliaries of human ancestry, ethnic grouping and nationality. Table 1f
*"..."	Common auxiliaries of time. Table 1g helps to make minute division of time e.g.:  "1993-1996''
*-0...	Common auxiliaries of general characteristics: Properties, Materials, Relations/Processes and Persons. Table 1k.
*-02	Common auxiliaries of properties. Table 1k
*-03	Common auxiliaries of materials. Table 1k
*-04	Common auxiliaries of relations, processes and operations. Table 1k
*-05	Common auxiliaries of persons and personal characteristics. Table 1k this table is repeated

==== Connecting signs ====
In order to preserve the precise meaning and enable accurate parsing of complex UDC expressions, a number of connecting symbols are made available to relate and extend UDC numbers. These are:
{| class="wikitable"
!Symbol !! Symbol name !! Meaning !! Example
|-
|&lt;nowiki&gt;+&lt;/nowiki&gt; || [[Plus and minus signs|plus]] || coordination, addition || e.g. 59+636 [[zoology]] and [[animal breeding]]
|-
|&lt;nowiki&gt;/&lt;/nowiki&gt; || [[Slash (punctuation)|stroke]] || consecutive extension || e.g. 592/599 Systematic zoology (everything from 592 to 599 inclusive)
|-
|&lt;nowiki&gt;:&lt;/nowiki&gt; || [[Colon (punctuation)|colon]] || relation || e.g. 17:7  Relation of [[ethics]] to [[art]]
|-
|&lt;nowiki&gt;[ ]&lt;/nowiki&gt; || square [[bracket]]s || subgrouping || e.g. 311:[622+669](485) [[statistics]] of [[mining]] and [[metallurgy]] in [[Sweden]] (the auxiliary qualifiers 622+669 considered as a unit)
|-
|&lt;nowiki&gt;*&lt;/nowiki&gt; || asterisk || Introduces non-UDC notation  || e.g. 523.4*433 Planetology, minor planet Eros (IAU authorized number after the asterisk)
|-
|&lt;nowiki&gt;A/Z&lt;/nowiki&gt; || alphabetical extension || Direct alphabetical specification  || e.g. 821.133.1MOL French literature, works of Molière
|}

== UDC outline ==

&lt;small&gt;UDC classes in this outline are taken from the Multilingual Universal Decimal Classification Summary (UDCC Publication No. 088) released by the UDC Consortium under the Creative Commons Attribution Share Alike 3.0 license (first release 2009, subsequent update 2012).&lt;ref name="UDCS" /&gt;&lt;/small&gt;

=== Main tables ===

====0 [[Outline of science|Science]] and [[Outline of knowledge|knowledge]]. Organization. [[Outline of computer science|Computer science]]. Information. Documentation. Librarianship. Institution. Publications====

  00          Prolegomena. Fundamentals of knowledge and culture. Propaedeutics
  001         [[Outline of science|Science]] and [[Outline of knowledge|knowledge]] in general. Organization of intellectual work
  002         Documentation. Books. Writings. Authorship
  003         Writing systems and scripts
  004         [[Outline of computer science|Computer science]] and technology. Computing
  004.2       Computer architecture
  004.3       Computer hardware
  004.4       [[Software]]
  004.5       Human-computer interaction
  004.6       Data
  004.7       Computer communication
  004.8       [[Outline of artificial intelligence|Artificial intelligence]]
  004.9       Application-oriented computer-based techniques
  005         [[Outline of business management|Management]]
  005.1       Management Theory
  005.2       Management agents. Mechanisms. Measures
  005.3       Management activities
  005.5       Management operations. Direction
  005.6       Quality management. Total quality management (TQM)
  005.7       Organizational management (OM)
  005.9       Fields of management
  005.92      Records management
  005.93      Plant management. Physical resources management
  005.94      Knowledge management
  005.95/.96  Personnel management. Human Resources management
  006         Standardization of products, operations, weights, measures and time
  007         Activity and organizing. Information. Communication and control theory generally (cybernetics)
  008         Civilization. [[Outline of culture|Culture]]. Progress   
  01          Bibliography and bibliographies. Catalogues
  02          Librarianship
  030         General reference works (as subject)
  050         Serial publications, periodicals (as subject)
  06          Organizations of a general nature
  069         Museums
  070         Newspapers (as subject). The Press. Outline of [[journalism]]
  08          Polygraphies. Collective works (as subject)
  09          Manuscripts. Rare and remarkable works (as subject)

====1 [[Outline of philosophy|Philosophy]]. [[Outline of psychology|Psychology]]====
  101        Nature and role of philosophy
  11         Metaphysics
  111        General metaphysics. Ontology
  122/129    Special Metaphysics
  13         Philosophy of mind and spirit. Metaphysics of spiritual life
  14         Philosophical systems and points of view
  159.9      [[Outline of psychology|Psychology]]
  159.91     Psychophysiology (physiological psychology). Mental physiology
  159.92     Mental development and capacity. Comparative psychology
  159.93     Sensation. Sensory perception
  159.94     Executive functions
  159.95     Higher mental processes
  159.96     Special mental states and processes
  159.97     Abnormal psychology
  159.98     Applied psychology (psychotechnology) in general
  16         [[Outline of logic|Logic]]. [[Outline of epistemology|Epistemology]]. Theory of knowledge. Methodology of logic
  17         Moral philosophy. [[Outline of ethics|Ethics]]. Practical philosophy

====2 [[Outline of religion|Religion]]. [[Outline of theology|Theology]]====

&lt;small&gt;The UDC tables for religion are fully faceted. Indicated in italics below, are special auxiliary numbers that can be used to express attributes (facets) of any specific faith. Any special number can be combined with any religion e.g.  ''-5 Worship'' can be used to express e.g. ''26-5 Worship in Judaism'', ''27-5 Worship in Christianity'', ''24-5 Worship in Buddhism''. The complete special auxiliary tables contain around 2000 subdivisions of various attributes that can be attached to express various aspects of individual faiths to a great level of specificity allowing equal level of detail for every religion.&lt;/small&gt;
  ''2-1/-9	Special auxiliary subdivision for religion''
  ''2-1	Theory and philosophy of religion. Nature of religion. Phenomenon of religion''
  ''2-2	Evidences of religion''
  ''2-3	Persons in religion''
  ''2-4	Religious activities. Religious practice''
  ''2-5	Worship broadly. Cult. Rites and ceremonies''
  ''2-6	Processes in religion''
  ''2-7	Religious organization and administration''
  ''2-8	Religions characterised by various properties''
  ''2-9	History of the faith, religion, denomination or church''
  21/29	Religious systems. Religions and faiths
  21	Prehistoric and primitive religions
  22	Religions originating in the Far East
  23	Religions originating in Indian sub-continent. Hindu religion in the broad sense
  24	[[Outline of Buddhism|Buddhism]]
  25	Religions of antiquity. Minor cults and religions
  26	[[Outline of Judaism|Judaism]]
  27	[[Outline of Christianity|Christianity]]
  28	[[Outline of Islam|Islam]]
  29	Modern spiritual movements

====3 [[Outline of social science|Social sciences]]====
  303   Methods of the social sciences
  304	Social questions. Social practice. Cultural practice. Way of life (Lebensweise)
  305	Gender studies
  308	Sociography. Descriptive studies of society (both qualitative and quantitative)
  311	[[Outline of statistics|Statistics]] as a science. Statistical theory
  314/316 [[Outline of society|Society]]
  314	Demography. Population studies
  316	[[Outline of sociology|Sociology]]
  32	[[Outline of politics|Politics]]
  33	[[Outline of economics|Economics]]. Economic science
  34	[[Outline of law|Law]]. Jurisprudence
  35	Public administration. Government. Military affairs
  36	Safeguarding the mental and material necessities of life
  37	[[Outline of education|Education]]
  39	Cultural anthropology. Ethnography. Customs. Manners. Traditions. Way of life

====4 Vacant====

This section is currently vacant.

====5 [[Outline of mathematics|Mathematics]]. [[Outline of natural science|Natural sciences]]====
  502/504  Environmental science. Conservation of natural resources. Threats to the environment and protection against them
  502	The environment and its protection
  504	Threats to the environment
  51	[[Outline of mathematics|Mathematics]]
  510	Fundamental and general considerations of mathematics
  511	Number theory
  512	[[Outline of algebra|Algebra]]
  514	[[Outline of geometry|Geometry]]
  517	Analysis
  519.1	Combinatorial analysis. Graph theory
  519.2	[[Outline of probability|Probability]]. Mathematical statistics
  519.6	Computational mathematics. [[Outline of numerical analysis|Numerical analysis]]
  519.7	Mathematical cybernetics
  519.8	Operational research (OR): mathematical theories and methods
  52	[[Outline of astronomy|Astronomy]]. Astrophysics. [[Outline of space exploration|Space research]]. Geodesy
  53	[[Outline of physics|Physics]]
  531/534  Mechanics
  535	Optics
  536	Heat. Thermodynamics. Statistical physics
  537	Electricity. Magnetism. Electromagnetism
  538.9	Condensed matter physics. Solid state physics
  539	Physical nature of matter
  54	[[Outline of chemistry|Chemistry]]. Crystallography. Mineralogy
  542	Practical laboratory chemistry. Preparative and experimental chemistry
  543	Analytical chemistry
  544	Physical chemistry
  546	Inorganic chemistry
  547	[[Outline of organic chemistry|Organic chemistry]]
  548/549 Mineralogical sciences. Crystallography. Mineralogy
  55	[[Outline of earth science|Earth sciences]]. Geological sciences
  56	Paleontology
  57	Biological sciences in general
  58	[[Outline of botany|Botany]]
  59	[[Outline of zoology|Zoology]]

====6 [[Outline of applied science|Applied sciences]]. [[Outline of medicine|Medicine]]. [[Outline of technology|Technology]]====

&lt;small&gt;Class 6 occupies the largest proportion of UDC schedules. It contains over 44,000 subdivisions. Each specific field of technology or industry usually contains more than one special auxiliary table with concepts needed to express operations, processes, materials and products. As a result, UDC codes are often created through the combination of various attributes. Equally, some parts of this class enumerate concepts to a great level of detail e.g.  ''621.882.212 Hexagon screws with additional shapes. Including: Flank screws. Collar screws. Cap screws''
&lt;/small&gt;

  60    [[Outline of biotechnology|Biotechnology]]
  61	Medical sciences
  611/612 Human biology
  613	Hygiene generally. Personal health and hygiene
  614	Public health and hygiene. Accident prevention
  615	Pharmacology. Therapeutics. Toxicology
  616	Pathology. Clinical medicine
  617	Surgery. Orthopaedics. Ophthalmology
  618	Gynaecology. Obstetrics
  62	[[Outline of engineering|Engineering]]. [[Outline of technology|Technology]] in general
  620	Materials testing. Commercial materials. Power stations. Economics of energy
  621	Mechanical engineering in general. Nuclear technology. Electrical engineering. Machinery
  622	[[Outline of mining|Mining]]
  623	Military engineering
  624	Civil and structural engineering in general
  625	Civil engineering of land transport. Railway engineering. Highway engineering
  626/627  Hydraulic engineering and construction. Water (aquatic) structures
  629	Transport vehicle engineering
  63	[[Outline of agriculture|Agriculture]] and related sciences and techniques. Forestry. Farming. Wildlife exploitation
  630	Forestry
  631/635	Farm management. Agronomy. Horticulture
  633/635	Horticulture in general. Specific crops
  636	Animal husbandry and breeding in general. Livestock rearing. Breeding of domestic animals
  64	Home economics. Domestic science. Housekeeping
  65	Communication and transport industries. Accountancy. Business management. Public relations
  654	Telecommunication and telecontrol (organization, services)
  655	Graphic industries. Printing. Publishing. Book trade
  656	Transport and postal services. Traffic organization and control
  657	Accountancy
  658	[[Outline of business management|Business management]], administration. Commercial organization
  659	Publicity. Information work. [[Outline of public relations|Public relations]]
  66	Chemical technology. Chemical and related industries
  67	Various industries, trades and crafts
  68	Industries, crafts and trades for finished or assembled articles
  69	Building ([[Outline of construction|construction]]) trade. Building materials. Building practice and procedure

====7 The arts. Recreation. [[Outline of entertainment|Entertainment]]. [[Outline of sports|Sport]]====
  ''7.01/.09	Special auxiliary subdivision for the arts''
  ''7.01	Theory and philosophy of art. Principles of design, proportion, optical effect''
  ''7.02	Art technique. Craftsmanship''
  ''7.03	Artistic periods and phases. Schools, styles, influences''
  ''7.04	Subjects for artistic representation. Iconography. Iconology''
  ''7.05	Applications of art (in industry, trade, the home, everyday life)''
  ''7.06	Various questions concerning art''
  ''7.07	Occupations and activities associated with the arts and entertainment''
  ''7.08	Characteristic features, forms, combinations etc. (in art, entertainment and sport)''
  ''7.091	Performance, presentation (in original medium)''
  71	Physical planning. Regional, town and country planning. Landscapes, parks, gardens
  72	[[Outline of architecture|Architecture]]
  73	Plastic arts
  74	[[Outline of drawing and drawings|Drawing]]. [[Outline of design|Design]]. [[Outline of crafts|Applied arts and crafts]]
  745/749	Industrial and domestic arts and crafts. Applied arts
  75	[[Outline of painting|Painting]]
  76	Graphic art, printmaking. Graphics
  77	[[Outline of photography|Photography]] and similar processes
  78	[[Outline of music|Music]]
  79	Recreation. [[Outline of entertainment|Entertainment]]. [[Outline of games|Games]]. [[Outline of sports|Sport]]
  791	Cinema. [[Outline of film|Films]] (motion pictures)
  792	[[Outline of theatre|Theatre]]. [[Outline of stagecraft|Stagecraft]]. Dramatic performances
  793	Social entertainments and recreations. Art of movement. [[Outline of dance|Dance]]
  794	Board and table games (of thought, skill and chance)
  796	[[Outline of sports|Sport]]. [[Outline of games|Games]]. [[Outline of exercise|Physical exercises]]
  797	Water sports. Aerial sports
  798	Riding and driving. Horse and other animal sports
  799	Sport fishing. Sport hunting. Shooting and target sports

====8 Language. [[Outline of linguistics|Linguistics]]. [[Outline of literature|Literature]]====

&lt;small&gt;Tables for class 8 are fully faceted and details are expressed through combination with common auxiliaries of language (Table 1c) and a series of special auxiliary tables to indicate other facets or attributes in Linguistics or Literature. As a result, this class allows for great specificity in indexing although the schedules themselves occupy very little space in UDC. The subdivisions of e.g. ''811 Languages'' or ''821 Literature'' are derived from common auxiliaries of language =1/=9 (Table 1c) by substituting a point for the equals sign, e.g. 811.111 English language (as a subject of a linguistic study) and ''821.111 English literature'' derives from ''=111 English language''. Common auxiliaries of place and time are also frequently used in this class to express place and time facets of Linguistics or Literature, e.g. ''821.111(71)"18" English literature of Canada in 19th century''
&lt;/small&gt;
  80	General questions relating to both linguistics and literature. Philology
  801	Prosody. Auxiliary sciences and sources of philology
  808	Rhetoric. The effective use of language  
  '''81	[[Outline of linguistics|Linguistics]] and languages'''
  ''81`1/`4	Special auxiliary subdivision for subject fields and facets of linguistics and languages''
 '' 81`1	General linguistics''
  ''81`2	[[Outline of semiotics|Theory of signs]]. Theory of translation. Standardization. Usage. Geographical linguistics''
  ''81`3	Mathematical and applied linguistics. Phonetics. Graphemics. Grammar. Semantics. Stylistics''
  ''81`4	Text linguistics, Discourse analysis. Typological linguistics''
  ''81`42	Text linguistics. Discourse analysis''
  ''81`44	Typological linguistics''
  811	Languages
        &lt;small&gt;Derived from the common auxiliaries of language =1/=9 (Table 1c) by replacing the equal sign = with prefix ''811.'' e.g. ''=111 English'' becomes ''811.111 Linguistics of English language''&lt;/small&gt;
  811.1/.9	All languages natural or artificial
  811.1/.8	Individual natural languages
  811.1/.2	Indo-European languages
  811.21/.22	Indo-Iranian languages
  811.3	Dead languages of unknown affiliation. Caucasian languages
  811.4	Afro-Asiatic, Nilo-Saharan, Congo-Kordofanian, Khoisan languages
  811.5	Ural-Altaic, Palaeo-Siberian, Eskimo-Aleut, Dravidian and Sino-Tibetan languages. Japanese. Korean. Ainu
  811.6	Austro-Asiatic languages. Austronesian languages
  811.7	Indo-Pacific (non-Austronesian) languages. Australian languages
  811.8	American indigenous languages
  811.9	Artificial languages
  '''82	[[Outline of literature|Literature]]'''
  ''82-1/-9	Special auxiliary subdivision for literary forms, genres''
  ''82-1	[[Outline of poetry|Poetry]]. Poems. Verse''
  ''82-2	Drama. Plays''
  ''82-3	[[Outline of fiction|Fiction]]. Prose narrative''
  ''82-31	Novels. Full-length stories''
  ''82-32	Short stories. Novellas''
  ''82-4	Essays''
  ''82-5	Oratory. Speeches''
  ''82-6	Letters. Art of letter-writing. Correspondence. Genuine letters''
  ''82-7	Prose satire. Humour, epigram, parody''
  ''82-8	Miscellanea. Polygraphies. Selections''
  ''82-9	Various other literary forms''
  ''82-92	Periodical literature. Writings in serials, journals, reviews''
  ''82-94	History as literary genre. Historical writing. Historiography. Chronicles. Annals. Memoirs''
  ''82.02/.09	Special auxiliary subdivision for theory, study and technique of literature''
  ''82.02	Literary schools, trends and movements''
  ''82.09	Literary criticism. Literary studies''
  ''82.091	Comparative literary studies. Comparative literature''
  821	Literatures of individual languages and language families
        &lt;small&gt;Derived from the common auxiliaries of language =1/=9 (Table 1c) by replacing the equal sign = with prefix ''821.'' e.g. ''=111 English'' becomes ''821.111 English literature''&lt;/small&gt;

====9 [[Outline of geography|Geography]]. Biography. [[Outline of history|History]]====

&lt;small&gt;Tables for Geography and History in UDC are fully faceted and place, time and ethnic grouping facets are expressed through combination with common auxiliaries of place (Table 1d), ethnic grouping (Table 1f) and time (Table 1g)
&lt;/small&gt;

  902/908	Archaeology. Prehistory. Cultural remains. Area studies
  902	[[Outline of archaeology|Archaeology]]
  903	Prehistory. Prehistoric remains, artefacts, antiquities
  904	Cultural remains of historical times
  908	Area studies. Study of a locality
  91	[[Outline of geography|Geography]]. Exploration of the Earth and of individual countries. Travel. [[Outline of geography#Regional geography|Regional geography]]
  910	General questions. Geography as a science. Exploration. Travel
  911	General geography. Science of geographical factors (systematic geography). Theoretical geography
  911.2	[[Outline of geography#Physical geography|Physical geography]]
  911.3	[[Outline of geography#Human geography|Human geography]] (cultural geography). Geography of cultural factors
  911.5/.9	Theoretical geography
  912	Nonliterary, nontextual representations of a region
  913	[[Outline of geography#Regional geography|Regional geography]]
  92	Biographical studies. Genealogy. Heraldry. Flags
  929	Biographical studies
  929.5	Genealogy
  929.6	Heraldry
  929.7	Nobility. Titles. Peerage
  929.9	Flags. Standards. Banners
  93/94	[[Outline of history|History]]
  930	Science of history. Historiography
  930.1	History as a science
  930.2	Methodology of history. Ancillary historical sciences
  930.25	Archivistics. Archives (including public and other records)
  930.85	History of civilization. Cultural history
  94	General

=== Common auxiliary tables ===

====Common auxiliaries of language. Table 1c====
  =1/=9	Languages (natural and artificial)
  =1/=8	Natural languages
  =1/=2	Indo-European languages
  =1	Indo-European languages of Europe
  =11	Germanic languages
  =12	Italic languages
  =13	Romance languages
  =14	Greek (Hellenic)
  =15	Celtic languages
  =16	Slavic languages
  =17	Baltic languages
  =18	Albanian
  =19	Armenian
  =2	Indo-Iranian, Nuristani (Kafiri) and dead Indo-European languages
  =21/=22	Indo-Iranian languages
  =21	Indic languages
  =22	Iranian languages
  =29	Dead Indo-European languages (not listed elsewhere)
  =3	Dead languages of unknown affiliation. Caucasian languages
  =34	Dead languages of unknown affiliation, spoken in the Mediterranean and Near East (except Semitic)
  =35	Caucasian languages
  =4	Afro-Asiatic, Nilo-Saharan, Congo-Kordofanian, Khoisan languages
  =41	Afro-Asiatic (Hamito-Semitic) languages
  =42	Nilo-Saharan languages
  =43	Congo-Kordofanian (Niger-Kordofanian) languages
  =45	Khoisan languages
  =5	Ural-Altaic, Palaeo-Siberian, Eskimo-Aleut, Dravidian and Sino-Tibetan languages. Japanese. Korean. Ainu
  =51	Ural-Altaic languages
  =521	Japanese
  =531	Korean
  =541	Ainu
  =55	Palaeo-Siberian languages
  =56	Eskimo-Aleut languages
  =58	Sino-Tibetan languages
  =6	Austro-Asiatic languages. Austronesian languages
  =61	Austro-Asiatic languages
  =62	Austronesian languages
  =7	Indo-Pacific (non-Austronesian) languages. Australian languages
  =71	Indo-Pacific (non-Austronesian) languages
  =72	Australian languages
  =8	American indigenous languages
  =81	Indigenous languages of Canada, USA and Northern-Central Mexico
  =82	Indigenous languages of western North American Coast, Mexico and Yucatán
  =84/=88	Central and South American indigenous languages
  =84	Ge-Pano-Carib languages. Macro-Chibchan languages
  =85	Andean languages. Equatorial languages
  =86	Chaco languages. Patagonian and Fuegian languages
  =88	Isolated, unclassified Central and South American indigenous languages
  =9	Artificial languages
  =92	Artificial languages for use among human beings. International auxiliary languages (interlanguages)
  =93	Artificial languages used to instruct machines. Programming languages. Computer languages

====(0...) Common auxiliaries of form. Table 1d====
  ''(0.02/.08)	Special auxiliary subdivision for document form''
  ''(0.02)	Documents according to physical, external form''
  ''(0.03)	Documents according to method of production''
  ''(0.032)	Handwritten documents (autograph, holograph copies). Manuscripts. Pictorial documents (drawings, paintings)''
  ''(0.034)	Machine-readable documents''
  ''(0.04)	Documents according to stage of production''
  ''(0.05)	Documents for particular kinds of user''
  ''(0.06)	Documents according to level of presentation and availability''
  ''(0.07)	Supplementary matter issued with a document''
  ''(0.08)	Separately issued supplements or parts of documents''
  (01)	Bibliographies
  (02)	Books in general
  (03)	Reference works
  (04)	Non-serial separates. Separata
  (041)	Pamphlets. Brochures
  (042)	Addresses. Lectures. Speeches
  (043)	Theses. Dissertations
  (044)	Personal documents. Correspondence. Letters. Circulars
  (045)	Articles in serials, collections etc. Contributions
  (046)	Newspaper articles
  (047)	Reports. Notices. Bulletins
  (048)	Bibliographic descriptions. Abstracts. Summaries. Surveys
  (049)	Other non-serial separates
  (05)	Serial publications. Periodicals
  (06)	Documents relating to societies, associations, organizations
  (07)	Documents for instruction, teaching, study, training
  (08)	Collected and polygraphic works. Forms. Lists. Illustrations. Business publications
  (09)	Presentation in historical form. Legal and historical sources
  (091)	Presentation in chronological, historical form. Historical presentation in the strict sense
  (092)	Biographical presentation
  (093)	Historical sources
  (094)	Legal sources. Legal documents

====(1/9) Common auxiliaries of place. Table 1e====
  (1)	Place and space in general. Localization. Orientation
  ''(1-0/-9)	Special auxiliary subdivision for boundaries and spatial forms of various kinds''
  ''(1-0)	Zones''
  ''(1-1)	Orientation. Points of the compass. Relative position''
  ''(1-11)	East. Eastern''
  ''(1-13)	South. Southern''
  ''(1-14)	South-west. South-western''
  ''(1-15)	West. Western''
  ''(1-17)	North. Northern''
  ''(1-19)	Relative location, direction and orientation''
  ''(1-2)	Lowest administrative units. Localities''
  ''(1-5)	Dependent or semi-dependent territories''
  ''(1-6)	States or groupings of states from various points of view''
  ''(1-7)	Places and areas according to privacy, publicness and other special features''
  ''(1-8)	Location. Source. Transit. Destination''
  ''(1-9)	Regionalization according to specialized points of view''
  (100)	Universal as to place. International. All countries in general
  (2)	Physiographic designation
  (20)	Ecosphere
  (21)	Surface of the Earth in general. Land areas in particular. Natural zones and regions
  (23)	Above sea level. Surface relief. Above ground generally. Mountains
  (24)	Below sea level. Underground. Subterranean
  (25)	Natural flat ground (at, above or below sea level). The ground in its natural condition, cultivated or inhabited
  (26)	Oceans, seas and interconnections
  (28)	Inland waters
  (29)	The world according to physiographic features
  (3)	Places of the ancient and mediaeval world
  (31)	Ancient China and Japan
  (32)	[[Outline of ancient Egypt|Ancient Egypt]]
  (33)	Ancient Roman Province of Judaea. The Holy Land. Region of the Israelites
  (34)	[[Outline of ancient India|Ancient India]]
  (35)	Medo-Persia
  (36)	Regions of the so-called barbarians
  (37)	Italia. [[Outline of ancient Rome|Ancient Rome]] and Italy
  (38)	[[Outline of ancient Greece|Ancient Greece]]
  (399)	Other regions. Ancient geographical divisions other than those of classical antiquity
  (4/9)	Countries and places of the modern world
  (4)	[[Outline of Europe|Europe]]
  (5)	[[Outline of Asia|Asia]]
  (6)	[[Outline of Africa|Africa]]
  (7)	[[Outline of North America|North]] and Central America
  (8)	[[Outline of South America|South America]]
  (9)	States and regions of the South Pacific and [[Outline of Australia|Australia]]. Arctic. Antarctic

====(=...) Common auxiliaries of human ancestry, ethnic grouping and nationality. Table 1f====

&lt;small&gt;''They are derived mainly from the common auxiliaries of language =... (Table 1c) and so may also usefully distinguish linguistic-cultural groups, e.g. =111 English is used to represent (=111) English speaking peoples''&lt;/small&gt;

  (=01)	Human ancestry groups
  (=011)	European Continental Ancestry Group
  (=012)	Asian Continental Ancestry Group
  (=013)	African Continental Ancestry Group
  (=014)	Oceanic Ancestry Group
  (=017)	American Native Continental Ancestry Group
  (=1/=8)	Linguistic-cultural groups, ethnic groups, peoples ['''derived from Table 1c''']
  (=1:1/9)	Peoples associated with particular places
                ''e.g. (=111:71) Anglophone population of Canada''

===="..." Common auxiliaries of time. Table 1g====
  "0/2"	Dates and ranges of time (CE or AD) in conventional Christian (Gregorian) reckoning
  "0"	First millennium CE
  "1"	Second millennium CE
  "2"	Third millennium CE
  "3/7"	Time divisions other than dates in Christian (Gregorian) reckoning
  "3"	Conventional time divisions and subdivisions: numbered, named, etc.
  "4"	Duration. Time-span. Period. Term. Ages and age-groups
  "5"	Periodicity. Frequency. Recurrence at specified intervals.
  "6"	Geological, archaeological and cultural time divisions
  "61/62" Geological time division
  "63"	Archaeological, prehistoric, protohistoric periods and ages
  "67/69" Time reckonings: universal, secular, non-Christian religious
  "67"	Universal time reckoning. Before Present
  "68"	Secular time reckonings other than universal and the Christian (Gregorian) calendar
  "69"	Dates and time units in non-Christian (non-Gregorian) religious time reckonings
  "7"	Phenomena in time. Phenomenology of time

====-0 Common auxiliaries of general characteristics. Table 1k====
  '''-02	Common auxiliaries of properties'''
  -021	Properties of existence
  -022	Properties of magnitude, degree, quantity, number, temporal values, dimension, size
  -023	Properties of shape
  -024	Properties of structure. Properties of position
  -025	Properties of arrangement
  -026	Properties of action and movement
  -027	Operational properties
  -028	Properties of style and presentation
  -029	Properties derived from other main classes
  '''-03 Common auxiliaries of materials'''
  -032	Naturally occurring mineral materials
  -033	Manufactured mineral-based materials
  -034	Metals
  -035	Materials of mainly organic origin
  -036	Macromolecular materials. Rubbers and plastics
  -037	Textiles. Fibres. Yarns. Fabrics. Cloth
  -039	Other materials
  '''-04 Common auxiliaries of relations, processes and operations'''
  -042	Phase relations
  -043	General processes
  -043.8/.9 Processes of existence
  -045	Processes related to position, arrangement, movement, physical properties, states of matter
  -047/-049	General operations and activities
  '''-05 Common auxiliaries of persons and personal characteristics'''
  -051	Persons as agents, doers, practitioners (studying, making, serving etc.)
  -052	Persons as targets, clients, users (studied, served etc.)
  -053	Persons according to age or age-groups
  -054	Persons according to ethnic characteristics, nationality, citizenship etc.
  -055	Persons according to gender and kinship
  -056	Persons according to constitution, health, disposition, hereditary or other traits
  -057	Persons according to occupation, work, livelihood, education
  -058	Persons according to social class, civil status

==See also==
'''Special classifications based on or used in combination with UDC'''
*[http://www.spri.cam.ac.uk/library/overview.html#classification Universal Decimal Classification for Use in Polar Libraries - Scott Polar Research Institute, Cambridge] 
*[[Lonclass|BBC LonClass]]
*[http://iufro.forintek.ca/GFDCDefault.aspx Global Forest Decimal Classification]

'''Other faceted classifications:'''
*[[Bliss bibliographic classification]]
*[[Colon classification]]
*[http://www.ucl.ac.uk/fatks/bso/ Broad System of Ordering]

'''Other general bibliographic classifications'''
*[[Dewey Decimal Classification]]
*[[Library of Congress Classification]]
* Russian Library-Bibliographical Classification (BBK) 
*[[Chinese Library Classification]]
*[[Harvard-Yenching Classification]]

==References==
{{reflist}}

==External links==
{{wikidata property|P1190}}
*[http://www.udcc.org/ Universal Decimal Classification Consortium]
**[http://www.udcc.org/about.htm About Universal Decimal Classification]
**[http://www.udcc.org/udcsummary/php/index.php Multilingual UDC Summary]
**[http://udcdata.info/ UDC Linked Data]

{{Library classification systems}}

{{Authority control}}

[[Category:Classification systems]]
[[Category:Library cataloging and classification]]
[[Category:Controlled vocabularies]]
[[Category:Knowledge representation]]</text>
      <sha1>1etgs0x13semlmevf7qkdu553rpc3om</sha1>
    </revision>
  </page>
  <page>
    <title>Enactivism</title>
    <ns>0</ns>
    <id>7082881</id>
    <revision>
      <id>757941878</id>
      <parentid>757806128</parentid>
      <timestamp>2017-01-02T16:29:38Z</timestamp>
      <contributor>
        <username>Mrmatiko</username>
        <id>12110782</id>
      </contributor>
      <comment>/* References */ Fixed refs &amp; wrapped</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="54252" xml:space="preserve">'''Enactivism''' argues that [[cognition]] arises through a dynamic interaction between an acting [[organism]] and its environment.&lt;ref name="Evan Thompson"/&gt; It claims that our environment is one which we selectively create through our capacities to interact with the world.&lt;ref name=Rowlands/&gt; "Organisms do not passively receive information from their environments, which they then translate into internal representations. Natural cognitive systems...participate in the generation of meaning ...engaging in transformational and not merely informational interactions: ''they enact a world''."&lt;ref name=Jaegher1/&gt; These authors suggest that the increasing emphasis upon enactive terminology presages a new era in thinking about cognitive science.&lt;ref name=Jaegher1/&gt; How the actions involved in enactivism relate to age-old questions about [[free will]] remains a topic of active debate.&lt;ref name=Manetti/&gt;

The term 'enactivism' is close in meaning to 'enaction', defined as "the manner in which a subject of perception creatively matches its actions to the requirements of its situation".&lt;ref name=Tascano0/&gt; The introduction of the term ''enaction'' in this context is attributed to  [[Francisco Varela]], [[Evan Thompson]], and [[Eleanor Rosch]],&lt;ref name=Tascano0/&gt;&lt;ref name=RWilson/&gt; who proposed the name to "emphasize the growing conviction that cognition is not the representation of a pre-given world by a pre-given mind but is rather the enactment of a world and a mind on the basis of a history of the variety of actions that a being in the world performs".&lt;ref name=Varela/&gt; This was further developed by Thompson and others,&lt;ref name="Evan Thompson"/&gt; to place emphasis upon the idea that experience of the world is a result of mutual interaction between the sensorimotor capacities of the organism and its environment.&lt;ref name=RWilson/&gt;

The initial emphasis of enactivism upon sensorimotor skills has been criticized as "cognitively marginal",&lt;ref name=ClarkA/&gt; but it has been extended to apply to higher level cognitive activities, such as social interactions.&lt;ref name="Jaegher1"/&gt; "In the enactive view,... knowledge is constructed: it is constructed by an agent through its sensorimotor interactions with its environment, co-constructed between and within living species through their meaningful interaction with each other. In its most abstract form, knowledge is co-constructed between human individuals in socio-linguistic interactions...Science is a particular form of social knowledge construction...[that] allows us to perceive and predict events beyond our immediate cognitive grasp...and also to construct further, even more powerful scientific knowledge."&lt;ref name=Rohde/&gt;

Enactivism is closely related to [[situated cognition]] and [[embodied cognition]], and is presented as an alternative to [[cognitivism (psychology)|cognitivism]], [[computationalism]], and [[Cartesian dualism]].

==Philosophical aspects&lt;!--'Enaction (philosophy)' redirects here--&gt;==

Enactivism is one of  a cluster of related theories sometimes known as the ''4Es'', As described by [[Mark Rowlands]], mental processes are:&lt;ref name=Rowlands/&gt;
* '''Embodied''' involving more than the brain, including a more general involvement of bodily structures and processes.
* '''Embedded''' functioning only in a related external environment.
* '''Enacted''' involving not only neural processes, but also things an organism ''does''.
* '''Extended''' into the organism's environment.

Enactivism proposes an alternative to [[Dualism (philosophy of mind)|dualism]] as a philosophy of mind, in that it emphasises the interactions between mind, body and the environment, seeing them all as inseparably intertwined in mental processes.&lt;ref name=EThompson/&gt; The self arises as part of the process of an embodied entity interacting with the environment in precise ways determined by its physiology.   In this sense, individuals can be seen to "grow into" or arise from their interactive role with the world.&lt;ref name=Burman/&gt;
:"Enaction is the idea that organisms create their own experience through their actions. Organisms are not passive receivers of input from the environment, but are actors in the environment such that what they experience is shaped by how they act."&lt;ref name=Hutchins/&gt;

In ''The Tree of Knowledge'' Maturana &amp; Varela proposed the term ''enactive''&lt;ref name=Maturana/&gt; "to evoke the view of knowledge that what is known is brought forth, in contraposition to the more classical views of either cognitivism&lt;ref group=Note name=Cognitivism/&gt; or connectionism.&lt;ref group=Note name=Connectionism/&gt; They see enactivism as providing a middle ground between the two extremes of [[representationalism]] and [[solipsism]]. They seek to "confront the problem of understanding how our existence-the [[Praxis (process)|praxis]] of our living- is coupled to a surrounding world which appears filled with regularities that are at every instant the result of our biological and social histories.... to find a ''via media'': to understand the regularity of the world we are experiencing at every moment, but without any point of reference independent of ourselves that would give certainty to our descriptions and cognitive assertions.  Indeed the whole mechanism of generating ourselves, as describers and observers tells us that our world, as the world which we bring forth in our coexistence with others, will always have precisely that mixture of regularity and mutability, that combination of solidity and shifting sand, so typical of human experience when we look at it up close."[''Tree of Knowledge'', p.&amp;nbsp;241]

Enactivism also addresses the [[hard problem of consciousness]], referred to by Thompson as part of the ''[[explanatory gap]]'' in explaining how consciousness and subjective experience are related to brain and body.&lt;ref name=EThompson2/&gt;  "The problem with the dualistic concepts of consciousness and life in standard formulations of the hard problem is that they exclude each other by construction".&lt;ref name=EThompson3/&gt; Instead, according to Thompson's view of enactivism, the study of consciousness or [[Phenomenology (philosophy)|phenomenology]] as exemplified by [[Husserl]] and [[Merleau-Ponty]] is to complement science and its objectification of the world. "The whole universe of science is built upon the world as directly experienced, and if we want to subject science itself to rigorous scrutiny and arrive at a precise assessment of its meaning and scope, we must begin by reawakening the basic experience of the world of which science is the second-order expression" (Merleau-Ponty, ''The phenomenology of perception'' as quoted by Thompson, p.&amp;nbsp;165). In this interpretation, enactivism asserts that science is formed or enacted as part of humankind's interactivity with its world, and by embracing phenomenology "science itself is properly situated in relation to the rest of human life and is thereby secured on a sounder footing."&lt;ref name=EThompson4/&gt;&lt;ref name=Baldwin/&gt;

Enaction has been seen as a move to conjoin [[representationalism]] with [[phenomenalism]], that is, as adopting a [[constructivist epistemology]], an epistemology centered upon the active participation of the subject in constructing reality.&lt;ref name=Mutelesi/&gt;&lt;ref name=Chiari/&gt; However, 'constructivism' focuses upon more than a simple 'interactivity' that could be described as a minor adjustment to 'assimilate' reality or 'accommodate' to it.&lt;ref name=Glaserfeld/&gt; Constructivism looks upon interactivity as a radical, creative, revisionist process in which the knower ''constructs'' a personal 'knowledge system' based upon their experience and tested by its viability in practical encounters with their environment. Learning is a result of perceived anomalies that produce dissatisfaction with existing conceptions.&lt;ref name=Glasersfeld2/&gt;

How does constructivism relate to enactivism? From the above remarks it can be seen that [[Ernst von Glasersfeld|Glasersfeld]] expresses an interactivity between the knower and the known quite acceptable to an enactivist, but does not emphasize  the structured probing of the environment by the knower that leads to the "perturbation relative to some expected result" that then leads to a new understanding.&lt;ref name=Glasersfeld2/&gt; It is this probing activity, especially where it is not accidental but deliberate, that characterizes enaction, and invokes ''affect'',&lt;ref name=Ward2/&gt; that is, the motivation and planning that lead to doing and to fashioning the probing, both observing and modifying the environment, so that "perceptions and nature condition one another through generating one another."&lt;ref name=Diettrich/&gt; The questioning nature of this probing activity is not an emphasis of [[Jean Piaget|Piaget]] and Glasersfeld.

Sharing enactivism's stress upon both action and embodiment in the incorporation of knowledge, but giving Glasersfeld's mechanism of viability an [[Introduction to evolution|evolutionary]] emphasis,&lt;ref name=Diettrich2/&gt; is [[evolutionary epistemology]]. Inasmuch as an organism must reflect its environment well enough for the organism to be able to survive in it, and to be competitive enough to be able to reproduce at sustainable rate, the structure and reflexes of the organism itself embody knowledge of its environment. This biology-inspired theory of the growth of knowledge is closely tied to [[universal Darwinism]], and is associated with evolutionary epistemologists such as [[Karl Popper]], [[Donald T. Campbell]], [[Peter Munz]], and [[Gary Cziko]].&lt;ref name=Gontier/&gt; According to Munz, "an organism is an ''embodied theory'' about its environment... Embodied theories are also no longer expressed in language, but in anatomical structures or reflex responses, etc."&lt;ref name=Gontier/&gt;&lt;ref name=Munz/&gt;

==Psychological aspects==
McGann &amp; others&lt;ref name=McGann&gt;{{cite journal |author1=Marek McGann |author2=Hanne De Jaegher |author3=Ezequiel Di Paolo |year= 2013 |title=Enaction and psychology |journal=Review of General Psychology |volume=17 |issue=2 |pages=203–209 |url= http://www.academia.edu/4993021/Enaction_and_Psychology |doi= 10.1037/a0032935}}
&lt;/ref&gt; argue that enactivism attempts to mediate between the explanatory role of the coupling between cognitive agent and environment and the traditional emphasis on brain mechanisms found in neuroscience and psychology.  In the interactive approach to social cognition developed by De Jaegher &amp;  others,&lt;ref name=Gallagher0&gt;{{cite journal |author=Shaun Gallagher |year=2001 |title=The practice of mind |journal=Journal of Consciousness Studies |volume=8 |issue=5–7 |pages=83–107 |url=http://www.ummoss.org/Gallagher01.pdf}}
&lt;/ref&gt;&lt;ref name=Gallager1&gt;
{{cite book |author=Shaun Gallagher |isbn=978-0199204168 |edition=Paperback |year=2006 |title=How the Body Shapes the Mind |publisher=Oxford University Press |url=https://books.google.com/books/about/How_the_Body_Shapes_the_Mind.html?id=zhv5F-GYm98C}}
&lt;/ref&gt;&lt;ref name=Ratcliffe&gt;
{{cite book |author=Matthew Ratcliffe |year=2008 |title=Rethinking Commonsense Psychology: A Critique of Folk Psychology, Theory of Mind and Simulation |publisher=Palgrave Macmillan |isbn=978-0230221208 |url=https://books.google.com/books/about/Rethinking_Commonsense_Psychology.html?id=-JNyQgAACAAJ}}
&lt;/ref&gt; the dynamics of interactive processes are seen to play significant roles in coordinating interpersonal understanding, processes that in part include what they call [[#Participatory sense-making|''participatory sense-making'']].&lt;ref name=DeJaeger0&gt;
{{cite journal |author1=Hanne De Jaegher |author2=Ezequiel Di Paolo |url=http://www.enactionschool.com/resources/papers/DeJaegherDiPaolo2007.pdf |year=2007 |title=Participatory Sense-Making: An enactive approach to social cognition |journal=Phenomenology and the Cognitive Sciences |volume=6 |issue=4 |pages=485–507 |doi=10.1007/s11097-007-9076-9}}
&lt;/ref&gt;&lt;ref name=DeJaegher1&gt;
{{cite journal |author1=Hanne De Jaegher |author2=Ezequiel Di Paolo |author3=Shaun Gallagher |year=2010 |title=Can social interaction constitute social cognition? |journal=Trends in Cognitive Sciences |volume=14 |issue=10 |pages=441–447 |url=http://ezequieldipaolo.files.wordpress.com/2011/10/dejaegher_dipaolo_gallagher_tics_2010.pdf |doi=10.1016/j.tics.2010.06.009 |pmid=20674467}}
&lt;/ref&gt; Recent developments of enactivism in the area of social neuroscience involve the proposal of ''The Interactive Brain Hypothesis''&lt;ref name=DiPaolo3&gt;
{{cite journal |url=http://journal.frontiersin.org/Journal/10.3389/fnhum.2012.00163/full |author1=Ezequiel Di Paolo |author2=Hanne De Jaegher |date=June 2012 |title= The Interactive Brain Hypothesis |journal=Frontiers in Human Neuroscience |volume=7 |issue=6 |doi=10.3389/fnhum.2012.00163}}&lt;/ref&gt; where social cognition brain mechanisms, even those used in non-interactive situations, are proposed to have interactive origins.

===Enactive views of perception===
In the enactive view, perception "is not conceived as the transmission of information but more as an exploration of the world by various means. Cognition is not tied into the workings of an 'inner mind', some cognitive core, but occurs in directed interaction between the body and the world it inhabits."&lt;ref name=McGann2&gt;
{{cite book |title=Consciousness &amp; Emotion: Agency, conscious choice, and selective perception |page=184 |author1=Marek McGann |author2=Steve Torrance |chapter=Doing It and Meaning It: And the relation between the two  |isbn=9789027294616 |publisher=John Benjamins Publishing |year=2005 |url=https://books.google.com/books?id=LZk6AAAAQBAJ&amp;pg=PA184 |editor1=Ralph D. Ellis |editor2=Natika Newton }}
&lt;/ref&gt;

[[Alva Noë]] in advocating an enactive view of perception&lt;ref name=Noe&gt;
{{cite book |author=Alva Noë |title= Action in Perception |url=https://books.google.com/books?id=kFKvU2hPhxEC&amp;pg=PA1 |pages=1 ''ff'' |chapter=Chapter 1: The enactive approach to perception: An introduction |isbn=9780262140881 |year=2004 |publisher=MIT Press}}
&lt;/ref&gt; sought to resolve how we perceive three-dimensional objects, on the basis of two-dimensional input.  He argues that we perceive this solidity (or 'volumetricity') by appealing to patterns of sensorimotor expectations. These arise from our agent-active  'movements and interaction' with objects, or 'object-active' changes in the object itself. The solidity is perceived through our expectations and skills in knowing how the object's appearance would change with changes in how we relate to it. He saw all perception as an active exploration of the world, rather than being a passive process, something which happens to us.

Noë's idea of the role of 'expectations' in three-dimensional perception has been opposed by several philosophers, notably by [[Andy Clark]].&lt;ref name=ClarkA1/&gt; Clark points to difficulties of the enactive approach.  He points to internal processing of visual signals, for example, in the ventral and dorsal pathways, [[Two-streams hypothesis|the two-streams hypothesis]]. This results in an integrated perception of objects (their recognition and location, respectively) yet this processing cannot be described as an action or actions. In a more general criticism, Clark suggests that perception is not a matter of expectations about sensorimotor mechanisms guiding perception. Rather, although the limitations of sensorimotor mechanisms constrain perception, this sensorimotor activity is drastically filtered to fit current needs and purposes of the organism, and it is these imposed 'expectations' that govern perception, filtering for the 'relevant' details of sensorimotor input (called "sensorimotor summarizing").&lt;ref name=ClarkA1/&gt;

Another application of enaction to perception is analysis of the human hand. The many remarkably demanding uses of the hand are not learned by instruction, but through a history of engagements that lead to the acquisition of skills. According to one interpretation, it is suggested that "the hand [is]...an organ of cognition", not a faithful subordinate working under top-down instruction, but a partner in a "bi-directional interplay between manual and brain activity."&lt;ref name=Hutto&gt;
{{cite book |title= Radicalizing Enactivism: Minds without content |author=[[Daniel D Hutto]], Erik Myin |pages=46 ''ff'' |chapter=A helping hand |url=https://books.google.com/books?id=pAj-96LlBuMC&amp;pg=PA46 |isbn= 9780262018548 |year=2013 |publisher=MIT Press}}
&lt;/ref&gt; According to [[Daniel Hutto]]: "Enactivists are concerned to defend the view that our most elementary ways of engaging with the world and others - including our basic forms of perception and perceptual experience - are mindful in the sense of being phenomenally charged and intentionally directed, despite being non-representational and content-free."&lt;ref name=Hutto2&gt;
{{cite book |title= Radicalizing Enactivism: Minds without content |author1=Daniel D Hutto |author2=Erik Myin |pages=12–13  |chapter=Chapter 1: Enactivism: The radical line |url=https://books.google.com/books?id=pAj-96LlBuMC&amp;pg=PA12 |isbn= 9780262018548 |year=2013 |publisher=MIT Press}}
&lt;/ref&gt; Hutto calls this position 'REC' (&lt;u&gt;R&lt;/u&gt;adical &lt;u&gt;E&lt;/u&gt;nactive &lt;u&gt;C&lt;/u&gt;ognition): "According to REC, there is no way to distinguish neural activity that is imagined to be genuinely content involving (and thus truly mental, truly cognitive) from other non-neural activity that merely plays a supporting or enabling role in making mind and cognition possible."&lt;ref name=Hutto2/&gt;

===Participatory sense-making===

[[Hanne De Jaegher]] and [[Ezequiel Di Paolo]] (2007)&lt;ref name="DeJaeger0"/&gt; have extended the enactive concept of sense-making&lt;ref name="EThompson3"/&gt; into the social domain. The idea takes as its departure point the process of interaction between individuals in a social encounter.&lt;ref name=DeJaegher_etal&gt;{{cite journal |author1=Hanne De Jaegher |author2=Ezequiel Di Paolo |author3=Shaun Gallagher |title= Can social interaction constitute social cognition? |journal=Trends in Cognitive Sciences |year=2010  |volume=14 |issue=10 |pages=441–447 |doi=10.1016/j.tics.2010.06.009 |pmid=20674467}}
&lt;/ref&gt; De Jaegher and Di Paolo argue that the interaction process itself can take on a form of autonomy (operationally defined). This allows them to define social cognition as the generation of meaning and its transformation through interacting individuals.

The notion of participatory sense-making has led to the proposal that interaction processes can sometimes play constitutive roles in social cognition (De Jaegher, Di Paolo, Gallagher, 2010).&lt;ref name="DeJaegher1"/&gt; It has been applied to research in [[social neuroscience]]''&lt;ref name="DiPaolo3"/&gt;&lt;ref name=SchilbachTimmermans&gt;
{{cite journal |author1=Leonhard Schilbach |author2=Bert Timmermans |author3=Vasudevi Reddy |author4=Alan Costall |author5=Gary Bente |author6=Tobias Schlicht |author7=Kai Vogeley |title= Toward a second-person neuroscience |journal=Behavioral and Brain Sciences |year=2013  |volume=36 |issue=4 |pages=393–414 |doi=10.1017/S0140525X12000660}}&lt;/ref&gt;'' and [[autism]].''&lt;ref name="DeJaegher_autism"&gt;{{cite journal |author= Hanne De Jaegher |title= Embodiment and sense-making in autism|journal=Frontiers in Integrative Neuroscience |year=2012  |volume=7 |pages=15 |doi=10.3389/fnint.2013.00015}}
&lt;/ref&gt;''

In a similar vein, "an inter-enactive approach to agency holds that the behavior of agents in a social situation unfolds not only according to their individual abilities and goals, but also according to the conditions and constraints imposed by the autonomous dynamics of the interaction process itself".&lt;ref name=STorrance&gt;
{{cite journal |title=An Inter-Enactive Approach to Agency: Participatory Sense-Making, Dynamics, and Sociality |author1=Steve Torrance |author2=Tom Froese |url=http://sacral.c.u-tokyo.ac.jp/pdf/froese_humana_2011.pdf |journal=Human Mente |volume=15 |pages=21–53 |year=2011 }} 
&lt;/ref&gt; According to Torrance, enactivism involves five interlocking themes related to the question "What is it to be a (cognizing, conscious) agent?" It is:&lt;ref name=STorrance/&gt;
:1. to be a biologically autonomous ([[Autopoiesis|autopoietic]]) organism
:2. to generate ''significance'' or ''meaning'', rather than to act via...updated internal representations of the external world
:3. to engage in sense-making via dynamic coupling with the environment
:4. to 'enact' or 'bring forth' a world of significances by mutual co-determination of the organism with its enacted world
:5. to arrive at an experiential awareness via lived embodiment in the world.

Torrance adds that "many kinds of agency, in particular the agency of human beings, cannot be understood separately from understanding the nature of the interaction that occurs between agents." That view introduces the social applications of enactivism. "Social cognition is regarded as the result of a special form of action, namely ''social interaction''...the enactive approach looks at the circular dynamic within a dyad of embodied agents."&lt;ref name=FuchsT&gt;
{{cite book |url=https://books.google.com/books?id=Olm10GVwV74C&amp;pg=PA206 |page=206 |chapter=Non-representational intersubjectivity |author1=Thomas Fuchs |author2=Hanne De Jaegher |isbn=9783794527915 |year=2010 |publisher=Schattauer Verlag |title=The Embodied Self: Dimensions, Coherence and Disorders |editor1=Thomas Fuchs |editor2=Heribert C. Sattel |editor3=Peter Henningsen }}
&lt;/ref&gt;
 
In [[cultural psychology]], enactivism is seen as a way to uncover cultural influences upon feeling, thinking and acting.&lt;ref name=Verheggen&gt;{{cite book |chapter=Chapter 8: Enactivism |author1=Cor Baerveldt |author2=Theo Verheggen |title=The Oxford Handbook of Culture and Psychology  |url=https://books.google.com/books?id=WljI1r2e-SUC&amp;pg=PA165 |pages=165''ff'' |doi=10.1093/oxfordhb/9780195396430.013.0009 |isbn=9780195396430 |date=May 2012 |quote= Whereas the enactive approach in general has focused on sense-making as an embodied and situated activity, enactive cultural psychology emphasizes the expressive and dynamically enacted nature of cultural meaning.}}&lt;/ref&gt;  Baerveldt and Verheggen argue that "It appears that seemingly natural experience is thoroughly intertwined with sociocultural realities." They suggest that the social patterning of experience is to be understood through enactivism, "the idea that the reality we have in common, and in which we find ourselves, is neither a world that exists independently from us, nor a socially shared way of representing such a pregiven world, but a world itself brought forth by our ways of communicating and our joint action....The world we inhabit is manufactured of 'meaning' rather than 'information'.&lt;ref name=Baerveldt&gt;
{{cite journal |title=Enactivism and the experiential reality of culture: Rethinking the epistemological basis of cultural psychology |author1=Cor Baerveldt |author2=Theo Verheggen |url=https://docs.google.com/file/d/0Bz8cVS8LoO7OTk9ZUkVqazFiU1U/edit |journal=Culture &amp; Psychology |volume=5 |issue=2 |pages=183–206 |year=1999 |doi=10.1177/1354067x9952006}}
&lt;/ref&gt;

[[Niklas Luhmann|Luhmann]] attempted to apply Maturana and Varela's notion of autopoiesis to social systems.&lt;ref name=Luhmann&gt;
{{cite book |title=Social systems |url=https://books.google.com/books?id=zVZQW4gxXk4C&amp;pg=PA34&amp;lpg=PA34 |isbn= 9780804726252 |year=1995 |publisher=Stanford University Press |author=Niklas Luhmann}}
&lt;/ref&gt; "A core concept of social systems theory is derived from biological systems theory: the concept of ''autopoiesis''. Chilean biologist Humberto Maturana come up with the concept to explain how biological systems such as cells are a product of their own production." "Systems exist by way of operational closure and this means that they each construct themselves and their own realities."&lt;ref name=Moeller&gt;
{{cite book  |chapter=Part 1: A new way of thinking about society |pages= 12 ''ff'' |author=Hans-Georg Moeller |year=2011 |isbn= 978-0812695984 |publisher=Open Court |title=Luhmann Explained: From Souls to Systems |url=https://books.google.com/books?id=tuKsEvpcj9MC&amp;pg=PA12}}
&lt;/ref&gt;

==Educational aspects==
The first definition of enaction was introduced by psychologist [[Jerome Bruner]],&lt;ref name=Pugliese&gt;
{{cite book |title=Intelligent Virtual Agents: |chapter=A framework for motion based bodily enaction with virtual characters; §2.1 Enaction |author1=Roberto Pugliese |author2=Klaus Lehtonen |url=https://books.google.com/books?id=QU9b_IjVMF4C&amp;pg=PA163 |page=163 |isbn=9783642239731 |publisher=Springer |year=2011}}
&lt;/ref&gt;&lt;ref name=Beck&gt;
{{cite book |url=https://books.google.com/books?id=8V9BAAAAQBAJ&amp;pg=PA104 |page=104 |title=From Diagnostics to Learning Success: Proceedings in Vocational Education and Training |isbn=978-9462091894 |edition=Paperback |year=2013 |publisher=Springer Science &amp; Business |author=Stephanie A Hillen |chapter=Chapter III: What can research on technology for learning in vocational educational training teach media didactics? |editor1=Klaus Beck |editor2=Olga Zlatkin-Troitschanskaia }}
&lt;/ref&gt; who introduced enaction as 'learning by doing' in his discussion of how children learn, and how they can best be helped to learn.&lt;ref name=Bruner&gt;{{cite book |author=[[Jerome Bruner]]|year=1966 |title=Toward a theory of instruction |publisher=Belknap Press of Harvard University Press |isbn=978-0674897007}}&lt;/ref&gt;&lt;ref name=Bruner2&gt;{{cite book |author=Jerome Bruner |year=1968 |title=Processes of cognitive growth: Infancy |publisher= Crown Pub |isbn= 978-0517517482}}{{oclc|84376}}&lt;/ref&gt; He associated enaction with two other ways of knowledge organization: [[Cultural icon|Iconic]] and [[Symbol]]ic.&lt;ref name=Bruner3&gt;Quote from
{{cite book |title=Toward a Theory of Instruction |author=Jerome Seymour Bruner |url=http://h.uib.no/examplewiki/en/images/5/5a/Bruner_1966_Theory_of_Instruction.pdf |isbn=9780674897014 |publisher=Harvard University Press |year=1966 |page=44}} as quoted from {{cite book |title=Fundamental Constructs in Mathematics Education |author=J Bruner |editor1=John Mason |editor2=Sue Johnston-Wilder |url=https://books.google.com/books?id=EA3LtKYTa7YC&amp;pg=PA260 |page=260 |chapter=Chapter 10: Sustaining mathematical activity |year=2004 |publisher=Taylor &amp; Francis |isbn= 0415326982 |edition=Paperback}}&lt;/ref&gt;

:"Any domain of knowledge (or any problem within that domain of knowledge) can be represented in three ways: by a set of actions appropriate for achieving a certain result (enactive representation); by a set of summary images or graphics that stand for a concept without defining it fully  (iconic representation); and by a set of symbolic or logical propositions drawn from a symbolic system that is governed by rules or laws for forming and transforming propositions (symbolic representation)"
The term 'enactive framework' was elaborated upon by [[Francisco Varela]] and [[Humberto Maturana]].&lt;ref name=Bopry&gt;
{{cite book |title=The Praeger Handbook of Education and Psychology, Volume 1 |author=Jeanette Bopry |chapter=Providing a warrant for constructivist practice: the contribution of Francisco Varela |quote=Varela's enactive framework beginning with his collaboration on [[autopoiesis]] theory with his mentor Humberto Maturana [and the development of] enaction as a framework within which these theories work as a matter of course. |editor1=Joe L. Kincheloe |editor2=Raymond A. Horn |year=2007 |publisher=Greenwood Publishing Group |isbn=9780313331237 |url=https://books.google.com/books?id=O1ugEIEid6YC&amp;pg=PA474 |pages=474 ''ff''}}
&lt;/ref&gt;

Sriramen argues that enactivism provides "a rich and powerful explanatory theory for learning and being."&lt;ref name= Sriraman&gt;
{{cite book |title=Theories of Mathematics Education: Seeking New Frontiers |author1=Bharath Sriraman |author2=Lyn English |isbn=3642007422 |year=2009 |publisher=Springer |url=https://books.google.com/books?id=Kd_LgW2AXIoC&amp;pg=PA42 |pages=42 ''ff'' |chapter=Enactivism}}&lt;/ref&gt; and that it is closely related to both the [[Piaget's theory of cognitive development|ideas of cognitive development]] of [[Jean Piaget|Piaget]], and also the [[social constructivism]] of [[Vygotsky]].&lt;ref name=Sriraman/&gt; Piaget focused on the child's immediate environment, and suggested cognitive structures like spatial perception emerge as a result of the child's interaction with the world.&lt;ref name=Roth&gt;
{{cite book |title=Geometry as Objective Science in Elementary School Classrooms: Mathematics in the Flesh |author=Wolff-Michael Roth |isbn=1136732209 |year=2012 |publisher=Routledge |pages=41 ''ff'' |url=https://books.google.com/books?id=cXSsAgAAQBAJ&amp;pg=PT41 |chapter=Epistemology and psychology: Jean Piaget and modern constructivism}}
&lt;/ref&gt; According to Piaget, children ''construct'' knowledge, using what they know in new ways and testing it, and the environment provides feedback concerning the adequacy of their construction.&lt;ref name= Cziko&gt;
{{cite book |title=Without Miracles: Universal Selection Theory and the Second Darwinian Revolution |author=Gary Cziko |chapter=Chapter 12: Education; The provision and transmission of truth, or the selectionist growth of fallible knowledge? |page=222 |url=https://books.google.com/books?id=v1JEypylerUC&amp;pg=PA222&amp;lpg=PA222 |year=1997 |isbn=9780262531474 |publisher=MIT Press}}
&lt;/ref&gt; In a cultural context, Vygotsky suggested that the kind of cognition that can take place is not dictated by the engagement of the isolated child, but is also a function of social interaction and dialogue that is contingent upon a sociohistorical context.&lt;ref name=Kincheloe&gt;
{{cite book |title=The Praeger Handbook of Education and Psychology, Volume 1 |chapter=Interpretivists drawing on the power of enactivism |url=https://books.google.com/books?id=O1ugEIEid6YC&amp;pg=PA24 |pages=24 ''ff'' |publisher=Greenwood Publishing Group |year=2007 |editor1=Joe L. Kincheloe |editor2=Raymond A. Horn |author=Joe L Kincheloe |isbn=0313331235}}
&lt;/ref&gt;  Enactivism in educational theory "looks at each learning situation as a complex system consisting of teacher, learner, and context, all of which frame and co-create the learning situation."&lt;ref name=Vithal&gt;
{{cite book |editor1=Renuka Vithal |editor2=Jill Adler |editor3=Christine Keitel |title=Researching Mathematics Education in South Africa: Perspectives, Practices and Possibilities |chapter=Chapter 9: Dilemmas of change: seeing the complex rather than the complicated?  |page=240 |author=Chris Breen |isbn=0796920478 |publisher=HSRC Press |year=2005 |url=https://books.google.com/books?id=byWHt_NVUEgC&amp;pg=RA6-PA240}}
&lt;/ref&gt; Enactivism in education is very closely related to [[situated cognition]],&lt;ref name=VanDeGevel&gt;
{{cite book |title=The nexus between artificial intelligence and economics |chapter=§3.2.2 Enactive artificial intelligence |quote=''Enactivism'' may be considered as the most developed model of embodied situated cognition...Knowing is inseparable from doing. |url=https://books.google.com/books?id=uek_AAAAQBAJ&amp;pg=PA21 |page=21 |author=Ad J. W. van de Gevel, Charles N. Noussair |isbn=3642336477 |publisher=Springer |year=2013}}
&lt;/ref&gt; which holds that "knowledge is situated, being in part a product of the activity, context, and culture in which it is developed and used."&lt;ref name=Collins&gt;
{{cite journal |title=Situated cognition and the culture of learning |author1=John Seely Brown |author2=Allan Collins |author3=Paul Duguid |url=http://www.exploratorium.edu/ifi/resources/museumeducation/situated.html |journal=Educational Researcher |volume=18 |number=1 |pages=32–42 |date=Jan–Feb 1989 |doi=10.3102/0013189x018001032}}
&lt;/ref&gt; This approach challenges the "separating of what is learned from how it is learned and used."&lt;ref name=Collins/&gt;

==Artificial intelligence aspects==
{{importance section|date=May 2014}}
{{main|Enactive interfaces}}
The ideas of enactivism regarding how organisms engage with their environment have interested those involved in [[Cognitive robotics|robotics]] and [[Human–computer interaction|man-machine interfaces]]. The analogy is drawn that a robot can be designed to interact and learn from its environment in a manner similar to the way an organism does that,&lt;ref name=Sandini&gt;
{{cite book |chapter=The ''iCub'' cognitive humanoid robot: An open-system research platform for enactive cognition |author1=Giulio Sandini |author2=Giorgio Metta |author3=David Vernon |title=50 Years of Artificial Intelligence: Essays Dedicated to the 50th Anniversary of Artificial Intelligence |editor1=Max Lungarella |editor2=Fumiya Iida |editor3=Josh Bongard |editor4=Rolf Pfeifer |publisher=Springer |year=2007 |isbn= 9783540772958}}
&lt;/ref&gt; and a human can interact with a computer-aided design tool or data base using an interface that creates an enactive environment for the user, that is, all the user's tactile, auditory, and visual capabilities are enlisted in a mutually explorative engagement, capitalizing upon all the user's abilities, and not at all limited to cerebral engagement.&lt;ref name=Bordegoni&gt;
{{cite book |title=Emotional Engineering: Service Development |chapter=§4.5.2 Design tools based upon enactive interfaces |url=https://books.google.com/books?id=ow-UFDj15rUC&amp;pg=PA78 |pages=78 ''ff'' |isbn=9781849964234 |year=2010 |publisher=Springer |author=Monica Bordegoni |editor=Shuichi Fukuda}}
&lt;/ref&gt; In these areas it is common to refer to [[affordance]]s as a design concept, the idea that an environment or an interface affords opportunities for enaction, and good design involves optimizing the role of such affordances.&lt;ref name=Norman&gt;
{{cite book |title=The Design of Everyday Things |edition=Revised and expanded |quote=An affordance is a relationship between the properties of an object and the capabilities of the agent that determine just how the object could possibly be used. |url=https://books.google.com/books?id=nVQPAAAAQBAJ&amp;pg=PT17 |year=2013 |page=11 |isbn=978-0465050659 |publisher=Basic Books |author=Don Norman |chapter=Affordances }}
&lt;/ref&gt;&lt;ref name=Kim&gt;
{{cite book |title=Encyclopedia of human computer interaction |chapter=The use and evolution of affordance in HCI  |url=https://books.google.com/books?id=h9iZh_I1YREC&amp;pg=PA668 |pages=668 ''ff'' |isbn=9781591407980 |year=2006 |publisher=Idea Group Inc |author=Georgios S Christou |editor=Claude Ghaoui}}
&lt;/ref&gt;&lt;ref name=Kaipainen&gt;
{{cite journal |title=Enactive Systems and Enactive Media: Embodied Human-Machine Coupling beyond Interfaces |url=http://www.mitpressjournals.org/doi/abs/10.1162/LEON_a_00244#.U3_JKygT0cs  |journal=Leonardo |volume=44 |pages=433–438 |date=October 2011 |issue=5 |doi=10.1162/LEON_a_00244 |author1=Mauri Kaipainen |author2=Niklas Ravaja |author3=Pia Tikka |display-authors=etal}} 
&lt;/ref&gt;&lt;ref name=Boy&gt;

{{cite book |title=Orchestrating Human-Centered Design |author=Guy Boy |url=https://books.google.com/books?id=I5gCTZCIL3AC&amp;pg=PA118&amp;lpg=PA118 |isbn=9781447143383 |year=2012 |publisher=Springer |page=118 |quote=The organization producing the system can itself be defined as an autopoietic system in Maturana and Varela's sense. An autopoietic system is producer and product at the same time. HCD [Human Centered Design] is both the process of design and the design itself.}}

&lt;/ref&gt;&lt;ref name=Thannhuber&gt;
{{cite journal |title=An autopoietic approach for knowledge management systems in manufacturing enterprises |author1=Markus Thannhuber |author2=Mitchell M Tseng |author3=Hans-Jörg Bullinger |url=http://www.researchgate.net/publication/223035600_An_Autopoietic_Approach_for_Building_Knowledge_Management_Systems_in_Manufacturing_Enterprises/file/50463525a5a320287e.pdf%26sa%3DX%26scisig%3DAAGBfm3GtB0hiqz1jul4MXuCQxnRzPbcHQ%26oi%3Dscholarr&amp;rct=j&amp;q=&amp;esrc=s&amp;sa=X&amp;ei=N-h_U6HtHIiEogSy_oHAAw&amp;ved=0CCcQgAMoADAA&amp;usg=AFQjCNEt_M1NOffumXQSxrJIVuZI48XRGQ&amp;cad=rja |journal=Annals of the CIRP-Manufacturing Technology |volume=50 |issue=1 |year=2001 |pages=313 ''ff'' |doi=10.1016/s0007-8506(07)62129-5}}&lt;/ref&gt;

The activity in the AI community also has influenced enactivism as whole. Referring extensively to modeling techniques for [[evolutionary robotics]] by Beer,&lt;ref name=Beer&gt;
{{cite journal |author=Randall D Beer |year=1995 |title=A dynamical systems perspective on agent-environment interaction.
 |journal= Artificial Intelligence |volume=72 |pages=173–215 |url=http://dx.doi.org/10.1016/0004-3702%2894%2900005-L |doi=10.1016/0004-3702(94)00005-l}}
&lt;/ref&gt; the modeling of learning behavior by Kelso,&lt;ref name=Kelso&gt;
{{cite book |author=James AS Kelso |year=2009 |chapter=Coordination dynamics |editor=R. A. Meyers |title= Encyclopedia of complexity and system science |pages= 1537–1564 |isbn=978-0-387-75888-6 |url=http://link.springer.com/referenceworkentry/10.1007%2F978-0-387-30440-3_101}}
&lt;/ref&gt; and to modeling of sensorimotor activity by Saltzman,&lt;ref name=Saltzman&gt;
{{cite book |author=Eliot L. Saltzman |year=1995 |chapter=Dynamics and coordinate systems in skilled sensorimotor activity |editor1=T. van Gelder |editor2=R. F. Port |title= Mind as motion: Explorations in the dynamics of cognition  |publisher= MIT Press |isbn=9780262161503 |url=https://books.google.com/books?id=e6HUM6V8QbQC&amp;pg=PA151 |page=151 ''ff''}}
&lt;/ref&gt; McGann, De Jaegher, and Di Paolo discuss how this work makes the dynamics of coupling between an agent and its environment, the foundation of enactivism, "an operational, empirically observable phenomenon."&lt;ref name=McGann3&gt;{{cite journal |author1=Marek McGann |author2=Hanne De Jaegher |author3=Ezequiel Di Paolo |year= 2013 |title=Enaction and psychology |journal=Review of General Psychology |volume=17 |issue=2 |pages=203–209 |url= http://www.academia.edu/4993021/Enaction_and_Psychology |doi= 10.1037/a0032935 |quote=Such modeling techniques allow us to explore the parameter space of coupling between agent and environment...to the point that their basic principles (the universals, if such there are, of enactive psychology) can be brought clearly into view.}}
&lt;/ref&gt; That is, the AI environment invents examples of enactivism using concrete examples that, although not as complex as living organisms, isolate and illuminate basic principles.

==See also==
{{colbegin}}
*[[Action-specific perception]]
*[[Autopoesis]]
*[[Biosemiotics]]
*[[Cognitive science]]
*[[Cognitive psychology]]
*[[Computational theory of mind]]
*[[Connectivism]]
*[[Cultural psychology]]
*[[Distributed cognition]]
*[[Embodied cognition]]
*[[Embodied embedded cognition]]
*[[Enactive interfaces]]
*[[Extended cognition]]
*[[Extended mind]]
*[[Externalism#Enactivism and embodied cognition]]
*[[Mind–body problem]]
*[[Phenomenology (philosophy)]]
*[[Representationalism]]
*[[Situated cognition]]
*[[Social cognition]]
{{colend}}

==References==
{{reflist|30em|refs=

&lt;ref name=Baldwin&gt;
{{cite book |author=Thomas Baldwin |title=[[Maurice Merleau-Ponty]]: Basic Writings |chapter-url=https://books.google.com/books?id=OS8FM-AFvvsC&amp;pg=PA65 |chapter=Part One: Merleau-Ponty's prospectus of his work |page=65  |quote=Science has not and never will have, by its nature, the same significance ''qua'' form of being as the world which we perceive, for the simple reason that it is a rationale or explanation of that world. |isbn= 978-0415315869 |year=2003 |publisher=Routledge}}
&lt;/ref&gt;

&lt;ref name=Burman&gt;
{{cite journal |author=Jeremy Trevelyan Burman  |year=2006 |journal=Journal of Consciousness Studies |title=Book reviews: ''Consciousness &amp; Emotion'' |url=http://www.imprint.co.uk/pdf/13_12_br.pdf  |volume=13 |issue=12 |pages=115–124}}  From a review of {{cite book |title=Consciousness &amp; Emotion: Agency, conscious choice, and selective perception |editor1=Ralph D. Ellis |editor2=Natika Newton |url=https://books.google.com/books?id=LZk6AAAAQBAJ&amp;printsec=frontcover |isbn=9789027294616 |year=2005 |publisher=John Benjamins Publishing}}
&lt;/ref&gt;

&lt;ref name=Chiari&gt;
{{cite web |title=Constructivism |author1=Gabriele Chiari |author2=M. Laura Nuzzo |work=The Internet Encyclopaedia of Personal Construct Psychology |url=http://www.pcp-net.org/encyclopaedia/constructivism.html}}
&lt;/ref&gt;

&lt;ref name=ClarkA&gt;
{{cite journal |author1=Andy Clark |author2=Josefa Toribio |title=Doing without representing |journal =Synthese |volume=101 |pages=401–434 |year=1994 |url=http://www.philosophy.ed.ac.uk/people/clark/pubs/DoingW-O-rep.pdf |doi=10.1007/bf01063896}}
&lt;/ref&gt;

&lt;ref name=ClarkA1&gt;
{{cite journal  |author=Andy Clark   |title=Vision as Dance? Three Challenges for Sensorimotor Contingency Theory |journal= Psyche |volume=12 |issue=1 |date=March 2006 |url= https://www.era.lib.ed.ac.uk/bitstream/1842/1444/1/Psyche%20Clark.pdf}}
&lt;/ref&gt;

&lt;ref name=Diettrich&gt;
{{cite book |author=Olaf Diettrich |chapter=The biological boundary conditions for our classical physical world view |title=Evolutionary Epistemology, Language and Culture |page=88 |year=2006 |publisher=Springer |editor1=Nathalie Gontier |editor2=Jean Paul van Bendegem |editor3=Diederik Aerts |isbn=9781402033957 |url=https://books.google.com/books?id=hp2JiTDBbWkC&amp;pg=PA88}}
&lt;/ref&gt;

&lt;ref name=Diettrich2&gt;
"The notion of 'truth' is replaced with 'viability' within the subjects' experiential world." From {{cite book |title= The handbook of evolution: The evolution of human societies and culture |author=Olaf Diettrich |chapter=Cognitive evolution; footnote 2 |page=61 |url=https://books.google.com/books?id=Ex5c_pyOsTwC&amp;pg=PA61&amp;lpg=PA61#v=onepage&amp;q&amp;f=false |editor1=Franz M. Wuketits |editor2=Christoph Antweiler |year=2008 |publisher=Wiley-Blackwell}} and in ''Evolutionary Epistemology, Language and Culture'' cited above, p. 90.
&lt;/ref&gt;

&lt;ref name=Glaserfeld&gt;
{{cite book |author= Ernst von Glasersfeld |title=Epistemology and education |chapter=Report no. 14: Piaget and the Radical Constructivist Epistemology |url=http://www.vonglasersfeld.com/034 |editor1=CD Smock |editor2=E von Glaserfeld |publisher=Follow Through Publications |pages=1–24 |year=1974}} 
&lt;/ref&gt;

&lt;ref name=Glasersfeld2&gt;
{{cite journal |author= Ernst von Glasersfeld |url=http://www.univie.ac.at/constructivism/EvG/papers/118.pdf |title=Cognition, construction of knowledge and teaching |journal=Synthese |volume=80 |issue=1 |pages=121–140 |year=1989 |doi=10.1007/bf00869951}}
&lt;/ref&gt;

&lt;ref name=Gontier&gt;
{{cite web |author=Nathalie Gontier |title=Evolutionary Epistemology |url=http://www.iep.utm.edu/evo-epis/ |work=Internet Encyclopedia of Philosophy |year=2006}}
&lt;/ref&gt;

&lt;ref name=Hutchins&gt;	
{{cite book |title=Cognition in the Wild |author=Edwin Hutchins |url= |isbn=9780262581462 |year=1996 |page=428 |publisher=MIT Press }} Quoted by {{cite journal |title=Cognitive, embodied or enacted? :Contemporary perspectives for HCI and interaction  |url=http://trans-techresearch.net/wp-content/uploads/2010/11/Rocha-01.pdf |author=Marcio Rocha |year=2011 |publisher=Transtechnology Research Reader |isbn=978-0-9538332-2-1}}	
&lt;/ref&gt;

&lt;ref name=Jaegher1&gt;
{{cite book |author1=Ezequiel A Di Paolo |author2=Marieke Rhohde |author3=Hanne De Jaegher |chapter=Horizons for the enactive mind: Values, social interaction, and play |title=Enaction: Toward a New Paradigm for Cognitive Science |editor1=John Stewart |editor2=Oliver Gapenne |editor3=Ezequiel A Di Paolo |url=https://books.google.com/books?id=UtFDJx-gysQC&amp;pg=PA39 |pages=33 ''ff'' |isbn=  978-0262526012 |publisher=MIT Press |year=2014}}
&lt;/ref&gt;

&lt;ref name=Manetti&gt;
A collection of papers on this topic is introduced by {{cite journal |title=Agency: From embodied cognition to free will |author1=Duccio Manetti |author2=Silvano Zipoli Caiani |journal=Humana Mente |volume=15 |date=January 2011 |pages=''V''-''XIII'' |url=http://www.humanamente.eu/PDF/Issue15_CompletePDF.pdf}}
&lt;/ref&gt;

&lt;ref name=Maturana&gt;
{{cite book |author1=Humberto R Maturana |author2=Francisco J Varela |year=1992 |title= The tree of knowledge: the biological roots of human understanding |edition=Revised |publisher=Shambhala Publications Inc |chapter=Afterword |page=255 |isbn=978-0877736424}}
&lt;/ref&gt;

&lt;ref name=Munz&gt;
{{cite book |url=https://books.google.com/books?id=tMuIAgAAQBAJ&amp;pg=PA154&amp;lpg=PA154 |page=154 |author=Peter Munz |title=Philosophical Darwinism: On the Origin of Knowledge by Means of Natural Selection |year=2002 |isbn=9781134884841 |publisher=Routledge}}
&lt;/ref&gt;

&lt;ref name=Mutelesi&gt;
{{cite journal |title=Radical constructivism seen with Edmund Husserl as starting point |author=Edmond Mutelesi |url=http://www.univie.ac.at/constructivism/journal/2/1/006.mutelesi |journal=Constructivist foundations |volume=2 |issue=1 |pages=6–16 |date=November 15, 2006}}
&lt;/ref&gt;

&lt;ref name=Rohde&gt;
{{cite book |title=Enaction, Embodiment, Evolutionary Robotics: Simulation Models for a Post-Cognitivist Science of Mind  |chapter= §3.1 The scientist as observing subject |pages=30 ''ff'' |author=Marieke Rohde |isbn=978-9078677239 |publisher=Atlantis Press |year=2010 |url=https://books.google.com/books?id=LlpZjLMPiHYC&amp;pg=PA30}}
&lt;/ref&gt;

&lt;ref name=Rowlands&gt;
{{cite book |author=Mark Rowlands |chapter=Chapter 3: The mind embedded §5 The mind enacted |pages=70 ''ff'' |year=2010 |isbn=0262014556 |publisher=MIT Press |url=https://books.google.com/books?id=AiwjpL-0hDgC&amp;pg=PA70 |title=The new science of the mind: From extended mind to embodied phenomenology}} Rowlands attributes this idea to {{cite book |author=D M MacKay |year=1967 |chapter=Ways of looking at perception |title=Models for the perception of speech and visual form (Proceedings of a symposium) |editor=W Watthen-Dunn |publisher=MIT Press |pages=25 ''ff'' |url=https://books.google.com/books?id=Ts9JAAAAMAAJ&amp;focus=searchwithinvolume&amp;q=MacKay+Ways+of+looking+at+perception}}
&lt;/ref&gt;

&lt;ref name=EThompson&gt;
{{cite book |title= Mind in life |chapter=The enactive approach |author=Evan Thompson |isbn=978-0674057517 |edition=Paperback |pages=13 ''ff'' |url=https://books.google.com/books?id=OVGna4ZEpWwC&amp;pg=PA13 |publisher=Harvard University Press |year=2007 }}  ToC, first 65 pages, and index [http://lchc.ucsd.edu/MCA/Mail/xmcamail.2012_03.dir/pdf3okBxYPBXw.pdf found here]
&lt;/ref&gt;

&lt;ref name=EThompson2&gt;
{{cite book |title= Mind in life |chapter=Autonomy and emergence |author=Evan Thompson |isbn=978-0674057517 |edition=Paperback |pages=37 ''ff'' |url=https://books.google.com/books?id=OVGna4ZEpWwC&amp;pg=PA13 |publisher=Harvard University Press |year=2007}} See also the Introduction, p. ''x''.
&lt;/ref&gt;

&lt;ref name=EThompson3&gt;
{{cite book |title= Mind in life |chapter=Chapter 8: Life beyond the gap |author=Evan Thompson |isbn=978-0674057517 |edition=Paperback |page=225  |url=https://books.google.com/books?id=OVGna4ZEpWwC&amp;pg=PA225 |publisher=Harvard University Press |year=2007}}
&lt;/ref&gt;

&lt;ref name=EThompson4&gt;
{{cite book |title= Mind in life |chapter=Life can be known only by life |author=Evan Thompson |isbn=978-0674057517 |edition=Paperback |page=165  |url=https://books.google.com/books?id=OVGna4ZEpWwC&amp;pg=PA165 |publisher=Harvard University Press |year=2007}}
&lt;/ref&gt;

&lt;ref name="Evan Thompson"&gt;
{{cite book |title=Mind in life:Biology, phenomenology, and the sciences of mind |author=Evan Thompson |url=http://lchc.ucsd.edu/MCA/Mail/xmcamail.2012_03.dir/pdf3okBxYPBXw.pdf |publisher=Harvard University Press |isbn= 978-0674057517 |chapter=Chapter 1: The enactive approach |year=2010}} ToC, first 65 pages, and index [http://lchc.ucsd.edu/MCA/Mail/xmcamail.2012_03.dir/pdf3okBxYPBXw.pdf found here].
&lt;/ref&gt;

&lt;ref name=Tascano0&gt;
{{cite book |title=A Dictionary of Continental Philosophy |editor=John Protevi |url=https://books.google.com/books?id=kRUZ61uISUMC&amp;pg=PA169 |pages=169–170 |chapter = Enaction |isbn=9780300116052 |publisher=Yale University Press |year=2006}}
&lt;/ref&gt;

&lt;ref name=Varela&gt;
{{cite book |title=The embodied mind: Cognitive science and human experience |author1=Francisco J Varela |author2=Evan Thompson |author3=Eleanor Rosch |url=https://books.google.com/books?id=QY4RoH2z5DoC&amp;printsec=frontcover#v=snippet&amp;q=We%20propose%20as%20a%20%20name%20the%20term%20%20enactive&amp;f=false |year=1992 |publisher=MIT Press |page=9 |quote= |isbn=978-0262261234}}&lt;/ref&gt;

&lt;ref name=Ward2&gt;
"The underpinnings of cognition are inextricable from those of affect, that the phenomenon of cognition itself is essentially bound up with affect.." See p. 104: {{cite book |author1=Dave Ward |author2=Mog Stapleton |year=2012 |url=https://books.google.com/books?id=Y1E7FogqvJ0C&amp;pg=PA89 |chapter=Es are good. Cognition as enacted, embodied, embedded, affective and extended |editor= Fabio Paglieri |title=Consciousness in Interaction: The role of the natural and social context in shaping consciousness |publisher=John Benjamins Publishing |pages=89 ''ff'' |isbn=978-9027213525}} [http://philpapers.org/archive/WAREAG.pdf On-line version here].
&lt;/ref&gt;

&lt;ref name=RWilson&gt;
{{cite web |author1=Robert A Wilson |author2=Lucia Foglia |title=Embodied Cognition: §2.2 Enactive cognition |work=The Stanford Encyclopedia of Philosophy (Fall 2011 Edition) |editor=Edward N. Zalta |url = http://plato.stanford.edu/archives/fall2011/entries/embodied-cognition/#EnaCog |date=July 25, 2011}}
&lt;/ref&gt;

}}

==Further reading==
* {{cite journal |author1=De Jaegher H. |author2=Di Paolo E. A. | year = 2007 | title = Participatory sense-making: An enactive approach to social cognition | url = | journal = Phenomenology and the Cognitive Sciences | volume = 6 | issue = 4| pages = 485–507 | doi=10.1007/s11097-007-9076-9}}
* Di Paolo, E. A., Rohde, M. and De Jaegher, H., (2010). ''Horizons for the Enactive Mind: Values, Social Interaction, and Play.'' In J. Stewart, O. Gapenne and E. A. Di Paolo (eds), Enaction: Towards a New Paradigm for Cognitive Science, Cambridge, MA: MIT Press, pp.&amp;nbsp;33 – 87. ISBN 9780262014601
* [[Daniel Hutto|Hutto, D. D.]] (Ed.)  (2006).  ''Radical Enactivism: Intentionality, phenomenology, and narrative.''  In R. D. Ellis &amp; N. Newton (Series Eds.), ''Consciousness &amp; Emotion, vol. 2.'' ISBN 90-272-4151-1
* McGann, M. &amp; Torrance, S. (2005).  Doing it and meaning it (and the relationship between the two).  In R. D. Ellis &amp; N. Newton, ''Consciousness &amp; Emotion, vol. 1: Agency, conscious choice, and selective perception''. Amsterdam: John Benjamins. ISBN 1-58811-596-8
* {{cite journal |title=The enactive approach: Theoretical sketches from cell to society |author1=Tom Froese |author2=Ezequiel A DiPaolo |citeseerx = 10.1.1.224.5504 |journal=Pragmatics and Cognition |volume=19 |issue=1 |year=2011 |pages=1–36 |doi=10.1075/pc.19.1.01fro}}
* {{cite journal |author1=Steve Torrance |author2=Tom Froese |title=An inter-enactive approach to agency: participatory sense-making, dynamics, and sociality. |journal=Humana. Mente |volume=15 |year=2011 |pages=21–53 |citeseerx = 10.1.1.187.1151 }}

==Notes==
{{reflist |group=Note |refs=
&lt;ref group=Note name=Cognitivism&gt;
Cognition as information processing like that of a digital computer. From {{cite book |author=Evan Thompson |title=Mind in Life |isbn=978-0674057517}} ''Cognitivism'', p. 4; See also {{cite web |title=The computational theory of mind |author=Steven Horst |work= The Stanford Encyclopedia of Philosophy (Spring 2011 Edition) |editor=Edward N. Zalta |url=http://plato.stanford.edu/archives/spr2011/entries/computational-mind/ |date=December 10, 2009}}
&lt;/ref&gt;

&lt;ref group=Note name=Connectionism&gt;
Cognition as emergent patterns of activity in a neural network. From {{cite book |author=Evan Thompson |title=Mind in Life |isbn=978-0674057517}} ''Connectionism'', p. 8; See also {{cite web |title=Connectionism |author=James Garson |work= The Stanford Encyclopedia of Philosophy (Spring 2011 Edition) |editor=Edward N. Zalta |url=http://plato.stanford.edu/archives/win2012/entries/connectionism/ |date=July 27, 2010}}
&lt;/ref&gt;

}}

==External links==
*{{cite web |title=Consciousness as the emergent property of the interaction between brain, body, &amp; environment: the crucial role of haptic perception  |author=Pietro Morasso |url=http://www.consciousness.it/iwac2005/Material/Morasso.pdf |year=2005 }} Slides related to a chapter on [[haptic perception]] (recognition through touch): {{cite book |editor1=Antonio Chella |editor2=Riccardo Manzotti |author=Pietro Morasso |chapter=Chapter 14: The crucial role of haptic perception |page=234 ''ff'' |title= Artificial Consciousness |publisher= Academic  |year=2007 |isbn=978-1845400705 |url=https://www.google.com/search?tbo=p&amp;tbm=bks&amp;q=isbn:1845400704&amp;num=10}}
*{{cite web |title=Questioning Life and Cognition: Some Foundational Issues in the Paradigm of Enaction |url=http://www.enactionseries.com/library/bookjs/co/Original_book_JS.html#Pk1qsEYBVxgUwAM6tVeiff |author=John Stewart |work=Enaction Series: Online Collaborative Publishing |editor1=Olivier Gapenne |editor2=Bruno Bachimont |publisher=Enaction Series |accessdate=April 27, 2014}} 
*{{cite web |title=Educational Multimedia Task Force – MM 1045, REPRESENTATION |url=http://halshs.archives-ouvertes.fr/docs/00/00/18/64/PDF/REPRDel1.pdf |publisher= |author1=George-Louis Baron |author2=Eric Bruillard |author3=Christophe Dansac |date=January 1999}} An overview of the rationale and means and methods for the study of representations that the learner constructs in his/her attempt to understand knowledge in a given field. See in particular §1.2.1.4 ''Toward social representations'' (p.&amp;nbsp;24)
*{{cite web |author=Randall Whittaker |year=2001 |title=Autopoiesis and enaction |url=http://www.enolagaia.com/AT.html |publisher=Observer Web}} An extensive but uncritical introduction to the work of [[Francisco Varela]] and [[Humberto Maturana]]
*{{cite journal|title=Enactivism: Arguments &amp; Applications.|journal=Avant|date=Autumn 2014|volume= V| issue =  2/2014|doi=10.12849/50202014.0109.0002|url=http://avant.edu.pl/en/22014-2|accessdate=27 November 2014}} Entire journal issue on enactivism's status and current debates.

[[Category:Behavioral neuroscience]]
[[Category:Cognitive science]]
[[Category:Consciousness]]
[[Category:Educational psychology]]
[[Category:Enactive cognition]]
[[Category:Epistemology of science]]
[[Category:Knowledge representation]]
[[Category:Metaphysics of mind]]
[[Category:Motor cognition]]
[[Category:Neuropsychology]]
[[Category:Perception]]
[[Category:Philosophical theories]]
[[Category:Philosophy of psychology]]
[[Category:Psychological concepts]]
[[Category:Psychological theories]]
[[Category:Sociology of knowledge]]
[[Category:Action (philosophy)]]</text>
      <sha1>2vpk0cqio8mf69fzay45ertsnmucmqk</sha1>
    </revision>
  </page>
  <page>
    <title>KL-ONE</title>
    <ns>0</ns>
    <id>17188</id>
    <revision>
      <id>738216007</id>
      <parentid>722298196</parentid>
      <timestamp>2016-09-07T16:34:11Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <comment>add pronunciation, by request on talk page</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3044" xml:space="preserve">{{distinguish | KL1 }}

'''KL-ONE''' (pronounced "kay ell won") is a well known [[knowledge representation]] system in the tradition of [[semantic networks]] and [[Frame (Artificial intelligence) | frames]]; that is, it is a [[frame language]]. The system is an attempt to overcome semantic indistinctness in semantic network representations and to explicitly represent conceptual information as a structured inheritance network.&lt;ref&gt;{{Cite journal | last1 = Woods | first1 = W. A. | authorlink1 = William Aaron Woods| last2 = Schmolze | first2 = J. G. | doi = 10.1016/0898-1221(92)90139-9 | title = The KL-ONE family | journal = Computers &amp; Mathematics with Applications | volume = 23 | issue = 2–5 | pages = 133 | year = 1992 | pmid =  | pmc = }}&lt;/ref&gt;&lt;ref&gt;{{Cite journal | last1 = Brachman | first1 = R. J. | authorlink1 = Ronald J. Brachman| last2 = Schmolze | first2 = J. G. | doi = 10.1207/s15516709cog0902_1 | title = An Overview of the KL-ONE Knowledge Representation System | journal = Cognitive Science | volume = 9 | issue = 2 | pages = 171 | year = 1985 | pmid =  | pmc = }}&lt;/ref&gt;&lt;ref&gt;{{cite book | last = D.A. Duce | first = G.A. Ringland  | title = Approaches to Knowledge Representation, An Introduction | year = 1988 | publisher = Research Studies Press, Ltd. | isbn = 0-86380-064-5 }}&lt;/ref&gt;

There is a whole family of KL-ONE-like systems. One of the innovations that KL-ONE initiated was the use of a [[deductive classifier]], an automated reasoning engine that can validate a frame ontology and deduce new information about the ontology based on the initial information provided by a domain expert. 

Frames in KL-ONE are called [[concepts]]. These form hierarchies using subsume-relations; in the KL-ONE terminology a [[superclass (computer science)|super class]] is said to subsume its [[Subclass (computer science) | subclasses]]. 
[[Multiple inheritance]] is allowed. Actually a concept is said to be well-formed only if it inherits from more than one other concept. All concepts, except the top concept (usually THING), must have at least one super class. 

In KL-ONE descriptions are separated into two basic classes of concepts: primitive and defined. Primitives are domain concepts that are not fully defined. This means that given all the properties of a concept, this is not sufficient to classify it. They may also be viewed as incomplete definitions. Using the same view, defined concepts are complete definitions. Given the properties of a concept, these are [[necessary and sufficient]] conditions to classify the concept.

The slot-concept is called roles and the values of the roles are role-fillers. There are several different types of roles to be used in different situations. The most common and important role type is the generic RoleSet that captures the fact that the role may be filled with more than one filler.

==See also==
* [[Ontology language]]

==References==
{{reflist}}


{{FOLDOC}}

[[Category:Artificial intelligence]]
[[Category:Knowledge representation]]
[[Category:Ontology languages]]</text>
      <sha1>kbm0e0qbero6e8lxz4ewdymipzb83j2</sha1>
    </revision>
  </page>
  <page>
    <title>Knowledge Engineering Environment</title>
    <ns>0</ns>
    <id>11856314</id>
    <revision>
      <id>674425454</id>
      <parentid>661542141</parentid>
      <timestamp>2015-08-03T21:08:17Z</timestamp>
      <contributor>
        <username>Hampton11235</username>
        <id>22860730</id>
      </contributor>
      <minor />
      <comment>/* External links */Typo fix, replaced: External references → External links using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2774" xml:space="preserve">'''KEE''' (Knowledge Engineering Environment) is a [[Frame language|frame-based]] development tool for [[Expert system|Expert Systems]].&lt;ref&gt;[http://portal.acm.org/citation.cfm?id=62001 An evaluation of expert system development tools]&lt;/ref&gt; KEE was developed and sold by [[IntelliCorp (Software)|IntelliCorp]]. It was first released in 1983 and ran on [[Lisp Machine]]s. KEE was later ported to Lucid [[Common Lisp]] with [[CLX (Common Lisp)|CLX]] (X11 interface for Common Lisp). This version was available on various Workstations.

On top of KEE several extensions were offered:

* Simkit,&lt;ref&gt;[http://doi.acm.org/10.1145/76738.76766 The SimKit system: knowledge-based simulation and modeling tools in KEE]&lt;/ref&gt;&lt;ref&gt;[http://portal.acm.org/citation.cfm?id=62001 SimKit: a model-building simulation toolkit]&lt;/ref&gt; a frame-based simulation library
* KEEconnection,&lt;ref&gt;[http://portal.acm.org/citation.cfm?id=62001 KEEConnection: a bridge between databases and knowledge bases]&lt;/ref&gt; [[database connection]] between the frame system and relational databases.

Frames are called ''Units'' in KEE. Units are used for both individual instances and classes. Frames have ''slots'' and slots have ''facets''. Facets for example describe the expected values of a slot, the inheritance rule for the slot or the value of a slot. Slots can have multiple values. Behavior can be implemented using the message-passing paradigm.

KEE provides an extensive graphical user interface to create, browse and manipulate frames.

KEE also includes a frame-based [[Production system (computer science)|rule system]]. Rules themselves are frames in the KEE knowledge base. Both forward and backward chaining inference is available.

KEE supports non-monotonic reasoning through the concepts of ''worlds''. Worlds allow provide alternative slot-values of frames. Through an assumption-based [[Truth maintenance system]] inconsistencies can be detected and analyzed.&lt;ref&gt;[http://portal.acm.org/citation.cfm?id=62001 Reasoning with worlds and truth maintenance]&lt;/ref&gt;

''ActiveImages'' allows graphical displays to be attached to slots of Units. Typical examples are buttons, dials, graphs and histograms. The graphics are also implemented as Units via ''KEEPictures'' - a frame-based graphics library.

==See also==
* [[Expert system]]
* [[Frame language]]
* [[Inference engine]]
* [[IntelliCorp (software)|IntelliCorp]]
* [[Knowledge base]]
* [[Knowledge-based system]]
* [[Knowledge representation]]

==References==
&lt;references/&gt;

==External links==
* [http://www.aaai.org/ojs/index.php/aimagazine/article/viewArticle/625 An Assessment of Tools for Building Large Knowledge-Based Systems]

[[Category:Knowledge engineering]]
[[Category:Knowledge representation]]
[[Category:Common Lisp software]]</text>
      <sha1>4levqolrpi3i0253usxau1t2znskt3z</sha1>
    </revision>
  </page>
  <page>
    <title>Babelfy</title>
    <ns>0</ns>
    <id>43480298</id>
    <revision>
      <id>757595047</id>
      <parentid>739017631</parentid>
      <timestamp>2016-12-31T15:56:58Z</timestamp>
      <contributor>
        <username>BattyBot</username>
        <id>15996738</id>
      </contributor>
      <minor />
      <comment>changed {{Notability}} to {{Notability|Products}} &amp; [[WP:AWB/GF|general fixes]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2767" xml:space="preserve">{{multiple issues|
{{context|date=August 2016}}
{{notability|Products|date=August 2016}}
}}
{{Infobox software
 |name = Babelfy
 |logo = [[File:Babelfy_logo.png|140px|Babelfy logo.]]
 |screenshot =
 |caption = Babelfy
 |developer = 
 |released = 
 |latest_release_version = Babelfy 1.0
 |latest_release_date = June 2014
 |genre = {{Flatlist|
* [[Word sense disambiguation]]
* [[Entity linking]]
}}
 |programming language = 
 |license = [[Attribution-NonCommercial-ShareAlike 3.0 Unported]]
 |website = {{URL|babelfy.org}}
 |alexa   = 
}}

'''Babelfy''' is an [[algorithm]] for the disambiguation of text written in any language. Specifically, Babelfy performs the tasks of [[Multilinguality|multilingual]] [[Word Sense Disambiguation]] (i.e., the disambiguation of common nouns, verbs, adjectives and adverbs) and [[Entity Linking]] (i.e. the disambiguation of mentions to encyclopedic entities like people, companies, places, etc.).&lt;ref&gt;A. Moro, A. Raganato, R. Navigli. [http://wwwusers.di.uniroma1.it/~navigli/pubs/TACL_2014_Babelfy.pdf Entity Linking meets Word Sense Disambiguation: a Unified Approach]. Transactions of the Association for Computational Linguistics (TACL), 2, pp. 231-244, 2014.&lt;/ref&gt; Babelfy is based on the [[BabelNet]] multilingual semantic network and performs disambiguation and entity linking in three steps:

* It associates with each [[Vertex (graph theory)|vertex]] of the BabelNet semantic network, i.e., either [[concept]] or [[named entity]], a semantic signature, that is, a set of related vertices. This is a preliminary step which needs to be performed only once, independently of the input text.
* Given an input [[Written text|text]], it extracts all the linkable fragments from this text and, for each of them, lists the possible [[meaning (linguistics)|meanings]] according to the [[semantic network]].
* It creates a [[Graph (data structure)|graph-based]] semantic interpretation of the whole text by linking the candidate meanings of the extracted fragments using the previously-computed semantic signatures. It then extracts a dense [[Glossary of graph theory#Subgraphs|subgraph]] of this representation and selects the best candidate meaning for each fragment.

As a result, the text, written in any of the 271 [[language]]s supported by BabelNet, is output with possibly overlapping semantic annotations.

==See also==
* [[BabelNet]]
* [[Entity linking]]
* [[Multilinguality]]
* [[Word sense disambiguation]]

== References ==
{{reflist}}

== External links ==
* {{Official website|http://babelfy.org}}

[[Category:Lexical semantics]]
[[Category:Semantics]]
[[Category:Knowledge representation]]
[[Category:Computational linguistics]]
[[Category:Artificial intelligence]]
[[Category:Multilingualism]]


{{prog-lang-stub}}</text>
      <sha1>q5dslziv8z3z3x7qaiylxx2l9kly41y</sha1>
    </revision>
  </page>
  <page>
    <title>Flex expert system</title>
    <ns>0</ns>
    <id>43757720</id>
    <revision>
      <id>720949065</id>
      <parentid>684958194</parentid>
      <timestamp>2016-05-18T22:22:51Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor />
      <comment>Remove blank line(s) between list items per [[WP:LISTGAP]] to fix an accessibility issue for users of [[screen reader]]s. Do [[WP:GENFIXES]] and cleanup if needed. Discuss this at [[Wikipedia talk:WikiProject Accessibility#LISTGAP]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5039" xml:space="preserve">{{multiple issues|
{{COI|date=September 2014}}
{{notability|Products|date=September 2014}}
{{third-party|date=September 2014}}
}}
Flex is a hybrid expert system toolkit developed by [[Logic Programming Associates|LPA]] which incorporates [[Frame language|frame-based]] reasoning with inheritance, [[rule-based programming]] and data-driven procedures.

Flex supports both forwards and backward chaining, and they can be interleaved.

Flex provides its own English-like Knowledge Specification Language, KSL, which helps ensure that knowledge-bases are readable by domain experts. Flex KSL can now be generated automatically for certain classes of problems from [[VisiRule]].

Flex is implemented in, and has access to, [[Prolog]]. As opposed to most expert system shells, which tend to be constrained, Flex is an open toolkit.

Flex has proved very popular in education and was licensed to the Open University as part of T396: 'Artificial intelligence for technology'.

Much of the material for this course is described by Prof Adrian Hopgood in his book: Intelligent Systems for Engineers and Scientists, Third Edition, and on his web-site.&lt;ref name = "AI Toolkit"&gt;{{citation |url=http://www.adrianhopgood.com/aitoolkit/aitoolkit.shtml | title=AI toolkit: support site for Intelligent Systems for Engineers and Scientists, Third Edition by Adrian Hopgood}}&lt;/ref&gt;

There is also a Flex tutorial on the LPA web-site.&lt;ref name = "Flex Tutorial"&gt;{{citation |url=http://www.lpa.co.uk/ftp/5000/flx_tut.pdf | title=Flex Tutorial by Clive Spenser on LPA web-site}}&lt;/ref&gt;

Flex has been used to power AllerGenius, an expert system specifically developed by leading allergologists to help interpret the results of modern in vitro allergy tests such as the ImmunoCAP ISAC. These tests can typically measure specific antibodies to more than 100 allergen components from more than 50 pre-selected allergen sources and require a lot of expert interpretation.&lt;ref name = "Allergenius"&gt;{{citation |url=http://www.allergenius.it/new/index.php/en/general-conceps/2-non-categorizzato/139-the-structure-of-the-exper-system | title=Allergenius web site}}&lt;/ref&gt;

==External links==
*[http://www.generation5.org/content/2001/prg04.asp "Introduction to Flex/KSL (Part I)", James Mathhews, Generation5]
*[http://www.lpa.co.uk/flx_det.htm Flex Technical Details], LPA
*[http://www.lpa.co.uk/wfs_dem.htm WebFlex demos], LPA
*[http://www.intbis.com/intbis_pages/train.php Flex Training], IbIS
*[http://dl.acm.org/citation.cfm?id=297981 "A flex-based expert system for sewage treatment works support", Dixon et al, PCAI Magazine]
*[http://www.lamsade.dauphine.fr/~tsoukias/papers/esse.pdf ESSE: An Expert System for Software Evaluation]
*[http://4c.ucc.ie/web/upload/publications/inProc/KCCP-2007%20Dokas-Nordlander-Wallace.pdf "Fuzzy Fault Tree Representation and Maintenance based on Frames and Constraint Technologies: A Case Study", Dokas, Nordlander and Wallace]
*[https://repositorium.sdum.uminho.pt/bitstream/1822/8868/1/A%20Knowledge-Based%20System%20for%20Spinning%20Management.pdf “A Knowledge-Based System for Spinning Management”]
*[http://iraj.in/up_proc/pdf/86-140412387293-96.pdf A NOVEL APPROACH FOR EXPERT SYSTEM AIDED DATACENTER DESIGN]
*[http://www.cscjournals.org/csc/manuscript/Journals/IJAE/volume1/Issue2/IJAE-10.pdf “An Expert System using A Decision Logic Charting Approach for Indian Legal Domain With specific reference to Transfer of Property Act”], N B Bilgi, Dr. R V Kulkarni &amp; C. Spenser
*[http://www.ijser.org/researchpaper%5CAN-EXPERT-SYSTEM-FOR-SEISMIC-DATA-INTERPRETATION.pdf “An expert system for Seismic data interpretation using visual and analytical tools”], Neelu Jyothi Ahuja and Parag Diwan
*[http://pubcouncil.kuniv.edu.kw/jer/files/19Nov2012102247An%20expert%20system%20machinability%20data%20bank%20%28ESMDB%29%20approach..pdf “An expert system machinability data bank”]
*[http://www.iis.sinica.edu.tw/APEC02/Program/chingyeh.pdf “Development of an Ontology-Based Portal for Digital Archive Services”], Ching-Long Yeh
*[http://orbit.dtu.dk/fedora/objects/orbit:88354/datastreams/file_7703263/content “A development process meta-model for Web based expert systems: the Web engineering point of view”], Ioannis M. Dokas and Alexandre Alapetite
*[http://www.pacis-net.org/file/1997/75.pdf “Behavioural issues in Information Systems Design”], Mike McGrath
*[http://www.slaai.lk/proc/2006/chatura.pdf “Artificial Intelligence Approach to Effective Career Guidance”], Chathra Hendahewa et al
*[http://www.icsd.aegean.gr/kkemalis/pubs/SETN_CAMES.pdf “DYNAMIC ACCESS CONTROL MANAGEMENT USING EXPERT SYSTEM TECHNOLOGY”], Prof. G. Pangalos et al
*[http://www.waojournal.org/content/7/1/15 “Allergenius, an expert system for the interpretation of allergen microarray results”], Giovanni Melioli, Clive Spenser et al

== References ==
{{Reflist}}

[[Category:Expert systems]]
[[Category:Rule engines]]
[[Category:Knowledge engineering]]
[[Category:Knowledge representation]]


{{compu-ai-stub}}</text>
      <sha1>plgb3cmsh68u251v9wdauju9ifs1mu7</sha1>
    </revision>
  </page>
  <page>
    <title>PowToon</title>
    <ns>0</ns>
    <id>38027627</id>
    <revision>
      <id>758471365</id>
      <parentid>758306928</parentid>
      <timestamp>2017-01-05T17:05:06Z</timestamp>
      <contributor>
        <username>Pinkbeast</username>
        <id>11291690</id>
      </contributor>
      <comment>/* History */ Please actually check when adding wikilinks that they go to the right place</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3054" xml:space="preserve">{{Underlinked|date=March 2014}}
{{Infobox company
| name = PowToon
| type = [[Private company|Private]]
| foundation = {{Start date and age|2012}}
| location_city =  28 Church Rd&lt;br /&gt;[[London, UK]]
| location_country = [[United Kingdom]]
| key_people = Ilya Spitalnik (Co-Founder and CEO), Daniel Zaturansky (Co-Founder and COO), Sven Hoffman (Co-Founder and CTO)
| industry = [[Internet Marketing]]
| products = PowToon Web-based animation software
| homepage = {{URL|www.powtoon.com}}
}}
'''PowToon''' is a company which sells cloud-based software [[Software as a service|(SaaS)]] for creating animated presentations and animated explainer videos.&lt;ref&gt;Perez, Sarah. [http://techcrunch.com/2012/06/26/now-everyone-can-make-marketing-videos-powtoon-launches-diy-presentation-tool/ TechCrunch], June 26th, 2012, "Now Everyone Can Make Marketing Videos: PowToon Launches DIY Presentation Tool"&lt;/ref&gt;

== History ==
PowToon was founded in January 2012. The company released a [[Software_release_life_cycle#BETA|beta]] version in August 2012 and has seen fast subscriber growth since.&lt;ref name="powtoon"&gt;[http://www.powtoon.com Powtoon Website]&lt;/ref&gt; In December 2012 PowToon secured $600,000 investment from LA based Venture Capital firm Startup Minds.&lt;ref&gt;Perez, Sarah, [http://techcrunch.com/2012/12/14/diy-animation-platform-powtoon-grabs-600k-for-its-video-creation-software/ TechCrunch] , Dec 14, 2012, "DIY Animation Platform PowToon Grabs $600K For Its Video Creation Software"&lt;/ref&gt; In February 2013 PowToon introduced a free account option allowing users to create animated videos that can be exported to [[YouTube]]. The free videos include the PowToon branding.

== Product ==
PowToon is Web-based animation software that allows users to create animated presentations  by manipulating pre-created objects, imported images, provided music and user created voice-overs.&lt;ref&gt;{{cite web|last=Mersand |first=Shannon |title=Product Review: PowToon|url=http://www.techlearning.com/product-reviews/0072/product-review-powtoon-/54971|publisher=''Tech and Learning''|accessdate=12 May 2014|date=May 2014}}&lt;/ref&gt; 
Powtoon uses an [[Apache Flex]] engine to generate an XML file that can be played in the Powtoon online viewer, exported to YouTube or downloaded as an MP4 file.&lt;ref name="powtoon" /&gt;

PowToon is also available on the Google Chrome Store&lt;ref&gt;{{citation |title=PowToon - Chrome Web Store|url=https://chrome.google.com/webstore/detail/powtoon/aomfhbjiekjcbeefclbidjgnikfbooem?hl=en|accessdate=25 February 2015|date=Feb 2015}}&lt;/ref&gt;  and has an application on Edmodo.com.&lt;ref&gt;{{citation |title=PowToon by PowToon Ltd|url=https://www.edmodo.com/store/app/powtoon-1|accessdate=25 February 2015|date=Feb 2015}}&lt;/ref&gt;

== References ==
{{reflist}}

== External links ==
* {{official website|http://www.powtoon.com/}}

[[Category:Animation software]]
[[Category:Companies established in 2012]]
[[Category:Cloud applications]]
[[Category:Computer animation]]
[[Category:Presentation software]]
[[Category:Knowledge representation]]

{{animation-stub}}</text>
      <sha1>cedlou6fgmuj7yhnweqsede3vdhdb2y</sha1>
    </revision>
  </page>
  <page>
    <title>Semantic network</title>
    <ns>0</ns>
    <id>29109</id>
    <revision>
      <id>757911417</id>
      <parentid>757910889</parentid>
      <timestamp>2017-01-02T12:21:55Z</timestamp>
      <contributor>
        <username>Krauss</username>
        <id>1222358</id>
      </contributor>
      <minor />
      <comment>/* See also */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="12503" xml:space="preserve">{{Network Science}}

A '''semantic network''', or '''frame network''',  is a network that represents [[Semantics|semantic]] relations between [[concept]]s. This is often used as a form of [[Knowledge representation and reasoning|knowledge representation]]. It is a [[directed graph|directed]] or [[undirected graph]] consisting of [[vertex (graph theory)|vertices]], which represent [[concept]]s, and [[graph theory|edges]], which represent semantic relations between concepts.&lt;ref name = 'Sowa'/&gt;

Typical standardized semantic networks are expressed as [[semantic triple]]s.

== History ==
[[Image:Semantic Net.svg|thumb|320px|Example of a semantic network]]
"Semantic Nets" were first invented for [[computers]] by [[Richard H. Richens]] of the Cambridge Language Research Unit in 1956 as an "[[Pivot language|interlingua]]" for [[machine translation]] of [[natural language]]s.{{citation needed|date=October 2013}}

They were independently developed by Robert F. Simmons,&lt;ref name='Simmons1963'&gt;{{cite journal | title=Synthetic language behavior | journal=Data Processing Management | year=1963 | last=Robert F. Simmons |volume=5 |issue=12 |pages=11–18}}&lt;/ref&gt; Sheldon Klein, Karen McConologue, M. Ross Quillian&lt;ref name='Quillian1963'&gt;Quillian, R. A notation for representing conceptual information: An application to semantics and mechanical English para- phrasing. SP-1395, System Development Corporation, Santa Monica, 1963.&lt;/ref&gt; and others at [[System Development Corporation]] in the early 1960s as part of the SYNTHEX project. It later featured prominently in the work of [[Allan M. Collins]] and Quillian (e.g., Collins and Quillian;&lt;ref name='Collins1969'&gt;{{cite journal | title=Retrieval time from semantic memory | journal=Journal of verbal learning and verbal behavior | year=1969 | last1=Allan M. Collins |author2= M. R. Quillian |volume=8 |issue=2 |pages=240–247 |doi=10.1016/S0022-5371(69)80069-1  }}&lt;/ref&gt;&lt;ref name='Collins1970'&gt;{{cite journal |title=Does category size affect categorization time? |journal=Journal of verbal learning and verbal behavior |year=1970 |first= |last=Allan M. Collins
|author2=M. Ross Quillian  |volume=9 |issue=4 |pages=432–438 |doi=10.1016/S0022-5371(70)80084-6 }}&lt;/ref&gt; Collins and Loftus&lt;ref name='Collins1975'&gt;{{cite journal |title=A spreading-activation theory of semantic processing |journal=Psychological Review |year=1975 |last=Allan M. Collins |author2=Elizabeth F. Loftus |volume=82 | doi = 10.1037/0033-295x.82.6.407 |pages=407–428}}&lt;/ref&gt; Quillian&lt;ref&gt;Quillian, M. R. (1967). Word concepts: A theory and simulation of some basic semantic capabilities. Behavioral Science, 12(5), 410-430.&lt;/ref&gt;&lt;ref&gt;Quillian, M. R. (1968). Semantic memory. Semantic information processing, 227–270.&lt;/ref&gt;&lt;ref&gt;{{cite journal | last1 = Quillian | first1 = M. R. | year = 1969 | title = The teachable language comprehender: a simulation program and theory of language | url = | journal = Communications of the ACM | volume = 12 | issue = 8| pages = 459–476 | doi=10.1145/363196.363214}}&lt;/ref&gt;&lt;ref&gt;Quillian, R. Semantic Memory. Unpublished doctoral dissertation, Carnegie Institute of Technology, 1966.&lt;/ref&gt;)

In the late 1980s, two [[Netherlands]] universities, [[University of Groningen|Groningen]] and [[University of Twente|Twente]], jointly began a project called ''Knowledge Graphs'', which are semantic networks but with the added constraint that edges are restricted to be from a limited set of possible relations, to facilitate algebras on the graph.&lt;ref&gt;{{cite book |last=Van de Riet |first=R. P. |date=1992 |title=Linguistic Instruments in Knowledge Engineering |url=http://www.stokman.org/artikel/92Jame.KnowGraphs.LIKE.pdf |publisher=Elsevier Science Publishers |page=98 |isbn=0444883940}}&lt;/ref&gt; In the subsequent decades, the distinction between semantic networks and knowledge graphs was blurred.&lt;ref&gt;{{cite conference |url=https://books.google.com/books?id=15PDCgAAQBAJ&amp;pg=PA444 |title=Path-Based Semantic Relatedness on Linked Data and Its Use to Word and Entity Disambiguation |last1=Hulpus |first1=Ioana |last2=Prangnawarat |first2=Narumol |date=2015 |publisher=Springer International Publishing |book-title=The Semantic Web - ISWC 2015: 14th International Semantic Web Conference, Bethlehem, PA, USA, October 11-15, 2015, Proceedings, Part 1 |pages=444 |conference=[[International Semantic Web Conference]] 2015}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=https://www.authorea.com/users/6341/articles/107281 |title=What is a Knowledge Graph? |last1=McCusker |first1=James P. |last2=Chastain |first2=Katherine |date=April 2016 |website=authorea.com |access-date=15 June 2016 |quote="usage [of the term 'knowledge graph'] has evolved"}}&lt;/ref&gt; In 2012, [[Google]] gave their knowledge graph the name [[Knowledge Graph]].

== Basics of semantic networks ==
A semantic network is used when one has knowledge that is best understood as a set of concepts that are related to one another.

Most semantic networks are cognitively based. They also consist of arcs and nodes which can be organized into a taxonomic hierarchy. Semantic networks contributed ideas of [[spreading activation]], [[inheritance]], and nodes as proto-objects.

== Examples ==

=== Semantic Net in [[Lisp (programming language)|Lisp]] ===
Using an association list.
&lt;source lang="lisp"&gt;
(defun *database* ()
'((canary  (is-a bird)
           (color yellow)
           (size small))
  (penguin (is-a bird)
           (movement swim))
  (bird    (is-a vertebrate)
           (has-part wings)
           (reproduction egg-laying))))
&lt;/source&gt;

You would use the "assoc" function with a key of "canary" to extract all the information about the "canary" type.&lt;ref&gt;{{cite web|last=Swigger|first=Kathleen|title=Semantic.ppt|url=http://zeus.csci.unt.edu/swigger/csci3210/semantic.ppt|accessdate=23 March 2011}}&lt;/ref&gt;

=== WordNet ===
{{Main|WordNet}}
An example of a semantic network is [[WordNet]], a [[lexicon|lexical]] database of [[English language|English]]. It groups English words into sets of synonyms called [[synsets]], provides short, general definitions, and records the various semantic relations between these synonym sets. Some of the most common semantic relations defined are [[meronymy]] (A is part of B, i.e. B has A as a part of itself), [[holonymy]] (B is part of A, i.e. A has B as a part of itself), [[hyponym]]y (or [[troponymy]])  (A is subordinate of B; A is kind of B), [[hypernym]]y (A is superordinate of B), [[synonym]]y (A denotes the same as B) and [[antonym]]y (A denotes the opposite of B).

WordNet properties have been studied from a [[Graph theory|network theory]] perspective and compared to other semantic networks created from [[Roget's Thesaurus]] and [[word association]] tasks.  From this perspective the three of them are a [[Small-world network|small world structure]].&lt;ref name=Steyvers2005&gt;{{cite journal
 | author = Steyvers, M.
 |author2=Tenenbaum, J.B.
  | year = 2005
 | title = The Large-Scale Structure of Semantic Networks: Statistical Analyses and a Model of Semantic Growth
 | journal = Cognitive Science
 | volume = 29
 | issue = 1
 | pages = 41–78
 | doi = 10.1207/s15516709cog2901_3
}}&lt;/ref&gt;

=== Other examples ===
It is also possible to represent logical descriptions using semantic networks such as the [[existential graph]]s of [[Charles Sanders Peirce]] or the related [[conceptual graph]]s of [[John F. Sowa]].&lt;ref name='Sowa'&gt;{{cite encyclopedia
|author=John F. Sowa
|editor=Stuart C Shapiro
|encyclopedia=Encyclopedia of Artificial Intelligence
|title=Semantic Networks
|url=http://www.jfsowa.com/pubs/semnet.htm
|accessdate=2008-04-29
|year=1987
|authorlink=John F. Sowa}}&lt;/ref&gt; These have expressive power equal to or exceeding standard [[first-order predicate calculus|first-order predicate logic]].  Unlike WordNet or other lexical or browsing networks, semantic networks using these representations can be used for reliable automated logical deduction.  Some automated reasoners exploit the graph-theoretic features of the networks during processing.

Other examples of semantic networks are [[Gellish]] models. [[Gellish English]] with its [[Gellish English dictionary]], is a [[formal language]] that is defined as a network of relations between concepts and names of concepts. Gellish English is a formal subset of natural English, just as Gellish Dutch is a formal subset of Dutch, whereas multiple languages share the same concepts. Other Gellish networks consist of knowledge models and information models that are expressed in the Gellish language. A Gellish network is a network of (binary) relations between things. Each relation in the network is an expression of a fact that is classified by a relation type. Each relation type itself is a concept that is defined in the Gellish language dictionary. Each related thing is either a concept or an individual thing that is classified by a concept. The definitions of concepts are created in the form of definition models (definition networks) that together form a Gellish Dictionary. A Gellish network can be documented in a Gellish database and is computer interpretable.

[[SciCrunch]] is a collaboratively edited knowledge base for scientific resources. It provides unambiguous identifiers (Research Resource IDentifiers or RRIDs) for software, lab tools etc. and it also provides options to create links between RRIDs and from communities.

Another example of semantic networks, based on [[category theory]], is [[olog]]s. Here each type is an object, representing a set of things, and each arrow is a morphism, representing a function. [[Commutative diagrams]] also are prescribed to constrain the semantics.

In the social sciences people sometimes use the term semantic network to refer to [[co-occurrence networks]].&lt;ref name='Atteveldt'&gt;{{cite book
|author=Wouter Van Atteveldt
|title=Semantic Network Analysis: Techniques for Extracting, Representing, and Querying Media Content
|publisher=BookSurge Publishing
|year=2008}}&lt;/ref&gt; The basic idea is that words that co-occur in a unit of text, e.g. a sentence, are semantically related to one another. Ties based on co-occurrence can then be used to construct semantic networks.

== Software tools ==
There are also elaborate types of semantic networks connected with corresponding sets of software tools used for [[Lexicon|lexical]] [[knowledge engineering]], like the Semantic Network Processing System ([[SNePS]]) of Stuart C. Shapiro&lt;ref&gt;[http://www.cse.buffalo.edu/~shapiro/ Stuart C. Shapiro]&lt;/ref&gt; or the [[MultiNet]] paradigm of Hermann Helbig,&lt;ref&gt;[http://pi7.fernuni-hagen.de/helbig/index_en.html Hermann Helbig]&lt;/ref&gt; especially suited for the semantic representation of natural language expressions and used in several [[Natural language processing|NLP]] applications.

Semantic networks are used in specialized information retrieval tasks, such as [[plagiarism]] detection. They provide information on hierarchical relations in order to employ [[semantic compression]] to reduce language diversity and enable the system to match word meanings, independently from sets of words used.

== See also ==
{{Div col}}
* [[Abstract semantic graph]]
* [[Chunking (psychology)]]
* [[Network diagram]]
* [[Ontology (information science)]]
* [[Repertory grid]]
* [[Semantic lexicon]]
* [[Semantic neural network]]
* [[SemEval]] - an ongoing series of evaluations of [[Semantic analysis (computational)|computational semantic analysis]] systems
* [[Sparse distributed memory]]
* [[Taxonomy (general)]]
* [[Unified Medical Language System]] (UMLS)
* [[Word-sense disambiguation]] (WSD)
{{Div col end}}

=== Other examples ===
* [[Cognition Network Technology]]
* [[Lexipedia]]
* [[Open Mind Common Sense]] (OMCS)
* [[Schema.org]]
* [[SNOMED CT]]
* [[Universal Networking Language]] (UNL)
* [[Wikidata]]

== References ==
{{reflist|30em}}

== Further reading ==
* Allen, J. and A. Frisch (1982). "What's in a Semantic Network". In: ''Proceedings of the 20th. annual meeting of ACL'', Toronto, pp.&amp;nbsp;19–27.
* John F. Sowa, Alexander Borgida (1991). ''Principles of Semantic Networks: Explorations in the Representation of Knowledge''.

== External links ==
{{Commons category|Semantic networks}}
* [http://www.jfsowa.com/pubs/semnet.htm "Semantic Networks"] by John F. Sowa
* [http://www.knowledgegrid.net/~H.Zhuge/SLN.htm "Semantic Link Network" ] by Hai Zhuge

{{Semantic Web}}
{{Use dmy dates|date=August 2011}}

{{Authority control}}

[[Category:Knowledge representation]]
[[Category:Networks]]</text>
      <sha1>l9kyidky576ats53bz60sym9m52rm9i</sha1>
    </revision>
  </page>
  <page>
    <title>Category:Library of Congress Classification</title>
    <ns>14</ns>
    <id>1138791</id>
    <revision>
      <id>671894608</id>
      <parentid>547541476</parentid>
      <timestamp>2015-07-17T19:14:13Z</timestamp>
      <contributor>
        <username>Fgnievinski</username>
        <id>6727347</id>
      </contributor>
      <comment>added [[Category:Knowledge representation]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="169" xml:space="preserve">{{Cat main|Library of Congress Classification}}
[[Category:Library of Congress]]
[[Category:Library cataloging and classification]]
[[Category:Knowledge representation]]</text>
      <sha1>7pn5rd3bi0rt3jmz591amovw8gfwbpw</sha1>
    </revision>
  </page>
  <page>
    <title>Flail space model</title>
    <ns>0</ns>
    <id>47369663</id>
    <revision>
      <id>747918838</id>
      <parentid>747918816</parentid>
      <timestamp>2016-11-05T05:16:35Z</timestamp>
      <contributor>
        <username>Materialscientist</username>
        <id>7852030</id>
      </contributor>
      <minor />
      <comment>Reverted edits by [[Special:Contributions/104.173.100.38|104.173.100.38]] ([[User talk:104.173.100.38|talk]]): Unexplained removal of content ([[WP:HG|HG]]) (3.1.21)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4767" xml:space="preserve">The '''flail space model (FSM)''' is a [[Physical model|model]] of how a [[passenger|car passenger]] moves in a [[vehicle]] that collides with a roadside feature such as a [[Traffic barrier|guardrail]] or a [[crash cushion]]. Its principal purpose is to assess the potential risk of harm to the hypothetical occupant as he or she impacts the interior of the passenger compartment and, ultimately, the efficacy of an experimental roadside feature undergoing full-scale vehicle crash testing.

The FSM eliminates the complexity and expense of using instrumented [[Crash test dummy|anthropometric dummies]] during the crash test experiments. Furthermore, while crash test dummies were developed to model collisions between vehicles, they are not accurate when used for the sorts of collision angles that occur when a vehicle collides with a roadside feature; by contrast, the FSM was designed for such collisions.&lt;ref name="gabauer"&gt;Gabauer, Douglas, "A methodology to evaluate the flail space model using event data recorder technology", Department of Mechanical Engineering, Rowan University, Glassboro, NJ, 2004.&lt;/ref&gt;

== History ==
The FSM is based on research performed at [[Southwest Research Institute]] in 1980&lt;ref&gt;Michie, J. D., "Development of improved criteria for evaluating safety performance of highway appurtenances", Final Report of  Internal Research Project No. 03-9254, Southwest Research Institute, San Antonio, Texas, June 1980.&lt;/ref&gt; and published in 1981 in the paper entitled "Collision Risk Assessment Based on Occupant Flail-Space Model" by Jarvis D. Michie.&lt;ref name=":0"&gt;Michie, J. D., "Collision risk assessment based on occupant flail space model," in Transportation Research Record 796, 1981, pp. 1–9.&lt;/ref&gt; The FSM (coined by Michie) was accepted by the highway community and published as a key part of the "Recommended Procedures for the Safety Evaluation of Highway Appurtenances" published in 1981 in [[National Cooperative Highway Research Program]] (NCHRP) Report 230.&lt;ref&gt;Michie, J. D.  National Cooperative Highway Research Program Report 230: Recommended Procedures for the Safety Performance Evaluation of Highway Appurtenances.  NCHRP Transportation Research Board, Washington, DC, March 1981.&lt;/ref&gt; In 1993, the NCHRP Report was updated and presented as NCHRP Report 350;&lt;ref&gt;Ross, H. E., Jr. et al.  National Cooperative Highway Research Program Report 350:  Recommended Procedures for the Safety Evaluation of Highway Features.  NCHRP Transportation Research Board, Washington, DC, 1993.&lt;/ref&gt; in this research effort performed by the [[Texas A&amp;M Transportation Institute|Texas Transportation Research Institute]], the FSM was reexamined and was unmodified in the new publication. In 2004, Douglas Gabauer further examined the efficacy of the FSM in his [[PhD thesis]].&lt;ref name="gabauer" /&gt; The [[American Association of State Highway and Transportation Officials]] (AASHTO) retained the FSM as the method of assessing the risk of harm to vehicle occupants in the 2009 "Manual for Assessing Safety Hardware" that replaced NCHRP Report 350, stating that the FSM had "served its intended purpose well".&lt;ref&gt;Manual for Assessing Safety Hardware.  American Association of State Highway and Transportation Officials, Washington, DC,  2009.&lt;/ref&gt;

== Details ==
The FSM hypothesis divides the collision into two stages.  In stage one, the unrestrained occupant is propelled forward and sideways in the compartment space due to vehicle collision [[Acceleration|accelerations]] and then impacts one or more surfaces (including the steering wheel) with velocity "V". According to the model, the vehicle (instead of the occupant) is the object that is accelerating. The occupant experiences no injury-producing force prior to contact with the compartment surfaces.&lt;ref name=":0" /&gt;

In stage two, the occupant is assumed to remain in contact with the compartment surface and experiences the same accelerations as the vehicle for the rest of the collision.  The occupant may sustain [[Blunt trauma|injury]] at the end of stage one based on the velocity of impact with the compartment surfaces and due to vehicle accelerations during stage two.  The occupant impact velocity and acceleration are computed from the vehicle collision acceleration history and the compartment geometry.  Finally, the hypothetical occupant impact velocity and acceleration are then compared to threshold values of [[Engineering tolerance|human tolerance]] to these forces.&lt;ref name=":0" /&gt;

==References==
{{reflist|colwidth=30em}}

[[Category:Articles created via the Article Wizard]]
[[Category:Scientific modeling]]
[[Category:Applied mathematics]]
[[Category:Knowledge representation]]
[[Category:Mathematical modeling]]
[[Category:Transport safety]]</text>
      <sha1>i20l5qnty5ur8n1rljkfu169uj0rmat</sha1>
    </revision>
  </page>
  <page>
    <title>Unified Modeling Language</title>
    <ns>0</ns>
    <id>32169</id>
    <revision>
      <id>760575574</id>
      <parentid>760106171</parentid>
      <timestamp>2017-01-17T21:02:30Z</timestamp>
      <contributor>
        <ip>139.102.14.206</ip>
      </contributor>
      <comment>Internal link to "Silver Bullet" goes to page about actual bullets made of silver.  Changed link to point to article about paper "No Silver Bullet".</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="20621" xml:space="preserve">{{Use American English|date=January 2012}}
[[File:UML logo.gif|thumb|UML logo]]
The '''Unified Modeling Language''' ('''UML''') is a general-purpose, developmental,  [[modeling language]] in the field of [[software engineering]], that is intended to provide a standard way to visualize the design of a system.&lt;ref name=":1" /&gt;

UML was originally motivated by the desire to standardize the disparate notational systems and approaches to software design developed by [[Grady Booch]], [[Ivar Jacobson]] and [[James Rumbaugh]] at [[Rational Software]] in 1994–1995, with further development led by them through 1996.&lt;ref name=":1" /&gt;

In 1997 UML was adopted as a standard by the [[Object Management Group]] (OMG), and has been managed by this organization ever since. In 2005 UML was also published by the [[International Organization for Standardization]] (ISO) as an approved ISO standard.&lt;ref&gt;{{cite web|url=http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=32620 |title=ISO/IEC 19501:2005 - Information technology - Open Distributed Processing - Unified Modeling Language (UML) Version 1.4.2 |publisher=Iso.org |date=2005-04-01 |accessdate=2015-05-07}}&lt;/ref&gt; Since then it has been periodically revised to cover the latest revision of UML.&lt;ref&gt;{{cite web|url=http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=32624 |title=ISO/IEC 19505-1:2012 - Information technology - Object Management Group Unified Modeling Language (OMG UML) - Part 1: Infrastructure |publisher=Iso.org |date=2012-04-20 |accessdate=2014-04-10}}&lt;/ref&gt;

== History ==

[[File:OO Modeling languages history.jpg|thumb|320px|History of object-oriented methods and notation]]

=== Before UML 1.x ===

UML has been evolving since the second half of the 1990s and has its roots in the object-oriented methods developed in the late 1980s and early 1990s. The timeline (see image) shows the highlights of the history of object-oriented modeling methods and notation.

It is originally based on the notations of the [[Booch method]], the [[object-modeling technique]] (OMT) and [[object-oriented software engineering]] (OOSE), which it has integrated into a single language.&lt;ref name=":0" /&gt;

[[Rational Software Corporation]] hired [[James Rumbaugh]] from [[General Electric]] in 1994 and after that the company became the source for two of the most popular object-oriented modeling approaches of the day:&lt;ref&gt;Andreas Zendler (1997) ''Advanced Concepts, Life Cycle Models and Tools for Objeckt-Oriented Software Development''. p.122&lt;/ref&gt; Rumbaugh's [[object-modeling technique]] (OMT) and [[Grady Booch]]'s method. They were soon assisted in their efforts by [[Ivar Jacobson]], the creator of the [[object-oriented software engineering]] (OOSE) method, who joined them at Rational in 1995.&lt;ref name=":1"&gt;{{cite book
 | title = Unified Modeling Language User Guide, The
 | publisher = Addison-Wesley
 | edition = 2
 | year = 2005
 | page = 496
 | url = http://www.informit.com/store/unified-modeling-language-user-guide-9780321267979
 | isbn = 0321267974
}}
, See the sample content, look for history&lt;/ref&gt;

=== UML 1.x ===

Under the technical leadership of those three (Rumbaugh, Jacobson and Booch), a consortium called the [[UML Partners]] was organized in 1996 to complete the ''Unified Modeling Language (UML)'' specification, and propose it to the Object Management Group (OMG) for standardisation. The partnership also contained additional interested parties (for example [[Hewlett-Packard|HP]], [[Digital Equipment Corporation|DEC]], [[IBM]] and [[Microsoft]]). The UML Partners' UML 1.0 draft was proposed to the OMG in January 1997 by the consortium. During the same month the UML Partners formed a group, designed to define the exact meaning of language constructs, chaired by [[Cris Kobryn]] and administered by Ed Eykholt, to finalize the specification and integrate it with other standardization efforts. The result of this work, UML 1.1, was submitted to the OMG in August 1997 and adopted by the OMG in November 1997.&lt;ref name=":1" /&gt;&lt;ref&gt;{{cite web|url=http://www.omg.org/cgi-bin/doc?ad/97-08-11 |title=UML Specification version 1.1 (OMG document ad/97-08-11) |publisher=Omg.org |accessdate=2011-09-22}}&lt;/ref&gt;

After the first release a task force was formed&lt;ref name=":1" /&gt; to improve the language, which released several minor revisions, 1.3, 1.4, and 1.5.&lt;ref&gt;{{cite web|url=http://www.omg.org/spec/UML/ |title=UML |publisher=Omg.org |accessdate=2014-04-10}}&lt;/ref&gt;

The standards it produced (as well as the original standard) have been noted as being ambiguous and inconsistent.&lt;ref&gt;Génova et alia 2004 "Open Issues in Industrial Use Case Modeling"&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.uml-forum.com/docs/papers/CACM_Jan02_p107_Kobryn.pdf |title=Will UML 2.0 Be Agile or Awkward? |format=PDF |accessdate=2011-09-22}}&lt;/ref&gt;

=== UML 2.x ===

UML 2.0 major revision replaced version 1.5 in 2005, which was developed with an enlarged consortium to improve the language further to reflect new experience on usage of its features.&lt;ref&gt;{{cite web|url=http://www.omg.org/spec/UML/2.0/ |title=UML 2.0 |publisher=Omg.org |accessdate=2011-09-22}}&lt;/ref&gt;

Although UML 2.1 was never released as a formal specification, versions 2.1.1 and 2.1.2 appeared in 2007, followed by UML 2.2 in February 2009. UML 2.3 was formally released in May 2010.&lt;ref name="spec"&gt;{{cite web|url=http://www.omg.org/spec/UML/ |title=UML |publisher=Omg.org |accessdate=2011-09-22}}&lt;/ref&gt; UML 2.4.1 was formally released in August 2011.&lt;ref name="spec"/&gt; UML 2.5 was released in October 2012 as an "In process" version and was officially released in June 2015.&lt;ref name="spec"/&gt;

There are four parts to the UML 2.x specification:

# The Superstructure that defines the notation and semantics for diagrams and their model elements
# The Infrastructure that defines the core metamodel on which the Superstructure is based
# The [[Object Constraint Language]] (OCL) for defining rules for model elements
# The UML Diagram Interchange that defines how UML 2 diagram layouts are exchanged

The current versions of these standards follow: UML Superstructure version 2.4.1, UML Infrastructure version 2.4.1, OCL version 2.3.1, and UML Diagram Interchange version 1.0.&lt;ref name="Versions"&gt;{{cite web|author=OMG|title=OMG Formal Specifications (Modeling and Metadata paragraph)|url=http://www.omg.org/spec/#M&amp;M|accessdate = 2016-02-12}}&lt;/ref&gt; It continues to be updated and improved by the revision task force, who resolve any issues with the language.&lt;ref&gt;{{cite web|url=http://www.omg.org/issues/uml2-rtf.open.html |title=Issues for UML 2.6 Revision task Force mailing list |publisher=Omg.org |accessdate=2014-04-10}}&lt;/ref&gt;

== Design ==

UML offers a way to visualize a system's architectural blueprints in a diagram (see image), including elements such as:&lt;ref name=":0"&gt;{{cite web|url=http://www.omg.org/spec/UML/2.4.1/Superstructure/PDF |title=OMG Unified Modeling Language (OMG UML), Superstructure. Version 2.4.1 |publisher=Object Management Group |accessdate=9 April 2014}}&lt;/ref&gt;

* any [[Activity (UML)|activities]] (jobs);
* individual [[Component (UML)|components]] of the system;
** and how they can interact with other [[Component-based software engineering|software components]];
* how the system will run;
* how entities interact with others (components and interfaces);
* external [[user interface]].

Although originally intended for object-oriented design documentation, UML has been extended to a larger set of design documentation (as listed above),&lt;ref&gt;Satish Mishra (1997). [http://www2.informatik.hu-berlin.de/~hs/Lehre/2004-WS_SWQS/20050107_Ex_UML.ppt "Visual Modeling &amp; Unified Modeling Language (UML): Introduction to UML"]. Rational Software Corporation. Accessed 9 November 2008.&lt;/ref&gt; and been found useful in many contexts.&lt;ref name="UML, Success Stories"&gt;{{cite web|url=http://www.uml.org/uml_success_stories/index.htm|title=UML, Success Stories|accessdate=9 April 2014}}&lt;/ref&gt;

=== Software development methods ===

UML is not a development method by itself;&lt;ref&gt;John Hunt (2000). ''The Unified Process for Practitioners: Object-oriented Design, UML and Java''. Springer, 2000. ISBN 1-85233-275-1. p.5.door&lt;/ref&gt; however, it was designed to be compatible with the leading object-oriented software development methods of its time, for example [[Object-modeling technique|OMT]], [[Booch method]], [[Objectory]] and especially [[Rational Unified Process|RUP]] that it was originally intended to be used with when work began at Rational Software.

=== Modeling ===

It is important to distinguish between the UML model and the set of diagrams of a system. A diagram is a partial graphic representation of a system's model. The set of diagrams need not completely cover the model and deleting a diagram does not change the model. The model may also contain documentation that drives the model elements and diagrams (such as written use cases).

UML diagrams represent two different views of a system model:&lt;ref&gt;Jon Holt Institution of Electrical Engineers (2004). ''UML for Systems Engineering: Watching the Wheels'' IET, 2004, ISBN 0-86341-354-4. p.58&lt;/ref&gt;

* Static (or ''structural'') view: emphasizes the static structure of the system using objects, attributes, operations and relationships. It includes [[class diagram]]s and [[composite structure diagram]]s.
* Dynamic (or ''behavioral'') view: emphasizes the dynamic behavior of the system by showing collaborations among objects and changes to the internal states of objects. This view includes [[sequence diagram]]s, [[activity diagram]]s and [[UML state machine|state machine diagrams]].

UML models can be exchanged among UML tools by using the [[XML Metadata Interchange]] (XMI) format.

== Diagrams ==
{{UML diagram types}}

UML 2 has many types of diagrams, which are divided into two categories.&lt;ref name=":0" /&gt; Some types represent ''structural'' information, and the rest represent general types of ''behavior'', including a few that represent different aspects of ''interactions''. These diagrams can be categorized hierarchically as shown in the following class diagram:&lt;ref name=":0" /&gt;

[[File:UML diagrams overview.svg|center|600px|Hierarchy of UML 2.2 Diagrams, shown as a [[class diagram]]]]

These diagrams may all contain comments or notes explaining usage, constraint, or intent.

=== Structure diagrams ===

[[Structure diagram]]s emphasize the things that must be present in the system being modeled. Since structure diagrams represent the structure, they are used extensively in documenting the [[software architecture]] of software systems. For example, the [[component diagram]] describes how a software system is split up into components and shows the dependencies among these components.

&lt;gallery class="center"&gt;
Policy Admin Component Diagram.PNG|[[Component diagram]]
BankAccount1.svg|[[Class diagram]]
&lt;/gallery&gt;

=== Behavior diagrams ===

Behavior diagrams emphasize what must happen in the system being modeled. Since behavior diagrams illustrate the behavior of a system, they are used extensively to describe the functionality of software systems. As an example, the [[activity diagram]] describes the business and operational step-by-step activities of the components in a system.

&lt;gallery class="center"&gt;
Activity conducting.svg|[[Activity diagram]]
UML Use Case diagram.svg|[[Use case diagram]]
&lt;/gallery&gt;

==== Interaction diagrams ====

Interaction diagrams, a subset of behavior diagrams, emphasize the flow of control and data among the things in the system being modeled. For example, the [[sequence diagram]] shows how objects communicate with each other in terms of a sequence of messages.

&lt;gallery class="center"&gt;
CheckEmail.svg|[[Sequence diagram]]
UML Communication diagram.svg|[[Communication diagram]]
&lt;/gallery&gt;

== Meta modeling ==
{{Main article|Meta-Object Facility}}

[[File:M0-m3.png|thumb|320px|Illustration of the Meta-Object Facility]]

The Object Management Group (OMG) has developed a [[metamodeling]] architecture to define the UML, called the [[Meta-Object Facility]].&lt;ref&gt;Iman Poernomo (2006) "[http://calcium.dcs.kcl.ac.uk/1259/1/acm-paper.pdf The Meta-Object Facility Typed]" in: ''Proceeding SAC '06 Proceedings of the 2006 ACM symposium on Applied computing''. pp. 1845-1849&lt;/ref&gt; MOF is designed as a four-layered architecture, as shown in the image at right. It provides a meta-meta model at the top, called the M3 layer. This M3-model is the language used by Meta-Object Facility to build metamodels, called M2-models.

The most prominent example of a Layer 2 Meta-Object Facility model is the UML metamodel, which describes the UML itself. These M2-models describe elements of the M1-layer, and thus M1-models. These would be, for example, models written in UML. The last layer is the M0-layer or data layer. It is used to describe runtime instances of the system.&lt;ref&gt;{{cite web|url=http://www.omg.org/spec/UML/2.4.1/Infrastructure/PDF/ |title=UML 2.4.1 Infrastructure |publisher=Omg.org |date=2011-08-05 |accessdate=2014-04-10}}&lt;/ref&gt;

The meta-model can be extended using a mechanism called [[stereotype (UML)|stereotyping]]. This has been criticised as being insufficient/untenable by [[Brian Henderson-Sellers]] and Cesar Gonzalez-Perez in "Uses and Abuses of the Stereotype Mechanism in UML 1.x and 2.0".&lt;ref name="UsesAbusesStereotype"&gt;B. Henderson-Sellers; C. Gonzalez-Perez (2006). "Uses and Abuses of the Stereotype Mechanism in UML 1.x and 2.0". in: ''Model Driven Engineering Languages and Systems''. Springer Berlin / Heidelberg.&lt;/ref&gt;

== Adoption ==

UML has been found useful in many design contexts.&lt;ref name="UML, Success Stories"/&gt;&lt;ref&gt;{{Cite web|url = http://www.drdobbs.com/architecture-and-design/uml-25-do-you-even-care/240163702?queryText=uml|title = UML 2.5: Do you even care?}} "UML truly is ubiquitous"&lt;/ref&gt;

It has been treated, at times, as a design [[no silver bullet|silver bullet]], which has led to problems in its usage. Misuse of it includes excessive usage of it (design every little part of the system's [[Programming code|code]] with it, which is unnecessary) and assuming that anyone can design anything with it (even those who haven't [[Programmer|programmed]]).&lt;ref&gt;{{Cite web|url = http://queue.acm.org/detail.cfm?id=984495|title = Death by UML Fever}}&lt;/ref&gt;

It is seen to be a large language, with many [[Syntax (programming languages)|constructs]] in it. Some (including [[Ivar Jacobson|Jacobson]]) feel that there are too many and that this hinders the learning (and therefore usage) of it.&lt;ref&gt;{{Cite web|url = http://www.infoq.com/interviews/Ivar_Jacobson|title = Ivar Jacobson on UML, MDA, and the future of methodologies}}&lt;/ref&gt;

== Criticisms ==
{{Criticism section|date=December 2010}}

Common criticisms of UML from industry include:&lt;ref name="petre"&gt;{{Cite conference| quote=The majority of those interviewed simply do not use UML, and those who do use it tend to do so selectively and often informally|conference=35th International Conference on Software Engineering 18–26 May 2013 |url=http://oro.open.ac.uk/35805/8/UML%20in%20practice%208.pdf|title=UML in practice|first=Marian|last=Petre| date=2013|pages=722–731}}&lt;/ref&gt;

* not useful: "[does] not offer them advantages over their current, evolved practices and representations"
* too complex, particularly for communication with clients: "unnecessarily complex" and "The best reason not to use UML is that it is not ‘readable’ for all stakeholders. How much is UML worth if a business user (the customer) can not understand the result of your modelling effort?"
* need to keep UML and code in sync, as with documentation generally

=== Critique of UML 1.x ===

; Cardinality notation: As with database Chen, Bachman, and ISO [[ER diagram]]s, class models are specified to use "look-across" [[Cardinality (data modeling)|cardinalities]], even though several authors ([[Merise]],&lt;ref&gt;Hubert Tardieu, Arnold Rochfeld and René Colletti La methode MERISE: Principes et outils (Paperback - 1983)&lt;/ref&gt; Elmasri &amp; Navathe&lt;ref&gt;Elmasri, Ramez, B. Shamkant, Navathe, Fundamentals of Database Systems, third ed., Addison-Wesley, Menlo Park, CA, USA, 2000.&lt;/ref&gt; amongst others&lt;ref&gt;[https://books.google.com/books?id=odZK99osY1EC&amp;pg=PA52&amp;img=1&amp;pgis=1&amp;dq=genova&amp;sig=ACfU3U3tDC_q8WOMqUJW4EZCa5YQywoYLw&amp;edge=0 ER 2004 : 23rd International Conference on Conceptual Modeling, Shanghai, China, 8-12 November 2004] {{webarchive |url=https://web.archive.org/web/20130527133330/https://books.google.com/books?id=odZK99osY1EC&amp;pg=PA52&amp;img=1&amp;pgis=1&amp;dq=genova&amp;sig=ACfU3U3tDC_q8WOMqUJW4EZCa5YQywoYLw&amp;edge=0 |date=27 May 2013 }}&lt;/ref&gt;) prefer same-side or "look-here" for roles and both minimum and maximum cardinalities. Recent researchers (Feinerer,&lt;ref&gt;{{cite web|url=http://publik.tuwien.ac.at/files/pub-inf_4582.pdf |title=A Formal Treatment of UML Class Diagrams as an Efficient Method for Configuration Management 2007 |format=PDF |accessdate=2011-09-22}}&lt;/ref&gt; Dullea et. alia&lt;ref&gt;{{cite web|url=http://www.ischool.drexel.edu/faculty/song/publications/p_DKE_03_Validity.pdf |title=James Dullea, Il-Yeol Song, Ioanna Lamprou - An analysis of structural validity in entity-relationship modeling 2002 |format=PDF |accessdate=2011-09-22}}&lt;/ref&gt;) have shown that the "look-across" technique used by UML and ER diagrams is less effective and less coherent when applied to ''n''-ary relationships of order strictly greater than 2.

: Feinerer says: "Problems arise if we operate under the look-across semantics as used for UML associations. Hartmann&lt;ref&gt;{{cite web|url=http://crpit.com/confpapers/CRPITV17Hartmann.pdf |title="Reasoning about participation constraints and Chen's constraints" S Hartmann - 2003 |format=PDF |accessdate=2013-08-17}}&lt;/ref&gt; investigates this situation and shows how and why different transformations fail.", and: "As we will see on the next few pages, the look-across interpretation introduces several difficulties which prevent the extension of simple mechanisms from binary to ''n''-ary associations."

== See also ==
{{Portal|Software}}

* [[Object Oriented Role Analysis and Modeling]]
* [[Model-based testing]]
* [[Model-driven engineering]]
* [[Applications of UML]]
* [[List of Unified Modeling Language tools]]

== References ==
{{FOLDOC}}

{{reflist|colwidth=30em}}

== Further reading ==

* {{cite book

 | first= Scott William
 | last = Ambler
 | year = 2004
 | url = http://www.ambysoft.com/books/theObjectPrimer.html
 | title = The Object Primer: Agile Model Driven Development with UML 2
 | publisher = Cambridge University Press
 | isbn=0-521-54018-6
}}

* {{cite book

 | first= Michael Jesse
 | last = Chonoles
 | author2=James A. Schardt
 | year = 2003
 | title = UML 2 for Dummies
 | publisher = Wiley Publishing
 | isbn=0-7645-2614-6
}}

* {{cite book

 | first = Martin
 | last = Fowler
 | authorlink = Martin Fowler
 | title = UML Distilled: A Brief Guide to the Standard Object Modeling Language
 | edition = 3rd
 | publisher = Addison-Wesley
 | isbn = 0-321-19368-7
}}

* {{cite book

 | first= Ivar
 | last = Jacobson |author2=Grady Booch |author3=James Rumbaugh
 | authorlink = Ivar Jacobson
 | year = 1998
 | title = The Unified Software Development Process
 | publisher = Addison Wesley Longman
 | isbn=0-201-57169-2
}}

* {{cite book

 | first = Robert Cecil
 | last = Martin
 | authorlink = Robert Cecil Martin
 | year = 2003
 | title = UML for Java Programmers
 | publisher = Prentice Hall
 | isbn = 0-13-142848-9
}}

* {{cite web

 | author = Noran, Ovidiu S.
 | url = http://www.cit.gu.edu.au/~noran/Docs/UMLvsIDEF.pdf
 | title = Business Modelling: UML vs. IDEF
 | format = PDF
 | accessdate = 2005-12-28
}}

* {{cite web

 | author = Horst Kargl
 | url = http://umlnotation.sparxsystems.eu/
 | title = Interactive UML Metamodel with additional Examples
 }}

* {{cite book

 | first = Magnus
 | last = Penker
 | author2=Hans-Erik Eriksson
 | author-link2= Hans-Erik Eriksson
 | year = 2000
 | title = Business Modeling with UML
 | publisher = John Wiley &amp; Sons
 | isbn = 0-471-29551-5
}}

== External links ==
{{Commons}}

{{Wikiversity|UML}}

* {{Official website}}

{{UML}}

{{Software engineering}}

{{ISO standards}}

{{Use dmy dates|date=July 2011}}

{{Authority control}}

[[Category:Architecture description language]]
[[Category:Data modeling languages]]
[[Category:Data modeling diagrams]]
[[Category:Diagrams]]
[[Category:Knowledge representation]]
[[Category:ISO standards]]
[[Category:Specification languages]]
[[Category:Unified Modeling Language| ]]
[[Category:Software modeling language]]</text>
      <sha1>pxj03bgrblz0zh00cravcupivradz38</sha1>
    </revision>
  </page>
  <page>
    <title>Script theory</title>
    <ns>0</ns>
    <id>18211613</id>
    <revision>
      <id>704727017</id>
      <parentid>690255203</parentid>
      <timestamp>2016-02-13T06:03:01Z</timestamp>
      <contributor>
        <username>Nyttend</username>
        <id>1960810</id>
      </contributor>
      <comment>Stray header</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3786" xml:space="preserve">'''Script theory''' is a [[Psychology|psychological]] theory which posits that [[human behaviour]] largely falls into patterns called "scripts" because they function analogously to the way a written script does, by providing a program for action. [[Silvan Tomkins]] created script theory as a further development of his [[affect theory]], which regards human beings' emotional responses to stimuli as falling into categories called "[[Affect (psychology)|affects]]": he noticed that the purely biological response of affect may be followed by awareness and by what we [[Cognition|cognitively]] do in terms of acting on that affect so that more was needed to produce a complete explanation of what he called "human being theory".

In script theory, the basic unit of analysis is called a "scene", defined as a sequence of events linked by the affects triggered during the experience of those events. Tomkins recognized that our affective experiences fall into patterns that we may group together according to criteria such as the types of persons and places involved and the degree of intensity of the effect experienced, the patterns of which constitute scripts that inform our behavior in an effort to maximize positive affect and to minimize negative affect.

== In artificial intelligence ==
[[Roger Schank]], [[Robert P. Abelson]] and their research group, extended Tomkins' scripts and used them in early artificial intelligence work as a method of representing [[procedural knowledge]]. In their work, scripts are very much like [[frame (artificial intelligence)|frames]], except the values that fill the slots must be ordered. A script is a structured representation describing a stereotyped sequence of events in a particular context. Scripts are used in natural language understanding systems to organize a knowledge base in terms of the situations that the system should understand.

The classic example of a script involves the typical sequence of events that occur when a person drinks in a restaurant:  ''finding a seat, reading the menu, ordering drinks from the waitstaff...''  In the script form, these would be decomposed into [[conceptual dependency theory|conceptual transitions]], such as '''MTRANS''' and '''PTRANS''', which refer to ''mental transitions [of information]'' and ''physical transitions [of things]''.

Schank, Abelson and their colleagues tackled some of the most difficult problems in [[artificial intelligence]] (i.e., [[story understanding]]), but ultimately their line of work ended without tangible success. This type of work received little attention after the 1980s, but it is very influential in later [[knowledge representation]] techniques, such as [[case-based reasoning]].

Scripts can be inflexible. To deal with inflexibility, smaller modules called [[memory organization packet]]s (MOP) can be combined in a way that is appropriate for the situation.{{citation needed|date=February 2012}}

== References ==
{{More footnotes|date=November 2014}}
* Nathanson, Donald L. ''Shame and Pride: Affect, Sex, and the Birth of the Self''. London: W.W. Norton, 1992 
* [[Eve Kosofsky Sedgwick|Sedgwick, Eve Kosofsky]] and Adam Frank, eds. 1995. ''Shame and Its Sisters: A Silvan Tomkins Reader''. Durham and London: Duke University Press.
* Tomkins, Silvan. "Script Theory". ''The Emergence of Personality''. Eds. Joel Arnoff, A. I. Rabin, and Robert A. Zucker. New York: Springer Publishing Company, 1987. 147–216. 
* Tomkins, Silvan. "Script Theory: Differential Magnification of Affects". Nebraska Symposium On Motivation 1978. Ed. Richard A. Deinstbier. Lincoln, NE: [[University of Nebraska Press]], 1979. 201–236.

[[Category:History of artificial intelligence]]
[[Category:Knowledge representation]]
[[Category:Psychological theories]]</text>
      <sha1>i3bbcrdk0rogc4rr9hipamp72ykd1o1</sha1>
    </revision>
  </page>
  <page>
    <title>UMBEL</title>
    <ns>0</ns>
    <id>48794339</id>
    <revision>
      <id>749240358</id>
      <parentid>749239198</parentid>
      <timestamp>2016-11-13T06:55:41Z</timestamp>
      <contributor>
        <username>Materialscientist</username>
        <id>7852030</id>
      </contributor>
      <minor />
      <comment>Reverted edits by [[Special:Contribs/208.54.32.253|208.54.32.253]] ([[User talk:208.54.32.253|talk]]) to last version by Mkbergman</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="8169" xml:space="preserve">{{for|the part of a plant|Umbel}}
{{Paid contributions|date=December 2015}}
{{Infobox software
 |name = UMBEL
 |logo =
 |screenshot =
 |caption = Upper Mapping and Binding Exchange Layer
 |developer =  Structured Dynamics
 |released = 16 July 2008
 |latest_release_version =  UMBEL 1.50
 |latest_release_date = 10 May 2016&lt;ref&gt;{{cite web |url=http://umbel.org/resources/news/new-major-upgrade-of-umbel-released-version-1.50/
 |title=New, Major Upgrade of UMBEL Released: version 1.50 |publisher=UMBEL Web site |date=11 May 2016}}&lt;/ref&gt;
 |operating_system =
 |genre = {{Flatlist|
* [[Ontology (information science)|Ontology]]
* [[Semantic Web]]
* [[Linked Data]]
* [[Artificial intelligence]]
}}
 |programming language = {{Flatlist|
* [[Web Ontology Language|OWL 2]]
* [[SKOS]]
}}
 |license = [[Creative Commons licenses|Creative Commons Attribution 3.0]]
 |website = {{URL|umbel.org}}
}}
'''UMBEL''' ('''U'''pper '''M'''apping and '''B'''inding '''E'''xchange '''L'''ayer) is a logically organized knowledge [[Graph (discrete mathematics)|graph]] of 34,000 concepts and entity types that can be used in [[information science]] for relating information from disparate sources to one another. The [[Symbol grounding problem|grounding]] of this information occurs by common reference to the permanent [[Uniform Resource Identifier|URIs]] for the UMBEL concepts; the connections within the UMBEL [[Upper ontology (information science)|upper ontology]] enable concepts from sources at different levels of abstraction or specificity to be logically related. Since UMBEL is an [[open source]] extract of the [[Cyc#OpenCyc|OpenCyc]] [[knowledge base]], it can also take advantage of the [[Inference engine|reasoning capabilities]] within [[Cyc]].

UMBEL has two means to promote the [[semantic interoperability]] of information:.&lt;ref&gt;{{cite web
 | url = http://techwiki.umbel.org/index.php/UMBEL_Specification
 | title = UMBEL (Upper Mapping and Binding Exchange Layer)
 | website = UMBEL Web Site
 | access-date = 8 February 2016
}}&lt;/ref&gt; It is:

* An [[ontology (information science)|ontology]] of about 35,000 reference concepts, designed to provide common [[Data mapping|mapping]] points for relating different ontologies or [[Conceptual schema|schema]] to one another, and
* A [[Controlled vocabulary|vocabulary]] for aiding that ontology mapping, including expressions of likelihood relationships distinct from exact identity or equivalence. This vocabulary is also designed for interoperable [[Ontology (information science)#Domain ontologies and upper ontologies|domain ontologies]].

[[File:LOD Cloud 2014.svg|thumb|400px|Diagram showing [[Linking Open Data]] datasets.  UMBEL is near the hub, below and to the right of the central DBpedia.]]

UMBEL is written in the [[Semantic Web]] languages of [[SKOS]] and [[Web Ontology Language|OWL 2]]. It is a [[Class (set theory)|class]] structure used in [[Linked Data]], along with OpenCyc, [[Yago (database)|YAGO]], and the [[DBpedia]] ontology. Besides data integration, UMBEL has been used to aid concept search,&lt;ref&gt;{{cite book
|last1= Sah
|first1= M
|last2= Wade
|first2= V
|year= 2012
|chapter= A novel concept-based search for the web of data using UMBEL and a fuzzy retrieval model
|chapter-url= https://www.researchgate.net/profile/Melike_Sah/publication/262218017_A_novel_concept-based_search_for_the_web_of_data_using_UMBEL_and_a_fuzzy_retrieval_model/links/555d9c5808ae8c0cab2ad795.pdf
|title= The Semantic Web: Research and Applications
|publisher= Springer
|location= Berlin Heidelberg
|publication-date= 27 May 2012
|pages= 103–118
}}&lt;/ref&gt;&lt;ref&gt;{{cite book
|last1= Sah
|first1= M
|last2= Wade
|first2= V
|year= 2013
|chapter= Personalized concept-based search and exploration on the web of data using results categorization
|chapter-url= http://eswc-conferences.org/sites/default/files/papers2013/sah.pdf
|title= The Semantic Web: Semantics and Big Data
|publisher= Springer
|location= Berlin Heidelberg
|publication-date= 26 May 2013
|pages= 532–547
}}&lt;/ref&gt; concept definitions,&lt;ref&gt;{{cite book
|last1= Ballatore
|first1= Andrea
|editor1-first= Harlan
|editor1-last= Onsrud
|editor2-first= Werner
|editor2-last= Kuhn
|year= 2016
|chapter= Prolegomena for an Ontology of Place
|chapter-url= http://eprints.cdlib.org/uc/item/0rw1n045.pdf
|title= Advancing Geographic Information Systems
|publisher= GSDI Association Press
|location= Needham, MA
|pages= 91–103
}}&lt;/ref&gt; [[Ranking (information retrieval)|query ranking]],&lt;ref&gt;{{cite journal
| last       = Stecher
| first      = R
| last2      = Costache
| first2     = S
| last3      = Niederée
| first3     = C
| last4      = Nejdl
| first4     = W
| date       = 7 Jun 2010
| title      = Query ranking in information integration
| url        = https://www.researchgate.net/profile/Stefania_Costache2/publication/220921101_Query_Ranking_in_Information_Integration/links/00b7d516d17814201b000000.pdf
| journal    = Advanced Information Systems Engineering
| publisher  = Springer Berlin Heidelberg
| pages      = 230–235
}}&lt;/ref&gt; ontology integration,&lt;ref&gt;{{cite book
|last1= Damova
|first1= M
|last2= et
|first2= al.
|year= 2012
|editor1-first= Maria Teresa
|editor1-last= Pazienza
|chapter= Creation and Integration of Reference Ontologies for Efﬁcient LOD Management
|url= http://www.igi-global.com/book/semi-automatic-ontology-development/58294
|title= Semi-Automatic Ontology Development: Processes and Resources
|publisher= IGI Global
|pages= 162–199
}}&lt;/ref&gt; and ontology consistency checking.&lt;ref&gt;{{cite journal
| last       = Sheng
| first      = Z
| last2      = Wang
| first2     = X
| last3      = Shi
| first3     = H
| last4      = Feng
| first4     = Z
| date       = 26 Oct 2012
| title      = Checking and handling inconsistency of DBpedia
| url        = http://link.springer.com/chapter/10.1007/978-3-642-33469-6_60
| journal    = Web Information Systems and Mining
| publisher  = Springer Berlin Heidelberg
| pages      = 480–488
}}&lt;/ref&gt; It has also been used to build large ontologies &lt;ref&gt;{{cite journal
| last       = Yablonsky
| first      = S
| date       = Jun 2009
| title      = Semantic Web framework for development of very large ontologies
| url        = http://www.scielo.org.mx/pdf/poli/n39/n39a4.pdf
| journal    = Polibits
| issue      = 39
| pages      = 19–26
}}&lt;/ref&gt; and for online [[question answering]] systems.&lt;ref&gt;{{cite journal
| last       = Bishop
| first      = B
| last2      = et
| first2     = al.
| date       = Jan 2011
| title      = Factforge: A fast track to the web of data
| url        = http://www.semantic-web-journal.net/sites/default/files/swj77_2.pdf
| journal    = Semantic Web
| volume     = 2
| issue      = 2
| pages      = 157–166
}}&lt;/ref&gt;

Including OpenCyc, UMBEL has about 65,000 formal mappings to [[DBpedia]], PROTON, [[GeoNames]], and [[schema.org]], and provides linkages to more than 2 million [[Wikipedia]] pages (English version). All of its reference concepts and mappings are organized under a hierarchy of 31 different "super types",&lt;ref&gt;See [http://techwiki.umbel.org/index.php/UMBEL_-_Annex_G Annex G] in the UMBEL specifications.&lt;/ref&gt; which are mostly disjoint from one another. Each of these "super types" has its own typology of entity classes to provide flexible tie-ins for external content. 90% of UMBEL is contained in these entity classes.

UMBEL was first released in July 2008. Version 1.00 was released in February 2011.&lt;ref&gt;See the [http://umbel.org/resources/news/finally-umbel-v-100] release announcement.&lt;/ref&gt; Its current release is version 1.50.&lt;ref&gt;See http://umbel.org/resources/news/new-major-upgrade-of-umbel-released-version-1.50/&lt;/ref&gt;

==See also==
* [[Cyc]]
* [[DBpedia]]

==Notes==
{{Reflist}}

==External links==
* [http://umbel.org/ Main page for UMBEL]
* [http://techwiki.umbel.org/index.php/UMBEL_Specification UMBEL specification], and its accompanying [http://umbel.org/specifications/annexes Annexes A - L, Z]

{{Semantic Web}}
{{Use dmy dates|date=October 2012}}

{{DEFAULTSORT:Umbel}}
[[Category:Knowledge representation]]
[[Category:Ontology (information science)]]
[[Category:Semantic Web]]
[[Category:Knowledge bases]]</text>
      <sha1>2xjq8tmjglxzpkeh7nzn5veqsxityyz</sha1>
    </revision>
  </page>
  <page>
    <title>Pattern language</title>
    <ns>0</ns>
    <id>182837</id>
    <revision>
      <id>752197416</id>
      <parentid>749932531</parentid>
      <timestamp>2016-11-30T00:37:56Z</timestamp>
      <contributor>
        <username>Quiddity</username>
        <id>210259</id>
      </contributor>
      <comment>/* top */ add year for originating book</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="24974" xml:space="preserve">{{About|the structured design approach by architect Christopher Alexander}}
A '''pattern language''' is a method of describing good design practices or patterns of useful organization within a field of expertise. The term was coined by architect [[Christopher Alexander]] and popularized by his 1977 book ''[[A Pattern Language]]''.

A pattern language can also be an attempt to express the deeper wisdom of what brings aliveness within a particular field of human endeavor, through a set of interconnected patterns. Aliveness is one placeholder term for "the quality that has no name": a sense of wholeness, spirit, or grace, that while of varying form, is precise and empirically verifiable.{{cn|date=March 2016}} Some advocates{{who|date=March 2016}} of this design approach claim that ordinary people can use it to successfully solve very large, complex design problems.

==What is a pattern?==
{{See also|Design pattern}}
When a designer designs something – whether a house, computer program, or lamp – they must make many decisions about how to solve problems. A single problem is documented with its typical place (the [[syntax]]), and use (the [[grammar]]) with the most common and recognized good solution seen in the wild, like the examples seen in [[dictionary|dictionaries]]. Each such entry is a single [[design pattern]]. Each pattern has a name, a descriptive entry, and some cross-references, much like a dictionary entry. A documented pattern should explain why that solution is good in the pattern's contexts.

Elemental or universal ''patterns'' such as "door" or "partnership" are versatile ideals of design, either as found in experience or for use as components in practice, explicitly described as holistic resolutions of the forces in recurrent contexts and circumstances, whether in architecture, medicine, software development or governance, etc. Patterns might be invented or found and studied, such as the naturally occurring patterns of design that characterize human environments.&lt;ref&gt;Henshaw, J. [http://www.synapse9.com/pub/2015_PURPLSOC-JLHfinalpub.pdf Guiding Patterns of Naturally Occurring Design: Elements. PURPLSOC 2015 proceedings, July 3-5 2015 Krems, Austria] PURPLSOC meeting on the many open scientific questions, e.g. regarding the theoretical background of patterns and the practical implementation of pattern methods in research and teaching.&lt;/ref&gt;

Like all languages, a pattern language has [[vocabulary]], [[syntax]], and [[grammar]] – but a pattern language applies to some complex activity other than communication. In pattern languages for design, the parts break down in this way:
* The language description – the ''vocabulary'' – is a collection of named, described solutions to problems in a field of interest. These are called ''design patterns''. So, for example, the language for architecture describes items like: settlements, buildings, rooms, windows, latches, etc.
* Each solution includes ''syntax'', a description that shows where the solution fits in a larger, more comprehensive or more abstract design. This automatically links the solution into a web of other needed solutions. For example, rooms have ways to get light, and ways to get people in and out.
* The solution includes ''grammar'' that describes how the solution solves a problem or produces a benefit. So, if the benefit is unneeded, the solution is not used. Perhaps that part of the design can be left empty to save money or other resources; if people do not need to wait to enter a room, a simple doorway can replace a waiting room.
* In the language description, grammar and syntax cross index (often with a literal alphabetic index of pattern names) to other named solutions, so the designer can quickly think from one solution to related, needed solutions, and document them in a logical way. In Christopher Alexander's book ''A Pattern Language'', the patterns are in decreasing order by size, with a separate alphabetic index.
* The web of relationships in the index of the language provides many paths through the design process.

This simplifies the design work because designers can start the process from any part of the problem they understand and work toward the unknown parts. At the same time, if the pattern language has worked well for many projects, there is reason to believe that even a designer who does not completely understand the design problem at first will complete the design process, and the result will be usable. For example, skiers coming inside must shed snow and store equipment. The messy snow and boot cleaners should stay outside. The equipment needs care, so the racks should be inside.

==Many patterns form a language==
Just as [[words]] must have [[Grammar|grammatical]] and [[Semantics|semantic]] relationships to each other in order to make a spoken [[language]] useful, design patterns must be related to each other in position and utility order to form a pattern language. Christopher Alexander's work describes a process of decomposition, in which the designer has a problem (perhaps a commercial assignment), selects a solution, then discovers new, smaller problems resulting from the larger solution. Occasionally, the smaller problems have no solution, and a different larger solution must be selected. Eventually all of the remaining design problems are small enough or routine enough to be solved by improvisation by the builders, and the "design" is done.

The actual organizational structure ([[Hierarchy|hierarchical]], [[Iterative method|iterative]], etc.) is left to the discretion of the designer, depending on the problem. This explicitly lets a designer explore a design, starting from some small part. When this happens, it's common for a designer to realize that the problem is actually part of a larger solution. At this point, the design almost always becomes a better design.

In the language, therefore, each pattern has to indicate its relationships to other patterns and to the language as a whole. This gives the designer using the language a great deal of guidance about the related problems that must be solved.

The most difficult part of having an outside expert apply a pattern language is in fact to get a reliable, complete list of the problems to be solved. Of course, the people most familiar with the problems are the people that need a design. So, Alexander famously advocated on-site improvisation by concerned, empowered users,&lt;ref&gt;A Pattern Language, ibid&lt;/ref&gt;&lt;ref&gt;Alexander, Christopher, The Oregon Project&lt;/ref&gt; as a powerful way to form very workable large-scale initial solutions, maximizing the utility of a design, and minimizing the design rework. The desire to empower users of architecture was, in fact, what led Alexander to undertake a pattern language project for architecture in the first place.

==Design problems in a context==
An important aspect of design patterns is to identify and document the key ideas that make a good system different from a poor system (that may be a house, a computer program or an object of daily use), and to assist in the design of future systems. The idea expressed in a pattern should be general enough to be applied in very different systems within its context, but still specific enough to give constructive guidance.

The range of situations in which the problems and solutions addressed in a pattern apply is called its context. An important part in each pattern is to describe this context. Examples can further illustrate how the pattern applies to very different situation.

For instance, Alexander's pattern "A PLACE TO WAIT" addresses bus stops in the same way as waiting rooms in a surgery, while still proposing helpful and constructive solutions. The [[Design Patterns|"Gang-of-Four" book ''Design Patterns'']] by Gamma et al. proposes solutions that are independent of the programming language, and the program's application domain.

Still, the problems and solutions described in a pattern can vary in their level of abstraction and generality on the one side, and specificity on the other side. In the end this depends on the author's preferences. However, even a very abstract pattern will usually contain examples that are, by nature, absolutely concrete and specific.

Patterns can also vary in how far they are proven in the real world. Alexander gives each pattern a rating by zero, one or two stars, indicating how well they are proven in real-world examples. It is generally claimed that all patterns need at least some existing real-world examples. It is, however, conceivable to document yet unimplemented ideas in a pattern-like format.

The patterns in Alexander's book also vary in their level of scale – some describing how to build a town or neighbourhood, others dealing with individual buildings and the interior of rooms. Alexander sees the low-scale artifacts as constructive elements of the large-scale world, so they can be connected to a [[#Aggregation in an associative network (pattern language)|hierarchic network]].

===Balancing of forces===
A pattern must characterize the problems that it is meant to solve, the context or situation where these problems arise, and the conditions under which the proposed solutions can be recommended.

Often these problems arise from a conflict of different interests or "forces". A pattern emerges as a dialogue that will then help to balance the forces and finally make a decision.

For instance, there could be a pattern suggesting a wireless telephone. The forces would be the need to communicate, and the need to get other things done at the same time (cooking, inspecting the bookshelf). A very specific pattern would be just "WIRELESS TELEPHONE". More general patterns would be "WIRELESS DEVICE" or "SECONDARY ACTIVITY", suggesting that a secondary activity (such as talking on the phone, or inspecting the pockets of your jeans) should not interfere with other activities.

Though quite unspecific in its context, the forces in the "SECONDARY ACTIVITY" pattern are very similar to those in "WIRELESS TELEPHONE". Thus, the competing forces can be seen as part of the essence of a design concept expressed in a pattern.

===Patterns contain their own rationale===
Usually a pattern contains a rationale referring to some given values. For Christopher Alexander, it is most important to think about the people who will come in contact with a piece of architecture. One of his key values is making these people feel more alive. He talks about the "quality without a name" (QWAN).

More generally, we could say that a good system should be accepted, welcomed and happily embraced as an enrichment of daily life by those who are meant to use it, or – even better – by all people it affects. For instance, when discussing a street café, Alexander discusses the possible desires of a guest, but also mentions people who just walk by.

The same thinking can be applied to technical devices such as telephones and cars, to social structures like a team working on a project, or to the user interface of a computer program. The qualities of a software system, for instance, could be rated by observing whether users spend their time enjoying or struggling with the system.

By focusing on the impacts on human life, we can identify patterns that are independent from changing technology, and thus find "timeless quality" (Alexander).

==Generic structure and layout==
Usually the author of a pattern language or collection chooses a generic structure for all the patterns it contains, breaking each into generic sections like context, problem statement, solution etc.

Christopher Alexander's patterns, for instance, each consist of a short name, a rating (up to two '*' symbols), a sensitizing picture, the context description, the problem statement, a longer part of text with examples and explanations, a solution statement, a sketch and further references. This structure and layout is sometimes referred to as the "Alexandrian form".

Alexander uses a special text layout to mark the different sections of his patterns. For instance, the problem statement and the solution statement are printed in bold font, the latter is always preceded by the "Therefore:" keyword. Some authors instead use explicit labels, which creates some degree of redundancy.

===Meaningful names===
When design is done by a team, pattern names will form a vocabulary they can share. This makes it necessary for pattern names to be easy to remember and highly descriptive. Some examples from Alexander's works are WINDOW PLACE (helps define where windows should go in a room) and A PLACE TO WAIT (helps define the characteristics of bus stops and hospital waiting rooms, for example).

==Aggregation in an associative network (pattern language)==
A pattern language, as conceived by Alexander, contains links from one pattern to another, so when trying to apply one pattern in a project, a designer is pushed to other patterns that are considered helpful in its context.

In Alexander's book, such links are collected in the "references" part, and echoed in the linked pattern's "context" part – thus the overall structure is a directed graph. A pattern that is linked to in the "references" usually addresses a problem of lower scale, that is suggested as a part of the higher-scale problem. For instance, the "PUBLIC OUTDOOR ROOM" pattern has a reference to "STAIR SEATS".

Even without the pattern description, these links, along with meaningful names, carry a message: When building a place outside where people can spend time ("PUBLIC OUTDOOR ROOM"), consider to surround it by stairs where people can sit ("STAIR SEATS"). If you are planning an office ("WORKSHOPS AND OFFICES"), consider to arrange workspaces in small groups ("SMALL WORKING GROUPS"). Alexander argues that the connections in the network can be considered even more meaningful than the text of the patterns themselves.

The links in Alexander's book clearly result in a hierarchic network. Alexander draws a parallel to the hierarchy of a grammar – that is one argument for him to speak of a pattern ''language''.

The idea of linking is generally accepted among pattern authors, though the semantic rationale behind the links may vary. Some authors, however, like Gamma et al. in ''[[Design Patterns]]'', make only little use of pattern linking – possibly because it did not make that much sense for their collection of patterns. In such a case we would speak of a ''pattern catalogue'' rather than a ''pattern language''.&lt;ref name="dearden"&gt;{{cite journal | author = Andy Dearden, Janet Finlay | title = Pattern Languages in HCI: A critical review | date = January 2006 | journal = Human Computer Interaction | volume = 21 | issue = 1 }}&lt;/ref&gt;

===Usage===
Alexander encouraged people who used his system to expand his language with patterns of their own. In order to enable this, his books do not focus strictly on architecture or civil engineering; he also explains the general method of pattern languages. The original concept for the book ''A Pattern Language'' was that it would be published in the form of a 3-ring binder, so that pages could easily be added later; this proved impractical in publishing.&lt;ref&gt;Portland Urban Architecture Research Laboratory
Symposium 2009, presentation by 4 of 6 original authors of ''A Pattern Language''.&lt;/ref&gt;  The pattern language approach has been used to document expertise in diverse fields. Some examples are [[Design pattern (architecture)|architectural patterns]], [[Design pattern (computer science)|computer science patterns]], [[interaction design pattern]]s, [[pedagogical patterns]],  social action patterns, and group facilitation patterns. The pattern language approach has also been recommended as a way to promote [[civic intelligence]] by helping to coordinate actions for diverse people and communities who are working together on significant shared problems (see &lt;ref&gt;Schuler, D. [http://publicsphereproject.org/sites/default/files/Critical%20Enablers%20of%20Civic%20Intelligence.reduced.pdf Choosing Success: Pattern Languages as Critical Enablers of Civic Intelligence]; PUARL Conference, Portland, OR. 2009&lt;/ref&gt; for additional discussion of motivation and rationale as well as examples and experiments).  Alexander's specifications for using pattern languages as well as creating new ones remain influential, and his books are referenced for style by experts in unrelated fields.

It is important to note that notations such as [[Unified Modeling Language|UML]] or the [[flowchart]] symbol collection are not pattern languages. They could more closely be compared to an alphabet: their symbols could be used to document a pattern language, but they are not a language by themselves. A [[recipe]] or other sequential set of steps to be followed, with only one correct path from start to finish, is also not a pattern language. However, the process of designing a new recipe might benefit from the use of a pattern language.

===Simple example of a pattern===
*''Name'': ChocolateChipRatio
*''Context'': You are baking chocolate chip cookies in small batches for family and friends
*''Consider these patterns first'': SugarRatio, FlourRatio, EggRatio
*''Problem'': Determine the optimum ratio of chocolate chips to cookie dough
*''Solution'': Observe that most people consider chocolate to be the best part of the chocolate chip cookie. Also observe that too much chocolate may prevent the cookie from holding together, decreasing its appeal. Since you are cooking in small batches, cost is not a consideration. Therefore, use the maximum amount of chocolate chips that results in a really sturdy cookie.
*''Consider next'': NutRatio or CookingTime or FreezingMethod

==Origin==
[[Christopher Alexander]], an architect and author, coined the term pattern language.&lt;ref&gt;{{Cite book | publisher = [[Oxford University Press]], USA | isbn = 0-19-501919-9 | last = Alexander | first = Christopher | title = A Pattern Language: Towns, Buildings, Construction | year = 1977 | page = 1216}}&lt;/ref&gt; He used it to refer to common problems of the [[design]] and [[construction]] of buildings and towns and how they should be solved. The solutions proposed in the book include suggestions ranging from how cities and towns should be structured to where windows should be placed in a room.

The framework and philosophy of the "pattern language" approach was initially popularized in the book ''[[A Pattern Language]]'' that was written by Christopher Alexander and five colleagues at the Center for Environmental Structure in Berkeley, California in the late 1970s. While ''A Pattern Language'' contains 253 "patterns" from the first pattern, "Independent Regions" (the most general) to the last, "Things from Your Life", Alexander's book ''[[The Timeless Way of Building]]'' goes into more depth about the motivation and purpose of the work. The following definitions of "pattern" and "pattern language" are paraphrased from ''A Pattern Language'':

"A ''pattern'' is a careful description of a perennial solution to a recurring problem within a building context, describing one of the configurations that brings life to a building.

Each pattern describes a problem that occurs over and over again in our environment, and then describes the core solution to that problem, in such a way that you can use the solution a million times over, without ever doing it the same way twice."

A ''pattern language'' is a network of patterns that call upon one another. Patterns help us remember insights and knowledge about design and can be used in combination to create solutions.

== Application domains ==
Christopher Alexander's idea has been adopted in other disciplines, often much more heavily than the original [[Pattern (architecture)|application of patterns to architecture]] as depicted the book ''[[A Pattern Language]]''. Recent examples include [[software design pattern]]s in software engineering and, more generally, [[Architectural pattern (computer science)|architectural patterns in computer science]], as well as [[interaction design pattern]]s. [[Pedagogical patterns]] are used to document good practices in teaching. The book ''Liberating Voices: A Pattern Language for Communication Revolution'', containing [http://www.publicsphereproject.org/patterns/lv 136 patterns] for using information and communication to promote sustainability, democracy and positive social change, was published in 2008. The deck "Group Works: A Pattern Language for Bringing Life to Meetings and Other Gatherings" was published in 2011. Recently, patterns were also introduced into [[systems architecture]] design.&lt;ref&gt;{{cite web|last=Hein|first=Andreas|title=Adopting Patterns for Space Mission and Space Systems Architecting|url=http://www.academia.edu/2110976/A.M._Hein_Adopting_Patterns_for_Space_Mission_and._Space_Systems_Architecting_|work=5 th International Workshop on System &amp; Concurrent Engineering for Space ApplicationsSECESA 2012|accessdate=2 March 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Hein|first=Andreas|title=Project Icarus: Stakeholder Scenarios for an Interstellar Exploration Program|url=http://www.academia.edu/1354848/PROJECT_ICARUS_STAKEHOLDER_SCENARIOS_FOR_AN_INTERSTELLAR_EXPLORATION_PROGRAM|work=Journal of the British Interplanetary Society, 64, 224-233, 2011|accessdate=2 March 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Cloutier|first=Robert|title=The Concept of Reference Architectures|url=http://www.calimar.com/TheConceptOfReferenceArchitectures.pdf|work=Systems Engineering Vol. 13, No. 1, 2010|accessdate=2 March 2013}}&lt;/ref&gt;  [[Chess]] [[Chess strategy|strategy]] and [[Chess tactics|tactics]] involve many patterns from [[Chess opening|opening]] to [[checkmate]].

==See also==
* [[Feng shui]]
* [[Method engineering]]
* [[Rule of thumb]]
* [[Typology (urban planning and architecture)]]

==References==
{{Reflist}}

==Further reading==
* Christopher Alexander, Sara Ishikawa, Murray Silverstein (1974). 'A Collection of Patterns which Generate Multi-Service Centres' in Declan and Margrit Kennedy (eds.): ''The Inner City.'' Architects Year Book 14, Elek, London. ISBN 0 236 15431 1.
* Alexander, C. (1977). ''[[A Pattern Language: Towns, Buildings, Construction]]''. USA: [[Oxford University Press]]. ISBN 978-0-19-501919-3.
* Alexander, C. (1979). ''The Timeless Way of Building''. USA: Oxford University Press. ISBN 978-0-19-502402-9.
* Schuler, D. (2008). ''Liberating Voices: A Pattern Language for Communication Revolution''. USA: [[MIT Press]]. ISBN 978-0-262-69366-0.
* Leitner, Helmut (2015): ''Pattern Theory: Introduction and Perspectives on the Tracks of Christopher Alexander''. ISBN 1505637430.

==External links==

===About patterns in general===
* [http://www.c2.com/cgi/wiki?TipsForWritingPatternLanguages Tips For Writing Pattern Languages], by [[Ward Cunningham]]
* [http://www.gardenvisit.com/landscape/architecture/3.1-patternlanguage.htm Essay on the pattern language as it relates to urban design]
* [http://www.academia.edu/1354848/PROJECT_ICARUS_STAKEHOLDER_SCENARIOS_FOR_AN_INTERSTELLAR_EXPLORATION_PROGRAM Use of patterns for scenario development for large scale aerospace projects]
* [http://torgronsund.wordpress.com/2010/01/06/lean-startup-business-model-pattern/ Lean Startup Business Model Pattern]
* [http://www.informit.com/articles/printerfriendly.aspx?p=30084 What Is a Quality Use Case?] from the book ''Patterns for Effective Use Cases''
* [http://groupworksdeck.org/what-we-mean-by-pattern Characteristics of group facilitation patterns]

===Online pattern collections===
* [http://www.patternlanguage.com/ patternlanguage.com], by the Center for Environmental Structure
* [http://www.fusedgrid.ca/ Fused Grid] – A Contemporary Urban Pattern "a collection and synthesis of neighbourhood patterns"
* [http://www.reliableprosperity.net ReliableProsperity.net] – Patterns for building a "restorative, socially just, and reliably prosperous society"
* [http://www.hcipatterns.org/ hcipatterns.org] – Patterns for HCI
* [http://www.c2.com/cgi/wiki?PatternIndex The Portland Pattern Repository]
* [http://developer.yahoo.com/ypatterns Yahoo! Design Pattern Library]
* [http://groupworksdeck.org Group Works: A Pattern Language for Bringing Life to Meetings and Other Gatherings] – A pattern language of group process
* [http://liveingreatness.com/core-protocols/ The Core Protocols] – A set of team communication patterns
* [http://www.publicsphereproject.org/patterns/lv Liberating Voices! Pattern Language Project] — Short versions of patterns available in [http://www.publicsphereproject.org/patterns_arabic Arabic], [http://www.publicsphereproject.org/patterns_chinese Chinese], and [http://www.publicsphereproject.org/patterns_spanish Spanish]

{{DEFAULTSORT:Pattern Language}}

[[Category:Architectural theory]]
[[Category:Cybernetics]]
[[Category:Design]]
[[Category:Knowledge representation]]
[[Category:Linguistics]]

[[fi:Suunnittelumalli]]</text>
      <sha1>jdjl2w2ch2ezm2gr2w2y1uc0oooyrw6</sha1>
    </revision>
  </page>
  <page>
    <title>OntoUML</title>
    <ns>0</ns>
    <id>33378172</id>
    <revision>
      <id>722714629</id>
      <parentid>715170597</parentid>
      <timestamp>2016-05-29T19:18:16Z</timestamp>
      <contributor>
        <ip>194.228.32.214</ip>
      </contributor>
      <comment>an UML -&gt; a UML</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="2806" xml:space="preserve">{{Infobox technology standard
| name = OntoUML
| year_started = 2005
| domain = [[Conceptual_model|Conceptual Modeling]]
| base_standards = [[Unified Foundational Ontology (UFO)]]
| related_standards = [[Unified Modeling Language|UML]]
| organization = [[Ontology &amp; Conceptual Modeling Research Group (NEMO)]]
| website = {{URL|http://nemo.inf.ufes.br/}}
}}
'''OntoUML''' is a [[Ontology|ontologically]] well-founded language for [[Ontology_(information_science)|Ontology]]-driven [[Conceptual_model|Conceptual Modeling]]. '''OntoUML''' is built as a [[Unified Modeling Language|UML]] extension based on the [[Unified Foundational Ontology (UFO)]]. UFO was created by Giancarlo Guizzardi in his Ph.D. thesis&lt;ref&gt;{{cite book|last1=Guizzardi|first1=Giancarlo|title=Ontological foundations for structural conceptual models|date=2005|publisher=Enschede: Telematica Instituut Fundamental Research Series|url=http://doc.utwente.nl/50826/1/thesis_Guizzardi.pdf}}&lt;/ref&gt; and used to evaluate and re-design a fragment of the UML 2.0 metamodel. Giancarlo Guizzardi is a lead researcher at the [[Ontology &amp; Conceptual Modeling Research Group (NEMO)]]&lt;ref&gt;{{cite web|title=Ontology &amp; Conceptual Modeling Research Group (NEMO)|url=http://nemo.inf.ufes.br/}}&lt;/ref&gt; located at the [[Federal University of Espírito Santo|Federal University of Espírito Santo (UFES)]] in [[Vitória,_Espírito_Santo|Vitória]] city, state of [[Espírito Santo]], [[Brazil]].

NEMO created an OntoUML infrastructure&lt;ref&gt;{{cite web|title=OntoUML infrastructure|url=http://code.google.com/p/rcarraretto/}}&lt;/ref&gt; using [[Eclipse_Modeling_Framework|Eclipse EMF]] for '''OntoUML''' model manipulation which serve as a basis for its tool support. NEMO has been actively working on tool support for the '''OntoUML''' Conceptual Modeling Language, respectively on:

# extensions of [[Unified Modeling Language|UML]] production-grade tools to support OntoUML, namely, the MDG for [[Enterprise_Architect_(software)|Enterprise Architect]].
# a standalone tool called [[OntoUML Lightweight Editor (OLED)]]&lt;ref&gt;{{cite web|title=OntoUML lightweight editor (OLED) repository|url=https://github.com/nemo-ufes/ontouml-lightweight-editor}}&lt;/ref&gt; to the development, evaluation and implementation of domain ontologies.
# a legacy OntoUML editor&lt;ref&gt;{{cite web|title=Legacy OntoUML editor|url=https://github.com/nemo-ufes/ontouml-editor-eclipse}}&lt;/ref&gt; based on an old version of [[Graphical_Modeling_Framework|Eclipse/GMF]].

Check out a list of all publications of NEMO about ontologies and OntoUML: &lt;ref&gt;{{cite web|title=NEMO publications|url=http://nemo.inf.ufes.br/publications/}}&lt;/ref&gt;

== References ==
&lt;references /&gt;

[[Category:UML tools]]
[[Category:Knowledge representation]]
[[Category:Ontology (information science)]]
[[Category:Ontology editors]]</text>
      <sha1>crzfhpm2vq2j4uz3fv5s0tfe8kpp5y1</sha1>
    </revision>
  </page>
  <page>
    <title>Moovly</title>
    <ns>0</ns>
    <id>50643936</id>
    <revision>
      <id>750926766</id>
      <parentid>737858018</parentid>
      <timestamp>2016-11-22T08:18:41Z</timestamp>
      <contributor>
        <ip>81.83.30.100</ip>
      </contributor>
      <comment>/* External links */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="4273" xml:space="preserve">{{Orphan|date=May 2016}}
{{Infobox company
| name             = Moovly
| type             = [[Public company|Public]]
| key_people       = Brendon Grunewald (Co-Founder and CEO), Geert Coppens (Co-Founder and CTO)
| industry         = [[Internet Marketing]]
| products         = Moovly web-based animation software video creation and generation platform.
| foundation       = {{Start date and age|2012}}
| location_city    = [[Vancouver]]
| location_country = [[Canada]]
| homepage         = {{URL|www.moovly.com}}
}}

'''Moovly''' is a company that provides a cloud-based platform [[Software as a service|(SaaS)]] that enables users to create and generate multimedia content: animated videos, video presentations, animated info graphics and any other video content that includes a mix of animation and motion graphics.&lt;ref&gt;Flanders Today, [http://www.flanderstoday.eu/business/moovly-offers-new-online-tool-develop-multimedia-designs Flanders Today], September 13th, 2013, "Moovly offers new online tool to develop multimedia designs"&lt;/ref&gt;

== History ==
Moovly was originally founded in Belgium in November 2012 by Brendon Grunewald and Geert Coppens with the vision of "becoming the number one platform for engaging customisable multimedia content creation". The company's mission is to "Enable everyone to create engaging multimedia content by making it affordable, Intuitive and Simple".  The company has seen steady subscriber growth from multinational corporations, small and medium business as well as educational institutions.. The company initially secured several rounds of external investment to fund its growth, and in July 2016, listed on the Toronto Venture Stock Exchange as Moovly Media Inc under the symbol MVY (TSX.V: MVY).&lt;ref&gt;Levak, Rachel, [https://www.crunchbase.com/organization/moovly/funding-rounds Crunchbase] , Apr 09, 2015, "Moovly - Funding Rounds |  CrunchBase"&lt;/ref&gt;

== Product ==
Moovly is a cloud based digital media and content creation software platform. Content can be created via various interfaces, including the editor as well as simple, custom-made video generation interfaces.

Using a combination of uploaded images, videos and sounds, as well as a pre-defined library of objects, users are able to quickly assemble new animated content. The final videos or presentations can be downloaded as an [[MP4]] for example, or published on a variety of video platforms.

Moovly provides a feature-rich free license allowing users to create animated videos&lt;ref&gt;Wilson, Liévano, [http://jsk.stanford.edu/news-notes/2014/10-things-you-need-to-know-when-producing-a-data-animation-for-a-newsroom/ John S. Knight Journalism Fellowships at Stanford], Sep 04th, 2014, "10 things you need to know when producing a data animation for a newsroom"&lt;/ref&gt; that can be exported to [[Facebook]] and [[YouTube]], as well as premium licenses for advanced and professional use. The free videos include the Moovly branding. As an educational tool&lt;ref&gt;Hart, Jane, [http://c4lpt.co.uk/top100tools/moovly/ Centre for Learning &amp; Performance Technologies], Sep 21, 2015, " Top 100 Tools for Learning | Centre for Learning &amp; Performance Technologies"&lt;/ref&gt; and for educational purposes,&lt;ref&gt;Janssens, Mieke, [http://www.klascement.eu/sites/60764/ KlasCement Educational Resources Network], Sep 22, 2015, "Moovly: Create animated videos and presentations | KlasCement Educational Resources Network"&lt;/ref&gt; Moovly offers specific licenses.&lt;ref&gt;[https://www.moovly.com/education-solutions Moovly Website]&lt;/ref&gt;

Companies and brands can use their own library of animated graphics, their own fonts and standard color set.&lt;ref&gt;Waldron, John, [http://www.markitwrite.com/moovly/ markITwrite] , Nov, 2014, "Moovly: Video Animation for Everyone |  markITwrite"&lt;/ref&gt;

== References ==
{{reflist}}

== External links ==
* {{official website|https://www.moovly.com/}}
* [http://web.tmxmoney.com/company.php?qm_symbol=MVY&amp;locale=EN Toronto Venture Exchange Moovly Page]
* [http://www.bloomberg.com/profiles/companies/1291746D:BB-moovly-nv Bloomberg overview]

[[Category:Animation software]]
[[Category:Companies established in 2012]]
[[Category:Cloud applications]]
[[Category:Computer animation]]
[[Category:Presentation software]]
[[Category:Knowledge representation]]</text>
      <sha1>1exwnuhwzlplp32slxdu3dh7s38ehbc</sha1>
    </revision>
  </page>
  <page>
    <title>Library system</title>
    <ns>0</ns>
    <id>21140383</id>
    <revision>
      <id>750147391</id>
      <parentid>748453823</parentid>
      <timestamp>2016-11-18T02:10:07Z</timestamp>
      <contributor>
        <username>AnomieBOT</username>
        <id>7611264</id>
      </contributor>
      <minor />
      <comment>[[User:AnomieBOT/docs/TemplateSubster|Substing templates]]: {{PDFlink}}. See [[User:AnomieBOT/docs/TemplateSubster]] for info.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="3074" xml:space="preserve">[[File:Boston Public Library Reading Room.jpg|thumb|Reading Room at [[Boston Public Library, McKim Building|McKim Building]] in 2013]]Library system is a central organization created to manage and coordinate operations and services in or between different centers, buildings or [[Library branch|libraries branches]] and libraries patrons. It uses a [[Library classification]] to organize their volumes and nowadays also uses a [[Integrated library system]], an [[enterprise resource planning]] system for a [[library]], used to track items owned, orders made, bills paid, and patrons who have borrowed.&lt;ref&gt;Adamson, Veronica, ''et al.'' (2008). {{cite web|url= http://www.jisc.ac.uk/media/documents/programmes/resourcediscovery/lmsstudy.pdf |title=''JISC &amp; SCONUL Library Management Systems Study'' }}&amp;nbsp;{{small|(1&amp;nbsp;MB)}}. Sheffield, UK: Sero Consulting. p. 51. Retrieved on 21 January 2009. "... a Library Management System (LMS or ILS 'Integrated Library System' in US parlance)."
Some useful library automation software are: KOHA ,Grennstone .LIBsis, and granthlaya.&lt;/ref&gt; Many counties, states or Universities have developed their own libraries systems, among them can be named [[Los Angeles Public Library|Los Angeles Public Library System]],&lt;ref&gt;{{Cite web|url=http://www.lapl.org/about-lapl/press/2013-library-facts|title=Los Angeles Public Library Facts 2013 (for fiscal year 2012-13) {{!}} Los Angeles Public Library|website=www.lapl.org|access-date=2016-03-06}}&lt;/ref&gt; [[Harvard Library|Harvard Library System]],.&lt;ref name="AR2013"&gt;{{cite web|title=Harvard Library Annual Report FY 2013 |url=http://library.harvard.edu/annual-report-fy-2013 |date=2013 |website=Harvard Library |author=Harvard University |accessdate=17 March 2015}}&lt;/ref&gt;

Most of [[County|counties]] of every country have their own '''library system'''s that usually have between 10 to 30 libraries on every city of their counties, some of them are; [[London Public Library]] on [[Canada]] with 16 library branches, [[Helsinki Metropolitan Area Libraries]], in [[Finland]], with 63 libraries,&lt;ref&gt;{{cite web| url=http://www.iii.com/news/pr_display.php?id=559 | title=Helsinki Metropolitan Area Libraries (Finland) Upgrades to Sierra Services Platform | publisher=Innovative | type= Press release | date=5 February 2013 | accessdate=1 August 2014 }}&lt;/ref&gt; and some countries, like Venezuela has only one library system for the whole country as is [[National Library of Venezuela]] with 685 branches.  In the United States can be named [[Boston Public Library|Boston Public Library System]], [[New York Public Library|New York Public Library System]], [[District of Columbia Public Library|District of Columbia Public Library System]], among others.

==See also==
* [[Integrated library system]]
* [[Library classification]]
* [[Library branch]]
* [[List of the largest libraries in the United States]]

==References==
{{reflist}}

[[Category:Public libraries]]
[[Category:Private libraries]]
[[Category:Libraries]]
[[Category:Culture]]
[[Category:Knowledge representation]]


{{Library-stub}}</text>
      <sha1>ne4w2mas5phml93uj35pyv1pt2sckb3</sha1>
    </revision>
  </page>
  <page>
    <title>Plinian Core</title>
    <ns>0</ns>
    <id>49236889</id>
    <revision>
      <id>753853621</id>
      <parentid>745667548</parentid>
      <timestamp>2016-12-09T15:51:54Z</timestamp>
      <contributor>
        <username>Gaurav</username>
        <id>6379</id>
      </contributor>
      <comment>Added some internal links, removed some external links</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="7182" xml:space="preserve">{{Multiple issues|
{{Orphan|date=June 2016}}
{{primary sources|date=January 2016}}
{{Underlinked|date=October 2016}}
}}

'''Plinian Core''' is a set of vocabulary terms that can be used to describe different aspects of [[biodiversity informatics|biological species information]]. Under "biological species Information" all kinds of properties or traits related to taxa—biological and non-biological—are included. Thus, for instance, terms pertaining descriptions,  legal aspects, conservation, management, demographics, nomenclature, or related resources are incorporated.

== Description ==

The '''Plinian Core''' is aimed to facilitate the exchange of information about the species and upper taxa.
	
What is in scope?
* 	Species level catalogs of any kind of biological objects or data. 
* 	Terminology associated with biological collection data. 
* 	Striving for compatibility with other biodiversity-related standards. 
* 	Facilitating the addition of components and attributes of biological data. 
	What is not in scope? 
* 	Data interchange protocols. 
* 	Non-biodiversity-related data. 
* 	Occurrence level data. 
	This standard is named after Pliny the Elder, a very influential figure in the study of the biological species.
	
'''Plinian Core''' design requirements includes: ease of use, to be self-contained, able to support data integration from multiple databases, and ability to handle different levels of granularity. Core terms can be grouped in its current version as follows:
* 	[[Metadata]]
* 	Base Elements
* 	Record Metadata
* 	[[Nomenclature#Nomenclature.2C classification and identification|Nomenclature and Classification]]
* 	[[Taxonomic description]]
* 	[[Natural history]]
* 	[[Invasive species]]
* 	[[Habitat|Habitat and Distribution]]
* 	[[Demography]] and Threats
* 	Uses, Management and Conservation
* 	associatedParty, MeasurementOrFact, References, AncillaryData

== Background ==
Plinian Core started as a collaborative project between [[Instituto Nacional de Biodiversidad]] and [[GBIF]] Spain in 2005. A series of iterations in which elements were defined and implanted in different projects resulted in a "Plinian Core Flat" [deprecated].

As a result, a new development was impulse to overcome them in 2012. New formal requirements, additional input and a will to better support the standard and its documentation, as well as to align it with the processes of [[TDWG]], the world reference body for biodiversity information standards.

A new version, '''Plinian Core v3.1''' was defined. This provides more flexibility to fully represent the information of a species in a variety of scenarios. New elements to deal with aspects such as IPR, related resources, referenced, etc. were introduced, and elements already included were better-defined and documented.

Partner for the development of Plinian Core in this new phase incorporated the [[University of Granada]] (UG, Spain), the [[Alexander von Humboldt Institute]] (IAvH, Colombia), the [[National Commission for the Knowledge and Use of Biodiversity]] (Conabio, Mexico) and the [[University of São Paulo]] (USP, Brazil).

A "Plinian Core Task Group" within TDWG "Interest Group on species Information"&lt;ref&gt;{{Cite web|title = TDWG: (TDWG) Species Information Interest Group - Charter|url = http://www.tdwg.org/activities/species-information/charter/|website = www.tdwg.org|access-date = 2016-01-29}}&lt;/ref&gt; in being proposed.

== Levels of the standard ==
Plinian Core is presented in to levels: the '''abstract model''' and the '''application profiles'''.

The abstract model (AM), comprising the abstract model schema(xsd)  and the terms' URIs, is the normative part. It is all comprehensive, and allows for different levels of granularity in describing species properties. The AM should be taken as a "menu" from which to choose terms and level of detail needed in any specific project.

The subsets of the abstract model intended to be implemented in specific projects are the "application profiles" (APs). Besides containing part of the elements of the AM, APs can impose additional specifications on the included elements, such as controlled vocabularies.  Some exampes of APs in use follow:
# Application profile [https://raw.githubusercontent.com/PlinianCore/Sources/master/xsd/application%20profiles/stable%20version/PliC3.2.1_AP_CONABIO.xsd CONABIO]
# Application profile [https://raw.githubusercontent.com/PlinianCore/Sources/master/xsd/application%20profiles/stable%20version/PliC3.2.1_AP_INBIO.xsd INBIO]
# Application profile [https://raw.githubusercontent.com/PlinianCore/Sources/master/xsd/application%20profiles/stable%20version/PliC3.2.1_AP_GBIF_ES.xsd GBIF.ES]
# Application profile [https://raw.githubusercontent.com/PlinianCore/Sources/master/xsd/application%20profiles/stable%20version/PliC3.2.1_AP_MAGRAMA.xsd MAGRAMA]
# Application profile [https://raw.githubusercontent.com/PlinianCore/Sources/master/xsd/application%20profiles/stable%20version/PliC3.2.1_AP_SIB-COLOMBIA.xsd SIB-COLOMBIA]

== Relation to other standards ==
Plinian incorporates a number of elements already defined by other standards. The following table summarizes these standards and the elements used in Plinian Core:
{| class="wikitable"
!Standard
!Elements
|-
|[[Darwin Core]]
|taxonConceptID, Hierarchy, MeasurementOrFact, ResourceRelationShip.
|-
|[[Ecological Metadata Language]]
|associatedParty, keywordSet, coverage, dataset
|-
|[[Encyclopedia of Life]] Schema
|AncillaryData: DataObjectBase
|-
|[[Global Invasive Species Network]]
|origin, presence, persistence, distribution, harmful, modified, startValidDate, endValidDate, countryCode, stateProvince, county, localityName, county, language, citation, abundance...
|-
|[http://www.tdwg.org/schemas/tcs/1.01 Taxon Concept Schema. TCS]
|scientificName
|}

== External Links ==
* [http://www.github.com/PlinianCore Main page]
* [http://tools.gbif.org/dwca-validator/extensions.do An Implementation of Plinian Core as GBIF's IPT Extensions]
* [https://github.com/PlinianCore/Documentation/wiki/PlinianCore_Terms Plinian Core Terms Quick Reference Guide]
* [https://raw.githubusercontent.com/PlinianCore/Sources/master/xsd/abstract%20models/stable%20version/PlinianCore_AbstractModel_v3.2.1.xsd Plinian Core Abstract Model (xsd). Current version (3.2.1)]
* [http://www.tdwg.org/ Biodiversity Information Standards (TDWG)]
* [http://www.ingurumena.ejgv.euskadi.eus/r49-u95/es/contenidos/informacion/naturaeuskadi/es_def/adjuntos/plinian.pdf Sistema de información de la naturaleza de euskadi. Aplicación del estandar Plinian Core]
* [http://www.magrama.gob.es/es/biodiversidad/servicios/banco-datos-naturaleza/informacion-disponible/BDN_Modelos_Datos.aspx Estándar Plinian Core para la gestión integrada de la información sobre especies. Ministerio de Agricultura, Alimentación y Medio Ambiente de España]
* [http://patrimonio.ambiente.gob.ec/bndv/modelo.php Modelo conceptual de la Base Nacional de Datos de Vegetación. Ministerio del Ambiente, Ecuador]

== References ==
&lt;references /&gt;

[[Category:Biodiversity informatics]]
[[Category:Knowledge representation]]
[[Category:Interoperability]]
[[Category:Metadata standards]]</text>
      <sha1>3gdxm7mdyv6e1tvpcs74klfttaffqzo</sha1>
    </revision>
  </page>
</mediawiki>